{
  "article_text": [
    "physical systems require very often different descriptions at the micro and macro scale . it is the case for instance in systems which exhibit emergent phenomena , and for systems which undergo a phase transition . in this case",
    ", one could argue that the degrees of freedoms change with the scale effectively , and thus phase space counting should be different depending on the lens with which one look at the system .",
    "this line of thinking has been very fruitful in the last century , since the very initial work of gell - man and low on the renormalization group .",
    "the concept of emergence , in particular , has enlighted many physical phenomena , giving them in the first place both a renewed appeal from the new interpretation . with this same line of reasoning",
    ", dynamical systems can often exhibit correlations which are not only time dependent , but that at short time scales with respect to thermalization typical time scale , exhibit different behaviors .",
    "the introduction of a macroscopic entropy functional for statistical systems has been introduced by lloyd and pagels in @xcite , and at the same time by lindgren @xcite .",
    "lloyd and pagels showed that the depth of a hamiltonian system is proportional to the difference between the system and the coarse grained entropy .",
    "this paper introduced the concept of `` thermodynamic depth '' .",
    "if @xmath1 is the probability that a certain system arrived at a macroscopic state @xmath2 , then the thermodynamic depth of that state is proportional to @xmath3 .",
    "this implies that the average depth of a system , the complexity , is proportionall to the shannon entropy , or the boltzman entropy .",
    "in addition , it has been shown in @xcite that the only functional that is continuous , monotonically increasing with system size , and is extensive is the boltzman functional up to a constant @xcite .",
    "one can show that such argument is true also for _ macroscopic states _",
    ", described by trajectories @xmath4 . in this case , the thermodynamic depth of this state is given by @xmath5 .",
    "in general , the average depth of a system with many macroscopic states can be very large .",
    "in fact , it has been shown in @xcite that the macroscopic entropy defined by : @xmath6 is monotonically increasing , i.e. @xmath7 , and @xmath8 .",
    "it has been also shown that , if in general the macrostate is described by a string of length @xmath9 , one can obtain a finite specific thermodynamic depth , @xmath10 with @xmath11 being the infinite string .",
    "the idea of thermodynamic depth has inspired ekroot and cover to introduce the entropy of markov trajectories in @xcite .",
    "if @xmath12 denote the @xmath2th row of a markov transition matrix , one can define the entropy of a state @xmath2 as : @xmath13 with @xmath14 being the markov operator .",
    "if one introduces the probability of a trajectory going from @xmath2 to @xmath15 as @xmath16 , then , the macroscopic entropy of the markov trajectory is given by : @xmath17 for markov chains , one has that @xmath18 , which thus leads to a recurrence relation : @xmath19 which follows from the chain rule of the entropy , and allows to calculate a closed formula for @xmath20 in terms of the entropy of the nodes , that we will call @xmath21 , and the asymptotic , stationary distribution of the markov chain , @xmath22 .    over the last decade , a huge effort has been devoted to understanding processes on networks @xcite , understanding their statistical properties , as interactions very often occur on nontrivial network topologies , as for instance scale free or small world networks , called complex networks . with this widespread interest in networks , the study of global properties of graphs and graph ensembles has given a renewed impetuous to the study of entropies on graphs .",
    "in general , in analogy with what happens for markov chains , one is interested in quantifying their complexity by means of information theory approach . since for strongly connected graphs , the transition kernel , given by @xmath23 , with @xmath24 being the adjacency matrix of the graph and @xmath25 being the diagonal matrix of degree with @xmath26 . if @xmath27 is an ergodic operator ( which depends on the topological properties of the underlying graph ) , one can study operators based on the asymptotic properties of a random walk .",
    "the dynamics and the structure of many physical networks , such as those involved in biological , physical , economical and technological systems , is often characterized by the topology of the network itself .    in order to quantify the complexity of a network , several measures of complexity of a network",
    "have been introduced , as for instance in @xcite , studying the entropy associate to a certain partitioning of a network .",
    "the standard boltzmann entropy per node was defined as the transition kernel of a random walk in @xcite . in general , in complex networks , one is interested in the average complexity of an ensemble of networks of the same type , as for instance erds - renyi or watts - strogats and barabsi - albert random graphs .",
    "along these lines in particular , we mention the entropy based on the transition kernel of anand and bianconi @xcite .",
    "one can in fact write the partition function of a network ensemble subject to a micro - canonical constraint ( the energy ) and then , given the probability of certain microcanonical ensemble , calculate its entropy , similarly to what proposed in @xcite for random graphs .    in general ,",
    "an entropy of a complex network can be associated from a test particle performing a diffusion process on the network , as in @xcite ; for scale free networks , it is found that the entropy production rate depends on the tail of the distribution of nodes , and thus on the exponent of the tail .    along these lines , in @xcite a von neumann entropy based on the graph laplacian has been introduced , merging results inspired from pure states in quantum mechanics , and networks , and finding that the von neumann entropy is related to the spectrum of the laplacian @xcite . in particular , it has been shown that many graph properties can be identified using this laplacian approach .",
    "a huge body of work has been done by the group of burioni and cassi , in defining the statistical properties of graphs for long walks , for instance using a heat kernel approach to study spectral and fractal dimensions of a graph , finite and infinite ( see @xcite and references therein ) .",
    "in general , these approaches rely on a local operator ( transition kernels , laplacians ) with support on the graph . therefore , if one is interested in knowing macroscopic properties of the graph , is indeed forced to use non - local operators .",
    "in addition to the theoretical interest of describing the macroscopic properties of a graph in terms of information theory quantities , it is important to remark that very often these have important applications in classifying systems according to their topological properties .",
    "for instance , in @xcite it has been shown that graph entropy can be used to differentiate and identify cancerogenic cells . in particular , @xcite shows the importance of studying entropies based on the non - local ( macroscopic ) properties of a network , as for instance the _ higher - order _ network entropy given by    @xmath28    with @xmath29 satisfying an approximate diffusion equation at @xmath30th order , @xmath31    in addition to the approaches just described , one could think of using , instead of the diffusion kernel above , a node - entropy based on diffusion as @xmath32 .",
    "it is easy to see , however that for @xmath33 , if the operator is ergodic , the asymptotic entropy is independent from the initial state : it easy a known fact that if @xmath27 has a unique perron root , @xmath34 , where @xmath35 is a nilpotent operator such that @xmath36 .",
    "the same happens for the diffusion kernel at long times : in this case the diffusion kernel approaches the asymptotic distribution , which indeed has forgotten from which node the diffusion started . with the aim of retaining the information on the node",
    ", we introduce the entropy on the paths originating at a node which , as we shall show , has very interesting asymptotic properties for long walks . in the next section",
    "we describe the construction of the non - local entropy , and an application to random graphs and fractals .",
    "conclusions follow .",
    "we shall start by introducing some basic definitions .",
    "let us consider a markov operator @xmath14 on @xmath35 states , i.e.@xmath37 , such that : @xmath38 with @xmath39 .",
    "entropy gives a measure of how mixing are some states , i.e. how much one state is related to the other states @xmath15 .",
    "we can the define the following quantity :    @xmath40    that for reasons it will be clear soon , we call first order entropy . ] .",
    "it is clear , however , the this definition is purely local , i.e. , the mixing defined by the first order entropy is a local concept , as the it gives a sense of how much mixing there is at the second step in a markov process for each node . generalizing this entropy for longer times ,",
    "i.e. when the @xmath27 operator is applied several times , is not obvious .",
    "one obstruction might be given , in fact , by the ergodicity of the operator : @xmath41 in this case , the operator @xmath42 is trivial , in the sense that each row of @xmath42 is identical , due to the ergodic theorem , and thus eqn .",
    "( [ entropy ] ) is non - trivially generalizable if one wants to assign a ranking to each node . in the approximation of",
    "long walks , if one used the operator @xmath42 , eqn .",
    "( [ entropy ] ) would become : @xmath43 as a result of the ergodic theorem , the entropy evaluated on asymptotic states is independent from the initial condition .",
    "however , here we argue that there is a definition of entropy which indeed depends on the initial condition , which is the macroscopic entropy evaluated on the space of trajectories .",
    "we thus introduced the following entropy on the paths a markov particle went through after @xmath0-steps , or @xmath0th order entropy : @xmath44 where @xmath45 is a factor which depends only on @xmath0 , and is used to keep the entropy finite in the limit @xmath33 .",
    "we will first assume that @xmath45 does not depend on any other parameter ; this choice easily leads to the factor @xmath46 . following the discussion in @xcite ,",
    "it is easy to show , after having defined the @xmath47 operation on the entropy on paths of length @xmath0 , @xmath48 that @xmath49 and @xmath50 , @xmath51 and @xmath52 .   +   + this implies also that this definition of macroscopic entropy has good asymptotic properties , i.e. that @xmath49 , @xmath53 a unique @xmath54 such that @xmath55    in order to better interpret this non - local entropy , we introduce the following notation . we denote with @xmath56 , a path , a string of states of length k , @xmath57 and with @xmath58 an infinite string of states of the form @xmath59 .",
    "we then denote as @xmath60 the ordered product @xmath61 and with @xmath62 the infinite product , @xmath63 we also denote with @xmath64 ( @xmath65 ) , the sums over all possible paths of length @xmath0 ( infinite ) starting in @xmath2 and ending in @xmath15 , and @xmath66 the sum over all possible paths of length @xmath0 ( infinite ) starting at @xmath2 .",
    "we can then write , compactly ( setting temporarily @xmath67 ) :    @xmath68    it is now easy to see that this can be written in terms of products : @xmath69 which gives an idea of how fast this product can grow as a function of @xmath0 . in order to set the stage for what follows ,",
    "let us consider the simpler case of a 2 dimensional markov chain with transition probabilities parametrized by two positive parameters , @xmath70 : @xmath71 and our aim is now to use a recursion relations for the infinite trees in order to calculate the exact values for @xmath72 and @xmath73 .",
    "this can be easily generalized , and in fact there is no obstruction to calculate this for generic markov matrices ; as we will see shortly the result is independent on the dimensionalilty of the matrix .",
    "let us thus consider the entropies for @xmath74 and @xmath75 .",
    "these can be written recursively , for @xmath0 , as @xmath76 now we can use the properties of logarithms , and the fact that @xmath77 following from the fact that the matrix is stochastic .",
    "we can at this point separate the various terms , obtaining :    @xmath78    and thus we reach the following recursive equation : @xmath79 where we see that the first order entropy enters : @xmath80 in doing the calculation , we observe that now we have a generic formula , which depends only on the markov operator @xmath27 . due to the linearity of the recursion relation ,",
    "it is easy to observe that it is independent from the dimensionality .",
    "we observe that the @xmath81 case can be taken into account by defining @xmath82 .    for generic @xmath0",
    ", this equation can be written as : @xmath83 first of all we notice that the limit @xmath84 is well defined , and so is its cesro mean . to see this",
    ", we see that @xmath27 is a positive bounded operator , @xmath85 .",
    "thus , @xmath86 by the cesro mean rule , we have then that @xmath87 and thus we discover that also for this entropy , @xmath88 , with @xmath89 , and thus we failed yet to distinguish the entropy of the paths for each single node .",
    "it is easy to realize that this is due to the normalization factor , @xmath90 , which thanks to the cesro rule leads to a different result we were looking for to begin with .    in order to do improve the counting , then , we can assume that now the normalization factor @xmath45 depends on an extra parameter @xmath91 , @xmath92 .",
    "in particular , we will be interested in the natural choice of contractions , i.e. @xmath93 , as this choice has nice asymptotic behavior and has a straightforward interpretation in terms of path lengths , as we will see after .",
    "we thus consider the following entropy functional :    @xmath94    this formula is now defined in terms of an extra parameter @xmath91 ; we will now show that thanks to the recursion rule , one can find a closed formula at @xmath33 .",
    "having gained experience on how to write the recursion rule in the previous section , we promptly modify the recursion rule in order to account for the normalization @xmath95 .",
    "thus , following the same decomposition in order to find the recurrence rule , we find :    @xmath96    which leads to the following closed formula for the recursion , in terms of @xmath27 , @xmath97 and @xmath91 : @xmath98 writing down all the terms , recursively , we find : @xmath99 and , realizing that we can now take the limit @xmath100 safely , : @xmath101 which is finite if @xmath102 , and is the main result of this paper . a compact way of rewriting equation in eqn .",
    "( [ entfin ] ) , is by multiplying and dividing by @xmath91 , and writing the entropy in terms of the matrix resolvent @xmath103 .",
    "@xmath104    we thus now see that we have traded the infinity for a `` forgetting '' parameter @xmath91 , which adds a further variable to the analysis , and which might seem puzzling at first .",
    "in particular , we do recognize that this operator has been widely used in several fields , which is reassuring .",
    "in fact , the entropy just introduced resembles several centrality measures on network , as for instance the katz centrality , although applied to a vector which is different than a column of ones , but has the entropy calculated at the first order for in each node .",
    "in particular , we realize that the resolvent is often used for measuring correlations ( for instance in @xcite ) .    it is worth",
    "make few comments regarding eqn .",
    "( [ entfin ] ) .",
    "it is striking , as often it happens , that it is indeed easier to calculate this path entropy , thanks to the fixed point , for an infinite number of steps , rather than a finite number .",
    "we can in fact easily convince ourselves that calculating all the paths of length @xmath0 on a graph can grow as @xmath105 in the worst case , which can be a rather big number for fairly small graphs after few steps . using the formula above",
    ", one can calculate this entropy by merely inverting a matrix which , apart from being the fortune of several search engines and big data analysts , and although being slow in some cases or can have convergence problems , can be done also for large matrices ( and thus graphs ) , which is rather convenient .",
    "if this was not enough , one can however also tune the parameter @xmath106 in order calculate the entropy ( on average ) after a finite number of steps @xmath107 , as we shall soon show .    in general",
    ", one could generalize this formula by refining on the type of paths one is interested of summing on ( for instance , self - avoiding loops , closed random walks ) . although this approach is definitely feasible , it is hard , at the end of the computation , to find a closed formula at the fixed point .",
    "the reason is that , by summing on all possible indices , the equations can be written in terms of matrix multiplication of the markov transition kernel , thus simplifying the final equation .    as a final remark",
    ", we note that , differently from the approach of @xcite , we define the macroscopic entropy not on markov trajectories defined by a source node @xmath2 and a destination node @xmath15 , but indeed are aimed at studying the path complexity attached to a node @xmath2 , given by all the possible paths which can be originated from it .",
    "the introduction of a renormalization parameter @xmath91 , able to keep the entropy finite in the asymptotic limit , and at the same time pertaining the information on the originating node , might seem puzzling at first .",
    "in general , as we shall show now , one can associate the parameter @xmath91 to the average length path to be considered . in fact , one can write the average path length , recursively , and obtain the formula :    @xmath108    and under the assumption that @xmath109 , one can obtain the roots of this equation for @xmath91 as a function of @xmath107 , @xmath110 it is easy to see that now @xmath111 $ ] for each value of @xmath107 , and that one can associate the parameter @xmath91 to how far back one wants to weight this parameter .",
    "one might argue that the entropy at @xmath112 diverges , but if one plots the entropy as a function of @xmath107 instead .",
    "@xmath113    one can easily show that for @xmath114 due to the fact that the number of paths grows as @xmath115 for a constant @xmath116 in the transient phase , but when thermalization occurs , the entropy grows linearly with @xmath107 . for large values of @xmath107 , this entropy grows at @xmath117 , given by : @xmath118 this indeed shows that a the complexity of a node @xmath2 , for @xmath119 , diverges as @xmath120 , with @xmath121 being a fitting parameter , which we hereon call _ asymptotic path complexity_. if one approximates the transient behavior of this entropy as a functio of the form @xmath122 , it is clear that the transient is characterized by the ratio @xmath123 .    before studying the growth of this entropy",
    ", we first write few identities .",
    "we write @xmath124 , with @xmath125 then , one can use the operatox @xmath126 and use the second resolvent identity @xmath127 $ ] , to put in relation the two :    @xmath128\\ ^1 \\vec s   \\nonumber \\\\",
    "= -\\frac{1}{\\epsilon_2 \\epsilon_1 } ( \\frac{1}{\\epsilon_2}-\\frac{1}{\\epsilon_1})[\\epsilon_2   \\ ^ * \\vec s_{\\epsilon_2}-\\epsilon_1 \\ ^ * \\vec s_{\\epsilon_1}]\\end{aligned}\\ ] ]    and thus find the identity : @xmath129\\end{aligned}\\ ] ] which , after rearrangement can be casted into the simpler form : @xmath130    showing that the @xmath91-path complexity can be _ evolved _ from one particular @xmath91 to another using the resolvent .",
    "we are now interested in showing how the path entropy can indeed provide important informations of the properties of a graph .",
    "we thus study the random walk on a graph , with transition matrix @xmath23 , with @xmath25 being the degree matrix .",
    "in particular , having shown that differently from analogous graph entropies present in the literature one can still distinguish asymptotically different nodes according to their path complexity , we would like to rank nodes according to their complexity for .",
    "a first test of this statement is applied to random ( positive ) matrices of different sizes @xmath35 , as in fig .",
    "[ fig : randomn ] . as a first comment ,",
    "it is easy to see that the complexity depends on the size of the graph , @xmath35 , showing that different growth curves appear as a function of @xmath107 .",
    "although for different sizes , the entropies are clustered around similar values , zooming onto the curve shows that indeed these pertain the memory on their path complexity in the factor @xmath131 , asymptotically .",
    "one can then perform a similar analysis for other type of random graphs .",
    "we extend this analysis also to the case of erds - renyi graphs in fig .",
    "[ erdosrenyi ] .",
    "we have generated various instances of random graphs , according to different realizations of the probability parameter @xmath132 to have a link or not ; we have considered graph with the same number of nodes , @xmath133 .",
    "it is easy to see tha different growth curves can be distinguished according to the parameter , although these curves become more and more similar for larger values of @xmath132 .",
    "-path complexity for erds - renyi graphs , generated with value of the probability parameter @xmath134 .",
    "the lower set of curves is associated with the probability parameter @xmath135 , and the higher with @xmath136.,title=\"fig : \" ] [ fig : random ]    as a case study , we evaluate the complexity of nodes on a self - similar graph , as for instance the sierpinsky fractal .",
    "the results are shown in fig .",
    "[ fig : sier ] .",
    "we have analyzed the growth curves for each node , for a graph with @xmath137 nodes , observing that few nodes exhibited lower growth curved as compared to the others . by plotting a heat map of the node complexity on the fractal",
    ", one observes that the nodes at the boundary of the sierpinsky fractal have lower path complexity .",
    "a histogram of the asymptotic complexity @xmath131 shows that most of the nodes exhibit a similar complexity , meanwhile fewer nodes can be clearly distinguished from the others .",
    "-path complexity for the nodes of a sierpinsky fractal as a function of @xmath107 .",
    "we observe that , although most of the nodes have similar entropies , there are few outliers . in order",
    "to identify which nodes exhibit lower complexity , we plot a heatmap of a sierpinsky fractal in the figure on the right . _",
    "central : _ a heat map of path complexity for the sierpinsky fractal evaluated at @xmath138 , and with a number of nodes @xmath139 .",
    "we observe how , although the self - similarity properties , the entropy is able to identify points which possess lower path complexity on the boundary .",
    "_ bottom : _ we plot the frequency distribution of asymptotic complexity . we see that a large fraction of node have analogous asymptotic path complexity value very close to 1.38 , meanwhile few nodes , showin in the top right figure , take lower values.,title=\"fig : \" ] -path complexity for the nodes of a sierpinsky fractal as a function of @xmath107 .",
    "we observe that , although most of the nodes have similar entropies , there are few outliers . in order",
    "to identify which nodes exhibit lower complexity , we plot a heatmap of a sierpinsky fractal in the figure on the right . _",
    "central : _ a heat map of path complexity for the sierpinsky fractal evaluated at @xmath138 , and with a number of nodes @xmath139 .",
    "we observe how , although the self - similarity properties , the entropy is able to identify points which possess lower path complexity on the boundary . _",
    "bottom : _ we plot the frequency distribution of asymptotic complexity . we see that a large fraction of node have analogous asymptotic path complexity value very close to 1.38 , meanwhile few nodes , showin in the top right figure , take lower values.,title=\"fig : \" ] -path complexity for the nodes of a sierpinsky fractal as a function of @xmath107 .",
    "we observe that , although most of the nodes have similar entropies , there are few outliers . in order",
    "to identify which nodes exhibit lower complexity , we plot a heatmap of a sierpinsky fractal in the figure on the right . _",
    "central : _ a heat map of path complexity for the sierpinsky fractal evaluated at @xmath138 , and with a number of nodes @xmath139 .",
    "we observe how , although the self - similarity properties , the entropy is able to identify points which possess lower path complexity on the boundary . _",
    "bottom : _ we plot the frequency distribution of asymptotic complexity . we see that a large fraction of node have analogous asymptotic path complexity value very close to 1.38 , meanwhile few nodes , showin in the top right figure , take lower values.,title=\"fig : \" ]",
    "in this paper we have introduced and studied the entropy associated with the number of paths originating at a node of a graph . motivated by distinguishing the asymptotic behavior of non - local entropy defined on a graphs , and inspired by earlier studies on macroscopic entropies , we have obtained a closed formula for the path complexity of a node in a graph .",
    "this entropy can be thought as the centrality operator applied to the local definition of entropy of a node in a graph , and depends on an external constant , that we introduced in order to keep the entropy finite asymptotically .",
    "although the entropy introduced in the present paper is a non - local , the extra parameter has a nice interpretation in terms of the average number of walks to be considered .",
    "this allows to study the average transient behavior of the entropy , and in particular to introduce the asymptotic path complexity , given by the constant which charachterizes the asymptotic behavior of the path complexity of a node .",
    "we have applied this entropy to studying ( normalized ) random matrices , random graphs and fractals , and in particular have shown that the overal complexity of a node depends in value on the size of the graph . for random graphs , we have shown that one that the average asymptotic behavior of a node depends on the value of the probability parameter @xmath132 .",
    "in addition , we have shown that this entropy is able to distinguish points in the bulk of a fractal from those at some specific boundaries , showing that these have lower path complexity as compared to the others .",
    "in general , we have the feeling of having introduced a new measure of macroscopic complexity for graphs , based on the fact that the number of paths generating at a node can differ substantially depending where a node is located with respect to the whole graph . given this non - local definition",
    ", one would expect that the path complexity can give important insights on the relevance of topological properties of networks in several of their applications .",
    "in addition , we have compared this entropy to those introduced in the past , showing that this entropy contributes to the growing literature on graph entropies . as a closing remark",
    ", we believe that this entropy has better asymptotic properties ( long walks ) as compared to those introduced so far , and thus can be used to study the properties of large graphs .",
    "we would like to thank j. d. farmer , j. mcnerney and f. caccioli for comments on an earlier drafting of this entropy .",
    "we aknowledge funding from icif , epsrc : ep / k012347/1 .",
    "+   +      s. lloyd , h. pagels , `` complexity as thermodynamic depth '' , ann . of phys .",
    "188 , ( 1988 ) k. lindgren , `` microscopic and macroscopic entropy '' , phys .",
    "rev . a , vol .",
    "38 no 9 , ( 1988 ) s. lloyd , `` valuable information '' , in _ complexity , entropy and the physics of information _ , sfi studies in the sciences of complexity .",
    "addison - wesley ma , ( 1990 ) l. ekroot , t. m. cover , `` the entropy of markov trajectories '' , ieee transactions on information theory , vol 39 , no 4 , ( 1993 ) a. barrat , m. barthelemy , a. vespignani , dynamical processes on complex networks , cambridge university press , cambridge , uk , ( 2009 ) d. gfeller , j .- c .",
    "chappelier and p. de los rios,``finding instabilities in the community structure of complex networks '' , phys .",
    "e 72 , 056135 , ( 2005 ) g. bianconi,``the entropy of randomized network ensembles '' , europhys .",
    "lett 81 , 28005 ( 2008 ) ; g. bianconi,``entropy of network ensembles '' , phys .",
    "e 79 , 036114 , ( 2009 ) k. anand , g. bianconi , `` entropy measures for complex networks : toward an information theory of complex topologies '' , phys .",
    "e 80 , 045102(r ) , ( 2009 )    l. bogacz , z. burda , b. waclaw,``homogeneous complex networks '' , physica a , 366 , 587 ( 2006 ) j. gomez - gardenes , v. latora , `` entropy rate of diffusion processes on complex networks '' , phys . rev .",
    "e 78 , 0655102 , ( 2008 ) s. l. braunstein , s. ghosh , s. severini , `` the laplacian of a graph as a density matrix : a basic combinatorial approach to separability of mixed states '' , annals of combinatorics , 10 , no 3 , ( 2006 ) ; f. passerini , s. severini , `` the von neumann entropy of networks '' , arxiv:0812.2597 , ( 2008 ) s. burioni , d. cassi , `` random walk on graphs : ideas , techniques and results'',topical review , jour .",
    "phys . a 38 , r45 , ( 2005 ) j. west , g. bianconi , s. severini , a. e. teschendorff , `` differential network entropy reveals cancer system hallmarks '' , scientific reports 2 : 802 , ( 2012 ) d. griffith , `` spatial autocorrelation : a primer '' .",
    "washington , dc : association of american geographers resource publication , ( 1987 )"
  ],
  "abstract_text": [
    "<S> thermalization is one of the most important phenomena in statistical physics . </S>",
    "<S> often , the transition probabilities between different states in the phase space is or can be approximated by constants . in this case </S>",
    "<S> , the system can be described by markovian transition kernels , and when the phase space is discrete , by markov chains . in this paper </S>",
    "<S> , we introduce a macroscopic entropy on the states of paths of length @xmath0 and , studying the recursion relation , obtain a fixed point entropy . </S>",
    "<S> this analysis leads to a centrality approach to markov chains entropy . </S>"
  ]
}