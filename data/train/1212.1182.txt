{
  "article_text": [
    "the classical statistical pattern recognition setting involves @xmath3 where the @xmath4 are observed feature vectors , and the @xmath5 are observed class labels , for some probability distribution @xmath6 on @xmath7 .",
    "let @xmath8 .",
    "a classifier @xmath9 whose probability of error @xmath10 $ ] approaches bayes - optimal as @xmath11 for all distributions @xmath12 is said to be _ universally consistent_. for example , the k - nn classifier with @xmath13 is universally consistent @xcite .",
    "* input * : @xmath14 , training set @xmath15 = \\{1,2,\\ldots , n\\}$ ] and labels @xmath16 .",
    "* output * : class labels @xmath17 \\setminus \\mathcal{t}\\}$ ]",
    ". _ step 1 _ : compute the eigen - decomposition of @xmath18 .",
    "_ step 2 _ : let @xmath19 be the `` elbow '' in the scree plot of @xmath20 , @xmath21 the diagonal matrix of the top @xmath19 eigenvalues of @xmath20 and @xmath22 the corresponding columns of @xmath23 .",
    "_ step 3 _ : define @xmath24 to be @xmath25 . denote by @xmath26 the @xmath27th row of @xmath24 .",
    "define @xmath28 the rows of @xmath24 corresponding to the indices set @xmath29 .",
    "@xmath24 is called the adjacency spectral embedding of @xmath20 . _ step 4 _ : find a _ linear _ classifier @xmath30 that minimizes the empirical @xmath2-loss when trained on @xmath31 where @xmath2 is a _",
    "_ loss function that is a surrogate for 01 loss",
    ". _ step 5 _ : apply @xmath30 on the @xmath32 \\setminus\\mathcal{t}\\}$ ] to obtain the @xmath33 \\setminus\\mathcal{t}\\}$ ] .    in this paper",
    ", we consider the case wherein the feature vectors are unobserved , and we observe instead a latent position graph @xmath34 on @xmath35 vertices with positive definite link function @xmath36 $ ] .",
    "the graph @xmath37 is constructed such that there is a one - to - one relationship between the vertices of @xmath37 and the feature vectors @xmath38 , and the edges of @xmath37 are _ conditionally independent _ bernoulli random variables given the _ latent _",
    "we show that there exists a universally consistent classification rule for this extension of the classical pattern recognition setup to latent position graph models , provided that the link function @xmath0 is an element of the class of _ universal kernels_. in particular , we show that a classifier similar to the one described in algorithm  [ algmain ] is universally consistent .",
    "algorithm  [ algmain ] is an example of a procedure that first embeds data into some euclidean space and then performs inference in that space .",
    "these kind of procedures are popular in analyzing graph data , as is evident from the vast literature on multidimensional scaling , manifold learning and spectral clustering .",
    "the above setting of classification for latent position graphs , with @xmath0 being the inner product in @xmath39 , was previously considered in  @xcite .",
    "it was shown there that the eigen - decomposition of the adjacency matrix @xmath20 yields a consistent estimator , up to some orthogonal transformation , of the latent vectors @xmath40 .",
    "therefore , the k - nn classifier , using the estimated vectors , with @xmath41 is universally consistent .",
    "when @xmath0 is a general , possibly unknown link function , we can not expect to recover the latent vectors .",
    "however , we can obtain a consistent estimator of some feature map @xmath42 of @xmath0 .",
    "classifiers that use only the feature map @xmath43 are universally consistent if the space @xmath44 is isomorphic to some dense subspace of the space of measurable functions on @xmath45 .",
    "the notion of a universal kernel characterizes those @xmath0 whose feature maps @xmath43 induce a dense subspace of the space of measurable functions on @xmath45 .",
    "the structure of our paper is as follows .",
    "we introduce the framework of latent position graphs in section  [ secframework ] . in section",
    "[ secestim - feat - maps ] , we show that the eigen - decomposition of the adjacency matrix @xmath20 yields a consistent estimator for a feature map @xmath46 of @xmath0 .",
    "we discuss the notion of universal kernels and the problem of vertex classification using the estimates of the feature map @xmath43 in section  [ seccons - vert - class ] .",
    "in particular , we show that the classification rule obtained by minimizing a convex surrogate of the 01 loss over a class of linear classifiers in @xmath39 is universally consistent , provided that @xmath47 in a specified manner .",
    "we conclude the paper with a discussion of how some of the results presented herein can be extended and other implications .",
    "we make a brief comment on the setup of the paper .",
    "the main contribution of the paper is the derivation of the estimated feature maps and their use in constructing a universally consistent vertex classifier .",
    "we have thus considered a less general setup of compact metric spaces , linear classifiers , and convex , differentiable loss functions .",
    "it is possible to extends the results herein to a more general setup where the latent positions are elements of a ( non - compact ) metric space , the class of classifiers are uniformly locally - lipschitz , and the convex loss function satisfies the classification - calibrated property @xcite .",
    "let @xmath48 be a compact metric space and @xmath1 a probability measure on the borel @xmath49-field of @xmath45 .",
    "let @xmath50 $ ] be a continuous , positive definite kernel .",
    "let @xmath51 be the space of square - integrable functions with respect to @xmath1 .",
    "we can define an integral operator @xmath52 by @xmath53 @xmath54 is a compact operator and is of trace class .",
    "let @xmath55 be the set of eigenvalues of @xmath54 ordered as @xmath56 .",
    "let @xmath57 be a set of orthonormal eigenfunctions of @xmath54 corresponding to the @xmath58 , that is , @xmath59 the following mercer representation theorem @xcite provides a representation for @xmath0 in terms of the eigenvalues and eigenfunctions of @xmath54 defined above .",
    "[ thm8 ] let @xmath48 be a compact metric space and @xmath60 $ ] be a continuous positive definite kernel .",
    "let @xmath61 be the eigenvalues of @xmath54 and @xmath62 be the associated eigenvectors",
    ". then @xmath63 the sum in equation ( [ eq8 ] ) converges absolutely for each @xmath64 and @xmath65 in @xmath66 and uniformly on @xmath66 .",
    "let @xmath44 denote the reproducing kernel hilbert space of @xmath0 .",
    "then the elements @xmath67 are of the form @xmath68 and the inner product on @xmath44 is given by @xmath69    by mercer s representation theorem , we have @xmath70 .",
    "we thus define the feature map @xmath71 by @xmath72 let @xmath19 be an integer with @xmath73 .",
    "we also define the following map @xmath74 @xmath75 we will refer to @xmath76 as the truncation of @xmath43 to @xmath39 .",
    "now , for a given @xmath77 , let @xmath78 .",
    "define @xmath79 .",
    "let @xmath20 be a symmetric random hollow matrix where the entries @xmath80 are _ conditionally independent _ bernoulli random variables given the @xmath81 with @xmath82 = \\mathbf{k}_{ij}$ ] for all @xmath83 $ ] , @xmath84 .",
    "@xmath20 is the adjacency matrix corresponding to a graph with vertex set @xmath85 .",
    "a graph @xmath37 whose adjacency matrix @xmath20 is constructed as above is an instance of a latent position graph  @xcite where the latent positions are sampled according to @xmath1 , and the link function is @xmath0 .      the latent position graph model and the related latent space approach  @xcite is widely used in network analysis .",
    "it is a generalization of the stochastic block model ( sbm ) @xcite and variants such as the degree - corrected sbm @xcite or the mixed - membership sbm  @xcite or the random dot product graph model  @xcite .",
    "it is also closely related to the inhomogeneous random graph model @xcite or the exchangeable graph model @xcite .",
    "there are two main sources of randomness in latent position graphs .",
    "the first source of randomness is due to the sampling procedure , and the second source of randomness is due to the conditionally independent bernoulli trials that gave rise to the edges of the graphs .",
    "the randomness in the sampling procedure and its effects on spectral clustering and/or kernel pca have been widely studied . in the manifold learning literature ,",
    "the latent positions are sampled from some manifold in euclidean space and @xcite among others studied the convergence of the various graph laplacian matrices to their corresponding laplace ",
    "beltrami operators on the manifold .",
    "the authors of @xcite studied the convergence of the eigenvalues and eigenvectors of the graph laplacian to the eigenvalues and eigenfunctions of the corresponding operators in the spectral clustering setting .",
    "the matrix @xmath86 can be considered as an approximation of @xmath54 for large @xmath77 ; that is , we expect the eigenvalues and eigenvectors of @xmath86 to converge to the eigenvalues and eigenfunctions of @xmath54 in some sense .",
    "this convergence is important in understanding the theoretical properties of kernel pca ; see , for example , @xcite .",
    "we summarized some of the results from the literature that directly pertain to the current paper in appendix  [ secspectra - mathbfk - spec ] .",
    "the bernoulli trials at each edge and their effects had also been studied .",
    "for example , a result in @xcite on matrix estimation for noise - pertubed and subsampled matrices showed that by thresholding the dimensions in the singular value decomposition of the adjacency matrix , one can recover an estimate @xmath87 of the kernel matrix @xmath88 with small @xmath89 .",
    "oliveira  @xcite studied the convergence of the eigenvalues and eigenvectors of the adjacency matrix @xmath20 to that of the integral operator @xmath54 for the class of inhomogeneous random graphs .",
    "the inhomogeneous random graphs in  @xcite have latent positions that are uniform @xmath90 $ ] random variables with the link function @xmath0 being arbitrary symmetric functions .    as we have mentioned in section  [ secintroduction ] , algorithm  [ algmain ] is an example of a popular approach in multidimensional scaling , manifold learning and spectral clustering where inference on graphs proceeds by first embedding the graph into euclidean space followed by inference on the resulting embedding .",
    "it is usually assumed that the embedding is conducive to the subsequent inference tasks .",
    "justification can also be provided based on the theoretical results about convergence , for example , the convergence of the eigenvalues and eigenvectors to the eigenvalues and eigenfunctions of operators , or the convergence of the estimated entries , cited above .",
    "however , these justifications do not consider the subsequent inference problem ; that is , these convergence results do not directly imply that inference using the embeddings are meaningful .",
    "recently , the authors of @xcite showed that the clustering using the embeddings are meaningful , that is , consistent , for graphs based on the stochastic block model and the extended planted partition model .",
    "the main impetus for this paper is to give similar theoretical justification for the classification setting .",
    "the latent position graph model is thus a surrogate model  a  widely - used model with sufficiently simple structure that allows for clear , concise theoretical results .",
    "we assume the setting of section  [ secframework ] .",
    "let us denote by @xmath91 and @xmath92 the set of @xmath93 matrices and @xmath94 matrices on @xmath95 , respectively .",
    "let @xmath96 be the eigen - decomposition of @xmath20 .",
    "for a given @xmath73 , let @xmath97 be the diagonal matrix comprised of the @xmath19 largest eigenvalues of @xmath20 , and let @xmath98 be the matrix comprised of the corresponding eigenvectors .",
    "the matrices @xmath99 are @xmath100 are defined similarly . for a matrix @xmath101 , @xmath102 refers to the spectral norm of @xmath101 while @xmath103 refers to the frobenius norm of @xmath101 . for a vector @xmath104",
    ", @xmath105 will denote the euclidean norm of @xmath106 .",
    "the key result of this section is the following theorem which shows that , given that there is a gap in the spectrum of @xmath54 at @xmath107 , by using the eigen - decomposition of @xmath20 we can accurately estimate the truncated map @xmath76 in section  [ secframework ] up to an orthogonal transformation .",
    "we note that the dependence on @xmath1 , the distribution of the @xmath108 , in the following result is implicit in the definition of the spectral gap @xmath109 of @xmath110 .",
    "[ thm1 ] let @xmath73 be given .",
    "denote by @xmath109 the quantity @xmath111 , and suppose that @xmath112 .",
    "then with probability greater than @xmath113 , there exists a unitary matrix @xmath114 such that @xmath115 where @xmath116 denotes the matrix in @xmath117 whose @xmath27th row is @xmath118 .",
    "let us denote by @xmath119 the @xmath27th row of @xmath120 .",
    "then , for each @xmath121 $ ] and any @xmath122 , @xmath123 \\leq27 \\delta_d^{-2 } \\varepsilon^{-1 } \\sqrt{\\frac{6d \\log{n}}{n}}.\\ ] ]    we now proceed to prove theorem  [ thm1 ] . a rough sketch of the argument goes as follows .",
    "first we will show that the projection of @xmath20 onto the subspace spanned by @xmath22 is `` close '' to the projection of @xmath88 onto the subspace spanned by @xmath100",
    ". then we will use results on the convergence of spectra of @xmath88 to the spectra of @xmath54 to show that the subspace spanned by @xmath22 is also `` close '' to the subspace spanned by  @xmath76 .",
    "we note that , for conciseness and simplicity in the exposition , all probability statements involving the matrix @xmath20 or its related quantities , for example , @xmath124 , are assumed to hold conditionally on the @xmath125 .",
    "we need the following bound for the perturbation @xmath126 from  @xcite .",
    "the convergence of the spectra of @xmath20 to that of @xmath54 as given by theorem 6.1 in  @xcite is similar to that given in the proof of theorem  [ thm1 ] in the current paper , but there are sufficient differences between the two settings , and we do not see an obvious way to apply the conclusions of theorem 6.1 in @xcite to the current paper .",
    "[ prop3 ] for @xmath20 and @xmath88 as defined above , with probability at least @xmath127 , we have @xmath128 where @xmath129 is the maximum vertex degree .",
    "the constant in equation ( [ eq10 ] ) was obtained by replacing a concentration inequality in  @xcite with a slightly stronger inequality from  @xcite .",
    "we now show that the projection matrix for the subspace spanned by @xmath22 is close to the projection matrix for the subspace spanned by @xmath130 .",
    "[ prop4 ] let @xmath131 and @xmath132 .",
    "denote by @xmath133 the quantity @xmath134 , and suppose that @xmath135 . if @xmath77 is such that @xmath136 .",
    "then with probability at least @xmath137 , @xmath138    by equation ( [ eq30 ] ) in theorem  [ thm5 ] , we have with probability at least @xmath127 , @xmath139 now , let @xmath140 and @xmath141 be defined as @xmath142 then we have , with probability at least @xmath127 , @xmath143\\\\[-8pt ] & \\geq & n \\delta_{d } - 4 ( 1 + \\sqrt{2 } ) \\sqrt{n \\log{(n/\\eta)}}. \\nonumber\\end{aligned}\\ ] ] suppose for the moment that @xmath140 and @xmath141 are disjoint , that is , that @xmath144 .",
    "let @xmath145 be the matrix for the orthogonal projection onto the  subspace spanned by the eigenvectors of @xmath20 whose corresponding eigenvalues lies in @xmath140 .",
    "let @xmath146 be defined similarly .",
    "then by the @xmath147 theorem  @xcite we have @xmath148 by equation ( [ eq57 ] ) and proposition  [ prop3 ] , we have , with probability at least @xmath149 , @xmath150 provided that @xmath151 .    to complete the proof",
    ", we note that if @xmath152 , then @xmath140 and @xmath141 are disjoint .",
    "thus @xmath153 . finally ,",
    "if @xmath154 , then the eigenvalues of @xmath20 that lie in @xmath140 are exactly the @xmath19 largest eigenvalues of @xmath20 and @xmath155 .",
    "equation ( [ eq4 ] ) is thus established .",
    "let @xmath44 be the reproducing kernel hilbert space for @xmath0 .",
    "we now introduce a linear operator @xmath156 on @xmath44 defined as follows : @xmath157 the operator @xmath156 is the extension of @xmath88 as an operator on @xmath158 to an operator on @xmath44 . that is , @xmath156 is a linear operator on @xmath44 induced by @xmath0 and the @xmath159 .",
    "the eigenvalues of @xmath156 and @xmath88 coincide , and furthermore , an eigenfunction of @xmath156 is a linear interpolation of the corresponding eigenvector of @xmath88 .",
    "the reader is referred to appendix  [ secspectra - mathbfk - spec ] for more details .",
    "the next result states that the rows of @xmath160 correspond to projecting the @xmath161 using @xmath162 , where @xmath162 is the projection onto the @xmath19-dimensional subspace spanned by the eigenfunctions associated with the @xmath19 largest eigenvalues of @xmath156 .",
    "we note that for large @xmath77 , @xmath162 is close to the projection onto the @xmath19-dimensional subspace spanned by the eigenfunctions associated with the @xmath19 largest eigenvalues of @xmath54 with high probability ; see theorem  [ thm5 ] .",
    "[ lem4 ] let @xmath162 be the projection onto the subspace spanned by the eigenfunctions corresponding to the @xmath19 largest eigenvalues of @xmath156 .",
    "the rows of @xmath163 then correspond , up to some orthogonal transformation , to projections of the feature map @xmath43 onto @xmath39 via @xmath162 , that is , there exists a unitary matrix @xmath164 such that @xmath165^{t},\\ ] ] where @xmath166 is the isometric isomorphism of a finite - dimensional hilbert space onto  @xmath39 .",
    "the proof of lemma  [ lem4 ] is given in the .",
    "proof of theorem  [ thm1 ] we first note that the sum of any row of @xmath20 is bounded from above by @xmath77 , thus @xmath167 .",
    "similarly , @xmath168 . on combining equation ( [ eq4 ] ) and equation ( [ eq10 ] ) , we have , with probability at least @xmath169 , @xmath170 by lemma  [ lem3 ] in the , there exists an orthogonal @xmath164 such that @xmath171 we note that @xmath172 provided that @xmath77 satisfies @xmath173 .",
    "thus , we have @xmath174 with probability at least @xmath169 .",
    "now , by lemma  [ lem4 ] , the rows of @xmath160 are ( up to some orthogonal transformation ) the projections of the feature map @xmath43 onto @xmath39 via @xmath162 . on the other hand",
    ", @xmath175 is the projection of @xmath176 onto @xmath39 via @xmath177 . by theorem  [ thm5 ] in the , for all @xmath178",
    ", we have @xmath179 with probability at least @xmath180 .",
    "we therefore have , for some orthogonal @xmath181 , @xmath182 with probability at least @xmath180 .",
    "equation ( [ eqxbnd ] ) in the statement of the theorem then follows from equation ( [ eq48 ] ) and equation ( [ eq74 ] ) .    to show equation ( [ eqxibnd ] )",
    ", we first note that as the @xmath81 are independent and identically distributed , the @xmath183 are exchangeable and hence identically distributed .",
    "let @xmath184 . by conditioning on the event in equation ( [ eqxbnd ] ) ,",
    "we have @xmath185 & \\leq & \\sqrt{\\mathbb{e}\\bigl[\\bigl\\| \\hat{\\phi}_{d}(x_i ) - \\phi_{d}(x_i ) \\bigr\\|^{2}\\bigr ] } \\nonumber \\\\ & \\leq & \\sqrt { \\frac{1}{n } \\mathbb{e}\\bigl[\\|\\hat{\\bolds { \\phi}}_{d}- \\bolds{\\phi}_{d } \\|_f^2 \\bigr ] } \\nonumber\\\\[-8pt]\\\\[-8pt ] & \\leq & \\frac{1}{\\sqrt{n}}\\sqrt { \\biggl(1-\\frac{2}{n^2 } \\biggr ) \\bigl(27 \\delta_d^{-2 } \\sqrt{3d \\log{n } } \\bigr)^{2 } + \\frac{2}{n^2 } 2n } \\nonumber \\\\ & \\leq & 27 \\delta_{d}^{-2 } \\sqrt { \\frac{6 d \\log{n}}{n } } , \\nonumber\\end{aligned}\\ ] ] because the worst case bound is @xmath186 with probability @xmath187 . equation ( [ eqxibnd ] ) follows from equation ( [ eq19 ] ) and markov s inequality .",
    "the results in section  [ secestim - feat - maps ] show that by using the eigen - decomposition of @xmath20 , we can consistently estimate the truncated feature map @xmath76 for any fixed , finite @xmath19 ( up to an orthogonal transformation ) .",
    "in the subsequent discussion , we will often refer to the rows of the eigen - decomposition of @xmath20 , that is , the rows of @xmath188 as the _ estimated vectors_. sussman , tang and priebe @xcite showed that , for the dot product kernel on a finite - dimensional space @xmath45 , the @xmath189-nearest - neighbors classifier on @xmath39 is universally consistent when we select the neighbors using the estimated vectors rather than the true but unknown latent positions .",
    "this result can be trivially extended to the setting for an arbitrary finite - rank kernel @xmath0 as long as the feature map @xmath43 of @xmath0 is injective .",
    "it is also easy to see that if the feature map @xmath43 is not injective , then any classifier that uses only the estimated vectors ( or the feature map @xmath43 ) is no longer universally consistent .",
    "this section is concerned with the setting where the kernel @xmath0 is an infinite - rank kernel with an injective feature map @xmath43 onto @xmath190 .",
    "well - known examples of these kernels are the class of universal kernels @xcite .",
    "[ def1 ] a continuous kernel @xmath0 on some metric space @xmath48 is a universal kernel if for some feature map @xmath191 of @xmath0 to some hilbert space  @xmath192 , the class of functions of the form @xmath193 is dense in @xmath194 ; that is , for any continuous function @xmath195 and any @xmath122 , there exists a @xmath196 such that @xmath197 .",
    "we note that if @xmath198 is dense in @xmath199 for some feature map @xmath43 and @xmath200 is another feature map of @xmath0 , then @xmath201 is also dense in @xmath194 , that is , the universality of @xmath0 is independent of the choice for its feature map",
    ". furthermore , every feature map of a universal kernel is injective .",
    "the following result lists several well - known universal kernels .",
    "[ prop2 ] let @xmath202 be a compact subset of @xmath39",
    ". then the following kernels are universal on @xmath202 :    * the exponential kernel @xmath203 ; * the gaussian kernel @xmath204 for all @xmath205 ; * the binomial kernel @xmath206 for @xmath207 ; * the inverse multiquadrics @xmath208 with @xmath209 and @xmath210 .",
    "if the kernel matrix @xmath88 is known , then results on the universal consistency of support vector machines with universal kernels are available ; see , for example ,  @xcite .",
    "if the feature map @xmath43 is known , then biau , bunea and wegkamp  @xcite showed that the @xmath189-nearest - neighbors on @xmath76 are universally consistent as @xmath211 and @xmath47 where @xmath189 and @xmath19 are chosen using a structural risk minimization approach .",
    "our universally consistent classifier operates on the estimated vectors and is based on an empirical risk minimization approach .",
    "namely , we will show that the classifier that minimizes a convex surrogate @xmath2 for 01 loss from a class of linear classifiers @xmath212 is universally consistent provided that the convex surrogate @xmath2 satisfies some mild conditions and that the complexity of the class @xmath212 grows in a controlled manner .",
    "first , we will expand our framework to the classification setting .",
    "let @xmath45 be as in section  [ secframework ] , and let @xmath213 be a distribution on @xmath214 .",
    "let @xmath215 , and let @xmath88 and @xmath20 be as in section  [ secframework ] .",
    "the @xmath216 are the class labels for the vertices in the graph corresponding to the adjacency matrix @xmath20 .",
    "we suppose that we observe only @xmath20 , the adjacency matrix , and @xmath217 , the class labels for all but the last vertex .",
    "our goal is to accurately classify this last vertex , so for convenience of notation we shall define @xmath218 and @xmath219 .",
    "let the rows of @xmath220 be denoted by @xmath221 ( even though the @xmath222 are unobserved / unknown ) .",
    "we want to find a classifier @xmath223 such that , for any distribution @xmath213 , @xmath224&:=&\\mathbb{e}\\bigl[{\\mathbb{p}}\\bigl[h_n\\bigl ( \\zeta_{d_n}(x)\\bigr ) \\neq y | \\bigl(\\zeta_{d_n}(x_1),y_1 \\bigr),\\ldots,\\bigl(\\zeta_{d_n}(x_n),y_n\\bigr ) \\bigr]\\bigr ] \\\\ & \\to&{\\mathbb{p}}\\bigl[h^*(x)\\neq y\\bigr]=:l^*,\\end{aligned}\\ ] ] where @xmath225 is the bayes - optimal classifier , and @xmath226 is its associated bayes - risk .",
    "let @xmath227 be the class of linear classifiers using the truncated feature map @xmath76 whose linear coefficients are normalized to have norm at most @xmath19 , that is , @xmath228 , if and only if @xmath229 is of the form @xmath230 for some @xmath231 with @xmath232 .",
    "we note that the @xmath233 are increasing , that is , @xmath234 for @xmath235 and that @xmath236 .",
    "because @xmath0 is universal , @xmath237 is dense in @xmath238 and as @xmath45 is compact , @xmath237 is dense in the space of measurable functions on @xmath45 .",
    "thus @xmath239 and so one can show that empirical risk minimization over the class @xmath212 for any increasing and divergent sequence @xmath240 yields a universally consistent classifier ( theorem 18.1 in  @xcite ) . the remaining part of this section is concerned with modifying this result so that it applies to the estimated feature map @xmath241 instead of the true feature map  @xmath76 .",
    "we now describe a setup for empirical risk minimization over @xmath227 for increasing @xmath19 where we use the estimated @xmath241 in place of the @xmath76 .",
    "let us write @xmath242 for the empirical error when using the @xmath241 , that is , @xmath243 we want to show that minimization of @xmath244 over the class @xmath212 for increasing @xmath240 leads to a universally consistent classifier for our latent position graphs setting .",
    "however , the loss function @xmath245 of a classifier @xmath246 as well as its empirical version @xmath247 is based on the 01 loss , which is discontinuous at @xmath248 . furthermore , the distribution of @xmath249 not available .",
    "this induces complications in relating @xmath250 to @xmath251 .",
    "that is , the classifier obtained by minimizing the 01 loss using @xmath252 might be very different from the classifier obtained by minimizing the 01 loss using  @xmath43 .    to circumvent this issue",
    ", we will work with some convex loss function @xmath2 that is a surrogate of the 01 loss .",
    "the notion of constructing classification algorithms that correspond to minimization of a convex surrogate for the 01 loss is a powerful one and the authors of @xcite , among others , showed that one can obtain , under appropriate regularity conditions , bayes - risk consistent classifiers in this manner .",
    "let @xmath253 .",
    "we define the @xmath2-risk of @xmath254 by @xmath255 given some data @xmath256 , the empirical @xmath2-risk of @xmath246 is defined as @xmath257 we will often write @xmath258 if the number of samples @xmath259 in @xmath260 is clear from the context .",
    "let @xmath261 index a linear classifier on @xmath227 . denote by @xmath262 , @xmath263 , @xmath264 and @xmath265 the various quantities analogous to @xmath266 , @xmath251 , @xmath267 and @xmath268 for 01 loss defined previously .",
    "let us also define @xmath269 as the minimum @xmath2-risk over all measurable functions @xmath270 .    in this paper",
    ", we will assume that the convex surrogate @xmath271 is differentiable with @xmath272 .",
    "this implies that @xmath2 is _ classification - calibrated",
    "_  @xcite .",
    "examples of classification - calibrated loss functions are the exponential loss function @xmath273 in boosting , the logit function @xmath274 in logistic regression and the square error loss @xmath275 . for classification - calibrated loss functions , we have the following result .    [ thm7 ]",
    "let @xmath276 be classification - calibrated .",
    "then for any sequence of measurable functions @xmath277 and every probability distribution @xmath6 , @xmath278 implies @xmath279 .",
    "we now state the main result of this section , which is that empirical @xmath2-risk minimization over the class @xmath212 for some diverging sequence @xmath240 yields a universally consistent classifier for the latent position graphs setting .",
    "[ thm6 ] let @xmath280 be fixed . for a given @xmath19 , let @xmath281 .",
    "suppose that @xmath282 is given by the following rule : @xmath283 let @xmath284 be the classifier obtained by empirical @xmath2-risk minimization over @xmath212 .",
    "then @xmath285 as @xmath286 and @xmath287 is universally consistent , that is , @xmath288 \\rightarrow l^{*}\\ ] ] as @xmath11 for any distribution @xmath213 .",
    "we note that due to the use of the estimated @xmath252 in place of the true  @xmath43 , theorem  [ thm6 ] is limited in two key aspects .",
    "the first is that we do not claim that @xmath284 is universally _ strongly _",
    "consistent for any @xmath213 and the second is that we can not specify @xmath282 in advance . in return , the minimization of the empirical @xmath2-risk over the class @xmath227 is a convex optimization problem and the solution can be obtained more readily than the minimization of empirical 01 loss .",
    "for example , by using squared error loss instead of 01 loss , the classifier that minimizes the empirical @xmath2-risk can be viewed as a ridge regression problem .",
    "we note also that as the only accumulation point in the spectrum of @xmath54 is at zero , the sequence @xmath240 as specified in equation ( [ eq69 ] ) exists .",
    "furthermore , such a sequence is only one possibility among many .",
    "in particular , the conclusion in theorem  [ thm6 ] holds for any sequence @xmath240 that diverges and satisfies the condition @xmath289 .",
    "choosing the right @xmath240 requires balancing the approximation error @xmath290 and the estimation error @xmath291 , and this can be done using an approach based on structural risk minimization ; see , for example , section  18.1 of @xcite and  @xcite .",
    "we now proceed to prove theorem  [ thm6 ] . a rough sketch of the argument goes as follows .",
    "first we show that any classifier @xmath229 using the estimated vectors @xmath241 induces a classifier @xmath292 using the true truncated feature map @xmath76 such that the empirical @xmath2-risk of @xmath229 is `` close '' to the empirical @xmath2-risk of @xmath292 . then by applying a vapnik  chervonenkis - type bound for @xmath292 , we show that the classifier @xmath293 ( using @xmath294 ) selected by empirical @xmath2-risk minimization induces a classifier @xmath295 ( using @xmath296 ) with the @xmath2-risk of @xmath295 being `` close '' to the minimum @xmath2-risk for the classifiers in the class @xmath227 .",
    "universal consistency of @xmath295 and hence of @xmath293 follows by letting @xmath19 grow in a specified manner .",
    "let @xmath297 .",
    "let @xmath25 be the embedding of @xmath20 into @xmath39 .",
    "let @xmath298 be an orthogonal matrix given by @xmath299    the following result states that if there is a gap in the spectrum of @xmath54 at @xmath107 , then @xmath300 and @xmath301 is close for all @xmath302 .",
    "that is , the empirical @xmath303-risk of a linear classifier using @xmath294 is not too different from the empirical @xmath2-risk of a related classifier ( the relationship is given by  @xmath304 ) using @xmath296 .",
    "[ prop9 ] let @xmath73 be such that @xmath305 , and let @xmath306 .",
    "then for any @xmath231 , @xmath307 , we have , with probability at least @xmath308 , @xmath309    we have @xmath310 now @xmath2 is convex and thus locally lipschitz - continuous . also , @xmath311 independent of @xmath77 and @xmath6 such that @xmath312 for all @xmath27 .",
    "thus , by theorem  [ thm1 ] , we have @xmath313\\\\[-8pt ] & & \\qquad\\leq \\frac{m}{\\sqrt{n } } \\biggl ( \\sum_{i=1}^{n } \\bigl\\| \\zeta_{d}(x_i ) - ( \\mathbf{w}_{d})^{t } \\phi_{d}(x_i ) \\bigr\\|^{2 } \\biggr)^{1/2 } \\nonumber \\\\ & & \\qquad\\leq \\frac{m}{\\sqrt{n } } \\bigl\\| \\mathbf{u}_{\\mathbf{a } } \\mathbf { s}_{\\mathbf{a}}^{1/2 } - ( \\mathbf{w}_{d})^{t } \\bolds { \\phi}_{d } \\bigr\\|_{f } \\nonumber \\\\ & & \\qquad\\leq 27 \\delta_{d}^{-2 } m \\sqrt{\\frac{3 d \\log{n}}{n } } \\nonumber\\end{aligned}\\ ] ] with probability at least @xmath314 . by the mean - value theorem , we can take @xmath315 to complete the proof .    the vapnik  chervonenkis theory for 01 loss function can also be extended to the convex surrogate setting @xmath316 . in particular , the following result provides a uniform deviation bound for @xmath317 for functions @xmath246 in some class @xmath318 in terms of the vc - dimension of @xmath318 .",
    "let @xmath318 be a class of functions with vc - dimension @xmath319 .",
    "suppose that the range of any @xmath320 is contained in the interval @xmath321 $ ] .",
    "let @xmath322 .",
    "then we have , with probability at least @xmath314 , @xmath323    the following result combines proposition  [ prop9 ] and lemma  [ lem1 ] and shows that minimizing @xmath264 over @xmath324 leads to a classifier whose @xmath2-risk is close to optimal in the class @xmath227 with high probability .",
    "[ lem2 ] let @xmath73 be such that @xmath305 and let @xmath306 .",
    "let @xmath325 minimize @xmath264 over @xmath326 .",
    "then with probability at least @xmath327 , @xmath328    for ease of notation , we let @xmath329 be the term in the right - hand side of equation ( [ eq49 ] ) , and let @xmath330 be the term in the right - hand side of equation ( [ eq53 ] ) .",
    "also let @xmath331 .",
    "we then have @xmath332 with probability at least @xmath333 .    equation ( [ eq54 ] ) is a vc - type bound .",
    "the term @xmath334 in equation ( [ eq54 ] ) can be viewed as contributing to the generalization error for the classifiers in @xmath227 .",
    "that is , because we are training using the estimated vectors in @xmath39 , the generalization error not only depends on the dimension of the embedded space , but also depends on how accurate the estimated vectors are in that space .",
    "we now have the necessary ingredients to prove the main result of this section .",
    "proof of theorem  [ thm6 ] let @xmath240 be a nondecreasing sequence of positive integers that diverges to @xmath335 and that @xmath336 by lemma  [ lem2 ] and the borel  cantelli lemma , we have @xmath337 = 0\\ ] ] almost surely",
    ". as @xmath240 diverges , @xmath338 by proposition  [ prop10 ] .",
    "we therefore have @xmath339 almost surely .",
    "now fix a @xmath77 .",
    "the empirical @xmath2-risk minimization on @xmath340 using the estimated vectors @xmath341 gives us a classifier @xmath342 .",
    "we now consider the difference @xmath343 . by a similar computation to that used in the derivation of equation ( [ eq52 ] ) , we have @xmath344 - \\mathbb{e}\\bigl [ \\varphi\\bigl(y \\bigl\\langle\\mathbf{w}_{d_n } { \\widetilde}{w}_{d_n } , \\phi_{d_n}(x)\\bigr\\rangle\\bigr)\\bigr]\\bigr| \\\\ & & \\qquad\\leq d_{n } c_{d_n } \\mathbb{e}\\bigl [ \\bigl\\| \\zeta_{d_n}(x ) - ( \\mathbf{w}_{d_n})^{t } \\phi_{d_n}(x ) \\bigr\\|\\bigr ] \\\\ & & \\qquad\\leq d_{n } c_{d_n } \\sqrt { \\mathbb{e}\\bigl [ \\bigl\\| \\zeta_{d_n}(x ) - ( \\mathbf{w}_{d_n})^{t } \\phi_{d_n}(x ) \\bigr\\|^{2}\\bigr ] } \\\\ & & \\qquad\\leq 27 \\delta_{d_n}^{-2 } d_{n } c_{d_n } \\sqrt { \\frac { 6 d \\log{n}}{n } } = o(1).\\end{aligned}\\ ] ] we therefore have @xmath345 thus , by theorem  [ thm7 ] , we have @xmath346 = l^{*}.\\ ] ] the only thing that remains is the use of @xmath347 as an estimate for  @xmath109 . by proposition  [ prop3 ] and",
    "theorem  [ thm1 ] , we have @xmath348 with probability at least @xmath333 . thus ,",
    "if @xmath282 satisfy equation ( [ eq69 ] ) , then equation ( [ eq78 ] ) implies that equation ( [ eq70 ] ) holds for @xmath349 with probability at least @xmath333 .",
    "finally , we note that as @xmath350 , there exists a sequence @xmath240 that satisfies equation ( [ eq69 ] ) and diverges to @xmath335 , as the only accumulation point in the spectrum of @xmath54 is at zero .",
    "in this paper we investigated the problem of finding a universally consistent classifier for classifying the vertices of latent position graphs .",
    "we showed that if the link function @xmath0 used in the construction of the graphs belong to the class of universal kernels , then an empirical @xmath2-risk minimization approach , that is , minimizing a convex surrogate of the 01 loss over the class of linear classifiers in @xmath351 for some sequence @xmath352 , yields universally consistent vertices classifiers .",
    "we have presented the universally consistent classifiers in the setting where the graphs are on @xmath353 vertices , there are @xmath77 labeled vertices and the task is to classify the remaining unlabeled vertex .",
    "it is easy to see that in the case where there are only @xmath354 labeled vertices , the same procedure given in theorem  [ thm6 ] with @xmath77 replaced by @xmath355 still yields universally consistent classifiers , provided that @xmath356 .",
    "the bound for the generalization error of the classifiers in section  [ seccons - vert - class ] is of the form @xmath357 .",
    "this bound depends on both the subspace projection error in section  [ secestim - feat - maps ] as well as the generalization error of the class @xmath212 .",
    "it is often the case that the bound on the generalization error of the class @xmath212 can be improved , as long as the classification problems satisfy a `` low - noise '' condition , that is , that the posterior probability @xmath358 $ ] is bounded away from @xmath359 .",
    "results on fast convergence rates in low - noise conditions , for example , @xcite can thus be used , but as the subspace projection error is independent of the low - noise condition , there might not be much improvement in the resulting error bound .    also related to the above issue",
    "is the choice of the sequence @xmath240 .",
    "if more is known about the kernel @xmath0 , then the choice for the sequence @xmath240 can be adjusted accordingly .",
    "for example , good bounds for @xmath360 , the sum of the tail eigenvalues of @xmath54 , along with bounds for the error between the truncated feature map @xmath76 and the feature map @xmath43 from @xcite can be used to select the sequence  @xmath240 .    the results presented in section  [ secestim - feat - maps ] and section  [ seccons - vert - class ] implicitly assumed that the graphs arising from the latent position model are dense .",
    "it is possible to extend these results to sparse graphs .",
    "a sketch of the ideas is as follows .",
    "let @xmath361 be a scaling parameter , and consider the latent position model with kernel @xmath0 and distribution @xmath1 for the latent features @xmath108 .",
    "given @xmath362 , let @xmath363 , that is , the entries of @xmath364 are given by the kernel @xmath0 scaled by the scaling parameter @xmath365 .",
    "this variant of the latent position model is also present in the notion of inhomogeneous random graphs @xcite . given @xmath364 , @xmath366 is the adjacency matrix .",
    "the factor @xmath365 controls the sparsity of the resulting latent position graph .",
    "for example , @xmath367 leads to sparse , connected graphs almost surely while @xmath368 leads to graphs with a single giant connected component @xcite .",
    "suppose now that @xmath369 .",
    "the following result is a restatement of theorem  [ thm1 ] for the latent position model in the presence of the scaling parameter @xmath365 .",
    "its proof is almost identical to that of theorem  [ thm1 ] provided that one uses the bound in term of the maximum degree @xmath129 in proposition  [ prop3 ] .",
    "we note that @xmath109 is defined in terms of the spectrum of @xmath54 which does not depend on the scaling parameter @xmath365 , and similarly for the feature map @xmath43 and its truncation @xmath76 .",
    "[ thm9 ] let @xmath73 be given .",
    "denote by @xmath109 the quantity @xmath111 , and suppose that @xmath112 .",
    "then with probability greater than @xmath113 , there exists a unitary matrix @xmath114 such that @xmath370 where @xmath116 denotes the matrix in @xmath117 whose @xmath27th row is @xmath118 .",
    "let us denote by @xmath119 the @xmath27th row of @xmath371 .",
    "then , for each @xmath121 $ ] and any @xmath122 ,",
    "@xmath372 \\leq27 \\delta_d^{-2 } \\varepsilon^{-1 } \\sqrt { \\frac{6d \\log{n}}{n \\rho_n}}.\\ ] ]    thus , for @xmath373 for some @xmath374 [ or even @xmath375 for some sufficient large @xmath189 ] , equation ( [ eq2 ] ) states that with high probability , the estimated feature map is ( after scaling by @xmath377 and rotation ) converging to the true truncated feature map @xmath296 as @xmath286 . the results from section  [ seccons - vert - class ] can then be modified to show the existence of a universally consistent linear classifier .",
    "the main difference between the sparse setting and the dense setting would be the generalization bounds in proposition  [ prop9 ] and lemma  [ lem2 ] .",
    "this would lead to a different selection rule for the sequence of embedding dimensions @xmath378 then the one in theorem  [ thm9 ] , that is , the @xmath282 would diverge more slowly for the sparse setting compared to the dense setting . a  precise statement and formulation of the results in section  [ seccons - vert - class ] for the sparse setting might require some care , but should be for the most part straightforward .",
    "we also note that even though @xmath365 is most likely unknown , one can scale the embedding @xmath379 by any value @xmath380 that is of the same order as @xmath377 . an appropriate value for @xmath380 is , for example ,",
    "one that makes @xmath381 where @xmath26 is the @xmath27th row of @xmath24 .",
    "finally , we note it is of potential interest to extend the results herein to graphs with attributes on the edges , latent position graphs with nonpositive definite link functions @xmath0 and graphs with errorfully observed edges .",
    "proof of lemma  [ lem4 ] let @xmath382 be the vector whose entries are @xmath383 for @xmath384 with @xmath385 .",
    "we note that @xmath386 .",
    "let @xmath387 be the eigenvectors associated with the @xmath19 largest eigenvalues of @xmath86 .",
    "we have @xmath388 the @xmath389th entry of @xmath390 is then given by @xmath391 let @xmath392 be the extensions of @xmath393 as defined by equation ( [ eq36 ] ) .",
    "we then have , for any @xmath394 , @xmath395 we thus have @xmath396 now let @xmath397 .",
    "@xmath398 is the embedding of the sequence @xmath399 into @xmath44 ; see equation ( [ eq82 ] ) . by equation ( [ eq39 ] ) and the definition of @xmath400 [ equation ( [ eq83 ] ) ] ,",
    "the @xmath389th entry of @xmath390 can be written as @xmath401 we note that , by the reproducing kernel property of @xmath402 , @xmath403 as the @xmath404 are orthogonal with respect to @xmath405 , the @xmath389th entry of @xmath390 can also be written as @xmath406 as the @xmath407 lies in a @xmath19-dimensional subspace of @xmath44 , they can be isometrically embedded into @xmath39 .",
    "thus there exists a matrix @xmath408 such that @xmath409 and that the rows of @xmath410 correspond to the projections @xmath411 .",
    "therefore , there exists a unitary matrix @xmath412 such that @xmath413 as desired .",
    "[ lem3 ] let @xmath20 and @xmath414 be @xmath415 positive semidefinite matrices with @xmath416 .",
    "let @xmath417 be of full column rank such that @xmath418 and @xmath419 .",
    "let @xmath420 be the smallest nonzero eigenvalue of  @xmath414 .",
    "then there exists an orthogonal matrix @xmath164 such that @xmath421    let @xmath422 . as @xmath423 is of full column rank",
    ", @xmath424 is invertible , and its smallest eigenvalue is @xmath420 .",
    "we then have @xmath425 let @xmath426 .",
    "we then have @xmath427 therefore , @xmath428 where @xmath429 refers to the positive semi - definite ordering for matrices .",
    "we thus have @xmath430 now let @xmath431 be the orthogonal matrix in the polar decomposition @xmath432 .",
    "we then have @xmath433 now , @xmath434 .",
    "indeed , @xmath435 we thus have @xmath436 and equation ( [ eq81 ] ) follows .",
    "[ prop10 ] let @xmath0 be a universal kernel on @xmath45 , and let @xmath437 be a feature map of @xmath0 .",
    "let @xmath438 be the sequence of classifiers of the form in equation ( [ eq11 ] ) .",
    "then @xmath439    we note that this result is a slight variation of lemma 1 in  @xcite . for completeness",
    ", we sketch its proof here .",
    "let @xmath440 be the function defined by @xmath441 where @xmath442 $ ] .",
    "then @xmath443 $ ] .",
    "now , for a given @xmath444 $ ] , let @xmath445 , and let @xmath446 be the complement of @xmath447 .",
    "we consider the decomposition @xmath448",
    "+ \\mathbb{e}\\bigl[f^{*}(x ) \\mathbf{1}\\{x \\in \\bar{h}_{\\beta}\\}\\bigr].\\ ] ] the restriction of @xmath440 to @xmath446 is measurable with range @xmath449 $ ] for some finite constant @xmath450 .",
    "the set of functions @xmath451 is dense in @xmath452 and hence also dense in @xmath453 .",
    "thus , for any @xmath122 , there exists a @xmath454 such that @xmath455 - \\mathbb{e}\\bigl[\\bigl\\langle w , \\phi(x)\\bigr\\rangle_{\\mathscr{h } } \\mathbf { 1}\\{x \\in\\bar{h}_{\\beta}\\}\\bigr ] < \\varepsilon.\\ ] ] furthermore , @xmath456 \\rightarrow0 $ ] as @xmath457 .",
    "indeed , @xmath458 so we can select @xmath459 so that @xmath460",
    "if @xmath461 and @xmath462 if @xmath463 . to complete the proof , we note that the @xmath227 are nested , that is , @xmath464 .",
    "hence @xmath465 is a decreasing sequence that converges to @xmath269 as desired .",
    "we can tie the spectrum and eigenvectors of @xmath88 to the spectrum and eigenfunctions of @xmath54 by constructing an extension operator @xmath156 for @xmath88 and relating the spectra of @xmath54 to that of @xmath466 @xcite .",
    "let @xmath44 be the reproducing kernel hilbert space for @xmath0 .",
    "let @xmath467 and @xmath468 be the linear operators defined by @xmath469 the operators @xmath470 and @xmath156 are defined on the same hilbert space @xmath44 , in contrast to @xmath54 and @xmath88 which are defined on the different spaces @xmath471 and @xmath158 , respectively .",
    "thus , we can relate the spectra of @xmath470 and @xmath156 .",
    "furthermore , we can also relate the spectra of @xmath54 and @xmath470 as well as the spectra of @xmath88 and @xmath156 , therefore giving us a relationship between the spectra of @xmath54 and @xmath88 . a precise statement of the relationships",
    "is contained in the following results .",
    "[ prop5 ] the operators @xmath470 and @xmath156 are positive , self - adjoint operators and are of trace class with @xmath156 being of finite rank .",
    "the spectra of @xmath54 and @xmath470 are contained in @xmath90 $ ] and are the same , possibly up to the zero eigenvalues .",
    "if @xmath472 is a nonzero eigenvalue of @xmath54 and @xmath473 and @xmath106 are associated eigenfunction of @xmath54 and @xmath470 , normalized to norm @xmath187 in @xmath51 and @xmath44 , respectively , then @xmath474\\\\[-8pt ] v(x ) & = & \\frac{1}{\\sqrt{\\lambda } } \\int_{\\mathcal{x } } \\kappa \\bigl(x , x'\\bigr ) u\\bigl(x'\\bigr ) \\,df\\bigl(x'\\bigr).\\nonumber\\end{aligned}\\ ] ] similarly , the spectra of @xmath86 and @xmath156 are contained in @xmath90 $ ] and are the same , possibly up to the zero eigenvalues .",
    "if @xmath475 is a nonzero eigenvalue of @xmath88 and @xmath476 and @xmath477 are the corresponding eigenvector and eigenfunction of @xmath86 and @xmath156 , normalized to norm @xmath187 in @xmath158 and @xmath44 , respectively , then @xmath478    equation ( [ eq36 ] ) in proposition  [ prop5 ] states that an eigenvector @xmath476 of @xmath86 , which is only defined for @xmath479 , can be extended to an eigenfunction @xmath480 of @xmath156 defined for all @xmath481 , and furthermore , that @xmath482 for all @xmath483 .    [ thm5 ] let @xmath484 be arbitrary .",
    "then with probability at least @xmath485 , @xmath486 where @xmath487 is the hilbert",
    " schmidt norm .",
    "let @xmath488 be a decreasing enumeration of the eigenvalues for @xmath470 , and let @xmath489 be an extended decreasing enumeration of @xmath156 ; that is , @xmath490 is either an eigenvalue of @xmath156 or @xmath491 .",
    "then the above bound and a lidskii theorem for infinite - dimensional operators @xcite yields @xmath492 with probability at least @xmath493 . for a given @xmath73 and @xmath484 ,",
    "if the number @xmath77 of samples @xmath494 satisfies @xmath495 then with probability greater than @xmath493 , @xmath496 where @xmath497 is the projection onto the subspace spanned by the eigenfunctions corresponding to the @xmath19 largest eigenvalues of @xmath54 , and @xmath162 is the projection onto the subspace spanned by the eigenfunctions corresponding to the @xmath19 largest eigenvalues of @xmath156 .",
    "the authors thank the anonymous referees for their comments which have improved the presentation and quality of the paper ."
  ],
  "abstract_text": [
    "<S> in this work we show that , using the eigen - decomposition of the adjacency matrix , we can consistently estimate feature maps for latent position graphs with positive definite link function @xmath0 , provided that the latent positions are i.i.d . from some distribution @xmath1 . </S>",
    "<S> we then consider the exploitation task of vertex classification where the link function @xmath0 belongs to the class of universal kernels and class labels are observed for a number of vertices tending to infinity and that the remaining vertices are to be classified . </S>",
    "<S> we show that minimization of the empirical @xmath2-risk for some convex surrogate @xmath2 of 01 loss over a class of linear classifiers with increasing complexities yields a universally consistent classifier , that is , a classification rule with error converging to bayes optimal for any distribution @xmath1 .    , </S>"
  ]
}