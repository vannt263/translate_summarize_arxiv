{
  "article_text": [
    "a markov network ( mn ) is a popular probabilistic graphical model that efficiently encodes the joint probability distribution for a set of random variables of a specific domain @xcite .",
    "mns usually represent probability distributions by using two interdependent components : an independence structure , and a set of numerical parameters over the structure .",
    "the first is a qualitative component that represents structural information about a problem domain in the form of conditional independence relationships between variables .",
    "the numerical parameters are a quantitative component that represents the strength of the dependencies in the structure .",
    "there is a large list of applications of mns in a wide range of fields , such as computer vision and image analysis @xcite , computational biology @xcite , biomedicine @xcite , and evolutionary computation @xcite , among many others .",
    "for some of these applications , the model can be constructed manually by human experts , but in many other problems this can become unfeasible , mainly due to the dimensionality of the problem .    learning the model from data",
    "consists of two interdependent problems : learning the structure ; and given the structure , learning its parameters .",
    "this work focuses on the task of learning the structure .",
    "the structures learned may be used to construct accurate models for inference tasks ( such as the estimation of marginal and conditional probabilities ) , and also may be interesting per se , since they can be used as interpretable models that show the most significant interactions of a domain @xcite .",
    "the first scenario is known in practice as the density estimation goal of learning , and the second one is known as the knowledge discovery goal of learning [ chapter  16  @xcite ] .",
    "an interesting approach to mn structure learning is to use constraint - based ( also known as independence - based ) algorithms @xcite .",
    "such algorithms proceed by performing statistical independence tests on data , and discard all structures inconsistent with the tests .",
    "this is an efficient approach , and it is correct under the assumption that the distribution can be represented by a graph , and that the tests are reliable .",
    "however , the algorithms that follow this approach are quite sensitive to errors in the tests , which may be unreliable for large conditioning sets @xcite .",
    "a second approach to mn structure learning is to use score - based algorithms @xcite .",
    "such algorithms formulate the problem as an optimization , combining a strategy for searching through the space of possible structures with a scoring function measuring the fitness of each structure to the data .",
    "the structure learned is the one that achieves the highest score .",
    "it is important to mention that both constraint - based and score - based approaches have been originally motivated by distinct learning goals . according to the existing literature @xcite , constraint - based methods are generally designed for the knowledge - discovery goal of learning @xcite , and their quality is often measured in terms of the correctness of the structure learned ( structural errors ) .",
    "in contrast , most score - based approaches have been designed for the density estimation goal of learning @xcite , and they are in general evaluated in terms of inference accuracy . for this reason ,",
    "score - based algorithms often work by considering the whole mn at once during the search , interleaving the parameters learning step .",
    "this makes them more accurate for inference tasks .",
    "however , since learning the parameters is known to be np - hard for mns @xcite , it has a negative effect on their scalability .",
    "recently , there has been a recent surge of interest towards efficient methods based on a bayesian approach .",
    "this strategy follows a score - based approach , but with the knowledge discovery goal in mind .",
    "basically , an undirected graph structure is learned by obtaining the probabilistic maximum - a - posteriori structure @xcite .",
    "such contributions consist in the design of efficient scoring functions for mn structures , expressing the problem formally as follows : given a complete training data set @xmath0 , find an undirected graph @xmath1 such that @xmath2 where @xmath3 is the posterior probability of a structure , and @xmath4 is the familiy of all the possible undirected graphs for the domain size .",
    "this class of algorithms has been shown to outperform constraint - based algorithms in the quality of the learned structures .",
    "the contribution of this paper follows this hybrid approach .",
    "the method proposed in this work can improve the quality of structure learning by examining the _ irregularity _ of each structure . according to @xcite ,",
    "the irregularity of an undirected graph can be computed by summing the imbalance of its edges : @xmath5 where @xmath6 is the degree of the node @xmath7 in that graph .",
    "clearly @xmath8 if and only if @xmath9 is regular . for non - regular graphs @xmath10 is a measure of the lack of regularity .",
    "although there are more complex measures of irregularity for undirected graphs @xcite , this nave definition will suffice for the purposes of this work . in this work",
    ", we present the _ blankets joint posterior _ ( bjp ) as a score that computes the posterior probability of mn structures by taking advantage of the irregularities of the evaluated structure .",
    "this allow us to improve the learning process for domains with complex networks , where the topologies exhibit irregularities , which is a common property in many real - world networks @xcite .",
    "after providing some preliminaries , notations and definitions in section  [ sec : stateoftheart ] , we introduce the bjp scoring function in section  [ sec : bjp ] .",
    "section  [ sec : experiments ] shows our experiments for several study cases .",
    "finally , section  [ sec : conclusions ] summarizes this work , and poses several possible directions of future work .",
    "we begin by introducing the notation used for mns . then we provide some additional background about these models and the problem of learning their independence structure , and also discuss the state - of - the - art of mn structure learning .",
    "have @xmath11 as a finite set of indexes , lowercase subscripts for denoting particular indexes , e.g. , @xmath12 , and uppercase subscripts for subsets of indexes , e.g. , @xmath13 .",
    "let @xmath14 be the set of random variables of a domain , denoting single variables as single indexes in @xmath11 , e.g. , @xmath15 when @xmath16 .",
    "for a mn representing a probability distribution @xmath17 its two components are denoted as follows : @xmath9 , and @xmath18 .",
    "@xmath9 is the structure , an undirected graph @xmath19 where the nodes @xmath20 are the indices of each random variable @xmath21 of the domain , and @xmath22 is the edge set of the graph .",
    "a node @xmath7 is a neighbor of @xmath23 when the pair @xmath24 .",
    "the edges encode direct probabilistic influence between the variables .",
    "instead , the absence of an edge manifests that the dependence could be mediated by some other subset of variables , corresponding to conditional independences between these variables .",
    "a variable @xmath21 is conditionally independent of another non - adjacent variable @xmath25 given a set of variables @xmath26 if @xmath27 .",
    "this is denoted by @xmath28 ( or @xmath29 for the dependence assertion ) . as proven by @xcite ,",
    "the independences encoded by @xmath9 allow the decomposition of the joint distribution into simpler lower - dimensional functions called factors , or potential functions .",
    "the distribution can be factorized as the product of the potential functions @xmath30 over each clique @xmath31 ( i.e. , each completely connected sub - graph ) of @xmath9 , that is @xmath32 where @xmath33 is a constant that normalizes the product of potentials .",
    "such potential functions are parameterized by the set of numerical parameters @xmath18 .    for each variable @xmath21 of a mn , its markov blanket ( mb )",
    "is composed by the set of all its neighbor nodes in the graph .",
    "hereon we denote the mb of a variable @xmath21 as @xmath34 .",
    "an important concept that is satisfied by mns is the local markov property , formally described as :    * * local markov property*. a variable is conditionally independent of all its non - neighbor variables given its mb .",
    "that is @xmath35    by using such property , the conditional independences of @xmath17 can be read from the structure @xmath9 .",
    "this is done by considering the concept of separability .",
    "each pair of non - adjacent variables @xmath36 are said to be separated by a set of variables @xmath37 when every path between @xmath21 and @xmath25 in @xmath9 contains some node in @xmath26 @xcite .    in machine learning ,",
    "statistical independence tests are a well - known tool to decide whether a conditional independence is supported by the data .",
    "examples of independence tests used in practice are mutual information @xcite , pearson s @xmath38 and @xmath39 @xcite , the bayesian statistical test of independence @xcite , and the partial correlation test for continuous gaussian data @xcite .",
    "such tests require the construction of a contingency table of counts for each complete configuration of the variables involved ; as a result , they would have an exponential cost in the number of variables @xcite .",
    "for this reason , the use of the local markov property has a positive effect for learning independence structures , allowing the use of smaller tests .",
    "accordingly , the scoring function proposed in this work takes advantage of this property to avoid the computation of potentially expensive and unreliable tests .",
    "this is achieved by examining the irregularities present in a structure .",
    "the structure of a mn can be learned from a training dataset @xmath40 , assumed to be a representative sample of the underlying distribution @xmath17 .",
    "commonly , @xmath0 has a tabular format , with a column for each variable of the domain @xmath14 , and one row per data point .",
    "this work assumes that each variable is discrete , with a finite number of possible values , and that no data point in @xmath0 has missing values .",
    "as mentioned in section  [ sec : intro ] , this work focuses on the bayesian approach for mn structure learning of ( [ eq : maxg ] ) . for this reason , in this subsection we discuss two recently proposed scoring functions that follow such approach : the marginal pseudo likelihood ( mpl ) score @xcite , and the independence - based score ( ib - score ) @xcite . in mpl , each graph is scored by using an efficient approximation to the posterior probability of structures given the data .",
    "this score approximates the posterior by considering @xmath41 .",
    "since the data likelihood of the graph @xmath42 is in general extremely hard to evaluate , mpl utilizes the well - known approximation called the pseudo - likelihood @xcite .",
    "this score was proved to be consistent , that is , in the limit of infinite data the solution structure has the maximum score . for finding the mpl - optimal structure ,",
    "two algorithms were presented : an exact algorithm using pseudo - boolean optimization , and a fast alternative to the exact method , which uses greedy hill - climbing with near - optimal performance .",
    "this algorithm learns the mb for each variable , locally optimizing the mpl for each node , independently of the solutions of the other nodes . for this",
    ", it uses an approximate deterministic hill - climbing procedure similar to the well - known iamb algorithm @xcite .",
    "finally , a global graph discovery method is applied by using a greedy hill - climbing algorithm , searching for the structure with maximum mpl score , but only restricting the search space to the conflicting edges . the independence - based score ( ib - score ) @xcite , is also based on the computation of the posterior , but using the statistics of a set of conditional independence tests . in this score the posterior @xmath43 is computed by combining the outcomes of a set of conditional independence assertions that completely determine @xmath9 .",
    "such set was called the _ closure _ of the structure , denoted @xmath44 .",
    "thus , when using ib - score the problem of structure learning is posed as the maximization of the posterior of the closure for each structure .",
    "formally , @xmath45    applying the chain rule over the posterior of the closure , @xmath46 the ib - score approximates such probability by assuming that all the independence assertions @xmath47 in the closure @xmath44 are mutually independent .",
    "the resulting scoring funtion is computed as : @xmath48 where each term @xmath49 is computed by using the bayesian statistical test of conditional independence @xcite .",
    "together with the ib - score , an efficient algorithm called ibmap - hc is presented to learn the structure by using a heuristic local search over the space of possible structures .",
    "this section proposes the blankets joint posterior ( bjp ) , a scoring function to compute the posterior probability of the independence structure of a mn .",
    "in particular , bjp has been designed in order to accurately approximate the posterior of structures for cases where the underlying structure contains irregularities .",
    "the correctness of bjp is discussed in the appendix  [ app : correctness ] .",
    "consider some graph @xmath9 representing the independence structure of a positive mn .",
    "it is a well - known fact that , by exploiting the graphical properties of such models , the independence structure can be decomposed as the unique collection of the mbs of the variables ( * ? ? ?",
    "* theorem 4.6 on p.  121 ) .",
    "thus , the computation of the posterior probability of @xmath9 given a dataset @xmath0 is equivalent to the joint posterior of the collection of mbs of @xmath9 , that is , @xmath50 in contrast with previous works , where the mb posteriors are simply assumed to be independent @xcite , the chain rule is applied to ( [ eq : gdecomposed ] ) , obtaining @xmath51 in this way the posterior probability of each mb can be described in terms of conditional probabilities , using the training dataset @xmath0 as evidence , together with the mb of the other variables .",
    "the computation of @xmath52 has to be done progressively , first calculating the posterior of the mb of a variable , and then , the knowledge obtained so far can be used as evidence to compute the posterior of the mb of other variables .",
    "however , this decomposition is not unique , since each possible ordering for the variables is associated to a particular decomposition . the basic idea underlying",
    "the computation of bjp is to sort the mbs by their size ( that is , the degree of the nodes in the graph ) in ascending order .",
    "this allows a series of inference steps , in order to avoid the computation of expensive and unreliable probabilities , and obtaining a more accurate data efficiency .",
    "this is due to the fact that as the size of the mb increases , greater amounts of data are required for accurately estimating its posterior probability . by using the proposed strategy ,",
    "the blanket posteriors of variables with fewer neighbors are computed first , and this information is used as evidence when computing the posteriors for variables with bigger blankets . as a result , the information obtained from the more reliable blanket posteriors is used for computing less reliable blankets posteriors .",
    "now consider an example probability distribution @xmath53 with four variables @xmath54 , represented by a mn whose independence structure @xmath9 is given by the graph of figure  [ fig : hub ] .",
    "when sorting its nodes by their degree in ascending order , the vector @xmath55 can be obtained , and the blankets joint posterior is decomposed as    @xmath56    this example allows us to illustrate the intuition behind bjp , since the sample complexity of the blanket posterior for variables @xmath57 , @xmath58 , and @xmath59 is lower than that of @xmath60 . for the sake of clarity ,",
    "appendix [ appendix : example ] shows the complete computation of the bjp score for this example .    ]",
    "given an undirected graph @xmath9 , denote @xmath61 the ordering vector which contains the variables sorted by their degree in ascending order .",
    "therefore , we reformulate ( [ eq : jointblankets2_withoutorder ] ) as    @xmath62    we now proceed to express the posterior of a mb in terms of probabilities of conditional independence and dependence assertions . the computation of @xmath63 can be derived from the posterior of the independences and dependences represented by each mb :    @xmath64    the two factors in this equation will be interpreted as follows :    * the first product computes the probability of independence between @xmath65 and its non - adjacent variables , conditioned on its mb , given the previously computed mbs and the dataset @xmath0 .",
    "it can be computed as + @xmath66 + here , @xmath67 indexes over the variables for which the mb posterior probability is not already computed . for the remaining variables the posterior of independence will be simply inferred as 1 .",
    "this inference can be done since the independence is determined by the mb of @xmath68 , which is in the evidence @xmath69 .",
    "we discuss the correctness of this inference step in appendix  [ app : correctness ] . * the second product in ( [ eq : blanketposterior ] )",
    "computes the posterior probability of dependence between @xmath65 and its adjacent variables , conditioned on its remaining neighbors , given the mbs computed previously and the dataset @xmath0 .",
    "it can be computed as + @xmath70 + here , again @xmath67 indexes over the variables for which the mb posterior is not already computed . for the remaining variables",
    "the posterior of dependence will be inferred as 1 .",
    "also , this inference can be done since the dependence is determined by the mb of @xmath68 , which is in the evidence @xmath69 .",
    "the correctness of this inference step is also discussed in appendix  [ app : correctness ] .",
    "the only approximation in bjp is made in ( [ eq : blanketposterior ] ) , by assuming that all the independence and dependence assertions that determine the mb of a variable @xmath65 are mutually independent .",
    "this is a common assumption , made implicitly by all the constraint - based mn structure learning algorithms @xcite , and also by the mpl score and the ib - score .",
    "for the computation of the posterior probabilities of independence @xmath71 and dependence @xmath72 used in ( [ eq : ciposterior ] ) and ( [ eq : cdposterior ] ) , respectively , bjp uses the bayesian test of @xcite , in the same way as the ib - score explained in the previous section .",
    "precisely , this statistical test computes the posterior of independence and dependence assertions , and has been proven to be statistically consistent in the limit of infinite data .",
    "we now discuss the computational complexity of the score . for a fixed structure ,",
    "the computational cost is directly determined by the number of statistical tests that it is required to perform on data .",
    "recall that the computational cost of each test is exponential in the number of variables involved @xcite . as stated in ( [ eq : jointblankets2 ] )",
    ", bjp computes the posterior probability of the mb for the @xmath73 variables of the domain .",
    "for each , it is required to perform @xmath74 statistical tests on data , by using ( [ eq : blanketposterior ] ) .",
    "then , one half of the tests are inferred when computing the posterior of independences and dependences of ( [ eq : ciposterior ] ) and ( [ eq : cdposterior ] ) .",
    "thus , only @xmath75 tests are required for computing the bjp score of a structure .",
    "we end this section with the optimization proposed in this work for learning the structure with the bjp score .",
    "the nave optimization consists in maximizing over all the possible undirected graphs for some specific problem domain , as in ( [ eq : maxg ] ) , computing with ( [ eq : jointblankets2 ] ) the score for each structure .",
    "since the discrete optimization space of the possible graphs @xmath4 grows rapidly with the number of variables @xmath73 , the search is clearly intractable even for small domain sizes .",
    "hence , in this work we test the performance of bjp with brute force only for small domains .",
    "for larger domains we use the ibmap - hc algorithm , as an efficient approximate solution proposed in @xcite .",
    "the optimization made by ibmap - hc is a simple heuristic hill - climbing procedure .",
    "the search is initialized by computing the score for an empty structure with no edges , and @xmath73 nodes .",
    "the hill - climbing search starts with a loop that iterates by selecting the next candidate structure at each iteration .",
    "a nave implementation of hill - climbing would select the neighbor structure with maximum score , computing the score for the @xmath76 neighbors that differ in one edge .",
    "such expensive computation is avoided by selecting the next candidate with a heuristic that flips the most promising edge .",
    "once the next candidate is selected , its score is computed to be compared to the best scoring structure found so far .",
    "the algorithm stops when the neighbor proposed does not improve the current score .",
    "this section presents several experiments in order to determine the merits of bjp in practical terms .",
    "two sets of experiments from low - dimensional and high - dimensional problems are presented .",
    "for the low - dimensional setting , we used brute force ( i.e. , exhaustive search ) to study the convergence of the scoring functions to the exact solution .",
    "the goal is to prove experimentally that the sample complexity for successfully learning the exact structure is better for bjp than for the competitors . for the high - dimensional setting , we used hill - climbing optimization for all the scoring functions .",
    "this experiments were performed in order to prove that , by using a similar search strategy , bjp identifies structures with fewer structural errors than the selected competitors .",
    "the software to carry out the experiments has been developed in java , and it is publicly available .",
    "a mn scoring function is consistent when the structure which maximizes the score over all the possible structures is the correct one , in the limit of infinite data .",
    "however , in practice the data is often too scarce to satisfy this condition , and the sample size needed to reach the correct structure varies across different scoring functions .",
    "this is referred to as the _ sample complexity _ of the score .",
    "the experiments here presented were carried out in order to measure the sample complexity of three different scoring functions : mpl , ib - score and bjp .",
    "this is achieved by measuring their ability to return , by brute force , the exact independence structure of the mn which generated the data .",
    "0.2 ) ; model 2 has @xmath77 ; model 3 has @xmath78 ; model 4 has @xmath79 ; models 5 and 6 have the maximum irregularity for six variables ( @xmath80).,title=\"fig : \" ]       0.2 ) ; model 2 has @xmath77 ; model 3 has @xmath78 ; model 4 has @xmath79 ; models 5 and 6 have the maximum irregularity for six variables ( @xmath80).,title=\"fig : \" ] [ fig : model2 ]     +   +   +    0.2 ) ; model 2 has @xmath77 ; model 3 has @xmath78 ; model 4 has @xmath79 ; models 5 and 6 have the maximum irregularity for six variables ( @xmath80).,title=\"fig : \" ]       0.2 ) ; model 2 has @xmath77 ; model 3 has @xmath78 ; model 4 has @xmath79 ; models 5 and 6 have the maximum irregularity for six variables ( @xmath80).,title=\"fig : \" ]     +   +   +    0.2 ) ; model 2 has @xmath77 ; model 3 has @xmath78 ; model 4 has @xmath79 ; models 5 and 6 have the maximum irregularity for six variables ( @xmath80).,title=\"fig : \" ]    0.2 ) ; model 2 has @xmath77 ; model 3 has @xmath78 ; model 4 has @xmath79 ; models 5 and 6 have the maximum irregularity for six variables ( @xmath80).,title=\"fig : \" ]    to make this comparative study , we selected the six different target structures shown in figure  [ fig : n6graphs ] .",
    "these graphs represent different cases of irregularity , according to ( [ eq : irregularity ] ) .",
    "the first target structure is regular ( irr = 0 ) , the second has a little irregularity , the third and fourth structures are irregular structures with a hub topology , and the fifth and sixth target structures have maximum irregularity for @xmath81 . for constructing a probability distribution from these independence structures according to ( [ eq : gibbs ] ) ,",
    "random numeric values were assigned to their maximal clique factors , sampled independently from a uniform distribution over @xmath82 .",
    "ten distributions were generated for each target structure , considering only binary discrete variables .",
    "then , for each one , ten different random seeds were used to obtain @xmath83 datasets for each graph , by using the gibbs sampling tool of the open - source libra toolkit @xcite .",
    "the gibbs sampler was run with 100 burn - in and 1000 sampling iterations , as commonly used in other works .",
    "since we have @xmath81 variables , the search space consists of @xmath84 different undirected graphs . the experiment consisted of evaluating the number of true structures returned by each score over the 100 datasets .",
    "this is called here the success rate of the scoring function .",
    "the success rate is computed for increasing dataset sizes @xmath85 .",
    "of course , since greater sizes of the dataset lead to better estimations , @xmath86 affects the quality of the structure learned .",
    "therefore , a score is considered better than another score when its success rate converges to @xmath87 with lower values of @xmath86 .",
    "table  [ table : consistency ] shows the results of the experiment .",
    "the first column shows the target structures , the second shows their irregularity , the third shows each sample size @xmath86 used , and the fourth shows the success rate . for all the cases , it can be seen how the success rate of the three scoring functions grows with the sample size @xmath86 .",
    "the results in the fourth column show that bjp has a better success rate in almost all cases . for structures 1 and 2",
    ", ib - score shows better convergence than bjp , but they would eventually converge similarly for greater @xmath86 sizes .",
    "in contrast , for structures 3 , 4 , 5 and 6 , bjp has in general the best success rate .",
    "for all the cases mpl has a slower convergence than ib - score and bjp .",
    "this is consistent with the experimental results shown in @xcite , where the quality for mpl with irregular structures is reported as very low .",
    "interestingly , bjp obtains improvements in success rate of up to 8.4% respect to ib - score , and up to 59% respect to mpl . in general , these results are consistent with the hypothesis of this work , since bjp has been designed to improve the sample complexity when learning irregular structures .",
    "the following section shows the performance of the three scoring functions for more complex domains .",
    ".success rate of bjp , ib - score and mpl over 100 datasets for the target structures on figure  [ fig : n6graphs ] .",
    "rates in bold face correspond to the best case .",
    "[ table : consistency ] [ cols=\"^,^,^,^,^,^,^ \" , ]      in this section , experiments in higher - dimensional setting are presented . for this",
    ", we evaluate the quality of the structures learned by using an approximate search mechanism . the bjp score and",
    "the ib - score were tested with the ibmap - hc algorithm proposed in @xcite , explained at the end of section  [ sec : bjp ] .",
    "the mpl scoring function was tested with the most efficient optimization algorithm proposed in @xcite , described in section  [ sec : structurelearningalgs ] .",
    "the goal in the experiments is to show how the bjp score can improve the quality of the structures learned over the competitor scores , mainly for irregular underlying structures . for this , the selected graphs capture the properties of several real - world problems , where the target structure has fewer nodes with large degrees , and the remaining nodes have very small degree .",
    "examples of problems with this characteristic include gene networks , protein interaction networks and social networks @xcite .",
    "thus , for this comparative study , we used two types of structures : hubs and scale - free networks generated by the barabasi - albert model @xcite .",
    "these structures have an increasing complexity both in @xmath73 and in @xmath88 . additionally , we used four real - world networks , taken from the sparse matrix collection of @xcite .",
    "the hub networks are shown in figure  [ fig : hubs ] , the scale - free networks are shown in figure  [ fig : scalefree ] , and the real - world networks are shown in figure  [ fig : realworldnets ] .",
    "for each target structure we generated @xmath89 random distributions and @xmath89 random samples for each distribution , with the gibbs sampler tool of the libra toolkit .",
    "thus , a total of @xmath83 datasets were obtained for each graph , with the same procedure explained in the previous section . as a quality measure ,",
    "we report the average edge hamming distance between the hundred learned structures and the underlying one , computed as the sum of false positives and false negatives in the learned structure . as in the previous section ,",
    "the algorithms were executed for increasing dataset sizes @xmath85 , to asses how their accuracy evolves with data availability .",
    "* hub 1 * , title=\"fig : \" ] * hub 2 * , title=\"fig : \" ] +   + * hub 3 * , title=\"fig : \" ] * hub 4 * , title=\"fig : \" ] +   +    * scale - free 1 * , title=\"fig : \" ] +    ' '' ''     + * scale - free 2 * , title=\"fig : \" ] +    ' '' ''     + * scale - free 3 * , title=\"fig : \" ] +    ' '' ''     + * scale - free 4 * , title=\"fig : \" ] +    ' '' ''     +    * a ) karate * , title=\"fig : \" ] +    ' '' ''     + * b ) curtis-54 * , title=\"fig : \" ] +    ' '' ''     + * b ) will-57 * , title=\"fig : \" ] +    ' '' ''     + * dolphins * , title=\"fig : \" ] +    ' '' ''     +    table  [ table : hubs ] shows the comparison of bjp against mpl and ib - score for the hub structures of figure  [ fig : hubs ] . the table shows the structures , their sizes @xmath73 , and their irregularities , in the first , second and third columns , respectively .",
    "the dataset sizes @xmath86 are in the fourth column .",
    "the fifth column shows the average and standard deviation of the hamming distance over the @xmath83 repetitions .",
    "the sixth column shows the corresponding runtimes ( in seconds ) .",
    "when analyzing these results , it can be seen that for all the algorithms the more complex the underlying structure ( determined by @xmath73 and @xmath88 ) , the larger is the number of structural errors for any value of @xmath86 .",
    "the results show that bjp obtains the best performance , reducing the number of errors of the structures learned for all the cases .",
    "when compared to ib - score , the improvements are more important as @xmath73 and @xmath88 grow .",
    "this is because in those cases bjp uses a set of independence tests with lower sample complexity than ib - score to estimate the posterior of the structures .",
    "it can also be seen that , for all the target structures , mpl has the slowest convergence in @xmath86 .",
    "this is consistent with the results shown in the previous section , obtained by using brute force . in terms of the respective runtimes , the optimization using the bjp score obtains in general runtimes comparable to mpl and ib - score . for the case of hub 4",
    ", bjp shows the best runtime for all the cases where @xmath90 .",
    "this is because the more complex the underlying structure the better the convergence of the bjp score to correct structures .",
    "table [ table : scalefree ] shows the comparison of bjp against mpl and ib - score for the scale - free networks of figure  [ fig : scalefree ] .",
    "the information of the table is organized in the same way as in table  [ table : hubs ] .",
    "for all the scores , it can be seen that the trends in these results are similar to those of the hub structures .",
    "in contrast with the hub structures , in the scale - free networks the size of the blankets is more variable . this can explain the diference in the trends of the hamming distance , when compared with the results obtained for the hub networks .",
    "for the two most complex structures ( scale - free 3 and 4 ) , bjp reduces the number of errors of the structures learned in all the cases . in terms of the respective runtimes",
    ", bjp obtains the best runtimes for almost all the cases .",
    "specifically , for scale - free 2 , for all the cases where @xmath91 ; for scale - free 3 , for all the cases ; and for scale - free 4 , for all the cases where @xmath90 .",
    "as the complexity of the target structures grows , we can see a better convergence of the bjp score to correct structures .",
    "finally , table [ table : realnets ] show the results for the real - world networks of figure  [ fig : realworldnets ] .",
    "again , the information of this table is organized in the same way as in the previous tables .",
    "the real network structures are ordered by their complexity ( in @xmath73 and @xmath88 ) .",
    "the trends in these results are consistent to those in the previous tables , in terms of quality and runtime .",
    "for the karate , curtis-54 and will-57 networks , bjp improves the quality of the structures learned for all the cases when @xmath92 . when @xmath93 ib - score obtains the best qualities .",
    "however , the differences in favor of ib - score are not statistically significant , and the runtime of the optimization is one or two orders of magnitude slower compared to bjp . for the dolphins network",
    ", bjp improves the quality of the structure learned for all the cases . regarding the runtimes",
    ", it can be seen again that bjp tends to improve the runtime over mpl and ib - score for almost all the cases .    in general",
    ", the results discussed confirm that bjp always outperforms the competitors when data are scarce . also , the improvements are greater both in quality and runtime , for the more complex models .",
    "this confirms the hypothesis that the bjp score takes advantage of irregularities to optimize the sample complexity .",
    "in this work we have introduced a novel scoring function for learning the structure of markov networks .",
    "the bjp score computes the posterior probability of independence structures by considering the joint probability distribution of the collection of markov blankets of the structures .",
    "the score computes the posterior of each markov blanket progressively , using information of other blankets as evidence .",
    "the blanket posteriors of variables with fewer neighbors is computed first , and then this information is used as evidence for computing the posteriors for variables with bigger blankets .",
    "thus , bjp can be useful to improve the data efficiency for problems with complex networks , where the topology exhibits irregularities , such as social and biological networks . in the experiments ,",
    "bjp scoring proved to improve the sample complexity when compared with the state - of - the - art competitors .",
    "the score is tested by using exhaustive search for low - dimensional problems and by using a heuristic hill - climbing mechanism for higher - dimensional problems .",
    "the results show that bjp produces more accurate structures than the selected competitors .",
    "we will guide our future work toward the design of more effective optimization methods , since the hill - climbing optimization has two inherent disadvantages : i ) by only flipping one edge per step it scales slowly with the number of variables of the domain @xmath73 , ii ) it is prone to getting stuck in local optima .",
    "moreover , we consider that the properties of bjp score have considerable potential for both further theoretical development , and applications .",
    "this work was supported by consejo nacional de investigaciones cientficas y tcnicas ( conicet ) [ pip 2013 117 ] , universidad nacional del litoral ( unl ) [ cai+d 2011 548 ] and agencia nacional de promocin cientfica y tecnolgica ( anpcyt ) [ pict 2014 2627 ] and [ pict-2012 - 2731 ] ."
  ],
  "abstract_text": [
    "<S> markov networks are extensively used to model complex sequential , spatial , and relational interactions in a wide range of fields . by learning the structure of independences of a domain </S>",
    "<S> , more accurate joint probability distributions can be obtained for inference tasks or , more directly , for interpreting the most significant relations among the variables . </S>",
    "<S> however , the performance of current available methods for learning the structure is heavily dependent on the choice of two factors : the structure representation , and the approach for learning such representation . </S>",
    "<S> this work follows the probabilistic maximum - a - posteriori approach for learning undirected graph structures , which has gained interest recently . </S>",
    "<S> thus , the _ blankets joint posterior _ score is designed for computing the posterior probability of structures given data . </S>",
    "<S> in particular , the score proposed can improve the learning process when the solution structure is irregular ( that is , when there exists an imbalance in the number of edges over the nodes ) , which is a property present in many real - world networks . </S>",
    "<S> the approximation proposed computes the joint posterior distribution from the collection of markov blankets of the structure . </S>",
    "<S> essentially , a series of conditional distributions are calculated by using , information about other markov blankets in the network as evidence . </S>",
    "<S> our experimental results demonstrate that the proposed score has better sample complexity for learning irregular structures , when compared to state - of - the - art scores . by considering optimization with greedy hill - climbing search , </S>",
    "<S> we prove for several study cases that our score identifies structures with fewer errors than competitors . </S>"
  ]
}