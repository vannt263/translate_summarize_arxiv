{
  "article_text": [
    "a problem in regression analysis is to determine how many basis functions to include in the regression model , for instance , when determining the calibration curve that best fits the data @xcite .",
    "any set of basis functions can be considered ; when they are polynomials , the problem is determining the degree of the regression .",
    "a maximum likelihood approach , which leads to the highest possible number of the basis functions , can not be the right choice .",
    "this problem has been considered by many authors in different statistical settings and their investigations led to a number of proposed solutions @xcite .",
    "an original and undeservedly neglected one is hidden in a tutorial paper on bayesian reasoning by gull @xcite , where the basic idea is to calculate and to compare each model probability , given the data .    in order to bring this result to the metrologist s attention ,",
    "we reassess the gull work and make clear its usefulness in selecting among linear models . in addition , by slightly changing the model parametrisation , we obtain an exact analytical solution and demonstrate that , in suitable limit cases , it reduces to the gull s approximate one .    here obtained results may have an impact when the polynomial coefficients are used for solving systems of partial differential equations @xcite . in this case , different choices of the polynomial degree lead to different sets of coefficients and , consequently , to different solutions .",
    "the availability of a rigorous criterion based on the probability calculus allows any arbitrary choice  in general , driven only by the residuals analysis  to be avoided . to illustrate the concepts here described ,",
    "it is shown how to determine the set of basis functions that best fits the measured values of the speed of sound in acetone , as a function of the temperature and pressure .",
    "we want to represent the @xmath0^{{\\rm t}}$ ] measurement results by the linear model @xmath1 where @xmath2^{{\\rm t}}$ ] are additive uncorrelated gaussian errors having unknown variance @xmath3 and zero mean , @xmath4^{{\\rm t}}$ ] are @xmath5 model parameters , @xmath6 is a @xmath7 matrix explaining the data , @xmath8 , and @xmath9 is a set of @xmath5 basis functions .",
    "the basis functions may be polynomials , for instance , @xmath10 , but , in general , they are any set of linearly independent functions .",
    "the problem is to find the set of basis functions most supported by the data ; when they are polynomials , this corresponds to find the optimal degree of the regression .    the interpretative model of the data is summarised by the matrix @xmath6 ; therefore , the problem is equivalent to find  within a set of matrices explaining the data  the one most supported by the data .",
    "since it explicitly appears in the final formulae and for the sake of notational simplicity , we label the @xmath11 matrices by the number @xmath5 of the model free - parameters . however , we can compare also models having the same number of parameters but different basis functions .",
    "according to the bayes theorem  by assigning the same probability to all the models  the probability of the @xmath5-th model to explain the data is proportional to the probability of the observed data given @xmath6 , no matter what the values of the model parameters may be . in turn , it is the normalising factor of the likelihood of the model parameters times the probability distribution synthesising the information available about the parameter values before the measurement results are available .    to steer the calculation , we must first determine the post - data probability density , @xmath12 , of the parameters of each model ( which parameters include the unknown variance @xmath3 of the data ) given the @xmath13 data and the data - explaining matrix @xmath6 .",
    "the post - data probability density is found via the product rule of probabilities @xcite , @xmath14 where the @xmath15-dimensional gaussian function @xmath16 is the likelihood of the @xmath17 and @xmath18 parameters , @xmath19 is their pre - data probability density , the sought normalisation factor of @xmath20 , @xmath21 is named model evidence , and the integration is carried out over the hypervolume @xmath22 associated to the possible @xmath17 and @xmath18 values .",
    "next , by observing that @xmath23 is also the probability density of the data given @xmath6  whatever the values of @xmath17 and @xmath18 may be  we get the post - data model - probability , @xmath24 , by applying again the product rule of probabilities to the @xmath25 pair .",
    "hence @xcite , @xmath26 where , prior the data are at hand , we assigned the same probability to each model and @xmath27 where @xmath28 is the number of models to be compared , is the normalisation factor of @xmath23 .",
    "therefore , to solve the stated problem , the calculation of the evidence ( [ evidence ] ) is central .",
    "to set the pre - data distribution of @xmath17 and @xmath18 , we assume that they are independent . hence , @xmath29 .",
    "as regards @xmath18 , we use the improper jeffreys prior @xcite @xmath30 which is invariant for a change of the measurement unit of the data .",
    "as regards the @xmath17 parameters , let the mean of @xmath31 , whatever the @xmath17 values may be , null .",
    "the relevant average is carried out over the joint distribution @xmath20 , not over the sampling distribution of the data @xmath32 , where the @xmath17 values are fixed .",
    "consequently , since @xmath33 and @xmath34 are zero - mean errors , also the pre - data mean of the @xmath17 parameters is zero .",
    "in addition , let @xmath35 be the pre - data covariance of @xmath31 , whatever the @xmath17 values may be and where @xmath36 is the unit matrix .",
    "also in this case , the stated covariance is relevant to the joint distribution @xmath20 .",
    "hence , by observing that @xmath37 , because of ( [ model ] ) , the pre - data covariance @xmath38 of the model parameters is @xmath39 where @xmath40 .",
    "eventually , since the prior distribution of @xmath17 is constrained by @xmath41 and ( [ caa ] ) , where the angle bracket indicate the mean , the principle of maximum entropy fixes the sought pre - data distribution to the @xmath5-dimensional gaussian distribution @xcite @xmath42 having zero mean and @xmath38 covariance .",
    "actually , the @xmath43 value is unknown .",
    "therefore , we should eliminate it by marginalisation , @xmath44 where @xmath45 , \\ ] ] and @xmath46 is a jeffreys prior .",
    "however , since the integration in ( [ prior : c ] ) can not be done analytically , we add @xmath43 to the model parameters and delay the marginalisation over @xmath43 as much as possible . hence , by putting ( [ jeffreys ] ) and ( [ prior : b ] ) together , we obtain the pre - data distribution of the full set of model parameters , @xmath47 .\\ ] ]    a comment is necessary . in general",
    ", the use of improper priors  like @xmath48 and @xmath49  should be avoided , because , in such a case , the model evidence ( [ evidence ] ) is defined only up to unknown scale factors .",
    "however , since in this case the same factor is included in all the evidences , this does not jeopardise the model comparison .",
    "by combining ( [ lik ] ) and ( [ prior : d ] ) , the evidence of @xmath6 is @xmath50             \\exp \\left [ -\\frac{{\\bi{a}}^{{\\rm t}}w^{{\\rm t}}w { \\bi{a}}}{2(\\beta^2-\\sigma^2 ) } \\right ] \\rmd{\\bi{a}}.\\end{aligned}\\ ] ]    before carrying out the integration , we observe that @xmath51 where @xmath52 is the least squares estimate of @xmath17 and @xmath53 is the measurand estimate . hence , the first integration is @xmath54   \\int_{-\\infty}^{+\\infty } \\exp \\left [ \\frac{-({\\bi{a}}-{\\hat{\\bi{a}}})^{{\\rm t}}w^{{\\rm t}}w ( { \\bi{a}}-{\\hat{\\bi{a}}})}{2\\sigma^2 } \\right ]   \\exp \\left [ \\frac{-{\\bi{a}}^{{\\rm t}}w^{{\\rm t}}w { \\bi{a}}}{2(\\beta^2-\\sigma^2 ) } \\right]\\ , \\rmd { \\bi{a}}= \\\\   \\left ( \\frac{\\sigma}{\\beta } \\right)^l \\sqrt { \\frac{(2\\pi)^l\\,(\\beta^2-\\sigma^2)^l}{\\det(w^{{\\rm t}}w ) } } \\ ,   \\exp \\left [ \\frac{-{\\bi{y}}^{{\\rm t}}{\\hat{\\bepsilon}}}{2\\sigma^2 } \\right ]   \\exp \\left [ \\frac{-|{\\hat{\\bi{y}}}|^2}{2\\beta^2 } \\right ] , \\end{aligned}\\ ] ] where @xmath55 are the residuals and @xmath56 .",
    "it must be noted that @xmath57 because @xmath58 no matter what the @xmath17 value may be .",
    "consequently , the right - hand side of ( [ formula ] ) is greater than zero also when @xmath59 .",
    "hence , @xmath60 .",
    "the next integration ,",
    "@xmath61   \\int_\\sigma^{+\\infty } \\frac{1}{\\beta^{l+1 } }   \\exp \\left [ \\frac{-|{\\hat{\\bi{y}}}|^2}{2\\beta^2 } \\right ] \\rmd\\beta = \\\\",
    "\\frac{\\sqrt{2^{l-2 } } \\left\\ { \\gamma(l/2 ) - \\gamma\\big[p/2,|{\\hat{\\bi{y}}}|^2/(2\\sigma^2)\\big ] \\right\\ } }   { \\sigma^{n+1-l}\\sqrt{(2\\pi)^n } \\ , |{\\hat{\\bi{y}}}|^l }   \\exp \\left [ \\frac{-{\\bi{y}}^{{\\rm t}}{\\hat{\\bepsilon}}}{2\\sigma^2 } \\right ] , \\end{aligned}\\ ] ] where @xmath62 is the euler gamma function , eliminates @xmath43 .    eventually , provided @xmath63 , the evidence is @xmath64}{\\sigma^{n+1-l } }   \\exp \\left [ \\frac{-{\\bi{y}}^{{\\rm t}}{\\hat{\\bepsilon}}}{2\\sigma^2 } \\right ] \\rmd\\sigma \\\\ \\fl   = \\frac{\\gamma\\left(\\frac{n - l}{2}\\right)}{4\\sqrt{\\pi^n } } \\left [   \\frac{\\gamma\\left(\\frac{l}{2}\\right)}{|{\\hat{\\bi{y}}}|^l ( { \\bi{y}}^{{\\rm t}}{\\hat{\\bepsilon}})^{(n - l)/2 } }   - \\frac{\\gamma\\left(\\frac{l}{2}\\right)\\ , _ 2{\\tilde f}_1\\left ( \\frac{l}{2 } , \\frac{n - l}{2 } ; \\frac{n+2-l}{2 } ;   \\frac{{\\bi{y}}^{{\\rm t}}{\\hat{\\bepsilon}}}{|{\\hat{\\bi{y}}}|^2 } \\right)}{|{\\hat{\\bi{y}}}|^n } \\right ] , \\end{aligned}\\ ] ] where @xmath65 is the regularised hypergeometric function .      by assuming that , prior the data are available , each @xmath6 has the same probability , according to ( [ degree ] ) and ( [ norm ] ) , the @xmath5-model probability is proportional to the @xmath5-model evidence ; that is , @xmath66 it is worth noting that , since @xmath23 is the marginal probability density of the data given @xmath6 ( no matter what the values of the model parameters may be ) , the dimensions of @xmath67 are the same of @xmath68 .    if @xmath69 explains the data exactly , then @xmath70 and @xmath71 , as expected",
    "furthermore , @xmath67 is independent of the @xmath31 scale .",
    "in fact , when @xmath72 , the evidence of @xmath5 transforms as @xmath73 , which leaves @xmath67 unchanged .",
    "in addition , @xmath23 depends only on @xmath31 and @xmath74 ; therefore , it is independent of the choice of the sampling points @xmath75 . eventually , @xmath76 is not invariant for translation of the origin of the @xmath77-axis ; this is a consequence of the @xmath78 assumption , which is embedded into the pre - data distribution of the @xmath17 coefficients .      in the case",
    "when @xmath79 , we can use the approximations @xmath80 and @xmath81 } .\\ ] ] in addition , for a large data sample , since @xmath82 , where @xmath83 is the sum of the squared data , and @xmath84 , it follows that @xmath85 , where @xmath86 indicates the sum of the squared residuals . therefore , apart from the @xmath87 const .",
    "factor that we omit , we can rewrite ( [ evidence : p ] ) as @xmath88\\gamma(l/2)}{(\\chi_\\epsilon/\\chi_y)^{n - l } }   - \\frac{\\gamma(n/2 ) } { ( 1 + \\chi_\\epsilon^2/\\chi_y^2)^{n/2 } } .\\ ] ]    eventually , if @xmath89  which means good data and good models  and @xmath90 , ( [ evidence : p1 ] ) simplifies further as @xmath91\\gamma(l/2)}{(\\chi_\\epsilon/\\chi_y)^{n - l } } .\\ ] ] this equation brings into light that , among the models having the same number of free parameters , the most supported by the data is that whose associated sum of the squared residuals is minimum . in additions , it shows that the optimal model minimises the residuals by keeping at the same time @xmath5 as small as possible , in order to maximise @xmath92 .    as a last step we write ( [ evidence : p2 ] ) as @xmath93 \\approx \\ln\\left[\\gamma\\left(\\frac{n - l}{2}\\right)\\right ] + \\ln\\left[\\gamma\\left(\\frac{l}{2}\\right)\\right ] + ( n - l)\\ln(\\chi_y/\\chi_\\epsilon ) , \\ ] ] which is the approximation given in @xcite .",
    "now , we study the case when the data are samples of a polynomial having degree @xmath94 and the @xmath34 variance tends to zero . in this case ,",
    "provided @xmath95 , the residuals are independent of the degree of the fitting polynomial , @xmath96 , and @xmath97 \\approx ( n - l)\\ln(\\chi_y/\\chi_\\epsilon ) , \\ ] ] which shows that the evidence is maximum when @xmath5 is minimum . therefore",
    ", the degree most supported by the data is @xmath94 , as expected .",
    "if @xmath98  which means bad data or bad models  and @xmath90 , ( [ evidence : p1 ] ) simplifies as @xmath99\\gamma(l/2 ) , \\ ] ] which indicates that the optimal data model has only one degree of freedom , that is , @xmath100 .    ) ; solid line are the most probable polynomials explaining these specific data sets ( left , a third degree polynomial ; right , a fifth degree polynomial).,title=\"fig:\",width=238 ] ) ; solid line are the most probable polynomials explaining these specific data sets ( left , a third degree polynomial ; right , a fifth degree polynomial).,title=\"fig:\",width=238 ]",
    "the figure [ data ] shows two independent sets of @xmath101 simulated data each , uniformly sampled in the @xmath102 $ ] interval from the fifth degree polynomial @xmath103 the outputs of a gaussian random - number generator having zero mean and 0.4 standard deviation were added to the data . in order to fulfill the @xmath104 requirement ,",
    "the data have been pre - processed to remove the arithmetic mean .",
    "the @xmath105 value of the @xmath34 standard deviation was chosen intermediate between the good and bad data limit cases .",
    "to explain the data , a set of ten polynomials  with degree from zero to nine  have been considered , each polynomial has been fitted to the data , and both the error and data estimates  @xmath106 and @xmath74 , respectively  have been calculated .",
    "eventually , the evidence of each polynomial has been found by application of ( [ evidence : p ] ) as well as , after normalisation , each polynomial probability to explain the data has been calculated . the results for each of the data sets shown in fig .",
    "[ data ] are shown in fig .  [ degree - fig ] .    with the @xmath105 choice , the degree of the polynomials that best explain the data have been always found equal to three or five , depending on the specific data set .",
    "the figures [ data ] and [ degree - fig ] show these alternating cases .",
    "it is worth noting that the probability to explain the data of a polynomial of fourth degree  whose basis function is missing in ( [ poly ] )  is very low . as expected , by reducing the noise level , the most likely degree stabilises on five , whereas , by increasing the noise level , it stabilises on three .",
    ".,title=\"fig:\",width=238 ] .,title=\"fig:\",width=238 ]",
    "in @xcite it was shown how to solve the thermodynamic differential equations @xmath107 \\label{pde2}\\end{aligned}\\]]relating density @xmath108 , heat capacity @xmath109 , and speed of sound @xmath110 , as a function of the temperature @xmath111 and pressure @xmath112 .",
    "these equations can be solved if initial conditions @xmath113 and @xmath114 are given at a the reference pressure , @xmath115 , and the speed of sound is known on the entire range of pressures and temperatures of interest .    when a numerical integration of ( [ eq1]-@xmath116 ) is carried out",
    ", the heat capacity shows often diverging values at the extremes of the temperature range . approaching the integration problem by using local polynomial representations of the thermodynamic quantities eliminates the divergence and allows the uncertainty of the results to be estimated .",
    "hence , by using the trial solutions @xmath117and  once the degrees of the polynomials have been fixed ",
    "the unknown coefficients @xmath118 , @xmath119 , and @xmath120 are obtained as described in @xcite .",
    "briefly , the best polynomial approximations of the initial conditions and speed of sound are used to determine @xmath121 , @xmath122 , and @xmath123 ; subsequently , the remaining coefficients @xmath118 and @xmath119 are calculated by means of the equations ( [ pde1 ] ) and ( [ pde2 ] ) .    as an application example",
    ", we show how to determine the optimal polynomial when smoothing the measured values of the speed of sound in acetone as a function of temperature and pressure @xcite ; a set of measurement results is shown in fig .",
    "[ speed ] .",
    "for the sake of numerical convenience , the temperature , pressure , and speed have been scaled in @xmath124 $ ] intervals according to @xmath125 , @xmath126 , and @xmath127 , where the offsets and scale factors have been suitably chosen .    as shown in fig .",
    "[ model-1 ] , the regression analyses using the seven basis - function sets @xmath128 , where @xmath129 and @xmath130 , indicate that the @xmath131 set is the only one supported by the data .",
    "this sharp selection is due to the fast increase of the number @xmath5 of basis functions as the polynomial degree increases .",
    "for instance , if @xmath132 , then @xmath133 ; if @xmath131 , then @xmath134 ; if @xmath135 , then @xmath136 .",
    "$ ] intervals ; @xmath137 is the temperature , @xmath77 is the pressure , and @xmath138 is the speed of sound . the polynomial model most likely supported by the data is also shown .",
    "red dots are the data higher than what predicted by the model , blue dots are those lower.,width=245 ]    .,width=245 ]    in order to carry out a more detailed analysis , regressions were carried out also by using the 190893 subsets of 14 , 15 , and 16 basis functions chosen in the @xmath139 list , where @xmath140 and @xmath141 is a legendre polynomials of degree @xmath142 .",
    "the results are shown in fig .",
    "[ subsets ] .",
    "the 14 basis functions whose linear combination  which corresponds to a fifth degree polynomial ",
    "most likely explains the data are @xmath143 , @xmath144 , @xmath145 , @xmath146 , @xmath147 , @xmath148 , @xmath149 , @xmath150 , @xmath151 , @xmath152 , @xmath153 , @xmath154 , @xmath155 , @xmath156 .",
    "a fallout of this bayesian analysis are the probabilities of all the sets of smoothing polynomials considered to model the data .",
    "consequently , when , as in this case , a number of basis function sets have a significant probability to explain the data , the quantities of interest  in this case , the speed of sound values  and the uncertainty inherent the model selection can be inferred by model averaging @xcite .     of the subsets from @xmath157 to @xmath158 of 14 , 15 , and 16 basis functions chosen in the @xmath159 list , where @xmath140 .",
    "the subsets are numbered with the shortest first and the later elements in the list omitted first .",
    "the probability of the remaining subsets is zero for all practical purposes.,width=245 ]",
    "an analytical solution has been found for the problem of finding what basis functions must be used in linear regression analyses .",
    "it relies on the bayesian model selection and complements the numerical results given in @xcite .",
    "it uses the probability algebra to select among different basis function sets and encodes a preference for the smallest set capable to explain the data . in practice , a probability density is assigned to the regression coefficients prior the data are available , consistently with the given prior information and according to the maximum entropy principle .",
    "next , the probability algebra allows this probability distribution to be updated according to the additional information delivered by the data .",
    "the regression probability is proportional to the normalising factor of the parameter likelihood times the parameter prior distribution .",
    "a feature of this solution is that , for a large data sample , the regression probability depends only on the residuals and the number of free parameters .",
    "the smaller are the residuals , the higher the probability ; but , a penalty exists for increasing the number of parameters .",
    "in addition , if a basis - function set explains the data exactly , its probability to explain the data is one .",
    "this work was jointly funded by the european metrology research programme ( emrp ) participating countries within the european association of national metrology institutes ( euramet ) and the european union .",
    "99 massa e and mana g 2013 an automated resistor network to inspect the linearity of resistance - thermometry measurements _ meas .",
    "technol . _ submitted anderson t w 1962 the choice of the degree of a polynomial regression as a multiple decision problem _ ann .",
    "statist . _ * 33 * 255 - 65 schwartz g 1978 estimating the dimension of a model _ ann .",
    "statist . _",
    "* 6 * 461 - 4 shao j 1996 bootstrap model selection _",
    "j.  amer .",
    "statist .",
    "assoc . _ * 91 * 655 - 65 philips r and guttman i 1998 a new criterion for variable selection _ statistics & probability letters _ * 38 * 11 - 9 guttman i , pena d and redondas d 2005 a bayesian approach for predicting with polynomial regression of unknown degree _ technometrics _ * 47 * 23 - 33 gull s f 1988 bayesian inductive inferences and maximum entropy _ in : maximum entropy and bayesian meyhods in science and engineering _ * 1 * 53 - 74 ( dordrecht , the netherlands : kluwer academic publishers ) lago s and giuliano albo p 2008 a new method to calculate the thermodynamical properties of liquids from accurate speed - of - sound measurements _ j.  chem .",
    "thermodynamics _ * 40 * 1558 - 64 lago s and giuliano albo p 2013 a novel application of recursive equation method for determining thermodynamic properties of single phase fluids from density and speed - of - sound measurements _",
    "j.  chem .",
    "thermodynamics _ * 58 * 422 - 7 sivia d and skilling j 2006 data analysis : a bayesian tutorial ( oxford : oxford university press ) jaynes e t 2003 probability theory : the logic of science ( cambridge : cambridge university press ) mc kay d jc 2003 information theory , inference , and learning algorithms ( cambridge : cambridge university press ) jaynes e t 1968 prior probabilities _ ieee trans .",
    "cybernetics _ * 4 * 227 - 41 waserman l 2000 bayesian model selection and model averaging _ journal of mathematical psychology _ * 44 * 92 - 107 mana g , massa e and predescu m 2012 model selection in the average of inconsistent data : an analysis of the measured planck - constant values _",
    "metrologia _ * 49 * 492 - 500"
  ],
  "abstract_text": [
    "<S> a widely used method to create a continuous representation of a discrete data - set is regression analysis . </S>",
    "<S> when the regression model is not based on a mathematical description of the physics underlying the data , heuristic techniques play a crucial role and the model choice can have a significant impact on the result . in this paper , </S>",
    "<S> the problem of identifying the most appropriate model is formulated and solved in terms of bayesian selection . besides , probability calculus is the best way to choose among different alternatives . </S>",
    "<S> the results obtained are applied to the case of both univariate and bivariate polynomials used as trial solutions of systems of thermodynamic partial differential equations . </S>"
  ]
}