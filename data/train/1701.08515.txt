{
  "article_text": [
    "bayesian inference is one of the most important scientific learning paradigms in use today .",
    "its core principle is the use of probability to quantify all aspects of uncertainty in a statistical model , and then given data @xmath0 , use conditional probability to update uncertainty via bayes theorem , @xmath1 where @xmath2 denotes the parameters of the model , @xmath3 the posterior update , @xmath4 is the data model , and @xmath5 is the prior on the model parameters .",
    "bayesian updating is _ coherent _ , see for example @xcite . the justification for bayesian updating proceeds on an assumption that the form of the data model , @xmath4 , is correct up to the unknown parameter value @xmath2 .",
    "bayesian learning is optimal , see @xcite , which means that posterior uncertainty is the appropriate reflection of prior uncertainty and the information provided by the data .",
    "however , this is only in the case when the model is true .",
    "this is at odds with the scientific desire for keeping models simple in order to focus on the essential aspects of the system under investigation . recently",
    "a number of papers have appeared seeking to address the mismatch and allow for bayesian learning under model misspecification ; the key reason is _ robustness _ , and the idea is to raise the likelihood to a power .",
    "see , for example , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and more generally @xcite .",
    "the paper by @xcite , in particular , provides a formal motivation using a coherency principle for raising the likelihood to a power .    for the formal bayesian analyst ,",
    "if @xmath4 is misspecified , then there is no connection between any @xmath2 and any observation from this model , and as a consequence no meaningful prior can be set . in this case",
    ", it is argued in @xcite that it is preferable to look at @xmath6 as simply a loss function linking @xmath2 and observation @xmath0",
    ". then a formal general - bayesian update of prior @xmath5 to posterior @xmath7 exists and for the update to remain coherent it was shown that it must be of the form , @xmath8 where @xmath9 is the normalising constant ensuring that the posterior distribution integrates to 1 , and @xmath10 is a weighting parameter calibrating the two loss functions for @xmath2 , namely @xmath11 and @xmath6 . in this way , @xmath12 controls the learning rate of the generalised - bayesian update , with @xmath13 returning the conventional bayesian solution . clearly for @xmath14 the update gives less weight to the data relative to the prior compared to the bayesian model , resulting in a posterior that is more diffuse , and with @xmath15 the data is given more prominence .",
    "the crucial question then becomes how to set @xmath10 in a formal manner .",
    "one needs to be careful as learning about @xmath10 can both be overdone ( @xmath10 set too high and the posterior uncertainty is underestimated ) and under done ( @xmath10 set too low and the posterior uncertainty is overestimated ) .",
    "the elegant and attractive nature of bayesian inference when the model precisely matches nature is that the learning is achieved optimally ; i.e at the correct speed .",
    "see @xcite , @xcite and @xcite .    in this paper",
    "we propose to set @xmath10 once a proper @xmath5 and model @xmath4 have been set by matching the prior expected gain in information between prior and posterior from two potential experiments ; for experiment 1 using @xmath16 we compute an expected information gain between @xmath17 and @xmath18 , denoted by @xmath19 , to be specified later . for experiment 2",
    "we consider the corresponding gain in information between posterior @xmath7 and @xmath5 , which will be @xmath20 .",
    "then we set @xmath10 so that @xmath21 where @xmath22 is the true , unknown , density and @xmath23 is the true parameter value if the parametric model is correct or else is the parameter value minimizing the kullback - leibler divergence between the true model and the parametric family of densities .",
    "so , if the model is correct , then @xmath24 and @xmath10 will automatically be 1 .",
    "the rationale for ( [ ident ] ) is coherence ; that the expected gain in information for learning about @xmath23 from a single sample for both experiments is the same . to elaborate : experiment 1 is assuming the data is not necessarily coming from the parametric model , the likelihood is @xmath25 with prior @xmath5 and @xmath26 . according to @xcite",
    ", the @xmath16 is a valid update for learning about the @xmath2 which minimizes the kullback - leibler divergence between @xmath22 and @xmath4 ; i.e. @xmath23 , and for @xmath27 the posterior @xmath17 will be consistent for @xmath23 for regular models .",
    "that this is being learnt about follows from @xcite .",
    "experiment 2 is assuming the data is coming from the parametric model , the likelihood is @xmath4 with prior @xmath5 and @xmath28 .",
    "both experiments are involved with learning about the same @xmath23 .",
    "we argue that the experimenter should be a priori indifferent between these two experiments with respect to the prior expected gain in information about @xmath23 .",
    "thus , @xmath10 is set so the prior expected gain in information is the same as that which would have been obtained if the parametric model were correct .",
    "we can evaluate both sides of ( [ ident ] ) using the observed data , @xmath29 , so the left side and right side of ( [ ident ] ) are evaluated as @xmath30 respectively , where @xmath31 is the maximum likelihood estimator .",
    "see @xcite about the theory for @xmath31 being the appropriate estimator for @xmath23 . in the next section",
    "we define @xmath19 and in section 3 we present some illustrations .",
    "to quantify the prior expected information of an experiment we utilise the well established notion of fisher information ; see @xcite .",
    "in particular we shall consider the expected divergence in fisher information , @xmath32 , between two density functions @xmath33 and @xmath34 , with exact form given below ; see for example @xcite .",
    "motivation for this choice is given in the appendix .",
    "the fisher relative information divergence of a posterior update from its prior , with likelihood @xmath4 , is given by @xmath35 where the @xmath36 operates on the @xmath37 dimensional @xmath2 .",
    "this is given by @xmath38 hence , with likelihood @xmath25 , we have @xmath39 , where @xmath40 .",
    "this leads to @xmath41 this result also highlights why fisher information is a convenient measure of information in the experiment as it leads to an explicit formula for the setting of @xmath10 .",
    "the actual setting of @xmath10 via ( [ eq : wset ] ) is hindered by the lack of knowledge of @xmath42 and @xmath23 .",
    "however , an empirical approach follows trivially since we can estimate @xmath22 with the empirical distribution function of the data and then estimate @xmath23 with @xmath31 , the maximum likelihood estimator .",
    "thus @xmath43 a common simplifying choice of model would be from the class of exponential family ; @xmath44 where the @xmath45 are a set of basis functions and @xmath46 is the normalizing constant .",
    "then straightforward calculations yield @xmath47 where @xmath23 is given by @xmath48 for all @xmath49 , and @xmath50 .",
    "hence @xmath51 in general we have , under the usual assumptions on the model that @xmath52 , and that @xmath53 :    if @xmath54 then @xmath55 in probability as @xmath56 .    _",
    "proof_. if we write @xmath57 then we have @xmath58 . also , @xmath59 and hence we have the result as @xmath60",
    "we consider illustrations chosen to highlight the essential features of setting @xmath10 , chosen when the model is exponential family ; specifically poisson and normal .      if the model is poisson , then for some @xmath61 the mass function for observation @xmath62 is given by @xmath63 for @xmath64 .",
    "then to find @xmath10 we need to evaluate the denominator and numerator in ( [ eq : wset ] ) , @xmath65 where @xmath66 @xmath23 maximizes @xmath67 and as @xmath4 is poisson we have @xmath68 as the expected values from @xmath42 , and @xmath69 the variance from @xmath42 .",
    "hence , letting @xmath70 and @xmath71 , we find , @xmath72 then for the poisson model fit to data arising from @xmath22 we have @xmath73 and @xmath74 .    on inspection of the result",
    "we see that when @xmath75 , where the data are `` overdispersed '' , we find that @xmath76 .",
    "the idea here is that the data will provide larger than expected observations , from a poisson model perspective , and unless the observations are down weighted , then inference will appear overly precise . downweighting",
    "the information in the observations will provide a more stable and practical inference for the unknown parameter .",
    "equally when the data are underdispersed then the bayesian learning will be adjusted to @xmath77 accounting for the increased precision in the data to learn about the parameter @xmath23 minimising the relative entropy of the model to the data distribution .    to illustrate the performance we conducted the following experiment .",
    "we took @xmath78 observations from an overdispersed model , so @xmath79 given @xmath80 is poisson with mean @xmath80 and @xmath80 is from the gamma distribution with mean @xmath81 and variance @xmath82 .",
    "thus the variance of the data is @xmath83 while the mean of the data is @xmath81 , so there is a substantial amount of overdispersion . the prior for @xmath2 in the poisson @xmath4 model",
    "was taken to be gamma with mean @xmath84 and variance @xmath84 .",
    "for this experiment we then computed @xmath85 using the sample mean @xmath86 and sample variance @xmath87 ; @xmath88 thus @xmath89 we plot the @xmath85 against sample size in fig  [ fig : f1 ] , and note that essentially the @xmath90 , with convergence to a number lower than 1 .",
    "on the other hand , if the model was true ( the so called @xmath91-closed perspective in @xcite ) , then @xmath92 , then @xmath93 .",
    "moreover , using standard asymptotic , large sample size @xmath94 , properties of models and estimators , we have that @xmath95 at a speed of @xmath96 .",
    "we provide some further analysis of the general case for the exponential family based on @xmath97 then following the same strategy as in the previous sub - section , and using ( [ eq : wset ] ) , where now @xmath98 and @xmath99 , we can show @xmath100 which is estimated via @xmath101 thus , @xmath10 will converge to 1 or otherwise depending on how the sample variance @xmath102 compares with the variance estimator from the model ; namely @xmath103 . even in the case of regression models",
    ", the basic idea is the same when @xmath104 is quadratic , as it would be for example in the case of a normal linear regression model .",
    "here we consider a normal model with unknown mean @xmath2 and variance 1 .",
    "the prior for @xmath2 is normal with mean 0 and precision parameter @xmath105 .",
    "the aim here is to compare our selection of @xmath10 with an alternative using the kullback - leibler divergence ; i.e. to set @xmath10 based on matching @xmath106 where @xmath107 .",
    "although there is no closed form solution for @xmath10 here , we can evaluate it numerically .",
    "first we considered the overdispersed case and so generated 50 observations from a normal distribution with precision 0.2 and use the prior for @xmath2 to have mean 0 and precision 0.01 .",
    "then we looked at the underdispersed case and generated 50 observations from a normal distribution with precision 4 and again use the prior for @xmath2 to have mean 0 and precision 0.01    in fig [ fig : f2 ] , on the left side , we plot three posterior distributions : blue is the posterior using the @xmath10 from our fisher information distance ; red is the posterior using the @xmath10 obtained from the kullback - leibler divergence , and the green is the correct posterior had the model been used with the correct precision parameter of 0.2 .    on the right side of fig",
    "[ fig : f2 ] we again plot three posterior distributions : blue is the posterior using the @xmath10 from our fisher information distance ; red is the posterior using the @xmath10 obtained from the kullback - leibler divergence , and the green is the correct posterior had the model been used with the correct precision parameter of 4 . in both cases",
    "we see that our posterior is closer to the posterior based on the correct model ; i.e. replacing 1 with the precisions 0.2 and 4 , respectively .",
    "it can be argued that all models are misspecified . under such a scenario",
    "there is no formal connection between any observed @xmath0 and any @xmath2 when looking at @xmath4 as a density function . on the other hand , when viewed as a loss function , @xmath6 , and learning about @xmath108",
    ", we can interpret the correspondence between @xmath0 and the object of inference @xmath2 .",
    "however , as pointed out in @xcite , in this setting there is a free parameter @xmath10 introduced by the model misspecification . in this paper",
    "we have introduced principles for the specification of @xmath10 which provides an a priori coherent agenda in terms of prior expected gain in information about @xmath23 .",
    "as shown in @xcite , the expected ( with respect to the prior predictive ) fisher information distance between prior and posterior is given by @xmath109 where @xmath110 is the fisher information for @xmath2 , @xmath111 is the prior predictive @xmath112 , and @xmath113 is known as the fisher information for the density @xmath5 , while @xmath114 is the fisher information for the posterior given @xmath79 .",
    "so it has similar properties to the kullback - leibler divergence which relies on expected differential entropy between prior and posterior .",
    "however , instead of using @xmath115 to get ( [ fishi ] ) , we use @xmath116 for the former is suited to the idealized setting of a correct model ; whereas we are trying to evaluate the prior and posterior discrepancy , i.e. @xmath117 where @xmath118 is the usual score function , with respect to prior beliefs , for it is only the prior beliefs we assume common to both experimenters ; i.e. the one using @xmath119 and the one using @xmath120 .",
    "we can elaborate further : the prior expected fisher information ; i.e. @xmath121 , is @xmath122 this would be the expected information in a single sample as an expected discrepancy between prior and posterior . however , this expected fisher information is provided under the idealized setting that the joint density of @xmath123 for the expectation of @xmath124 is @xmath125 it would be unrealistic for us to assume the marginal density for @xmath0 is @xmath111 , even for the bayesian assuming @xmath126 is correct .",
    "a more realistic estimation of the expected squared score function , i.e. information in a single sample , would be to use the empirically determined joint density @xmath127 .    for the bayesian using @xmath128 , the score function is @xmath129 , and so would estimate the information , using the product measure of the prior and empirical distribution function , @xmath130 , since this bayesian is assuming the model incorrect . matching these two forms of information from a single sample and about the same parameter ,",
    "we have @xmath131 where the term on the left is given by @xmath132 and recall that @xmath133 . in short , we are using the square of the score function as a measure of information in a single sample which also has the interpretation in terms of fisher distance between prior and posterior .",
    "the authors are grateful to two anonymous referees and an associate editor for comments and suggestions on a previous version of the paper ."
  ],
  "abstract_text": [
    "<S> bayesian approaches to data analysis and machine learning are widespread and popular as they provide intuitive yet rigorous axioms for learning from data ; see @xcite and @xcite . </S>",
    "<S> however , this rigour comes with a caveat , that the bayesian model is a precise reflection of nature . </S>",
    "<S> there has been a recent trend to address potential model misspecification by raising the likelihood function to a power , primarily for _ robustness _ reasons , though not exclusively . in this paper </S>",
    "<S> we provide a coherent specification of the power parameter once the bayesian model has been specified in the absence of a perfect model . </S>"
  ]
}