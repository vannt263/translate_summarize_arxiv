{
  "article_text": [
    "cylindrical algebraic decomposition ( cad ) is a key tool in real algebraic geometry .",
    "it was first introduced by collins @xcite to implement quantifier elimination over the reals , but has since been applied to applications including robot motion planning @xcite , programming with complex valued functions @xcite , optimisation @xcite and epidemic modelling @xcite .",
    "decision methods for real closed fields are of great use in theorem proving @xcite .",
    "metitarski @xcite , for example , decides the truth of statements about special functions using cad and rational function bounds .",
    "when using cad , we often have a choice over which variable ordering to use .",
    "it is well known that this choice is very important and can dramatically affect the feasibility of a problem .",
    "in fact , brown and davenport @xcite presented a class of problems in which one variable ordering gave output of double exponential complexity in the number of variables and another output of a constant size .",
    "heuristics have been developed to help with this choice , with dolzmann  et  al .",
    "@xcite giving the best known study .",
    "however , in cicm last year @xcite , it was shown that even the best known heuristic could be misled .",
    "although that paper provided an alternative heuristic , this had its own shortcomings , and it now seems likely that no one heuristic is suitable for all problems .",
    "our thesis is that the best heuristic to use is dependent upon the problem considered .",
    "however , the relationship between the problems and heuristics is far from obvious and so we investigate whether machine learning can help with these choices .",
    "machine learning is a branch of artificial intelligence .",
    "it uses statistical methods to infer information from supplied data which is then used to make predictions for previously unseen data @xcite .",
    "we have applied machine learning ( specifically a support vector machine ) to the problem of selecting a variable ordering for both cad itself and quantifier elimination by cad , using the nlsat dataset @xcite of fully existentially quantified problems .",
    "our results show that the choices made by machine learning are on average superior to both any individual heuristic and to picking a heuristic at random .",
    "the results also provide some new insight on the heuristics themselves .",
    "this appears to be the first application of machine learning to problem formulation for computer algebra , although it follows recent application to theorem proving @xcite .",
    "we conclude the introduction with background theory on cad and machine learning .",
    "then in sections [ sec : methodology ] , [ sec : results ] and [ sec : future ] we describe our experiment , its results and how they may be extended in the future .",
    "finally in section [ sec : conclusion ] we give our conclusions and ideas for future work .",
    "let @xmath0 be quantifiers and @xmath1 be some quantifier free formula .",
    "then given @xmath2 _ quantifier elimination _",
    "( qe ) is the problem of producing a quantifier free formulae @xmath3 equivalent to @xmath4 . in the case",
    "@xmath5 this reduces to the _ decision problem _",
    ", is @xmath4 true ?",
    "tarski proved that qe was possible for semi - algebraic formulae ( polynomials and inequalities ) over @xmath6 @xcite .",
    "however , the complexity of tarski s method is non - elementary ( indescribable as a finite tower of exponentials ) and so cad was a major breakthrough when introduced , despite complexity doubly exponential in the number of variables .",
    "for some problems qe is possible through algorithms with better complexity ( see for example the survey by basu @xcite ) , but cad implementations remain the best general purpose approach .",
    "collins algorithm @xcite works in two stages .",
    "first , _ projection _ calculates sets of projection polynomials @xmath7 in variables @xmath8 .",
    "this is achieved by repeatedly applying a projection operator onto a set of polynomials , producing a set with one variable fewer .",
    "we start with the polynomials from @xmath1 and eliminate variables this way until we have the set of univariate polynomials @xmath9 .",
    "then in the _ lifting _ stage , decompositions of real space in increasing dimensions are formed according to the real roots of those polynomials .",
    "first , the real line is decomposed according to the roots of the polynomials in @xmath9 .",
    "then over each cell @xmath10 in that decomposition , the bivariate polynomials @xmath11 are taken at a sample point and a decomposition of @xmath12 is produced according to their roots . taking the union gives the decomposition of @xmath13 and we proceed this way to a decomposition of @xmath14 .",
    "the decompositions are cylindrical ( projections of any two cells onto their first @xmath15 coordinates are either identical or disjoint ) and each cell is a semi - algebraic set ( described by polynomial relations ) .",
    "collins original algorithm used a projection operator which guaranteed cads of @xmath14 on which the polynomials in @xmath1 had constant sign , and thus @xmath1 constant truth value , on each cell .",
    "hence only a single sample point from each cell needed to be tested and the equivalent quantifier free formula @xmath16 could be generated from the semi - algebraic sets defining the cells in the cad of @xmath17 for which @xmath4 is true .    since the publication of the original algorithm ,",
    "there have been numerous improvements , optimisations and extensions of cad ( with a summary of the first 20 years given by collins @xcite ) . of great importance",
    "is the improvement to the projection operator used .",
    "hong @xcite proved that a refinement of collins operator was sufficient and then mccallum @xcite presented a further refinement which could only be used for input that was _ well - oriented _ and was in turn improved by brown @xcite .",
    "further refinements are possible by removing the need for sign - invariance of polynomials while maintaining truth - invariance of a formula , with mccallum @xcite presenting an operator for use when an equational constraint is present ( an equation logically implied by a formula ) and bradford _ et al_. @xcite extending this to the case of multiple formulae .",
    "collins and hong @xcite described partial cad for qe , where lifting over a cell is aborted if there already exists sufficient information to determine the truth of @xmath1 on that cell .",
    "other recent cad developments of particular note include the use of symbolic - numeric techniques in the lifting stage @xcite and the alternative to projection and lifting offered by decompositions of complex space via regular chains technology @xcite .",
    "when using cad we have to assign an ordering to the variables ( the labels @xmath15 on the @xmath18 in the discussion above ) .",
    "this dictates the order in which the variables are eliminated during projection and thus the sub - spaces for which cads are produced en route to a cad of @xmath19 . for some applications this order is fixed but for others there may be a free or constrained choice . when using cad for qe we must project quantified variables before unquantified ones .",
    "further , the quantified variables should be projected in the order they occur , unless successive ones have the same quantifier in which case they may be swapped .",
    "the ordering can have a big effect on the output and performance of cad @xcite .",
    "machine learning @xcite deals with the design of programs that can learn rules from data .",
    "this is often a very attractive alternative to manually constructing them when the underlying functional relationship is very complex .",
    "machine learning techniques have been widely used in many fields , such as web searching@xcite , text categorization@xcite , robotics@xcite , expert systems@xcite and many others .",
    "various machine learning techniques have been developed .",
    "mcculloch and pitts@xcite created the first computational model for _ neural networks _ called _",
    "threshold logic_. following that , rosenblatt  @xcite proposed the _",
    "perceptron _ as an iterative algorithm for supervised classification of an input into one of several possible non - binary outputs .",
    "a later development was the _ decision tree _",
    "@xcite , which is a simple representation for classifying examples . the main idea here is to apply serial classifications which refine the output state . at the same time as the _ decision tree _ was being developed , the _ multi - layer perceptron _",
    "@xcite was explored .",
    "it is a modification of the standard linear perceptron and can distinguish data that are non - linearly separable .    in the last decade , the use of machine learning has spread rapidly following the invention of the _ support vector machine _ ( svm ) @xcite .",
    "this was a development of the perceptron approach and gives a powerful and robust method for both classification and regression .",
    "_ classification _ refers to the assignment of input examples into a given set of classes ( the output being the class labels ) . _",
    "regression _ refers to a supervised pattern analysis in which the output is real - valued .",
    "the svm technology can deal efficiently with high - dimensional data , and is flexible in modelling diverse sources of data .",
    "the standard svm classifier takes a set of input data and predicts one of two possible classes from the input .",
    "given a set of examples , each marked as belonging to one of two classes , an svm training algorithm builds a model that assigns new examples into one of the classes .",
    "the examples used to fit the model are called training examples .    an important concept in the svm theory",
    "is the use of a kernel function @xcite , which maps data into a high dimensional kernel - defined feature space and then separates samples in the transformed space .",
    "kernel functions enable operations in feature space without ever computing the coordinates of the data in that space .",
    "instead they simply compute the inner products between all pairs of data vectors .",
    "this operation is generally computationally cheaper than the explicit computation of the coordinates .",
    "the machine learning experiment described in this paper uses svm - light ( see joachims  @xcite ) which is an implementation of svms in c. the svm - light software consists of two programs : svm learn and svm classify .",
    "svm learn fits the model parameters based on the training data and user inputs ( such as the kernel function and the parameter values ) .",
    "svm classify uses the generated model to classify new samples .",
    "it calculates a hyperplane of the @xmath20-dimensional transformed feature space , which is an affine subspace of dimension @xmath21 dividing the space into two corresponding to the two distinct classes .",
    "svm classify outputs margin values which are a measure of how far the sample is from this separating hyperplane .",
    "hence the margins are a measure of the confidence in a correct prediction .",
    "a large margin represents high confidence in a correct prediction .",
    "the accuracy of the generated model is largely dependent on the selection of the kernel functions and parameter values .",
    "for the machine learning experiment we decided to focus on a single cad implementation , qepcad @xcite .",
    "we note that other cad implementations are available , as discussed further in section [ sec : future ] .",
    "qepcad is an interactive command line program written in c for performing * * q**uantifier * * e**limination with * * p**artial * cad*. it was chosen as it is a competitive implementation of both cad and qe that also allows the user some control and information during its execution .",
    "we used qepcad with its default settings which implement mccallum s projection operator @xcite and partial cad @xcite .",
    "it can also makes use of an equational constraint automatically ( via the projection operator @xcite ) when one is explicit in the formula , ( where _ explicit _ means the formula is a conjunction of the equational constraint with a sub - formula ) . in the experiment we used three existing heuristics for picking a cad variable ordering :    brown",
    ": : :    this heuristic chooses a variable ordering according to the following    criteria , starting with the first and breaking ties with successive    ones :    +    1 .",
    "eliminate a variable first if it has lower overall degree in the    input .    2 .",
    "eliminate a variable first if it has lower ( maximum ) total degree    of those terms in the input in which it occurs .",
    "eliminate a variable first if there is a smaller number of terms    in the input which contain the variable .",
    "+    it is labelled after brown who suggested it @xcite .",
    "sotd : : :    this heuristic constructs the full set of projection polynomials for    each permitted ordering and selects the ordering whose corresponding    set has the lowest sum of total degrees for each of the monomials in    each of the polynomials .",
    "it is labelled sotd for _ sum of total degree _    and was suggested by dolzmann , seidell and sturm @xcite , whose study    found it to be a good heuristic for both cad and qe by cad .",
    "ndrr : : :    this heuristic constructs the full set of projection polynomials for    each ordering and selects the ordering whose set has the lowest number    of distinct real roots of the univariate polynomials within .",
    "it is    labelled ndrr for _ number of distinct real roots _ and was suggested by    bradford _",
    "et al_. @xcite .",
    "ndrr was shown to assist with examples    where sotd failed .",
    "brown s heuristic has the advantage of being very cheap , since it acts only on the input and checks only simple properties .",
    "the ndrr heuristic is the most expensive ( requiring real root isolation ) , but is the only one to explicitly consider the real geometry of the problem ( rather than the geometry in complex space ) .",
    "all three heuristics may identify more than one variable ordering as a suitable choice . in this case",
    "we took the heuristic s choice to be the first of these after they had been ordered lexicographically .",
    "problems were taken from the nlsat dataset  @xcite , chosen over more traditional cad problem sets ( such as wilson _",
    "et al_. @xcite ) as these did not have sufficient numbers of problems for machine learning .",
    "7001 three - variable cad problems were extracted for our experiment .",
    "the number of variables was restricted for two reasons . first to make it feasible to test all possible variable orderings and second to avoid the possibility that qepcad will produce errors or warnings related to well - orientedness with the mccallum projection @xcite .",
    "two experiments were undertaken , applying machine learning to cad itself and to qe by cad .",
    "qe is clearly very important throughout engineering and the sciences , but increasingly cad has been applied outside of this context , as discussed in the introduction .",
    "we performed separate experiments since for quantified problems qepcad can use the partial cad techniques to stop the lifting process early if the outcome is already determined , while the full process is completed for unquantified ones and the two outputs can be quite different .",
    "the problems from the nlsat dataset are all fully existential ( satisfiability or sat problems ) .",
    "a second set of problems for the quantifier free experiment was obtained by simply removing all quantifiers .",
    "an example of the qepcad input for a sat problem is given in figure [ fig : qin ] with the corresponding input for the unquantified problem in figure [ fig : qfin ] .",
    "of course , for such quantified problems there are better alternatives to building a cad ( see for example the work of jovanovic and de  moura @xcite ) .",
    "however , our decision to use only sat problems was based on availability of data rather than it being a requirement of the technology , and so we focus on cad only here and discuss how we might generalise our data in section [ sec : future ] .",
    "for both experiments , the problems were randomly split into training sets ( 3545 problems in each ) , validation sets ( 1735 problems in each ) and test sets ( 1721 problems in each ) .    ....",
    "( x0,x1,x2 ) 0 ( ex0)(ex1)(ex2)[[((x0 x0 ) + ( ( x1 x1 ) + ( x2 x2 ) ) ) = 1 ] ] .",
    "go go go d - stat go finish ....    [ fig : qin ]    .... ( x0,x1,x2 ) 3 [ [ ( ( x0 x0 ) + ( ( x1 x1 ) + ( x2 x2 ) ) ) = 1 ] ] .",
    "go go d - proj - factors d - proj - polynomials go d - fpc - stat go ....    [ fig : qfin ]      since each problem has three - variables and all the quantifiers are the same , all six possible variable orderings are admissible . for each ordering",
    "we had qepcad build a cad and measured the number of cells .",
    "the best ordering was defined as the one resulting in the smallest cell count , ( and if more than one ordering gives the minimal both orderings are considered the best ) . the decision to focus on cell counts ( rather than",
    "say computation time ) was made so that our experiment could validate the use of machine learning to cad theory , rather than just the qepcad implementation .",
    "further , it is usually the case that cell counts and timings are strongly correlated .",
    "the heuristics ( brown , sotd and ndrr ) have been implemented in maple ( as part of the freely available ` projectioncad ` package @xcite ) and for each problem the orderings suggested by the heuristics were recorded and compared to the cell counts produced by qepcad .",
    "note that all three heuristics do not discriminate on the structure of any quantifiers . as discussed above ,",
    "some heuristics are more expensive than others . however ,",
    "since none of the costs were prohibitive for our data set they are not considered here .",
    "machine learning was applied to predict which of the three heuristics will give an _",
    "optimal _ variable ordering for a given problem , where _ optimal _ means the lowest cell count of the selected cads .",
    "note that in the quantified case qepcad can collapse stacks when sufficient truth values for the constituent cells have been discovered to determine a truth value for the base cell .",
    "hence , since our problems are all fully existential , the output for all quantified problems is always a single cell : true or false .",
    "therefore , in these cases it was not the number of cells in the output that was used but instead the number of cells constructed during the process ( hence the statistics commands in figures [ fig : qin ] and [ fig : qfin ] differ ) .      to apply machine learning , we need to identify features of the cad problems that might be relevant to the correct choice of the heuristics .",
    "a feature is an aspect or measure of the problem that may be expressed numerically .",
    "table 1 shows the 11 features that we identified , where @xmath22 are the three variable labels used in all our problems .",
    "the number of features is quite small , compared to other machine learning experiments .",
    "they were chosen as easily computable features of the problems which could affect the performances of the heuristics .",
    "other features were considered ( such as the maximum coefficient and the proportion of constraints that were equations ) but were not found to be useful .",
    "further investigation into feature selection may be a topic of our future work .",
    ".description of the features used .",
    "the proportion of a variable occurring in polynomials is the number of polynomials containing the variable divided by total number of polynomials .",
    "the proportion of a variable occurring in monomials is the number of terms containing the variable divided by total number of terms in polynomials . [",
    "cols=\"^,<\",options=\"header \" , ]     for the quantifier free problems there were 399 problems where every heuristic picked the optimal , 499 where two did and 823 where one did . hence for this problem set the chances of picking a successful heuristic at random is @xmath23 which compares with @xmath24 for machine learning .",
    "for the quantified problems the figures are @xmath25 and @xmath26 .",
    "hence machine learning performs significantly better than a random choice in both cases .",
    "further , if we were to use only the heuristic that performed the best on this data , the brown heuristic , then we would pick a successful ordering for approximately @xmath25 of the quantifier free problems and @xmath27 of the quantified problems .",
    "so we see that a machine learned choice is also superior to using any one heuristic .",
    "although a large data set of real world problems was used , we note that in some ways the data was quite uniform",
    ". a key area of future work is experimentation on a wider data set to see if these results , both the benefit of machine learning and the superiority of brown s heuristic , are verified more generally . an initial extension would be to relax the parameters used to select problems from the nlsat dataset , for example by allowing problems with more variables .",
    "one key restriction with this dataset is that all problems have one block of existential quantifiers .",
    "note that our restriction to this case followed the availability of data rather than any technical limitation of the machine learning .",
    "possible ways to generalise the data include randomly applying quantifiers to the the existing problems , or randomly generating whole problems",
    ". however , this would mean the problems no longer originate from real applications , and it has been noted in the past that random problems for cad can be unrepresentative .",
    "we do not suggest svm as the only suitable machine learning method for this experiment , but overall a svm with the rbf kernel worked well here .",
    "it would be interesting to see if other machine learning methods could offer similar or even better selections .",
    "further improvements may also come from more work on the feature selection .",
    "the features used here were all derived from the polynomials involved in the input .",
    "one possible extension would be to consider also the type of relations present and how they are connected logically ( likely to be particularly beneficial if problems with more variables or more varied quantifiers are allowed )",
    ".    a key extension for future work will be the testing of other heuristics .",
    "for example the greedy sotd heuristic @xcite which chooses an ordering one variable at a time based on the sotd of new projection polynomials or combined heuristics , ( where we narrow the selection with one and then breaking the tie with another ) .",
    "we also note that there are other questions of cad problem formulation besides variable ordering @xcite for which machine learning might be of benefit .",
    "finally , we note that there are other cad implementations .",
    "in addition to qepcad there is ` projectioncad ` @xcite , ` regularchains ` @xcite and ` synrac ` @xcite in maple , mathematica @xcite and ` redlog ` @xcite in reduce .",
    "each implementation has its own intricacies and often different underlying theory so it would be interesting to test if machine learning can assist with these as it does with qepcad .",
    "we have investigated the use of machine learning for making the choice of which heuristic to use when selecting a variable ordering for cad , and quantifier elimination by cad .",
    "the experimental results confirmed our thesis , drawn from personal experience , that no one heuristic is superior for all problems and the correct choice will depend on the problem .",
    "each of the three heuristics tested had a substantial set of problems for which they were superior to the others and so the problem was a suitable application for machine learning .",
    "using machine learning to select the best cad heuristic yielded better results than choosing one heuristic at random , or just using any of the individual heuristics in isolation , indicating there is a relation between the simple algebraic features and the best heuristic choice .",
    "this could lead to the development of a new individual heuristic in the future .",
    "the experiments involved testing heuristics on 1721 cad problems , certainly the largest such experiment that the authors are aware of . for comparison , the best known previous study on such heuristics @xcite tested with six examples .",
    "we observed that brown s heuristic is the most competitive for our example set , and this is despite it involving less computation than the others .",
    "this heuristic was presented during an issac tutorial in 2004 ( see brown @xcite ) , but does not seem to be formally published .",
    "it certainly deserves to be better known .",
    "finally , we note that cad is certainly not unique amongst computer algebra algorithms in requiring the user to make such a choice of problem formulation .",
    "more generally , computer algebra systems ( cass ) often have a choice of possible algorithms to use when solving a problem .",
    "since a single formulation or algorithm is rarely the best for the entire problem space , cass usually use _ meta - algorithms _ to make such choices , where decisions are based on some numerical parameters @xcite .",
    "these are often not as well documented as the base algorithms , and may be rather primitive . to the best of our knowledge , the present paper appears to be the first applying machine learning to problem formulation for computer algebra .",
    "the positive results should encourage investigation of similar applications in the field of symbolic computation .",
    "this work was supported by the epsrc grant : ep / j003247/1 and the china scholarship council ( csc ) .",
    "the authors thank the anonymous referees for useful comments which improved the paper .",
    "r.  bradford , j.  davenport , m.  england , and d.  wilson . optimising problem formulations for cylindrical algebraic decomposition . in _",
    "intelligent computer mathematics _ ( lncs 7961 ) , pages 1934 .",
    "springer berlin heidelberg , 2013 .",
    "j.  p. bridge . .",
    "university of cambridge computer laboratory technical report ucam - cl - tr-792 , 2010 .",
    "+ available from : \\texttt{http://www.cl.cam.ac.uk / techreports / ucam - cl - tr-792.pdf}[\\texttt\\{http://www.cl.cam.ac.uk / techreports / ucam - cl - tr-792.pdf } ] .",
    "g.  collins .",
    "quantifier elimination for real closed fields by cylindrical algebraic decomposition . in _ proc .",
    "2nd gi conference on automata theory and formal languages _ ,",
    "pages 134183 .",
    "springer - verlag , 1975 .",
    "g.  collins .",
    "quantifier elimination by cylindrical algebraic decomposition ",
    "20 years of progress . in _",
    "quantifier elimination and cylindrical algebraic decomposition _ ,",
    "texts & monographs in symbolic computation , pages 823 .",
    "springer - verlag , 1998 .",
    "m.  england .",
    "an implementation of cad in maple utilising problem formulation , equational constraints and truth - table invariance .",
    "university of bath department of computer science technical report 2013 - 04 , 2013 .",
    "available from : ` http://opus.bath.ac.uk/35636/ ` ,      i.  fotiou , p.  parrilo , and m.  morari .",
    "nonlinear parametric optimization using cylindrical algebraic decomposition . in _",
    "decision and control , 2005 european control conference .",
    "cdc - ecc 05 .",
    "_ , pages 37353740 , 2005 .",
    "s.  mccallum .",
    "an improved projection operation for cylindrical algebraic decomposition . in _",
    "quantifier elimination and cylindrical algebraic decomposition _ , texts & monographs in symbolic computation , pages 242268 .",
    "springer - verlag , 1998 .",
    "a.  tarski .",
    "a decision method for elementary algebra and geometry . in _",
    "quantifier elimination and cylindrical algebraic decomposition _ , texts and monographs in symbolic computation , pages 2484 .",
    "springer - verlag , 1998 ."
  ],
  "abstract_text": [
    "<S> cylindrical algebraic decomposition(cad ) is a key tool in computational algebraic geometry , particularly for quantifier elimination over real - closed fields . when using cad , there is often a choice for the ordering placed on the variables </S>",
    "<S> this can be important , with some problems infeasible with one variable ordering but easy with another . </S>",
    "<S> machine learning is the process of fitting a computer model to a complex function based on properties learned from measured data . in this paper </S>",
    "<S> we use machine learning ( specifically a support vector machine ) to select between heuristics for choosing a variable ordering , outperforming each of the separate heuristics . </S>"
  ]
}