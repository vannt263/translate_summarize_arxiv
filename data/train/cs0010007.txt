{
  "article_text": [
    "models of computation are essential for abstracting the complexity of real machines and enabling the design and analysis of algorithms .",
    "the widely - used ram model owes its longevity and usefulness to its simplicity and robustness .",
    "while it is far removed from the complexities of any physical computing device , it successfully predicts the relative performance of algorithms based on an abstract notion of operation counts .",
    "the ram model assumes a flat memory address space with unit - cost access to any memory location . with the increasing use of caches in modern machines",
    ", this assumption grows less justifiable . on modern computers ,",
    "the running time of a program is as much a function of operation count as of its cache reference pattern .",
    "a result of this growing divergence between model and reality is that operation count alone is not always a true predictor of the running time of a program , and manifests itself in anomalies such as a matrix multiplication algorithm demonstrating @xmath0 running time instead of the expected @xmath1 behavior predicted by the ram model  @xcite .",
    "such shortcomings of the ram model motivate us to seek an alternative model that more realistically models the presence of a memory hierarchy . in this paper",
    ", we address the issue of better and systematic utilization of caches starting from the algorithm design stage .",
    "a challenge in coming up with a good model is achieving a balance between abstraction and fidelity , so as not to make the model unwieldy for theoretical analysis or simplistic to the point of lack of predictiveness .",
    "memory hierarchy models used by computer architects to design caches have numerous parameters and suffer from the first shortcoming  @xcite .",
    "early algorithmic work in this area focussed on a two - layered memory model@xcite  a very large capacity memory with slow access time ( secondary memory ) and a limited size faster memory ( internal memory ) .",
    "all computation is performed on elements in the internal memory and there is no restriction on placement of elements in the internal memory ( fully associative ) .",
    "the focus of this paper is on the interaction between main memory and _ cache _ , which is the first level of memory hierarchy once the address is provided by the cpu .",
    "the structure of a single level hierarchy of cache memory is adequately characterized by the following three parameters : * * a**ssociativity , * * b**lock size , and * * c**apacity . capacity and block size",
    "are in units of the minimum memory access size ( usually one byte ) .",
    "a cache can hold a maximum of @xmath2 bytes .",
    "however , due to physical constraints , the cache is divided into _ cache frames _ of size @xmath3 that contain @xmath3 contiguous bytes of memory  called a _",
    "memory block_. the associativity @xmath4 specifies the number of different frames in which a memory block can reside . if a block can reside in any frame ( i.e. , @xmath5 ) , the cache is said to be _ fully associative _ ; if @xmath6 , the cache is said to be _ direct - mapped _ ; otherwise , the cache is _",
    "@xmath4-way set associative_.    for a given memory access , the hardware inspects the cache to determine if the corresponding memory element is resident in the cache .",
    "this is accomplished by using an indexing function to locate the appropriate set of cache frames that may contain the memory block .",
    "if the memory block is not resident , a cache miss is said to occur . from an architectural standpoint",
    ", cache misses can be classified into one of three classes  @xcite .    * a _ compulsory miss _ ( also called a _ cold miss _ ) is one that is caused by referencing a previously unreferenced memory block .",
    "eliminating a compulsory miss requires prefetching the data , either by an explicit prefetch operation or by placing more data items in a single memory block .",
    "* a reference that is not a compulsory miss but misses in a fully - associative cache with lru replacement is classified as a _",
    "capacity miss_. capacity misses are caused by referencing more memory blocks than can fit in the cache .",
    "restructuring the program to re - use blocks while they are in cache can reduce capacity misses . * a reference that is not a compulsory miss that hits in a fully - associative cache but misses in an @xmath4-way set - associative cache is classified as a _ conflict miss_. a conflict miss to block x indicates that block x has been referenced in the recent past , since it is contained in the fully - associative cache , but at least @xmath4 other memory blocks that map to the same cache set have been accessed since the last reference to block x. eliminating conflict misses requires transforming the program to change either the memory allocation and/or layout of the two arrays ( so that contemporaneous accesses do not compete for the same sets ) or the manner in which the arrays are accessed .",
    "conflict misses pose an additional challenge in designing efficient algorithms in the cache .",
    "this class of misses is not present in the i / o models , where the mapping between internal and external memory is fully associative .",
    "existing memory hierarchy models  @xcite do not model certain salient features of caches , notably the lack of full associativity in address mapping and the lack of explicit control over data movement and replacement . unfortunately , these small differences are malign in the effect .",
    "the _ conflict misses _ that they introduce make analysis of algorithms much more difficult  @xcite .",
    "carter and gatlin  @xcite conclude a recent paper saying    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ what is needed next is a study of `` messy details '' not modeled by umh ( particularly cache associativity ) that are important to the performance of the remaining steps of the fft algorithm .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in the first part of this paper , we develop a two - level memory hierarchy model to capture the interaction between cache and main memory .",
    "our model is a simple extension of the two - level i / o model that aggarwal and vitter  @xcite proposed for analyzing external memory algorithms .",
    "however , it captures three additional constraints of caches : lower miss penalties ; lack of full associativity in address mapping ; and lack of explicit program control over data movement .",
    "the work in this paper shows that the constraint imposed by limited associativity can be tackled quite elegantly , allowing us to extend the results of the i / o model to the cache model very efficiently .",
    "most modern architectures have a memory hierarchy consisting of multiple cache levels . in the second half of this paper",
    ", we extend the two - level cache model to a multi - level cache model .",
    "the remainder of this paper is organized as follows .",
    "surveys related work . defines our cache model and establishes an efficient emulation scheme between the i / o model and our cache model . as direct corollaries of the emulation scheme",
    ", we obtain cache - optimal algorithms for several fundamental problems such as sorting , fft , and an important class of permutations .",
    "illustrates the importance of the emulation scheme by demonstrating that a direct ( , bypassing the emulation ) implementation of an i / o - optimal sorting algorithm ( multiway mergesort ) is provably inferior , even in the average case , in the cache model .",
    "describes a natural extension of our model to multiple levels of caches .",
    "we present an algorithm for transposing a matrix in the multi - level cache model that attains optimal performance in the presence of any number of levels of cache memory .",
    "our algorithm is not cache - oblivious , , we do make explicit use of the sizes of the cache at various levels .",
    "next , we show that with some simple modifications , the funnel - sort algorithm of frigo et al . attains optimal performance in a single level ( direct mapped ) cache in an oblivious sense , , without prior knowledge of memory parameters .",
    "finally , presents conclusions , possible refinements to the model , and directions for future work .",
    "the i / o model assumes that most of the data resides on disk and has to be transferred to main memory to do any processing . because of the tremendous difference in speeds",
    ", it ignores the cost of internal processing and counts only the number of i / os .",
    "floyd  @xcite originally defined a formal model and proved tight bounds on the number of i / os required to transpose a matrix using two pages of internal memory .",
    "hong and kung  @xcite extended this model and studied the i / o complexity of fft when the internal memory size is bounded by @xmath7 .",
    "aggarwal and vitter  @xcite further refined the model by incorporating an additional parameter @xmath3 , the number of ( contiguous ) elements transferred in a single i / o operation .",
    "they gave upper and lower bounds on the number of i / os for several fundamental problems including sorting , selection , matrix transposition , and fft .",
    "following their work , researchers have designed i / o - optimal algorithms for fundamental problems in graph theory  @xcite and computational geometry  @xcite .",
    "researchers have also modeled multiple levels of memory hierarchy .",
    "aggarwal  @xcite defined the _",
    "hierarchical memory model _ ( hmm ) that assigns a function @xmath8 to accessing location @xmath9 in the memory , where @xmath10 is a monotonically increasing function .",
    "this can be regarded as a continuous analog of the multi - level hierarchy .",
    "aggarwal  @xcite added the capability of block transfer to the hmm , which enabled them to obtain faster algorithms .",
    "alpern  @xcite described the _ uniform memory hierarchy _",
    "( umh ) model , where the access costs differ in discrete steps .",
    "very recently , frigo  @xcite presented an alternate strategy of algorithm design on these models which has the added advantage that explicit values of parameters related to different levels of the memory hierarchy are not required .",
    "bilardi and peserico @xcite investigate further the complexity of designing algorithms without the knowledge architectural parameters .",
    "other attempts were directed towards extracting better performance by parallel memory hierarchies  @xcite , where several blocks could be transferred simultaneously .",
    "ladner  @xcite describe a stochastic model for performance analysis in cache .",
    "our work is different in nature , as we follow a more traditional worst - case analysis .",
    "our analysis of sorting in provides a better theoretical basis for some of the experimental work of lamarca and ladner  @xcite .    to the best of our knowledge , the only other paper that addresses the problem of limited associativity in cache is recent work of mehlhorn and sanders@xcite .",
    "they show that for a class of algorithms based on merging multiple sequences , the i / o algorithms can be made nearly optimal by use of a simple randomized shift technique .",
    "the emulation theorem in of this paper not only provides a deterministic solution for the same class of algorithms , but also works for a very general situation .",
    "the results in @xcite are nevertheless interesting from the perspective of implementation .",
    "the ( two - level ) i / o model of aggarwal and vitter  @xcite captures the interaction between a slow ( secondary ) memory of infinite capacity and a fast ( primary ) memory of limited capacity .",
    "it is characterized by two parameters : @xmath7 , the capacity of the fast memory ; and @xmath3 , the size of data transfers between slow and fast memories .",
    "such data movement operations are called _ i / o operations _ or _",
    "block transfers_. the use of the model is meaningful when the problem size @xmath11 .",
    "the i / o model contains the following further assumptions .    1 .",
    "a datum can be used in a computation iff it is present in fast memory .",
    "all data initially resides in slow memory .",
    "data can be transferred between slow and fast memory ( in either direction ) by i / o operations .",
    "since the latency for accessing slow memory is very high , the average cost of transfer per element can be reduced by transferring a block of @xmath3 elements at little additional cost",
    ". this may not be as useful as it may seem at first sight , since these @xmath3 elements are not arbitrary , but are contiguous in memory .",
    "the onus is on the programmer to use all the elements , as traditional ram algorithms are not designed for such restricted memory access patterns .",
    "we denote the map from a memory address to its block address by .",
    "the internal memory can hold at least three blocks , i.e. , @xmath12 .",
    "the computation cost is ignored in comparison to the cost of an i / o operation .",
    "this is justified by the high access latency of slow memory .",
    "4 .   a block of data from slow memory can be placed in any block of fast memory .",
    "i / o operations are explicit in the algorithm .",
    "the goal of algorithm design in this model is to minimize the number of i / o operations .",
    "we adopt much of the framework of the i / o model in developing a cache model to capture the interactions between cache and main memory . in this case , the cache is the fast memory , while main memory is the slow memory .",
    "assumptions 1 and 2 of the i / o model continue to hold in our cache model .",
    "however , assumptions 35 are no longer valid and need to be replaced as follows .",
    "* the difference between the access times of slow and fast memory is considerably smaller than in the i / o model , namely a factor of 5100 rather than factor of 10000 .",
    "we will use @xmath13 to denoted the _ normalized _ cache latency .",
    "this cost function assigns a cost of 1 for accessing an element in cache and @xmath13 for accessing an element in the main memory .",
    "this way , we also account for the computation in cache .",
    "* main memory blocks are mapped into cache sets using a _ fixed _ and pre - determined mapping function that is implemented in hardware .",
    "typically , this is a modulo mapping based on the low - order address bits .",
    "however , the results of this section will hold as long as there is a _ fixed _",
    "address mapping function that distributes the main memory evenly in the cache .",
    "we denote this mapping from main memory blocks to cache sets by .",
    "we will occasionally slightly abuse this notation and apply   directly to a memory address @xmath9 rather than to @xmath14 . *",
    "the cache is not visible to the programmer ( not even at the assembly level ) .",
    "when a program issues a reference to a memory location @xmath9 , an _ image _ ( copy ) of the main memory block @xmath15 is brought into the cache set @xmath16 if it is not already present there .",
    "the block @xmath17 continues to reside in cache until it is evicted by another block @xmath18 that is mapped to the same cache set ( , @xmath19 ) . in other words , a cache set",
    "@xmath20 contains the latest memory block referenced that is mapped to this set .",
    "to summarize , we use the notation @xmath21 to denote our three - parameter cache model , and the notation @xmath22 to denote the i / o model with parameters @xmath7 and @xmath3 .",
    "we will use @xmath23 and @xmath24 to denote @xmath25 and @xmath26 respectively .",
    "the assumptions of our cache model parallel those of the i / o model , except as noted above .",
    "the goal of algorithm design in the cache model is to minimize _ running time _ , defined as the number of cache accesses plus @xmath13 times the number of main memory accesses .",
    "the differences between the two models listed above would appear to frustrate any efforts to naively map an i / o algorithm to the cache model , given that we neither have the control nor the flexibility of the i / o model .",
    "our main result in this section establishes a connection between the i / o model and the cache model using a very simple emulation scheme .",
    "an algorithm @xmath4 in @xmath22 using @xmath27 block transfers and @xmath28 processing time can be converted to an equivalent algorithm @xmath29 in @xmath30 that runs in @xmath31 steps .",
    "the memory requirement of @xmath29 is an additional @xmath32 blocks beyond that of @xmath4 .",
    "[ u_bnd ]    note that @xmath28 is usually not accounted for in the i / o model , but we will keep track of the internal memory computation done in @xmath4 in our emulation . the idea behind the emulation is as follows .",
    "we will mimic the behavior of the i / o algorithm @xmath4 in the cache model , using an array of @xmath24 blocks to play the role of the fast memory .",
    "we will view the main memory in the cache model as an array of @xmath3-element blocks .",
    "although is also part of the memory , we are using different notations to make their roles explicit in this proof .",
    "likewise , we will view the cache as an array of sets and denote the @xmath33th set by @xmath34 $ ] . as discussed above",
    ", we do not have explicit control on the contents of the cache locations .",
    "however , we can control the memory access pattern through a level of indirection so as to maintain a 1 - 1 correspondence between and the cache .",
    "wlog , we assume that  maps block @xmath33 of to cache set @xmath34 $ ] for @xmath35 $ ] .",
    "we divide the i / o algorithm into rounds , where in each round , the i / o algorithm @xmath4 transfers a block between the slow memory and the fast memory and ( possibly ) does some computations . the cache algorithm @xmath29 transfers the same blocks between and and then does the identical computations in .",
    "figure [ emproc ] formally describes the procedure .",
    "note that the @xmath3 elements must be explicitly copied in the cache model .",
    "_ i / o algorithm @xmath4 _ & _ cache emulation @xmath29 _ + & + & +    it must be obvious that the final outcome of algorithm @xmath29 is the same as algorithm @xmath4 .",
    "the more interesting issue is the cost of the emulation .",
    "a block of size @xmath3 is transferred into cache if its image does not exist in the cache at the time of reference .",
    "the invariant that we try to maintain at the end of each round is that there is a 1 - 1 correspondence between and @xmath2 .",
    "this will ensure that all the @xmath28 operations are done within the cache at minimal cost .",
    "assume that we have maintained the above invariant at the end of round @xmath36 .",
    "in round @xmath37 , we transfer block @xmath38 $ ] into @xmath39 $ ] . accessing the memory block",
    "@xmath38 $ ] will displace the existing block in cache set @xmath40 $ ] , where @xmath41 . from the invariant ,",
    "we know that the block displaced from @xmath40 $ ] is @xmath42 $ ] , which must be restored to cache to restore the invariant .",
    "we can bring it back by a single memory reference and charge this to the round @xmath37 itself , which is @xmath13 .",
    "( actually it will be brought back during the subsequent reference , so the previous step is only to simplify the accounting . )",
    "the cost of copying @xmath38 $ ] to @xmath39 $ ] is @xmath43 assuming that @xmath38 $ ] and @xmath44 $ ] are not mapped to the same cache set ( @xmath45 ) .",
    "otherwise it will cause alternate cache misses ( _ thrashing _ ) of the blocks @xmath38 $ ] and @xmath44 $ ] leading to @xmath46 steps for copying .",
    "this can be prevented by transferring through an intermediate memory block @xmath47 $ ] such that @xmath48 .",
    "having two such intermediate buffers that map to distinct cache sets would suffice in all cases .",
    "so , we first transfer @xmath38 $ ] to @xmath47 $ ] followed by @xmath47 $ ] to @xmath49 $ ] .",
    "the first copying has cost @xmath50 since both blocks must be fetched from main memory .",
    "the second transfer is between blocks , one of which is present in the cache , so it has cost @xmath51 . to this",
    "we must also add cost @xmath13 for restoring the block of that was mapped to the same cache set as @xmath47 $ ] .",
    "so , the total cost of the _ safe _ method is @xmath52 .",
    "the internal processing remains identical .",
    "if @xmath53 denotes the internal processing cost of step @xmath37 , the total cost of the emulation is at most @xmath54 .    *",
    "a possible alternative to using intermediate memory - resident buffers to avoid thrashing is to use registers , since register access is much faster .",
    "in particular , if we have @xmath3 registers , then we can save two extra memory accesses , bringing down the emulation cost to @xmath55 .",
    "* we can make the emulation somewhat simpler by using a randomized mapping scheme .",
    "that is , if we choose the starting location of array randomly , then the probability that @xmath38 $ ] and @xmath39 $ ] have the same image is @xmath56 .",
    "so the expected emulation cost is @xmath57 without using any intermediate copying .",
    "* the basic idea of copying data into contiguous memory locations to reduce interference misses has been exploited before in some specific contexts like matrix multiplication  @xcite and bit - reversal permutation  @xcite .",
    "unifies these previous results within a common framework .",
    "the term @xmath58 is subsumed by @xmath59 if computation is done on at least a constant fraction of the elements in the block transferred by the i / o algorithm .",
    "this is usually the case for efficient i / o algorithms .",
    "we will call such i / o algorithms _ block - efficient_.    a block - efficient i / o algorithm for @xmath22 that uses @xmath27 block transfers and @xmath28 processing can be emulated in @xmath30 in @xmath60 steps .",
    "[ goodemu ]    the algorithms for sorting , fft , matrix transposition , and matrix multiplication described in aggarwal and vitter  @xcite are block - efficient .",
    "the trend in modern memory architectures is to allow limited flexibility in the address mapping between memory blocks and cache sets .",
    "the @xmath61-way set - associative cache has the property that a memory block can reside in any ( one ) of @xmath61 cache frames .",
    "thus , @xmath62 corresponds to the direct - mapped cache we have considered so far , while @xmath63 corresponds to a fully associative cache .",
    "values of @xmath61 for data caches are generally small , usually in the range 14 .",
    "if all the @xmath61 sets are occupied , a replacement policy like lru is used ( by the hardware ) to find an assignment for the referenced block .",
    "the emulation technique of the previous section would extend to this scenario easily if we had explicit control on the replacement .",
    "this not being the case , we shall tackle it indirectly by making use of an useful property of lru that frigo  @xcite exploited in the context of designing cache - oblivious algorithms for a fully associative cache .    for any sequence @xmath64 , @xmath65 ,",
    "the number of misses incurred by lru with cache size @xmath66 is no more than @xmath67 , where @xmath68 is the minimimum number of misses by an optimal replacement strategy with cache size @xmath69 .",
    "[ st ]    we use this lemma in the following way .",
    "we run the emulation technique for only half the cache size , , we choose the buffer to be of size @xmath70 , such that for every @xmath61 cache lines in a set , we have only @xmath71 buffer blocks . from lemma [ st ] , we know that the number of misses in each each cache set is no more than twice the optimal , which is in turn bounded by the number of misses incurred by the i / o algorithm .    an algorithm @xmath4 in @xmath72 using @xmath27 block transfers and @xmath28 processing time can be converted to an equivalent algorithm @xmath29 in the @xmath61-way set - associative cache model with parameters @xmath73 that runs in @xmath31 steps .",
    "the memory requirement of @xmath29 is an additional @xmath74 blocks beyond that of @xmath4 .",
    "aggarwal and vitter  @xcite prove the following lower bound for sorting and fft in the i / o model .",
    "the average - case and the worst - case number of i / o s required for sorting @xmath75 records and for computing the @xmath75-input fft graph in @xmath22 is @xmath76 .",
    "[ srtlb ]    the lower bound for sorting in @xmath30 is @xmath77 .",
    "[ lbnd_srt ]    any lower bound in the number of block transfers in @xmath22 carries over to @xmath30 .",
    "since the lower bound is the maximum of the lower bound on number of comparisons and the bound in lemma [ srtlb ] , the theorem follows by dividing the sum of the two terms by 2 .    in @xmath30",
    ", @xmath75 numbers can be sorted in @xmath78 steps and this is optimal .    the @xmath26-way mergesort algorithm described in aggarwal and vitter @xcite has an i / o complexity of @xmath79 .",
    "the processing time involves maintaining a heap of size @xmath26 and @xmath80 per output element . for @xmath75 elements ,",
    "the number of phases is @xmath81 , so the total processing time is @xmath82 . from corollary",
    "[ goodemu ] , and remark  [ remark : blkeff ] , the cost of this algorithm in the cache model is @xmath83 .",
    "optimality follows from theorem [ lbnd_srt ] .",
    "the @xmath26-way distribution sort ( multiway quicksort ) also has the same upper bound .",
    "we can prove a similar result for fft computations .",
    "the fft of @xmath75 numbers can be computed in @xmath84 in @xmath30 .",
    "the fftw algorithm @xcite is optimal only for @xmath85 .",
    "barve @xcite has independently obtained a similar result .",
    "the class of bit matrix multiply complement ( bmmc ) permutations include many important permutations like matrix transposition and bit reversal . combining the work of cormen  @xcite with our emulation scheme , we obtain the following result .    the class of bmmc permutations for @xmath75 elements can be achieved in @xmath86 steps in @xmath87 .",
    "[ bmmc ]    many known geometric  @xcite and graph algorithms  @xcite in the i / o model , such as convex hull and graph connectivity , can be transformed optimally into the cache model .",
    "in this section , we analyze the average - case performance of @xmath61-way mergesort in the cache model . of the three classes of misses described in",
    ", we note that compulsory misses are unavoidable and that capacity misses are minimized while designing algorithms for the i / o model .",
    "we are therefore interested in bounding the number of conflict misses for a straightforward implementation of the i / o - optimal @xmath61-way mergesort algorithm .",
    "it is easy to construct a worst - case input permutation where there will be a conflict miss for every input element ( a cyclic distribution suffices ) , so the average case is more interesting .",
    "we assume that @xmath64 cache sets are available for the leading blocks of the @xmath61 runs @xmath88 .",
    "in other words , we ignore the misses caused by heap operations ( or equivalently ensure that the heap area in the cache does not overlap with the runs ) .",
    "we create a random instance of the input as follows .",
    "consider the sequence @xmath89 , and distribute the elements of this sequence to runs by traversing the sequence in increasing order and assigning element @xmath33 to run @xmath90 with probability @xmath91 . from the nature of our construction",
    ", each run @xmath92 is sorted .",
    "we denote @xmath93-th element of @xmath92 as @xmath94 .",
    "the expected number of elements in any run @xmath92 is @xmath95 .    during the @xmath61-way merge ,",
    "the leading blocks are critical in the sense that the heap is built on the _ leading element _ of every sequence @xmath92 .",
    "the leading element of a sequence is the smallest element that has not been added to the merged ( output ) sequence .",
    "the _ leading block _ is the cache line containing the leading element .",
    "let @xmath96 denote the leading block of run @xmath92 .",
    "_ conflict _ can occur when the leading blocks of different sequences are mapped to the same cache set .",
    "in particular , a _ conflict miss",
    "_ occurs for element @xmath97 when there is at least one element @xmath98 , for some @xmath99 , such that @xmath100 and @xmath101 .",
    "( we do not count conflict misses for the first element in the leading block , , @xmath94 and @xmath97 must belong to the same block , but we will not be very strict about this in our calculations . )    let @xmath102 denote the probability of conflict for element @xmath103 $ ] . using indicator random variables @xmath104 to count the conflict miss for element @xmath33 , the total number of conflict misses @xmath105 .",
    "it follows that the expected number of conflict misses @xmath106 = \\sum_i e [ x_i ] = \\sum_i p_i$ ] . in the remaining section",
    "we will try to estimate a lower bound on @xmath102 for @xmath33 large enough to validate the following assumption .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * a1 * the cache sets of the leading blocks , @xmath107 , are randomly distributed in cache sets @xmath108 independent of the other sorted runs .",
    "moreover , the exact position of the leading element within the leading block is also uniformly distributed in positions @xmath109 . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    a recent variation of the mergesort algorithm ( see  @xcite ) actually satisfies * a1 * by its very nature .",
    "so , the present analysis is directly applicable to its average - case performance in cache .",
    "a similar observation was made independently by sanders  @xcite who obtained upper - bounds for mergesort for a set associative cache .    from our previous discussion and the definition of a conflict miss",
    ", we would like to compute the probability of the following event .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * e1 * for some @xmath110 , for all elements @xmath9 , such that @xmath100 , @xmath111 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in other words , none of the leading blocks of the sorted sequences @xmath90 , @xmath112 , conflicts with @xmath96 .",
    "the probability of the complement of this event ( , @xmath113 $ ] ) is the probability that we want to estimate .",
    "we will compute an upper bound on @xmath114 $ ] , under the assumption a1 , thus deriving a lower bound on @xmath115 $ ] .    for @xmath116 , @xmath117 < 1 - \\delta$ ] , where @xmath118 and @xmath119 are positive constants ( dependent only on @xmath64 and @xmath61 but not on @xmath23 or @xmath3 ) .",
    "[ weakbnd ]    see .",
    "thus we can state the main result of this section as follows .",
    "the expected number of conflict misses in a random input for doing a @xmath61-way merge in an @xmath64-set direct - mapped cache , where @xmath61 is @xmath120 , is @xmath121 , where @xmath75 is the total number of elements in all the @xmath61 sequences .",
    "therefore the ( ordinary i / o - optimal ) @xmath26-way mergesort in an @xmath26-set cache will exhibit @xmath122 cache misses which is asymptotically larger than the optimal value of @xmath123 .",
    "the probability of conflict misses is @xmath124 when @xmath61 is @xmath120 .",
    "therefore the expected total number of conflict misses is @xmath125 for @xmath75 elements .",
    "the i / o - optimal mergesort uses @xmath26-way merging at each of the @xmath126 levels , hence the second part of the theorem follows .",
    "intuitively , by choosing @xmath127 , we can minimize the probability of conflict misses resulting in an increased number of merge phases ( and hence running time ) .",
    "this underlines the critical role of conflict misses _ vis - a - vis _ capacity misses that forces us to use only a small fraction of the available cache .",
    "recently , sanders  @xcite has shown that by choosing @xmath61 to be @xmath128 in an @xmath129-way set associative cache with a modified version of mergesort of @xcite , the expected number of conflict misses per phase can be bounded by @xmath130 . in comparision , the use of the emulation theorem guarantees minimal worst - case conflict misses while making good use of cache .",
    "most modern architectures have a memory hierarchy consisting of multiple levels of cache .",
    "consider two cache levels @xmath131 and @xmath132 preceding main memory , with @xmath131 being faster and smaller .",
    "the operation of the memory hierarchy in this case is as follows .",
    "the memory location being referenced is first looked up in @xmath131 . if it is not present in @xmath131 , then it is searched for in @xmath132 ( these can be overlapped with appropriate hardware support ) .",
    "if the item is not present in @xmath131 but it is in @xmath132 , then it is brought into @xmath131 . in case",
    "that it is not in @xmath132 , then a cache line is brought in from main memory into @xmath132 and into @xmath131 .",
    "the size of cache line brought into @xmath132 ( denoted by @xmath133 ) is usually no smaller than the one brought into @xmath131 ( denoted by @xmath134 ) .",
    "the expectation is that the more frequently used items will remain in the faster cache .",
    "the multi - level cache model is an extension to multiple cache levels of the previously introduced cache model .",
    "let @xmath135 denote the @xmath33-th level of cache memory .",
    "the parameters involved here are the problem size _",
    "n _ , the size of @xmath135 which is denoted by _",
    "@xmath136 _ , the frame size ( unit of allocation ) of @xmath135 denoted by @xmath137 and the latency factor @xmath138 .",
    "if a data item is present in the @xmath135 , then it is present in @xmath139 for all @xmath140 ( sometimes referred to as the * inclusion property * ) .",
    "if it is not present in @xmath135 , then the cost for a miss is @xmath138 plus the cost of fetching it from @xmath141 ( if it is present in @xmath141 , then this cost is zero ) . for convenience ,",
    "the latency factor @xmath138 is the ratio of time taken on a miss from the @xmath33-th level to the amount of time taken for a unit operation .",
    "figure [ 2lcache ] shows the memory mapping for a two - level cache architecture .",
    "the shaded part of main memory is of size @xmath134 and therefore occupies only a part of a line of the @xmath132 cache which is of size @xmath133 .",
    "there is a natural generalization of the memory mapping to multiple levels of cache .",
    "we make the following assumptions in this section , which are consistent with existing architectures .    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ a1 . for all @xmath33 , @xmath137 , @xmath142",
    "are powers of 2",
    ". + a2 . @xmath143 and the number of cache lines @xmath144 .",
    "+ a3 . @xmath145 and @xmath146 ( i.e. @xmath147 ) where @xmath148 is the largest and slowest cache .",
    "this implies that @xmath149 this will be useful for the analysis of the algorithms and are sometimes termed as _ tall cache _ in reference to the aspect ratio . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _      in this section , we provide an approach for transposing a matrix in the multi - level cache model .",
    "the trivial lower bound for matrix transposition of an @xmath150 matrix in the multi - level cache hierarchy is clearly the time to scan @xmath151 elements , namely , @xmath152 where + @xmath137 is the number of elements in one cache line in @xmath135 cache ; @xmath142 is the number of cache lines in @xmath135 cache , which is @xmath153 ; and @xmath138 is the latency for @xmath135 cache .",
    "our algorithm uses a more general form of the emulation theorem to get the submatrices to fit into cache in a regular fashion .",
    "the work in this section shows that it is possible to handle the constraints imposed by limited associativity even in a multi - level cache model .",
    "we subdivide the matrix into @xmath154 submatrices .",
    "thus we get @xmath155 submatrices from an @xmath156 submatrix .",
    "@xmath157    note that the submatrices in the last row and column need not be square as one side may have @xmath158 rows or columns .",
    "let @xmath159 then    @xmath160    for simplicity , we describe the algorithm as transposing a square matrix @xmath4 in another matrix @xmath3 , i.e. @xmath161 . the main procedure is * rec_trans*(@xmath162 ) , where @xmath4 is transposed into @xmath3 by dividing @xmath4 and @xmath3 into @xmath163 submatrices and then recursively transposing the sub - matrices .",
    "let @xmath164 ( @xmath165 ) denote the submatrices for @xmath166",
    ". then @xmath161 can be computed as * rec_trans * ( @xmath167 ) for all @xmath110 and some appropriate @xmath168 which depends on @xmath169 and @xmath170 . in general ,",
    "if @xmath171 denote the values of @xmath168 at the @xmath172 level of recursion , then @xmath173 .",
    "if the submatrices are @xmath174 ( base case ) , then perform the transpose exchange of the symmetric submatrices directly .",
    "we perform matrix transpose as follows , which is similar to the familiar recursive transpose algorithm .",
    "+ 1 . subdivide the matrix as shown into @xmath175 submatrices .",
    "move the symmetric submatrices to contiguous memory locations .",
    "* rec_trans * ( @xmath176 ) .",
    "write back the @xmath175 submatrices to original locations .    in the following subsections we analyze the data movement of this algorithm to bound the number of cache misses at various levels .",
    "to move a submatrix we will move it cache line by cache line . by choice of size of submatrices ( @xmath175 )",
    "each row will be an array of size @xmath169 , but the rows themselves may be far apart .    if two memory blocks @xmath9 and @xmath177 of size @xmath169 are aligned in @xmath148-cache map to the same cache set in @xmath135-cache for some @xmath178 , then @xmath9 and @xmath177 map to the same set in each @xmath139-cache for all @xmath179 .    if @xmath9 and @xmath177 map to the same cache set in @xmath135 cache then their @xmath33-th level memory block numbers ( to be denoted by @xmath180 and @xmath181 ) differ by a multiple of @xmath142",
    ". let @xmath182 .",
    "since @xmath183 ( both are powers of two ) , @xmath184 where @xmath185 .",
    "let @xmath186 be the _ corresponding _ sub - blocks of @xmath9 and @xmath177 at the @xmath93-th level .",
    "then their block numbers @xmath187 differ by @xmath188 , i.e. , a multiple of @xmath189 as @xmath190 .",
    "note that blocks are aligned across different levels of cache .",
    "therefore @xmath9 and @xmath177 also collide in @xmath139 .",
    "if two blocks of size @xmath169 that are aligned in @xmath148-cache do not conflict in level @xmath33 they do not conflict in any level @xmath93 for all @xmath191 .",
    "there is an algorithm which moves a set of blocks of size @xmath169 ( where there are @xmath61 levels of cache with block size @xmath137 for each @xmath192 ) into a contiguous area in main memory in @xmath193 where n is the total data moved and @xmath138 is the cost of a cache miss for the @xmath194 level of cache .",
    "[ blkmove ]    let the set of blocks of size @xmath169 be @xmath28 ( we are assuming that the blocks are aligned ) . let the target block in the contiguous area for each block @xmath195 be in the corresponding set @xmath196 where each block @xmath197 is also aligned with a cache line in @xmath148 cache .",
    "let block @xmath129 map to @xmath198 , @xmath199 where @xmath198 denote the set of cache lines in the @xmath200-cache .",
    "( since @xmath129 is of size @xmath169 , it will occupy several blocks in lower levels of cache . )",
    "let the @xmath194 block map to set @xmath201 of the @xmath148 cache .",
    "let the target block @xmath93 map to set @xmath202 . in the worst case , @xmath202 is equal to @xmath201",
    ". thus in this case the line @xmath201 has to be moved to a temporary block say @xmath9 ( mapped to @xmath203 ) and then moved back to @xmath202 .",
    "we choose @xmath9 such that @xmath204 and @xmath205 do not conflict and also @xmath204 and @xmath206 do not conflict .",
    "such a choice of @xmath9 is always possible because our temporary storage area @xmath207 of size @xmath208 has at least @xmath209 lines of @xmath148-cache ( @xmath33 and @xmath93 will take up two blocks of @xmath148-cache , thus leaving at least one block free to be used as temporary storage ) .",
    "_ this is why we have the assumption that @xmath146_. that is , by dividing the @xmath131-cache into @xmath210 zones , there is always a zone free for @xmath9 .    for convenience of analysis",
    ", we maintain the invariant that _",
    "@xmath207 is always in @xmath148-cache_. by application of the previous corollary on our choice of @xmath9 ( such that @xmath211 ) we also have @xmath212 for all @xmath213 .",
    "thus we can move @xmath33 to @xmath9 and @xmath9 to @xmath93 without any conflict misses .",
    "the number of cache misses involved is three for each level  one for getting the @xmath194 block , one for writing the @xmath214 block , and one to maintain the invariant since we have to touch the line displaced by @xmath33 .",
    "thus we get a factor of @xmath215 .",
    "thus the cost of this process is @xmath216 where @xmath75 is the amount of data moved .    for blocks @xmath28",
    "that are not aligned in @xmath148 cache , the constant would increase to 4 since we would need to bring up to 2 cache lines for each @xmath217 .",
    "the rest of the proof would remain the same .",
    "a @xmath175 submatrix can be moved into contiguous locations in the memory in @xmath218 time in a computer that has @xmath61 levels of ( direct - mapped ) cache .",
    "[ matcopy ]    this follows from the preceding discussion .",
    "we allocate memory say @xmath2 of size @xmath175 for placing the submatrix and memory , say , @xmath207 of size @xmath208 for temporary storage and keep both these areas distinct .",
    "if we have set associativity ( @xmath219 ) in all levels of cache then we do not need an intermediate buffer @xmath9 as line @xmath33 and @xmath93 can both reside in cache simultaneously and movement from one to the other will not cause thrashing . thus the constant will come down to two . since at any point in time we will only be dealing with two cache lines and will not need the lines @xmath33 or @xmath93 once we have read or written to them the replacement policy of the cache does not affect our algorithm .",
    "if the capacity of the register file is greater than the size of the cache line ( @xmath169 ) of the outermost cache level ( @xmath148 ) then we can move data without worrying about collision by copying from line @xmath33 to registers and then from registers to line @xmath93 .",
    "thus even in this case the constant will come down to two .",
    "once we have the submatrices in contiguous locations we perform the transpose as follows . for each of the submatrices we divide the @xmath220 submatrix ( say @xmath221 ) in level @xmath222 ( for @xmath223 ) further into @xmath224 size submatrices as before .",
    "each @xmath225 size subsubmatrix fits into @xmath226 cache completely ( since @xmath227 from equation ( [ bandsize ] ) ) .",
    "let @xmath228 .",
    "thus we have the submatrices as @xmath229    so we perform matrix transpose of each @xmath94 in place without incurring any misses as it resides completely inside the cache .",
    "once we have transposed each @xmath94 we exchange @xmath94 with @xmath230 .",
    "we will show that @xmath94 and @xmath230 can not conflict in @xmath226-cache for @xmath231 .    since @xmath94 and @xmath230 lie in different parts of the @xmath222-cache lines , they will map to different cache sets in the @xmath226-cache .",
    "the rows of @xmath94 and @xmath230 correspond to @xmath232 and @xmath233 where @xmath234 and @xmath235 if these conflict then @xmath236 since @xmath237 and @xmath238 and @xmath239 ( all powers of two ) @xmath240 therefore @xmath241 divides @xmath226 ( because @xmath242 ) .",
    "hence @xmath243 since @xmath244 the above implies @xmath245    note that @xmath246 s do not have to be exchanged .",
    "thus , we have shown that a @xmath220 matrix can be divided into @xmath224 which completely fits into @xmath226-cache . moreover , the symmetric sub - matrices do not interfere with each other .",
    "the same argument can be extended to any @xmath247 submatrix for @xmath248 . applying this recursively we end up dividing the @xmath175 size matrix in @xmath148-cache to @xmath174 sized submatrices in @xmath131-cache , which can then be transposed and exchanged easily . from the preceding discussion",
    ", the corresponding submatrices do not interfere in any level of the cache .",
    "( note that even though we keep subdividing the matrix at every cache level recursively and claim that we then have the submatrices in cache and can take the transpose and exchange them , the actual movement , i.e. , transpose and exchange happens only at the @xmath131-cache level , where the submatrices are of size @xmath174 . )",
    "the time taken by this operation is @xmath249    this is because each @xmath94 and @xmath230 pair ( such that @xmath250 ) has to be brought into @xmath226 cache only once for transposing and exchanging of @xmath174 submatrices .",
    "similarly , at any level of cache , a block from the matrix is brought in only once .",
    "the sequence of the recursive calls ensures that each cache line is used completely as we move from sub - matrix to sub - matrix .    finally , we move the transposed symmetric submatrices of size @xmath175 to their location in memory , , reverse the process of bringing in blocks of size @xmath169 from random locations to a contiguous block .",
    "this procedure is exactly the same as in theorem [ blkmove ] in the previous section that has the constant 3 .",
    "the above constant of 3 for writing back the matrix to an appropriate location depends on the assumption that we can keep the two symmetric submatrices of size @xmath175 in contiguous locations at the same time .",
    "this would allow us to exchange the matrices during the write back stage .",
    "if we are restricted to a contiguous temporary space of size @xmath251 only , then we will have to move the data twice , incurring the cost twice .",
    "even though in the above analysis we have always assumed a square matrix of size @xmath150 the algorithm works correctly without any change for transposing a matrix of size @xmath252 if we are transposing a matrix @xmath4 and storing it in @xmath3 .",
    "this is because the same analysis of subdividing into submatrices of size @xmath175 and transposing still holds .",
    "however if we want to transpose a @xmath252 matrix in place then the algorithm fails because the location to write back to would not be obvious and the approach used here would fail .",
    "the algorithm for matrix transpose runs in @xmath253 steps in a computer that has @xmath61 levels of direct - mapped cache .    if we have temporary storage space of size @xmath254 and assume block alignment of all submatrices then the constant is 7 .",
    "this includes @xmath215 for initial movement to contiguous location , @xmath255 for transposing the symmetric submatrices of size @xmath175 and @xmath256 for writing back the transposed submatrix to its original location .",
    "note that the constant is independent of the number of levels of cache .",
    "even if we have set associativity ( @xmath219 ) in any level of cache the analysis goes through as before ( though the constants will come down for data copying to contiguous locations ) . for",
    "the transposing and exchange of symmetric submatrices the set associativity will not come into play because we need a line only once in the cache and are using only 2 lines at a given time .",
    "so either lru or even fifo replacement policy would only evict a line that we have already finished using .      we first consider a restriction of the model described above where data can not be transferred simultaneously across non - consecutive cache levels .",
    "we use @xmath257 to denote @xmath258",
    ".    the lower bound for sorting in the restricted multi - level cache model is @xmath259 .",
    "[ restr ]    the proof of aggarwal and vitter can be modified to disregard block transfers that merely rearrange data in the external memory .",
    "then it can be applied separately to each cache level , noting that the data transfer in the higher levels do not contribute for any given level .",
    "these lower bounds are in the same spirit as those of vitter and nodine  @xcite ( for the s - umh model ) and savage  @xcite , that is , the lower bounds do not capture the simultaneous interaction of the different levels .",
    "if we remove this restriction , then the following can be proved along similar lines as theorem [ lbnd_srt ] .",
    "the lower bound for sorting in the multi - level cache model is @xmath260    this bound appears weak if @xmath61 is large . to rectify this , we observe the following . across each cache boundary , the minimum number of i / os follow from aggarwal and vitter s arguments .",
    "the difficulty arises in the multi - level model as a block transfer in level @xmath33 propagates in all levels @xmath261 although the block sizes are different .",
    "the minimum number of i / os from ( the highest ) level @xmath61 remains unaffected , namely , @xmath262 . for level @xmath263",
    ", we will subtract this number from the lower bound of @xmath264 .",
    "continuing in this fashion , we obtain the following lower bound .",
    "[ glbnd_srt ] the lower bound for sorting in the multi - level cache model is @xmath265    if we further assume that @xmath266 , we obtain a relatively simple expression that resembles theorem [ restr ] .",
    "note that the consecutive terms in the expression in the second summation of the previous lemma decrease by a factor of 3 .",
    "the lower bound for sorting in the multi - level cache model with geometrically decreasing cache sizes and cache lines is @xmath267 .",
    "[ nearly ]    in a multi - level cache , where the @xmath137 blocks are composed of @xmath268 blocks , we can sort in * expected * time @xmath269 .",
    "we perform a @xmath270-way mergesort using the variation proposed by barve  @xcite in the context of parallel disk i / os .",
    "the main idea is to shift each sorted stream cyclically by a random amount @xmath271 for the @xmath33th stream . if @xmath272 $ ] , then the leading element is in any of the cache sets with equal likelihood . like barve  @xcite",
    ", we divide the merging into phases where a phase outputs @xmath24 elements , where @xmath24 is the merge degree . in the previous section we counted",
    "the number of conflict misses for the input streams , since we could exploit symmetry based on the random input .",
    "it is difficult to extend the previous arguments to a worst case input .",
    "however , it can be shown easily that if @xmath273 ( where @xmath64 is the number of cache sets ) , the expected number of conflict misses is @xmath274 in each phase .",
    "so the total expected number of cache misses is @xmath275 in the level @xmath33 cache for all @xmath276 .",
    "the cost of writing a block of size @xmath134 from level @xmath61 is spread across several levels .",
    "the cost of transferring @xmath277 blocks of size @xmath134 from level @xmath61 is @xmath278 .",
    "amortizing this cost over @xmath279 transfers gives us the required result .",
    "recall that @xmath280 @xmath134 block transfers suffice for @xmath281-way mergesort .",
    "this bound is reasonably close to that of corollary [ nearly ] if we ignore constant factors .",
    "extending this to the more general emulation scheme of theorem [ u_bnd ] is not immediate as we require the block transfers across various cache boundaries to have a nice pattern , namely the _ sub - block _ property .",
    "this is satisfied by the mergesort and quicksort and a number of other algorithms but can not be assumed in general .      in this section",
    ", we will focus on a two - level cache model that has limited associativity .",
    "one of the _ cache - oblivious _ algorithms presented by frigo  @xcite is the funnel sort algorithm .",
    "they showed that the algorithm is optimal in the i / o model ( which is fully associative ) .",
    "however it is not clear whether the optimality holds in the cache model . in this section ,",
    "we show that , with some simple modification , funnel sort is optimal even in the direct - mapped cache model .",
    "the funnel sort algorithm can be described as follows .",
    "* split the input into @xmath282 contiguous arrays of size @xmath283 and sort these arrays recursively .",
    "* merge the @xmath282 sorted sequences using a @xmath282-merger , where a @xmath61-merger works as follows .",
    "a @xmath61-merger operates by recursively merging sorted sequences . unlike mergesort",
    ", a @xmath61-merger stops working on a merging sub - problem when the merged output sequence becomes `` long enough '' and resumes working on another merging sub - problem ( see figure [ kmerger ] ) .",
    "invariant the invocation of a @xmath61-merger outputs the first @xmath284 elements of the sorted sequence obtained by merging the @xmath61 input sequences .",
    "base case @xmath285 producing @xmath286 elements whenever invoked .    note the intermediate buffers are twice the size of the output obtained by a @xmath287 merger .",
    "to output @xmath284 elements , the @xmath61-merger is invoked @xmath288 times . before each invocation",
    "the @xmath61-merger fills each buffer that is less than half full so that every buffer has at least @xmath288 elements  the number of elements to be merged in that invocation .",
    "frigo  @xcite have shown that the above algorithm ( that does not make explicit use of the various memory - size parameters ) is optimal in the i / o model . however , the i / o model does not account for conflict misses since it assumes full associativity .",
    "this could be a degrading influence in the presence of limited associativity ( in particular direct - mapping ) .",
    "it is sufficient to get a bound on cache misses in the cache model since the bounds for capacity misses in the cache model are the same as the bounds shown in the i / o model .",
    "let us get an idea of what the structure of a @xmath61-merger looks like by looking at a 16-merger ( see figure [ 16merger ] ) .",
    "a @xmath61-merger , unrolled , consists of 2-mergers arranged in a tree - like fashion .",
    "since the number of 2-mergers gets halved at each level and the initial input sequences are @xmath61 in number there are @xmath289 levels .    if the buffers are randomly placed and the starting position is also randomly chosen ( since the buffers are cyclic this is easy to do ) the probability of conflict misses is maximized if the buffers are less than one cache line long .",
    "[ confprob ]    the worst case for conflict misses occurs when the buffers are less than one cache line in size .",
    "this is because if the buffers collide then all data that goes through them will thrash .",
    "if however the size of the buffers were greater than one cache line then even if some two elements collide the probability of future collisions would depend upon the data input or the relative movement of data in the two buffers .",
    "the probability of conflict miss is maximized when the buffers are less than one cache line",
    ". then probability of conflict is @xmath290 , where @xmath24 is equal to the cache size @xmath7 divided by the cache line size @xmath3 , , the number of cache lines .",
    "the analysis for compulsory and capacity misses goes through without change from the i / o model to the cache model .",
    "thus , funnel sort is optimal in the cache model if the conflict misses can be bounded by @xmath291    if the cache is 3-way or more set associative , there will be no conflict misses for a 2-way merger .",
    "[ setassoc ]    the two input buffers and the output buffer , even if they map to the same cache set can reside simultaneously in the cache .",
    "since at any stage only one 2-merger is active there will be no conflict misses at all and the cache misses will only be in the form of capacity or compulsory misses .      for an input of size @xmath75 , a @xmath292-merger",
    "is created .",
    "the number of levels in such a merger is @xmath293 ( i.e. , the number of levels of the tree in the unrolled merger ) .",
    "every element that travels through the @xmath292-merger sees @xmath293 2-mergers ( see figure [ expkmerger ] ) . for an element passing through a 2-merger",
    "there are 3 buffers that could collide .",
    "we _ charge _",
    "an element for a conflict miss if it is swapped out of the cache before it passes to the output buffer or collides with the output buffer when it is being output .",
    "so the expected number of collisions is @xmath294 times the probability of collision between any two buffers ( two input and one output ) .",
    "thus the expected number of collisions for a single element passing through a 2-merger is @xmath295 where @xmath296 .",
    "if @xmath297 is the probability of a cache miss for element @xmath33 in level @xmath93 then summing over all elements and all levels we get @xmath298    the expected performance of funnel sort is optimal in the direct - mapped cache model if @xmath299 .",
    "it is also optimal for a 3-way associative cache .",
    "if @xmath7 and @xmath3 are such that @xmath300 we have the total number of conflict misses @xmath301 note that the condition is satisfied for @xmath302 for any fixed @xmath303 which is similar to the _ tall - cache _ assumption made by frigo .",
    "the set associative case is proved by lemma [ setassoc ] .",
    "the same analysis is applicable between successive levels @xmath135 and @xmath141 of a multi - level cache model .",
    "this yields an optimal algorithm for sorting in the multilevel cache model .    in a multi - level cache model ,",
    "the number of cache misses at level @xmath135 in the funnel sort algorithm can be bounded by @xmath304 .",
    "this bound matches the lower bound of lemma [ restr ] within a constant factor , which makes it an optimal algorithm when simultaneous transfers are not allowed across multiple levels .",
    "we have presented a cache model for designing and analyzing algorithms . our model ,",
    "while closely related to the i / o model of aggarwal and vitter , incorporates three additional salient features of cache : lower miss penalty , limited associativity , and lack of direct program control over data movement .",
    "we have established an emulation scheme that allows us to systematically convert an i / o - efficient algorithm into a cache - efficient algorithm .",
    "this emulation provides a generic starting point for cache - conscious algorithm design ; it may be possible to further improve cache performance by problem - specific techniques to control interference misses .",
    "we have also demonstrated the relevance of the emulation scheme by demonstrating that a direct mapping of an i / o - efficient algorithm does not guarantee a cache - efficient algorithm .",
    "finally , we have extended our basic cache model to multiple cache levels .",
    "our single - level cache model is based on a blocking direct - mapped cache that does not distinguish between reads and writes . modeling a non - blocking cache or",
    "distinguishing between reads and writes would appear to require queuing - theoretic extensions and does not appear to be appropriate at the algorithm design stage .",
    "the _ translation lookaside buffer _ or tlb is another important cache in real systems that caches virtual - to - physical address translations .",
    "its peculiar aspect ratio and high miss penalty raise different concerns for algorithm design .",
    "our preliminary experiments with certain permutation problems suggests that tlbs are important to model and can contribute significantly to program running times .",
    "we have begun to implement some of these algorithms to validate the theory on real machines , and also using cache simulation tools like _ fast - cache _ , atom , or _",
    "cprof_. preliminary observations indicate that our predictions are more accurate with respect to miss ratios than actual running times ( see  @xcite ) .",
    "we have traced a number of possible reasons for this .",
    "first , because the cache miss latencies are not astronomical , it is important to keep track of the constant factors . an algorithmic variation that guarantees lack of conflict misses at the expense of doubling the number of memory references",
    "may turn out to be slower than the original algorithm .",
    "second , our preliminary experiments with certain permutation problems suggests that tlbs are important to model and can contribute significantly to program running times .",
    "third , several low - level details hidden by the compiler related to instruction scheduling , array address computations , and alignment of data structures in memory can significantly influence running times . as argued earlier , these factors are more appropriate to tackle at the level of implementation than algorithm design .",
    "several of the cache problems we observe can be traced to the simple array layout schemes used in current programming languages .",
    "it has shown elsewhere  @xcite that nonlinear array layout schemes based on quadrant - based decomposition are better suited for hierarchical memory systems .",
    "further study of such array layouts is a promising direction for future research .",
    "we are grateful to alvin lebeck for valuable discussions related to present and future trends of different aspects of memory hierarchy design .",
    "we would like to acknowledge rakesh barve for discussions related to sorting , fftw , and brp .",
    "the first author would also like to thank jeff vitter for his comments on an earlier draft of this paper .",
    "10    a.  agarwal , m.  horowitz , and j.  hennessy . an analytical cache model . , 7(2):184215 , may 1989 .",
    "a.  aggarwal , b.  alpern , a.  chandra , and m.  snir . a model for hierarchical memory .",
    "in _ proceedings of acm symposium on theory of computing _ , pages 305314 , 1987 .",
    "a.  aggarwal , a.  chandra , and m.  snir .",
    "hierarchical memory with block transfer . in",
    "_ proceedings of ieee foundations of computer science _ , pages 204216 , 1987 .",
    "a.  aggarwal and j.  vitter .",
    "the input / output complexity of sorting and related problems .",
    ", 31(5):11181127 , 1988 .",
    "b.  alpern , l.  carter , e.  feig , and t.  selker .",
    "the uniform memory hierarchy model of computation .",
    ", 12(2):72109 , 1994 .",
    "r.  barve .",
    "private communication .",
    "r.  barve , e.  grove , and j.  vitter .",
    "simple randomized mergesort on parallel disks . , 23(4):109118 , 1997 .",
    "a preliminary version appeared in spaa 96 .",
    "g.  bilardi and e.  peserico .",
    "efficient portability across memory hierarchies , 2000 .",
    "unpublished manuscript .",
    "l.  carter and k.  gatlin . towards an optimal bit - reversal permutation program . in _ proceeding of ieee foundations of computer science _ , 1998 .",
    "s.  chatterjee , v.  v. jain , a.  r. lebeck , s.  mundhra , and m.  thottethodi .",
    "nonlinear array layouts for hierarchical memory systems . in _ proceedings of the 1999 acm international conference on supercomputing _ , pages 444453 , rhodes , greece , june 1999 .",
    "s.  chatterjee , a.  r. lebeck , p.  k. patnala , and m.  thottethodi .",
    "recursive array layouts and fast parallel matrix multiplication . in _ proceedings of eleventh annual acm symposium on parallel algorithms and architectures _ , pages 222231 , saint - malo , france , june 1999 .",
    "s.  chatterjee and s.  sen . cache - efficient matrix transposition . in _ proceedings of hpca-6",
    "_ , pages 195205 , toulouse , france , jan",
    ". 2000 .",
    "y.  chiang , m.  goodrich , e.  grove , r.  tamassia , d.  vengroff , and j.  vitter . external memory graph algorithms . in _ proceedings of the acm - siam symposium of discrete algorithms _ ,",
    "pages 139149 , 1995 .",
    "t.  h. cormen , t.  sundquist , and l.  f. wisniewski .",
    "asymptotically tight bounds for performing bmmc permutations on parallel disk systems .",
    ", 28(1):105136 , 1999 .",
    "r.  floyd .",
    "permuting information in idealized two - level storage . in r.",
    "e. miller and j.  w. thatcher , editors , _ complexity of computer computations _ , pages 105109 .",
    "plenum press , new york , ny , 1972 .    c.  fricker , o.  temam , and w.  jalby .",
    "influence of cross - interference on blocked loops : a case study with matrix - vector multiply .",
    "17(4):561575 , july 1995 .    m.  frigo and s.  g. johnson . : an adaptive software architecture for the fft . in _ proceedings of icassp98",
    "_ , volume  3 , page 1381 , seattle , wa , 1998 .",
    "m.  frigo , c.  e. leiserson , h.  prokop , and s.  ramachandran .",
    "cache - oblivious algorithms . in _ proceedings of the 40th annual symposium on foundations of computer science ( focs 99 ) _ , new york , ny , oct .",
    "m.  goodrich , j.  tsay , d.  vengroff , and j.  vitter . external memory computational geometry . in _ proceeding of ieee foundations of computer science _ , pages 714723 , 1993 .",
    "m.  d. hill and a.  j. smith .",
    "evaluating associativity in cpu caches . , c-38(12):16121630 , dec .",
    "j.  hong and h.  kung .",
    "complexity : the red blue pebble game . in _ proceedings of acm symposium on theory of computing _ , 1981 .",
    "a.  kamath , r.  motwani , k.  palem , and p.  spirakis .",
    "tail bounds for occupancy and the satisfiability threshold conjecture . in _",
    "proceeding of ieee foundations of computer science _ ,",
    "pages 592603 , 1994 .",
    "r.  ladner , j.  fix , and a.  lamarca . cache performance analysis of algorithms . in _ proceedings of the acm - siam symposium of discrete algorithms _ , 1999 .",
    "m.  s. lam , e.  e. rothberg , and m.  e. wolf . the cache performance and optimizations of blocked algorithms . in _ proceedings of the fourth international conference on architectural support for programming languages and operating systems _ , pages 6374 , apr . 1991 .",
    "a.  lamarca and r.  ladner . the influence of cache on the performance of sorting . in _ proceedings of the acm - siam symposium of discrete algorithms",
    ", pages 370379 , 1997",
    ".    s.  a. przybylski . .",
    "morgan kaufmann publishers , san mateo , ca , 1990 .",
    "p.  sanders . accessing multiple sequences through set associative caches . in _ proceedings of icalp _",
    "a more recent version by mehlhorn and sanders was communicated to the authors in dec 1999 .",
    "j.  savage . extending the hong - kung model to memory hierarchies . in _ proceedings of cocoon _ , volume lncs 959 , pages 270281 .",
    "springer verlag , 1995 .    s.  sen and s.  chatterjee .",
    "towards a theory of cache - efficient algorithms . in _ proceedings of the symposium on discrete algorithms _ , 2000 .",
    "d.  sleator and r.  tarjan .",
    "amortized efficiency of list update and paging rules .",
    ", 28(2):202208 , 1985 .",
    "m.  thottethodi , s.  chatterjee , and a.  r. lebeck .",
    "tuning strassen s matrix multiplication for memory efficiency . in _ proceedings of sc98 ( cd - rom )",
    "_ , orlando , fl , nov . 1998 .",
    "available from http://www.supercomp.org/sc98 .",
    "j.  vitter and m.  nodine .",
    "large scale sorting in uniform memory hierarchies . , 17:107114 , 1993 .",
    "j.  vitter and e.  shriver .",
    "algorithms for parallel memory i : two - level memories . , 12(2):110147 , 1994 .",
    "let @xmath305 be the number of elements between @xmath94 and @xmath306 , , one less than the difference in ranks of @xmath94 and @xmath306 .",
    "( @xmath305 may be 0 , which guarantees event e1 . )",
    "let @xmath307 denote the event that @xmath308",
    ". then @xmath114 = \\sum_m \\pr [ e1 \\cap e_m ] $ ] , since @xmath307 s are disjoint . for each @xmath24 ,",
    "@xmath309 = \\pr [ e1 | e_m ] \\cdot \\pr [ e_m ] $ ] .",
    "the events @xmath307 correspond to a geometric distribution , , @xmath310 = \\pr [ \\mu = m ] = \\frac{1}{k } { \\left ( 1 - \\frac{1}{k } \\right)}^{m}. \\label{geomdist } \\label{eqn}{geomdist}\\ ] ]    to compute @xmath311 $ ] , we further subdivide the event into cases about how the @xmath24 numbers are distributed into the sets @xmath312 .",
    "wlog , let @xmath313 to keep notations simple .",
    "let @xmath314 denote the case that @xmath315 numbers belong to sequence @xmath90 ( @xmath316 ) .",
    "we need to estimate the probability that for sequence @xmath90 , @xmath317 does not conflict with @xmath318 ( recall that we have fixed @xmath313 ) during the course that @xmath315 elements arrive in @xmath90 .",
    "this can happen only if @xmath319 ( the cache set position of the leading block of @xmath90 right after element @xmath320 ) does not lie roughly @xmath321 blocks from @xmath322 . from assumption a1 and some careful counting",
    "this is @xmath323 for @xmath324 . for @xmath325 ,",
    "this probability is 1 since no elements go into @xmath90 and hence there is no conflict .",
    "these events are independent from our assumption a1 and hence these can be multiplied .",
    "the probability for a fixed partition @xmath326 is the multinomial @xmath327 ( @xmath24 is partitioned into @xmath263 parts ) .",
    "therefore we can write the following expression for @xmath328 $ ]",
    ". @xmath329 = \\sum _ { m_2 + \\cdots + m_k = m }   \\frac{m ! } { m_2 ! \\cdots m_k ! } \\cdot { \\left(\\frac{1}{k-1 } \\right)}^m \\prod_{m_i \\neq 0 } \\left ( 1 -   \\frac { m_j -1 + b}{s b } \\right ) \\label{eqncond } \\label{eqn}{eqncond}\\ ] ]    in the remainder of this section , we will obtain an upper bound on the right hand side of .",
    "let @xmath330 denote the number of @xmath93s for which @xmath331 ( non - zero partitions ) .",
    "then can be rewritten as the following inequality .",
    "@xmath329 \\leq \\sum _ { m_2 + \\cdots + m_k = m } \\frac{m ! } { m_2 !",
    "\\cdots m_k ! } \\cdot { \\left(\\frac{1}{k-1 } \\right)}^m { \\left ( 1 - \\frac { 1}{s } \\right)}^{nz ( m_2 \\ldots m_k ) } \\label{ineqcond } \\label{eqn}{ineqcond}\\ ] ] since @xmath332 for @xmath324 . in other words ,",
    "the right side is the expected value of @xmath333 , where @xmath334 denotes the number of non - empty bins when @xmath24 balls are thrown into @xmath263 bins . using and the preceding discussion , we can write down an upper bound for the ( unconditional ) probability of @xmath335 as @xmath336 \\label{simpeqn } \\label{eqn}{simpeqn}\\ ] ]      let @xmath337 , and @xmath338 be the number of empty bins when @xmath24 balls are thrown randomly into @xmath23 bins . then @xmath339 = n { \\left ( 1 - \\frac{1}{m } \\right)}^{m } \\sim n e^{-r}\\ ] ] and for @xmath340 @xmath341 | \\geq \\lambda ] \\leq   2 \\exp \\left ( -\\frac{\\lambda^2 ( n-1)/2 } { n^2 - \\mu^2 } \\right).\\ ] ] [ nzbins ]",
    "let @xmath342 be the number of non - empty bins when @xmath24 balls are thrown into @xmath61 bins .",
    "then @xmath343 = k ( 1- e^{-m / k } ) \\ ] ] and @xmath344 | \\geq \\alpha\\sqrt{2 k \\log k } ] \\leq 1/k^\\alpha.\\ ] ]      ( of lemma [ weakbnd ] ) : we will split up the summation of into two parts , namely , @xmath349 and @xmath350 .",
    "one can obtain better approximations by refining the partitions , but our objective here is to demonstrate the existence of @xmath118 and @xmath119 and not necessarily obtain the best values .",
    "@xmath351 & = & \\sum_{m=0}^{ek/2k }   \\frac{1}{k } { \\left ( 1 - \\frac{1}{k } \\right)}^{m } \\cdot e [ { \\left ( 1 - \\frac { 1}{s } \\right)}^{nz ( m , k-1 ) } ] \\nonumber \\\\ & + &   \\sum_{m = ek/2 + 1}^{\\infty }   \\frac{1}{k }   { \\left ( 1 - \\frac{1}{k } \\right)}^{m } \\cdot e [ { \\left ( 1 - \\frac { 1}{s } \\right)}^{nz ( m , k-1 ) } ] \\label{aptwoparts } \\label{eqn}{aptwoparts}\\end{aligned}\\ ] ]      the second term can be bounded using using @xmath354 . @xmath355 & \\leq & \\sum_{m = ek/2 + 1}^{\\infty }   \\frac{1}{k } { \\left ( 1 - \\frac{1}{k } \\right)}^{m } \\cdot 1/k^2 \\left ( 1 - 1/s \\right ) \\nonumber\\\\ & + & \\sum_{m = ek/2 + 1}^{\\infty }   \\frac{1}{k } { \\left ( 1 - \\frac{1}{k } \\right)}^{m } \\cdot { \\left ( 1 - \\frac { 1}{s } \\right)}^ { k ( 1- e^{-m / k } -       \\alpha\\sqrt{2 k \\log k } /k ) } \\label{apeqn2 } \\label{eqn}{apeqn2}\\end{aligned}\\ ] ]    the first term of the previous equation is less than @xmath91 and the second term can be bounded by @xmath356 for sufficiently large @xmath61 ( @xmath357 suffices ) . this can be bounded by @xmath358 , so can be bounded by @xmath359 .",
    "adding this to the first term of , we obtain an upper bound of @xmath360 for @xmath361 .",
    "subtracting this from 1 gives us @xmath362 , , @xmath363 ."
  ],
  "abstract_text": [
    "<S> we describe a model that enables us to analyze the running time of an algorithm in a computer with a memory hierarchy with limited associativity , in terms of various cache parameters . </S>",
    "<S> our model , an extension of aggarwal and vitter s i / o model , enables us to establish useful relationships between the cache complexity and the i / o complexity of computations . as a corollary </S>",
    "<S> , we obtain cache - optimal algorithms for some fundamental problems like sorting , fft , and an important subclass of permutations in the single - level cache model . </S>",
    "<S> we also show that ignoring associativity concerns could lead to inferior performance , by analyzing the average - case cache behavior of mergesort . </S>",
    "<S> we further extend our model to multiple levels of cache with limited associativity and present optimal algorithms for matrix transpose and sorting . </S>",
    "<S> our techniques may be used for systematic exploitation of the memory hierarchy starting from the algorithm design stage , and dealing with the hitherto unresolved problem of limited associativity . </S>"
  ]
}