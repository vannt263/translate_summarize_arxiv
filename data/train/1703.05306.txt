{
  "article_text": [
    "in this paper , our goal is to design new decoding algorithms that can enhance techniques known to date for rm codes .",
    "in general , rm codes can be designed from the set @xmath6 of all @xmath7-variate boolean polynomials of degree @xmath8 or less . here",
    "each polynomial @xmath9 @xmath10  @xmath6 is defined on the @xmath7-dimensional space @xmath11 for any @xmath12 we consider the sequence of binary values  @xmath13 obtained as argument @xmath14 runs through @xmath15 .",
    "these sequences - codewords @xmath16 - form an rm code , which is below denoted @xmath17 and has length @xmath18 dimension @xmath19 and distance @xmath20 as follows:@xmath21 the decoding algorithms discussed in this paper ( including the new algorithms ) can be applied to any rm code .",
    "however , we will mostly focus on their asymptotic performance obtained for long rm codes of _ fixed order _ @xmath1 to define their error - correcting performance , we use the following definition . given an infinite sequence of codes @xmath22 we say that a decoding algorithm @xmath23 has a sequence @xmath24 and a sequence @xmath25 if for @xmath26    @xmath27 @xmath23 correctly decodes all but a vanishing fraction of error patterns of weight @xmath28 or less ;    @xmath27 @xmath23 fails to decode a nonvanishing fraction of error patterns of weight @xmath24 or less can satisfy the same definition@xmath29@xmath30    nonexponential decoding algorithms known for rm codes can be loosely separated into three groups .",
    "first , _ _",
    "_ was developed in the seminal paper @xcite .",
    "the algorithm requires complexity of order @xmath31 or less .  for rm codes of fixed order @xmath32 it was proven in @xcite that majority decoding achieves maximum possible threshold @xmath33 ( here and below we omit index @xmath34 with a residual @xmath35 where @xmath36 is a constant that does not depend on @xmath7 and @xmath1    the second type of decoding algorithms makes use of the symmetry group of rm codes .",
    "one very efficient algorithm is presented in @xcite . for long rm codes",
    "@xmath37 this algorithm reduces the residual term @xmath38 from ( [ eps - maj ] ) to its square @xmath39 where @xmath40 . on the other hand ,",
    "the complexity order of @xmath41 of majority decoding is also increased in algorithm @xcite to almost its square @xmath42 the corresponding thresholds for higher orders @xmath43 are yet unknown .",
    "another result of @xcite concerns maximum - likelihood ( ml ) decoding .",
    "it is shown that ml decoding of rm codes of fixed order @xmath8 yields a substantially lower residual @xmath44 where @xmath45 however , even the best known algorithm of ml decoding designed by the multilevel trellis structure in @xcite has yet complexity that is exponent in @xmath0 .    finally , various techniques were introduced in @xcite , @xcite , @xcite , and @xcite .",
    "all these algorithms use different recalculation rules but rely on the same code design based on the _ _",
    "plotkin construction _ _",
    "@xmath46 the construction allows to decompose rm codes @xmath17 onto shorter codes , by taking subblocks * *  * * @xmath47  and @xmath48 from codes @xmath49 and @xmath50 the results from @xcite , @xcite , and @xcite show that this recursive structure enables both encoding and bounded distance decoding with the lowest complexity order of @xmath51 known for rm codes of an arbitrary order @xmath8 .    in the same vein , below",
    "we also employ plotkin construction .",
    "the basic recursive procedure will split rm code @xmath17 of length @xmath0 into two rm codes of length @xmath52 .",
    "decoding is then relegated further to the shorter codes of length @xmath53 and so on , until we reach basic codes of order @xmath54 or @xmath55 at these points , we use maximum likelihood decoding or the variants derived therefrom .",
    "by contrast , in all intermediate steps , we shall only recalculate the newly defined symbols . here",
    "our goal is to find efficient _ recalculation rules _ that can _ provably _ improve the performance of rm codes .",
    "our results presented below in theorems [ th:1 - 2 ] and [ th:1 - 1 ] show that recursive techniques indeed outperform other polynomial algorithms known for rm codes .",
    "these results also show how decoding complexity can be traded for a higher threshold.@xmath56    [ th:1 - 2 ] long rm codes @xmath17 of fixed order @xmath8 can be decoded with linear complexity @xmath57 and decoding threshold@xmath58 @xmath56    [ th:1 - 1 ] long rm codes @xmath17 of fixed order @xmath8 can be decoded with quasi - linear complexity  @xmath59 and decoding threshold@xmath60",
    "@xmath56    rephrasing theorems [ th:1 - 2 ] and [ th:1 - 1 ] , we obtain the following@xmath61    long rm codes @xmath17 of fixed order @xmath8 can be decoded with vanishing output error probability and linear complexity @xmath57  ( or quasi - linear complexity @xmath62 on a binary channel with crossover error probability @xmath63 ( correspondingly , @xmath64 as @xmath65    note that theorem [ th:1 - 2 ] increases decoding threshold of the recursive techniques introduced in @xcite and @xcite from the order of @xmath66 to @xmath52 while keeping linear decoding complexity .",
    "th:1 - 1 ] improves both the complexity and residual of majority decoding of rm codes .",
    "when compared with the algorithm of @xcite , this theorem reduces the quadratic complexity @xmath67 to a quasi - linear complexity @xmath59 and also extends this algorithm to an arbitrary order @xmath68 of rm codes .",
    "the algorithms designed below differ from the former algorithms of @xcite , @xcite , and @xcite in both the intermediate recalculations and the stopping rules .",
    "firstly , we employ  new _ intermediate recalculations _ , which yield the exact decoding thresholds , as opposed to the bounded distance threshold @xmath66 established in @xcite and @xcite .",
    "this leads us to theorem [ th:1 - 2 ] .",
    "secondly , by analyzing the results of theorem [ th:1 - 2 ] , we also change the former _ stopping rules _",
    ", all of which terminate decoding at the repetition codes .",
    "now we terminate decoding earlier , once we achieve the biorthogonal codes .",
    "this change yields theorem [ th:1 - 1 ] and substantially improves decoding performance ( this is discussed in section 7 ) .",
    "finally , we employ a new probabilistic analysis of recursive algorithms . in section 7",
    ", we will see that this analysis not only gives the actual thresholds but also shows how the algorithms can be advanced further .",
    "below in section 2 we consider recursive structure of rm codes in more detail . in section 3",
    ", we proceed with decoding techniques and design two different recursive algorithms @xmath69 and @xmath70 these algorithms are analyzed in sections 4 , 5 , and 6 , which are concluded with theorems [ th:1 - 2 ] and [ th:1 - 1 ] . in section 7",
    ", we briefly discuss extensions that include decoding lists , subcodes of rm codes , and soft decision channels . for the latter case",
    ", we will relate the noise power to the quantity @xmath71 thus , the residual @xmath4 will serve as a measure of the highest noise power that can be withstood by a specific low - rate code .",
    "consider any @xmath7-variate boolean polynomial @xmath72 and the corresponding codeword @xmath16  with symbols @xmath73 below we assume that positions @xmath74 are ordered lexicographically , with @xmath75 being the senior digit .",
    "note that any polynomial @xmath9 can be split as @xmath76 where we use the new polynomials @xmath77 and @xmath78 these polynomials are defined over @xmath79 variables and have degrees at most @xmath8 and @xmath80 respectively .",
    "correspondingly , one can consider two codewords @xmath81  and @xmath82 that belong to the codes @xmath49 and @xmath83",
    ". then representation ( [ poly ] ) converts any codeword @xmath84 @xmath17 to the form @xmath46 this is the well known by continuing this process on codes @xmath49 and @xmath85 we obtain rm codes of length @xmath86 and so on . finally , we arrive at the end nodes , which are repetition codes @xmath87 for any @xmath88  and full spaces @xmath89 for any @xmath90 this is schematically shown in fig . 1 for rm codes of length 8",
    ".    now let @xmath91 be a block of @xmath92information bits @xmath93  that encode a vector @xmath94 by decomposing this vector into @xmath47 and @xmath95 we also split @xmath96 into two information subblocks @xmath97 and @xmath98 that encode vectors @xmath47 and @xmath95 respectively .",
    "in the following steps , information subblocks are split further , until we arrive at the end nodes @xmath87 or @xmath89 .",
    "this is shown in fig .",
    "2 . note that only one information bit is assigned to the left - end ( repetition ) code @xmath99 while the right - end code @xmath89 includes @xmath100 bits .",
    "below these @xmath100 bits will be encoded using the unit generator matrix . summarizing , we see that any codeword can be encoded from the information strings assigned to the end nodes @xmath87 or @xmath89 , by repeatedly combining codewords @xmath101 and @xmath48 in the @xmath102-construction .",
    "0,0   @xmath103   1,0  1,1   @xmath104  @xmath105  @xmath106  @xmath107 @xmath108 @xmath109    2,0",
    "2,1  2,2     @xmath107 @xmath108  @xmath107  @xmath108  @xmath107 @xmath108   3,0  3,1  3,2  3,3     fig .",
    "1@xmath110 decomposition of rm codes of length 8 .    given any algorithm @xmath111 in the sequel we use notation @xmath112  for its complexity .",
    "let @xmath113 denote the encoding  described above for the code @xmath114 taking a complexity estimate from @xcite and its enhancement from @xcite , we arrive at the following lemma .",
    "@xmath115   @xmath116  @xmath117   @xmath118  @xmath119   @xmath120  @xmath121 @xmath122    @xmath123  @xmath124  @xmath125  @xmath126    @xmath121  @xmath121  @xmath121  @xmath121     fig .",
    "2 . decomposition of information paths    rm codes @xmath17 can be recursively encoded with complexity @xmath127    _ proof .",
    "_ first , note that the end nodes @xmath87 and @xmath89 require no encoding and therefore satisfy the complexity bound ( [ encoding ] ) .",
    "second , we verify that code @xmath17 satisfies ( [ encoding ] ) if the two constituent codes do .",
    "let the codewords @xmath128 and @xmath129 have encoding complexity @xmath130 and @xmath131 that satisfies ( [ encoding ] ) .",
    "then their @xmath132-combination requires complexity @xmath133 where @xmath52 extra additions ( @xmath134 were included to find the right half @xmath135 .",
    "now we substitute estimates ( [ encoding ] ) for quantities @xmath131 and @xmath136 . if @xmath137 then @xmath138  the two other cases , namely @xmath139 and @xmath140 can be treated similarly .",
    "@xmath141    now consider an information bit @xmath93 associated with a left node @xmath99 where @xmath142.$ ] we will map @xmath93 onto a specific `` binary path''@xmath143 of length @xmath7 leading from the origin @xmath17 to the end node  @xmath144 .",
    "to do so , we first define the senior bit @xmath145{ll}% 0 , & \\text{if}\\;a_{j}\\in\\mathbf{a}_{\\,\\,r-1}^{m-1},\\smallskip\\smallskip\\\\ 1 , & \\text{if}\\;a_{j}\\in\\mathbf{a}_{\\,\\,\\,\\,\\,\\,r}^{m-1}. \\end{array } \\right.\\ ] ] next , we take @xmath146 if @xmath93 encodes the left descendant subcode on the following step .",
    "otherwise , @xmath147 similar procedures are then repeated at the steps @xmath148 and give some @xmath149 that arrives at the node @xmath150 we then add @xmath151 right - hand steps and obtain a full path @xmath152  of length @xmath7 that arrives at the node @xmath153 using notation @xmath154 for the sequence of @xmath151 ones , we write @xmath155    now consider any right - end node @xmath89 , where @xmath156 $ ] and let @xmath149 be any right - end path that ends at this node . then @xmath149 is associated with @xmath100  information bits",
    ". therefore we extend @xmath149 to the full length @xmath7 by adding any binary suffix @xmath157 this allows us to consider separately all @xmath100 information bits and use common notation @xmath158 .",
    "when all left- and right - end paths are considered together , we obtain all paths of length @xmath7 and binary weight @xmath159 or more .",
    "this gives one - to - one mapping between @xmath160 information bits and extended paths @xmath161 below all @xmath152 are ordered lexicographically , as @xmath7-digital binary numbers .",
    "now we turn to recursive decoding algorithms .",
    "we map any binary symbol @xmath162 onto @xmath163 and assume that all code vectors belong to @xmath164 obviously , the sum @xmath165 of two binary symbols is being mapped onto the product of their images .",
    "then we consider any codeword @xmath166 transmitted over a binary symmetric channel with crossover probability @xmath167 the received block @xmath168 consists of two halves @xmath169 and @xmath170 , which are the corrupted images of vectors @xmath47 and @xmath171 .",
    "we start with a basic algorithm @xmath172 that will be later used in recursive decoding . in our decoding",
    ", vector @xmath173 will be replaced by the vectors whose components take on real values from the interval @xmath174.$ ] therefore we take a more general approach and assume that @xmath175    * step 1*.  we first try to find the codeword @xmath48 from @xmath176 in the absence of noise , we have the equality @xmath177 ( which gives the binary sum of vectors @xmath169 and @xmath170 in the former notation ) . on a noisy channel ,",
    "we first find the `` channel estimate''@xmath178 of @xmath179 next , we employ ( any ) decoding @xmath180 which will be specified later .",
    "the output is some vector @xmath181 and its information block @xmath182.@xmath183    * step 2*.  we try to find the block  @xmath184 given @xmath185 from step 1 . here",
    "we take two corrupted versions of vector @xmath47 , namely @xmath169 in the left half and @xmath186 in the right half .",
    "these two _ real _ vectors are added and combined in their `` midpoint '' @xmath187 then we use some decoding @xmath188 which is also specified later .",
    "the output is some vector @xmath189 and its information block @xmath190 so , decoding @xmath172 is performed as follows.@xmath183 @xmath191{l}% \\text{algorithm } \\psi_{\\text{rec}}(\\mathbf{y)}.\\medskip\\\\ \\text{1 .",
    "calculate vector } \\mathbf{y}^{v}=\\mathbf{y}^{\\prime}\\mathbf{y}% ^{\\prime\\prime}\\text{.}\\\\ \\text{find } \\mathbf{\\hat{v}=}\\psi(\\mathbf{y}^{v})\\text { and } \\hat { \\mathbf{a}}^{v}.\\medskip\\\\ \\text{2 . calculate vector } \\mathbf{y}^{u}=(\\mathbf{y}^{\\prime}+\\mathbf{y}% ^{\\prime\\prime}\\hat{\\mathbf{v}})/2\\text{.}\\\\ \\text{find } \\mathbf{\\hat{u}=}\\psi(\\mathbf{y}^{u})\\text { and } \\hat { \\mathbf{a}}^{u}.\\medskip\\\\ \\text{3 . output decoded components:}\\\\ \\hat{\\mathbf{a}}:=(\\hat{\\mathbf{a}}^{v}\\mid\\hat{\\mathbf{a}}% ^{u});\\quad\\mathbf{\\hat{c}}:=(\\mathbf{\\hat{u}}\\mid\\mathbf{\\hat{u}\\hat{v } } ) . \\end{array } $ } % \\ ] ]    in a more general scheme @xmath69 , we repeat this recursion by decomposing subblocks @xmath192 and @xmath193 further . on each intermediate step",
    ", we only recalculate the newly defined vectors @xmath192 and @xmath193  using ( [ 1 ] ) when decoder moves left and ( [ 3 ] ) when it goes right . finally , vectors @xmath192 and @xmath193 are decoded , once we reach the end nodes @xmath87 and @xmath89 . given any end code @xmath194 of length @xmath195 and any estimate @xmath196",
    "we employ the ( soft decision ) ( md ) decoding @xmath197 that outputs a codeword @xmath198 closest to @xmath199 in the .",
    "equivalently , @xmath198 maximizes the inner product @xmath200 the algorithm is described below .",
    "@xmath191{l}% \\text{algorithm } \\psi_{\\,r}^{m}(\\mathbf{y)}.\\medskip\\\\ \\text{1 . if } 0<r < m\\text { , perform } \\psi_{\\text{rec}}(\\mathbf{y)}\\text { } \\\\ \\text{using } \\psi(\\mathbf{y}^{v})=\\psi_{\\,\\,r-1}^{m-1}\\text { and } % \\psi(\\mathbf{y}^{u})=\\psi_{\\,\\,\\,\\,\\,r}^{m-1}\\text{.}\\medskip\\\\ \\text{2 . if } r=0,\\text { perform md decoding } \\\\",
    "\\psi(\\mathbf{y}^{v})\\text { for code } \\left\\ { % tcimacro{\\qatop{r}{0}}% % beginexpansion \\genfrac{}{}{0pt}{}{r}{0}% % endexpansion \\right\\ }   .\\medskip\\\\ \\text{3 .",
    "if } r = m,\\text { perform md decoding } \\\\ \\psi(\\mathbf{y}^{u})\\text { for code } \\left\\ { % tcimacro{\\qatop{r}{r}}% % beginexpansion \\genfrac{}{}{0pt}{}{r}{r}% % endexpansion \\right\\ }   .",
    "\\end{array } $ } % \\ ] ]    in the following algorithm @xmath201 , we refine algorithm @xmath202 by terminating decoding @xmath23 at the biorthogonal codes @xmath203 .",
    "@xmath191{l}% \\text{algorithm } \\phi_{\\,r}^{m}(\\mathbf{y)}.\\medskip\\\\ \\text{1 . if }",
    "1<r < m\\text { , perform } \\psi_{\\text{rec}}(\\mathbf{y)}\\text { } \\\\ \\text{using } \\psi(\\mathbf{y}^{v})=\\phi_{\\,\\,r-1}^{m-1}\\text { and } % \\psi(\\mathbf{y}^{u})=\\phi_{\\,\\,\\,\\,\\,r}^{m-1}\\text{.}\\medskip\\\\ \\text{2 .",
    "if } r=1,\\text { perform md decoding } \\\\",
    "\\phi(\\mathbf{y}^{v})\\text { for code } \\left\\ { % tcimacro{\\qatop{r}{1}}% % beginexpansion \\genfrac{}{}{0pt}{}{r}{1}% % endexpansion \\right\\ }   .\\medskip\\\\ \\text{3 .",
    "if } r = m,\\text { perform md decoding } \\\\ \\phi(\\mathbf{y}^{u})\\text { for code } \\left\\ { % tcimacro{\\qatop{r}{r}}% % beginexpansion \\genfrac{}{}{0pt}{}{r}{r}% % endexpansion \\right\\ }   .",
    "\\end{array } $ } % \\ ] ]    thus , procedures @xmath69 and @xmath201 have a recursive structure that calls itself until md decoding is applied on the end nodes . now",
    "the complexity estimate follows.@xmath183    for any rm code @xmath204 algorithms @xmath69 and @xmath201 have decoding complexity@xmath205 and @xmath206 md decoding can be executed in @xmath0 operations and satisfies the bound ( [ comp - fi ] ) ( here we assume that finding the sign of a real value requires one operation ) . for biorthogonal codes , their md decoding @xmath207 can be executed in @xmath208 operations using the green machine or @xmath209 operations using fast hadamard transform ( see @xcite or @xcite , section 14.4 ) .",
    "obviously , this decoding satisfies the upper bound ( [ comp - f ] ) .",
    "second , for both algorithms @xmath23 and @xmath210 , vector @xmath192 in ( [ 1 ] ) can be calculated in @xmath52 operations while vector @xmath193 in ( [ 3 ] ) requires @xmath211 operations .",
    "therefore our decoding complexity satisfies the same recursion @xmath212 finally , we verify that ( [ comp - fi ] ) and ( [ comp - f ] ) satisfy the above recursion , similarly to the derivation of ( [ encoding ] ) .",
    "@xmath141    _ discussion .",
    "_    both algorithms @xmath69 and @xmath213 admit bounded distance decoding .",
    "this fact can be derived by adjusting the arguments of @xcite for our recalculation rules ( [ 1 ] ) and ( [ 3 ] ) .",
    "algorithm @xmath214 is also similar to decoding algorithms of @xcite and @xcite .",
    "however , our _ recalculation rules _ are different from those used in the above papers . for example",
    ", the algorithm of @xcite performs the so - called `` min - sum '' recalculation@xmath215 instead of ( [ 1 ] ) .",
    "this ( simpler ) recalculation ( [ 1 ] ) will allow us to substantially expand the _ `` provable '' _ decoding domain versus the bounded - distance domain established in @xcite and @xcite .",
    "we then further extend this domain in theorem [ th:1 - 1 ] , also using the new _ stopping rule _ that replaces @xmath216 in @xmath69 with @xmath217 in @xmath218 however , it is yet an open problem to find the decoding domain using any other recalculation rule , say those from @xcite , @xcite , @xcite , or @xcite .    finally , note that the scaling factor @xmath219 in recalculation rule ( [ 3 ] ) brings any component @xmath220 back to the interval @xmath174 $ ] used before this recalculation .",
    "this scaling will also allow us to simplify some proofs , in particular that of lemma [ lm : nei1 ] .",
    "however , replacing ( [ 3 ] ) by the simpler rule @xmath221 does not change any decoding results . though being equivalent , the new rule also reduces complexity estimates ( [ comp - fi ] ) and ( [ comp - f ] ) to@xmath222 ) for the only reason to simplify our proofs .",
    "we begin with the algorithm @xmath69 and later will use a similar analysis for the algorithm @xmath70 note that @xmath69 enters each end node multiple times , by taking all paths leading to this node .",
    "it turns out that the output bit error rate ( ber ) significantly varies on different nodes and even on different paths leading to the same node .",
    "therefore our first problem is to fix a path @xmath152 and estimate the output ber for the corresponding information symbol @xmath223 in particular , we will define the most error - prone paths .",
    "consider any ( sub)path of some length @xmath224 let @xmath149 be its prefix of length @xmath225 so that @xmath226 where@xmath227 . \\label{sub}%\\ ] ] first , note that algorithm @xmath202 repeatedly recalculates its input @xmath173 , by taking either an estimate @xmath192 from ( [ 1 ] ) when a path @xmath152 turns left or @xmath228  from ( [ 3 ] ) otherwise .",
    "the following lemma shows that recursive decoding follows lexicographic order of our paths @xmath229",
    "@xmath230    for two paths @xmath231 and @xmath232 the bit @xmath158 is decoded after @xmath233 if  @xmath234    _ proof .",
    "_  given two paths @xmath152 and @xmath235 let @xmath236 be the first ( senior ) position where they disagree . if @xmath237 @xmath235 then @xmath238 and @xmath239 thus , after @xmath236 steps , @xmath231 moves left while @xmath152 moves right .",
    "correspondingly , @xmath231 proceeds first.@xmath240    on any subpath @xmath152 of length @xmath241 algorithm @xmath202 outputs some vector @xmath242 of length @xmath243 next , we derive a recursive expression for @xmath242 using formulas ( [ 1 ] ) and ( [ 3 ] ) . in any step @xmath236 ,",
    "the algorithm first splits @xmath244 into halves @xmath245 and @xmath246 for @xmath247 @xmath242 is given by recursion ( [ 1 ] ) and is rewritten below in the upper line of ( [ main ] ) .    if @xmath248 then @xmath242 is obtained from ( [ 3 ] ) .",
    "here we also need the vector @xmath249 decoded on the preceding subpath @xmath250 .",
    "the corresponding output is written in the second line of ( [ main ] ) : @xmath251{ll}% \\mathbf{y}^{\\prime}(\\,\\underline{\\xi}\\,)\\mathbf{y}^{\\prime\\prime}% ( \\,\\underline{\\xi}\\ , ) , & \\text{if}\\;\\xi_{s}=0,\\\\ \\mathbf{y}^{\\prime}(\\,\\underline{\\xi}\\,)/2+\\hat{\\mathbf{v}}% ( \\xi)\\mathbf{y}^{\\prime\\prime}(\\,\\underline{\\xi}\\,)/2 , & \\text{if}\\;\\xi_{s}=1 .",
    "\\end{array } \\right .",
    "\\label{main}%\\ ] ] finally , consider any left - end path @xmath252 that passes some repetition code @xmath87 .",
    "note that no preceding decodings are used after @xmath152 reaches the repetition code @xmath150 here we define the end result on the path @xmath152 as @xmath253 by taking @xmath254 in the last @xmath151 steps of recursion ( [ main ] ) .",
    "note that md decoding also makes its decision on the entire sum of symbols @xmath255 and outputs the symbol @xmath256 for any right - end code @xmath257 the output is some vector @xmath258 of length @xmath259 again , md decoding takes every symbol @xmath260 on the full path @xmath152 and converts it into the information bit @xmath261 making bit - by - bit decision ( [ md1])@xmath262 this is summarized as @xmath183    for any end path @xmath232 the algorithm @xmath69 decodes the outputs @xmath260 into the information bits @xmath263 using the rule ( [ md1 ] ) .",
    "next , we consider the decoding error probability @xmath264 for any information bit @xmath223 on an additive binary symmetric channel , @xmath264 does not depend on the transmitted codeword @xmath265 and we can assume that @xmath266 .",
    "according to our decoding rule ( [ md1 ] ) , an error event @xmath267 has probability @xmath268 ( here we follow footnote 2 and assume that @xmath269 with probability 1/2 if @xmath270 )    note , however , that recursive output @xmath260 depends on the outputs @xmath271 obtained on all preceding paths @xmath272 to simplify our calculations , we wish to consider the above event @xmath273 conditioned that all preceding decodings are correct@xmath262 this implies that any path @xmath274 gives an information bit @xmath275 and a codeword @xmath271 as follows : @xmath276 this assumption also allows us to simplify our recalculations ( [ 1 ] ) , ( [ 3 ] ) , and ( [ main ] ) by removing all vectors @xmath277 : @xmath278@xmath251{ll}% \\mathbf{y}^{\\prime}(\\,\\underline{\\xi}\\,)\\mathbf{y}^{\\prime\\prime}% ( \\,\\underline{\\xi}\\ , ) , & \\text{if}\\;\\xi_{s}=0,\\\\ \\mathbf{y}^{\\prime}(\\,\\underline{\\xi}\\,)/2+\\mathbf{y}^{\\prime\\prime } ( \\,\\underline{\\xi}\\,)/2 , & \\text{if}\\;\\xi_{s}=1 .",
    "\\end{array } \\right .",
    "\\smallskip\\label{main2}%\\ ] ] therefore our first goal is to find how much unconditional probabilities @xmath264 change given that preceding decodings are correct .",
    "first , let @xmath279 be the leftmost path that begins with @xmath8 zeros followed by @xmath159 ones .",
    "for any path @xmath152 , let @xmath280 and @xmath281 denote the events @xmath282 which include all error vectors that are correctly decoded on the paths @xmath283 or @xmath284 respectively .",
    "we define the complete ensemble of all error vectors by @xmath285 in the sequel , we replace each probability @xmath264 by the probability @xmath286 conditioned that all previous decodings are correct .",
    "the following upper bound ( [ tot2 ] ) conservatively assumes that an information symbol @xmath287 is _ always _ incorrect whenever a failure occurs in any step @xmath283 .",
    "similarly , the upper bound in ( [ tot3 ] ) uses the formula of total probability and adds up probabilities @xmath288 over all paths @xmath152 .",
    "by contrast , the lower bound takes into account that the block is always incorrect given the decoding failure on the first step @xmath289 .",
    "[ lm : prob0]for any path @xmath290  its bit error rate @xmath264  satisfies inequality@xmath291 block error probability @xmath292  satisfies inequalities @xmath293 @xmath183    _ proof .",
    "_  the probability @xmath264 can be estimated as@xmath294 similarly , the total probability @xmath292 is bounded as @xmath295 @xmath240      given any path @xmath232 we now assume that decoder gives correct solutions @xmath296 on all previous paths @xmath272 our next goal is to estimate the decoding error probability@xmath297 where @xmath260 is a random variable ( rv ) , which satisfies simplified recalculations ( [ main2 ] ) . here",
    "we begin with the original probability distribution @xmath298{ll}% 1-p , & \\text{if}\\;y_{i}=+1,\\\\ p , & \\text{if}\\;y_{i}=-1 , \\end{array } \\right .",
    "\\label{init}%\\ ] ] where @xmath299 are @xmath0 independent identically distributed  ( i.i.d . )",
    "rv that form the received vector @xmath300    _ remark . _",
    "note that the above problem is somewhat similar to that of `` probability density evolution '' researched in iterative algorithms .",
    "namely , in both algorithms the original rv @xmath299  undergo two different transformations , similar to ( [ main2 ] ) .",
    "however , in our setting these transformations can also be mixed in an arbitrary ( irregular ) order that only depends on a particular path @xmath232 in general , and on its current symbol @xmath301 in particular .",
    "@xmath183    to simplify this problem , below we estimate @xmath288 using only the first two moments of variables @xmath299 and their descendants",
    ". this will be done as follows .",
    "first , note that the blocks @xmath169 and @xmath302 used in ( [ main1 ] ) always include different channel bits@xmath262 consequently , their descendants @xmath303 and @xmath304 used in ( [ main2 ] ) are also obtained from different channel bits .",
    "these bits are combined in the same operations .",
    "therefore all symbols @xmath305 of the vector @xmath242 are  i.i.d .",
    "this allows us to use the common notation @xmath260 for any random variable @xmath305 obtained on the subpath @xmath161    \\2 .",
    "let @xmath306 @xmath260 denote the expectation of any rv @xmath307 below we study the normalized random variables @xmath308 all of which have expectation 1 .",
    "our goal is to estimate their variances@xmath309 then decoding error probability always satisfies chebyshev s inequality @xmath310    \\3 . to prove theorem 1 ,",
    "we first consider those left - end paths @xmath152 that pass through the nodes @xmath87 with growing @xmath311  for any such path , we show that the corresponding rv @xmath260 satisfies the central limit theorem as @xmath312 this will allow us to replace chebyshev s inequality ( [ cheb1 ] ) by ( a stronger ) gaussian approximation .",
    "we will also see that the variance @xmath313 rapidly declines as decoding progresses over the new paths @xmath152 .",
    "for this reason , we shall still use chebyshev s inequality ( [ cheb1 ] ) on the remaining paths with @xmath314 which will only slightly increase the block error probability @xmath292 defined in ( [ tot3 ] ) .",
    "our next goal is to recalculate the variances @xmath313 defined in ( [ cheb])@xmath262  let the channel residual @xmath315 be fixed . according to ( [ init ] ) , original channel outputs @xmath299 have the means @xmath316 in which case rv @xmath317 @xmath318 have the variance @xmath319    for any path @xmath320 the variance @xmath313 satisfies the recursions@xmath321{ll}% $ \\mu(\\xi)+1=(\\mu(\\,\\underline{\\xi}\\,)+1)^{2},$ & $ \\text{if}\\;\\xi_{s}=0,$% \\end{tabular } \\ \\label{t12}%\\]]@xmath321{ll}% $ \\qquad\\qquad\\mu(\\xi)=\\mu(\\,\\underline{\\xi}\\,)/2,$ & $ \\text{if}\\;\\xi _ { s}=1.\\medskip$% \\end{tabular } \\",
    "\\label{t1}%\\ ] ]    _ proof . _",
    "first , we need to find the means @xmath322 of rv @xmath260 to proceed with new variables @xmath323 here we simply replace all three rv used in ( [ main2 ] ) by their expectations . then for any @xmath226 the means @xmath322 satisfy the recursion@xmath324{ll}% e^{2}(\\underline{\\xi } ) , & \\text{if}\\;\\xi_{s}=0,\\medskip\\\\ e(\\underline{\\xi } ) , & \\text{if}\\;\\xi_{s}=1.\\medskip \\end{array } \\right .   \\label{e10}%\\ ] ] here we also use",
    "the fact that vectors @xmath325 and @xmath326 are independent and have symbols with the same expectation @xmath327 now we see that the normalized rv @xmath328 satisfy the recursion @xmath329{ll}% z^{\\prime}(\\underline{\\xi})\\cdot z^{\\prime\\prime}(\\underline{\\xi } ) , & \\text{if}\\;\\xi_{s}=0,\\medskip\\\\ z^{\\prime}(\\underline{\\xi})/2+z^{\\prime\\prime}(\\underline{\\xi})/2 , & \\text{if}\\;\\xi_{s}=1,\\medskip \\end{array } \\medskip\\medskip\\right .",
    "\\label{main5}%\\ ] ] similarly to ( [ main2 ] ) . by taking @xmath330",
    "we immediately obtain ( [ t12 ] ) and ( [ t1]).@xmath331    _ discussion . _ note that  the means @xmath322 only depend on the hamming weight @xmath332 of a binary subpath @xmath161 indeed , a subpath @xmath152 has @xmath333 zero symbols @xmath334 according to ( [ e10 ] ) , the original expectation @xmath335 of rv @xmath299 is squared @xmath333 times and is left unchanged @xmath332 times",
    ". therefore @xmath336 by contrast , equalities ( [ t12 ] ) and ( [ t1 ] ) show that variance @xmath337 depends on positions of all ones in vector @xmath152 .",
    "thus , direct ( nonrecurrent ) calculations of @xmath313  become more involved . in lemma",
    "[ lm : mu ] , we will see that even the simplest paths give rather bulky expressions ( [ mu1 ] ) for @xmath338 for this reason , we use a different approach .",
    "namely , in the next section we find the paths @xmath152 that maximize @xmath313 .",
    "_ _ preliminary discussion .",
    "_ _ consider a channel with crossover error probability _ _  _ _ @xmath339 and residual @xmath340 initially , rv @xmath328 have the variance @xmath341 and always satisfy inequality @xmath342 by definition ( [ cheb ] ) .",
    "according to ( [ t12 ] ) , @xmath343 is always squared when a path @xmath149 is appended by @xmath344 thus , moving from a code @xmath17 to its left descendant @xmath83 is equivalent to the replacement of the original residual @xmath4 by its square @xmath345 in other words , any left - hand movement makes the descendant channel noisier . for small @xmath346 ( very high quality channel ) , squaring @xmath347 is almost insignificant .",
    "however , it becomes more substantial as @xmath348 grows .",
    "by contrast , @xmath348 is always cut in half@xmath349 when @xmath350 in general , any right - hand movement makes the descendant channel less noisy .",
    "for example , we obtain @xmath351 on ( bad ) channels with small residual @xmath340 then performing the right step , the recursion replaces this residual @xmath4 with the quantity almost equal to @xmath352 therefore our first conclusion is that variance @xmath313 increases if @xmath238 is replaced by @xmath353 .",
    "_ neighboring paths .",
    "_ our next step is to consider two `` equally balanced ''  movements .",
    "namely ,  in ( [ loop ] ) below , we consider two subpaths @xmath354 and @xmath355  of length @xmath236 that have the same prefix @xmath356of length @xmath357 but diverge in the last two positions as follows @xmath358{l}% \\xi_{-}=(\\underline{\\xi},0,1),\\ \\ \\ \\ \\ \\ \\ \\ \\ \\,\\ \\,\\nearrow\\nwarrow\\ \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\,\\ \\,\\",
    "^{0}% \\nwarrow\\nearrow\\,^{1}\\ \\\\ \\xi_{+}=(\\underline{\\xi},1,0).\\ \\ \\ \\ \\ \\underline{\\xi}=\\xi_{1}, ... ,\\xi_{s-2}% \\end{array } \\right .   \\label{loop}%\\ ] ] we say that @xmath354 and @xmath355 are left and right , correspondingly .",
    "[ lm : nei1 ] any two neighbors @xmath354 and @xmath355 satisfy inequality@xmath359    _ proof .",
    "_ let @xmath360",
    "then we use recursive equations ( [ t12 ] ) and ( [ t1 ] ) , which give @xmath361 therefore ( [ in1 ] ) holds@xmath262@xmath331    _ the weakest paths .",
    "_ now we see that any path @xmath152 that includes two adjacent symbols @xmath362 increases its @xmath313 after permutation ( @xmath363 in this case , we say that this path @xmath152 becomes . from now on , let @xmath364  be the complete set of  @xmath160 extended paths @xmath161 also , let @xmath365 be the subset  of all left - end paths @xmath152 that enter the node @xmath87 and @xmath366 be the subset of the right - end paths .",
    "given any subset @xmath367 we now say that @xmath368 is the weakest path in @xmath369 if @xmath370 then we have the following.@xmath371    [ lm : path]the weakest path on the full set @xmath364 of all @xmath160 paths is the leftmost path ( [ w1 ] ) .",
    "more generally , for any @xmath142,$ ] the weakest path on the subset @xmath365 is its leftmost path@xmath372 @xmath183    _ proof . _",
    "first , note that on all left - end paths @xmath152 , the variances @xmath373 are calculated after @xmath7 steps , at the same node @xmath153 by contrast , all right - end paths @xmath152 end at different nodes @xmath374 therefore their variances @xmath313 are found after @xmath375 steps . to use lemma [ lm : nei1 ] , we consider an extended right - end path @xmath376 obtained by adding @xmath377 zeros .",
    "then we have inequality @xmath378 since the variance @xmath379 increases after zeros are added . despite this fact ,",
    "below we prove that @xmath289 from ( [ w1 ] ) and @xmath380 from ( [ w2 ] ) still represent the weakest paths , even after this extension .",
    "indeed , now all paths have the same length @xmath7 and the same weight @xmath381 so we can apply lemma [ lm : nei1 ] . recall that each path @xmath382 ends with the same suffix @xmath383 . in this case , @xmath380 is the leftmost path on @xmath365 . by lemma [ lm : nei1 ] , @xmath380 maximizes the variance @xmath313 over all @xmath384 finally , note that @xmath289  is the leftmost path on the total set @xmath364 since all @xmath8 zeros form its prefix @xmath385 .",
    "thus , @xmath289 is the weakest  path .",
    "now we find the variances @xmath386 and @xmath387 for the weakest paths @xmath289 and @xmath388    [ lm : mu ] for crossover error probability @xmath339 , the weakest paths @xmath289 and @xmath380 give the variances@xmath389@xmath390 @xmath183    _ proof .",
    "_  consider the weakest path @xmath289 from ( [ w1 ] ) .",
    "the recursion ( [ t12 ] ) begins with the original quantity @xmath391 after completing @xmath8 left steps @xmath392 the result is @xmath393 then we proceed with @xmath159 right steps , each of which cuts @xmath394 in half according to ( [ t1 ] ) .",
    "thus , we obtain equality ( [ t0 ] ) .",
    "formula ( [ mu1 ] ) follows from representation ( [ w2 ] )  in a similar ( though slightly longer ) way .",
    "@xmath240    lemma [ lm : mu ] allows us to use chebyshev s inequality @xmath395 for any path @xmath161  however , this bound is rather loose and insufficient to prove theorem 1 .",
    "therefore we improve this estimate , separating all paths into two different sets .",
    "namely , let @xmath396  be the subset  of all left - end paths that enter the node @xmath87 with @xmath311    we will use the fact that any path @xmath397 satisfies the central limit theorem as @xmath7 grows .",
    "however , we still use chebyshev s inequality on the complementary subset @xmath398 in doing so , we take @xmath4 equal to the @xmath399 from theorem 1 : @xmath400    [ lm : fi]for rm codes with @xmath401 and fixed order @xmath8 used on a binary channel with crossover error probability @xmath402 algorithm @xmath69 gives on a path @xmath152 the asymptotic bit error rate @xmath403 with asymptotic equality on the weakest path @xmath289 .    _ proof .",
    "_  according to ( [ md2 ] ) , any left - end path @xmath152 gives the rv @xmath404 which is the sum of @xmath405 i.i.d .",
    "limited rv @xmath406 for @xmath407 this number grows as @xmath408 or faster as @xmath312 in this case , the normalized rv @xmath328 satisfies the central limit theorem and its probability density function ( pdf ) tends to the gaussian distribution @xmath409    according to lemmas [ lm : path ] and [ lm : mu ] , the weakest path @xmath410 gives the maximum variance @xmath411 in particular , for @xmath412 equality ( [ t0 ] ) gives @xmath413 using gaussian distribution @xmath414 to approximate @xmath415 we take @xmath416 standard deviations and obtain ( see also remark 1 following the proof ) @xmath417 here we also use the asymptotic@xmath321{ll}% $ q(x)$ & $ \\overset{\\text{def}}{=}\\int_{x}^{\\infty}\\exp\\{-x^{2}/2\\}dx/\\sqrt { 2\\pi}\\medskip$\\\\ & $ \\sim\\exp\\{-x^{2}/2\\}/(x\\sqrt{2\\pi})$% \\end{tabular } \\ \\ \\\\ ] ] valid for large @xmath418 this yields asymptotic equality for @xmath419 in ( [ gauss1 ] ) .",
    "for any other path @xmath407 @xmath328  is approximated by the normal rv with a smaller variance @xmath420 therefore we use inequality in ( [ gauss1 ] ) : @xmath421 finally , consider any path @xmath152 with @xmath422 in this case , we directly estimate the asymptotics of @xmath423 namely , we use the substitution @xmath412 in ( [ mu1 ] ) , which gives a useful estimate:@xmath424{ll}% 2^{-m - r+g}(2r\\ln m)^{-1 } , & \\text{if}\\;g>\\frac{m - r}{2}+\\ln m,\\smallskip\\\\ 2^{-(m - r-2)/2}(2r\\ln m)^{-1/2 } , & \\text{if}\\;g<\\frac{m - r}{2}-\\ln m , \\end{array } \\right .",
    "\\label{mu - low}%\\ ] ]    @xmath183 thus , we see that for all @xmath314 variances @xmath425 have the same asymptotics and decline exponentially in @xmath426 as opposed to the weakest estimate ( [ small ] )",
    ". then we have @xmath427 which also satisfies ( [ gauss1 ] ) as @xmath312@xmath428    _ discussion .",
    "considering approximation ( [ gauss ] ) for a general path @xmath232 we arrive at the estimate @xmath429 according to theorem xvi.7.1 from @xcite , this approximation is valid if the number of standard deviations @xmath430 is small relative to the number @xmath431 of rv @xmath432 in the sum @xmath433 @xmath434 in particular , we can use ( [ gauss ] ) for the path @xmath435 since ( [ small ] ) gives @xmath436 @xmath183 2 .",
    "note that for @xmath437 variance @xmath438 in ( [ mu - low ] ) declines exponentially as @xmath151 moves away from @xmath159 . on the other hand",
    ",  we can satisfy  asymptotic condition ( [ gauss3 ] ) for any path @xmath407 if @xmath439 in ( [ gauss3 ] ) is replaced with parameter of a fixed degree as @xmath312 ] @xmath441 as @xmath312 we then use inequality @xmath442 valid for any @xmath407 instead of the weaker inequality ( [ gauss2 ] ) .",
    "thus , the bounds on probabilities @xmath288 rapidly decline as @xmath151 moves away from @xmath381 and the total block error rate @xmath292 also satisfies the same asymptotic bound ( [ gauss1])@xmath30    \\3 .",
    "note that  the same minimum residual @xmath399 can also be used for majority decoding .",
    "indeed , both the majority and the recursive algorithms are identical on the weakest path @xmath289 .",
    "namely , both algorithms first estimate the product of @xmath443 channel symbols and then combine @xmath444 different estimates in ( [ md2 ] ) .",
    "however , a substantial difference between the two algorithms is that recursive decoding uses the previous estimates to process any other path @xmath152 . because of this , the algorithm outperforms majority decoding in both the complexity and ber @xmath288 for any @xmath445    \\4 .",
    "theorem [ lm : fi ] almost entirely carries over to any @xmath446 .",
    "namely , we use the normal pdf @xmath447 for any @xmath397 . here",
    "any variance @xmath373 declines as @xmath4 grows",
    ". therefore we can always employ inequality ( [ gauss2 ] ) , by taking the _ maximum possible _ variance @xmath386 obtained in ( [ small ] ) . on the other hand ,",
    "asymptotic equality ( [ gauss ] ) becomes invalid as @xmath4 grows .    in this case ,",
    "tighter bounds ( say , the chernoff bound ) must be used on @xmath448 however , in this case , we also need to extend the second - order analysis of lemma [ lm : nei1 ] to exponential moments .",
    "such an approach can also give the asymptotic error probability @xmath288 for any @xmath449 @xmath450 however , finding the bounds on @xmath288 is an important issue still open to date .",
    "@xmath183    \\5 .",
    "it can be readily proven that for sufficiently large @xmath451 the variance @xmath438 becomes independent of  @xmath452 similar to the estimates obtained in the second line of ( [ mu - low ] ) .",
    "more generally , more and more paths yield almost equal contributions to the block error rate as @xmath4 grows .",
    "this is due to the fact that the neighboring paths exhibit similar performance on sufficiently good channels .",
    "now theorem 1 directly follows from theorem [ lm : fi ] .",
    "_ proof of theorem 1 .",
    "_  consider a channel with crossover probability @xmath453  for @xmath312 the output block error probability @xmath292 of the algorithm @xmath69 has the order @xmath106at most @xmath454 where @xmath160 is the number of information symbols .",
    "this number has polynomial order of @xmath455 on the other hand ,  formula ( [ gauss1 ] ) shows that @xmath419 declines faster than @xmath456 for any @xmath457 as a result , @xmath458    next , we note that the error patterns of weight @xmath459 or less occur with a probability @xmath460 since @xmath461 the above argument shows that decoder fails to decode only a vanishing fraction of error patterns of weight @xmath459 or less@xmath262    next , we need to prove that @xmath69 fails to correct nonvanishing fraction of errors of weight  @xmath52 or less .  in proving this , consider a higher crossover probability @xmath462 where@xmath463 for this @xmath464 our estimates ( [ t0 ] ) and ( [ gauss ] ) show that @xmath465 and @xmath466 also , according to ( [ tot3 ] ) , @xmath467 on the other hand , the central limit theorem shows that the errors of weight @xmath52 or more still occur with a vanishing probability @xmath468 thus , we see that @xmath69 necessarily fails on the weights  @xmath52 or less , since the weights over @xmath52 still give a vanishing contribution to the nonvanishing error rate @xmath419 . @xmath240",
    "before proceeding with a proof of theorem [ th:1 - 1 ] , we summarize three important points that will be used below to evaluate the threshold of the algorithm @xmath201 .    * 1 . *",
    "the received rv @xmath299 , all intermediate recalculations ( [ main ] ) , and end decodings on the right - end paths @xmath152 are identical in both algorithms @xmath201 and @xmath469    by contrast , any left - end path @xmath470 first arrives at some biorthogonal code @xmath471 of length @xmath472 and is then followed by the suffix @xmath473 also , let @xmath474 denote the @xmath236th codeword of @xmath475 where @xmath476 here we also assume that the first two codewords form the repetition code : @xmath477 for each @xmath478 define its support @xmath479  as the subset of positions that have symbols @xmath480 . here",
    "@xmath481 codewords with @xmath482 have support of the same size @xmath483 whereas @xmath484 also , below @xmath158 denotes any information symbol associated with a path @xmath152 .",
    "let the all - one codeword @xmath485 be transmitted and @xmath173 be received .",
    "consider the vector @xmath486 obtained on some left - end path @xmath149 that ends at the node @xmath487 by definition of md decoding , @xmath486 is incorrectly decoded into any @xmath488 with probability @xmath489 where @xmath490 in our probabilistic setting , each event @xmath491 is completely defined by the symbols @xmath492 which are i.i.d .",
    "recall that lemma [ lm : prob0 ] is `` algorithm - independent '' and therefore is  left intact in @xmath201 .",
    "namely , we again consider the events @xmath280 and @xmath281 from ( [ events ] ) .",
    "similarly to ( [ tot4 ] ) , we assume that all preceding decodings are correct and replace the unconditional error probability @xmath264 with its conditional counterpart @xmath493 this probability satisfies the  bounds@xmath494 here we take the probability of incorrect decoding into any single codeword @xmath474 as a lower bound ( in fact , below we choose @xmath482 ) , and the union bound as its upper counterpart .",
    "now we take parameters @xmath495    for rm codes with @xmath401 and fixed order @xmath8 used on a binary channel with crossover error probability @xmath496 algorithm @xmath201 gives for any path @xmath152 a vanishing bit error rate@xmath497 @xmath56    _ proof .",
    "_  consider any left - end path @xmath149 that ends at the node @xmath487 for any vector @xmath486 and any subset @xmath369  of @xmath498 positions , define the sum @xmath499 here @xmath500 form @xmath498 i.i.d .",
    "thus , the sum @xmath501 has the same pdf for any @xmath369 . in turn , this allows us to remove index @xmath369 from @xmath502 and use common notation @xmath503 then we rewrite bounds ( [ b - fi ] ) as@xmath504 equivalently , we use the normalized rv @xmath505 with expectation 1 and rewrite the latter bounds as@xmath506 similarly to the proof of theorem [ lm : fi ] , note that the sum @xmath328 also satisfies the central limit theorem for any @xmath507 and has pdf that tends to @xmath508 as @xmath312 thus , we see that @xmath288 depends only on the variance @xmath509 obtained on the sum @xmath328 of i.i.d .",
    "this variance can be found using calculations identical to those performed in lemmas [ lm : nei1 ] to [ lm : mu ] . in particular , for any @xmath151 we can invoke the proof of lemma [ lm : path ] , which shows that @xmath313 achieves its maximum on the leftmost path@xmath510 similarly to ( [ t0 ] )",
    ", we then find@xmath511 direct substitution @xmath512 in ( [ t14 ] ) gives @xmath513 now we almost repeat the proof of theorem [ lm : fi ] . for the first path @xmath435 we employ gaussian approximation @xmath514 as @xmath312 for maximum @xmath515 the latter inequality and ( [ main6 ] ) give the upper bound @xmath516 also , @xmath517 for any other path @xmath152 with @xmath518 we can use the same estimates in ( [ main6 ] ) due to the inequalities @xmath519 and @xmath520    finally , consider any path @xmath152 with @xmath521 in this case , we use chebyshev s inequality instead of gaussian approximation .",
    "again , for any node @xmath475 we can consider its leftmost path @xmath522 similarly to our previous calculations in ( [ t0 ] ) and ( [ mu1 ] ) , it can be verified that@xmath523 then for small @xmath524 , substitution @xmath525 gives the equality @xmath526 thus , we obtain chebyshev s inequality in the form @xmath527 and complete the proof , since bound ( [ log2 ] ) combines both estimates ( [ t15 ] ) and ( [ cheb3]).@xmath528    _ proof of theorem 2 .",
    "_  we repeat the proof of theorem 1 almost entirely .",
    "consider a channel with crossover probability @xmath529  as @xmath312 the output block error probability @xmath292 of the algorithm @xmath201 satisfies the estimate @xmath530 where the number @xmath160 of different paths @xmath152 is bounded by @xmath455 formula ( [ log2 ] ) shows that all @xmath288 decline exponentially in @xmath531 as a result , we obtain asymptotic estimate @xmath458 on the other hand , the error patterns of weight @xmath459 or less occur with a total probability that tends to @xmath532 so decoder fails to decode only a vanishing fraction of these error patterns .",
    "now we take a smaller residual@xmath533 and consider a channel with crossover probability @xmath534 then direct substitution of @xmath4 in ( [ t14 ] ) gives @xmath535 .",
    "then formula ( [ lo1 ] ) shows that the decoding block error rate is @xmath536 note also that errors of weight @xmath52 or more occur with vanishing probability .",
    "thus , @xmath69 fails on errors of weight @xmath52 or less .",
    "@xmath537    _ discussion .",
    "_    the proofs of theorems 1 and 2 also reveal the main shortcoming of our probabilistic technique , which employs rather loose estimates for probabilities @xmath538 indeed , the first two moments of the random variables @xmath260 give tight approximation only for gaussian rv .",
    "by contrast , error probabilities @xmath288 slowly decline as @xmath539 whenever chebyshev s inequality ( [ cheb3 ] ) is applied for small parameters @xmath422 as a result , we can obtain a vanishing block error rate only if @xmath540 this is the case of rm codes of fixed order @xmath1    by contrast , the number of information symbols @xmath160 is linear in @xmath0 for rm codes of fixed rate @xmath541 this fact does not allow us to extend theorems 1 and 2 for nonvanishing code rates .",
    "more sophisticated arguments - that include the moments @xmath542@xmath543 of  an arbitrary order @xmath236 - can be developed in this case .",
    "the end result of this study is that recursive decoding of rm codes @xmath17 of fixed rate @xmath544 achieves the error - correcting threshold @xmath545 this increases @xmath546 times the threshold of bounded distance decoding .",
    "however , the overall analysis becomes more involved and is beyond the scope of this paper .",
    "now consider an infinite sequence of optimal binary codes of a low code rate @xmath544 used on a channel with high crossover error probability @xmath547 . according to the shannon coding theorem , ml decoding of such a sequence gives a vanishing block error probability if @xmath548 where @xmath549 is the inverse ( binary ) entropy function .",
    "note that for @xmath550@xmath551 correspondingly , a vanishing block error probability is obtained for any residual @xmath552 next , recall that rm codes @xmath553 of fixed order @xmath8 have code rate @xmath554 for this rate , ml decoding of optimal codes gives @xmath555 thus , we see that optimal codes give approximately the same residual order ( [ ord - ml ] ) as the former order ( [ ml ] ) derived in @xcite for rm codes @xmath553 .",
    "in other words , rm codes of low rate @xmath544 can achieve nearly optimum performance for ml decoding .",
    "by contrast , low - complexity algorithm @xmath201 has a substantially higher residual that has the order of @xmath556 .",
    "this performance gap shows that further advances are needed for the algorithm @xmath70 the main problem here is whether possible improvements can be coupled with low complexity order of @xmath557    the performance gap becomes even more noticeable , if  a binary symmetric channel is considered as a `` hard - decision '' image of an awgn channel .",
    "indeed , let the input symbols @xmath558 be transmitted over a channel with the additive white gaussian noise @xmath559 for code sequences of rate @xmath560 we wish to obtain a vanishing block error probability when @xmath561 in this case , the transmitted symbols @xmath558 are interchanged with very high crossover probability @xmath562 which gives residual @xmath563 thus , @xmath564 serves ( up to a small factor of @xmath565 as a measure of noise power @xmath566  in particular , ml decoding operates at the above residual @xmath567 from ( [ ord - ml ] ) and can withstand noise power @xmath568 of order up to @xmath569    by contrast , algorithm @xmath201 can successfully operate only when noise power @xmath568 has the lower order of @xmath570  similarly , algorithm @xmath69 is efficient when @xmath568 is further reduced to the order of @xmath571 .  therefore for long rm codes , algorithm @xmath201 can increase @xmath571 times the noise power that can be sustained using the algorithm @xmath69 or majority decoding .",
    "however , performance of @xmath201 also degrades for longer blocks when compared to optimum decoding , though this effect is slower in @xmath201 than in other low - complexity algorithms known for rm codes .    for moderate lengths ,",
    "this relative degradation is less pronounced , and algorithm @xmath201 achieves better performance . in particular , some simulation results are presented in fig .",
    "[ fig2 ] to fig .",
    "[ fig4 ] for rm codes @xmath572 , @xmath573 , and @xmath574 , respectively . on the horizontal axis ,",
    "we plot both input parameters - the signal - to noise ratio @xmath575 of an awgn channel and the crossover error probability @xmath576 of the corresponding binary channel",
    ". the output code word error rates ( wer ) of algorithms @xmath69 and @xmath201  represent the first two ( rightmost ) curves@xmath262 decoding is performed on a binary channel , without using any soft - decision information .",
    "these simulation results show that @xmath201 gains about @xmath577 db over @xmath69  on the code @xmath573 and about  0.5 db on the code @xmath574 even for high wer . a subsequent improvement can be obtained if we consider _ soft - decision _ decoding@xmath349 which recursively recalculates the _ posterior probabilities _ of  the new variables obtained in both steps 1 and 2 .",
    "these modifications of algorithms @xmath69 and @xmath201 - called below @xmath578 and @xmath579 - are designed along these lines in @xcite .",
    "the simulation results for the algorithm @xmath580 are also presented in fig .",
    "[ fig2 ] to fig .",
    "[ fig4 ] , where these results are given by the third curve .",
    "this extra gain can be further increased if a few most plausible code candidates are recursively retrieved and updated in all intermediate steps .",
    "we note that the list decoding algorithms have been of substantial interest not only in the area of error control but also in the learning theory . for _ long _ _ biorthogonal codes _",
    "@xmath581 the pioneering randomized algorithm is presented in @xcite . for @xmath401 and",
    "@xmath582 @xmath583 this algorithm outputs a complete list of codewords located within the distance @xmath584  from any received vector , while taking only a polynomial time @xmath585 to complete this task with high probability @xmath586 substantial further advances are obtained for some low - rate @xmath587-ary rm codes in @xcite and the papers cited therein .    for binary rm codes of _ any order _ @xmath32 we mention three different soft decision list decoding techniques , all of which reduce the output wer at the expense of higher complexity .",
    "the algorithm of @xcite and @xcite reevaluates the most probable information subblocks on a _",
    "single run_. for eachpath @xmath232 the decoder - called below @xmath588 - updates the list of @xmath589 most probable information subblocks @xmath590 obtained on the previous paths @xmath591 this algorithm has overall complexity of order @xmath592 the technique of @xcite proceeds recursively at any intermediate node , by choosing @xmath589 codewords closest to the input vector processed at this node .",
    "these lists are updated in _",
    "multiple recursive runs .",
    "_ finally , the third novel technique @xcite executes sequential decoding using the main stack , but also utilizes the complementary stack in this process .",
    "the idea here is to lower - bound the minimum distance between the received vector and the closest codewords that will be obtained in the _ future steps_.    computer simulations show that the algorithm of @xcite achieves the best complexity - performance trade - off known to date for rm codes of moderate lengths 128 to 512 . in fig .",
    "[ fig2 ] to fig .",
    "[ fig4 ] , this algorithm @xmath588 is represented by the fourth curve , which shows a gain of about 2 db over @xmath593 .",
    "here we take @xmath594 in fig .",
    "[ fig2 ] and @xmath595 in fig .",
    "[ fig3 ] and [ fig4 ] .",
    "finally , complexity estimates ( given by the overall number of floating point operations ) are presented for all three codes in table 1 .",
    "[ c]|c|c|c|c|c|code & @xmath596 & @xmath597 & @xmath598 & @xmath599 + @xmath600 & 857 & 1264 & 6778 & 29602 , @xmath601 + @xmath602 & 1753 & 2800 & 16052 & 220285 , @xmath595 + @xmath603 & 2313 & 2944 & 12874 & 351657 , @xmath595 +      recall also that different information bits - even those retrieved in consecutive steps - become much better protected as recursion progresses .",
    "this allows one to improve code performance by considering a subcode of the original code , obtained after a few least protected information bits are removed .",
    "the corresponding simulation results can be found in @xcite and @xcite .    summarizing this discussion ,",
    "we outline a few important open problems .",
    "recall that the above algorithms @xmath69 and @xmath201  use two simple recalculation rules@xmath607 therefore the first important issue is to define whether any asymptotic gain can be obtained :          the second important problem is to obtain tight bounds on the decoding error probability in addition to the decoding threshold derived above .",
    "this is an open problem even for the simplest recalculations ( [ rule1 ] ) utilized in this paper , let alone other rules , such as ( [ boss ] ) or those outlined in @xcite .    from the practical perspective ,",
    "recursive algorithms show substantial promise at the moderate lengths up to 256 , on which they efficiently operate at signal - to - noise ratios below 3 db .",
    "it is also interesting to extend these algorithms for low - rate subcodes of  rm codes , such as the duals of the bch codes and other sequences with good auto - correlation .    in summary ,",
    "the main result of the paper is a new probabilistic technique that allows one to derive exact asymptotic thresholds of recursive algorithms .",
    "firstly , we disintegrate decoding process into a sequence of recursive steps .",
    "secondly , these dependent steps are estimated by independent events , which occur when all preceding decodings are correct .",
    "lastly , we develop a second - order analysis that defines a few weakest paths over the whole sequence of consecutive steps .",
    "i. dumer and k. shabunov , `` recursive constructions and their maximum likelihood decoding , '' _ _ proc .",
    "38__@xmath610 _  allerton conf . on commun .",
    ", cont . , and comp . , _ ,",
    "monticello , il , usa , 2000 , pp .",
    "71 - 80 .",
    "r. lucas , m. bossert , and a. dammann , `` improved soft - decision decoding of reed - muller codes as generalized multiple concatenated codes , '' _ proc .",
    "itg  conf . on source and channel coding , _",
    "aahen , germany , 1998 , pp .",
    "137 - 141 .",
    "n. stolte and u. sorger , `` soft - decision stack decoding of binary reed - muller codes with `` look - ahead '' technique , '' _ _ proc .",
    "7__@xmath610 _  int .",
    "workshop `` algebr . and comb .",
    "coding theory '' _ , bansko , bulgaria , 2000 , pp .",
    "293 - 298 ."
  ],
  "abstract_text": [
    "<S> recursive decoding techniques are considered for reed - muller ( rm ) codes of growing length @xmath0 and fixed order @xmath1 an algorithm is designed that has complexity of order @xmath2 and corrects most error patterns of weight up to @xmath3 given that @xmath4 exceeds @xmath5 this improves the asymptotic bounds known for decoding rm codes with nonexponential complexity .    to evaluate decoding capability , </S>",
    "<S> we develop a probabilistic technique that disintegrates decoding into a sequence of recursive steps . </S>",
    "<S> although dependent , subsequent outputs can be tightly evaluated under the assumption that all preceding decodings are correct . in turn , this allows us to employ the second - order analysis and find the error weights for which the decoding error probability vanishes on the entire sequence of decoding steps as the code length @xmath0 grows .    </S>",
    "<S> * keywords * - decoding threshold , plotkin construction , recursive decoding , reed - muller codes . </S>"
  ]
}