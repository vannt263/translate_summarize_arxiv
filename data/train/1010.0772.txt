{
  "article_text": [
    "in many applications , such as information retrieval or gene ranking , one is given a finite set of data of interest sharing a particular property , and wishes to find other data sharing the same property . in information retrieval ,",
    "for example , the finite set can be a user query , or a set of documents known to belong to a specific category , and the goal is to scan a large database of documents to identify new documents related to the query or belonging to the same category . in gene ranking , the query is a finite list of genes known to have a given function or to be associated to a given disease , and the goal is to identify new genes sharing the same property @xcite .",
    "in fact this setting is ubiquitous in many applications where identifying a data of interest is difficult or expensive , e.g. , because human intervention is necessary or expensive experiments are needed , while unlabeled data can be easily collected . in such cases there is a clear opportunity to alleviate the burden and cost of interesting data identification with the help of machine learning techniques .    more formally ,",
    "let us assign a binary label to each possible data : positive ( @xmath0 ) for data of interest , negative ( @xmath1 ) for other data .",
    "unlabeled data are data for which we do not know whether they are interesting or not . denoting @xmath2 the set of data",
    ", we assume that the `` query '' is a finite set of data @xmath3 with positive labels , and we further assume that we have access to a ( possibly large ) set @xmath4 of unlabeled data .",
    "our goal is to learn , from @xmath5 and @xmath6 , a way to identify new data with positive labels , a problem often referred to as _",
    "pu learning_. more precisely we make a distinction between two flavors of pu learning :    * _ inductive pu learning _",
    ", where the goal is to learn from @xmath5 and @xmath6 a function @xmath7 able to associate a score or probability to be positive @xmath8 to any data @xmath9 . this may typically be the case in an image or document classification system , where a subset of the web is used as unlabeled set @xmath6 to train the system , which must then be able to scan any new image or document .",
    "* _ transductive pu learning _ , where the goal is estimate a scoring function @xmath10 from @xmath5 and @xmath6 , i.e. , where we are just interested is finding positive data in the set @xmath6 .",
    "this is typically the case in the disease gene ranking application , where the full set of human genes is known during training and split between known disease genes @xmath5 and the rest of the genome @xmath6 . in that case",
    "we are only interested in finding new disease genes in @xmath6 .",
    "several methods for pu learning , reviewed in section [ sec : related ] below , reduce the problem to a binary classification problem where we learn to discriminate @xmath5 from @xmath6 .",
    "this can be theoretically justified , at least asymptotically , since the log - ratio between the conditional distributions of positive and unlabeled examples is monotonically increasing with the log - ratio of positive and negative examples @xcite , and has given rise to state - of - the - art methods such as _ biased support vector machine ( biased svm ) _",
    "@xcite or weighted logistic regression @xcite .",
    "although this reduction suggests that virtually any method for ( weighted ) supervised binary classification can be used to solve pu learning problems , we put forward in this paper that some methods may be more adapted than others in a non - asymptotic setting , due to the particular structure of the unlabeled class . in particular , we investigate the relevance of methods based on aggregating classifiers trained on artificially perturbed training sets , in the spirit of bagging @xcite . such methods are known to be relevant to improve the performance of unstable classifiers , a situation which , we propose , may occur particularly in pu learning . indeed , in addition to the usual instability of learning algorithms confronted to a finite - size training sets , the content of a random subsample of unlabeled data in positive and negative examples is likely to strongly affect the classifier , since the contamination of @xmath6 in positive examples makes the problem more difficult .",
    "variations in the contamination rate of @xmath6 may thus have an important impact on the trained classifier , a situation which bagging - like classifiers may benefit from .",
    "based on this idea , we propose a general and simple scheme for inductive pu learning , akin to an asymetric form of bagging for supervised binary classification .",
    "the method , which we call _ bagging svm _ , consists in aggregating classifiers trained to discriminate @xmath5 from a small random subsample of @xmath6 , where the size of the random sample plays a specific role .",
    "this method can naturally be adapted to the transductive pu learning framework .",
    "we demonstrate on simulated and real data that bagging svm performs at least as well as existing methods for pu learning , while being often faster in particular when @xmath11 .",
    "this paper is organized as follows . after reviewing related work in section [ sec : related ] , we present the bagging svm for inductive pu learning in section [ sec : inductive ] , and its extension to transductive pu learning in section [ sec : transductive ] .",
    "experimental results are presented in [ sec : res ] , followed by a discussion in section [ sec : discussion ] .",
    "a growing body of work has focused on pu learning recently .",
    "the fact that only positive and unlabeled examples are available prevents a priori the use of supervised classification methods , which require negative examples in the training set . a first approach to overcome the lack of negative examples is to disregard unlabeled examples during training and simply learn from the positive examples , e.g. , by ranking the unlabeled examples by decreasing similarity to the mean positive example @xcite or using more advanced learning methods such as 1-class svm @xcite    alternatively , the problem of inductive pu learning has been studied on its own from a theoretical viewpoint @xcite , and has given rise to a number of specific algorithms .",
    "several authors have proposed two - step algorithms , heuristic in nature , which first attempt to identify negative examples in the unlabeled set , and then estimate a classifier from the positive , unlabeled and likely negative examples @xcite .",
    "alternatively , it was observed that directly learning to discriminate @xmath5 from @xmath6 , possibly after rebalancing the misclassification costs of the two classes to account for the asymetry of the problem , leads to state - of - the - art results for inductive pu learning .",
    "this approach has been studied , with different weighting schemes , using a logistic regression or a svm as binary classifier @xcite .",
    "inductive pu learning is also related to and has been used for novelty detection , when @xmath5 is interpreted as `` normal '' data and @xmath6 contains mostly positive examples @xcite , or to data retrieval from a single query , when @xmath5 is reduced to a singleton @xcite .",
    "transductive pu learning is arguably easier than inductive pu learning , since we know in advance the data to be screened for positive labels .",
    "many semi - supervised methods have been proposed to tackle transductive learning when both positive and negative examples are known during training , including transductive svm @xcite , or many graph - based methods , reviewed by @xcite .",
    "comparatively little effort has been devoted to the specific transductive pu learning problem , with the notable exception of @xcite , who call the problem _ partially supervised classification _ and proposes an iterative method to solve it , and @xcite who formulate the problem as a combinatorial optimization problem over a graph .",
    "finally , @xcite recently proposed a bagging approach which shares similarities with ours , but is more complex and was only tested on a specific application .",
    "our starting point to learn a classifier in the pu learning setting is the observation that learning to discriminate positive from unlabeled samples is a good proxy to our objective , which is to discriminate positive from negative samples .",
    "even though the unlabeled set is contaminated by hidden positive examples , it is generally admitted that its distribution contains some information which should be exploited .",
    "that is for instance , the foundation of semi - supervised methods .",
    "indeed , let us assume for example that positive and negative examples are randomly generated by class - conditional distributions @xmath12 and @xmath13 with densities @xmath14 and @xmath15 .",
    "if we model unlabeled examples as randomly sampled from @xmath12 with probability @xmath16 and from @xmath13 with probability @xmath17 , then the distribution of unlabeled has a density @xmath18 now notice that @xmath19 showing that the log - ratio between the conditional distributions of positive and unlabeled examples is monotonically increasing with the log - ratio of positive and negative examples @xcite .",
    "hence any estimator of the conditional probability of positive vs. unlabeled data should in theory also be applicable to discriminate positive from negative examples .",
    "this is the case for example of logistic regression or some forms of svm @xcite . in practice",
    "it seems useful to train classifiers to discriminate @xmath5 from @xmath6 by penalizing more false negative than false positive errors , in order to account for the fact that positive examples are known to be positive , while unlabeled examples are known to contain hidden positives . using soft margin svm",
    "while giving high weights to false negative errors and low weights to false positive errors leads to the biased svm approach described by @xcite , while the same strategy using a logistic regression leads to the weighted logistic regression approach of @xcite .",
    "both methods , tested on text categorization benchmarks , were shown to be very efficient in practice , and in particular outperformed all approaches based on heuristic identifications of true negatives in @xmath6 .    among the many methods for supervised binary classification which could be used to discriminate @xmath5 from @xmath6 , bootstrap aggregating or `` bagging '' is an interesting candidate @xcite .",
    "the idea of bagging is to estimate a series of classifiers on datasets obtained by perturbing the original training set through bootstrap resampling with replacement , and to combine these classifiers by some aggregation technique .",
    "the method is conceptually simple , can be applied in many settings , and works very well in practice @xcite .",
    "bagging generally improves the performance of individual classifiers when they are not too correlated to each other , which happens in particular when the classifier is highly sensitive to small perturbations of the training set .",
    "for example , @xcite showed that the difference between the expected mean square error ( mse ) of a classifier trained on a single bootstrap sample and the mse of the aggregated predictor increases with the variance of the classifier .",
    "we propose that , by nature , pu learning problems have a particular structure that leads to instability of classifiers , which can be advantageously exploited by a bagging - like procedure which we now describe . intuitively , an important source of instability in pu learning situations is the empirical contamination @xmath20 of @xmath6 with positive examples , i.e. , the percentage of positive examples in @xmath6 which on average equals @xmath16 in ( [ eq : gamma ] ) .",
    "if by chance @xmath6 is mostly made of negative examples , i.e. , has low contamination by positive examples , then we will probably estimate a better classifier than if it contains mostly positive examples , i.e. , has high contamination .",
    "moreover , we can expect the classifiers in these different scenarii to be little correlated , since intuitively they estimate different log - ratios of conditional distribution .",
    "hence , in addition to the `` normal '' instability of a classifier trained on a finite - size sample , which is exploited by bagging in general , we can expect an increased instability in pu learning due to the sensitivity of the classifier to the empirical contamination @xmath20 of @xmath6 in positive examples . in order to exploit this sensitivity in a bagging - like procedure",
    ", we propose to randomly subsample @xmath6 and train classifiers to discriminate @xmath5 from each subsample , before aggregating the classifiers . by subsampling @xmath6",
    ", we hope to vary in particular the empirical contamination between samples .",
    "this will induce a variety of situations , some lucky ( small contamination ) , some less lucky ( large contamination ) , which eventually will induce a large variability in the classifiers that the aggregation procedure can then exploit .    in opposition to classical",
    "bagging , the size @xmath21 of the samples generated from @xmath6 may play an important role to balance the accuracy against the stability of individual classifiers . on the one hand ,",
    "larger subsamples should lead on average to better classifiers , since any classification method generally improves on average when more training points are available . on the other hand",
    ", the empirical contamination varies more for smaller subsamples .",
    "more precisely , let us denote by @xmath20 the true contamination rate in @xmath6 , that is , the true proportion of positive examples hidden in @xmath6 . whenever a bootstrap sample @xmath22 of size @xmath21 is drawn from @xmath6 , its empirical number of positive examples is a binomial random variable @xmath23 , leading to a contamination rate @xmath24 with mean and variance : @xmath25 smaller values of @xmath21 therefore increase the proportion of `` lucky '' subsamples , and more generally the variability of classifiers , a property which is beneficial for the aggregation procedure .",
    "finally this suggests that the size @xmath21 of subsample is a parameter whose effect should be studied and perhaps tuned .    in summary ,",
    "the method we propose for pu learning is presented in algorithm [ algobootinduct ] .",
    "we call it bagging svm when the classifier used to discriminate @xmath5 from a random subsample of @xmath6 is a biased svm",
    ". it is akin to bagging to learn to discriminate @xmath5 from @xmath6 , with two important specificities .",
    "first , only @xmath6 is subsampled .",
    "this is to account for the fact that elements in @xmath5 are known to be positive , and moreover that the number of positive examples is often limited.second , the size of subsamples is a parameter @xmath21 whose effect needs to be studied .",
    "if an optimal value exists , then this parameter may need to be adjusted .",
    "the number @xmath26 of bootstrap samples is also a user - defined parameter .",
    "intuitively , the larger @xmath26 the better , although we observed empirically little improvement for @xmath26 larger than @xmath27 . finally ,",
    "although we propose to aggregate the @xmath26 classifiers by a simple average , other aggregation rules could easily be used . on preliminary experiments on simulated and real data ,",
    "we did not observed significant differences between the simple average and majority voting , another popular aggregation method .",
    "we now consider the situation where the goal is only to assign a score to the elements of @xmath6 reflecting our confidence that these elements belong to the positive class .",
    "@xcite have studied this same problem which they call `` partially supervised classification '' . their proposed technique combines naive bayes classification and the expectation - maximization algorithm to iteratively produce classifiers .",
    "the training scores of these classifiers are then directly used to rank @xmath6 . following this approach , a straightforward solution to the transductive pu learning problem is to train any classifier to discriminate between @xmath5 and @xmath6 and to use this classifier to assign a score to the unlabeled data that were used to train it . using svms",
    "this amounts to using the biased svm training scores .",
    "we will subsequently denote this approach by transductive biased svm .    however , one may argue that assigning a score to an unlabeled example that has been used as negative training example is problematic .",
    "in particular , if the classifier fits too tightly to the training data , a false negative @xmath28 will hardly be given a high training score when used as a negative . in a related situation in the context of semi - supervised learning",
    ", @xcite showed for example that unlabeled examples used as negative training examples tend to have underestimated scores when a svm is trained with the classical hinge loss . more generally , most theoretical consistency properties of machine learning algorithms justify predictions on samples outside of the training set , raising questions on the use of all unlabeled samples as negative training samples at the same time .",
    "alternatively , the inductive bagging pu learning lends itself particularly well to the transductive setting , through the procedure described in algorithm [ algoboottransduc ] . each time a random subsample @xmath22 of @xmath6 is generated , a classifier is trained to discriminate @xmath5 from @xmath22 , and used to assign a predictive score to any element of @xmath29 . at the end the score of any element @xmath30 is obtained by aggregating the predictions of the classifiers trained on subsamples that did not contain @xmath31 ( the counter @xmath32 simply counts the number of such classifiers ) . as such , no point of @xmath6 is used simultaneously to train a classifier and to test it . in practice , it is useful to ensure that all elements of @xmath6 are not too often in @xmath22 , in order to average the predictions over a sufficient number of classifiers .    :",
    "@xmath33 , @xmath34 , @xmath35 size of bootstrap samples , @xmath36 number of bootstraps + : a function @xmath37 draw a subsample @xmath38 of size @xmath21 from @xmath34 .",
    "+ train a classifier @xmath39 to discriminate @xmath33 against @xmath38 . + return @xmath40    : @xmath33 , @xmath34 , @xmath35 size of bootstrap samples , @xmath36 number of bootstraps + : a score @xmath41 initialize @xmath42 draw a bootstrap sample @xmath38 of size @xmath21 in @xmath34 .",
    "+ train a classifier @xmath39 to discriminate @xmath33 against @xmath38 .",
    "+ for any @xmath43 , update : @xmath44 return @xmath45 for @xmath46",
    "in this section we investigate the empirical behavior of our bagging algorithm on one simulated dataset ( section [ sec : toydataset ] ) and two real applications : text retrieval with the 20 newsgroup benchmark ( section 5.2 ) , and reconstruction of gene regulatory networks ( section 5.3 ) .",
    "we compare the new bagging svm to the state - of - the - art biased svm , and also add in the comparison for real data two one - class approaches , namely , ranking unlabeled examples by decreasing mean similarity to the positive examples ( called _ baseline _ below ) , and the one - class svm @xcite . both bagging and biased methods involve an svm with asymetric penalties @xmath47 and @xmath48 for the positive and negative class , respectively . by default",
    "we always set them to ensure that the total penalty is equal for the two classes , i.e. , @xmath49 , where @xmath50 and @xmath51 are the number of positive and negative examples fed to the svm , and optimized the single parameter @xmath52 over a grid .",
    "we checked on all experiments that this choice was never significantly outperformed by other penalty ratio @xmath53 .",
    "a first series of experiments were conducted on simulated data to compare our bagging procedure to the biased approach in an inductive setting .",
    "we consider the simple situation where the positive examples are generated from an isotropic gaussian distribution in @xmath54 : @xmath55 , with @xmath56 and @xmath57 , while the negative examples are generated from another gaussian distribution with same isotropic covariance and a different mean , of norm @xmath58 .",
    "we replicate the following iteration @xmath59 times for different values of @xmath16  :    * draw a sample @xmath5 of 5 positives examples , and a sample @xmath6 of 50 unlabeled examples from @xmath60 .",
    "* train respectively the biased and bagging logit ( with 200 bootstraps ) . *",
    "compare their performance on a test set of 1000 examples containing @xmath61 positives .    for @xmath21 ,",
    "we tested equally spaced values between 1 and 50 , and we varied @xmath16 on the interval @xmath62 $ ] .",
    "the performance is measured by computing the area under the receiving operator characteristic curve ( auc ) on the independent test set .",
    "figure [ fig : toyresults ] ( left ) shows the performance of bagging logit for different levels of contamination of @xmath6 , as a function of @xmath21 , the size of the random samples .",
    "the uppermost curve thus corresponds to @xmath63 , i.e. , the case where all unlabeled data are negative , while the bottom curve corresponds to @xmath64 , i.e. , the case where @xmath65 of unlabeled data are positive .",
    "note that @xmath66 corresponds to classical bagging on the biased logit classifier , i.e. , to the case where all unlabeled examples are used to train the classifier .",
    "we observe that in the classical setting of supervised binary classification where @xmath6 is not contaminated by positive samples ( @xmath63 ) , the bagging procedure does not improve performance , whatever the size of the bootstrap samples . on the other hand , as contamination increases , we observe an overall decrease of the performance , confirming that the classification problem becomes more difficult when contamination increases .",
    "in addition , the bagging logit always succeeds in reaching at least the same performance for some value of @xmath21 below @xmath59 , even for high rates of contamination .",
    "figure [ fig : toyresults ] ( right ) shows the evolution of auc as @xmath16 increases , for both methods . for the bagging logit we report the auc reached for the best @xmath21 value .",
    "we see that bagging logit slightly outperforms biased logit method .    ,",
    "the size of the bootstrap samples , on simulated data . each curve , from top to bottom , corresponds to a contamination level @xmath67 .",
    "_ right _ performance of two methods as a function of @xmath16 , the contamination level , on simulated data .",
    "the performance of bagging logit was taken at the optimal @xmath21 value.,title=\"fig:\",width=264 ] , the size of the bootstrap samples , on simulated data .",
    "each curve , from top to bottom , corresponds to a contamination level @xmath67 .",
    "_ right _ performance of two methods as a function of @xmath16 , the contamination level , on simulated data .",
    "the performance of bagging logit was taken at the optimal @xmath21 value.,title=\"fig:\",width=264 ]    to further illustrate the assumption that motivated bagging svm , namely that decreasing @xmath21 would decrease the average performance of single classifiers but would increase their variance due to the variations in contamination , we show in figure [ fig : aucgammahat ] a scatter plot of the auc of individual classifiers as a function of the empirical contamination of the bootstrap sample @xmath20 , for two values of @xmath21 ( @xmath68 and @xmath69 ) . here",
    "the mean contamination was set to @xmath70 .",
    "obviously , the variations of @xmath20 are much larger for @xmath71 ( between @xmath72 and @xmath73 ) than for @xmath74 ( between @xmath75 and @xmath76 ) .",
    "the correlation coefficient between @xmath20 and the performance ( reported above each plot ) is strongly negative , in particular for smaller @xmath21 .",
    "it is quite clear that less contaminated subsamples tend to yield better classifiers , and that the variation in the contamination is an important factor to increase the variance between individual predictors , which aggregation can benefit from .     over the 500 iterations of one bootstrap loop on the simulated dataset , @xmath70 .",
    "]      the 20 newsgroup benchmark is widely used to test pu learning methods .",
    "the version we used is a collection of 11293 articles partitioned into 20 subsets of roughly the same size ( around 500 ) , corresponding to post articles of related interest . for each newsgroup ,",
    "the positive class consists of those @xmath77500 articles known to be relevant , while the negative class is made of the remainder .",
    "after pre - processing , each article is represented by a @xmath78-dimensional vector , using the tfidf representation over a dictionnary of @xmath78 words @xcite .    to simulate a pu learning problem",
    ", we applied the following strategy . for a given newsgroup",
    ", we created a set @xmath5 of known positive examples by randomly selecting a given number of positive examples , while @xmath6 contains the non - selected positive examples and all negative examples .",
    "we varied the size @xmath79 of @xmath5 in @xmath80 to investigate the influence of the number of known positive examples .",
    "for each newsgroup and each value of @xmath79 , we train all 4 methods described above ( bagging svm , biased svm , baseline , one - class svm ) and rank the samples in @xmath6 by decreasing score ( transductive setting ) .",
    "we then compute the area under the roc curve ( auc ) , and average this measure over 10 replicates of each newsgroup and each value of @xmath79 .",
    "for bagging and biased svm , we varied the @xmath81 parameter over the grid @xmath82 $ ] , while we vary parameter @xmath83 in @xmath84 $ ] for @xmath58-class svm .",
    "we only used the linear kernel .",
    "we first investigated the influence of @xmath26 .",
    "figure [ fig : bootstrapsize ] shows , for the first newsgroup , the performance reached as a function of @xmath26 , for different settings in @xmath79 and @xmath21 .",
    "as expected we observe that in general the performance increases with @xmath26 , but quickly reaches a plateau beyond which additional bootstraps do not improve performance .",
    "overall the smaller @xmath21 , the larger @xmath26 must be to reach the plateau . from these preliminary results we set @xmath85 for @xmath86 , and @xmath87 for @xmath88 , and kept it fix for the rest of the experiments .",
    "to further clarify the benefits of bagging , we show in figure [ fig : baggingimprove ] the performance of the bagging svm versus the performance of a svm trained on a single bootstrap sample ( @xmath89 ) , for different values of @xmath21 and a fixed number of positives @xmath90 .",
    "we observe that , for @xmath21 below @xmath91 , aggregating classifiers over several bootstrap subsamples is clearly beneficial , while for larger values of @xmath21 it does not really help .",
    "this is coherent with the observation that svm usually rarely benefit from bagging : here the benefits come from our particular bagging scheme .",
    "interestingly , we see that very good performance is reached even for small values of @xmath21 with the bagging .",
    ", for different values of @xmath79 and @xmath21.,width=604,height=359 ]    .,title=\"fig:\",width=340,height=207 ] [ fig : baggingimprove ]    figure [ fig : aucbaggingk_vs_biased ] shows the mean auc averaged over the 10 folds and the 20 newsgroups for bagging svm as a function of @xmath21 , and compares it to that of the biased svm .",
    "more precisely , each point on the curve corresponds to the performance averaged over the 20 newsgroups after choosing a posteriori the best @xmath81 parameter for each newsgroup .",
    "this is equivalent to comparing optimal cases for both methods .",
    "contrary to what we observed on simulated date , we observe that @xmath21 has in general very little influence on the performance .",
    "the auc of the bagging svm is similar to that of the biased svm for most values of @xmath21 , although for @xmath79 larger than @xmath59 , a slight advantage can be observed for the biased svm over bagging svm when @xmath21 is too small .",
    "we conclude that in practice , parameter @xmath21 may not need to be finely tuned and we advocate to keep it moderate . in all cases",
    ", @xmath92 seems to be a safe choice for the bagging svm .",
    "finally , figure [ fig : comp4methods ] shows the average auc over the 20 newsgroups for all four methods , as a function of @xmath79 .",
    "overall all methods are very similar , with the baseline slightly below the others . in details",
    ", the bagging svm curve dominates all other methods for @xmath93 , while the @xmath58-class svm is the one which dominates for smaller values of @xmath79 .",
    "although the differences in performance are small , the bagging svm outperforms the biased svm significantly for @xmath94 according to a wilcoxon paired sample test ( at @xmath95 confidence ) . for small values of @xmath79 however , no significant difference can be proven in either way between bagging svm and @xmath58-class svm , which remains a very competitive method .    .",
    "the dashed horizontal lines show the auc level of the biased svm .",
    "the curves are plotted for different values of @xmath79 , the size of the positive set.,width=283,height=226 ]    -class svm , the biased svm and the newly proposed bagging svm methods on the 20 newsgroups dataset .",
    "each curve shows how the mean auc varies with the number of positive training examples @xmath79 . for each value of @xmath79 ,",
    "the performance of bagging svm is computed at the optimal value for @xmath21 , as shown in figure [ fig : aucbaggingk_vs_biased].,width=283,height=226 ]      in this section we test the different pu learning strategies on the problem of inferring the transcription regulatory network of the bacteria _ escherichia coli _ from gene expression data .",
    "the problem is , given a transcription factor ( tf ) , to predict which genes it regulates .",
    "following @xcite , we can formulate this problem as transductive pu learning by starting from known regulated genes ( considered positive examples ) , and looking for additional regulated genes in the bacteria s genome .    to represent the genes",
    ", we use a compendium of microarray expression profiles provided by @xcite , in which 4345 genes of the _ e. coli _ genome are represented by vectors in dimension @xmath96 , corresponding to their expression level in 445 different experiments .",
    "we extracted the list of known regulated genes for each tf from regulondb @xcite .",
    "we restrict ourselves to @xmath97 tfs with at least @xmath98 known regulated genes .    for each tf , we ran a double 3-fold cross validation with an internal loop on each training set to select parameter @xmath81 of the svm ( or @xmath83 for the @xmath58-class svm ) .",
    "following @xcite , we normalize the expression data to unit norm , use a gaussian rbf kernel with @xmath99 , and perform a particular cross - validation scheme to ensure that operons are not split between folds .",
    "finally , following our previous results on simulated data and the newsgroup benchmark , we test two variants of bagging svm , setting @xmath21 successively to @xmath79 and @xmath100 .",
    "these choices are denoted respectively by _",
    "bagging1 svm _ and _",
    "bagging5 svm_.    figure [ fig : precisionecoli ] shows the average precision / recall curves of all methods tested .",
    "overall we observe that all three pu learning methods give significantly better results than the two methods which use only positive examples ( wilcoxon paired sample test at 5% significance level ) .",
    "no significant difference was found between the three pu learning methods .",
    "this confirms again that for different values of @xmath21 bagging svm matches the performance of biased svm .",
    "-class svm and the baseline method.,title=\"fig : \" ] [ fig : precisionecoli ]",
    "the main contribution of this work is to propose a new method , bagging svm , both for inductive and transductive pu learning , and to assess in detail its performance and the influence of various parameters on simulated and real data .    the motivation behind bagging svm was to exploit an intrinsic feature of pu learning to benefit from classifier aggregation through a random subsample strategy . indeed , by randomly sampling @xmath21 examples from the unlabeled examples",
    ", we can expect various contamination rates , which in turn can lead to very different single classifiers ( good ones when there is little contamination , worse ones when contamination is high ) .",
    "aggregating these classifiers can in turn benefit from the variations between them .",
    "this suggests that @xmath21 may play an important role in the final performance of bagging svm , since it controls the trade - off between the mean and variance of individual classifiers . while we showed on simulated data that this is indeed the case , and that there can be some optimum @xmath21 to reach the best final accuracy",
    ", the two experiments on real data did not show any strong influence of @xmath21 and suggested that @xmath92 may be a safe default choice .",
    "this is a good news since it does not increase the number of parameters to optimize for the bagging svm and leads to balanced training sets that most classification algorithms can easily handle .    the comparison between different methods is mitigated .",
    "while bagging svm outperforms biased svm on simulated data , they are not significantly different on the two experiments with real data .",
    "interestingly , while these pu learning methods were significantly better than two methods that learned from positive examples only on the gene regulatory network example , the @xmath58-class svm behaved very well on the 20 newsgroup benchmark , even outperforming the pu learning methods when less than @xmath68 training examples were provided . many previous works , including @xcite and @xcite discard @xmath58-class svms for showing a bad performance in terms of accuracy , while @xcite report the lack of robustness of this method arguing that it has proved very sensitive to changes of parameters .",
    "our results suggest that there are cases where it remains very competitive , and that pu learning may not always be a better strategy than simply learning from positives .",
    "finally , the main advantage of bagging svm over biased svm is the computation burden , in particular when there are far more unlabeled than positive examples .",
    "indeed , a typical algorithm , such as an svm , trained on @xmath101 samples , has time complexity proportional to @xmath102 , with @xmath103 between @xmath104 and @xmath105 . therefore , biased svm has complexity proportional to @xmath106 while bagging svm s complexity is proportional @xmath107 . with the default choice",
    "@xmath108 ratio of cpu time to train the biased svm vs the bagging svm can therefore be expected to be @xmath109 .",
    "then we conclude that bagging svm should be faster than biased svm as soon as @xmath110 .",
    "for example , taking @xmath85 and @xmath111 , bagging svm should be faster than biased svm as soon as @xmath112 , a situation very often encountered in practice where the ratio @xmath113 is more likely to be several orders of magnitude larger .",
    "in the two real datasets , this was always the case .",
    "table [ tab : cputimes ] reports cpu time and performance measure for training bagging svm on the first fold of newsgroup 1 with @xmath81 fixed at its best value a posteriori and @xmath90 .      in comparison ,",
    "the biased svm s cpu time is 227s for @xmath114 and @xmath115 .",
    "this confirms that for reasonable values of @xmath26 and @xmath21 , the bagging svm is much faster than the biased svm for a comparable performance .                    c.  elkan and k.  noto .",
    "learning classifiers from only positive and unlabeled data . in _",
    "kdd 08 : proceeding of the 14th acm sigkdd international conference on knowledge discovery and data mining _ , pages 213220 , new york , ny , usa , 2008 .",
    "acm .",
    "t.  joachims .",
    "transductive inference for text classification using support vector machines . in _",
    "icml 99 : proceedings of the sixteenth international conference on machine learning _ , pages 200209 , san francisco , ca , usa , 1999 .",
    "morgan kaufmann publishers inc .",
    "isbn 1 - 55860 - 612 - 2 .",
    "t.  joachims . a probabilistic analysis of the rocchio algorithm with tfidf for text categorization . in _",
    "icml 97 : proceedings of the fourteenth international conference on machine learning _ , pages 143151 , nashville , tennessee , usa , 1997 .",
    "morgan kaufmann publishers inc .",
    "w.  s. lee and b.  liu .",
    "learning with positive and unlabeled examples using weighted logistic regression . in t.",
    "fawcett and n.  mishra , editors , _ machine learning , proceedings of the twentieth international conference ( icml 2003 _ , pages 448455 .",
    "aaai press , 2003 .",
    "x.  li and b.  liu . learning to classify texts using positive and unlabeled data . in _",
    "ijcai03 : proceedings of the 18th international joint conference on artificial intelligence _ , pages 587592 , san francisco , ca , usa , 2003 .",
    "morgan kaufmann publishers inc .",
    "liu et  al .",
    "partially supervised classification of text documents . in _",
    "icml 02 : proceedings of the nineteenth international conference on machine learning _ , pages 387394 , san francisco , ca , usa , 2002 .",
    "morgan kaufmann publishers inc .",
    "isbn 1 - 55860 - 873 - 7 .                c.  scott and g.  blanchard .",
    "novelty detection : unlabeled data definitely help . in d.",
    "van dyk and m.  welling , editors , _ proceedings of the twelfth international conference on artificial intelligence and statistics ( aistats ) 2009 _ , volume  5 , pages 464471 , clearwater beach , florida , 2009 .",
    "jmlr : w&cp 5 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of learning a binary classifier from a training set of positive and unlabeled examples , both in the inductive and in the transductive setting . </S>",
    "<S> this problem , often referred to as _ pu learning _ </S>",
    "<S> , differs from the standard supervised classification problem by the lack of negative examples in the training set . </S>",
    "<S> it corresponds to an ubiquitous situation in many applications such as information retrieval or gene ranking , when we have identified a set of data of interest sharing a particular property , and we wish to automatically retrieve additional data sharing the same property among a large and easily available pool of unlabeled data . </S>",
    "<S> we propose a conceptually simple method , akin to bagging , to approach both inductive and transductive pu learning problems , by converting them into series of supervised binary classification problems discriminating the known positive examples from random subsamples of the unlabeled set . </S>",
    "<S> we empirically demonstrate the relevance of the method on simulated and real data , where it performs at least as well as existing methods while being faster . </S>"
  ]
}