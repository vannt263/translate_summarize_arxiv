{
  "article_text": [
    ", @xmath97 , @xmath98 , and @xmath99 conditioned to node @xmath41 can be broken in the sum of pairwise conditional entropies .",
    "each observed node is responsible for one of these contributions , in the sense that the variable associated with it is conditioned to a variable associated with another node .",
    "the node that acts on the conditional part of each pairwise entropy is given either by the first observed node encountered along the path towards @xmath41 or by node @xmath41 itself . in the illustration above ,",
    "the path connecting @xmath99 to @xmath41 passes through node @xmath98 , giving rise to the contribution @xmath100 .",
    "the paths connecting @xmath98 and @xmath97 to @xmath41 pass both through node @xmath46 , giving rise to the contributions @xmath101 and @xmath102 , respectively .",
    "there are instead no observed nodes along the path between node @xmath46 and node @xmath41 , thus the contribution of node @xmath46 to the joint entropy is @xmath103 . , scaledwidth=48.0% ]    in that case the second term of eq .",
    "( [ eq : alg1 ] ) is @xmath104 . by using the chain rule",
    "it can be written as @xmath105 where the last step takes into account the tree topology . again",
    "the consideration of the topology indicates that of @xmath97 , conditioned to the value of @xmath46 is independent from the values of @xmath98 and @xmath99 conditioned to @xmath46 ( conditional independence ) .",
    "hence @xmath106 the application of the chain rule on the last term leads to @xmath107 the total conditional entropy is thus reduced to the sum of pairwise conditional entropies , one for each observed node .",
    "the argument can be readily generalized to any tree , by multiple applications of the chain rule and of conditional independence , leading to @xmath108 in the sum , the variable associated with each observed node @xmath46 is conditioned to a variable associated with another node @xmath47 .",
    "the node @xmath48 that acts on the conditional part of each pairwise entropy is given either by the first observed node encountered along the path towards @xmath41 or by node @xmath41 itself .",
    "notice that dashed lines in fig .",
    "[ fig1 ] need not to be direct connections : other unobserved nodes may lie between observed ones .",
    "our algorithm requires us to have prior knowledge of the unconditional entropy of every single node in the graph .",
    "the computation of these quantities scales as @xmath109 .",
    "the algorithm also requires prior knowledge of the pairwise conditional entropy among all pairs of nodes , whose computation scales as @xmath110 . at stage @xmath30 of the algorithm , we need to find the node @xmath41 that maximizes the entropy @xmath111 .",
    "this means that we have to compute the function for every unobserved node .",
    "we thus have to walk back on the tree from any already observed node towards the node whose entropy is being computed , visiting all edges @xmath112 of the tree , with an algorithm that scales as @xmath112 . as we need to perform such an operation for all nodes that are still in the set of unobserved nodes , the computational complexity of estimating the conditional entropy of all unobserved nodes is @xmath113 .",
    "these operations must be repeated at any stage of the greedy algorithm adding a factor @xmath1 to the computational complexity . in conclusion ,",
    "the computational complexity of the entire algorithm is @xmath114 .",
    "several considerations and computational techniques may be used to speed up the algorithm .",
    "first , knowledge of the pairwise entropy is not required for all pairs of nodes . _ a priori _ , we do nt know which pairwise entropies will be used by the algorithm , but we can compute them on - the - fly when they are needed .",
    "second , we know that the entropy of individual nodes conditioned to the set of observed nodes can only decrease during the algorithm , i.e. , @xmath115 . we can therefore use a lazy algorithm that computes eq .",
    "( [ eq : alg1 ] ) for an unobserved node only if needed  @xcite . to keep track of the ranking of unobserved nodes in a computationally cheap way",
    ", we can make use of a standard queue algorithm .",
    "using these tricks , the effective computational complexity of the algorithm is greatly reduced ( fig .",
    "[ fig7 ] ) .",
    "[ [ a - dijkstra - like - algorithm - to - determine - the - tree - with - minimal - entropy - rooted - in - a - specific - node ] ] a dijkstra - like algorithm to determine the tree with minimal entropy rooted in a specific node ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    the following provides a description of the algorithm used to determine the tree with minimal entropy rooted in a specific node .",
    "the algorithm is a variant of the well - known dijkstra s algorithm .",
    "we used this algorithm whenever we wanted to provide an estimate of the conditional entropy @xmath42 in loopy networks . here",
    ", @xmath41 is the index of node that acts as a root of the tree , @xmath116 are instead the indices of the nodes already observed .",
    "we assume that these variables are given , as well as pairwise conditional entropies among pairs of nodes , and the entire topology of the network .",
    "define @xmath117 , with @xmath118 for all @xmath119 .",
    "this vector serves to account for the fact that a node has been already visited .",
    "define @xmath120 , with @xmath121 forall @xmath122 , except for @xmath123 .",
    "this vector serves to keep track of the pairwise conditional entropies that enter in the sum .",
    "define @xmath124 , where @xmath125 for all @xmath126 , except for the observed nodes @xmath127 . finally , define the vector @xmath128 , with @xmath129 for all @xmath119 , except for @xmath130 .",
    "this vector represents the distance of a generic node from node @xmath41 .",
    "distance is measured in terms of the value of pairwise entropies along the path .    1 .",
    "select @xmath131 , i.e. , the node at minimal distance from @xmath41 among those not yet visited .",
    "2 .   look at all neighbors of node @xmath132 . for a given neighbor @xmath133 that has not yet been visited , i.e. , @xmath134 , apply one of the following mutually exclusive operations : * if @xmath135 and @xmath136 , set @xmath137 . if @xmath138 or @xmath139 , set @xmath140",
    "otherwise , set @xmath141 .",
    "this part applies to observed nodes that we did nt have yet considered . * if @xmath142 and @xmath143 , compute @xmath144 . if @xmath145 , then set @xmath146 and @xmath147 .",
    "this part applies to observed nodes that we have already considered , but for which we may find a shorter path towards @xmath41 . * if @xmath148 , set @xmath149 and @xmath150 .",
    "this part applies to non observed nodes .",
    "3 .   set @xmath151 , and go back to point 1 until all nodes are marked as visited .",
    "[ [ results - for - the - susceptible - infected - susceptible - sis - model - on - scale - free - networks ] ] results for the susceptible - infected - susceptible ( sis ) model on scale - free networks ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~          to provide further evidence on how the details of a stochastic system may affect the identification of the best set of observed nodes , we study the independent cascade model ( icm ) on a star - like configuration .",
    "the topology is given by a network composed of @xmath1 peripheral nodes , with labels @xmath153 , attached to a central node with label @xmath154 . in the icm model",
    ", nodes can be in three different states : @xmath155 for susceptible , @xmath156 for infected , and @xmath157 for recovered .",
    "the rules of the dynamics are simple . at every istant of time ,",
    "every infected node passes the infection to its all neighbors that are in the @xmath155 state with probability @xmath158 .",
    "after that , the nodes recover , and they do longer participate in the dynamics .",
    "the process continues if new nodes have been infected , otherwise , it stops leaving us with final configuration where nodes are only in the states @xmath155 or @xmath157 . here , we focus our attention on the final configuration of the system .",
    "the infection probability @xmath158 is one ingredient which we can play with .",
    "the second ingredient is the initial configuration of the system , namely @xmath159 , where we assume that for a generic node @xmath41 we can have @xmath160 or @xmath155 . in the various cases below",
    ", we compute the entropy associated with the central node or with a peripheral node .",
    "the node with larger entropy is the one initially observed according to the maximum entropy principle .",
    "let us consider the case in which we do nt know anything about the initial configuration , so that every initial configuration has the same probability of appearance .",
    "the total number of possible initial configurations is @xmath161 , and their associated probability is @xmath162 .",
    "suppose the total number of initially infected nodes is @xmath132 .",
    "the probability that one these configurations includes among the selected nodes the central node is given by @xmath163 the probability that the total number of nodes that are initially infected is @xmath132 reads as @xmath164 the probability that the final state of the central node is @xmath157 depends on the initial configuration .",
    "in particular , this depends on whether the central node is initially infected or not .",
    "we have @xmath165 for a peripheral node , we have @xmath166 we know that @xmath167 thus @xmath168 \\right\\ } \\label{eq : icm_star_center}\\ ] ] for a peripheral node , we have instead @xmath169 thus @xmath170 \\right\\ } \\end{array } \\label{eq : icm_star_per}\\ ] ] eqs .",
    "( [ eq : icm_star_center ] ) and  ( [ eq : icm_star_per ] ) can be finally used to compute the entropy associated with the central node or a generic peripheral node for given values of @xmath158 and @xmath1 according to @xmath171 + [ 1- p(x_i = r ) ] \\log_2 [ 1- p(x_i = r ) ] \\ ; , \\ ; \\textrm { with } \\ ; i = c \\textrm { or } p \\;. \\label{eq : entropy_icm}\\ ] ]        if we replace the probability of eq .",
    "( [ eq : uniform ] ) with the binomial distribution @xmath172 every single node has a probability @xmath173 to be initially in the @xmath156 state .",
    "we can therefore replace eq .",
    "( [ eq : icm_star_center ] ) with @xmath174 \\right\\ } \\label{eq : icm_star_center_binomial}\\ ] ] and eq .",
    "( [ eq : icm_star_per ] ) with @xmath175 \\right\\ } \\end{array } \\label{eq : icm_star_per_binomial}\\ ] ]        finally , let us focus only on starting configurations where the one and only one node is infected and all others are in the @xmath155 state .",
    "the probability that the central node is in the final configuration in state @xmath157 is @xmath177 the probability that a generic peripheral node is in state @xmath157 in the final configuration is @xmath178"
  ],
  "abstract_text": [
    "<S> many real - world systems are characterized by stochastic dynamical rules where a complex network of dependencies among individual elements probabilistically determines their state . </S>",
    "<S> even with full knowledge of the network structure and of the stochastic rules of the dynamical process , the ability to predict system configurations is generally characterized by large uncertainty . sampling a fraction of the nodes and deterministically observing their state may help to reduce the uncertainty about the unobserved nodes . </S>",
    "<S> however , choosing these points of observation with the goal of maximizing predictive power is a highly nontrivial task , depending on the nature of the stochastic process and on the structure of the underlying network . here </S>",
    "<S> , we introduce a computationally efficient algorithm to determine quasi - optimal solutions for arbitrary stochastic processes defined on generic sparse topologies . </S>",
    "<S> we show that the method is effective for various processes on different substrates . </S>",
    "<S> we further show how the method can be fruitfully used to identify the best nodes to label in semi - supervised probabilistic classification algorithms .    </S>",
    "<S> stochastic phenomena are so ubiquitous in nature that they are studied in any field of science , including biology  @xcite , ecology  @xcite , physics  @xcite , neuroscience  @xcite , and finance  @xcite . roughly speaking , </S>",
    "<S> in a stochastic system composed of multiple elements the states of the individual elements obey probabilistic rules that depend on the states of other elements in the system . in many cases of interest , the underlying structure that determines how elements depend one on the other is a sparse network  @xcite . to have a concrete example in mind , consider the seasonal flu spreading . </S>",
    "<S> the epidemics usually starts from a few epicenters . </S>",
    "<S> a person not immunized can contract the disease with a certain probability only if in contact with an infected individual . at the same time , infected people can recover and are thus no longer able to transmit the disease . </S>",
    "<S> the social network underlying the spreading process determines how the state of every individual depends on the state of the others . at any given time , </S>",
    "<S> the system is characterized by a certain degree of uncertainty , in the sense that different configurations have a non - vanishing probability to appear . </S>",
    "<S> such an uncertainty is due to the stochastic nature of the spreading process , and it is present regardless of the level of knowledge that one has about the probabilistic rules of the process and about the contact pattern underlying the dynamics .    an obvious way to reduce uncertainty is to survey the system , i.e. , observe the state of a sample of elements . </S>",
    "<S> in the example of flu spreading , this means obtaining full knowledge about the health state of some people . with such additional knowledge , </S>",
    "<S> the probabilistic prediction of the state of unobserved elements becomes less uncertain . </S>",
    "<S> in particular , the larger is the sample , the lower is the uncertainty , with the limiting case of null uncertainty when the entire system is observed . </S>",
    "<S> resource constraints make however complete observation generally impossible : only a small fraction of elements of the system can be sampled . </S>",
    "<S> is there an efficient way of identifying the best elements to observe in the sense that the uncertainty for the rest of system is minimized ? </S>",
    "<S> the question is answered from an information theoretic point of view , by the so - called principle of maximum entropy sampling  @xcite . </S>",
    "<S> its rationale is very intuitive : to reduce the uncertainty about the system as much as possible , observations must be performed on the elements for which uncertainty is maximal . </S>",
    "<S> maximum entropy sampling is often contemplated in the literature as a possible solution to problems of experimental design  @xcite . </S>",
    "<S> an example is represented by the problem of choosing the placement of thermometers in a room to provide the most accurate picture of temperature in the entire room  @xcite . </S>",
    "<S> some works , such as refs .  </S>",
    "<S> @xcite , have dealt with special settings where the solution to the problem of maximum entropy sampling can be efficiently approximated or achieved exactly with ad - hoc algorithms . </S>",
    "<S> these studies have , however , considered very small systems , where the scalability of the algorithms is not a concern . </S>",
    "<S> further , the problem has been studied only in regular topologies , such as lattices or fully connected networks . </S>",
    "<S> no attention instead has been devoted so far to the maximum entropy sampling problem when the underlying topology is given by a large complex network structure . </S>",
    "<S> this is the focus of the present paper  .    </S>",
    "<S> we consider an arbitrary stochastic process defined on a graph @xmath0 , composed of @xmath1 nodes . </S>",
    "<S> every node @xmath2 is characterized by a state variable @xmath3 that can assume @xmath4 distinct values . </S>",
    "<S> connections among nodes in the network stand for dependency relations among their associated variables . </S>",
    "<S> suppose the joint probability distribution @xmath5 associated with each of the @xmath6 possible configurations of the system is known . </S>",
    "<S> we are still left with potentially large uncertainty quantified by the information theoretic joint entropy of the graph @xmath7 \\ ; .   </S>",
    "<S> \\label{eq : entropy}\\ ] ] the computation of @xmath8 involves a sum over @xmath6 possible configurations , thus becoming unfeasible even for small values of @xmath1 .    </S>",
    "<S> suppose we are allowed to observe a subset of nodes @xmath9 . </S>",
    "<S> observing these nodes means removing any uncertainty on their state , and thus considering the joint probability distribution of the unobserved part of the graph , @xmath10 , conditioned on the state of observed nodes @xmath11 , namely @xmath12 , where @xmath13 , @xmath14 , and we defined for shortness of notation @xmath15 and @xmath16 . for a particular choice of the set @xmath11 , the uncertainty about the rest of the system is quantified by the conditional entropy @xmath17 \\ ; .   </S>",
    "<S> \\label{eq : cond_entropy}\\ ] ] for @xmath18 , eq .  </S>",
    "<S> ( [ eq : cond_entropy ] ) is identical to eq .  </S>",
    "<S> ( [ eq : entropy ] ) . for @xmath19 </S>",
    "<S> , we have instead @xmath20 .    for a given number @xmath21 of observed nodes , to reduce uncertainty on the system we want to observe the @xmath21 nodes that minimize the conditional entropy of the unobserved part , eq .  </S>",
    "<S> ( [ eq : cond_entropy ] ) . </S>",
    "<S> in particular , since @xmath22 , the minimization of eq .  </S>",
    "<S> ( [ eq : cond_entropy ] ) is equivalent to finding the group of nodes @xmath23 having maximum joint entropy , i.e. , @xmath24 where @xmath25 $ ] . </S>",
    "<S> the maximization is performed over all sets @xmath11 of fixed size @xmath21 . </S>",
    "<S> this principle is known as maximum entropy sampling , and the associated problem is @xmath26-hard  @xcite . </S>",
    "<S> the exact solution of the optimization problem requires the consideration of all possible choices of the set @xmath11 , and for each of them the computation of the associated joint entropy . </S>",
    "<S> the computational complexity of both operations scales exponentially with @xmath21 </S>",
    "<S> .    a quasi - optimal solution to the problem can however be obtained at a reduced computational cost , exploiting the sub - modularity of the entropy function  @xcite . </S>",
    "<S> such a property allows to implement a greedy strategy where the set of observed nodes is constructed in a sequential manner , leading to a solution that is provably close to the optimum . </S>",
    "<S> it provides a solution corresponding to a value of the function to be optimized that is at least @xmath27 times the value of the global maximum . in the context of our problem , </S>",
    "<S> the greedy algorithm consists in sequentially adding , to the set of observed nodes , the node with maximal entropy conditioned to the set of variables that are already observed . </S>",
    "<S> more specifically , the algorithm starts at stage @xmath28 with an empty set of observed nodes , @xmath29 . </S>",
    "<S> the @xmath30-th point of observation , namely @xmath31 , is chosen , among the nodes not yet part of the set of observed nodes @xmath32 , according to the rule @xmath33 the algorithm can be run up to arbitrary values @xmath34 . </S>",
    "<S> this procedure addresses the issue of the extensive search over all possible groups of nodes . </S>",
    "<S> however , at every stage @xmath30 , the computation of each of the @xmath35 conditional entropies in eq .  </S>",
    "<S> ( [ eq : greedy_max_entropy ] ) still requires a number of operations scaling as @xmath36 . </S>",
    "<S> this makes the algorithm usable only for the construction of very small sets of observed nodes .    </S>",
    "<S> the issue of scalability can not be solved in general . however , if the underlying graph @xmath0 is a tree , a computationally efficient algorithm to compute the conditional entropies in eq .  </S>",
    "<S> ( [ eq : greedy_max_entropy ] ) can be deployed as follows . </S>",
    "<S> this is the main contribution of the present paper . </S>",
    "<S> we suppose that the tree is connected , so that every node is reachable from every other node along one and only one path . if the graph is composed of multiple connected components , these can be considered independently . </S>",
    "<S> the algorithm makes use of three main properties of conditional entropy : ( i ) chain rule , ( ii ) conditional independence , and ( iii ) bayes rule . </S>",
    "<S> suppose we are at stage @xmath30 of the greedy algorithm . </S>",
    "<S> we need to compute the conditional entropy @xmath37 for a generic node @xmath38 . </S>",
    "<S> thanks to the bayes rule , we can write @xmath39    in the above expression , @xmath40 is the unconditional entropy of node @xmath41 , @xmath42 is the joint entropy of the nodes @xmath43 conditioned to node @xmath41 , and @xmath44 is the joint unconditional entropy of the nodes @xmath43 . the first term on the r.h.s . of eq .  </S>",
    "<S> ( [ eq : alg1 ] ) can be easily and cheaply estimated .    </S>",
    "<S> given that the network is a tree , the second term can be written ( see appendix for details ) , as the sum of pairwise conditional entropies , one per observed node @xmath45 every observed node corresponds to a term in the sum , given by the entropy associated with that observed node , @xmath46 , conditioned to another node @xmath47 . </S>",
    "<S> such a node @xmath48 is either the first observed node encountered along the unique path connecting @xmath46 to @xmath41 , or node @xmath41 itself .    </S>",
    "<S> finally , thanks to the chain rule , for the rightmost term in eq .  </S>",
    "<S> ( [ eq : alg1 ] ) we know that @xmath49 hence , the rightmost term of eq .  </S>",
    "<S> ( [ eq : alg1 ] ) can be computed at each stage @xmath30 by reusing results of the algorithm in previous stages . </S>",
    "<S> we note that the entire procedure can be used for arbitrary sequences of added nodes , not just the one decided according to the greedy algorithm . </S>",
    "<S> hence , eqs .  </S>",
    "<S> ( [ eq : alg1]-[eq : alg2 ] ) allow us to compute the joint entropy of any subset of nodes , including the entire graph . </S>",
    "<S> this is a side product of the algorithm , yet a very important and useful result .        </S>",
    "<S> the algorithm requires prior knowledge of the unconditional entropy of the individual nodes , and the pairwise entropy among pairs of nodes . </S>",
    "<S> these quantities can be estimated either from numerical simulations or experimental observations of the stochastic process , or theoretically , using for example a belief - propagation algorithm  @xcite . </S>",
    "<S> it is worth to remark the great advantages brought by our approach compared with the naive method to compute joint conditional entropies when a system is studied with numerical simulations ( or experimental observations ) . </S>",
    "<S> this may be the case of most situations . </S>",
    "<S> if the size of the set of observed nodes is @xmath21 , the maximum value of the entropy is @xmath50 . </S>",
    "<S> this corresponds to the case in which each of the @xmath51 configurations has exactly the same probability to occur . </S>",
    "<S> if the number of simulations is @xmath52 , the maximum value of the entropy that can be measured with the naive approach is @xmath53 . for large values of @xmath21 </S>",
    "<S> , the number of simulations will be unavoidably @xmath54 , thus leading to systematically biased estimates . with our method instead </S>",
    "<S> , the maximum value of the pairwise conditional entropy between a pair of nodes is @xmath53 . </S>",
    "<S> the total entropy of the set of observed nodes will be given by @xmath21 terms of this type , leading to a maximum possible value equal to @xmath55 . </S>",
    "<S> a number of simulations @xmath56 , computationally feasible in most situations , is then sufficient to avoid numerical problems .    from a computational point of view </S>",
    "<S> , the running time scales as @xmath57 in the worst - case scenario . </S>",
    "<S> we found , however , that some computational tricks allow for a great reduction of the complexity  @xcite , which effectively scales as @xmath58 ( see appendix ) . </S>",
    "<S> this makes the present algorithm easily applicable even to large systems .    </S>",
    "<S> the above algorithm is exact if the graph @xmath0 is a tree . </S>",
    "<S> we argue that the algorithm can be still effectively used in loopy but sparse graphs , as many of the networks describing real systems  @xcite . in the case of loopy graphs , the conditional entropy @xmath59 </S>",
    "<S> is still computed under the tree assumption by generating a spanning tree rooted in @xmath41 . </S>",
    "<S> this provides us with an upper - bound of its true value . </S>",
    "<S> the rooted tree can be generated arbitrarily . </S>",
    "<S> however , to keep the upper - bound as tight as possible , we use a djikstra - like algorithm suitably modified for this context ( see appendix ) . </S>",
    "<S> the results presented below are based on this choice . </S>",
    "<S> we believe that our algorithm may be useful for any stochastic process where the dependence among variables is represented by a sparse network . </S>",
    "<S> this is a common setting in several realistic scenarios . </S>",
    "<S> we expect a decrease of performance not only in dense networks , but also in sparse networks with spatial embedding ( as for example road networks or power grids ) , or networks not compatible with the locally tree - like ansatz ( as for example collaboration networks ) . in practical applications , assuming that the network structure is perfectly known may not be necessarily true . </S>",
    "<S> the method can be modified to include such an additional degree of uncertainty , as long as sparsity of the network is among the ingredients of the stochastic model .    as a proof of concept to verify the effectiveness of the algorithm </S>",
    "<S> , we consider the bond percolation model  @xcite . </S>",
    "<S> this is a prototypical example of a stochastic process on a complex network topology with relevance in the analysis of networks robustness  @xcite , and of spreading phenomena such as those belonging to the susceptible - infected - recovered class  @xcite . in a single realization of the model , </S>",
    "<S> every edge is considered active or occupied with bond occupation probability @xmath60 . </S>",
    "<S> nodes that are connected by at least a path formed by occupied edges form clusters . </S>",
    "<S> we focus our attention on the largest among these clusters . </S>",
    "<S> we suppose that the state of a given node @xmath41 can take only @xmath61 values : @xmath62 if node @xmath41 is part of the largest cluster in the network , and @xmath63 , otherwise . for simplicity , </S>",
    "<S> we assume that the value of the bond occupation probability @xmath60 is known in advance . to estimate the unconditional entropy @xmath40 of a generic node @xmath41 , and the pairwise conditional entropy @xmath64 of a generic pair of nodes @xmath41 and @xmath65 </S>",
    "<S> , we rely on the belief - propagation algorithm recently introduced in ref .  </S>",
    "<S> @xcite . </S>",
    "<S> this algorithm provides us with a set of probabilities @xmath66 for every node @xmath41 to be part of the largest cluster . to obtain conditional probabilities we can rely on the same algorithm by simply blocking the value of the conditional variables . </S>",
    "<S> the belief - propagation algorithm is developed under the treelike approximation , so it may not provide the best estimate of the marginal and conditional entropy in loopy networks  @xcite . </S>",
    "<S> further , we note that the belief - propagation algorithm provides a precise value of the percolation threshold , namely @xmath67 , below which the largest cluster does nt exist although this is not true for a network with finite size . </S>",
    "<S> the consequence of this fact is that , for @xmath68 , the entropy of the system is estimated to be equal to zero . </S>",
    "<S> null entropy is also associated to the system for @xmath69 . at intermediate values of @xmath60 , </S>",
    "<S> the system is characterized by a non - null uncertainty . </S>",
    "<S> this is the regime where a proper sampling of the network is needed . in our analysis </S>",
    "<S> , we compare the performance of maximum entropy sampling with random sampling , i.e. , a simple strategy where the set of observed nodes is iteratively constructed by adding nodes in random order . to compare the performance of two sampling strategies , we use two independent tests . </S>",
    "<S> the first consists in the comparison between the values of the joint entropy of the set observed nodes . </S>",
    "<S> the higher this quantity , the better the strategy . </S>",
    "<S> the second test is based on inference . </S>",
    "<S> we simulate the bond percolation model @xmath52 independent times . in every simulation </S>",
    "<S> , we identify the largest cluster ( if more clusters have maximal size , we randomly choose one of them to be the largest ) , and we assign a value @xmath70 to every node @xmath41 belonging to the largest cluster , or @xmath71 , otherwise . then , for a given set of observed nodes @xmath72 , we use the belief - propagation algorithm by blocking the value of the variables obtained in the simulation for all observed nodes , that means @xmath73 for all @xmath74 . </S>",
    "<S> the algorithm provides us with the marginal conditional probability @xmath75 to be part of the largest cluster for every node @xmath76 . </S>",
    "<S> we note that this does nt exactly correspond to make inference on the unobserved part of the networks , as the required probability would have been @xmath77 . </S>",
    "<S> we finally compare the probabilistic prediction with the actual configuration . </S>",
    "<S> figure  [ fig2 ] summarizes the results obtained for the bond percolation model applied to the zachary karate club network  @xcite . </S>",
    "<S> the percolation threshold of the network is @xmath78 according to the belief - propagation method  @xcite . for values of @xmath60 close to @xmath67 , </S>",
    "<S> the uncertainty of the system is high , as it appears from the joint entropy in figure  [ fig2]a . </S>",
    "<S> sampling the network according to the maximum entropy principle allows us to systematically construct a set of observed nodes with joint entropy that is slightly larger than the one obtained for a set of observed nodes chosen at random . </S>",
    "<S> the prediction error obtained with both strategies is systematically decreasing as the number of observed nodes increases ( fig .  </S>",
    "<S> [ fig2]b ) , with maximum entropy sampling obtaining slightly better performances . </S>",
    "<S> we note that maximum entropy sampling tends to first select peripheral and low - degree nodes , while central and larges - degree nodes are generally included in the set of observed nodes only at the end of the sampling procedure ( fig .  </S>",
    "<S> [ fig2]c ) . for large value of the occupation probability @xmath60 , </S>",
    "<S> the difference in performance between maximum entropy and random sampling becomes more evident . </S>",
    "<S> this is visible in both figures  [ fig2]d and  [ fig2]e . </S>",
    "<S> as soon as we reach a set of observed nodes that has joint entropy equal to the one of the entire graph ( around @xmath79 ) , the rest of the system becomes essentially deterministic , and the state of the unobserved part of the network is predicted with no error . </S>",
    "<S> still , best observation points are given by low - degree nodes , however different from those identified at low @xmath60 values ( fig .  </S>",
    "<S> [ fig2]f ) . </S>",
    "<S> random sampling has instead very poor performances . </S>",
    "<S> this is particularly apparent from the large values of the prediction error . in a single realization of random sampling , </S>",
    "<S> the fact that the prediction error may increase as the size of observed set increases is due to the fact that the belief - propagation algorithm may provide `` wrong '' predictions in a loopy network . </S>",
    "<S> the main message of the analysis is , however , particularly important : with random sampling there is a concrete risk of wasting resources , as we do not learn much about the rest of the system , even by observing a large fraction of its nodes .     </S>",
    "<S> nodes divided in @xmath61 ( panels a and b ) and @xmath80 ( panels c and d ) groups . </S>",
    "<S> average internal degree is @xmath81 . </S>",
    "<S> @xmath82 of the nodes have average external degree @xmath83 . for the remaining @xmath84 , we set @xmath85 . </S>",
    "<S> we use the expectation maximization algorithm of ref .  </S>",
    "<S> @xcite to recover communities . </S>",
    "<S> the starting configuration of the iterative algorithm is the true community assignment of the nodes as suggested in ref .  </S>",
    "<S> @xcite . </S>",
    "<S> a and c ) joint entropy of the set of observed nodes as a function of the number of observed nodes . maximum entropy sampling ( black full line ) is compared against a single instance of random sampling ( red dashed line ) . </S>",
    "<S> b and d ) error in the prediction of the labels of the unobserved part of the system as a function of the number of observed nodes . </S>",
    "<S> the error is calculated according to the formula @xmath86 ^ 2 $ ] , where @xmath87 is the index of the pre - assigned community of node @xmath41 , and @xmath88 if @xmath89 , whereas @xmath90 , otherwise . </S>",
    "<S> , scaledwidth=48.0% ]    from the former analysis , it seems that peripheral low - degree nodes are the best points of observation to infer the state of the rest of the system . </S>",
    "<S> this is somehow reminiscent of what noticed in refs .  </S>",
    "<S> @xcite , although the systems studied there were defined on two - dimensional lattices . </S>",
    "<S> while finding the best points of observation at the border of the system could be a rather general trend in several stochastic systems ( and may be fruitfully used in heuristic methods for the optimal placement of sensors in a system ) , we argue that this is not always true . </S>",
    "<S> several systems can exhibit different behaviors and , even for the same system and the same topology , the behaviour can be modified by particular choices of the system parameters . in figure  </S>",
    "<S> [ fig3 ] , we show results obtained from the analysis of the bond percolation model on an uncorrelated configuration model with scale - free degree distribution  @xcite . we see that top nodes in the set of observed nodes are chosen differently depending on the value of the bond occupation probability @xmath60 . to strengthen this message , in figure  [ fig6 ] </S>",
    "<S> , we consider the susceptible - infected - susceptible model relying on the approach of ref .  </S>",
    "<S> @xcite . </S>",
    "<S> qualitative results are identical to those of the percolation model . </S>",
    "<S> further in the appendix , we study analytically the behavior of the independent cascade model and show that , in star - like networks , the choice of the best nodes to observe is highly sensible to the parameter of the model , and the initial configuration used for the stochastic dynamical process .    </S>",
    "<S> so far , we focused on stochastic processes taking place on networks , but our method may be important also in classification problems on networks , such as those considered by probabilistic semi - supervised machine learning methods  @xcite . in this context , the set of observed nodes represents the set of points to be labelled . among the applications of semi - </S>",
    "<S> supervised classification algorithms in sparse networks is community detection , where many probabilistic models for the detection of clusters have been considered  @xcite . </S>",
    "<S> we conclude the paper with an analysis in this direction ( fig .  </S>",
    "<S> [ fig4 ] ) . </S>",
    "<S> we consider artificial networks introduced in ref .  </S>",
    "<S> @xcite , where @xmath1 nodes are divided in @xmath4 groups of identical size . </S>",
    "<S> every node has an internal degree coming from a poisson distribution with average @xmath91 , and an external degree towards any other group coming from a poisson distribution with average degree @xmath92 ( average external degree to all other groups is thus @xmath93 ) . to create imbalance among nodes in the groups , we force external degrees of a fraction @xmath94 of nodes to be equal to zero , so that these nodes have only connections to nodes within their pre - assigned groups . for given internal and external degree sequences , </S>",
    "<S> the graph is created according to the same scheme of the configuration model . </S>",
    "<S> we apply the expectation maximization algorithm of ref .  </S>",
    "<S> @xcite to identify communities . </S>",
    "<S> the algorithm provides us with a probability @xmath95 , with @xmath96 , for every node @xmath41 to be in a group . </S>",
    "<S> the algorithm provides us also with conditional probabilities obtained by blocking the value of some nodes . </S>",
    "<S> we use these quantities to compute marginal and pairwise conditional entropies required by our method , and to perform inference by blocking the values of the nodes in the observed set . </S>",
    "<S> results of the analysis are reported in figure  [ fig4 ] . </S>",
    "<S> maximum entropy sampling allows us to reduce uncertainty in a systematic manner . </S>",
    "<S> if the community detection method has sufficient knowledge , then it is even able to recover perfectly the labels of the unobserved nodes . </S>",
    "<S> random sampling instead does nt help much , as the community detection algorithm learns very slowly if labeled nodes are chosen without a precise criterion . </S>",
    "<S> community detection is just a particular case study . </S>",
    "<S> so far , the maximum entropy principle has been considered only in some semi - supervised algorithms that incorporate active learning  @xcite . </S>",
    "<S> the approach has been never considered in the context of sparse network structures . </S>",
    "<S> we believe that the results of this paper may be helpful for future research in machine learning problems on sparse networks .    </S>",
    "<S> we thank a. faqeeh , a. flammini , and s. fortunato for comments on the manuscript . </S>",
    "<S> f.r . acknowledges support from the national science foundation ( grant no . </S>",
    "<S> cmmi-1552487 ) and the u.s . army research office ( grant no . w911nf-16 - 1 - 0104 ) .    </S>",
    "<S> 30ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ]  </S>",
    "<S> + 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty @noop _ _ ,  </S>",
    "<S> vol .  </S>",
    "<S> ( ,  ) @noop _ _  ( ,  ) @noop _ _  ( ,  ) @noop _ _  ( ,  ) @noop _ _  ( , ) @noop _ _  ( ,  ) @noop * * ,   ( ) @noop ( ) @noop * * , ( ) @noop * * ,   ( ) @noop ( ) in  @noop _ _  ( ,  )  pp .   in  link:\\doibase 10.1145/1150402.1150479 [ _ _ ] ,  ( ,  ,  )  pp .   @noop * * ,   ( ) @noop * * ,   ( ) in  @noop _ _  ( ,  )  pp .   @noop _ _  ( , ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) link:\\doibase 10.1086/jar.33.4.3629752 [ * * ,   ( ) ] @noop * * , ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * , ( ) @noop _ _  ( ,  ) </S>"
  ]
}