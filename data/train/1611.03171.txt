{
  "article_text": [
    "in the era of time - domain survey astronomy , dedicated telescopes scan the sky every night and strategically revisit the same area several times .",
    "the raw data are images , but surveys commonly provide , not only image data , but also _ catalogs _ , summaries of the image data that aim to enable a wide variety of studies without requiring users to analyze raw or processed image data",
    ". catalogs typically report object properties , based on algorithms that detect sources in images with a measure of statistical significance above some threshold , chosen so that the resulting catalog is likely to be highly pure ( i.e. , with few or no spurious sources ) .",
    "the question we address in this paper is how to combine information from a sequence of independent observations to maximize the ability to detect faint objects at or near a chosen detection threshold , ameliorating the data explosion due to false detections from a lower threshold that would be required by a suboptimal method .",
    "focusing on the faint objects that typically dominate the collected data is an important and timely problem for a number of ongoing surveys and vital for planning the next - generation data processing pipelines .",
    "there are two different ways one can approach the problem .",
    "a traditional , resource - intensive approach is to wait until all observations are completed , performing detection by stacking the multi - epoch image data ( with potential complications related to registration , resampling , and point spread function matching ) .",
    "an optimal procedure for threshold - based detection with image stacks was introduced by @xcite .",
    "once a master object catalog is produced from the stacked images , time series of source measurements are created by forced photometry at the master catalog locations .",
    "an alternative ( non - exclusive ) approach is to perform source detection independently for each observation , producing a catalog of candidate sources at each epoch @xcite . the detection threshold may be different for each epoch .",
    "interim object catalogs may be produced by analysis of the series of overlapping source detections potentially associated with a single object using any available data ; a final catalog would be built using the catalogs from all epochs .",
    "of course , a catalog based on data from many epochs should be able to include many dim sources that escape confident detection in single - epoch or few - epoch catalogs . to enable construction of a deep multi - epoch catalog ,",
    "the single - epoch catalogs must report information for candidate sources with relatively low statistical significance ; i.e. , the single - epoch catalogs must have reduced purity .",
    "if we set the single - epoch threshold too high , there will be too few detections ; we will not have well - sampled time series for dim sources , and the final catalog will be too small .",
    "if , on the other hand , we set the threshold too low , the single - epoch catalogs will be overwhelmed with ( mostly false ) detections that were seen only once , requiring wasteful expenditure of storage and computing resources for constructing multi - epoch catalogs .",
    "an optimal threshold might preserve the size and quality of the final catalog , while enabling users to build interim catalogs on - the - fly , potentially tailored to specific , evolving needs .",
    "[ tamas ]    we here address the second alternative , considering how best to accumulate evidence from possibly marginal detections while the observations are in progress , to prune spurious source detections but keep the sources associated with genuine objects .",
    "the study presented here is exploratory , to establish the basic ideas and provide initial metrics for studying the feasibility of the incremental approach . to make the analysis analytically tractable and the results easy to interpret",
    ", we restrict ourselves to an idealized setting ; we will present a more general and formal treatment in a subsequent paper .",
    "we adopt the terminology of lsst and other time - domain synoptic surveys , using _ source _ to refer to single - epoch detection and measurement results , and _ object _ to refer to a unique underlying physical system ( e.g. , a star or galaxy ) that may be associated with one or more sources .",
    "( here we limit ourselves to objects that would appear as a single source . ) for simplicity , we consider observations in a single band unless stated otherwise .",
    "consider an object with constant flux @xmath0 and direction @xmath1 ( a unit - norm vector on the sky ) . at each epoch @xmath2 ,",
    "analysis of the image data @xmath3 corresponding to a small patch of sky of solid angle @xmath4 produces a _ source likelihood function _",
    "( slf ) for the basic observables , flux @xmath5 and direction @xmath1 , of a candidate source in the patch .",
    "the slf is the probability for the data as a function of the ( uncertain ) values of the observables , @xmath6 where @xmath7 denotes various contextual assumptions influencing the analysis , e.g. , specification of the photometric model and information about instrumental and sky backgrounds .",
    "( since @xmath7 is common to all subsequent probabilities , we henceforth consider it as implicitly present . ) for example , if photometry is done via point spread function ( psf ) fitting with weighted least squares , and if the noise level and backgrounds are known , then it may be a good approximation to take @xmath8 $ ] , where @xmath9 is the familiar sum of squared weighted residuals as a function of the source flux and direction .",
    "we consider a catalog at a given epoch to report summaries of the likelihood functions for candidate sources that have met some detection criteria .",
    "the most commonly reported summaries are best - fit fluxes ( or magnitudes ) with a quantification of flux uncertainty ( typically a standard error or the half - width of a 68% confidence region ) , and , separately , best - fit sky coordinates with an uncertainty for each coordinate.we here take these summaries to correspond to a factored approximation of the source likelihood function , @xmath10 where the epoch - specific flux factor , @xmath11 , is a gaussian with mode @xmath12 ( the catalog flux estimate at epoch @xmath2 ) and standard deviation @xmath13 , and the direction factor , @xmath14 , is an azimuthally symmetric bivariate gaussian with mode @xmath15 , and standard deviation @xmath16 . , because direction is a two - dimensional quantity . if @xmath16 is the single - coordinate standard deviation , the angular radius of a 68.3% ( `` @xmath17 '' ) confidence region or flat - prior credible region is @xmath18 . ] this may be a rather crude approximation ; we will address it further elsewhere , here merely noting that it is implicitly adopted for most survey catalogs . for simplicity , we take the flux factors to have the same standard deviation at all epochs , @xmath19 .    we adopt a simple source detection criterion : a candidate source with a flux likelihood mode @xmath20 larger than a single - epoch threshold value , @xmath21 , is deemed a detection .",
    "the probability for detection in a single - epoch catalog is the probability that source with true flux @xmath0 will produce a single - epoch measurement @xmath12 that falls above the threshold .",
    "this probability is just the integral of the gaussian flux likelihood function above the threshold , which we denote by @xmath22 where @xmath23 is the complementary error function .",
    "for comparison , consider detection probabilities in the case of stacked exposures from @xmath24 observations .",
    "we assume that the objects are stationary and have a constant flux , and that the dominant source of noise is still the sky , so the relative noise is reduced by @xmath25 after stacking . for a stacked exposure flux threshold @xmath26 ,",
    "the probability for detection is @xmath27 figure  [ fig:1 ] displays the detection probability as a function of true object flux for single - epoch and stacked data , for various choices of the single - epoch and stacked thresholds .",
    "the dotted green lines represent the single - exposure situation @xmath28 as a function of the true flux in @xmath29 units for detection thresholds of 2 , 3 , 4 , and 5@xmath29 .",
    "similarly the solid yellow lines correspond to the stacked detections with exposures .",
    "consider two curves corresponding to the same threshold , so @xmath30 .",
    "the probability for detection at @xmath31 is 50% for both a single exposure and stacked exposures .",
    "but the curve for stacked exposures is much steeper , with a higher probability for detecting sources brighter than @xmath21 , and a lower probability for detecting sources dimmer than @xmath21 .",
    "that is , when constructed with a common threshold , the catalog built from stacked data will be more complete above threshold , and will more effectively exclude sources with true flux below threshold .",
    "faint sources will not always be detected .",
    "the probability for making @xmath32 detections among observations follows the binomial distribution , giving the multi - epoch detection probability , @xmath33 an interesting quantity is the probability that an object would lead to source detections in @xmath34 or more observations .",
    "this is simply the sum @xmath35 ( this can be expressed in terms of the incomplete beta function ) . in figure  [ fig:2 ]",
    "we plot these probabilities as a function of @xmath0 ( again in @xmath29 units ) for observations . from left to right , the solid red curves show the probability for detecting an object of given flux in exactly 1 , 2 , etc .",
    ", up to 9 observations . similarly the dashed blue curves correspond to cases @xmath36 ( 1 or more ) , @xmath37 , and so on .",
    "( note that the functions coincide for the case @xmath38 . )",
    "figure  [ fig:3 ] compares detection probability curves for the stacked exposure case ( solid yellow curves , as in figure  [ fig:1 ] ) and the multi - epoch , @xmath39 detection case ( dashed blue curves , as in figure  [ fig:2 ] ) . for a particular stacked exposure case ,",
    "we see there is a multi - epoch case whose detection probability curve displays very similar performance . for example , the @xmath40 stacked exposure curve is very similar to the multi - epoch @xmath41 detection case .",
    "this indicates that collecting sources with @xmath41 detections from _",
    "single - epoch _ @xmath40 catalogs is nearly equivalent in terms of catalog completeness and purity to producing a separate , new @xmath40 stacked exposure catalog . analyzing the single - epoch catalogs",
    "has a number of advantages .",
    "it can be implemented in an incremental fashion that follows the schedule of the survey , and the time series data are readily available at a given time ; there is no need to go back to old images and to performed forced photometry at locations that are revealed only in the final stack .",
    "the previous calculations addressed detectability of a source of known true flux , @xmath0 . in real - life scenarios ,",
    "the problem is quite the opposite  we are presented with the observations and would like to understand the properties of the sources . in this context ,",
    "our focus is on how one can reliably distinguish noise peaks from real sources .",
    "it is important to emphasize that we have more information than just the fact that a source has been detected ; we also have flux measurements , at multiple epochs .",
    "our approach is motivated by bayesian hypothesis testing , where the strength of evidence for presence of a source is quantified by the posterior probability for the source - present hypothesis , or equivalently , by the posterior odds in favor of a source being present vs.  being absent ( the odds is the ratio of probabilities for the rival hypotheses ) .",
    "the posterior odds is the product of prior odds and the data - dependent _ bayes factor_. the prior odds depends on population properties ; it may be specified a priori when there is sufficient knowledge of the population under study , or learned adaptively by using hierarchical bayesian methods ( for examples of this in the related context of cross - identification , see @xcite ) . here",
    "we focus on the bayes factor ; we will address hierarchical modeling in a follow - up paper .",
    "the bayes factor is the ratio of marginal likelihoods for the competing hypotheses , one that claims that the sources are associated with a real object , and its complement that assumes there is just noise : @xmath42 each marginal likelihood , @xmath43 , is the integral , with respect to all free parameters for the hypothesis , of the product of the likelihood function and the prior probability density for the parameters .",
    "let us now assume that out of @xmath24 observations , we measure @xmath32 detections with measured fluxes @xmath44 .",
    "we consider the two competing hypotheses separately .",
    "let @xmath45 denote the set of indices for epochs with detections , and @xmath46 denote the set of indices for epochs with nondetections : @xmath47 for a candidate object with @xmath32 source detections among @xmath24 catalogs , the likelihood for a candidate true flux @xmath0 is @xmath48 where is the probability of not detecting an object with true flux @xmath0 , which happens times , and @xmath49 is the flux likelihood function defined above ( gaussians with means equal to @xmath12 ) .",
    "the marginal likelihood for the real - object hypothesis is obtained by averaging over all possible true flux values , @xmath0 . for an object that is a member of a population with known flux probability density @xmath50 ,",
    "the prior probability for @xmath0 , used for the averaging in the marginal likelihood , is @xmath50 , so that @xmath51 which is a one - dimensional integral that can be analytically or numerically quickly evaluated .",
    "( when the population distribution is not known a priori , it may be estimated via joint analysis of the catalog data for many candidate objects , within a hierarchical model , a significant complication that we will elaborate on elsewhere . )",
    "the alternative hypothesis is that the detections are simply random noise peaks in the image .",
    "the noise hypothesis marginal likelihood , @xmath52 , is the probability for the catalog data presuming no real object is present .",
    "for epochs with a candidate source reported in the catalog , the datum is the flux measurement , @xmath12 , and the relevant factor in the marginal likelihood is @xmath53 , the _ noise peak distribution _ , evaluated at @xmath12 .",
    "this distribution will depend on the noise statistics for each catalog .    for epochs with no reported detection , we instead know only that @xmath54 , so the relevant factor is the fraction of _ missed _ noise peaks , , not true flux ; in the gaussian regime assumed here the measured value may be negative , albeit with small probability .",
    "the estimated flux would be constrained to be positive via the prior density , @xmath50 , which would multiply the flux likelihood when computing posterior flux estimates . ]",
    "@xmath55 the probability for a false detection is then @xmath56 .    to compute these quantities comprising @xmath52 , we need to know the noise peak distribution , @xmath57 .",
    "this distribution is not trivial to specify ; it will depend both on the noise sources , and on the source detection algorithm .",
    "typically , a source finder performs a scan , identifying local peaks of the measured fluxes smoothed with a kernel , e.g. , corresponding to a specified point source aperture . under the noise hypothesis",
    ", the source finder will be finding peaks of a smooth random field .",
    "the locations and amplitudes of the peaks will form a point process , whose statistical properties can be analytically calculated @xcite .",
    "the most important consequence is that even though the underlying noise at the pixel level may be independent and gaussian , the source finder output will correspond to sampling from a point process with a more complicated distribution of fluxes . in particular , although the pixel - level noise distributions are symmetric ( about the mean background ) , the distribution for ( falsely ) detected fluxes is skewed toward positive values .",
    "the relevant calculation is presented in the appendix .",
    "figure  [ fig : surface ] shows the surface density of noise peaks as a function of the detection statistic @xmath20 ( in @xmath29 units ) , in the scenario when the sky noise is spatially independent and gaussian .",
    "the surface density is in units of objects per @xmath58 , where @xmath59 is the width of the point spread function ( see appendix ) .",
    "the noise peak distribution is the normalized version of this function .",
    "the surface density has a mode at @xmath60 and its shape is well approximated with a gaussian with standard deviation @xmath61 for all positive values of @xmath20 .",
    "the shaded ( magenta ) area highlights the excess density over the gaussian at negative @xmath20 values .",
    "as noted above , we obtain the probability for detecting a noise peak , @xmath62 , by integrating @xmath57 above the flux threshold .",
    "figure  [ fig : frac]a shows the results as a function of the flux threshold in @xmath29 units ( left ) , and on a scale corresponding to an lsst - like magnitude ( right ; see ",
    "[ sec : disc ] for a description of the magnitude scale ) .",
    "we see that the fraction of noise peaks above threshold is about 62% at 1@xmath29 , dropping quickly to about a few percent at 3@xmath29 , and becoming negligible at 5@xmath29 .",
    "based on just this figure , it is tempting to set a high detection threshold to reject such `` ghost peaks '' and keep the catalog of detections nearly pure ; but that would mean we lose the opportunity to recover the numerous really faint sources .",
    "our multi - epoch approach suggests a different strategy : instead of seeking to make the catalogs for _ each _ epoch pure , we can adopt a lower single - epoch threshold , relying on the fusion of data across epochs to weed out ghosts .",
    "the marginal likelihood and bayes factor computations accomplish this data fusion .",
    "the marginal likelihood for the noise hypothesis is a product of the terms for the detections and non - detections : @xmath63 we now have all the ingredients for computing the bayes factor of eq .",
    "[ eq : bfac ] , providing an objective measure of how much the data prefer a real - source origin to a noise peak origin .",
    "so far we have only used the flux information in the data .",
    "genuine sources should have both consistent fluxes and consistent directions across all epochs . in practice , due to the noise and astrometric errors",
    ", the detections of the same object will shift in each exposure , thus the resulting catalogs have to be cross - matched . using",
    "a probabilistic method can be to our direct benefit here , enabling straightforward combination of the flux and direction information .",
    "the detections from a real source are all connected , they are just displaced by a random astrometric error ; but noise peaks ( ghosts ) will be independent of each other and their associations can only be by chance . as we are working under the approximation that the flux and sky position estimates are independent ( see eq .",
    "[ eq : eplike ] ) , the bayes factor using both the photometric and astrometric information factors , @xmath64 the astrometric cross - match bayes factor , @xmath65 , has been derived in @xcite ( see eq .",
    "( 17 ) there , and eq .  ( 19 ) for the tangent plane gaussian limit that holds for high - precision astrometry ) .",
    "that work also discusses generalizations that account for proper motion and other complications .    in the following section we assess the discriminative power of multi - epoch source detection by applying it to simulated galaxies and noise peaks , both omitting and including the astrometric data",
    "we here describe simulations that demonstrate the detection capability of our multi - epoch approach in a setting with known ground truth .",
    "the simulation parameters were chosen to produce data similar to that provided by modern large - scale optical surveys .",
    "we assume that galaxies are brighter than 28 magnitudes and that the 5@xmath29 detection limit is 24 magnitudes , corresponding roughly to parameters of lsst photometry .",
    "panel  ( b ) of figure  [ fig : frac ] shows the noise peak detection probability as a function of magnitude based on these parameters , in contrast to the dimensionless presentation in panel  ( a ) . to compute the marginal likelihood for the source - present hypothesis",
    ", we must specify a prior for the source flux , @xmath5 .",
    "here we use a standard faint - galaxy number counts law , with the number counts following the empirical formula of ; see @xcite .",
    "that approximately translates to the properly normalized population distribution of @xmath66 where @xmath67 is the limiting flux that corresponds to the previously defined magnitude limit .",
    "we generate sets of random detections for 20,000 galaxies with true fluxes between 28 and 23 magnitudes by simply drawing @xmath12 values from a gaussian centered on the actual fluxes .",
    "we also generate 2,000 ghost detection @xmath12 values from @xmath57 by inverting its cumulative distribution ( computed numerically on a grid ) .",
    "the number of exposures is set to the previously used @xmath68 with a single - epoch flux threshold of just 1@xmath29 , deep in the noise . in observations with our specified parameters",
    ", the number of ghost detections will greatly outnumber the galaxy detections with this low threshold .",
    "the numbers of galaxies and ghosts were chosen to enable display of the distributions of bayes factors for the two classes of detections ( noise and true ) .",
    "we first analyze the simulated data considering only the photometric information ( i.e. , ignoring the directional bayes factors ) . in figure  [ fig : bf - photo ] the ( red ) points represent the resulting bayes factors for the real sources ( right of the double dashed vertical lines ) and the noise peaks ( on the left ) .",
    "superficially , the bayes factors may appear surprisingly large ; even for dim sources the bayes factors are often @xmath69 , often considered strong evidence in settings where the competing hypotheses are assigned prior odds of unity . but here , the prior odds for a genuine association vs.  a noise peak match are extremely small , because chance associations are likely due to the high spatial density of galaxies .",
    "@xcite , @xcite , and @xcite discuss how to compute the prior odds in various settings .",
    "figure  [ fig : bf - photo ] shows that , as one would expect , the weight of evidence is strong for the bright galaxies but weakens for the faint galaxies .",
    "the smallest bayes factors arise for galaxies with true magnitudes near 26.5 , which corresponds to the mode of the noise peak distribution , @xmath57 .",
    "perhaps surprisingly , sources dimmer than magnitude 26.5 can have larger bayes factors than those with magnitude 26.5 .",
    "this happens because @xmath57 peaks away from @xmath70 , i.e. , we do not expect noise peaks to have arbitrarily small measured fluxes ; the peak - finding process biases the noise peak distribution away from zero flux . for the weakest detectable sources , the most likely number of detections among the @xmath68 epochs is one .",
    "the flat top of the distribution at the faint end corresponds to very dim sources detected only once , very near threshold .",
    "the smaller bayes factors in that region of the plot correspond to unlikely larger numbers of detections near the threshold ; the discreteness in the number of detections produces a subtle banding in the distribution .",
    "we now consider the astrometric data , by itself . for simplicity , we assume a constant direction uncertainty of @xmath71 for all detections . ]",
    "we simulate the coordinates for the mock galaxies as follows . around the true direction of each object ,",
    "we randomly generate points from a 2d gaussian .",
    "this flat sky approximation is excellent in this regime ; for such tight scatters , the approximation error is below the limit of the numerical representation of double precision floating point numbers .",
    "the coordinates of noise peaks are generated homogeneously .",
    "the surface density of the ghosts is analytically calculated and its integral above the 1@xmath29 detection threshold yields .",
    "a simple algorithm is to pick a large enough square , with area @xmath4 , and randomly draw the number of peaks from a poisson distribution with expectation value @xmath72 . out of these ghosts",
    ", we pick a number equal to the number of flux detections , with locations such that are closest to the center , where the simulated object is placed .",
    "figure  [ fig : bf - astro ] shows the distribution of astrometric bayes factors , for the real ( mock ) and noise sources .",
    "note the larger span of the ( log ) bayes factor axis .",
    "banding due to discreteness in the number of detections is now clearly apparent among the true - object bayes factors ; the 9 levels correspond to the different number of detections with the lowest being 1 .",
    "comparing to figure  [ fig : bf - photo ] , we see that directional cross - matching is a stronger discriminant between real and noise sources in the dim source regime .",
    "the photometric data grow in importance as sources grow brighter .",
    "the astrometric bayes factors are essentially constant vs.  magnitude for a given number of detections among the 9 epochs .",
    "this is a consequence of our simplifying assumption of a constant direction uncertainty .",
    "as noted in footnote  [ fn : const - sig ] , in real surveys the astrometric precision decreases with increasing magnitude ( decreasing flux ) in the weak - source regime ; this would lead to some decrease in the bayes factors with increasing magnitude .",
    "figure  [ fig : bf - both ] shows the distribution of bayes factors , for the real and noise sources , now combining the photometric and astrometric factors . for the lowest band , corresponding to a single detection , @xmath65 is unity by definition ( no constraint coming from a single detection ) , producing the same bayes factor distribution as in the flux - only calculation . for multiple detections ,",
    "the bayes factors for the true sources are greatly enhanced by including astrometric information ( note that the ordinate is logarithmic ) ; just two detections produces quite strong evidence for the true - object hypothesis .",
    "the bayes factors for the noise peaks have moved to much lower values , due to the low likelihood of directional coincidences .",
    "this computation demonstrates that flux and astrometric catalog data , combined across epochs , can strongly distinguish real objects from spurious detections .",
    "we have not addressed what threshold bayes factor to use for producing a multi - epoch catalog , or , in bayesian terms , how to convert bayes factors into posterior probabilities for candidate objects . as noted above",
    ", when the object population density ( on the sky and in flux ) is known a priori , the calculation is straightforward ( e.g. , the prior odds will be proportional to the ratio of true object and noise peak sky densities ) .",
    "when these quantities are unknown , a possibly complicated hierarchical bayesian calculation can jointly estimate them and the object properties .",
    "when many objects are detected , an approximate approach , plugging in empirical estimates of the densities based on the data , is likely to suffice , as described in @xcite .",
    "this paper presents an exploratory study of a new , incremental approach to the analysis of multi - epoch survey data , based on fusion of single - epoch catalogs produced using a source detection algorithm with a modest or low threshold .",
    "although the single - epoch catalogs will include many noise sources ( they may even be dominated by them ) , we show that probabilistic fusion of the single - epoch data can produce interim or final multi - epoch catalogs with properties similar to those expected from catalogs based on image stacking .",
    "the approach is essentially a generalization of cross - matching , where object detection corresponds to identifying a set of candidate sources that match in both flux and direction across epochs .",
    "using a probabilistic approach directly provides the required quantities , enabling fusion of information both across epochs , and between flux and direction , by straightforward multiplication of the relevant probabilities .",
    "the final quantification of strength of evidence is via marginal likelihoods and bayes factors ; these can be used for final thresholding , or for producing posterior probabilities for source detections when population properties such as sky densities are known or can be accurately estimated ( perhaps as part of the catalog analysis ) .    the bayes factor compares predictions of the observed data based on true - object and noise - peak hypotheses for the data , and thus requires knowledge of the distribution of noise peaks .",
    "we derive the spatial properties of noise peaks that commonly appear in catalogs .",
    "the flux - dependent surface density of ghosts is asymmetric in flux , skewed toward positive flux values .",
    "it can be accurately approximated by a shifted gaussian for most practical purposes .",
    "based on the bayes factor , sources with single - epoch measured fluxes over a threshold start to separate out from the noise peaks when data are combined across just a few epochs .",
    "the evidence for a source becomes very strong once the single - epoch fluxes exceed ( 24 mag ) .",
    "when considering only the flux measurements , the faintest sources are hard to distinguish from the noise peaks with measurements at just a few epochs ; but astrometric data ( celestial coordinate estimates ) greatly help to separate genuine and spurious detections .    in general , the specificity and the selectivity of the proposed discriminator depends on a number of parameters , most of which we discuss as part of the simulated case study .",
    "the pixel size , the single - epoch detection threshold , the number of exposures , and the point - spread function all affect the frequency of noise peaks .",
    "the population distribution of source properties also directly impacts detectability of faint sources",
    ". a hierarchical generalization of the approach could learn important features of the population distribution as part of the analysis .",
    "we have treated only the case of detection of constant - flux sources .",
    "detecting variable and transient sources can be accommodated by introducing one or more time series models into the flux matching part of the algorithm .",
    "models that accurately describe particular classes of sources will produce optimal catalogs , but flexible models  perhaps simple stochastic processes , or even histogram or other partion - based models , with appropriate priors on variability  may suffice for producing general - purpose multi - epoch catalogs for studying variable sources .",
    "this is a potentially complicated generalization of our framework that we plan to explore in future work .",
    "of course , variable source detection using image stacks is also an open research problem ; our framework provides an alternative avenue to address it .",
    "our exploratory study made simplifying assumptions .",
    "a strong assumption was that source candidates are isolated enough that the image space can be partitioned into patches that have at most one candidate source .",
    "when source candidates are close to each other , the matching across epochs must account for multiple possible source association hypotheses .",
    "similar complications appear when considering classes of objects that may be comprised of multiple sources per epoch , e.g. , radio galaxies .",
    "in other work , we have developed techniques for directional cross - matching in contexts with multiple candidate associations , and with complex object structure and object motion @xcite .",
    "these methods can be extended to include flux matching criteria to generalize the multi - epoch detection framework described here .",
    "the strategy we have described is quite different from conventional approaches to producing survey catalogs . implementing it will raise new processing and database management challenges ; users of the resulting catalogs will need to think about catalogs in a different way .",
    "in particular , a low - threshold single - epoch catalog will contain many spurious sources ; with a low enough threshold , the spurious sources will greatly outnumber real sources .",
    "however , evidence mounts quickly as catalogs are merged .",
    "if interim catalogs are produced consecutively , cumulative culling of early single - epoch catalogs could reduce the storage burden for catalogs subsequent to the first catalog .",
    "such issues , and the generalizations described above , will be topics for future study .",
    "the authors gratefully acknowledge valuable and inspiring discussions with andy connolly and robert lupton on various aspects of the topic .",
    "this study was supported by the nsf via grants ast-1412566 and ast-1312903 , and the nasa via the awards nng16pj23c and stsci-49721 under nas5 - 26555 .",
    "adler , r.  j.  1981 , the geometry of random fields , chichester : wiley , 1981 ,    bardeen , j.  m. , bond , j.  r. , kaiser , n. , & szalay , a.  s.  1986 , , 304 , 15    bond , j.  r. , & efstathiou , g.  1987 , , 226 , 655    budavri , t. , & szalay , a.  s.  2008 , , 679 , 301    budavri , t.  2011 , , 736 , 155    budavri , t.  2012 , statistical challenges in modern astronomy v , 291302    budavri , t. , & szalay , a.  s.  2014 , astronomical data analysis software and systems xxiii , 485 , 207    budavri , t. , & loredo , t.  j.  2015 , annual review of statistics and its application , 2 , 113139    kaiser , n.  2004 , `` the likelihood of point sources in pixellated images '' , pan - starrs internal report , psdc-002 - 010-xx    kessler , r. , bernstein , j.  p. , cinabro , d. , et al .",
    "2009 , , 121 , 1028    loredo , t.  j.  2013 , statistical challenges in modern astronomy v , 303308    lund , j. , & rudemo , m.  2000 , biometrika , 87 , 2 , pp.235 - 249 ( http://www.jstor.org/stable/2673461 )    kerekes , g. , budavri , t. , csabai , i. , connolly , a.  j. , & szalay , a.  s.  2010 , , 719 , 59    madau , p. , & thompson , c.  2000 , , 534 , 239    press , w.  h.  1997 , unsolved problems in astrophysics , p.49 - 60 , arxiv : astro - ph/9604126    riess , a.  g. , press , w.  h. , & kirshner , r.  p.  1995",
    ", , 438 , l17    szalay , a.  s. , connolly , a.  j. , & szokoly , g.  p.  1999",
    ", , 117 , 68",
    "consider a two dimensional gauusian random field @xmath73 , with a known power spectrum .",
    "its gradient would be @xmath74 , and the second derivative tensor @xmath75 .",
    "we would like to find out the density of peaks of this field above a certain height .",
    "we will follow the procedure outlined in @xcite .",
    "we will expand the field and its gradient to second order around a peak at the position @xmath76 : @xmath77 where we already use the fact that the gradient of the field at a peak is zero , i.e. @xmath78 .",
    "provided that @xmath75 is non - singular at @xmath76 , we can express @xmath79 from the second equation : @xmath80 we can write a dirac delta that picks all extremal points of @xmath0 as @xmath81.\\ ] ] this expression turns a continous random field , defined at all points over our two - dimensional space into a discrete point process , that of the extremal points of the field , @xmath82.\\ ] ]    in order to pick the peaks of the gaussian random field we will also need to have a negative definite @xmath75 . if we only want peaks of a certain height , we need to calculate the appropriate ensemble average of this density over the constrained range of the variables .",
    "we have six random variables , the field @xmath0 , the three components of the symmetric @xmath75 tensor , and the two components of the gradient @xmath74 .",
    "the correlations can be computed in a straight - forward manner , given the power spectrum of the field .",
    "the gradient is uncorrelated with both the field and the second derivatives , due to the parity of the fourier representation .",
    "let us denote the correlation matrix of the field and the hessian by @xmath83 , and that of the gradient as @xmath84 .",
    "furthermore , let us define the different @xmath24-moments of the power spectrum characterizing the field as @xmath85    we can now explicitely write down the correlation matrix @xmath83 of @xmath86 and @xmath84 for @xmath87 , as @xmath88 @xmath89 with these we can write the multivariate gaussian distribution using the inverse of the correlation matrix as a product of two independent distributions @xmath90 before we proceed further , the second derivative tensor can be described more conveniently with the two eigenvalues @xmath91 and a rotation angle , @xmath92 , as follows : @xmath93 for simplicity let us introduce the dimensionless variables @xmath94 , the trace of the second derivative tensor , @xmath95 , and @xmath96 .",
    "the jacobian of the transformation from @xmath97 to @xmath98 is @xmath99 let us also introduce the dimensionless @xmath100 and the characteristic scale @xmath101 as @xmath102 the quadratic form containing @xmath103 in the exponent can be written with the new variables as @xmath104 the determinants of @xmath83 and @xmath84 are @xmath105 in these variables , the unconstrained probability distribution for @xmath106 becomes @xmath107 in order to properly handle the symmetries of the problem , we can assume that @xmath108 . then still any @xmath109 pair can be mapped onto itself by a 180 degree rotation , so the valid range of @xmath92 is @xmath110 . since",
    "none of the terms depend on @xmath92 , we can integrate over @xmath92 , resulting in @xmath111               \\frac{\\,dx\\ dz}{2\\pi \\sqrt{1-\\gamma^2 } }          \\left ( e^{-y^2}\\,2y\\ dy\\right).\\ ] ] the constraint @xmath112 maps onto @xmath113 .",
    "if we perform the integration over @xmath114 , and @xmath115 over @xmath116 , we get 1 , as we should , for the unconstrained probability for a general point .    as we introduce the peak constraints",
    ", we need to first consider the impact on the gradient .",
    "the constrained probability distribution is @xmath117                        \\frac{d^2 { \\mathbf{h } } } { 2\\pi|h|^{1/2}}.\\ ] ] after integrating over @xmath118 we get the extremum weight @xmath119 this will multiply the unconstrained probability for the density of extremal points of the random field , @xmath120          \\exp\\left[-\\frac{x^2 + 2\\gamma x z + z^2}{2(1-\\gamma^2)}\\right ]               \\frac{\\,dx\\ dz}{2\\pi \\sqrt{1-\\gamma^2}}.\\ ] ] for a peak both eigenvalues of the second derivative tensor must be negative",
    ". in the rotated coordinates @xmath121 , this means that @xmath122 , and @xmath123 .",
    "we can easily integrate over the allowed range of @xmath124 next , yielding @xmath125 we are left with @xmath126      \\frac{\\,dx\\ dz}{2\\pi \\sqrt{1-\\gamma^2}}.\\ ] ] let us introduce the function @xmath127 as @xmath128.\\ ] ] evaluating the integral over @xmath129 in mathematica , we obtain @xmath130 b(s,1 )              + b(s , 3- 2\\gamma^2 ) ,          \\bigg ] , \\label{eq : npk}\\ ] ] with @xmath131 we get the full surface density of noise peaks , @xmath132 , by integrating the conditional surface density in eq .",
    "[ eq : npk ] over all peak heights",
    "@xmath133 : @xmath134 finally , we need to evaluate the shape parameter @xmath100 .",
    "assume that the window function applied to the random field is a gaussian with a scale @xmath59 , @xmath135 its fourier transform is also a gaussian , @xmath136 we model the sky noise as a white noise with a flat spectrum .",
    "thus the correlations in the measured random field are determined by the window function , i.e. , @xmath137 with this power spectrum it is straightforward to compute the scale and the shape parameters as @xmath138 with this choice of psf and @xmath100 , we get @xmath139 in eq .",
    "[ eq : npk ] .",
    "now we are in a position to compute the probability that a noise peak is within a radius @xmath140 of our point of interest located at the origin .",
    "the spatial distribution of the noise peaks is described by a poisson process with the surface density @xmath132 .",
    "the cumulative probability that the peak is within a radius @xmath140 is given by the well - known expression @xmath141.\\ ] ] the differential probability is given by its derivative with respect to to @xmath140 , as @xmath142\\ ] ] both of these probabilities are shown on fig .",
    "[ fig : pk - shift ] .",
    "the differential probability starts off around the origin scaling with @xmath140 , due to the available area ( phase space for configuration ) .",
    "this in turn causes the cumulative function to rise as @xmath143 , resulting in a very small probability ( @xmath144 ) that a noise peak will appear within a psf scale .",
    "thus we can safely ignore noise peaks as a major contributor to false detections at a significant level ."
  ],
  "abstract_text": [
    "<S> observational astronomy in the time - domain era faces several new challenges . </S>",
    "<S> one of them is the efficient use of observations obtained at multiple epochs . </S>",
    "<S> the work presented here addresses faint object detection with multi - epoch data , and describes an incremental strategy for separating real objects from artifacts in ongoing surveys , in situations where the single - epoch data are summaries of the full image data , such as single - epoch catalogs of flux and direction estimates ( with uncertainties ) for candidate sources . </S>",
    "<S> the basic idea is to produce low - threshold single - epoch catalogs , and use a probabilistic approach to accumulate catalog information across epochs ; this is in contrast to more conventional strategies based on co - added or stacked image data across all epochs . </S>",
    "<S> we adopt a bayesian approach , addressing object detection by calculating the marginal likelihoods for hypotheses asserting there is no object , or one object , in a small image patch containing at most one cataloged source at each epoch . </S>",
    "<S> the object - present hypothesis interprets the sources in a patch at different epochs as arising from a genuine object ; the no - object ( noise ) hypothesis interprets candidate sources as spurious , arising from noise peaks . </S>",
    "<S> we study the detection probability for constant - flux objects in a simplified gaussian noise setting , comparing results based on single exposures and stacked exposures to results based on a series of single - epoch catalog summaries . </S>",
    "<S> computing the detection probability based on catalog data amounts to generalized cross - matching : it is the product of a factor accounting for matching of the estimated fluxes of candidate sources , and a factor accounting for matching of their estimated directions ( i.e. , directional cross - matching across catalogs ) . we find that probabilistic fusion of multi - epoch catalog information can detect sources with only modest sacrifice in sensitivity and selectivity compared to stacking . the probabilistic cross - matching framework underlying our approach plays an important role in maintaining detection sensitivity , and points toward generalizations that could accomodate variability and complex object structure . </S>"
  ]
}