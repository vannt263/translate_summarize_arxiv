{
  "article_text": [
    "let us consider a data - gathering sensor network , where a sink node is interested in losslessly gathering the data from @xmath0 sensor nodes .",
    "sensor data in such networks is assumed to be correlated .",
    "for the purpose of data - gathering at the sink , the sensor nodes communicate with the sink and communication consumes energy .",
    "however , sensor nodes are assumed to be severely energy - constrained .",
    "therefore , to achieve long operational lifetime of the network , we need to come up with communication protocols and architectures which allow the sensor nodes to expend as little energy in communication as possible .",
    "one approach to accomplish this is to use the framework of _ distributed source coding _",
    "( dsc ) , @xcite .    motivated by the problem of maximizing the worst - case operational lifetime of the data - gathering sensor networks , in @xcite we addressed a variant of dsc problem .",
    "we assumed that the correlation in sensor data is modeled by discrete and finite probability distribution @xmath1 , as in @xcite .",
    "further for the worst - case analysis , we introduced a new information measure , called _ information ambiguity_. we assumed asymmetric communication scenario @xcite , where only the sink knows @xmath1 .",
    "finally , we gave a communication protocol ( * bsercom * protocol ) that computed the optimal number of informant bits required , in the worst - case , to let the sink learn the data - vector revealed to the informants when the data - vectors were derived from @xmath1 .    while generalizing some portion of our work in @xcite , we made following interesting observations :    _ observation 1 : _ in general",
    ", the cardinality of a support - set is a good indicator of the minimum number of informant bits required , in the worst - case , to describe any data - vector derived from it .",
    "our observation indicated some violations of this general correspondence .",
    "there are situations , where to describe the data - vectors derived from two support - sets with same cardinality , different minimum number of informant bits are required .",
    "similarly , there are instances where to describe the data - vectors derived from a support - set with smaller cardinality requires more number of informant bits than those derived from a support - set with larger cardinality .",
    "this implies that the worst - case information measures such as information ambiguity @xcite , which are based only on the cardinality of the support - set , can not be reliably used to predict the number of informant required , in _ all _ scenarios .",
    "_ observation 2 : _ in our work on the application of the worst - case dsc to the problem of maximizing the worst - case operational lifetime of data - gathering sensor networks , we encountered the situations where one or more nodes were much more energy - constrained than other nodes .",
    "therefore , for the long lifetime of network , querying such energy - starved nodes needed to be avoided . however , there exist support - sets describing the correlation in sensor data , which have small cardinality , yet require querying all sensor nodes to let the sink to unambiguously learn of any data - vector derived from such support - sets , even if it leads to smaller network lifetime .",
    "this motivated us to address in the dsc framework , not only the problem of minimizing the number of informant bits , but also the problem of minimizing the number of informants to be queried to allow the sink to exactly learn the data - vector revealed to the informants .",
    "it is important to formally address these two observations because first , it leads to the development of _ good _ worst - case information measures and second , it allows for sound worst - case analysis of various distributed data - gathering problems of interest .",
    "this paper documents our efforts in this direction and some insights we gained from such efforts .",
    "_ related work : _ the work on the application of _ compressed sensing _ ( @xcite ) framework to distributed source coding problem , as in @xcite , relates most closely to our work , though more in terms of the problem being addressed and some of the results obtained , than in terms of the approach taken to solve the problem .",
    "let us consider a discrete and finite distribution @xmath1 for @xmath0 random variables @xmath2 , where @xmath3 is the discrete and finite alphabet of size @xmath4 .",
    "let us consider a @xmath0-tuple of random variables @xmath5 .",
    "the _ support set _ of @xmath6 is defined as : @xmath7 let us denote the cardinality of @xmath8 as @xmath9 . in @xcite , we address @xmath10 also as the _ information ambiguity _ of @xmath6 .",
    "so , the minimum number of bits required to describe an element of @xmath8 , in the worst - case , is @xmath11 . note that all the logarithms used in this paper are to the base 2 .",
    "the _ support set _ @xmath12 of @xmath13 , is the set @xmath14 of all possible @xmath15 values .",
    "let us denote the cardinality of @xmath12 as @xmath16 .",
    "so , the minimum number of bits required to describe an element of @xmath12 , in the worst - case , is @xmath17 .",
    "we define * sparsity * @xmath18 of distribution @xmath1 as the fraction of @xmath19 ( the maximum cardinality of the support - set of any discrete and finite distribution @xmath1 of @xmath0 random variables which derive their values from a discrete alphabet of size @xmath20 ) , that is : @xmath210 , 1]\\ ] ]",
    "let @xmath22 denote the minimum number of informant bits required , in the worst - case , to describe any data - vector derived from the distribution @xmath1 .",
    "similarly , let @xmath23 denote the minimum number of informants required to be queried , in the worst - case , to describe any data - vector derived from the distribution @xmath1 .",
    "let us introduce @xmath24 and @xmath25    then , the * worst - case compressibility * of distribution @xmath1 is defined as one or both of the following :    * bit - compressibility : * a discrete and finite distribution @xmath1 is called bit - compressible if the minimum number of informant bits required , in the worst - case , to describe any input - vector derived from the distribution is smaller than the total number of informant bits required to describe an instance of @xmath6 when each @xmath15 is separately described in @xmath17 bits , that is , if for the given distribution @xmath26 .",
    "otherwise if @xmath27 , it is called bit - incompressible .",
    "* informant - compressibility : * a discrete and finite distribution @xmath1 is called informant - compressible if the minimum number of informants required to participate in the worst - case data - gathering is smaller than the total number of informants , that is , if for the given distribution @xmath28 .",
    "otherwise if @xmath29 , it is called informant - incompressible .    sparsity @xmath18 of a distribution generally implies its bit - compressibility .",
    "that is , for a sample space defined by the pair @xmath30 , we have following sequence of implications : @xmath31 in other words , the data - vectors derived from sparse distributions require fewer informant bits to describe than those derived from dense distributions .",
    "however , we needed to introduce the notion of bit - compressibility of a distribution to address the scenarios where it is not so , that is , the discrete and finite distributions which are sparse , but incompressible .",
    "similarly , the sparsity of a distribution implies its informant - compressibility , though the implication in this case is not so obvious . in this case too , there are sparse distributions which require more informants to participate in data - gathering than dense distributions and there are sparse distributions which are informant - incompressible . to formally address such scenarios",
    ", we needed to introduce the notion of informant - compressibility of the distribution .",
    "as mentioned in the introduction , the notion of informant - compressibility is quite important in data - gathering sensor networks where the sink is interested in exactly learning the instance of data - vector revealed to the sensors , without querying one or more of energy - constrained sensor nodes and we want to know whether it can be done for the given support - set .",
    "another implication of informant - compressibility in the context of sensor networks is the following .",
    "if the distribution describing the correlation in sensor data is informant - compressible , then the sink needs to query only @xmath32 , sensor nodes , which implies that only these @xmath33 nodes need to sample the phenomenon being observed .",
    "therefore , only @xmath33 observations are sufficient to exactly learn the sample values at all @xmath0 sensor locations when @xmath34 sensors are not even sensing . in terms of energy costs ,",
    "this not only leads to saving of the communication energy , but also the energy expended in sensing and processing at the concerned nodes .",
    "further , this also allows the nodes to more equitably share the burden of data - gathering as any @xmath33 out of @xmath0 nodes need to be active at any time , so in @xmath0 successive rounds , a node is active only during @xmath33 rounds .",
    "_ example 1 : _ let us first address an example of situations where to describe the data - vectors derived from the support - set with smaller cardinality actually requires more number of informant bits , in the worst - case , than to describe the data - vectors derived from the support - set with larger cardinality .",
    "let us consider distributions in figures  [ fig : cardinalityexa].(a)-(b ) .",
    "the support - set in ( a ) has cardinality @xmath35 , but it requires @xmath36 informant bits , in the worst - case , to describe any data - vector . on the other hand , the support - set in ( b ) has cardinality of @xmath37 , yet it requires @xmath38 informant bits , in the worst - case , to describe any data - vector .",
    "_ example 2 : _ now , let us discuss an example of situations where to describe the data - vectors derived from the support - set with smaller cardinality actually requires more number of informant to be queried , in the worst - case , than to describe the data - vectors derived from the support - set with larger cardinality .",
    "consider the distributions in figures  [ fig : cardinalityexa].(c)-(d ) .",
    "the support - set in ( c ) has cardinality @xmath39 , but it requires both the informants to communicate to describe any data - vector . on the other hand , the support - set in ( d )",
    "has cardinality @xmath40 , yet it requires only one informant to communicate to describe any data - vector .",
    "these two examples demonstrate that sparser - pairs . however , when all the distributions under consideration are derived from same sample - space defined by given @xmath30-pair , as in this paper , then it is equivalent to talk in terms of sparsity and cardinality of the distributions and",
    "we follow this in the rest of the paper . ]",
    "distributions may neither be more bit - compressible nor be more informant - compressible .",
    "sparsity of a distribution may not imply its compressibility in either sense , defined above .",
    "also , it should be noted that this holds for any communication protocols , consistently deploying any , _ a priori _ agreed upon , encoding and decoding scheme over all support - sets in the sample space .    in the next two subsections ,",
    "we address the sparsity - compressibility issue in more detail for two distributed communication scenarios .",
    "first , the distributed source coding problem in asymmetric communication scenarios , as in @xcite and second , the distributed function computation problem in asymmetric communication scenarios , as in @xcite .",
    "we first discuss the sparsity - compressibility issue with respect to the dsc problem in asymmetric communication scenarios of @xcite .",
    "we address both , bit - compressibility and informant - compressibility .",
    "we are interested in answering two questions : ( 1 ) `` what is the smallest cardinality of support - sets for which there is at least one support - set that is incompressible ? '' , and ( 2 ) `` what is smallest cardinality of support - sets , such that all support - sets with cardinalities more than this are incompressible ? '' .",
    "these questions are addressed in following lemmas , which we state without proof for the sake of brevity .",
    "[ lemma : m1dsc ] the smallest cardinality @xmath41 of the support - sets for which there is at least one support - set that requires each of @xmath0 informants to send ( worst - case ) @xmath42 bits is @xmath43 .",
    "[ lemma : m2dsc ] the smallest cardinality @xmath44 of the support - sets , such that for all support - sets with cardinality @xmath45 , each of @xmath0 informants sends ( worst - case ) @xmath42 bits is @xmath46 .",
    "similarly ,    [ lemma : m3dsc ] the smallest cardinality @xmath47 of the support - sets for which there is at least one support - set that requires each of @xmath0 informants to transmit is @xmath48 .",
    "[ lemma : m4dsc ] the smallest cardinality @xmath49 of the support - sets , such that for all support - sets with cardinality @xmath50 , each of @xmath0 informants transmits is @xmath51 .",
    "the support - sets in figure  [ fig : cardinalityexa].(a)-(d ) correspond to the support - sets with cardinalities @xmath52 , and @xmath47 of lemma  [ lemma : m1dsc]-lemma  [ lemma : m4dsc ] , respectively , for @xmath53 and @xmath54 .",
    "the total number of support - sets with cardinality @xmath41 are @xmath55 . out of these",
    ", the number of support - sets which result in every informant sending its worst - case number of bits ( bit - incompressible ) is @xmath56 .",
    "therefore , the fraction of all possible support - sets with cardinality @xmath41 which are bit - incompressible is @xmath57    similarly , the total number of support - sets with cardinality @xmath47 are @xmath58 .",
    "out of these , the number of support - sets which result in every informant participating in data - gathering ( informant - incompressible ) is @xmath59 .",
    "therefore , the fraction of all possible support - sets with cardinality @xmath47 which are informant - incompressible is @xmath60    for large values of @xmath0 and @xmath4 , it is easy to see , using stirling s approximation for factorial , that the ratios in and are much smaller than @xmath61 .",
    "similar ratios can be computed for the numbers of bit- and informant- incompressible support - sets corresponding to cardinalities @xmath44 and @xmath49 , respectively , and shown to be very close to @xmath61 .",
    "this implies that most of the support - sets with small cardinality ( _ sparse _ support - sets ) are compressible and most of the support - sets with large cardinality ( _ dense _ support - sets ) are incompressible , confirming our intuition regarding such situations .            in figures  [ fig :",
    "bitcompressibilityregions ] and [ fig : informantcompressibilityregions ] , we plot the bit - compressibility and informant - compressibility regions , respectively , with respect to the cardinality of the support - set . it should be noted from these compressibility - region plots that all distributions with cardinalities smaller than @xmath47 are both , bit- and informant- compressible , while all distributions with cardinalities more than @xmath44 , are both bit- and informant- incompressible .",
    "further , comparing the values of @xmath52 and @xmath49 defined in lemma  [ lemma : m1dsc]-lemma  [ lemma : m4dsc ] , we have :    @xmath62 , for all @xmath63 and @xmath64 .",
    "this implies that there are informant - incompressible distributions , which are bit - compressible ( for example , the distributions with cardinality @xmath65 such that @xmath66 ) , but no bit - incompressible distribution that is informant - compressible .",
    "we discuss here the sparsity - compressibility issue with respect to the distributed function computation problem in asymmetric communication scenarios as in @xcite , with the function under consideration being ` bitwise or ' function .",
    "as before , we are interested in both , bit - compressibility and informant - compressibility and address the questions on smallest cardinalities which lead to incompressibility .",
    "these questions are addressed in following lemmas , stated without proof for the sake of brevity .",
    "[ lemma : m1bor ] the smallest cardinality @xmath41 of the support - sets for which there is at least one support - set that for computing ` bitwise or ' requires each of @xmath0 informants to send ( worst - case ) @xmath42 bits is @xmath43 and there is only one such support - set .    similarly ,    [ lemma : m3bor ] the smallest cardinality @xmath47 of the support - sets for which there is at least one support - set that for computing ` bitwise or ' requires each of @xmath0 informants to transmit is @xmath48 .",
    "the number of support - sets with cardinality @xmath47 , which require each informant to transmit , for computing ` bitwise or ' is : @xmath67 where @xmath68 is the number @xmath69 of equal output values of the ` bitwise or ' function computation , different from its output value for @xmath70 location , obtained by holding indices @xmath71 constant .",
    "there are two interesting inferences which can be drawn from our work on sparsity - compressibility issue .",
    "* worst - case compressibility is a stronger notion than sparsity : * a pessimistic interpretation says that for cardinalities as small as , @xmath41 and @xmath47 , there are support - sets which require , respectively , each informant to send all bits in the representation of its data - value ( bit - incompressibility ) and each informant to transmit ( informant - incompressibility ) . for such support - sets , interactive communication between the sink and informants is of no help in reducing the incompressibility of either type associated with the distribution . while an optimistic outlook can say that all support - sets with cardinalities smaller than @xmath41 are bit - compressible and all support - sets with cardinalities smaller than @xmath47 are informant - compressible .",
    "further , even for the cardinalities as large as @xmath44 and @xmath49 , there are bit- and informant- compressible support - sets , respectively . for such support - sets , the interaction between the sink and informants does help in reducing the number of informant bits sent and the number of informants needed to be queried , respectively . also , when a support - set is bit - incompressible ( such as support - set with cardinality more than @xmath44 ) , then this simplifies the communication problem to trivial - case : sink asks each informant to send all of its information bits and the order in which informants communicate with the sink does not matter , leading to trivially simple communication protocols .",
    "this along with the figures  [ fig : bitcompressibilityregions]-[fig : informantcompressibilityregions ] , leads us to assert that for finite and discrete distributions and with our definitions of sparsity and worst - case compressibility , the sparsity of the distribution is not a good indicator of its worst - case compressibility , in general .",
    "however , the worst - case compressibility of a distribution always implies its sparsity .",
    "* information ambiguity does not always indicate the worst - case compressibility : * our work shows the inadequacy of the worst - case information measure of _ information ambiguity _",
    "@xcite , which is defined as the cardinality of the support - set of a distribution , in indicating the worst - case compressibility .",
    "this is unlike the behavior of the average - case information measure of information entropy that has one - to - one correspondence with the average - case compressibility of a distribution .",
    "we argue that characterizing the support - set of a distribution , in the worst - case , by a single number , such as information ambiguity , involves discarding a lot of potentially useful information about how the support - set is populated .",
    "it is an interesting problem to develop a worst - case information measure that also accounts for such information embedded in a support - set .",
    "this paper documents our efforts towards formally addressing some observations with respect to worst - case distributed source coding problem of @xcite and its applications to data - gethering sensor networks .",
    "we observed that in the distributed data - gathering problem in asymmetric communication scenarios , smaller cardinality of a support - set may neither lead to fewer number of informant bits nor to fewer number of informants to be queried . in this paper , we proposed two notions of _ bit - compressibility _ and _ informant - compressibility _ of support - sets to address these two situations , respectively .",
    "we provided results to compute the bit- and informant- compressibilities regions of the support - sets as a function of their cardinality .",
    "our results led us to make the proposition that the worst - case compressibility of a support - set implies its sparsity , but converse may not always be true .",
    "our work on the interesting formal connections of the results presented here with the results obtained using compressed sensing framework for dsc is in a preliminary stage and has to be left for the future . also , as we discussed above , it is interesting to develop a more powerful information measure for the worst - case information - theoretic analysis than information ambiguity and we are presently working at it ."
  ],
  "abstract_text": [
    "<S> in the worst - case _ distributed source coding _ ( dsc ) problem of @xcite , the smaller cardinality of the support - set describing the correlation in informant data , may neither imply that fewer informant bits are required nor that fewer informants need to be queried , to finish the data - gathering at the sink . </S>",
    "<S> it is important to formally address these observations for two reasons : first , to develop good worst - case information measures and second , to perform meaningful worst - case information - theoretic analysis of various distributed data - gathering problems . towards this goal </S>",
    "<S> , we introduce the notions of _ bit - compressibility _ and _ informant - compressibility _ of support - sets . </S>",
    "<S> we consider dsc and distributed function computation problems and provide results on computing the bit- and informant- compressibilities regions of the support - sets as a function of their cardinality . </S>"
  ]
}