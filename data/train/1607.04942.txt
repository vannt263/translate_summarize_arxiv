{
  "article_text": [
    "in recent years , sparse representation of signals has drawn considerable interest and has shown to be powerful in many applications  particularly in compression and denoising .",
    "it is based on the observation that most natural signals can be sparsely represented in an appropriate representation .",
    "applications of sparse signal representations can be found in various fields such as image denoising @xcite , restoration @xcite , visual tracking @xcite , detection @xcite , and classification @xcite .",
    "recent work in @xcite , wright _ et al .",
    "_ proposed a sparse representation - based classification ( src ) for face recognition .",
    "the basic idea of src is to learn a sparse representation for a test sample as a ( sparse ) linear combination of all training samples ( over - complete dictionary ) , wherein the class - specific dictionary yielding the lowest reconstruction error determines the class label for the test sample .",
    "src has also been actively applied in various classification problems including vehicle classification @xcite , multimodal biometrics @xcite , digit recognition @xcite , speech recognition @xcite , hyperspectral image classification @xcite .    finding the sparsest solution in src is a combinatorial problem as it involves searching through every combination of @xmath0 atoms in a dictionary , where @xmath0 denotes the optimal sparsity level .",
    "there are two major approaches to approximate this problem .",
    "one is to relax this non - convex combinatorial problem into an @xmath1 convex optimization problem  also known as basis pursuit .",
    "several methods have been proposed to solve this @xmath1-norm problem including interior - point method @xcite , gradient projection @xcite etc .",
    "the other major category is based on iterative greedy pursuit algorithms such as matching pursuit , orthogonal matching pursuit ( omp ) and orthogonal least square ( ols ) .",
    "these greedy approaches have been widely used due to their computational simplicity and easy implementation .",
    "they find an atom at a time based on different criterion and update the sparse solution iteratively . among these approaches ,",
    "the omp algorithm is by far the most popular approach and is used in a wide range of applications .",
    "the main difference between omp and mp is that omp uses an orthogonal dictionary while mp does not .",
    "making the dictionary orthogonal will reduce the redundancy of the dictionary when estimating the signal .",
    "ols is similar to omp except for the atom selection process .",
    "a major difference between omp and ols relies on their atom selection procedure in that omp selects an atom that best correlates with the current residual , while ols selects an atom giving the smallest residual after orthogonalization .",
    "the time complexity of omp is @xmath2 where @xmath3 is number of features , @xmath4 is the dictionary size and @xmath0 is the sparsity level .",
    "the time complexity of ols is slightly higher than omp which is caused by the difference in the atom selection process .",
    "note that the first atom selected by omp is identical to ols .",
    "for more detailed information about the differences between these two algorithms , readers can refer to @xcite and a @xmath5-step analysis of omp and ols can be found in @xcite .",
    "ols has been widely used in many applications @xcite , but it has not gained much attention for classification problems . in @xcite ,",
    "the authors implement src in a classwise manner to improve the classification accuracy , in which the sparse coefficient is recovered by omp . in this work",
    ", we implement a class - dependent version of ols to perform classification . since ols produces lower signal reconstruction error compared to omp under similar condition @xcite ( such as the same sparsity level , same dictionary etc . )",
    " an observation that will be further analyzed and explained in the next section , we hypothesize that more accurate signal estimation will further improve the classification performance of src .",
    "compared with convex optimization based techniques such as interior point and gradient projection methods @xcite , greedy pursuit - based approaches are more efficient and appropriate to recover the sparse coefficient in src due to their low time - complexity . by using the kernel trick , we extend the proposed cdols into its kernel variant to handle nonlinearly separable data as well .",
    "the remainder of this paper is organized as follows . in sec .",
    "[ sec : sparse ] , we briefly introduce the basic concept of src and illustrate the recovery performance of omp and ols using an illustrative case study .",
    "the proposed cdols as well as its kernel variant are also described in sec .",
    "[ sec : sparse ] .",
    "experimental hyperspectral datasets and comparative classification results are presented in sec .",
    "[ sec : experiment ] .",
    "we provide concluding remarks in sec .",
    "[ sec : conclusion ] .",
    "assume @xmath6 represent the @xmath7-th training sample from class @xmath8 , @xmath9 $ ] , where @xmath10 \\in \\mathbb{r}^{d \\times n_{i}}$ ] is the @xmath8-th class training sample set , @xmath11 is the number of classes , @xmath12 represents the number of training samples from class @xmath8 , and @xmath4 is the total number of training samples , @xmath13 . based on the assumption of src , a test sample @xmath14 from class @xmath8 approximately lies in the linear span of training samples from class @xmath8 which can be described as @xmath15 [ \\beta_{i1},\\beta_{i2 } , \\hdots , \\beta_{in_{i}}]^{\\top } \\nonumber \\\\      & = & \\boldsymbol{a}_{i } \\boldsymbol{\\beta}_{i } \\end{aligned}\\ ] ] where @xmath16 is a coefficient vector whose entries are the weights of the corresponding training samples in @xmath17 .    in real - world classification problems ,",
    "the true label of the test sample is unknown .",
    "thus @xmath18 needs to be represented as a linear combination of all training samples in @xmath19 as described below @xmath20 where @xmath21 $ ] is a coefficient vector corresponding to @xmath19 .",
    "ideally , the entries of @xmath22 are all zeros except those related to the training samples from the same class as the test sample .",
    "the residual of each class can be calculated via @xmath23 where @xmath24 denotes the entries of the coefficient vector @xmath22 associated with the training samples from the @xmath8-th class .",
    "finally , @xmath18 is assigned a class label @xmath8 corresponding to a class that resulted in the minimal residual .      the sparsest solution of @xmath18 in can be obtained by solving @xmath25 where the l0-norm @xmath26 simply counts the number of nonzero entries in @xmath22 .",
    "the problem in is np - hard , and it can not be solved in polynomial time .",
    "there are several different approaches @xcite to solving this sparse approximation problem in  , in this letter , we focus on the two greedy pursuit based approaches  omp and ols . both omp and ols can be used to approximate the sparsest solution in  . in each iteration",
    ", the atom selected by omp is not designed to minimize the residual norm after projecting the target signal onto the selected elements , while ols selects the atom that minimizes the residual based on the previously selected atoms .",
    "thus the final residual norm generated by ols is always smaller than omp under similar conditions .",
    "however , ols does not always give the sparsest solution .",
    "to find an optimal @xmath0-term representation of an signal @xmath18 in  , a simple approach to finding the sparsest solution then is to search over all possible linear combinations of @xmath0 atoms in @xmath19 .",
    "let us denote this exhaustive searching algorithm as combinatorial orthogonal least square ( cols ) .",
    "the first atom selected by ols and omp is the same and fixed .",
    "however , cols iteratively select each of the atom as the first atom and remaining atoms are selected based on ols .",
    "specifically , it first selects the first atom and then select the remaining ( @xmath27 ) atoms based on ols . after selecting @xmath0 atoms",
    ", it uses them to estimate the signal and calculates the residual ( least square error ) between the signal and the estimated signal . following this , it selects the next atoms as the first set of atoms and repeats the above process . after calculating all @xmath4 ( @xmath4 is the dictionary size ) residuals using each atom as the first atom",
    ", it chooses the minimal residual as the final output .",
    "this is further explained graphically in fig . 1 next .",
    "we use an intuitive example to illustrate the differences of omp , ols and cols algorithms . in @xcite ,",
    "the authors use a graphical interpretation to show the difference between omp and ols in terms of atom selection procedure . in this example",
    ", we will further illustrate that the norm of residual generated by ols is smaller than omp but they are both not optimal .",
    "we will demonstrate later that the signal reconstruction performance of ols is close to optimal .",
    "assume the true sparsity level in   is @xmath0 .",
    "let @xmath28 , @xmath29 and @xmath30 be the axes in a 3-dimensional space , and @xmath31 , @xmath32 , @xmath33 be the atoms in a dictionary @xmath34 . without loss of generality ,",
    "assume @xmath31 and @xmath28 are overlapped with each other , and @xmath32 and @xmath33 are in the @xmath35-plane and @xmath36-plane respectively .",
    "let @xmath18 be a target signal , and assume that @xmath31 is the most correlated with @xmath18 than @xmath32 and @xmath33 .",
    "let @xmath37 .",
    "let @xmath38 and @xmath39 be the angles between @xmath32 and @xmath40 , and @xmath33 and @xmath40 respectively . under this scenario , we will analyze the optimal sparse @xmath0-term representation using omp , ols and cols , where @xmath0 equals to 2 .",
    "1 ) omp first selects the most correlated atom which is @xmath31 , and produces the residual @xmath41 by projecting @xmath18 onto it .",
    "next , omp selects an atom that is mostly correlated with @xmath41 . since @xmath42 and @xmath43 , omp selects @xmath32 .",
    "therefore , the final residual norm produced by omp is @xmath44 , which is obtained by projecting @xmath18 onto @xmath45-plane .",
    "2 ) for ols , the first atom selected is @xmath31 , since omp and ols are the same in the first iteration .",
    "next , ols calculates the residual norms of @xmath46 and @xmath44 obtained by projecting @xmath18 onto @xmath47-plane and @xmath45-plane respectively , and selects @xmath33 , since @xmath48 .",
    "thus , the final residual norm of ols is @xmath46 obtained by projecting @xmath18 onto @xmath36-plane .",
    "3 ) cols calculates all residuals by projecting @xmath18 onto planes formed by every combination of two atoms . since @xmath49 , cols selects @xmath32 and @xmath33 .",
    "the final residual norm is @xmath50 .",
    "for the special case when @xmath34 is an orthonormal dictionary , all of the above three methods will find an optimal @xmath0-term representation @xcite .",
    "overall , the performance of these methods with regard to the reconstruction error are cols @xmath51 ols @xmath51 omp .",
    "+      the recent work in @xcite demonstrates that operating src in a class - wise manner can significantly improve the classification performance of src .",
    "as is explained in the previous section , the recovery ability of ols is always better than omp in terms of the least square error under the same condition ( i.e. the same sparsity level ) .",
    "therefore , it is expected that the classification performance can be significantly enhanced by replacing omp with ols under this framework .",
    "we name this algorithm class - dependent ols ( cdols ) .",
    "note that the stopping criterion in cdols is based on the sparsity level .",
    "this is because the signal estimation error monotonically decreases as the sparsity level increases .",
    "hence , we use the same sparsity level for each class to circumvent this bias .",
    "we also extend cdols to a `` kernel '' cdols ( kcdols ) .",
    "the cdols and kcdols algorithms are described in algorithm  [ alg : cdols ] and algorithm  [ alg : kcdols ] . for a faster implementation of ols",
    ", readers can refer to @xcite .",
    "* input : * a training dataset @xmath52 , test sample @xmath14 and sparsity level @xmath0 .",
    "+ set @xmath53 , @xmath54 , and iteration counter @xmath55 .",
    "update the support set @xmath56 by solving @xmath57 where @xmath58 calculate the residual @xmath59 by solving @xmath60 where @xmath61 .",
    "@xmath62 calculate the @xmath63-th class residual norm @xmath64 .",
    "class label of @xmath18 : @xmath65 . + * output : * a class label @xmath66 .",
    "we validate the proposed cdols and kcdols and compare with various baselines using two benchmark hyperspectral datasets .",
    "the first dataset is acquired using an itres - casi ( compact airborne spectrographic imager ) 1500 hyperspectral imager over the university of houston campus and the neighboring urban area in 2012 .",
    "this image has a spatial dimension of @xmath67 with a spatial resolution of @xmath68 .",
    "there are 15 number of classes and 144 spectral bands over the @xmath69 wavelength range .",
    "[ data_uh ] shows the true color image of university of houston dataset inset with the ground truth .",
    "+     the second hyperspectral data is acquired using prospectir instrument in may 2010 over an agriculture area in indiana , usa .",
    "this image covering agriculture fields has @xmath70 spatial dimension with 2@xmath71 spatial resolution .",
    "it has 360 spectral bands over @xmath72 wavelength range with approximately @xmath73 spectral resolution .",
    "the 19 classes consist of agriculture fields with different residue cover .",
    "[ data_i ] shows the true color image of the indian pines dataset with corresponding ground truth .    [",
    "cols=\"^,^ \" , ]      +",
    "in this paper , we present a class - dependent ols - based classification method named cdols for the problem of hyperspectral image classification .",
    "we also extend cdols into its kernel variant . through two real - world",
    "hyperspectral datasets , we demonstrate that our proposed methods outperform cdomp , kcdomp as well as svm . we also demonstrate that the classification performance of the proposed methods are close to that of cdcols and kcdcols .",
    "our proposed developments are based on the observation that ols is generally better suited for sparse coefficient recovery .",
    "we also present an _ combinatorial _",
    "ols based classifier - cols , that acts as an upper bound on the performance of such classifiers , and can itself be used as well when the training dictionary is small . for scenarios where training dictionaries are not small",
    ", the more feasible cdols method has very similar performance to cdcols ( in both the input and kernel induced space ) .",
    "this work was funded in part by nasa grant nnx14ai47 g .",
    "j.  a. wright , a.  y. yang , a.  ganesh , s.  s. sastry , and y.  ma , `` robust face recognition via sparse representation , '' _ ieee trans .",
    "pattern anal .",
    "_ , vol .  31 , no .  2 , pp .",
    "210227 , february 2009 .",
    "s.  shekhar , v.  patel , n.  nasrabadi , and r.  chellappa , `` joint sparse representation for robust multimodal biometrics recognition , '' _ ieee trans . pattern anal .",
    "_ , vol .",
    "36 , no .  1 ,",
    "pp . 113126 , january 2014 .",
    "k.  labusch , e.  barth , and t.  martinetz , `` simple method for high - performance digit recognition based on sparse coding , '' _ ieee trans .",
    "neural networks learn .",
    "_ , vol .  19 , no .  11 , pp . 19851989 , 2008 .",
    "j.  f. gemmeke , t.  virtanen , and a.  hurmalainen , `` exemplar - based sparse representations for noise robust automatic speech recognition , '' _ ieee trans",
    ". audio speech lang . process .",
    "_ , vol .  19 , no .  7 , pp . 20672080 , 2011 .",
    "y.  chen , n.  m. nasrabadi , and t.  d. tran , `` hyperspectral image classification using dictionary - based sparse representation , '' _ ieee trans .",
    "remote sens .",
    "_ , vol .",
    "49 , no .",
    "10 , pp . 39733985 , october 2011 .",
    "s. j. kim , k. koh , m. lustig , s. boyd , and d. gorinevsky , `` an interiorpoint method for large - scale l1-regularized least squares , '' _ ieee j. sel .",
    "topics signal process . _ ,",
    "vol . 1 , no",
    ". 4 , pp . 606617 , dec . 2007 .",
    "m. a. t. figueiredo , r. d. nowak , and s. j. wright , `` gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems , '' _ ieee j. sel .",
    "topics signal process . _ ,",
    "vol . 1 , no",
    "586597 , dec . 2007 .",
    "m.  cui and s.  prasad , `` class  dependent sparse representation classifier for robust hyperspectral image classification , '' _ ieee trans .",
    "remote sens .",
    "_ , vol .",
    "53 , no .  5 , pp .",
    "26832695 , september 2015 .",
    "s.  j. kim , k.  koh , m.  lustig , s.  boyd , and d.  gorinevsky , `` an interior - point method for large - scale @xmath74 regularized least squares , '' _ ieee j. sel .",
    "topics signal process . _ ,",
    "vol .  1 , no .  4 , pp . 606617 , 2007 .",
    "m.  a.  t. figueiredo , r.  d. nowak , and s.  j. wright , `` gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems , '' _ ieee j. sel .",
    "topics signal process . _ , vol .  1 , no .  4 , pp . 586597 , 2007 .        c.  soussen , r.  gribonval , j.  idier , and c.  herzet , `` joint k - step analysis of orthogonal matching pursuit and orthogonal least squares , '' _ ieee trans",
    "inf . theory _ ,",
    "59 , no .  5 , pp .",
    "31583174 , january 2013 .",
    "huang and w .- b .",
    "zhao , `` determining the centers of radial basis probabilistic neural networks by recursive orthogonal least square algorithms , '' _ app .",
    "162 , no .  1 ,",
    "pp . 461473 , 2005 .",
    "l.  zhang , k.  li , e .- w .",
    "bai , and g.  w. irwin , `` two - stage orthogonal least squares methods for neural network construction , '' _ ieee trans .",
    "neural networks learn .",
    "_ , vol .  26 , no .  8 , pp .",
    "16081621 , 2014 ."
  ],
  "abstract_text": [
    "<S> spare representation of signals has received significant attention in recent years . based on these developments , a sparse representation - based classification ( src ) </S>",
    "<S> has been proposed for a variety of classification and related tasks , including face recognition . </S>",
    "<S> recently , a class dependent variant of src was proposed to overcome the limitations of src for remote sensing image classification . </S>",
    "<S> traditionally , greedy pursuit based method such as orthogonal matching pursuit ( omp ) are used for sparse coefficient recovery due to their simplicity as well as low time - complexity . however , </S>",
    "<S> orthogonal least square ( ols ) has not yet been widely used in classifiers that exploit the sparse representation properties of data . </S>",
    "<S> since ols produces lower signal reconstruction error than omp under similar conditions , we hypothesize that more accurate signal estimation will further improve the classification performance of classifiers that exploiting the sparsity of data . in this paper , we present a classification method based on ols , which implements ols in a classwise manner to perform the classification </S>",
    "<S> . we also develop and present its _ </S>",
    "<S> kernelized _ variant to handle nonlinearly separable data . </S>",
    "<S> based on two real - world benchmarking hyperspectral datasets , we demonstrate that class dependent ols based methods outperform several baseline methods including traditional src and the support vector machine classifier .    </S>",
    "<S> orthogonal least square , orthogonal matching pursuit , sparse representation - based classification , hyperspectral image classification . </S>"
  ]
}