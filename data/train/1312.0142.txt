{
  "article_text": [
    "principal component analysis is a classical statistical tool used to project data into a lower dimensional space while maximizing the variance [ @xcite ( @xcite ) ] .",
    "when the sample size @xmath0 is small compared to the number of variables @xmath1 , @xcite ( @xcite ) show that the standard pca may fail in the sense that the leading eigenvector of the sample covariance can be nearly orthogonal to the true eigenvector .",
    "therefore , the recovery of principal components in the high - dimensional setting requires extra structural assumptions .",
    "the sparse pca , assuming that the leading eigenvectors or eigen - subspace only depend on a relatively small number of variables , is applied in a wide range of applications .",
    "estimation methods for sparse pca problems are proposed in @xcite ( @xcite ) and @xcite ( @xcite ) .",
    "@xcite ( @xcite ) and @xcite ( @xcite ) obtain rates of convergence of sparse pca methods under the spiked covariance model proposed in @xcite ( @xcite ) .",
    "minimax rates of sparse pca problems are established by @xcite ( @xcite ) , @xcite ( @xcite ) and @xcite ( @xcite ) under various interesting settings .",
    "bayesian methods have been very popular in high - dimensional estimation , but there is little work to connect frequentist properties and bayesian methodologies for high - dimensional models .",
    "this paper serves as a bridge between the frequentist and bayesian worlds by addressing the following question for high - dimensional pca : is it possible for a bayes procedure to optimally recover the leading principal components in the sense that the posterior distribution contracts to the truth with a minimax rate ?",
    "the optimal posterior contraction rate immediately implies that the posterior mean attains the optimal convergence rate as a point estimator .    in this paper",
    "we consider a spiked covariance model with an unknown growing rank .",
    "we propose a sparse prior on the covariance matrix with a spiked structure and show that the induced posterior distribution contracts to the truth with an optimal minimax rate .",
    "the assumptions are nearly identical to those in @xcite ( @xcite ) , where the rank of the principal space @xmath2 and the number of nonzero entries of each spike @xmath3 is allowed to be at the order of @xmath4 for any @xmath5 , as long as the minimax rate @xmath6 . in addition , we prove that the posterior distribution consistently estimates the rank . to the best of our knowledge ,",
    "this is the first work where a bayes procedure is able to adapt to both the sparsity and the rank .",
    "there are two key ingredients in our approach .",
    "the first ingredient is in the design of the prior .",
    "we propose a prior that imposes a spiked structure on a random covariance matrix , under which each spike is sparse and orthogonal to each other .",
    "this leads to sufficient prior concentration together with the sparse property .",
    "in addition , each spike has a bounded @xmath7 norm under the prior distribution such that there is a fixed eigen - gap between the spikes and the noise , which eventually leads to consistent rank estimation .",
    "the second ingredient is in constructing appropriate tests in the proof of posterior contraction under spectral and frobenius norms .",
    "we first construct a test with the alternative hypothesis outside of the neighborhood of the true covariance under the spectral norm .",
    "for the covariance matrices inside the neighborhood of the truth under the spectral norm , we propose a delicate way to divide the region into many small pieces , where the likelihood ratio test is applicable in each small region",
    ". a final test is then constructed by combining these small tests .",
    "the errors are controlled by correctly calculating the covering number under the metric for measuring the distance of subspaces .",
    "the theoretical tools we use for this problem follow the recent line of developments in bayesian nonparametrics pioneered by @xcite ( @xcite ) and @xcite ( @xcite ) , which generalize the testing theory of @xcite ( @xcite ) and @xcite ( @xcite ) to construct an exponentially consistent test on the essential support of a prior to prove posterior consistency .",
    "the idea was later extended by @xcite ( @xcite ) and @xcite ( @xcite ) to prove rates of convergence of posterior distribution .",
    "compared to bayesian nonparametrics , little work has been done for bayesian high - dimensional estimation , especially in the sparse setting .",
    "@xcite ( @xcite ) is the first work in this area .",
    "they prove rates of convergence in sparse vector estimation for a large class of priors .",
    "the works closely related to this paper are @xcite ( @xcite ) and @xcite ( @xcite ) .",
    "@xcite ( @xcite ) study rates of convergence for bayesian precision matrix estimation by considering a conjugate prior .",
    "but as discussed in @xcite ( @xcite ) , estimation of sparse or bandable covariance / precision matrix is different from that of sparse principal subspace .",
    "the optimal rates of convergence can be different .",
    "@xcite ( @xcite ) study bayesian covariance matrix estimation for a sparse factor model , which is similar to the spiked covariance model in the pca problem . instead of estimating the principal subspace as in the pca problem , they consider estimating the whole covariance matrix .",
    "the posterior rate of convergence they obtain is not optimal , especially when the rank @xmath8 is allowed to grow with the sample size @xmath0 .",
    "the paper is organized as follows . in section",
    "[ sec : setting ] , we introduce the sparse pca problem and define the parameter space . in section  [ sec : main ] , we propose a prior and state the main result of the posterior convergence . section  [ sec : disc ] introduces an algorithm to compute the posterior mean in the rank - one case along with other discussions .",
    "all the proofs are presented in section  [ sec : proof ] , with some technical results given in the supplementary material [ @xcite ( @xcite ) ] .",
    "let @xmath9 be i.i.d .",
    "observations from @xmath10 , with @xmath11 being a @xmath12 covariance matrix with a spiked structure @xmath13 where @xmath14 for any @xmath15 .",
    "it is easy to see that @xmath16 are the first @xmath8 eigenvectors of @xmath11 , with the corresponding eigenvalues @xmath17 . the rest @xmath18 eigenvalues are all @xmath19 .",
    "the spiked covariance is proposed by @xcite ( @xcite ) to model data with a sparse and low - rank structure .",
    "an equivalent representation of the data is @xmath20 where @xmath21 and @xmath22 are independent .",
    "the matrix @xmath23 is defined as @xmath24 $ ] and @xmath25 . in such latent variable representation , @xmath26 models the signal part , which lives in an @xmath8-dimensional subspace , and",
    "@xmath27 is the noise part , which has the same variance on every direction .",
    "since the @xmath8-dimensional subspace is determined by its projection matrix @xmath28 , the goal here is to recover the principal subspace by estimating its projection matrix in the frobenius loss , @xmath29    in a high - dimensional setting , extra structural assumptions are needed for consistent estimation .",
    "we assume that the first @xmath8 eigenvectors are sparse , in the sense that each of them only depends on a few coordinates among the total number @xmath1 .",
    "define @xmath30 for @xmath31 , the support of the @xmath32th eigenvector .",
    "we assume @xmath33 sparsity on each spike by @xmath34 .",
    "the parameter space for the covariance matrix is @xmath35 where @xmath36 is a constant , which we treat as being known in this paper .",
    "the sparsity we consider matches the column sparsity in @xcite ( @xcite ) in the @xmath33 case .",
    "we require both upper and lower bounds for @xmath37 .",
    "the lower bound implies an eigengap , which leads to rank adaptation and subspace estimation , while the upper bound controls the spectral norm of @xmath11 , which leads to estimation of the whole covariance matrix .",
    "@xcite ( @xcite ) prove that under the assumptions @xmath38 the minimax rate of principal subspace estimation is @xmath39 the goal of this paper is to prove an alternative result , adaptive bayesian estimation , by designing an appropriate prior @xmath40 , such that @xmath41 where @xmath42 is the minimax rate and @xmath43 .",
    "the number @xmath44 satisfies @xmath45 . the posterior contraction ( [ eq : postconverge ] )",
    "leads to a risk bound of a point estimator .",
    "let @xmath46 be the expectation under the prior distribution @xmath40 .",
    "consider the posterior mean of the subspace projection matrix @xmath47 .",
    "its risk upper bound is given in the following proposition .",
    "we prove the proposition in the supplementary material [ @xcite ( @xcite ) ] .",
    "[ prop : pointestimate ] equation ( [ eq : postconverge ] ) implies @xmath48    in this paper , the number @xmath49 in ( [ eq : postconverge ] ) is at an order of @xmath50 for some @xmath51 . thus the dominating term in @xmath52 is @xmath53 .",
    "the posterior mean is a rate - optimal point estimator .",
    "the matrix @xmath54 may not be a projection matrix .",
    "however , it is still a valid estimator of the true projection matrix @xmath28 .",
    "a  projection matrix estimator can be obtained by projecting the posterior mean @xmath47 to the space of projection matrices under the frobenius norm .",
    "denote the projection by @xmath55 .",
    "it can be shown that @xmath56 .      in this paper",
    ", we use @xmath57 to denote a @xmath12 spiked covariance matrix with structure @xmath58 , where @xmath59 $ ] is a @xmath60 matrix with orthogonal columns .",
    "we use @xmath61 to denote the support of @xmath62 for each @xmath63 .",
    "define @xmath64 , \\\\",
    "\\lambda & = & \\operatorname{diag } \\bigl ( \\vert \\eta_{1}\\vert ^{2},\\vert \\eta_{2}\\vert ^{2},\\ldots,\\vert \\eta_{\\xi } \\vert ^{2 } \\bigr).\\end{aligned}\\ ] ] then @xmath65 is a @xmath60 unitary matrix , and @xmath57 has an alternative representation @xmath66 .",
    "we use @xmath67 to denote the probability or the expectation under the multivariate normal distribution @xmath68 and @xmath69 to denote the product measure .",
    "the symbol @xmath70 stands for a generic probability whose distribution will be made clear through the context .",
    "correspondingly , we use @xmath71 to denote the true version of @xmath72 .    for a matrix @xmath73 ,",
    "we use @xmath74 to denote its spectral norm and @xmath75 for the frobenius norm .",
    "we define @xmath76 to be the space of all @xmath77 unitary matrices for @xmath78 such that for any @xmath79 , @xmath80 . for any @xmath81 ,",
    "define the distance @xmath82 by @xmath83 for some diagonal matrix @xmath84 .",
    "we omit the subscript @xmath84 and write @xmath85 whenever @xmath86 .",
    "the number @xmath87 stands for the minimax rate @xmath88 throughout the paper .",
    "we propose a prior @xmath40 from which we can sample a random covariance matrix with structure @xmath89 , where @xmath73 is a @xmath60 matrix .",
    "the prior @xmath40 is described as follows :    for each @xmath90 \\}$ ] , we randomly choose @xmath91 by letting the indicator @xmath92 for each @xmath93 follow a bernoulli distribution with parameter @xmath94 ;    given @xmath95})$ ] , we sample a @xmath96 $ ] matrix @xmath97}]$ ] from @xmath98})}$ ] to be specified below , and then let @xmath99 .",
    "the @xmath100 $ ] matrix @xmath101 ( figure [ fig : prior ] ) may contain some zero columns under the above sampling procedure . with slight abuse of notation ,",
    "we gather those nonzero columns to form the matrix @xmath102 $ ] , with @xmath103 being the support of the column @xmath104 .",
    "note that @xmath105 , where @xmath73 is a @xmath60 matrix .",
    "after specifying the distribution @xmath98})}$ ] , the number of nonzero columns @xmath106 is also the rank of @xmath73 .    }",
    "the parts inside the dashed lines correspond to @xmath107 defined in ( [ eq : dashed ] ) . ]",
    "the number @xmath108 is a fixed constant in the prior . with @xmath109 as the mean for @xmath110 , the cardinality @xmath111 is small with high probability under the prior distribution .",
    "the number @xmath112 $ ] is an upper bound of the rank @xmath106 . in this paper",
    ", we assume that the true rank @xmath8 is at the order of @xmath113 . since @xmath114}$ ] , the range of @xmath106 covers the range of @xmath8 .",
    "we need to define a distribution @xmath115 on @xmath116 to help introduce @xmath98})}$ ] .",
    "let @xmath117 follow @xmath118 and @xmath119 follow the uniform distribution on the interval @xmath120 $ ] .",
    "then @xmath115 is defined to be the distribution of @xmath121 now we are ready to specify the random matrix prior @xmath98})}$ ] , which induces a distribution over the matrix @xmath122}]$ ] . for any vector @xmath123 and any subset @xmath124",
    ", we use the notation @xmath125 .",
    "we describe the prior through a sequential sampling procedure . if @xmath126 , we set @xmath127 . otherwise , we sample @xmath128 and let @xmath129 suppose we have already obtained @xmath130 and then sample @xmath131 , conditioning on @xmath130 .",
    "we set @xmath132 .",
    "the prior distribution of @xmath133 depends on @xmath134 , through values of @xmath135 s on the index set @xmath136 . for simplicity , denote @xmath137 define @xmath138 .",
    "if @xmath139 , we set @xmath140 .",
    "otherwise , let @xmath141 be the projection matrix from @xmath142 to the subspace spanned by @xmath143 .",
    "there is a bijective linear isometry @xmath144 induced by @xmath141 such that @xmath145 remember that a linear isometry preserves the norms in the sense that @xmath146 .",
    "we sample @xmath147 from @xmath148 and let @xmath149 .",
    "set @xmath150 .",
    "then we have specified @xmath151 , which is @xmath152 . repeating this step , we obtain @xmath153}]$ ] .",
    "the prior @xmath40 on the random covariance matrix @xmath57 is now fully specified .",
    "after collecting the nonzero @xmath154 s , we observe that the prior @xmath155 explicitly samples a spiked covariance matrix @xmath156 with the number of spikes being @xmath106 .",
    "the prior @xmath40 imposes orthogonality on the spikes , since @xmath131 is sampled on the orthogonal complement of the space @xmath157 .",
    "therefore , @xmath158 for each @xmath159 , and @xmath160 are the eigenvectors . for each eigenvector @xmath161 ,",
    "its support is in @xmath61 , whose cardinality is small under the prior distribution .",
    "moreover , the first @xmath106 eigenvalues are all bounded from @xmath19 and @xmath162 because @xmath163 $ ] .    given the data @xmath164 , the posterior distribution is defined as @xmath165 for any measurable set @xmath166 .",
    "the following theorem is the main result of this paper .",
    "the posterior distribution contracts to the truth with an optimal minimax rate .",
    "[ thmm : postsub ] assume @xmath167 , @xmath168 and @xmath169 for some constant @xmath170 .",
    "then there exists @xmath171 , such that for any @xmath172 , we have @xmath173 for some constant @xmath174 only depending on @xmath175 .    note that we have obtained the optimal posterior contraction rate under a `` mildly growing rank '' regime @xmath176 , which is also assumed in @xcite ( @xcite ) , for them to match the upper and lower bounds for minimax estimation .",
    "the assumption @xmath169 is a convenient but mild condition in high - dimensional statistics to prove rates of convergence in expectation rather than with high probability ; see @xcite ( @xcite ) , @xcite ( @xcite ) , etc .",
    "the posterior contraction result implies the same rate of convergence in expectation of a point estimator ( corollary  [ cor : pointconverge ] ) , and thus we need such an assumption to hold .",
    "additionally , we assume @xmath177 , which means that the level of the rank is not above the level of sparsity .",
    "this assumption is due to the fact that @xmath23 can be only identified up to a unitary transformation , that is , @xmath178 for any @xmath179 , and for some @xmath180 such that each row of @xmath181 may have at least @xmath8 nonzero entries .",
    "as shown in proposition  [ prop : pointestimate ] , we can use the posterior mean as a point estimator to achieve the minimax optimal rate of convergence .",
    "[ cor : pointconverge ] under the setting of theorem  [ thmm : postsub ] , we have @xmath182 for sufficiently large @xmath183 .",
    "the result follows from the fact that the @xmath184 part of proposition  [ prop : pointestimate ] is exponentially small ; hence , it is dominated by @xmath185 .",
    "in section  [ sec : spec ] , we state a result on posterior contraction rate under the spectral norm . a computationally efficient algorithm is developed in section  [ sec : comp ] for the rank - one case . in section  [ sec : furtherremark ] , we discuss the possibility of using a simpler prior for sparse pca .      in proving theorem  [ thmm : postsub ] , there are some by - products serving as intermediate steps .",
    "the following theorem says that the posterior distribution concentrates on the true covariance matrix under the spectral norm , and the subspace projection matrix concentrates on the true subspace projection matrix under the spectral norm .",
    "in addition , the posterior distribution consistently estimates the rank of the true subspace .",
    "the theorem holds under a slightly weaker assumption without assuming @xmath177 .",
    "[ thmm : postspec ] consider the same prior @xmath40 and rate @xmath186 as in theorem  [ thmm : postsub ] .",
    "assume @xmath167 , @xmath187 and @xmath169 for some constant @xmath170 .",
    "then there exists @xmath188 , such that for any @xmath189 , we have @xmath190 for some constant @xmath191 only depending on @xmath192 .    it is not practical to assume that @xmath193 is known in theorems [ thmm : postsub ] and [ thmm : postspec ] . to weaken the assumption",
    ", we can replace the prior in ( [ eq : priorcons ] ) by sampling @xmath194 $ ] , for some sequence @xmath195 slowly grows to infinity as @xmath196 .",
    "then the conclusions of the two theorems still hold without knowing @xmath193",
    ".    the posterior rate of convergence ( [ eq : noneedforlowerk ] ) for estimating the whole covariance matrix under the spectral norm does not require the assumption @xmath197 in the definition of @xmath198 . to remove this assumption",
    ", we need a slightly different prior with ( [ eq : priorcons ] ) modified by sampling @xmath199 $ ] .",
    "however , such modification may not lead to rank adaptation ( [ eq : rankconsistency ] ) due to lack of eigengap , which is critical for establishing the result in theorem  [ thmm : postsub ] .",
    "results ( [ eq : noneedforlowerk ] ) and ( [ eq : rankconsistency ] ) together imply posterior convergence of the whole covariance matrix under the frobenius norm .",
    "this is because when @xmath200 , we have @xmath201 . hence the convergence rate for the loss @xmath202 is @xmath203 .",
    "@xcite ( @xcite ) consider estimating the whole covariance matrix under spectral norm in a sparse factor model . under their assumption @xmath204",
    ", they obtain a posterior convergence rate of @xmath205 under the loss function @xmath206 , compared with our rate @xmath207 .    though an improvement over the result of @xcite ( @xcite ) , whether @xmath208 is the optimal rate of convergence for the loss functions @xmath206 and @xmath209 is still an open problem . to the best of our knowledge ,",
    "the only minimax result addressing these two loss functions for sparse pca problem is in @xcite ( @xcite ) .",
    "however , they consider a different sparsity class , defined as @xmath210 under the current setting , the results of @xcite ( @xcite ) can be written as @xmath211 observe the relation that @xmath212 hence when @xmath213 , the minimax rates for the class @xmath214 under both loss functions lie between @xmath215 and @xmath88 .",
    "we claim that the posterior convergence rate obtained in theorem  [ thmm : postspec ] is optimal when @xmath216 . for a growing @xmath8",
    ", it at most misses a factor of @xmath8 .",
    "bayesian procedures using sparse priors are usually harder to compute because the sampling procedure needs to mix all possible subsets .",
    "@xcite ( @xcite ) develop an efficient algorithm for computing exact posterior mean in the setting of bayesian sparse vector estimation .",
    "they explore the combinatorial nature of the posterior mean formula and show that it is sufficient to compute the coefficients of some @xmath217th order polynomials . in this section ,",
    "we use their idea to develop an algorithm for computing approximate posterior mean for the single spike model . in this rank - one case",
    ", there is no need for the prior to adapt to the rank",
    ". we do not need the prior to put constraint on the @xmath7 norm of the eigenvector as in ( [ eq : priorcons ] ) .",
    "thus we use the following simple prior on the single spiked covariance :    sample a cardinality @xmath218 according to the distribution @xmath219 supported on @xmath220 ;    given @xmath218 , sample a support @xmath221 with cardinality @xmath222 uniformly from all @xmath223 subsets ;    given @xmath124 , sample @xmath224 , let @xmath225 and the covariance matrix is @xmath226 .",
    "we choose @xmath219 to be @xmath227 for some constant @xmath228 .",
    "we let @xmath229 be the minimax rate when @xmath230 .",
    "the posterior distribution induced by the above prior has the following desired property :    [ thmm : rankone ] assume @xmath167 and @xmath169 for some constant @xmath170 .",
    "then there exists @xmath231 , such that for any @xmath232 , we have @xmath233 for some constant @xmath234 only depending on @xmath235 .    note that the loss function is the @xmath7 norm , which is stronger than the loss function used in theorem  [ thmm : postsub ] .",
    "the theorem above is proved in the supplementary material [ @xcite ( @xcite ) ] .",
    "we use the posterior mean @xmath236 to estimate the spike @xmath237 .",
    "we present a way for computing @xmath236 . under the rank - one situation , representation ( [ eq : latent ] ) can be written as @xmath238 with @xmath239 and @xmath240 following i.i.d .",
    "@xmath241 for all @xmath242 and @xmath243 .",
    "representation ( [ eq : latentone ] ) resembles the gaussian sequence model considered in castillo and van  der vaart ( @xcite ) .",
    "following their idea , the @xmath243th coordinate of @xmath236 can be written as @xmath244 where @xmath245 and @xmath246 is the density function of @xmath241 . by fubini s theorem",
    ", we have @xmath247 where for each @xmath248 , @xmath249 by the definition of the prior . in the same way ,",
    "@xmath250 define @xmath251 then we may rewrite @xmath252 and @xmath253 as @xmath254 the critical fact observed by @xcite ( @xcite ) is that @xmath255 is the coefficient of @xmath256 of the polynomial @xmath257 and @xmath258 is the coefficient of @xmath256 of the polynomial @xmath259 for a given @xmath248 , the coefficients @xmath260 and @xmath261 can be computed efficiently . in the gaussian sequence model , there is no randomness by @xmath248 , and the posterior mean can be computed exactly by finding the coefficients of the above polynomials . in the pca case , we propose an approximation by first drawing @xmath262 i.i.d . from @xmath263 and then computing @xmath264 \\\\[-8pt ] \\eqntext{\\mbox{for } j=1,2,\\ldots , p.}\\end{aligned}\\ ] ] one set of coefficients takes at most @xmath265 steps to compute .",
    "thus the total computational complexity is @xmath266 for computing coefficients of @xmath267 polynomials and computing all the values of @xmath268 , @xmath269 and @xmath270",
    ".    the above strategy can be directly generalized to the multiple rank case .",
    "however , it only works for the following prior without the ability for rank adaptation . to be specific",
    ", we assume the rank @xmath8 is known .",
    "then , the third step of the prior is modified as follows :    given @xmath124 , sample an @xmath271 matrix @xmath272 , with each entry i.i.d .",
    "@xmath241 . let the matrix @xmath73 be defined as @xmath273 the covariance matrix is @xmath274 .",
    "note that instead of sampling an individual support @xmath103 for each column of @xmath73 , we sample a common support @xmath124 for all columns .",
    "when @xmath216 , this will not be a problem because of the simple observation @xmath275 .",
    "the theoretical justification of the prior is stated in theorem  [ thmm : rankmultiple ] .",
    "denote the @xmath243th row of @xmath73 by @xmath276 .",
    "then the posterior mean has formula @xmath277 where for each @xmath278 , we have @xmath279 and a similar formula for @xmath280 .",
    "note that the only difference from the rank - one case is the inner product @xmath281 .",
    "the notation @xmath278 stands for @xmath282 , where each @xmath283 is an @xmath8-dimensional standard gaussian vector .",
    "a similar formula holds for @xmath280 .",
    "thus we can apply the same monte carlo approximation ( [ eq : montecarlo ] ) for @xmath284 as is done in the rank - one case .",
    "in addition to our method , there are other methods proposed in the literature .",
    "a gaussian shrinkage prior for bayesian pca have been developed by @xcite ( @xcite ) in the classical setting , but it is not appropriate for sparse pca . more general shrinkage priors have been discussed in @xcite ( @xcite ) and @xcite ( @xcite ) for high - dimensional mean vector estimation .",
    "one can extend the framework to sparse pca and develop gibbs sampling by taking advantage of the latent representation ( [ eq : latent ] ) .",
    "we refer to @xcite ( @xcite ) and van  der pas , kleijn and van  der vaart ( @xcite ) for some theoretical justifications of shrinkage priors .      the prior we proposed in section  [ sec : main ] on the random covariance matrix @xmath274 imposes orthogonality on the columns of @xmath73 .",
    "the orthogonality constraint is convenient for creating an eigengap between the spikes and the noise .",
    "this leads to the rank adaptation ( [ eq : rankconsistency ] ) .",
    "one may wonder if a simpler prior such as the one proposed in section  [ sec : comp ] without orthogonality constraint would also lead to a desired eigengap .",
    "the answer is negative in the current proof technique .",
    "let us consider the simplest case where the supports @xmath285 are known and @xmath286 .",
    "when the rank @xmath8 is not known , it is necessary to sample @xmath106 according to some prior distribution .",
    "then , after sampling the rank @xmath106 , we only need to sample a @xmath287 submatrix of @xmath73 , with rows in @xmath288 .",
    "let us denote the submatrix by @xmath289 .",
    "consider the prior distribution of @xmath289 where each element follows i.i.d .",
    "assume @xmath290 so that we can also restrict @xmath291 .",
    "it is easy to see that the @xmath292th eigenvalue of the matrix @xmath274 is @xmath293 .",
    "hence the eigengap is @xmath294 . for rank adaptation ( [ eq : rankconsistency ] ) , we need a positive eigengap @xmath295 . by nonasymptotic random matrix theory [ @xcite ( @xcite ) ] , @xmath296 for any @xmath297 . for @xmath298",
    ", @xmath299 can not be larger than @xmath300 , leading to a tail not smaller than @xmath301 . in order that there is an eigengap under the posterior distribution , the desired tail needed in the classical bayes nonparametric theory",
    "[ see @xcite ( @xcite ) and @xcite ( @xcite ) ] is @xmath302 for some @xmath303 .",
    "hence the random matrix theory tail in ( [ eq : vrmt ] ) is not enough for our purpose , and the current proof technique does not lead to the desired posterior convergence for this simpler prior .",
    "one may consider a larger support @xmath124 with @xmath304 in the prior distribution , such that the tail probability in ( [ eq : vrmt ] ) is @xmath305 for some @xmath303 .",
    "however , it can be shown that the prior does not have sufficient mass around the truth .",
    "nonetheless , if we assume the rank is known and @xmath216 , then rank adaptation is not needed . in this case , the prior in section  [ sec : comp ] leads to the desired posterior rate of convergence .",
    "remember @xmath306 .",
    "[ thmm : rankmultiple ] assume @xmath167 , @xmath307 and @xmath308 for some constant @xmath170 .",
    "then there exists @xmath231 , such that for any @xmath232 , we have @xmath309 for some constant @xmath234 only depending on @xmath235 .    it would be an interesting problem to consider whether new techniques can be developed to prove optimal posterior rate of convergence for a simpler prior when the rank @xmath8 is not known .",
    "the results of theorems [ thmm : postsub ] and [ thmm : postspec ] are special cases for bounding @xmath310 where @xmath311 and @xmath312 for different @xmath166 . to bound ( [ eq : tobound ] ) ,",
    "it is sufficient to upper bound the numerator @xmath313 and lower bound the denominator @xmath314 . following barron , schervish and wasserman ( @xcite ) and @xcite ( @xcite ) ,",
    "this involves three steps :    show the prior @xmath40 puts sufficient mass near the truth ; that is , we need @xmath315 where @xmath316 .",
    "choose an appropriate subset @xmath317 , and show the prior is essentially supported on @xmath317 in the sense that @xmath318 this controls the complexity of the prior .",
    "note that it is sufficient to have @xmath319 .",
    "construct a testing function @xmath246 for the following testing problem : @xmath320 we need to control the testing error in the sense that @xmath321    notice the constants @xmath322 s are different in the above three steps , and should satisfy some constraints in the proof .",
    "step 1 lower bounds the prior concentration near the truth , which leads to a lower bound for @xmath323 . in its original form [ @xcite ( @xcite ) ] , @xmath324 is taken to be a fixed neighborhood of the truth defined through kullback ",
    "leibler divergence .",
    "step 2 and step 3 are mainly for upper bounding @xmath325 . the testing idea in step 3",
    "is initialized by @xcite ( @xcite ) and @xcite ( @xcite ) .",
    "step 2 goes back to @xcite ( @xcite ) , who proposes the idea to choose an appropriate @xmath317 to regularize the alternative hypothesis in the test ; otherwise the testing function for step 3 may never exist ; see @xcite ( @xcite ) and @xcite ( @xcite ) .",
    "we list key technical lemmas needed in the proof for all three steps as follows . from now on , all capital letters @xmath322 with or without subscripts are absolute constants .",
    "they do not depend on other quantities unless otherwise mentioned .",
    "[ lem : denominator ] assume @xmath167 . then for any @xmath326",
    ", we have @xmath327 where @xmath328 is an absolute constant .",
    "[ lem : priorcon ] assume @xmath167 and @xmath329 for some @xmath170 .",
    "then we have @xmath330 with some absolute constant @xmath331 .",
    "lemma  [ lem : denominator ] lower bounds the denominator @xmath323 .",
    "it is a general result for all gaussian covariance matrix estimation problems .",
    "lemma  [ lem : priorcon ] lower bounds @xmath332 in step 1 .",
    "[ cor : sparsity ] let @xmath333 .",
    "assume @xmath334 .",
    "when @xmath335 for some @xmath170 , we have @xmath336 for any @xmath337 .",
    "lemma  [ cor : sparsity ] establishes the sparse property of the prior @xmath40 .",
    "it corresponds to step  2 , where @xmath317 is the sparse subset @xmath338 .",
    "note that the parameter space we consider requires @xmath339 .",
    "the sparsity constraint in @xmath340 is much weaker , which means @xmath317 is larger than the parameter space we consider . since we only need @xmath317 to control the regularity of the parameters in the alternative for hypothesis testing in step 3 , the oversized @xmath317 here does not cause a problem . in many bayes nonparametric problems ,",
    "the parameter space can be negligible compared with the set @xmath317 .",
    "@xcite ( @xcite ) provides an example where the parameter space receives no prior probability , while the set @xmath341 receives prior probability close to one ; see also @xcite ( @xcite ) .",
    "[ lem : test1 ] assume @xmath167 .",
    "there exists some constant @xmath342 depending only on @xmath343 , such that for any @xmath344 , we have a testing function @xmath246 satisfying @xmath345 and @xmath346    the existence of a test and its error rates in step 3 are established in lemma  [ lem : test1 ] .",
    "these lemmas prove theorem  thmm : postspec .    in order to prove theorem  [ thmm : postsub ]",
    ", we need to establish a stronger testing procedure .",
    "since we have the conclusion of theorem  [ thmm : postspec ] , it is sufficient to consider the subset @xmath347 .",
    "more specifically , we are going to test @xmath348 against the following alternative : @xmath349 note that @xmath333 is the joint support .",
    "the existence of the test is established by the following lemma .",
    "[ lem : test2 ] assume @xmath167 , @xmath350 and @xmath177 for some absolute constant @xmath170 .",
    "there exists some constant @xmath351 only depending on @xmath343 , and for any @xmath352 , we have a testing function @xmath246 such that @xmath353 and @xmath354 where @xmath355 , @xmath356 only depending on @xmath193 , and @xmath357 is an absolute constant .",
    "we are going to develop the proofs in several parts . in section  sec : proofmain , we establish the main results based on the key lemmas above .",
    "all key lemmas are proved in the later sections . in section  sec : priorcon , we prove lemma  lem : priorcon , which is for the prior concentration ( step 1 ) . in section  [ sec : sparsity ] , we prove lemma  [ cor : sparsity ] by showing that the prior puts most mass on a sparse set ( step 2 ) . sections  [ sec : testspec ] and  [ sec : testfrob ] are devoted in proving lemmas [ lem : test1 ] and [ lem : test2 ] , respectively ( step 3 ) .",
    "the proof of lemma  [ lem : denominator ] is stated in supplementary material [ @xcite ( @xcite ) ] .      in this section",
    "we prove theorems [ thmm : postsub ] and  [ thmm : postspec ] .",
    "since the proof of theorem  [ thmm : postsub ] depends on the conclusion of theorem  [ thmm : postspec ] , we prove the latter one first .",
    "we decompose the posterior by @xmath358 where @xmath333 . by lemma  [ cor : sparsity ] , we have @xmath359 for any @xmath360 .",
    "from now on , we fix @xmath73 to be @xmath361 .",
    "then it is sufficient to bound @xmath362 let @xmath246 be the testing function in lemma  [ lem : test1 ] , and we have @xmath363    there are three terms on the right - hand side above . by lemma  [ lem : test1 ] , @xmath364 for sufficiently large @xmath365 . by lemma  [ lem : denominator ]",
    ", we have @xmath366 .",
    "now it remains to bound the first term .",
    "let @xmath367 .",
    "we have @xmath368 which is bounded by @xmath369 because @xmath370 is upper bounded by lemma  [ lem : test1 ] , and @xmath332 is lower bounded by lemma  lem : priorcon for sufficiently large  @xmath365 . by summing up the error probability , we have @xmath371 for some constant @xmath191 only depending on @xmath192 .    to obtain the rest of the results",
    ", it is sufficient to prove @xmath372 and @xmath373 note that @xmath374 and the eigenvalues of the covariance @xmath57 are @xmath375 , where the first @xmath106 eigenvalues are in the range @xmath376 $ ] as specified by the prior .",
    "similarly , the eigenvalues of the covariance @xmath11 are @xmath377 , and the first @xmath8 eigenvalues are in the range @xmath378 $ ] .",
    "suppose @xmath379 , let @xmath380 and @xmath381",
    ". then @xmath382 and @xmath383 , which contradicts @xmath384 .",
    "the same argument leads to contradiction when @xmath385 .",
    "thus we must have @xmath200 when @xmath384 .",
    "finally , ( [ eq : implysubspace ] ) is an immediate consequence of the davis  kahan sin - theta theorem ( lemma  [ lem : sintheta ] ) .",
    "it is easy to check that the eigengap @xmath49 in lemma  [ lem : sintheta ] is @xmath386 .      with the results from lemma  [ cor : sparsity ] and",
    "theorem  [ thmm : postspec ] , we decompose the posterior distribution as follows : @xmath387 where the last inequality is due to ( [ eq : implyrank ] ) .",
    "note that the later two terms converge to zero , as shown in lemma  [ cor : sparsity ] and theorem  [ thmm : postspec ] .",
    "therefore , we only need to bound @xmath388 remembering the definition of @xmath389 , then , by lemma  [ lem : test2 ] , there exists a testing function @xmath246 for @xmath390 with the desired error bound . using a similar argument as in the proof of theorem  [ thmm : postspec ] , we have established theorem  thmm : postsub .",
    "we prove lemma  [ lem : priorcon ] in this section .",
    "the main strategy for proving lemma  [ lem : priorcon ] is to explore the structure of the prior .",
    "specifically , since the prior @xmath155 is defined by a sampling procedure for @xmath131 conditioning on @xmath391 , we need to take advantage of this feature by using the chain rule and conditional independence .",
    "proof of lemma  [ lem : priorcon ] since @xmath392 , we have @xmath393 write @xmath394})=(s_{01 } , \\ldots , s_{0r},\\varnothing,\\ldots,\\varnothing ) \\bigr).\\end{aligned}\\ ] ] the second term in the above product is @xmath395})=(s_{01 } , \\ldots , s_{0r},\\varnothing,\\ldots,\\varnothing ) \\bigr ) \\\\ & & \\qquad\\geq \\prod_{l=1}^{r}\\pi ( s_{l}=s_{0l } ) \\prod_{l = r+1}^{[p^{\\gamma/2 } ] } \\biggl(1-\\frac{1}{p^{\\gamma+1 } } \\biggr)^{p } \\\\ & & \\qquad\\geq \\biggl(1-\\frac{1}{p^{\\gamma+1 } } \\biggr)^{p^{1+\\gamma/2 } } \\prod _ { l=1}^{r } \\biggl ( \\frac{1}{p^{\\gamma+1 } } \\biggr ) ^{|s_{0l}| } \\\\ & & \\qquad\\geq \\exp \\bigl(-2p^{-\\gamma/2 } \\bigr)p^{-rs(\\gamma+1 ) } \\\\ & & \\qquad\\geq\\exp \\bigl ( -(\\gamma+2)rs\\log p \\bigr)\\end{aligned}\\ ] ] because @xmath396 is at a smaller order of @xmath397 . then we lower bound @xmath398 when @xmath95})=(s_{01},\\ldots , s_{0r},\\varnothing , \\ldots,\\varnothing)$ ] , we have @xmath399 we use the notation @xmath400 to represent the probability @xmath401 defined in section  [ sec : main ] . by conditional independence , we have @xmath402 where @xmath403 . in particular , we choose @xmath404 with @xmath405 .",
    "then as long as @xmath406 , we have @xmath407 and @xmath408 define @xmath409 with @xmath410 using the chain rule , we have @xmath411 for each @xmath412 , we present a lower bound and prove it in the supplementary material [ @xcite ( @xcite ) ] .",
    "[ prop : chainrule ] for each @xmath413 , we have @xmath414 moreover , @xmath415 can be lower bounded by the above formula with @xmath416 .    using this result , we have @xmath417 for some absolute constant @xmath331 when @xmath418 .",
    "therefore , we have @xmath419 since @xmath420 we have @xmath421 under the assumption @xmath335 for some constant @xmath170 .",
    "we prove lemma  [ cor : sparsity ] in this section .",
    "the result is implied by the prior sparsity stated in the following lemma .",
    "[ lem : sparsity ] for the sparsity prior specified above , we have for any @xmath422 , @xmath423    proof of lemma  [ lem : sparsity ] first , we have @xmath424}|>\\operatorname{ars } \\bigr).\\ ] ] note that there is a slight abuse of notation above .",
    "the @xmath425 on the left - hand side are from @xmath426}$ ] on the right - hand side by excluding those @xmath103 with @xmath427 .",
    "let @xmath428}|$ ] .",
    "note that @xmath166 is a binomial random variable with parameter @xmath429 satisfying @xmath430 .",
    "therefore , @xmath431}^{p } { \\pmatrix{p \\cr k}}\\alpha ^{k}(1-\\alpha)^{p - k } \\leq\\sum _ { k=[\\operatorname{ars}]}^{p}{\\pmatrix{p \\cr k}}\\alpha ^{k } \\\\ & \\leq&\\sum_{k=[\\operatorname{ars}]}^{p}\\exp ( k\\log p ) \\bigl(p^{-1-\\gamma /2 } \\bigr)^{k } \\\\ & \\leq&\\sum_{k=[\\operatorname{ars}]}^{p}\\exp \\biggl(-k \\frac{\\gamma}{2}\\log p \\biggr ) \\leq\\exp \\biggl(-\\frac{a\\gamma}{4}rs\\log p \\biggr).\\end{aligned}\\ ] ] thus the proof is complete .",
    "now we are ready to prove lemma  [ cor : sparsity ] by upper bounding the numerator and lower bounding the denominator of @xmath432 .",
    "this can be done by combining the results of lemmas [ lem : sparsity ] , [ lem : denominator ] and [ lem : priorcon ] .",
    "proof of lemma  [ cor : sparsity ] since @xmath433 and @xmath434 , we have @xmath435 where we have used lemma  [ lem : denominator ] . using lemmas lem : sparsity and lemma  [ lem : priorcon ]",
    ", we have @xmath436 hence by choosing @xmath437 , we have @xmath438 the conclusion then follows by letting @xmath439 .",
    "we prove lemma  [ lem : test1 ] in this section . because of the constraint @xmath440",
    ", we can break the testing problem into many low - dimensional testing problems . then",
    "a final test can be constructed by combining the small tests .",
    "the following lemma establishes the existence of such a low - dimensional test and bounds its error probability .",
    "[ lem : spectest ] for the random variable @xmath441 in @xmath442 and any @xmath443 , there exists a testing function @xmath246 , such that @xmath444 with some absolute constant @xmath445 .",
    "notice @xmath446 is a general @xmath447 covariance matrix for some @xmath448 .",
    "it will be specified in the proof of lemma  [ lem : test1 ] . to prove lemma  [ lem : spectest ] , we need the following random matrix inequality . its proof is given in the supplementary material [ @xcite ( @xcite ) ] .",
    "[ lem : specconcen ] let @xmath449 be i.i.d . from @xmath450 , where @xmath446 is a @xmath447 covariance matrix .",
    "let @xmath451 be the sample covariance matrix , and then there is an absolute constant @xmath445 , such that for any @xmath297 , @xmath452    proof of lemma  [ lem : spectest ] denote the alternative set by @xmath453 , and then it will have following decomposition : @xmath454 where @xmath455 and for @xmath456 , @xmath457 we divide the alternative set into pieces so that the spectral norm of @xmath458 is bounded in each piece . for the prior in section  [ sec : main ] ,",
    "this is not needed because the prior only samples a random covariance matrix with bounded spectrum .",
    "however , the prior in section  [ sec : comp ] does not impose a bounded spectrum constraint .",
    "the strategy for dividing the alternative set is general for both cases .",
    "we test each alternative hypothesis separately and then combine the test and use the union bound to control the error . to test against @xmath459 , we use @xmath460 to test against @xmath461 , we use @xmath462 from lemma  [ lem : specconcen ] , we have @xmath463 and @xmath464 next , we control the type ii error . for any @xmath465 , we have @xmath466 for any @xmath461 , we have @xmath467 now we combine the test by @xmath468 .",
    "the error of the combined test can be bounded by @xmath469 and @xmath470 thus the proof is complete .    to prove lemma  [ lem : test1 ] , we combine the small tests and control the error by union bound .    proof of lemma  [ lem : test1 ] we denote the alternative set by @xmath471 define @xmath333 and @xmath472 .",
    "we decompose @xmath473 by @xmath474 where @xmath475 .",
    "define @xmath476 , and it is easy to see that @xmath477 where @xmath478 thus it is sufficient to test the following sub - problem in @xmath479 for each @xmath166 : @xmath480 by lemma  [ lem : spectest ] , there exists @xmath481 depending on the observations @xmath482 , such that @xmath483 then we combine the tests by @xmath484 . by the union bound , we have @xmath485}{\\pmatrix{p \\cr q } } \\biggr)3\\exp \\biggl(-c_{3 } \\biggl(\\frac{m^{2}}{4k^{2}}-(a+1 ) \\biggr)n \\varepsilon^{2 } \\biggr ) \\\\ & \\leq&3\\operatorname{ars}\\exp \\biggl(\\operatorname{ars}\\log\\frac{ep}{\\operatorname{ars } } \\biggr)\\exp \\biggl(-c_{3 } \\biggl ( \\frac{m^{2}}{4k^{2}}-(a+1 ) \\biggr)n \\varepsilon^{2 } \\biggr ) \\\\ & \\leq&3\\exp ( 2\\operatorname{ars}\\log p ) \\exp \\biggl(-c_{3 } \\biggl ( \\frac { m^{2}}{4k^{2}}-(a+1 ) \\biggr)n\\varepsilon^{2 } \\biggr ) \\\\ & \\leq&3\\exp \\biggl(- \\biggl(\\frac{c_{3}m^{2}}{4k^{2}}-c_{3}(a+1)-2a \\biggr)n\\varepsilon^{2 } \\biggr)\\end{aligned}\\ ] ] and @xmath486 hence the proof is complete by choosing sufficiently large @xmath365 .",
    "we prove lemma  [ lem : test2 ] in this section . at",
    "first thought , there seems to be no obvious test for testing the subspace projection matrix under the distance @xmath487 due to the complicated sparse and low - rank structure .",
    "our strategy is to break the alternative set into many levels and pieces .",
    "the goal is that for each piece , it is a low - dimensional small testing problem in the following form : @xmath488 the small testing problem can be solved by considering the likelihood ratio test .",
    "the error bound is stated in the following lemma . its proof is given in the supplementary material [ @xcite ( @xcite ) ] .",
    "[ lem : testfrob ] consider observations @xmath441 in @xmath442 .",
    "there exist constants @xmath489 and @xmath490 only depending on @xmath193 , and a testing function @xmath246 such that @xmath491 where @xmath492 is an absolute constant .",
    "we need a lemma to bound the covering number under different subspace distances .",
    "we use @xmath493 to denote the @xmath49 -covering number of @xmath494 under the distance @xmath495 .",
    "the proof of lemma  [ lem : dcover ] is given in the supplementary material [ @xcite ( @xcite ) ] .",
    "[ lem : dcover ] for any @xmath496 , @xmath497 and @xmath498 with @xmath499 , we have @xmath500    last but not least , we need the following sin - theta theorem to bound the difference of subspaces by the difference of matrices .    [",
    "lem : sintheta ] consider symmetric matrices @xmath501 and  @xmath502 , with eigenvalue decomposition @xmath503 if the eigenvalues @xmath504 are contained in an interval @xmath505 , and the eigenvalues @xmath506 are excluded from the interval @xmath507 for some @xmath508 , then @xmath509 and @xmath510    proof of lemma  [ lem : test2]the proof has two major steps .",
    "_ step _ 1 : decompose the alternative set into many levels and pieces .",
    "we first decompose @xmath390 by @xmath511 , where @xmath512 define @xmath513 with @xmath472 , and @xmath514 , \\\\",
    "v_{0,\\bar{b}}&= & \\bigl [ \\vert \\theta_{1,\\bar{b}}\\vert ^{-1}\\theta_{1,\\bar{b}},\\ldots,\\vert \\theta_{r,\\bar{b } } \\vert ^{-1}\\theta_{r,\\bar{b } } \\bigr].\\end{aligned}\\ ] ] note that both @xmath515 and @xmath516 are @xmath517 matrices with @xmath518 , and @xmath519 .",
    "then we can rewrite @xmath520 as @xmath521 where we omit @xmath200 for simplicity of notation , and we consider both @xmath84 and @xmath522 @xmath523 diagonal matrices from now on .    note that @xmath524 for any @xmath525 .",
    "we can show there exists diagonal matrices @xmath526 such that @xmath527 where @xmath528 , because we regard @xmath529 as a subset of @xmath530 so that it is essentially a covering number calculation in @xmath531 as in @xcite ( @xcite ) .",
    "we further decompose @xmath520 by @xmath532 , where @xmath533 and decompose @xmath534 by @xmath535 , where @xmath536    according to lemma  [ lem : dcover ] , there exists @xmath537 such that for some constant @xmath538 only depending on @xmath193 , @xmath539 where @xmath540 , and we may bound @xmath541 by @xmath542 when we choose @xmath543 . using the triangle inequality , we have @xmath544 therefore , @xmath545 by the sin - theta theorem ( lemma  [ lem : sintheta ] ) , we have @xmath546 hence @xmath547 our final decomposition is @xmath548 , where @xmath549    _ step _ 2 : combine tests from all levels and pieces . we have reduced the original testing problem to the above small pieces for each @xmath550 . for each small piece , it is equivalent to the testing problem in lemma  lem : testfrob . since we already know the coordinates @xmath551 , the testing problem is on @xmath552 . the observations in lemma  lem :",
    "testfrob is @xmath553 .",
    "the triple @xmath554 in lemma  lem : testfrob corresponds to @xmath555 for every @xmath550 .",
    "then by the conclusion of lemma  lem : testfrob , there exists a testing function @xmath556 with error bounded by @xmath557 for some @xmath558 only depending on @xmath193 and some absolute constant @xmath357 . since @xmath559 , we have @xmath560    now we are ready to integrate these little tests step by step for each index . for each @xmath561 , define @xmath562 and we have @xmath563 since we assume @xmath335 and @xmath177 , we have @xmath564,@xmath565 and @xmath566 .",
    "hence @xmath567 as long as we pick @xmath568 in addition , for each @xmath561 , @xmath569    for each @xmath570 , we define @xmath571 whose errors are bounded as follows : @xmath572 and @xmath573    for each @xmath166 , we define @xmath574 and we have the errors bounded by @xmath575 and @xmath576 finally , the ultimate test is defined as @xmath577 with type i error @xmath578 bounded by @xmath579 } { \\pmatrix{p \\cr q } } \\biggr ) 3\\exp \\biggl(-\\frac{1}{4}c_{5 } \\delta_{k}^{\\prime}\\bar{m}^{2}n\\varepsilon^{2 } \\biggr ) \\\\ & \\leq&3\\operatorname{ars}\\exp ( \\operatorname{ars}\\log p ) \\exp \\biggl(-\\frac { 1}{4}c_{5 } \\delta _ { k}^{\\prime}\\bar{m}^{2}n\\varepsilon^{2 } \\biggr ) \\\\ & \\leq&3\\exp ( 2\\operatorname{ars}\\log p ) \\exp \\biggl(-\\frac{1}{4}c_{5}\\delta _ { k}^{\\prime}\\bar{m}^{2}n\\varepsilon^{2 } \\biggr ) \\\\ & \\leq&3\\exp \\biggl(- \\biggl(\\frac{1}{4}c_{5 } \\delta_{k}^{\\prime}\\bar { m}^{2}-2a \\biggr)n\\varepsilon^{2 } \\biggr ) \\\\ & \\leq&3\\exp \\biggl(-\\frac{1}{8}c_{5}\\delta_{k}^{\\prime } \\bar{m}^{2}n\\varepsilon^{2 } \\biggr),\\end{aligned}\\ ] ] as long as we choose @xmath580 , and for type ii error we have @xmath581 thus the proof is complete .",
    "we thank the referees and the associate editor for giving valuable and insightful suggestions , which lead to significant improvement of the paper ."
  ],
  "abstract_text": [
    "<S> principal component analysis ( pca ) is possibly one of the most widely used statistical tools to recover a low - rank structure of the data . in the high - dimensional settings </S>",
    "<S> , the leading eigenvector of the sample covariance can be nearly orthogonal to the true eigenvector . </S>",
    "<S> a sparse structure is then commonly assumed along with a low rank structure . </S>",
    "<S> recently , minimax estimation rates of sparse pca were established under various interesting settings . on the other side , bayesian methods are becoming more and more popular in high - dimensional estimation , but there is little work to connect frequentist properties and bayesian methodologies for high - dimensional data analysis . in this paper , we propose a prior for the sparse pca problem and analyze its theoretical properties . </S>",
    "<S> the prior adapts to both sparsity and rank . </S>",
    "<S> the posterior distribution is shown to contract to the truth at optimal minimax rates . </S>",
    "<S> in addition , a computationally efficient strategy for the rank - one case is discussed . </S>"
  ]
}