{
  "article_text": [
    "in the geostatistical literature , likelihood - based methods for parameter estimation make explicit assumptions on the statistical distributions of the spatial variable in question , thus contrast with some traditional methods such as ones based on curve - fitting for an empirical variogram model @xcite .",
    "the distributional assumptions are manifested , for example , in `` model - based geostatistics '' @xcite and `` trans - gaussian '' models @xcite .",
    "first employed apparently in the early 1980s , likelihood - based methods have by now established themselves as a standard approach ( see * ? ? ? * for a review ) .    given the distributional assumptions ,",
    "one may obtain maximum likelihood ( ml ) estimates of the parameters , such as parameters in the spatial covariance function .",
    "however , estimating an `` optimal '' value for the parameters of the covariance function via ml methods is flawed because , according to @xcite , the profile likelihood can be multimodal and is often nearly flat in the neighborhood of its mode .",
    "these problems suggest that ml estimates may be difficult to find and such estimates , if found , may not be truly `` representative '' or `` optimal '' . @xcite",
    "further emphasize that `` plug - in '' measures of prediction uncertainty based on ml parameter estimates tend to be over optimistic .",
    "they argue that a bayesian approach mitigates this problem . indeed",
    ", parallel to the growth of bayesian statistics in general , bayesian geostatistics ( or bayesian kriging ) has enjoyed steady growth since the 1980s @xcite .    in this study , we take a typical geostatistical formulation and present an algorithm for deriving a numerical representation of the posterior distribution of the parameters . the parameter vector , @xmath1 ,",
    "consists of standard elements such as trend coefficients @xmath2 , variance @xmath3 , scale @xmath4 , smoothness @xmath5 , etc .",
    "it also accommodates geometric anisotropy .",
    "the prior specification combines standard non - informative priors ( for @xmath2 and @xmath3 ) and informative priors ( for @xmath4 and @xmath5 ) .",
    "the algorithm uses a normal mixture to approximate the posterior density ; the approximation is updated iteratively to approach the true posterior distribution .",
    "the proposed algorithm avoids two empirical treatments that have been used in the literature , namely , imposing bounds on an unbounded parameter and discretizing a continuous parameter .",
    "one parameter that has received such treatments is the scale parameter , @xmath4 @xcite .",
    "these treatments have conceptual as well as practical complications .",
    "conceptually , one needs to take great care to defend one s subjectively chosen bounds for an unbounded parameter by showing that the posterior inference is not sensitive to these bounds @xcite .",
    "practically , discretizing a continuous parameter renders the computational cost directly proportional to the resolution of the discretization .",
    "if discretization and artificial bounds are applied simultaneously on one parameter , say the scale @xmath4 , the desires to use wide bounds and dense discretization exert great burden on computation .",
    "furthermore , the discretization approach is not scalable , in the sense that the computational cost grows exponentially as the number of parameters being discretized increases .",
    "the central step of the algorithm , multivariate kernel density estimation , is a standard task with unsolved difficulties @xcite .",
    "our algorithm makes particular efforts to determine two tuning parameters , one concerning localization and the other concerning bandwidth , by a likelihood criterion .",
    "another technical difficulty arises from high skewness of importance weights , especially in early iterations .",
    "this problem is alleviated by a `` flattening '' transform .",
    "this study intends to provide a generic , or routine , procedure for the posterior inference of bayesian kriging parameters .",
    "as already mentioned , the procedure specifies a prior with limited user intervention , improves an initial , rough approximation iteratively , and avoids some ad hoc treatments that have been used in the literature .",
    "moreover , the procedure is readily applicable to other model formulations so long as all the model parameters are defined on @xmath0 or can be transformed to be so . in this sense ,",
    "application of the algorithm extends beyond geostatistics .",
    "the article is organized as follows .",
    "the geostatistical parameterization for a spatial variable is described in section  [ sec : parameterization ] , with details on the matrn correlation function , geometric anisotropy , the likelihood function , and specification of the prior .",
    "the technical core of this article , the iterative algorithm for posterior inference , is the subject of section  [ sec : algorithm ] .",
    "the proposed method is capable of dealing with a type of `` change - of - support '' problems ; this is briefly discussed in section  [ sec : change - of - support ] . in section  [ sec : examples ] , we apply the geostatistical model and the algorithm in two examples , one using synthetic data and the other using historical data . section  [ sec : concl ] concludes the article with a summary .",
    "let @xmath6 be a spatial random variable , where @xmath7 is location in @xmath8-dimensional space ( hence @xmath9 is a @xmath8-vector , @xmath10 ) .",
    "we adopt the following mixed - effects model for @xmath6 : @xmath11 where @xmath12 is a @xmath13-vector of deterministic covariates ( e.g. polynomials of the spatial coordinates ) ; @xmath2 is a @xmath13-vector of trend coefficients ; @xmath14 is a zero - mean , unit - variance , stationary gaussian process ; @xmath15 is an iid standard normal white - noise process ; @xmath16 is the standard deviation of @xmath6 ; and @xmath17 is a nugget parameter .",
    "the process @xmath14 is characterized by a correlation function parameterized by a @xmath18-vector @xmath19 .",
    "in addition , we assume @xmath14 and @xmath15 are independent of each other .",
    "we denote the full parameter vector by @xmath20 . modeling efforts are typically concentrated on inferencing and interpreting the nugget parameter @xmath21 , the variance @xmath3 , and the correlation parameter(s ) @xmath19 .",
    "the content of @xmath19 depends on the specific correlation function employed ; our choice here is the matrn correlation function , to be described below .",
    "in addition , the formulation above allows for geometric anisotropy , parameters of which are also contained in @xmath19 .",
    "we assume spatial stationarity for @xmath22 , hence the correlation between @xmath23 and @xmath24 is a function of scaled distance , denoted by @xmath25 , where @xmath4 is the `` scale '' parameter .",
    "the marginal distribution of @xmath6 in the formulation  ( [ eq : y - mixed - model ] ) is @xmath26 . of the total variance @xmath3 ,",
    "@xmath27 is contributed by the white noise component @xmath28 , whereas the spatially correlated component @xmath29 contributes a variance of @xmath30 . in other words ,",
    "the so - called `` nugget effect '' accounts for a fraction @xmath21 of the total variance .",
    "the covariance between @xmath23 and @xmath24 is @xmath31 where @xmath32 is the identity function , assuming value 1 if @xmath33 coincides with @xmath34 and 0 otherwise , and @xmath35 is taken to be the matrn correlation function : @xmath36 where @xmath37 is the gamma function , @xmath38 is the modified bessel function of the third kind of order @xmath39 ( * ? ? ?",
    "* secs  9.6 and 10.2 ) ; and @xmath39 is the smoothness parameter @xcite .    in summary , in a 1-d model or 2- and 3-d isotropic models ,",
    "the correlation parameter @xmath19 contains two elements , @xmath4 and @xmath39 . in an anisotropic model",
    ", @xmath19 contains additional elements that characterize anisotropy , as we discuss next .",
    "when @xmath40 , our formulation allows for geometric anisotropy .",
    "@xcite distinguishes three forms of geometric anisotropy , including anisotropy in sill , in range ( the `` scale '' parameter here ) , and in nugget , respectively .",
    "range anisotropy is the most commonly discussed and is also the anisotropy considered here .",
    "when such anisotropy is present , the scaled distance , @xmath41 , is calculated in a transformed coordinate system , which is obtained by rotating the `` natural '' axes to the major and minor directions of anisotropy , and using different scales ( @xmath4 s ) along different axes .",
    "specifically , in 2-d we need one angle ( @xmath42 ) and two scales ( @xmath43 , @xmath44 ) to describe such anisotropy , whereas in 3-d we need three angles ( @xmath45 , @xmath46 , @xmath47 ) and three scales ( @xmath43 , @xmath44 , @xmath48 ) .",
    "the rotational angles determine a transformation matrix , @xmath49 ( see , * ? ? ?",
    "* ) . with the matrix @xmath49 and directional scales @xmath50 ,",
    "define @xmath51    { \\ensuremath{{{{\\boldsymbol{a}}}}^t } } , \\ ] ] where @xmath52 denotes the diagonal matrix with @xmath53 on the main diagonal .",
    "the scaled distance is then defined by @xmath54}^t } }      { \\boldsymbol{b } } \\ ,      [ { x}_1 - { x}_2 ] } .\\ ] ] note that the @xmath55 is a column vector of length @xmath8 .",
    "this @xmath41 is used in  ( [ eq : matern - corr ] ) to calculate @xmath35 .    in a study of geometric anisotropy in 2-d ,",
    "@xcite treat the matrix @xmath56 as parameter and derive @xmath49 and @xmath4 from @xmath56 .",
    "they discuss , in a bayesian context , how to specify a prior for @xmath56 using the wishart distribution .",
    "the parameterization we adopt goes in the opposite direction . with our parameterization , priors for the angle(s ) @xmath42 and the scales @xmath4 are specified .",
    "this parameterization has some advantages in terms of interpretation and intuition .",
    "these two parameterizations contain the same number of unknowns .",
    "@xcite studies more general forms of anisotropy that are not considered here .    in summary , in a 2-d anisotropic model ,",
    "the correlation parameter @xmath19 consists of elements @xmath39 , @xmath42 , and @xmath43 , @xmath44 ; in a 3-d anisotropic model , @xmath19 consists of @xmath39 , @xmath45 , @xmath46 , @xmath47 , and @xmath43 , @xmath44 , @xmath48 .      the parameter vector is denoted by @xmath20 .",
    "the content of the correlation parameter @xmath19 depends on the spatial dimension @xmath8 and whether anisotropy is considered , as discussed above .",
    "suppose we have measurements of @xmath22 , denoted by @xmath57 , at @xmath58 locations @xmath59 .",
    "the likelihood function of @xmath1 with respect to @xmath57 is equal to @xmath60 where @xmath61 is the `` design matrix '' of covariates corresponding to @xmath59 , each row being @xmath12 for a single location @xmath9 ; @xmath62 is the correlation matrix between the locations @xmath59 , calculated using the relations ( [ eq : cov ] ) and  ( [ eq : matern - corr ] ) .    in applications ,",
    "the parameter vector @xmath1 could be simplified depending on the actual situation of the spatial variable and emphasis of the investigation .",
    "for example , one might choose to fix the smoothness @xmath39 at a certain value , say 0.5 or 1.5 .",
    "( then , @xmath39 would not appear in @xmath1 . ) for another example , one might decide , based on background knowledge , not to consider anisotropy , hence @xmath4 would be a scalar , and @xmath19 would not contain @xmath42 .",
    "when anisotropy is considered , the coordinate rotation affects location - aware calculations including the trend function ( via @xmath12 and @xmath2 ) and the correlation .",
    "we choose to define the trend function in terms of the original coordinates .",
    "the angles ( @xmath42 s ) define the rotations ; the scales ( @xmath4 s ) are applied along the axes after the rotation .",
    "the parameters @xmath39 , @xmath21 , and @xmath3 do not have directional components because we consider range anisotropy only .",
    "we specify a prior with independent components : @xmath63 ( remember that @xmath42 appears in anisotropic models only . in anisotropic models , @xmath4 contains , and @xmath42 may contain , more than one elements . ) this specifies a flat prior on @xmath0 for the trend coefficients @xmath2 and a conventional noninformative prior for the variance @xmath3 .",
    "it is sensible to use a diffuse prior for @xmath4 , whereas subjective information ( or preference ) about @xmath39 and @xmath21 may be injected into their priors .",
    "relevant discussions can be found in @xcite , @xcite , @xcite , and @xcite .",
    "additional parameters need to be chosen for the beta and gamma distributions .",
    "some details are listed below",
    ". however , bear in mind that these particularities are empirical and subject to adjustment .",
    "@xmath64 : this is taken to be @xmath65 .",
    "this prior places more weight on small nugget values .",
    "@xmath66 : this is taken to be @xmath67 , where @xmath68 is the size of the model domain .",
    "this is actually the exponential distribution with median @xmath69 .",
    "@xmath70 : parameters of this gamma distribution are chosen such that its mode is 1.5 and its variance is 4.0 .",
    "we estimate the posterior distribution of the parameter vector , @xmath1 , by normal mixtures in an iterative procedure .",
    "the versatility of normal mixtures in approximating complex densities is documented by @xcite .",
    "the normal kernel implies that all components of @xmath1 must be defined on @xmath0 .",
    "this requires that the support of each component of @xmath1 is one of @xmath0 , @xmath71 , @xmath72 , and @xmath73 , where @xmath74 , @xmath75 , and @xmath76 are constants .",
    "parameters defined on half - bounded intervals ( such as @xmath3 , @xmath4 , and @xmath39 ) may be log - transformed .",
    "parameters defined on bounded intervals ( such as @xmath21 and @xmath42 ) may be logit - transformed .",
    "in fact , the algorithm below is applicable to any model formulation as long as all the parameters are defined on @xmath0 , either directly or after transformation .",
    "the prior given by  ( [ eq : prior ] ) is for the parameters on their natural scale .",
    "the prior for the transformed parameters are determined by ( [ eq : prior ] ) and the transformations . to avoid clutter in notation , we shall still use @xmath1 to denote the parameter vector ( now transformed ) and refer to ( [ eq : prior ] ) for its prior , although in reality the prior of the transformed parameter is a modified form of ( [ eq : prior ] ) .",
    "the algorithm actually derives posterior distribution of these _ transformed _ parameters .",
    "distribution of the parameters on their natural scale can be studied based on back - transformed samples from the derived posterior distribution .",
    "we begin with an initial approximation to the posterior , denoted by @xmath77 , which is taken to be a sufficiently diffuse multivariate normal distribution . in the @xmath78th iteration",
    ", the current approximation @xmath79 is updated to @xmath80 in three steps as follows .    1 .",
    "take a random sample , @xmath81 , from @xmath79 .",
    "2 .   for @xmath82 , compute the non - normalized posterior density @xmath83 and the proposal density @xmath84 ; let @xmath85 be the importance weight of @xmath86 .",
    "3 .   update the approximate posterior from @xmath79 to @xmath87 this is a mixture of @xmath58 normal densities ( denoted by @xmath88 ) , each with mean @xmath86 and covariance matrix @xmath89 .",
    "computation of @xmath89 is detailed in section  [ sec : localization ] .",
    "this algorithm does not require one to be able to draw a random sample from the prior of @xmath1 .",
    "instead , a convenient initial approximation , @xmath90 , starts the procedure .",
    "one only needs to be able to _ calculate _ the prior density for any particular value of @xmath1 .",
    "this provides great flexibilities in choosing the prior @xmath91 and the initial approximation @xmath90 .",
    "sampling from a normal mixture distribution is easy .",
    "note that @xmath80 is a normal mixture , just like @xmath79 , and is ready to be updated in the next iteration .",
    "alternatively , one may terminate the iteration according to certain empirical criterion , and take @xmath80 as the final approximate posterior distribution of the parameter @xmath1 .",
    "some properties of @xmath80 may be examined semi - analytically .",
    "more often , one is more interested in the unknown field @xmath22 or a function thereof than in the parameter @xmath1 itself .",
    "properties of @xmath22 or a function thereof may be investigated via sampling @xmath1 from @xmath92 and simulating ( i.e. sampling ) @xmath93 ( realization of the field ) according to  ( [ eq : likelihood ] ) .",
    "the step  3 of the algorithm entails kernel density estimation , which is a standard but un - settled task @xcite .",
    "the covariance matrix @xmath89 may be expressed as @xmath94 , where @xmath95 is the empirical weighted covariance matrix of the sample points ( @xmath1 s ) in a certain neighborhood of @xmath86 , and @xmath96 is a `` bandwidth '' parameter .",
    "while @xmath95 specifies the shape of the kernel centered at @xmath86 , the bandwidth @xmath96 further adjusts the spread of this kernel .",
    "several factors complicate the computation of @xmath89 .",
    "first , the distribution of the importance weights , @xmath97 , can be highly skewed , especially in early iterations when the proposal distribution tends to be very different from the true posterior distribution . in not - so - rare pathological cases , a few sample points ( or a single sample point ) carry a dominant fraction of the total weight , making the other sample points negligible .",
    "when this happens , @xmath95 may contain variance entries that are nearly 0 . to mitigate this problem",
    ", we use a `` flattened '' version of weights in the computation of @xmath95 .",
    "let @xmath98 the exponent @xmath99 is the `` entropy '' of @xmath100 , a measure of the uniformity of the weights ( see * ? ? ?",
    "if @xmath100 are all equal , then @xmath101 . at the other extreme , if one @xmath97 is 1 and all the other weights are 0 , then @xmath102 .",
    "note that the weights @xmath103 are used in calculating the empirical weighted covariance matrix @xmath95 ; they do not replace the weights @xmath100 in  ( [ eq : mixture - normal ] ) .",
    "as the algorithm proceeds in iterations , the importance weights @xmath100 become more uniform , hence @xmath99 becomes closer to 1 , and the adjustment to @xmath100 by the above `` flattening '' becomes minor .",
    "the second complication is in the `` localization '' of @xmath95 , that is , we choose to define @xmath95 as the empirical weighted covariance matrix of sample points in a `` neighborhood '' of @xmath86 , say sample points that take up a fraction @xmath104 , @xmath105 , of the entire sample .",
    "such localization is important if the target distribution ( i.e. the posterior ) is severely multi - modal @xcite .",
    "however , there is no guidance on the determination of the fraction @xmath104 .",
    "the third complication is due to the bandwidth @xmath96 .",
    "a number of adaptive procedures have been proposed for choosing @xmath106 @xcite .",
    "however , most of the literature in kernel density estimation is developed based on a _ random _ sample , whereas what we have here is a _ weighted _ one .",
    "in addition , localization , as parameterized by @xmath104 , has received less attention in the literature .",
    "traditional rule - of - thumb choices for the bandwidth parameter @xcite do not apply directly to a localized algorithm , because the rules are based on analysis of global estimators .",
    "a localized algorithm encounters other difficulties , such as edge effects , that do not arise in a global analysis .    to simplify the matter",
    ", we use a common bandwidth parameter , denoted by @xmath106 ( where @xmath107 ) , and a common localization parameter , denoted by @xmath108 ( where @xmath109 ) , for all the mixture components .",
    "sensible choices of these two tuning parameters depend on the sample size @xmath58 , the dimensionality of @xmath1 , and characteristics of the target distribution @xmath110 .",
    "we determine their values by a maximum likelihood cross - validation criterion : @xmath111 where @xmath112 note that @xmath113 is a function of @xmath108 , @xmath114 , and @xmath103 . in words , @xmath115 is the usual log likelihood of @xmath108 and @xmath106 with respect to the weighted sample @xmath114 , except that the density at @xmath86 is calculated by the mixture density _ leaving out _ the mixture component centered at @xmath86 . leaving out the offending mixture component is key .",
    "otherwise , maximizing @xmath115 would drive @xmath106 to be arbitrarily small .",
    "the idea above is quite general , and may be extended to determine other tuning parameters .    to reduce the computational cost of this optimization , a combination of discrete search for @xmath108 and continuous search for @xmath106 is performed as sketched below .    1 .",
    "let @xmath116 , that is , assign value @xmath117 to the variable @xmath118 .",
    "2 .   let @xmath119 .",
    "+ let @xmath120 .",
    "+ compute the empirical weighted covariance matrix of the entire sample @xmath81 , using the flattened weights @xmath103 instead of the original weights @xmath100 .",
    "denote the result by @xmath121 , and let @xmath122 .",
    "[ it : optim - h ] find @xmath106 that maximizes  ( [ eq : obj - kernel ] ) . denote the maximizing @xmath106 by @xmath123 and the achieved maximum @xmath115 by @xmath124 .",
    "+ ( this is a univariate optimization problem . since we have values of @xmath125 , ... ,",
    "@xmath126 , the parameter @xmath108 does not appear in the calculation of  ( [ eq : obj - kernel ] ) . )",
    "+ if @xmath127 , then let @xmath128 , @xmath129 , @xmath130 , and @xmath131 for @xmath82 .",
    "[ it : halve - r ] if @xmath108 is below a pre - specified threshold fraction , say @xmath132 , go to step  [ it : conclude ] . otherwise , let @xmath133 and go to step  [ it : localize ] .",
    "[ it : localize ] for @xmath82 , 1 .   within the set @xmath134 , identify the @xmath135 closest neighbors of @xmath86 in the mahalonobis sense measured by the covariance matrix @xmath95 .",
    "update @xmath134 to be the set that contains these newly identified neighbors .",
    "2 .   compute the empirical weighted covariance matrix , @xmath95 , based on the sample @xmath134 and the corresponding relative weights @xmath103 . + go to step  [ it : optim - h ] .",
    "[ it : conclude ] adopt @xmath136 and @xmath137 as the final values for @xmath108 and @xmath106 , respectively , and let the @xmath89 in  ( [ eq : mixture - normal ] ) be @xmath138 for @xmath82 . + this concludes the search for optimum values of @xmath108 and @xmath106 .",
    "the ideas of normal mixture and iterative updating are used by @xcite .",
    "the procedure in @xcite sets the bandwidth parameter following empirical rules and does not consider localization .",
    "the data @xmath57 in  ( [ eq : likelihood ] ) are the values of @xmath22 at individual locations @xmath59 .",
    "this formulation can be easily generalized to use data that are _ linear _ functions of @xmath22 .",
    "let the data vector , denoted by @xmath139 , be expressed as @xmath140 where @xmath57 is a @xmath58-vector of @xmath22 at locations @xmath59 and @xmath141 is a @xmath142 matrix of rank @xmath143 , where @xmath144 .",
    "correspondingly , the likelihood  ( [ eq : likelihood ] ) is replaced by @xmath145^{-1 }         \\bigl({{\\boldsymbol{z}}}- { { \\boldsymbol{h}}}{{\\boldsymbol{x}}}{\\beta}\\bigr )         \\bigr ) , \\ ] ] where @xmath61 is the design matrix for the locations @xmath59 , and @xmath62 is the correlation matrix between the locations @xmath59 .",
    "the algorithm described in section  [ sec : algorithm ] works for this form of data and likelihood without modification .    in applications , measurements of @xmath22",
    "often have areal or volume ( or , in 1-d , interval ) support rather than point support .",
    "consider studies in which the model domain is discretized into a numerical grid .",
    "let us call the numerical grid the `` basic '' spatial unit ( or support ) in the model .",
    "it may occur that a data value is some function ( e.g. the average ) of @xmath146 in more than one basic spatial unit .",
    "specifically , we may distinguish `` point data '' , which have the basic support in the model , and `` linear data '' , which have aggregated support and can be expressed as linear functions ( such as simple averages ) of point data . increasingly in environmental studies , available data include a mix of satellite- and ground - based measurements that span a hierarchy of spatial supports .",
    "this entails the so - called `` change of support '' problems @xcite , a typical example being `` downscaling '' problems .",
    "the method described above is able to use data on a variety of supports as long as they are all _ linear _ functions of @xmath22 on the basic support .    besides being a natural situation due to data resources on disparate scales",
    ", linear data can also be useful by methodological design .",
    "for example , @xcite proposes an inverse algorithm in which a key model device is linear functions of the random field .",
    "we illustrate the algorithm with two examples .",
    "the first example uses a satellite elevation dataset to demonstrate the algorithm s performance in a 2-d , anisotropic setting . because the true field is known in this case , model performance can be assessed by comparing conditional simulations with the true field .",
    "the second example uses a historical dataset of soil moisture . in this realistic setting ,",
    "the true field is unknown , and the sampling locations of the measurements are not ideal as far as interpolation is concerned ( as is a common situation with historical data ) .",
    "however , the point of the examples is not to interpolate , but to illustrate how the algorithm works with available data to approximate the posterior distribution of the model parameters in an iterative procedure .",
    "we extracted satellite tomography data from the national elevation dataset ( ned ) on the web site of the national map seamless server operated by the u.s .",
    "geological survey , eros data center , sioux falls , south dakota .",
    "the particular dataset we used covers a region in the appalachian mountains on a @xmath147 grid .",
    "the elevation map is shown in figure  [ fig : appal - datamap ] , which also marks 20 randomly selected locations that provided synthetic measurements .",
    "we modeled the elevation , denoted by @xmath6 , by the geostatistical formulation described in section  [ sec : parameterization ] with a linear trend function and geometric anisotropy .",
    "the smoothness @xmath39 was fixed at 1.5 .",
    "this formulation has eight parameters : @xmath148 , @xmath149 , @xmath150 , @xmath42 , @xmath151 , @xmath152 , @xmath3 , and @xmath21 . as mentioned in section  [ sec : algorithm ] , the algorithm works with a transformed parameter vector : @xmath153 .",
    "it is straightforward to study the parameters in their natural units by back - transforming samples from the estimated posterior distribution of @xmath1 .",
    "the initial approximation @xmath77 was taken to be the product of eight independent and fairly diffuse normal distributions , one for each component of @xmath1 .",
    "this initial distribution was updated eight times in the iterative algorithm . during the iterations , the approximate posterior , @xmath80 , converged to the true posterior , @xmath154 ( up to a normalizing factor ) .",
    "the convergence was examined via two measures that indicate the `` closeness '' or `` distance '' of two distributions .",
    "the first measure is the entropy of importance sampling weights .",
    "consider the sample @xmath155 obtained in step  1 ( see section  [ sec : algor ] ) of the @xmath78-th iteration , which is a random sample from the density @xmath156 .",
    "the entropy of the importance weights @xmath157 , obtained in step  2 of the algorithm , is the @xmath99 defined in  ( [ eq : flatten - by - entropy ] ) .",
    "the entropy can be used as an indicator of how close the estimated distribution , @xmath156 , is to the true posterior distribution .",
    "an entropy value close to 1 indicates good approximation @xcite .",
    "the second measure is the @xmath158 distance between the estimated posterior , say @xmath159 , and the true posterior , @xmath160 , where @xmath74 is an unknown normalizing constant .",
    "the @xmath158 distance is defined as @xmath161 , hence @xmath162 }      \\biggr\\rvert , \\end{split}\\ ] ] where the two expectations are with respect to the distribution @xmath159 .",
    "therefore , a monte carlo estimate of this distance is @xmath163 making use of the fact that the sample mean of @xmath164 is @xmath165 , i.e. @xmath166 .    the values of @xmath99 and @xmath167 in each iteration are listed in table  [ tab : appal - converge ] , which shows definite trends of increase in @xmath99 and decrease in @xmath167 .",
    "l|cccccccccc iteration ( @xmath78 ) & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 + sample size ( @xmath58 ) & 3000 & 2800 & 2620 & 2458 & 2312 & 2181 & 2063 & 1957 + entropy ( @xmath99 ) & 0.12 & 0.35 & 0.37 & 0.57 & 0.80 & 0.88 & 0.91 & 0.92 + @xmath158 distance ( @xmath167 ) & 1.99 & 1.92 & 1.86 & 1.68 & 1.22 & 1.05 & 0.88 & 0.84 +    l|ccccccccc parameter & @xmath148 & @xmath149 & @xmath150 & @xmath151 & @xmath152 & @xmath168 & @xmath3 & @xmath21 + sample mean & 941 & -9.01 & 0.81 & 36.1 & 2.90 & 0.32 & 4.95e04 & 0.06 + sample s.d . & 168 & 4.67 & 11.6 & 30.7 & 2.15 & 0.08 & 7.12e04 & 0.09 +    in total we obtained nine approximate posterior distributions , including the initial approximation and the eight subsequent updates . from each of these approximations we drew 1000 samples of the parameter vector @xmath1 .",
    "the marginal distribution of each component of @xmath1 is plotted in figure  [ fig : appal - geost - marginal ] .",
    "it can be seen that the posterior approximations stabilized after six or seven iterations , consistent with the quantitative indicators in table  [ tab : appal - converge ] .",
    "the stabilized approximate distributions of the scale , variance , and nugget parameters are more outspread than those of the trend and rotation parameters .",
    "this is related to the interactions between the former group of parameters , which cause identifiability difficulties ( see * ? ? ?",
    "to provide some idea of the posterior mean and uncertainty of the individual parameter components , table  [ tab : appal - mean ] lists the empirical means and standard deviations of the eight parameter components calculated using the 1000 samples from @xmath169 , the final approximation .",
    "the empirical posterior mean of the rotation , @xmath42 , is 0.32 , i.e. 18 degrees .",
    "the empirical posterior mean of the scale along the rotated horizontal axis ( @xmath151 ) is 36.1 , in contrast to 2.90 along the vertical axis .",
    "the angle and scales are in keeping with what we observe in the synthetic true field .",
    "we note that , with the 20 irregularly located measurements in this example , these anisotropy parameters would be difficult to estimate by methods based on curve - fitting for empirical variograms ( see ,",
    "e.g. * ? ? ?",
    "* ) .    in geostatistical analysis ,",
    "one of the usual goals is to provide an interpolated map of the spatial field accompanied by a measure of uncertainty .",
    "we conducted 100 conditional simulations based on the final approximation to the posterior distribution of the parameter .",
    "the point - wise median of the simulations is shown in figure  [ fig : appal - medianmap ] .",
    "a comparison of figure  [ fig : appal - medianmap ] with figure  [ fig : appal - datamap ] confirms that the model and algorithm have captured main features of the true spatial field .",
    "the point - wise standard deviation of the simulations is shown in figure  [ fig : appal - uncertaintymap ] .",
    "the level of uncertainty in the simulated fields is largely uniform throughout the model domain , because the locations of the observations are reasonably balanced in the model domain .      in the second example",
    ", we used a hydrologic dataset provided by the southeast watershed research laboratory ( sewrl ) of the u.s .",
    "department of agriculture .",
    "the dataset contains long - term records of a number of hydrologic variables for the little river experimental watershed in south - central georgia , united states .",
    "this research program as well as its data products are described in @xcite .",
    "for illustration , we focused on a time snapshot ( specifically , at 18:00 on 1 august 2007 ) of soil moisture represented by measurements at 29 irregularly - located gauges .",
    "the measurements are shown in figure  [ fig : soil - datamap ] .",
    "details about the regional geography and the moisture data can be found in @xcite and @xcite , respectively .",
    "because soil moisture is a percentage , we took its logit transform as the spatial variable @xmath146 , that is , @xmath170 .",
    "( the upper bound was taken to be 0.5 because it was noticed that the largest observed value is 0.34 . )",
    "such a transformation is needed because @xmath146 must be defined on @xmath171 in order to be modeled as a gaussian variable .",
    "this treatment is in line with generalized linear models in model - based geostatistics @xcite ; but see @xcite for an alternative approach .    with the parameterization described in section  [ sec : parameterization ]",
    ", we took the trend model @xmath172 to be a constant @xmath2 , fixed the smoothness parameter @xmath39 at 0.5 ( i.e. used an exponential correlation function ) , and did not consider anisotropy .",
    "this left us with four parameters : @xmath2 , @xmath3 , @xmath4 , and @xmath21 .",
    "the algorithm worked with a transformed parameter vector : @xmath173 .",
    "construction of the prior and the initial approximate posterior distributions followed procedures similar to those in the first example .",
    "the initial distribution was updated five times .",
    "the convergence of the approximate posterior distributions to the truth were examined via the two measures @xmath99 and @xmath167 as listed in table  [ tab : soil - converge ] .",
    "the values listed show definite trends of increase in @xmath99 and decrease in @xmath167 .",
    "as @xmath99 and @xmath167 approached their respective limits , 1 and 0 , their values began to `` level off '' .",
    "l|ccccccc iteration ( @xmath78 ) & 1 & 2 & 3 & 4 & 5 + sample size ( @xmath58 ) & 2000 & 1880 & 1772 & 1675 & 1587 + entropy ( @xmath99 ) & 0.47 & 0.92 & 0.98 & 0.98 & 0.99 + @xmath158 distance ( @xmath167 ) & 1.82 & 0.87 & 0.40 & 0.36 & 0.31 +    l|ccccccc parameter & @xmath2 & @xmath4 & @xmath3 & @xmath21 + sample mean & -0.78 & 30897 & 0.52 & 0.21 + sample s.d . & 0.39 & 56140 & 0.41 & 0.16 +    table  [ tab : soil - mean ] lists the empirical means and standard deviations of the four parameter components obtained using 1000 samples from the final posterior approximation .    a major purpose of a geostatistical analysis like the current one is to generate an ensemble of soil moisture maps to be used in subsequent studies that require the value of soil moisture in the entire domain .",
    "we divided the model domain into 41 ( east - west ) by 74 ( north - south ) grids , each of size @xmath174 , and conducted 100 conditional simulations based on the approximate posterior distribution obtained in the final iteration of the algorithm .",
    "each simulation was back - transformed to give a moisture map with values in @xmath175 , in contrast to the variable @xmath176 in the model .",
    "the point - wise median of the simulations is shown in figure  [ fig : soil - medianmap ] , which confirms a high - moisture area in the upper - central section of the model domain .",
    "more scientific insights are expected if one examines the simulated soil moisture maps in the context of other hydrologic variables as well as the geography .",
    "we have described a bayesian geostatistical framework and proposed an iterative algorithm for deriving the posterior distribution of the parameter vector . the contribution of this study is to provide a general inference procedure that avoids some difficult elements utilized in practice , including ( 1 ) fitting a variogram curve ; ( 2 ) imposing bounds on an unbounded parameter ; ( 3 ) discretizing a continuous parameter . moreover ,",
    "the procedure can be applied to other model formulations as long as the model parameters , or transforms thereof , are defined on @xmath0 .",
    "common transformations that achieve this goal include the logarithmic and logistic transformations , as exemplified in section  [ sec : examples ] .",
    "the algorithm centers on normal kernel density estimation .",
    "particular efforts are made to determine the localization and bandwidth parameters in a systematic fashion .",
    "difficulties caused by highly skewed importance sampling weights are alleviated by `` flattening '' the weights .",
    "the method was demonstrated by two examples using synthetic and historical data . in both examples",
    ", we examined convergence of the approximate posterior distributions , as well as features of the marginal posterior distributions .",
    "the estimated posterior distributions served as a basis for conditional simulations of the spatial field .",
    "* acknowledgement : *   the author s senior visiting scholarship at tsinghua university was funded by the excellent state key lab fund no . 50823005 , national natural science foundation of china , and the r&d special fund for public welfare industry no . 201001080 , chinese ministry of water resources .",
    "d.  d. bosch , j.  m. sheridan , and l.  k. marshall . precipitation , soil moisture , and climate database , little river experimental watershed , georgia , united states .",
    "_ water resour .",
    "_ , 43:0 w09472 , 2007 .",
    "doi : 10.1029/2006wr005834 .",
    "o.  f. christensen , p.  j. diggle , and p.  j. ribeiro jr .",
    "analysing positive - valued spatial data : the transformed gaussian model . in p.",
    "monestiez , d.  allard , and r.  froidevaux , editors , _ geoenv iii  geostatistics for environmental applications _ , pages 287298 .",
    "kluwer , 2001 .",
    "m.  k. cowles , j.  yan , and b.  smith .",
    "reparameterized and marginalized posterior and predictive sampling for complex bayesian geostatistical models .",
    "_ j. comp . graph .",
    "_ , 180 ( 2):0 262282 , 2009 .",
    "doi : 10.1198/jcgs.2009.08012 ."
  ],
  "abstract_text": [
    "<S> we propose a method for estimating the posterior distribution of a standard geostatistical model . after choosing the model formulation and specifying a prior </S>",
    "<S> , we use normal mixture densities to approximate the posterior distribution . </S>",
    "<S> the approximation is improved iteratively . some difficulties in estimating the normal mixture densities , including determining tuning parameters concerning bandwidth and localization , </S>",
    "<S> are addressed . </S>",
    "<S> the method is applicable to other model formulations as long as all the parameters , or transforms thereof , are defined on the whole real line , @xmath0 . </S>",
    "<S> ad hoc treatments in the posterior inference such as imposing bounds on an unbounded parameter or discretizing a continuous parameter are avoided . </S>",
    "<S> the method is illustrated by two examples , one using digital elevation data and the other using historical soil moisture data . </S>",
    "<S> the examples in particular examine convergence of the approximate posterior distributions in the iterations .    </S>",
    "<S> * key words : * geostatistics ; importance sampling ; normal mixture ; kernel density estimation ; anisotropy ; change of support </S>"
  ]
}