{
  "article_text": [
    "in the last few decades it has become apparent that many problems in information theory , and the channel coding problem in particular , can be mapped onto ( and interpreted as ) analogous problems in the area of statistical physics of disordered systems ( such as spin glass models ) .",
    "such analogies are useful because physical insights , as well as statistical mechanical tools and analysis techniques ( like the replica method ) , can be harnessed in order to advance the knowledge and the understanding with regard to the information ",
    "theoretic problem under discussion .",
    "a very small , and by no means exhaustive , sample of works along this line includes references [ 1][29 ] .    in this paper , we shall also adopt the statistical mechanical viewpoint on channel coding .",
    "we focus on the classical random code ensemble ( rce ) for communicating over a discrete memoryless channel ( dmc ) , in the same setting as described in ( * ? ? ?",
    "6 ) and @xcite , which in a nutshell , is as follows : consider a dmc , @xmath0 , fed by an input @xmath1vector that belongs to a codebook @xmath2 , @xmath3 , with uniform priors , where @xmath4 is the coding rate in nats per channel use .",
    "the induced posterior , for @xmath5 , is then : @xmath6 } } { \\sum_{{\\mbox{\\boldmath $ x$}}'\\in{{\\cal c}}}e^{-\\ln[1/p({\\mbox{\\boldmath $ y$}}|{\\mbox{\\boldmath $ x$}}')]}}.\\end{aligned}\\ ] ] here , the second line is written in a form that resembles the boltzmann distribution of statistical physics , according to which the probability of a certain ` state ' ( or ` configuration ' ) of the system , designated by @xmath7 , is given by @xmath8 where @xmath9 is the inverse temperature , @xmath10 is boltzmann s constant , , that is , in units of energy , and then @xmath11 . ]",
    "@xmath12 is temperature , @xmath13 is the energy associated with @xmath7 , and @xmath14 is the _ partition function_. in our case , of course , @xmath15 and the energy function ( which depends on the given @xmath16 ) is @xmath17 $ ] .",
    "but this analogy with the boltzmann distribution ( [ boltzmann ] ) naturally suggests ( cf .",
    "e.g. , @xcite ) to consider , more generally , the posterior distribution parametrized by @xmath18 , that is @xmath19}}{\\sum_{{\\mbox{\\boldmath $ x$}}'\\in{{\\cal c } } } e^{-\\beta\\ln[1/p({\\mbox{\\boldmath $ y$}}|{\\mbox{\\boldmath $ x$}}')]}}\\nonumber\\\\ & { \\stackrel{\\delta } { = } } & \\frac{e^{-\\beta\\ln[1/p({\\mbox{\\boldmath $ y$}}|{\\mbox{\\boldmath $ x$}})]}}{z(\\beta|{\\mbox{\\boldmath $ y$}})}.\\end{aligned}\\ ] ] there are a few motivations for introducing the temperature parameter in ( [ pbeta ] ) .",
    "first , it allows a degree of freedom in case there is some uncertainty regarding the channel noise level ( small @xmath18 corresponds to high noise level ) .",
    "second , it is inspired by the ideas behind simulated annealing techniques : by sampling from @xmath20 while gradually increasing @xmath18 ( cooling the system ) , the minima of the energy function ( ground states ) can be found .",
    "third , by applying symbolwise map decoding , i.e. , decoding the @xmath21th symbol of @xmath7 as @xmath22 , where @xmath23 we obtain a family of _ finite  temperature decoders _ ( originally proposed by rujn @xcite ; see also @xcite , ( * ? ? ?",
    "* section 6.3.3),@xcite,@xcite ) parametrized by @xmath18 , where @xmath15 corresponds to minimum symbol error probability ( with respect to the true channel ) and @xmath24 corresponds to minimum block error probability . finally , and this is the motivation that drives the research reported in this paper : the corresponding partition function , @xmath25 , namely , the sum of ( conditional ) probabilities raised to some power @xmath18 , is an expression frequently encountered in rnyi information measures as well as in the analysis of random coding exponents using gallager s techniques .",
    "since the partition function plays a key role in statistical mechanics , as many physical quantities can be derived from it , then it is natural to ask if it can also be used to gain some insights regarding the behavior of random codes at various temperatures and coding rates .",
    "the main contribution of this paper is in exploring this direction .    to sharpen the last point a little further",
    ", it is noted that when one considers the random coding regime , as we do in this paper , then even if @xmath16 is given , the energy levels pertaining to the boltzmann distribution ( [ pbeta ] ) are themselves random variables since they depend on the randomly chosen codevectors .",
    "as explained in @xcite , this then falls under the umbrella of the so called _ random energy model _ ( rem ) in statistical physics , which was invented by derrida @xcite with the motivation to capture disorder in spin glass systems .",
    "the interesting fact about the rem is that it is typically subjected to _ phase transitions _ , and then so is the model ( [ pbeta ] ) for random codes .    more specifically , as described in ( * ? ?",
    "6 ) , @xcite , and as will be briefly reviewed in the next section , the partition function pertaining to finite  temperature decoding of a ( typical ) randomly chosen code is known to have three types of behavior , corresponding to three phases in the plane of rate vs.  temperature : the _ ferromagnetic phase _ , corresponding to correct decoding , the _",
    "paramagnetic phase _ , of complete disorder , which is dominated by exponentially many incorrect codewords , and the _ glassy phase _ ( or the condensed phase ) , where the system is frozen at minimum energy and dominated by subexponentially many incorrect codewords .",
    "we show that the statistical physics associated with the two latter phases are intimately related to random coding exponents . in particular , the exponent associated with the probability of correct decoding at rates above capacity",
    "is directly related to the free energy in the glassy phase , and the exponent associated with probability of error ( the error exponent ) at rates below capacity , is strongly related to the free energy in the paramagnetic phase .",
    "in fact , we derive alternative expressions of these exponents in terms of the corresponding free energies , and make an attempt to obtain some insights from these expressions .",
    "an additional interesting byproduct of the statistical mechanical point of view that we adopt in this work , is that it suggests a more refined analysis technique , as an alternative to the customary use of jensen s inequality , for which it is clear that the resulting expressions are exponentially tight , and not just bounds .",
    "another way to look at this is to observe that the analysis technique , inspired by statistical mechanical point of view , provides us with insights with regard to the conditions under which jensen s inequality provides a tight bound in this context .",
    "we believe that this technique may be useful in other applications as well .",
    "we shall elaborate more on this in the sequel .    as a side result",
    ", we also compare the phase diagram associated with a certain universal decoder ( namely , the minimum conditional entropy universal decoder ) for discrete memoryless channels , to that of the finite  temperature decoder that is aware of the channel statistics , and show that in spite of the fact that this universal decoder is asymptotically optimum , in the sense of attaining optimum random coding error exponents @xcite , its phase diagram is substantially different .",
    "the outline of the remaining part of this paper is as follows . in section 3 ,",
    "we provide some background , which mostly follows the presentation in @xcite ( with a few missing details filled in ) , but will be useful here to keep this paper self contained .",
    "section 3 also includes a subsection with the phase diagram for universal decoding , as described in the previous paragraph . in section 4",
    ", we derive the alternative formula for the exponent of correct decoding above capacity , and in section 5 , we do the same regarding the random coding exponent at rates below capacity .",
    "throughout this paper , scalar random variables ( rv s ) will be denoted by capital letters , like @xmath26 and @xmath27 , their sample values will be denoted by the respective lower case letters , and their alphabets will be denoted by the respective calligraphic letters .",
    "a similar convention will apply to random vectors and their sample values , which will be denoted with the same symbols in the boldface font .",
    "thus , for example , @xmath28 will denote a random @xmath1-vector @xmath29 , and @xmath30 is a specific vector value in @xmath31 , the @xmath1-th cartesian power of @xmath32 .",
    "sources and channels will be denoted generically by the letters @xmath33 and @xmath34 .",
    "specific letter probabilities corresponding to a source @xmath34 will be denoted by the corresponding lower case letters , e.g. , @xmath35 is the probability of a letter @xmath36 .",
    "a similar convention will be applied to the channel @xmath33 and the corresponding transition probabilities , @xmath37 , @xmath36 , @xmath38 .",
    "the expectation operator will be denoted by @xmath39 .",
    "the empirical distribution pertaining to a vector @xmath40 will be denoted by @xmath41 . in other words , @xmath42 , where @xmath43 , @xmath44 being the number of occurrences of the letter @xmath45 in @xmath7 .",
    "similar conventions will apply to empirical joint distributions of pairs of letters , @xmath46 , extracted from the corresponding pairs of vectors @xmath47 , that is , the joint empirical distribution @xmath48 is the vector of relative frequencies of joint occurrences of @xmath49 and @xmath50 , @xmath51 .",
    "similarly , @xmath52 will denote the empirical conditional probability of @xmath53 given @xmath54 ( with convention that @xmath55 ) , and @xmath56 will denote @xmath57 . the expectation w.r.t .",
    "the empirical distribution of @xmath47 will be denoted by @xmath58 , i.e. , for a given function @xmath59 , we define @xmath60 as @xmath61 , where in this notation , @xmath26 and @xmath27 are understood to be random variables jointly distributed according to @xmath48 .",
    "the cardinality of a finite set @xmath62 will be denoted by @xmath63 . for two positive sequences @xmath64 and @xmath65",
    ", the notation @xmath66 means that @xmath67 and @xmath68 are asymptotically of the same exponential order , that is , @xmath69 .",
    "information theoretic quantities like entropies and mutual informations will be denoted following the usual conventions of the information theory literature .",
    "when we wish to make it clear that such an information theoretic quantity is induced by a certain probability distribution , say @xmath34 , we use this probability distribution as a subscript , e.g. , @xmath70 , @xmath71 , etc .",
    "when the underlying probability distribution is an empirical distribution , we will subscript it by the sequences(s ) from which the empirical distribution is extracted , and we will use hats , e.g. , @xmath72 , @xmath73 .      consider a dmc with a finite input alphabet @xmath32 and a finite output alphabet @xmath74 , which when fed by an input vector @xmath40 , it generates an output vector @xmath75 distributed according to @xmath76 where @xmath77 are given single ",
    "letter transition probabilities .",
    "let @xmath78 be a codebook of @xmath3 codewords , where @xmath4 is the coding rate ( in nats per channel use ) .",
    "next consider the posterior distribution ( [ pbeta ] ) and the corresponding partition function @xmath79 where @xmath80 .",
    "we shall think of @xmath25 as the sum of two contributions , the first is @xmath81 , pertaining to the correct codeword @xmath82 ( that was actually transmitted across the channel ) , and the second is associated with the remaining ( incorrect ) codewords , @xmath83 let us focus on @xmath84 first . as mentioned in the introduction , when the codebook @xmath85 is selected at random , this is a disordered system in the framework of the rem , which exhibits phase transitions .    to describe these phase transitions ,",
    "it is instructive to begin with the relatively simple special case of the binary symmetric channel ( bsc ) , as we do in subsection 2.2.1 , and then extend the scope to general dmc s , as in subsection 2.2.2 .",
    "finally , subsection 2.2.3 ( which is not included in @xcite ) is about a phase diagram pertaining to universal decoding ( cf .",
    "second to the last paragraph of the introduction ) .",
    "this subsection can be skipped without loss of continuity .",
    "when the contribution of @xmath120 is taken into account , and we consider the total partition function @xmath121 , the situation changes : since @xmath122 is typically about the level of @xmath123 , and thus the corresponding free energy density is @xmath124 , we have yet another phase referred to as the _ ordered phase _ or the _",
    "ferromagnetic phase_. this phase exists whenever @xmath121 is dominated by @xmath125 , i.e. , @xmath126 . for @xmath114 ,",
    "this is the case whenever @xmath127 , or equivalently , @xmath128 , where @xmath129 is the capacity of the bsc .",
    "for @xmath130 the boundary between the ferromagnetic phase and the paramagnetic phase is given by the solution @xmath131 to the equation @xmath132.\\ ] ]    to summarize , while there are only two phases ( glassy and paramagnetic ) pertaining to @xmath133 , there is a third , additional phase ( ferromagnetic ) associated with @xmath125 . in the ferromagnetic phase ,",
    "the system is dominated by one state corresponding to the correct codeword .",
    "thus , similarly as in the glassy phase , the entropy of the ferromagnetic phase is zero .",
    "the boundaries between the three phases in the plane defined by @xmath4 and @xmath134 , are as follows ( see fig .",
    "1 ) : the ferro  glassy boundary is the straight line @xmath135 , the glassy  paramagnetic boundary is the curve @xmath136 , and the and the ferro  paramagnetic boundary @xmath137 is given by eq .",
    "( [ ferropara ] ) .",
    "the triple point where all boundaries intersect is the point @xmath138 .",
    "( 0,0 )    # 1#2#3#4#5 @font    ( 5569,4482)(751,-4244 ) ( 2555,-4232)(0,0)[lb ] ( 6320,-4114)(0,0)[lb ] ( 1300 , 43)(0,0)[lb ] ( 751,-2388)(0,0)[lb ] ( 4126,-3286)(0,0)[lb ] ( 2101,-811)(0,0)[lb ]    in spite of the fact that in the glassy phase there are only few configurations that dominate the behavior , it is no different from the paramagnetic phase in terms of the typical ranking of the likelihood of the correct codeword among all codewords : in both phases , the typical location of the correct codeword in the list of descending likelihoods , @xmath139 , is about @xmath140 ( @xmath141 ) .",
    "although the glassy phase exhibits less uncertainty , or equivalently , more certainty , ( sublinear conditional entropy given @xmath16 about the channel input ) , this relative certainty is misleading because the posterior probability mass is captured mostly by incorrect codewords . in this sense ,",
    "the glassy phase is even more problematic than the paramagnetic one : since the certainty is fictitious , it is more difficult to detect errors .",
    "the extension to general dmc s is essentially quite straightforward . consider a dmc parametrized by @xmath142 . for the sake of simplicity , let us consider the uniform random coding distribution according to which each codeword is selected independently at random with probability distribution @xmath143 for all @xmath40 . for a given channel output vector @xmath16 ,",
    "the probability of selecting a random codeword @xmath7 whose conditional empirical distribution with @xmath16 is @xmath56 is of the exponential order of @xmath144}$ ] , @xcite , thus the expected number of codewords with this conditional distribution is exponentially @xmath145}.\\ ] ] in analogy to the explanation provided in the previous subsection ( and in @xcite ) , in the context of the bsc , those conditional distributions @xmath146 for which the exponent on the right  hand side is negative , are typically not populated .",
    "thus , for a typical random code @xmath147\\}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & \\exp\\left\\{n\\left(r-\\ln|{{\\cal x}}|+ \\max_{q_{x|y}:~h_q(x|y)\\ge \\ln|{{\\cal x}}|-r } \\left[h_q(x|y)-\\beta{\\mbox{\\boldmath $ e$}}_q\\{\\ln[1/p(y|x]\\}\\right]\\right)\\right\\}\\nonumber\\\\ & { \\stackrel{\\delta } { = } } & e^{-n\\beta f_e(\\beta;y)},\\end{aligned}\\ ] ] where @xmath27 designates a rv distributed according to the empirical distribution @xmath148 of @xmath16 .",
    "a word on notation is now in order : here and throughtout the sequel , we adopt the common abuse of notation , customarily used in the information theory literature , that when a rv appears as an argument or a subscript of a certain function , this means that it is actually a functional of its distribution , not a function of the value of the random variable itself . whenever we wish to emphasize the dependence of this quantity on the empirical distribution @xmath148 , we will replace @xmath27 by @xmath148 or simply by @xmath16 itself , provided that the context does not leave room for ambiguity .",
    "similar comments will apply to other quantities to be defined throughout this subsection and in the sequel .",
    ", due to the symmetry of the bsc .",
    "] for some of these quantites , we will not denote the dependence on the distribution of @xmath27 explicitly , in order to avoid cumbersome notation , but it will be made clear that they do depend on it in general .",
    "consider now the expression @xmath149,\\ ] ] where @xmath150 .",
    "first , it is easy to prove ( see appendix , subsection a.1 ) that for fixed @xmath18 and @xmath16 , the function @xmath151 is concave in @xmath4 .",
    "this means that the inequality constraint @xmath152 is met with equality as long as @xmath153 , where @xmath154 with @xmath155 being the achiever of @xmath156,\\ ] ] that is , @xmath157 we will also use the notation @xmath158 and @xmath159 thus @xmath160 .",
    "let @xmath161 obviously , @xmath162 increases with @xmath4 , or equivalently , @xmath111 is decreasing with @xmath4 ( @xmath163 ) .",
    "this forms the boundary curve between the glassy and the paramagnetic phases .",
    "note that when @xmath164 , the mutual information induced by the uniform distribution on @xmath32 and by @xmath165 , then @xmath166 .",
    "thus , @xmath167 is a point on the curve @xmath136 .    for @xmath153 , or equivalently , @xmath168 ,",
    "the constraint @xmath169 is attained with equality .",
    "thus , in this range of low rates , @xmath170\\nonumber\\\\ & = & \\ln|{{\\cal x}}|-r-\\beta\\cdot   \\min_{\\{q_{x|y } : h_q(x|y)=\\ln|{{\\cal x}}|-r\\}}{\\mbox{\\boldmath $ e$}}_q\\{d(x , y)\\}\\nonumber\\\\ & = & \\ln|{{\\cal x}}|-r-\\beta d_y(\\beta_r)\\end{aligned}\\ ] ] where @xmath171 is the solution to the equation @xmath172 .",
    "we will also use the notation @xmath173 .",
    "is the generalization of the gv distance that was defined in subsection 2.2.1 .  for the bsc .",
    "] it follows then that @xmath174 , which is the glassy phase .    for @xmath175 , @xmath176=h_y(\\beta)-\\beta d_y(\\beta)\\ ] ]",
    "thus , for @xmath177 , @xmath178 which is the paramagnetic phase . it should be pointed out that for a general decoding metric @xmath179 ( not necessarily ml matched to the channel ) , the boundary between the paramagnetic and the glassy phases depends only on the random coding distribution and this decoding metric @xmath179 , not on the channel itself ( cf .",
    "subsection 2.2.3 ) .",
    "the boundaries with the ferromagnetic phase are the ones that depend on the channel .    in the ordered ( ferromagnetic ) phase ,",
    "the free energy density is given by @xmath180 , where @xmath26 is uniform and @xmath27 given @xmath26 is distributed according to the channel . as long as @xmath181",
    ", we have @xmath182 .",
    "in fact , the line connecting the points @xmath183 and @xmath184 forms the boundary between the ordered ferromagnetic phase and the glassy phase .    for @xmath181 , the boundary between the ferromagnetic and paramagnetic phases is given by the solution @xmath185 ( or @xmath186 ) to the equation @xmath187 which is above the curve @xmath136 for @xmath181 .",
    "it should be emphasized that @xmath162 , @xmath185 , and @xmath171 all depend on the ( distribution of the ) rv @xmath27 , namely , the empirical distribution of @xmath16 .",
    "it is instructive to compare the phase diagram of finite  temperature map decoding to those of finite  temperature universal decoders .",
    "one simple example of a universal decoder for which it is especially easy to derive the phase diagram is the minimum conditional entropy decoder @xcite , which given @xmath16 , selects the codeword @xmath188 for which @xmath189 is minimum .",
    "it is well known that this universal decoder is asymptotically optimum in the random coding sense , in that it achieves the same random coding error exponent as the ml decoder , provided that the random coding distribution is uniform over @xmath31 .    the partition function corresponding to this universal decoder is the same as before , except that @xmath190 is replaced by @xmath73 . in this case , @xmath191\\right)\\right\\}\\nonumber\\\\ & { \\stackrel{\\delta } { = } } & e^{-n\\beta f_e(\\beta , y)}\\end{aligned}\\ ] ] now , it is easy to see how phase transitions behave ( see fig .",
    "2 ) : if @xmath192 , then the maximum is @xmath193 and we get @xmath194},\\ ] ] thus @xmath195 . if @xmath196 , we get @xmath197},\\ ] ] thus , @xmath198 . therefore , the boundary between the two phases is the horizontal line @xmath199 ( independently of @xmath4 )",
    "this means that the glassy region here is larger than in ml decoding for @xmath141 .",
    "the boundary between the ferromagnetic and the glassy phases continues to be @xmath164 as before .",
    "the ferromagnetic  paramagnetic boundary is now @xmath200 , or , equivalently , @xmath201 , which is below the ferromagnetic  paramagnetic boundary of the map decoder .",
    "this can easily be shown by setting @xmath202 ( which is this boundary ) in the r.h.s .",
    "of the equation defining @xmath203 and showing that the resulting expression is larger than @xmath204 ( for @xmath205 ) , which is the l.h.s .  of this equation",
    "( thus , we are still in the ferromagnetic phase of map decoding ) : specifically , the l.h.s .  of the equation defining @xmath203",
    "is : @xmath206 where the first equality is since @xmath202 on the boundary , and the last equality is since @xmath207 .",
    "thus , although this decoder achieves the optimum random coding error exponent , it has a phase diagram which is worse than that of map decoding , as the ferromagnetic region is smaller and the glassy region is larger .    (",
    "0,0 )    # 1#2#3#4#5 @font    ( 5542,4482)(778,-4244 ) ( 2555,-4232)(0,0)[lb ] ( 6320,-4114)(0,0)[lb ] ( 1300 , 43)(0,0)[lb ] ( 4351,-2236)(0,0)[lb ] ( 1876,-1261)(0,0)[lb ]",
    "we now proceed to establish relationships between the phase diagram of a random code , decoded by a finite temperature map decoder , and the exponent of correct decoding at rates above capacity , or to be more precise , rates above @xmath208 , the mutual information induced by the uniform input distribution and the channel .",
    "the above upper bound to @xmath215 can be also written as : @xmath216^{1/\\beta}\\right\\},\\ ] ] where here @xmath92 denotes the number of codewords @xmath217 for which @xmath218 , and @xmath219 is the set of values that the function @xmath220 can take on for a given @xmath16 , as @xmath7 exhausts the codebook @xmath85 .",
    "note that as @xmath221 depends only on the empirical joint distribution of @xmath7 and @xmath16 , then @xmath222 can not exceed the number of empirical conditional distributions ( or conditional type classes ) corresponding to pairs of @xmath1sequences , and so , @xmath222 is upper bounded by a polynomial in @xmath1 .",
    "now , when a random code is considered , then instead of applying jensen s inequality for @xmath223 ( as was done in @xcite ) , and thereby insert the expectation operator into the square brackets , let us adopt another approach .",
    "consider the following events : @xmath224_++\\epsilon]\\}~ \\mbox{for some $ d\\in{{\\cal d}}_n$}\\right\\},\\ ] ] where @xmath225_+{\\stackrel{\\delta } { = } } = \\max\\{0,t\\}$ ] and where @xmath226 is defined as the maximum of @xmath71 subject to the constraints that @xmath227 and that @xmath27 is distributed according to @xmath148 . also , define @xmath228 where we recall that @xmath229 is the solution to the equation @xmath230 .",
    "note that @xmath231 are disjoint events .",
    "now , for @xmath232 : @xmath233^{1/\\beta}\\right\\}\\nonumber\\\\ & \\le&\\mbox{pr}\\{{{\\cal b}}\\}\\cdot[e^{nr}\\cdot e^{-\\beta \\cdot 0}]^{1/\\beta}+\\nonumber\\\\ & & + \\sum_{d\\le d_0({\\mbox{\\boldmath $ y$}})}\\mbox{pr}\\{{{\\cal w}}_d\\cap{{\\cal b}}^c\\}\\cdot \\left[e^{n\\epsilon}e^{-\\beta d}\\right]^{1/\\beta}+\\nonumber\\\\ & & + \\mbox{pr}\\{{{\\cal w}}_0^c\\cap{{\\cal w}}_1^c\\cap\\ldots \\cap{{\\cal w}}_{d_0({\\mbox{\\boldmath $ y$}})}^c\\cap{{\\cal b}}^c\\}\\cdot e^{-nf_g(y)}\\cdot e^{n\\epsilon/\\beta},\\end{aligned}\\ ] ] this inequality calls for some explanation : we are dividing the set of configurations of the rv s @xmath234 into three classes , defined by the events @xmath235 and @xmath231 . in the first class , corresponding to the first term on the right  hand side , @xmath94 fall in @xmath235 , where there is at least one value of @xmath93 for which @xmath92 is _ exponentially _ larger ( by at least @xmath236 ) than its expectation .",
    "we bound the value of @xmath237^{1/\\beta}$ ] , in this class , very `` generously '' , by the maximum possible value it can possibly take , that is , when all @xmath95 codewords are at zero distance from @xmath16 , but this quantity is weighted by @xmath238 , which as is shown in the appendix ( subsection a.2 ) , decays double  exponentially rapidly , at least as fast as @xmath239 , and so this first term is negligible .",
    "the other two classes correspond to @xmath240 , where for all @xmath241 , @xmath92 does not exceed its expectation times @xmath242 . here",
    "we distinguish between two cases ( corresponding to the two other classes ) : in one of them , ( at least ) one of the distances below the generalized gv distance @xmath243 is populated by subexponentially guarantees that there are only subexponentially many codewords at distances below @xmath244 .",
    "] many codewords . since we are operating in the glassy regime , the dominant contribution to @xmath237^{1/\\beta}$ ]",
    "will be due to these minimum distance codewords , and the weighting of the event of minimum distance @xmath93 is , of course , according to @xmath245 . in the other case , which captures most of the probability mass ( since it is the typical configuration of @xmath94 ) , none of the distances below the generalized gv distance is populated by codewords , whereas for larger distances , @xmath94 are all ( within a factor of @xmath242 ) about their expectations . in this case",
    ", our expression again behaves according to the glassy regime , where the generalized gv distance dominates the partition function .",
    "now , regarding the second term , for @xmath246 , @xmath247 where the latter expression is shown ( appendix , subsection a.2 ) to decay at the exponential rate of @xmath248}$ ] .",
    "thus , @xmath233^{1/\\beta}\\right\\}\\nonumber\\\\ & \\le & e^{-e^{n\\epsilon}}\\cdot[e^{nr}]^{1/\\beta}+\\nonumber\\\\ & & + \\sum_{\\delta\\le \\delta_y(r ) } e^{-n[\\ln|{{\\cal x}}|-r - h_0(\\delta|{\\mbox{\\boldmath $ y$}})]}\\cdot \\left[e^{n\\epsilon}e^{-\\beta n\\delta}\\right]^{1/\\beta}+ e^{-nf_g(y)}\\cdot e^{n\\epsilon/\\beta}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & e^{n(r-\\ln|{{\\cal x}}|)}\\cdot e^{n\\epsilon/\\beta } \\exp\\{n\\max_{\\delta\\le\\delta_y(r ) } [ h_0(\\delta|{\\mbox{\\boldmath $ y$}})-\\delta]\\ } + e^{-nf_g(y)}\\cdot e^{n\\epsilon/\\beta}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & e^{n(r-\\ln|{{\\cal x}}|)}\\cdot e^{n\\epsilon/\\beta } \\exp\\{n[h_0(\\delta_y(r)|{\\mbox{\\boldmath $ y$}})-\\delta_y(r)]\\ } + e^{-nf_g(y)}\\cdot e^{n\\epsilon/\\beta}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & e^{n(r-\\ln|{{\\cal x}}|)}\\exp\\{n[\\ln|{{\\cal x}}|-r-\\delta_y(r)]\\ } \\cdot e^{n\\epsilon/\\beta } + e^{-nf_g(y)}\\cdot e^{n\\epsilon/\\beta}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & e^{-nf_g(y)}\\cdot e^{n\\epsilon/\\beta}.\\end{aligned}\\ ] ] since @xmath236 can be chosen arbitrarily small for large @xmath1 ( in fact , one may let @xmath236 vanish with @xmath1 sufficiently slowly ) , the exponential rate of the expression under discussion is actually bounded by @xmath249 .",
    "note that whenever @xmath250 , this expression no longer depends on @xmath18 .",
    "finally , substituting this bound back into the bound on @xmath215 , we get : @xmath251}\\nonumber\\\\ & = & e^{-n(r-\\max_y[h(y)-f_g(y)])}.\\end{aligned}\\ ] ] this calculation can be shown to be exponentially tight : a lower bound can be obtained by confining the calculation to the ( high probability ) event @xmath252 with the additional restriction that @xmath253 for all @xmath254 ( i.e. , the last term only in the above derivation ) .",
    "note that in arimoto s paper , where jensen s inequality is used , the expectation of @xmath255 is computed , and this actually corresponds to the paramagnetic regime ( without the constraint @xmath152 ) .",
    "the resulting bound might not be exponentially tight in general .",
    "finally , the optimization @xmath256 $ ] can be carried out explicitly , yielding @xmath257 , where where @xmath258 .",
    "we have obtained then a random coding exponent formula in terms of the free energy density in the glassy phase , from which we learn that the free energy density of the glassy phase plays a central role in the calculation the exponent of correct decoding . to obtain some insight , it is instructive to examine this expression in the special case of the bsc . here , since @xmath259 does not depend on the probability distribution of @xmath27 , we get : @xmath260}\\nonumber\\\\ & = & e^{n[h(\\delta_{gv}(r))-\\delta_{gv}(r)\\ln\\frac{1}{p}- ( 1-\\delta_{gv}(r))\\ln\\frac{1}{1-p}]}\\nonumber\\\\ & = & e^{-nd(\\delta_{gv}(r)\\|p)},\\end{aligned}\\ ] ] where for @xmath261 , @xmath262 .",
    "this result has the intuitively appealing interpretation of the probability of the large deviations event that the channel makes @xmath116 errors or less , although @xmath263 ) , in which case the correct codeword ` penetrates ' into the sphere of radius @xmath116 , whose surface is populated by the codewords that dominate the glassy phase .",
    "of course , when such an event happens , the correct codeword dominates the partition function , and thus the decoding is correct .",
    "in the special case of the bsc , where @xmath277 and does not depend on @xmath27 and @xmath278 does not depend on @xmath279 , we get the exponential rate of @xmath280\\nonumber\\\\ & = & \\beta\\rho f_p(\\beta)-(\\ln[p^\\beta+(1-p)^\\beta]-\\ln 2)-\\ln 2\\nonumber\\\\ & = & \\rho(\\ln 2-r)-(1+\\rho)\\ln[p^\\beta+(1-p)^\\beta]\\nonumber\\\\ & = & \\rho \\ln 2-(1+\\rho)\\ln[p^{1/(1+\\rho ) } + ( 1-p)^{1/(1+\\rho)}]-\\rho r\\nonumber\\\\ & = & e_0(\\rho)-\\rho r\\nonumber\\\\ & { \\stackrel{\\delta } { = } } & e_0(\\rho , r)\\end{aligned}\\ ] ] which is , as expected , gallager s reliability function for the bsc .",
    "the optimum choice of @xmath281 depends on @xmath4 . as is shown in @xcite , in the range @xmath282 , that is , @xmath283 , we have @xmath284 , which means @xmath285 . for @xmath286 $ ] ,",
    "the optimum @xmath281 is in @xmath287 , and it satisfies @xmath288 , or , equivalently , @xmath289 , which means that we move along the boundary between the the glassy phase and the paramagnetic phases of @xmath84 .",
    "let @xmath291 and @xmath292 achieve @xmath293 and @xmath294 , respectively .",
    "now , let @xmath295 for some @xmath296 .",
    "first , observe that by the concavity of the conditional entropy in @xmath297 for fixed @xmath298 , we have @xmath299 it follows then that @xmath300 .",
    "but , on the other hand @xmath301+(1-\\alpha ) [ h_{q_2}(x|y)-\\beta{\\mbox{\\boldmath $ e$}}_{q_2}d(x , y)]\\nonumber\\\\ & = & \\alpha j_y(\\beta , r_1)+(1-\\alpha)j_y(\\beta , r_2).\\end{aligned}\\ ] ] thus , @xmath302      for @xmath303 $ ] , consider the binary divergence @xmath304\\end{aligned}\\ ] ] to derive a lower bound to @xmath305 , let us use the inequality @xmath306 and then @xmath307 now , let @xmath92 denote the number of codewords for which @xmath218 . as mentioned earlier , @xmath92 is the sum of the @xmath95 independent binary random variables @xmath308 , where the probability that @xmath309 is exponentially @xmath310}$ ] , @xmath226 being the maximum of @xmath71 subject to the constraints that @xmath227 , @xmath311 , and that @xmath27 is distributed according to @xmath148 .",
    "the event @xmath312 , for @xmath313 and @xmath314 , means that the relative frequency of the event @xmath308 is at least @xmath315 .",
    "thus , by the chernoff bound : @xmath316})\\right\\}\\nonumber\\\\ & \\le & \\exp\\left\\{-e^{nr}\\cdot e^{-n(r - a)}(n[(\\ln|{{\\cal x}}|-r- h_0(\\delta|{\\mbox{\\boldmath $ y$}})+a]-1)\\right\\}\\nonumber\\\\ & \\le & \\exp\\left\\{-e^{na}(n[\\ln|{{\\cal x}}|-r - h_0(\\delta|{\\mbox{\\boldmath $ y$}})+a]-1)\\right\\}.\\end{aligned}\\ ] ] now , for @xmath317_++\\epsilon$ ] , the term in the square brackets is at least @xmath318 , and thus @xmath319 decays double  exponentially rapidly , not slower than @xmath239 .",
    "the probability of the union of the ( polynomially many ) events @xmath320 , which is upper bounded by the sum of the probabilities , is still double  exponentially small .",
    "thus , @xmath238 decays double  exponentially rapidly .",
    "now , the event @xmath321 corresponds to the choice @xmath322 . for @xmath323 ,",
    "@xmath229 being the solution to the equation @xmath324 , which means that @xmath325 , this gives an ordinary exponential decay at the rate of @xmath248}$ ] .",
    "n.  sarshar and x.  wu , `` statistical mechanics of optimal networked source coding , '' preprint .",
    "t.  mutayama , `` statistical mechanics of the data compression theorem , '' _ j.  phys .  a : math .",
    "_ , vol .",
    "35 , pp .",
    "l95l100 , 2002 .",
    "a.  montanari , `` the glassy phase of gallager codes , '' arxiv : cond - mat/0104079v1 , april 4 , 2001 .",
    "p.  rujn , `` finite temperature error  correcting codes , '' _ phys .  rev .",
    "_ , vol .",
    "70 , no .",
    "19 , pp .",
    "29682971 , may 1993 ."
  ],
  "abstract_text": [
    "<S> the partition function pertaining to finite  temperature decoding of a ( typical ) randomly chosen code is known to have three types of behavior , corresponding to three phases in the plane of rate vs.  temperature : the _ ferromagnetic phase _ , corresponding to correct decoding , the _ paramagnetic phase _ , of complete disorder , which is dominated by exponentially many incorrect codewords , and the _ glassy phase _ ( or the condensed phase ) , where the system is frozen at minimum energy and dominated by subexponentially many incorrect codewords . we show that the statistical physics associated with the two latter phases are intimately related to random coding exponents . in particular , the exponent associated with the probability of correct decoding at rates above capacity is directly related to the free energy in the glassy phase , and the exponent associated with probability of error ( the error exponent ) at rates below capacity , is strongly related to the free energy in the paramagnetic phase . in fact , we derive alternative expressions of these exponents in terms of the corresponding free energies , and make an attempt to obtain some insights from these expressions . finally , as a side result </S>",
    "<S> , we also compare the phase diagram associated with a simple finite  temperature universal decoder , for discrete memoryless channels , to that of the finite  temperature decoder that is aware of the channel statistics .    </S>",
    "<S> * index terms : * random coding , free energy , partition function , random energy model ( rem ) , phase transitions , error exponents .    </S>",
    "<S> department of electrical engineering + technion - israel institute of technology + haifa 32000 , israel + </S>"
  ]
}