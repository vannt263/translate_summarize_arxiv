{
  "article_text": [
    "in the following article , we will consider a bayesian mixture of lasso regressions with @xmath0errors that is motivated by a particular problem in finance .",
    "the specifics of the data are explained in section [ sec : data ] , but the model and resulting mcmc algorithm are generic and hence we consider a general presentation during the article .",
    "the data we are presented with is a collection of @xmath3 paired observations @xmath4 where @xmath5 is the response variable and @xmath6 is the corresponding vector of explanatory variables .",
    "the specific objective is to cluster linear regression curves which satisfy the following constraints :    * the regression curves are resistant to outliers * each regression curve is specific to each cluster , in that the predictors for one curve may not be present in another * one would like to have relatively few predictors in each curve    it is remarked that whilst we have been motivated by a problem in finance , this particular scenario is present in other real problems , such as gene expression data ; see for example cozzini et al .",
    "( 2011 ) .    in the context of the problem",
    ", we will then consider a mixture of regressions , from the bayesian perspective .",
    "mixture of regressions has been well studied ; see for example goldfield & quandt ( 1973 ) and hurn et al .",
    "in addition , the issue of variable selection has also been substantially investigated , both in the supervised and unsupervised mixture modelling setting , by raftery & dean ( 2006 ) and yau & holmes ( 2011 ) , for example . however , to our knowledge , there are very few articles which develop a ( bayesian ) model with component specific variable selection , which we incorporate into our model .",
    "an exception in the case of frequentist statistics is khalili & chen ( 2007 ) .",
    "we remark that we do not address the issue of selecting the number of components in the mixture , but this is discussed in section [ sec : summary ] . to deal with the issues of robustness to outliers and sparseness of solutions , we consider well - known procedures , by incorporating heavy - tailed regression error as well as a bayesian lasso type structure ( e.g.  park & casella ( 2008 ) ) .",
    "the latter idea has also been followed by yau & holmes ( 2011 ) .",
    "these model components lead to a bayesian statistical model which is very high - dimensional . in order to draw statistical inference , after marginalization , we are left with a posterior distribution on the class labels of the mixture , component specific variable selection indicators and some additional parameters . due to the complexity of the resulting posterior ,",
    "very sophisticated computational tools are required .",
    "we focus on using pmcmc ( andrieu et al .",
    "2010 ) , which is particularly useful for statistical models with latent variables .",
    "the pmcmc algorithm uses an smc algorithm ( e.g.  doucet et al .",
    "( 2001 ) ) to update latent variables : we focus on the class labels which have a larger state - space than the variable selection in the examples considered .",
    "we develop an smc algorithm and subsequently a conditional smc algorithm for our particle gibbs algorithm ( a special case of pmcmc ) .",
    "the pmcmc algorithm reflects the current state - of - the - art in bayesian computation and gives us the best chance of reliable inference from the posterior ; although we remark that it is far from infallible and can break down for sufficiently complex problems .",
    "the outline of the paper is as follows . in section [ sec : regmdl ]",
    "we describe the hierarchical representation of the model and justify the choice of priors that lead to the posteriors of interest . in section [ sec : regmeth ] we present our pmcmc algorithm . in section",
    "[ sec : exres ] we investigate the model and algorithm on simulated data . in section [ sec : data ] we describe the applied problem and the data which we analyze . in section [ sec : summary ] the article is concluded and some avenues of future work are discussed .",
    "generalising the peculiarities of the financial data we want to investigate , let us first highlight the relevant aspects of the problem that motivate the mixture of regression model we propose .",
    "recall we have a collection of @xmath3 paired observations @xmath4 where @xmath5 is the response variable and @xmath6 is the corresponding vector of explanatory variables .",
    "to simplify notation we use @xmath7 to indicate the collection of covariates at the @xmath8 sample and set the first element @xmath9 to be @xmath10 to allow a more convenient formulation of the model .",
    "the defining characteristic of the data is that the @xmath11 samples are generated from a heterogeneous population and only few of the @xmath12 covariates convey any useful information to explain the variability of the @xmath13 .    to answer these demanding conditions",
    ", we propose a bayesian mixture model which postulates that there are @xmath14 possible linear regression curves ( one can consider more general basis function , but this is not done here ) to describe the data and that each curve potentially depends upon a different collection of the variables @xmath15 . to facilitate the derivation of a sparse solution",
    ", we introduce a @xmath12-dimensional binary vector @xmath16 , where we use @xmath17 to denote @xmath18 , which encodes whether each of @xmath12 observed covariates should be included or not in the @xmath19 regression curve for @xmath20 .",
    "similarly , we use @xmath21 as a subscript indicator which deletes the elements corresponding to @xmath22 for @xmath23 and returns a vector of length @xmath24 ( @xmath25norm ) .",
    "the mixture model is then defined as the conditional distribution of @xmath13 given @xmath26 @xmath27 where @xmath28 is the @xmath29dimensional normal distribution of mean @xmath30 and covariance @xmath31 .",
    "note that , to simplify notation , when @xmath32 we drop the subscript .",
    "is a mixture of normal distributions with parameters    * @xmath33 with @xmath34 for @xmath35 such that @xmath36 , are the mixing proportion of the @xmath37 components .",
    "* @xmath38 with @xmath39 for @xmath40 , is the collection of regression coefficients .",
    "* @xmath41 , with @xmath42 for @xmath43 is a variable introduced to allow a student @xmath0regression error .",
    "having defined the model , the values of the parameters @xmath44 are unknown and will have to be inferred from the data @xmath45 using a bayesian approach .",
    "note that throughout our discussion we assume that the number of clusters @xmath37 is known . in a different situation",
    ", we could have included @xmath37 in the set of unknown parameters and modified the estimation process accordingly .",
    "while this would be a standard procedure , it adds another level of complexity to the model that we rather avoid here since it is not the focus of our investigation ; see section [ sec : summary ] for some discussion .      whilst a mixture of gaussian distributions as described in is a fairly general model",
    ", it is also flexible enough to allow us to choose convenient priors that achieve the objective of making the model robust to outliers and selecting only the relevant covariates .",
    "this task is facilitated by using a hierarchical representation of the mixture model and having different levels of priors and hyperpriors .",
    "following the standard missing data approach , see diebolt & robert ( 1994 ) , we introduce , for every @xmath46data point , the latent allocation variable @xmath47 which indicates the membership of @xmath13 to the @xmath48cluster .",
    "thus , we can simplify the mixture structure and note that the conditional distribution of @xmath13 given @xmath49 , with probability @xmath50 , is the gaussian distribution @xmath51 assuming the mixture weights follow a dirichlet distribution , the prior on @xmath52 is @xmath53 where @xmath54 is the symmetric dirichlet distribution .      following the hierarchical representation , given @xmath49 , the distribution of the variance parameter @xmath56 in ,",
    "is set to be @xmath57 where @xmath58 is gamma distribution of mean @xmath59 .",
    "the hyperparameter @xmath60 corresponds to the degrees of freedom of the student-@xmath61 distribution .",
    "a very important feature of the model we propose is that it combines , in a mixture framework , shrinkage and variable selection .",
    "it achieves this result by adopting specific priors for the regression coefficients @xmath62 and the binary indicator variables @xmath63 .",
    "tibshirani ( 1996 ) showed that using a ml approach in a single mixture component framework , one can regularise the estimated linear regression coefficients @xmath64 introducing the penalty term : @xmath65 for some @xmath66 and @xmath67 .",
    "the effect of penalising the likelihood function is to shrink the vector of mle of @xmath64 toward zero with the possibility of setting some coefficients exactly equal to zero .",
    "it is well known that similar results to the lasso penalty can be achieved by assuming that @xmath38 have independent laplace , i.e.  double - exponential priors , @xmath68 where @xmath69 determines the scaling of the regression coefficients in the @xmath48curve and @xmath67 is the smoothness parameter that controls the tail decay . since the mass of is quite highly concentrated around zero with a distinct peak at zero , the regression coefficient estimates corresponding to the posterior mean and posterior mode are shrunk towards zero in equivalent fashion to the penalisation least squares estimation procedure .",
    "the double - exponential distribution can be represented as a scale mixture of normals with exponential mixing distribution .",
    "therefore , introducing a latent vector of scale variables we obtain a more tractable hierarchical formulation of the prior on @xmath70 . ignoring for the moment the @xmath71 indicator and assuming a single component mixture , consider the following hierarchical prior on the @xmath72 regression coefficient : @xmath73 where the hyperparameter @xmath74 itself has hyperprior @xmath75 ( @xmath76 is the exponential distribution of mean @xmath77 . ) .",
    "we note that marginally @xmath78 still follows a laplace distribution with parameter @xmath79 , @xmath80    the modular structure of hierarchical modelling allows us to extend , in a straightforward way , the bayesian lasso method to our proposed mixture of linear regression . together with the prior on @xmath81 we also specify priors on the hyperparameters @xmath82 , with @xmath83 , to control the scaling , and @xmath84 , with @xmath85 , to induce shrinkage on the coefficients of the @xmath19 regression curve .",
    "@xmath86 @xmath87 is the inverse - gamma distribution of mean @xmath88 ( @xmath89 ) .",
    "whilst in our discussion we assume @xmath79 is given , park & casella ( 2008 ) have shown , in a non - mixture bayesian framework with known @xmath90 , that the lasso parameter can be chosen by marginal maximum likelihood or using an appropriate hyperprior .",
    "many authors , such as george & mcculloch ( 1997 ) , kim et al .",
    "( 2006 ) and schfer & chopin ( 2012 ) have proposed an effective solution by once again specifying a convenient prior for the selection indicator @xmath91 .",
    "we specify selection priors that fit into the mixture framework of regularised regressions .",
    "a suitable prior for @xmath91 is the bernoulli distribution @xmath92 mutually independent across independent components .",
    "we should also point out the level of the flexibility of the mixture model . by making @xmath17 cluster specific",
    ", each regression curve can be a function of its own different set of covariates .",
    "on the other hand , the combinations of competing models to be evaluated grows exponentially with the number of explanatory variables and linearly with the clusters , @xmath93 . in theory , for the given prior",
    ", we could compute the posterior probability of each model before selecting the best one .",
    "in practice , it is evident that a full exploratory search is unfeasible and we need to incorporate a selection procedure into the sampling algorithm .      the hierarchical representation of our bayesian model can be observed in figure [ fig : dag ] .",
    "we have also discussed in the previous section how the desired properties of the model are achieved by specifying a convenient structure .",
    "we now give some details on the posterior of interest    , width=529,height=377 ]    using a synthetic notation to indicate the unknown parameters of the model @xmath94 , and the fixed , assumed known , hyperparameters of the model @xmath95 , we can say that , after observing the covariates @xmath96 and the responses @xmath97 , the posterior distribution of @xmath98 is @xmath99 where @xmath100 is the likelihood function and @xmath101 the prior distributions we have previously defined .    since our main focus is to draw inference on the cluster membership of the observations and identify the relevant explanatory variables , we remove as many other variables as possible . we integrate out the parameters @xmath102 , @xmath103 and @xmath52 in @xmath104 and",
    "obtain the marginal posterior density of interest up to a normalizing constant @xmath105\\ , \\frac{\\prod_{k=1}^k\\gamma(\\delta+n_k)}{\\gamma(\\sum_{k=1}^k[n_k+\\delta])}\\end{aligned}\\ ] ] where @xmath106 is the gamma function , @xmath107 is the gamma density of mean @xmath59 , @xmath108 the number of observations assigned to the @xmath19 cluster , @xmath109 is the collection of observations assigned to the @xmath19 cluster .",
    "given @xmath110 , we can derive @xmath111 with @xmath112 where @xmath113 .",
    "we should also be aware of label switching problem ( e.g.  jasra et al .",
    "( 2005 ) ) which is a common issue when estimating the parameters of a bayesian mixture model .",
    "this is addressed in section [ sec : exres ] .",
    "we adopt an mcmc strategy ( see robert & casella ( 2004 ) for a review ) to sample from the target distribution .",
    "we should first note that , within the mixture modelling literature , there has been work done on perfect sampling and direct sampling , making use of the full conditional distributions . for example ,",
    "mukhopadhyay & bhattacharya ( 2011 ) proposed a perfect sampling methodology for fitting mixture models .",
    "fearnhead & meligkotsidou ( 2007 ) instead proposed a direct sampling method that returns independent samples from the true posterior .",
    "unfortunately , the described algorithms have limited applicability in our context .    in light of recent work presented by andrieu et al .",
    "( 2010 ) , we adopt a particle markov chain monte carlo ( pmcmc ) simulation procedure which combines mcmc and smc methods and takes advantage of the strengths of both .",
    "the key feature of pmcmc algorithms is that they are in fact exact approximations of idealised mcmc algorithms , while they use sequential monte carlo methods to build high dimensional proposal distributions . on the other hand ,",
    "compared to stand alone smc , pmcmc sampling is more robust to the path degeneracy problem , described later on .",
    "more precisely , here we implement a particle metropolis - within - gibbs algorithm . below we describe the constituents of the algorithm , which is summarized in section [ sec : sampproc ] .",
    "smc methods are a general class of algorithms that use a set of weighted particles to recursively approximate a sequence of distributions of increasing dimension .",
    "it has been originally introduced to deal with situations with dynamic observations .",
    "nonetheless , it has demonstrated to be highly effective also in static problems like mixture models and it is an integral part of pmcmc . before illustrating how smc",
    "algorithms are used in our sampling procedure , we refer the reader to doucet et al .",
    "( 2001 ) for a detailed review of smc methods .",
    "in particular , we assume the reader is familiar with sequential importance sampling ( sis ) .",
    "we use an smc method to sample sequentially from @xmath114 as @xmath115 increases . following algorithm [ al : smc ] , we first initialize @xmath116 by sampling their respective priors , and then alternate sequential importance sampling and resampling steps .",
    "more explicitly , the sequential importance sampling targets the full conditional density of the latent labels variables @xmath117 which , after the first @xmath118 data points , is @xmath119\\bigg/\\gamma(\\sum_{k=1}^k(n_k^{(i)}+\\delta))\\ ] ] where @xmath120 denotes the data allocated to the @xmath19 cluster out of the first @xmath115 observations and @xmath121 their total number .",
    "sis is subject to the problem of weight degeneracy .",
    "as new incoming observations are fed into the algorithm , the variance of the importance weights typically increases at an exponential rate until all the mass concentrates on one single particle , leaving the remaining particles with weights tending to zero .    to avoid spending a large computational effort to update trajectories whose contribution to the final estimate is negligible",
    ", we execute a resampling step with the intention of replacing the unpromising lowest weighted particles with new particles that hopefully lie in regions of high target density .",
    "the exact procedure consists in sampling @xmath122 particles from the approximated target distribution to obtain @xmath122 new particles which will then be equally weighted . on the other hand ,",
    "if one resamples very often , we will rapidly deplete the number of distinct particles and the approximation of the target will suffer because the paths of @xmath117 become very similar ( path degeneracy ) .    to find a balance between weights degeneracy and path degeneracy , del moral et al .",
    "( 2012 ) among others , suggest to resample only when the variance of the unnormalized weights is above a fixed threshold . in the solution",
    "we adopt , the threshold is a function of the effective sample size ( ess ) @xmath123 which takes values between 1 and @xmath122 and , as described in algorithm [ al : smc ] , we resample only when it is below @xmath124 . it should be noted here that executing the resampling step only when the condition @xmath124 is satisfied , does not alter the property of the algorithm that still returns an unbiased estimate of the normalising constant , as noted in a personal communication by c. andrieu and n. whiteley - see the work of arnaud & le gland ( 2009 ) .",
    "* step 1 . * sample @xmath122 labels , @xmath125 , from @xmath126 and set the corresponding weights @xmath127 for @xmath128 . + * step 2 . * for @xmath129 repeat the following    1 .",
    "if @xmath124 , for each @xmath130 resample @xmath131 using the discrete distribution @xmath132 otherwise keep all the current particles by @xmath133 for @xmath134 .",
    "2 .   sample , for each @xmath135 , a label @xmath136 from @xmath137 where @xmath138 and @xmath139 .",
    "set @xmath140 .",
    "3 .   set , for each @xmath135 @xmath141\\bigg/\\gamma(\\sum_{k=1}^k(n_k^{(i),j}+\\delta ) ) }   { \\bigg[\\prod_{k=1}^k \\xi_k(z_{1:i-1}^k,\\gamma_{1:p}^k,\\tau_{1:p}^{2,k}|\\widetilde{\\mathcal{d}}_k^{(i-1 ) } )   \\gamma(\\delta + n_k^{(i-1),j})\\bigg]\\bigg/\\gamma(\\sum_{k=1}^k(n_k^{(i-1),j}+\\delta))}\\ ] ] and @xmath142 .",
    "the conditional smc algorithm we iterate in the second stage of our sampling procedure is essentially the smc algorithm described in section [ sec : smcalgo ] except it preserves the path of one particle .    to describe the algorithm",
    ", we need to introduce a sequence of indexes @xmath143 to represent the genealogy of the @xmath144 particle for @xmath145 . once we have set @xmath146 , the genealogy of @xmath144 particle",
    "can then be defined recursively @xmath147 for @xmath148 where the @xmath149 are the recorded samples from the previous iteration of the smc algorithm .    as we can see from the algorithm [ al : consmc ] , the sampling sequence is similar to what is implemented in a standard smc algorithm except that one randomly chosen particle @xmath61 with its ancestral lineage @xmath150",
    "is fixed and ensured to survive , whereas the remaining @xmath151 particles are regenerated as usual .",
    "* step 1 . * sample @xmath152 labels @xmath153 from @xmath126 , for @xmath154 while @xmath155 ( i.e. excluding @xmath156 ) , and set all the weights @xmath127 for @xmath128 . + * step 2 . * for @xmath129 repeat the following    1 .   if @xmath124 , for each @xmath134 except @xmath157 , resample @xmath131 using the discrete distribution @xmath132 otherwise keep all the current particles by @xmath133 .",
    "sample @xmath153 from @xmath137 for each @xmath134 except @xmath157 , and update the corresponding path @xmath158 .",
    "set @xmath159 as for the smc algorithm , for each @xmath135 , ( this includes the fixed particle @xmath160 )      in the smc algorithm we sample from the posterior distribution of the latent label indicator variable @xmath161 . with",
    "the mcmc steps our objective is to update the other parameters of the mixture model that control the regression error distribution , the regularization of the regression coefficients and the variable selection process .",
    "the mcmc steps which target the posterior are as follows .      to update the @xmath162 , given all the other variables are fixed",
    ", we can use the following procedure . for each @xmath163 , assuming @xmath164 , sample for each @xmath60 where @xmath165 , @xmath166 with @xmath167 a user - set parameter and @xmath168 , independent for each @xmath60 . accept all the @xmath169 with probability @xmath170 otherwise keep the current @xmath171 .      to update @xmath172 ,",
    "given all the other variables are fixed , we can use the following procedure .",
    "for each @xmath173 , @xmath163 propose @xmath174 where @xmath175 is a user - set parameter ( potentially different from the @xmath176 above ) and @xmath177 , independent for each @xmath115 .",
    "note that @xmath178 features only one changed value from @xmath179 .",
    "the proposed move then is accepted with probability @xmath180 otherwise keep the current @xmath181 .      to update @xmath71 ,",
    "given all the other variables are fixed , we can use the following procedure . for each @xmath182 , @xmath163 ( i.e.  propose to change only one element each time ) , if @xmath22 we propose @xmath183 and draw @xmath184 from its prior ( @xmath185 ) .",
    "the proposed move is accepted with probability @xmath186 otherwise we keep @xmath22 .",
    "if @xmath165 , we propose to set it to be zero , removing the corresponding @xmath187 and using the same expression as above to accept / reject ( with the appropriate changes i.e. the proposed state here has fewer variables than the current model ) . in this proposal , we are adding or removing columns from our design matrix .",
    "note that this algorithm is best suited for scenarios similar to the ones we investigate in this paper , where the number of components @xmath188 and the number of data points @xmath189 make the space to be sampled much bigger than the one for the explanatory variables .",
    "the sampling procedure consists of    * * stage i : * initialise the algorithm .",
    "sample @xmath116 from the respective priors .",
    "run the smc algorithm , as described in section [ sec : smcalgo ] , storing all the @xmath122 particles labels @xmath190 and their genealogy @xmath191 .",
    "sample one particle index @xmath192 according to the normalized weights @xmath193 . * * stage ii : * repeat the following steps until convergence 1 .",
    "run the conditional smc algorithm , as described in section [ sec : condsmcalgo ] .",
    "sample @xmath192 according to new weights @xmath193 .",
    "store @xmath194 and @xmath195 .",
    "3 .   given @xmath194 ,",
    "update the current values of @xmath116 following the mcmc steps described in section [ sec : mcmcalgo ] .",
    "this provides a valid mcmc algorithm with the posterior of interest as an appropriate marginal ; see andrieu et al .",
    "we assume one basic scenario that we then perturb to highlight the different properties of the model and different important aspects of the simulation procedure . in the standard scenario the parameters of the model",
    "have been randomly generated from the following priors @xmath196 and each data point is then sampled from the mixture model .",
    "each dataset we generate contains @xmath197 paired observations sampled from a mixture of three components @xmath198 .",
    "the covariates @xmath26 of dimension @xmath199 are sampled from a centered gaussian distribution whose dispersion depends on the cluster membership .",
    "the only parameters of the simulation algorithm we need to set are : the number of particles , say @xmath200 ; the step length of the mcmc move for @xmath201 , say @xmath202 ; the step length of the error update , say @xmath203 , and also the number of repeats of the sampling procedure , say a few thousand .",
    "an important aspect of the simulation behaviour that we can partially control is the weight degeneracy . by introducing the adaptive resampling step we limit the risk of the empirical probability mass collapsing on a single particle .",
    "we are equally aware that resampling tends to replicate the most likely paths and might lead to an impoverished diversity of explored paths .",
    "this effect is marginally alleviated by limiting the frequency of the resampling .",
    "figure [ fig : weightsdegen ] shows that , in our case , adaptive resampling ultimately is beneficial to preserve both path and weight diversity .",
    "we note in the left column that if we resample after every new observation is processed , we end up fairly quickly with a single path that gets replicated for all @xmath122 particle . with adaptive resampling ,",
    "on the other hand , the degeneracy of weights and paths is maintained at a tolerable level . in the right column , we preserve a variety of paths that might have different likelihood as shown by the more disperse ess plot .",
    "note also how in some instances no resample is performed for several runs and the number of particles remains stable as it is their weight . even if at the end of every iteration of the markov chain we only need to store one single particle , it is important that we are able to preserve a richer variety of paths and consequently a more disperse weight distribution from which we can sample .",
    ", width=264,height=226 ]    , width=264,height=226 ]     +    , width=264,height=226 ]    , width=264,height=226 ]     +      to test the clustering accuracy of the model , we generate datasets using the simulation settings described in section [ sec : simset ] .",
    "we then let the algorithm run and for each iteration we save one particle that represents one sample from the posterior distribution of the label indicator variables .",
    "once we have collected enough samples we analyse the distribution of the adjusted rand index score over the sampled paths . to deal with the label switching problem we permute the labelling to maximize the adjusted rand index , computed w.r.t .",
    "the cluster assignment associated to an external model or corresponding to the null hypothesis we want to test ( like the macro sector partition in our real life problem ) .    in figure",
    "[ fig : adjridx ] we can see that the distribution is highly skewed towards @xmath10 , which means that most of the time the suggested clustering assignment perfectly matches the true clustering . in other words , given that the classification probability distribution we try to approximate is fairly accurate ( which seemed to be the case on the basis of our convergence assessment ) the model seems to provide a good clustering , at least in this example .    , width=453,height=188 ]",
    "the other major point we want to investigate is the accuracy of the variable selection approach .",
    "we would hope that the model identifies as many informative variables as possible , and at the same time is sufficiently parsimonious to exclude as many as possible of the noise variables . to that end",
    "the sensitivity index is the ratio of the number of true variables detected to the sum of the same value added to false negatives and specificity index is the ratio of the true negatives to the the sum of the same value added to false positives .",
    "these are the measures used to assess the accuracy of the variable selection .    in figures [ fig : senspec ]",
    "we look , as before , at the distribution over all mcmc iterations of the relevant indexes , in this case the sensitivity and specificity indexes .",
    "we remark that given the relatively small number of variables , @xmath199 , we should not be surprised to observe some very coarse distributions , since there are only so many informative or noise variables . in both plots",
    "it is evident that the overall variable selection accuracy is considerable .",
    "the sensitivity of the selection algorithm is fairly high , since most of the informative covariates are included and play a role in the regression curves .",
    "conversely , the specificity index is equally good if not better , as very few noise variables are retained at all .",
    "we can explain the marginally lower sensitivity compared to the specificity , by noticing that the model is successfully parsimonious and achieves a satisfactory clustering performance even with only a smaller subset of the informative variables .",
    ", width=264,height=188 ]    , width=264,height=188 ]     +",
    "in the financial literature it is common practice to group markets into macro sectors based on the type and nature of the good exchanged .",
    "practitioners operating in financial markets adhere to this convention and consider each sector as a separate area of expertise .",
    "this approach is reasonable for fundamental investors who have to be knowledgeable on the underlying factors driving demand / offer and have to elaborate the relevant information as news become public .",
    "a partition of the markets that mirrors the macro sectors is less suitable to systematic traders who take investment decisions based on algorithms which depend only on the evolution of prices . under these circumstances ,",
    "developing and optimizing a quantitative strategy on a sector by sector basis seems rather arbitrary .",
    "this is because the only input considered when engineering the strategy is the time series of prices whose behaviour is not necessarily a function of the sector .",
    "a clustering method which is more consistent with a systematic and objective approach , should identify homogeneous clusters of markets that share similar price dynamics characteristics .",
    "our approach starts by selecting , across all sectors , those major financial markets for which we have records spanning up to twenty years of trading ( see the following section ) . under the assumption that all the relevant information about a market can be extracted from the historical prices",
    ", we then compute for each market the summary statistics that measure the critical features of the distribution and the temporal dependence of time series of returns .    in a supervised learning framework ,",
    "the statistics of the price dynamics can be seen as explanatory variables that can help us understand why the trading performance is different across markets . when we apply a trading algorithm to every market",
    ", one can observe that the risk - adjusted profit we obtain is not consistent across markets .",
    "the supervised model we propose should be able to regress the profitability of the trading algorithm on some of the features we record for each market .",
    "an unsupervised approach is considered in cozzini ( 2012 ) , where the relative benefits of supervised versus unsupervised learning are investigated .",
    "assuming we achieve a more accurate partition of the markets , we are in an ideal position to develop a systematic trading strategy that better suits the markets within each group .",
    "a strategy that has been optimized on a market by market basis would likely be overfitted and would not have enough back testing ( i.e.  for testing the algorithm ) data . if , instead , we devise a trading algorithm that consistently performs on a group of markets , we are bound to obtain a more robust and convincing result . at the same time , the significant features that are responsible for driving the clustering process give us an insight on the critical aspects of price dynamics that should be exploited by the trading strategy .    in order to reach credible conclusions about how to partition markets and what are the informative features of the price dynamics , we need a clustering method which is able to address the following issues characteristic of financial data :    * outliers and potential skewness * non informative variables * fewer observations than explanatory variables .",
    "if we succeed in proposing a model whose performance is not hindered by these issues , we will have increased confidence in the trading strategy that we develop based on the outcome of clustering and variable selection process . by being able to implement a more targeted strategy on each group of markets",
    ", we should achieve better investment returns .",
    "the dataset analysed has been kindly provided by ahl research , a quantitative investment manager , and integrates external sources with proprietary records of live prices sampled during actual trading activity .",
    "the selection of markets considered covers several sectors , assets classes and regions .",
    "the details of each market considered are listed in table [ tab : modelse ] .",
    "the frequency of the samples is daily , typically the end of day official settlement price , whenever the exchange provides one",
    ".    .data . *",
    "market : * code identifies market . *",
    "type : * contract used for transaction .",
    "cash , x , exchange traded futures , f or forwards c. * start : * first date of daily records .",
    "* ccy : * currency .",
    "[ tab : modelse ] [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     as a further check of the relevance of the clustering proposed in table [ tab : clusmkts ] we can observe the value of the response variable @xmath204 for each cluster . in figure [ fig : boxplotsr ] we note a certain differentiation of the sharpe ratio between the two clusters , with cluster a markets generally showing a better performance of the trading strategy .    , width=453,height=377 ]",
    "we have considered a bayesian mixture of lasso regressions with @xmath0errors , designed for a financial data analysis problem .",
    "we applied a pmcmc algorithm to sample from a marginalized posterior and investigated the model and algorithm on simulated and real data . in our real data example",
    ", we saw that the clusters returned by our model could perform better in terms of profitability on an in - sample basis , than the physical clustering of the market .",
    "there are many issues that can be investigated in future work .",
    "firstly , with regards to the theoretical properties of the model .",
    "we did not investigate , for example , the issue of lindley s paradox , which can manifest itself in mixtures ( e.g.  jennison ( 1997 ) ) .",
    "that is , we would like to know if there are some combinations of prior parameters , which would lead one to favouring statistical models with a single component . in connection to this , whether the complex posterior also satisfies a collection of inequalities for model probabilities as is the case for some standard bayesian mixtures ; see nobile ( 2005 ) .    secondly , the computational procedure of selecting the number of components .",
    "there are at least two options which we intend to consider in future work .",
    "the first is simply to use our pmcmc algorithm in each model .",
    "then , as one can easily obtain a marginal likelihood estimate ( indeed using the proposed particles -`all the samples ' - see andrieu et al .",
    "( 2010 ) ) and compute bayes factors - see e.g. nobile ( 1994 ) .",
    "the second idea is to build a trans - dimensional sampler based upon pmcmc and smc samplers ( del moral et al .",
    "2006 ) . here",
    ", one uses a trans - dimensional version of the pmmh sampler .",
    "suppose one has a target density @xmath205 in dimension @xmath206 and our overall target density is : @xmath207 where @xmath208 is a prior on the dimension ( here the number of components in the mixture ) .",
    "thus we have defined a target density on @xmath209 now introduce a sequence of targets of dimension @xmath206 : @xmath210 where @xmath211 for some @xmath212 given .",
    "our trans - dimensional proposal is as follows : given a model order @xmath206 , propose a model order @xmath213 and use an smc sampler to simulate the sequence @xmath214 .",
    "the acceptance probability , when resampling at each time - point of the smc algorithm , of such a move is : @xmath215 where @xmath216 is the proposal density of moving from @xmath206 to @xmath213 and @xmath217 is the marginal likelihood estimate from the smc sampler in dimension @xmath213 .",
    "this allows one a possibility of producing very competitive trans - dimensional proposals .",
    "we thank ahl for providing the data .",
    "work completed whilst the first author was phd student at imperial college london .",
    "the second author was supported by an moe grant .",
    "we thank nicolas chopin and dave stephens for discussions on this work .",
    "we also thank elena erlich and james martin for their comments on the manuscript .                    di matteo , t. , aste , t. , & dacorogna , m.  ( 2004 ) .",
    "long term memories of developed and emerging markets : using the scaling analysis to characterize their stage of development . _ journal of banking & finance _ , * 4 * 29:46 ."
  ],
  "abstract_text": [
    "<S> motivated by a challenging problem in financial trading we are presented with a mixture of regressions with variable selection problem . in this </S>",
    "<S> regard , one is faced with data which possess outliers , skewness and , simultaneously , due to the nature of financial trading , one would like to be able to construct clusters with specific predictors that are fairly sparse . </S>",
    "<S> we develop a bayesian mixture of lasso regressions with @xmath0errors to reflect these specific demands . </S>",
    "<S> the resulting model is necessarily complex and to fit the model to real data , we develop a state - of - the - art particle markov chain monte carlo ( pmcmc ) algorithm based upon sequential monte carlo ( smc ) methods . </S>",
    "<S> the model and algorithm are investigated on both simulated and real data . +    _ some key words _ : mixture of regressions , variable selection , particle markov chain monte carlo    * a bayesian mixture of lasso regressions with @xmath0errors *    by alberto cozzini@xmath1 , ajay jasra@xmath2 & giovanni montana@xmath1    @xmath1department of mathematics , imperial college london , london , sw7 2az , uk . </S>",
    "<S> + e-mail:`a.m.cozzini@ic.ac.uk , g.montana@ic.ac.uk ` + @xmath2department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`staja@nus.edu.sg ` </S>"
  ]
}