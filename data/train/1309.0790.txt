{
  "article_text": [
    "in modern astronomy , one is increasingly faced with the problem of analysing large , complicated and multidimensional data sets .",
    "such analyses typically include tasks such as : data description and interpretation , inference , pattern recognition , prediction , classification , compression , and many more .",
    "one way of performing such tasks is through the use of machine learning methods . for accessible accounts of machine learning and its use in astronomy ,",
    "see , for example , @xcite , @xcite and @xcite .",
    "moreover , machine learning software easily used for astronomy , such as the python - based astroml package , or c - based fast artificial neural network library ( fann ) have recently started to become available .",
    "two major categories of machine learning are : _ supervised learning _ and _ unsupervised learning_. in supervised learning , the goal is to infer a function from labeled training data , which consist of a set of training examples .",
    "each example has both ` properties ' and ` labels ' .",
    "the properties are known ` input ' quantities whose values are to be used to predict the values of the labels , which may be considered as ` output ' quantities .",
    "thus , the function to be inferred is the mapping from properties to labels .",
    "once learned , this mapping can be applied to datasets for which the values of the labels are not known .",
    "supervised learning is usually further subdivided into _ classification _ and _ regression_. in classification , the labels take discrete values , whereas in regression the labels are continuous .    in astronomy , for example , using multifrequency observations of a supernova lightcurve ( its properties ) to determine its type ( e.g. ia , ib , ii , etc . )",
    "is a classification problem since the label ( supernova type ) is discrete ( see , e.g. , @xcite ) , whereas using the observations to determine ( say ) the energy output of the supernova explosion is a regression problem , since the label ( energy output ) is continuous .",
    "classification can also be used to obtain a distribution for an output value that would normally be treated as a regression problem .",
    "this is demonstrated by  @xcite for measuring redshifts in cfhtlens .",
    "a particularly important recent application of regression supervised learning in astrophysics and cosmology ( and beyond ) is the acceleration of the statistical analysis of large data sets in the context of complicated models . in such analyses ,",
    "one typically performs many ( @xmath0 ) evaluations of the likelihood function describing the probability of obtaining the data for different sets of values of the model parameters . for some problems , in particular in cosmology ,",
    "each such function evaluation can take up to tens of seconds , making the analysis very computationally expensive . by performing regression",
    "supervised learning to infer and then replace the mapping between model parameters and likelihood value , once can reduce the computation required for each likelihood evaluation by several orders of magnitude , thereby vastly accelerating the analysis ( see , e.g. , @xcite ) .    in unsupervised learning ,",
    "the data have no labels .",
    "more precisely , the quantities ( often termed ` observations ' ) associated with each data item are not divided into properties ( inputs ) and labels ( outputs ) .",
    "this lack of a ` causal structure ' , where the inputs are assumed to be at the beginning and outputs at the end of a causal chain , is the key difference from supervised learning . instead ,",
    "all the observations are considered to be at the end of a causal chain , which is assumed to begin with some set of ` latent ' ( or hidden ) variables .",
    "the aim of unsupervised learning is to infer the number and/or nature of these latent variables ( which may be discrete or continuous ) by finding similarities between the data items .",
    "this then enables one to summarize and explain key features of the dataset .",
    "the most common tasks in unsupervised learning include _ density estimation _ , _ clustering _ and _ dimensionality reduction_. indeed , in some cases ,",
    "dimensionality reduction can be used as a pre - processing step to supervised learning , since classification and regression can sometimes be performed in the reduced space more accurately than in the original space .    as an astronomical example of unsupervised learning one",
    "might wish to use multifrequency observations of the lightcurves of a set of supernovae to determine how many different types of supernovae are contained in the set ( a clustering task ) .",
    "alternatively , if the data set also includes the type of each supernova ( determined using spectroscopic observations ) , one might wish to determine which properties , or combination of properties , in the lightcurves are most important for determining their type photometrically ( a dimensionality reduction task ) .",
    "this reduced set of property combinations could then be used instead of the original lightcurve data to perform the supernovae classification or regression analyses mentioned above .",
    "an intuitive and well - established approach to machine learning , both supervised and unsupervised , is based on the use of artificial neural networks ( nns ) , which are loosely inspired by the structure and functional aspects of a brain .",
    "they consist of a group of interconnected nodes , each of which processes information that it receives and then passes this product on to other nodes via weighted connections . in this way ,",
    "nns constitute a non - linear statistical data modeling tool , which may be used to represent complex relationships between a set of inputs and outputs , to find patterns in data , or to capture the statistical structure in an unknown joint probability distribution between observed variables .",
    "in general , the structure of a nn can be arbitrary , but many machine learning applications can be performed using only feed - forward nns . for such networks",
    "the structure is directed : an input layer of nodes passes information to an output layer via zero , one , or many ` hidden ' layers in between .",
    "such a network is able to ` learn ' a mapping between inputs and outputs , given a set of training data , and can then make predictions of the outputs for new input data .",
    "moreover , a universal approximation theorem assures us that we can accurately and precisely approximate the mapping with a nn of a given form .",
    "a useful introduction to nns can be found in  @xcite .    in astronomy ,",
    "feed - forward nns have been applied to various machine learning problems for over 20 years ( see , e.g. , @xcite ) .",
    "nonetheless , their more widespread use in astronomy has been limited by the difficulty associated with standard techniques , such as backpropagation , in training networks having many nodes and/or numerous hidden layers ( i.e. ` large ' and/or ` deep ' networks ) , which are often necessary to model the complicated mappings between the numerous inputs and outputs in modern astronomical applications .    in this paper",
    ", we therefore present the first public release of skynet : an efficient and robust neural network training tool that is able to train large and/or deep feed - forward networks .",
    "skynet is able to achieve this by using a combination of the ` pre - training ' method of @xcite to obtain a set of network weights close to a good optimum of the training objective function , followed by further optimisation of the weights using a regularised variant of newton s method based on that developed for the memsys software package @xcite . in particular ,",
    "second - order derivative information is used to improve convergence , but without the need to evaluate or store the full hessian matrix , by using a fast approximate method to calculate hessian - vector products @xcite .",
    "skynet is implemented in the standard ansi c programming language and parallelised using mpi .",
    "we also note that skynet has already been combined with multinest  @xcite , to produce the blind accelerated multimodal bayesian inference ( bambi ) package @xcite , which is a generic and completely automated tool for greatly accelerating bayesian inference problems ( by up to a factor of @xmath1 ; see , e.g. , @xcite ) .",
    "multinest is a fully - parallelised implementation of nested sampling @xcite , extended to handle multimodal and highly - degenerate distributions . in most astrophysical ( and particle physics )",
    "bayesian inference problems , multinest typically reduces the number of likelihood evaluations required by an order of magnitude or more , compared to standard mcmc methods , but bambi achieves further substantial gains by speeding up the evaluation of the likelihood itself by replacing it with a trained regression neural network .",
    "bambi proceeds by first using multinest to obtain a specified number of new samples from the model parameter space , and then uses these as input to skynet to train a network on the likelihood function .",
    "after convergence to the optimal weights , the network s ability to predict likelihood values to within a specified tolerance level is tested .",
    "if it fails , sampling continues using the original likelihood until enough new samples have been made for training to be performed again . once a network is trained that is sufficiently accurate , its predictions are used in place of the original likelihood function for future samples for multinest . on typical problems in cosmology , for example , using the network reduces the likelihood evaluation time from seconds to less than a millisecond , allowing multinest to complete the analysis much more rapidly . as a bonus , at the end of the analysis the user also obtains a network that is trained to provide more likelihood evaluations near the peak if needed , or in subsequent analyses . with the public release of skynet",
    ", we now also make bambi publically available .",
    "the structure of this paper is as follows . in section  [ sec : nnstruct ]",
    "we describe the general structure of feed - forward nns , including a particular special case of such networks , called autoencoders , which may be used for performing non - linear dimensionality reduction . in section",
    "[ sec : nntrain ] we present the procedures used by skynet to train networks of these types .",
    "skynet is then applied to some toy machine learning examples in section  [ sec : nntoyex ] , including a regression task , a classification task , and a dimensionality reduction task using autoencoders .",
    "we also apply skynet to the problem of classifying images of handwritten digits from the mnist database , which is a widely - used benchmarking test of machine learning algorithms .",
    "the application of skynet to astronomical machine learning examples is presented in section  [ sec : nnex_astro ] , including : a regression task to determine the projected ellipticity of a galaxy from blurred and noisy images of the galaxy and of a field star ; a classification task , based on a simulated gamma - ray burst detection pipeline for the swift satellite @xcite , to determine if a grb with given source parameters will be detected ; and a dimensionality reduction task using autoencoders to compress and denoise galaxy images .",
    "finally , we present our conclusions in section  [ sec : nndiscuss ] .",
    "a multilayer perceptron feed - forward neural network is the simplest type of network and consists of ordered layers of perceptron nodes that pass scalar values from one layer to the next .",
    "the perceptron is the simplest kind of node , and maps an input vector @xmath2 to a scalar output @xmath3 via @xmath4 where @xmath5 and @xmath6 are the parameters of the perceptron , called the ` weights ' and ` bias ' , respectively . for a 3-layer nn , which consists of an input layer , a hidden layer , and an output layer , as shown in fig .",
    "[ fig : neuralnet ] , the outputs of the nodes in the hidden and output layers are given by the following equations : @xmath7 where @xmath8 runs over input nodes , @xmath9 runs over hidden nodes , and @xmath10 runs over output nodes .",
    "the functions @xmath11 and @xmath12 are called activation functions and must be smooth and monotonic for our purposes .",
    "we use @xmath13 ( sigmoid ) and @xmath14 ; the non - linearity of @xmath11 is essential to allowing the network to model non - linear functions . to expand the nn to include more hidden layers , we iterate for each connection from one hidden layer to the next , each time using the same activation function , @xmath11",
    ". the final hidden layer will connect to the output layer using the relation .",
    "the weights and biases are the values we wish to determine in our training ( described in section  [ sec : nntrain ] ) .",
    "as they vary , a huge range of non - linear mappings from inputs to outputs is possible .",
    "in fact , a universal approximation theorem  @xcite states that a nn with three or more layers can approximate any continuous function to some given accuracy , as long as the activation function is locally bounded , piecewise continuous , and not a polynomial ( hence our use of sigmoid @xmath11 , although other functions would work just as well , such as @xmath15 ) . by increasing the number of hidden nodes",
    ", one can achieve more accuracy at the risk of overfitting to our training data .",
    "other activation functions have also been proposed , such as the rectified linear function wherein @xmath16 or the ` softsign ' function where @xmath17 .",
    "it has been argued that the former removes the need for pre - training ( as described in section  [ sec : nnpretrain ] )  @xcite and serves as a better model of biological neurons .",
    "the ` softsign ' is similar to @xmath15 , but with slower approach to the asymptotes of @xmath18 ( quadratic rather than exponential )  @xcite .",
    "autoencoders are a specific type of feed - forward neural network containing one or more hidden layers , where the inputs are mapped to themselves , i.e. the network is trained to approximate the identity operation ; when more than one hidden layer is used this is typically referred to as a ` deep ' autoencoder .",
    "such networks typically contain several hidden layers and are symmetric about a central layer containing fewer nodes than there are inputs ( or outputs ) .",
    "a basic diagram of an autoencoder is shown in fig .  [",
    "fig : autoencoder ] , in which the three inputs @xmath19 are mapped to themselves via three symmetrically - arranged hidden layers , with two nodes in the central layer .    ) defines the weight matrices @xmath20 and @xmath21 . ]",
    "an autoencoder can thus be considered as two half - networks , with one part mapping the inputs to the central layer and the second part mapping the central layer values to the outputs ( which approximate as closely as possible the original inputs ) .",
    "these two parts are called the ` encoder ' and ` decoder ' , respectively , and map either to or from a reduced set of ` feature variables ' embodied in the nodes of the central layer ( denoted by @xmath22 and @xmath23 in fig .",
    "[ fig : autoencoder ] ) .",
    "these variables are , in general , non - linear functions of the original input variables .",
    "one can determine this dependence for each feature variable in turn simply by decoding @xmath24 , @xmath25 , and so on , as the corresponding @xmath26 value is varied ; in this way , for each feature variable , one obtains a curve in the original data space .",
    "conversely , the collection of feature values @xmath27 in the central layer might reasonably be termed the feature vector of the input data .",
    "autoencoders therefore provide a very intuitive approach to non - linear dimensionality reduction and constitute a natural generalisation of linear methods such as principal component analysis ( pca ) and independent component analysis ( ica ) , which are widely used in astronomy .",
    "indeed , an antoencoder with a single hidden layer and linear activation functions may be shown to be identical to pca @xcite .",
    "this topic is explored further in section  [ sec : nntoy_ae ] .",
    "it is worth noting that encoding from input data to feature variables can also be useful in performing clustering tasks ; this is illustrated in section  [ sec : mnist ] .",
    "autoencoders are , however , notoriously difficult to train , since the objective function contains a broad local maximum where each output is the average value of the inputs  @xcite .",
    "nonetheless , this difficulty can be overcome by the use of pre - training methods , as discussed in section  [ sec : nnpretrain ] .",
    "an important choice when training a nn is the number of nodes in its hidden layers .",
    "the optimal number and organisation into one or more layers has a complicated dependence on the number of training data points , the number of inputs and outputs , and the complexity of the function to be trained .",
    "choosing too few nodes will mean that the nn is unable to learn the relationship to the highest possible accuracy ; choosing too many will increase the risk of overfitting to the training data and will also slow down the training process . using empirical evidence  @xcite and theoretical considerations",
    "@xcite , it has been suggested that the optimal architecture for approximating a continuous function is one hidden layer containing @xmath28 nodes , where @xmath29 is the number of input nodes .",
    "@xcite also find empirical support for this suggestion .",
    "such a choice allows the network to model the form of the mapping function without unnecessary work .    in practice",
    ", it can be better to over - estimate ( slightly ) the number of hidden nodes required . as described in section  [ sec : nntrain ] , skynet performs basic checks to prevent over - fitting , and the additional training time associated with having more hidden nodes is not a large penalty if an optimal network can be obtained in an early attempt . in any case ,",
    "given a particular problem , the optimal network structure , both in terms of the number of hidden nodes and how they are distributed into layers , can be determined by comparing the correlation and error squared of different trained nns ; this is illustrated in section  [ sec : nntoyex ] .",
    "in training a nn , we wish to find the optimal set of network weights and biases that maximise the accuracy of the predicted outputs . however , we must be careful to avoid overfitting to our training data , which may lead to inaccurate predictions from inputs on which the network has not been trained .",
    "the set of training data inputs and outputs ( or ` targets ' ) , @xmath30 , is provided by the user ( where @xmath31 counts training items ) .",
    "approximately @xmath32 per cent should be used for actual nn training and the remainder retained as a validation set that will be used to determine convergence and to avoid overfitting .",
    "this ratio of 3:1 gives plenty of information for training but still leaves a representative subset of the data for checks to be made .",
    "it is prudent to ` whiten ' the data before training a network .",
    "whitening normalises the input and/or output values , so that it easier to train a network starting from initial weights that are small and centred on zero .",
    "the network weights in the first and last layers can then be ` unwhitened ' after training so that the network will be able to perform the mapping from original inputs to outputs .",
    "standard whitening transforms each input to a standard distribution by subtracting the mean and dividing by the standard deviation over all elements in the training data , such that    [ eq : whitening1 ] @xmath33    an alternative whitening transform is also commonly used , wherein all values are scaled and shifted into the interval @xmath34 $ ] , such that @xmath35    one of these transforms may be chosen by the user if they wish to whiten the inputs of the training data .",
    "the whitening is normally performed separately on each input , but can be calculated across all inputs if they are related .",
    "the mean , standard deviation , minimum , or maximum would then be computed over all inputs for all training data items .",
    "the chosen whitening transform is also used for whitening the outputs . since both transforms consist of subtracting an offset and multiplying by a scale factor , they can easily be performed and reversed . to unwhiten network weights",
    "the inverse transform is applied , with the offset and scale determined by the source input node or target output node .",
    "outputs for a classification network are not whitened since they are already just probabilities ( see below ) .",
    "let us denote the network weights and biases collectively by the network parameter vector @xmath36 .",
    "skynet considers the parameters @xmath36 to be random variables with a posterior distribution given by @xmath37 where @xmath38 is the likelihood , which also depends on a set of hyperparameters @xmath39 that describe the standard deviation of the outputs ( see below ) .",
    "the likelihood encodes how well the nn , characterised by a given set of parameters @xmath36 , is able to reproduce the known training data outputs .",
    "this is modulated by the prior @xmath40 , which is assumed to have the ( logarithmic ) form @xmath41 where @xmath42 is a hyperparameter that plays the role of a regularisation parameter during optimisation since it determines relative importance of the prior and the likelihood .",
    "this prior can also be seen as an @xmath43-norm penalty .",
    "the form of the likelihood depends on the type of network being trained .      for regression problems",
    ", skynet assumes a log - likelihood function for the network parameters @xmath36 given by the standard @xmath44 misfit function @xmath45 ^ 2},\\end{aligned}\\ ] ] where @xmath29 is the number of outputs , @xmath46 is the number of training data examples and @xmath47 is the nn s predicted output vector for the input vector @xmath48 and network parameters @xmath36 .",
    "the hyperparameters @xmath49 describe the standard deviation ( error size ) of each of the outputs .      for classification problems ,",
    "skynet again uses continuous outputs ( rather than discrete ones ) , which are interpreted as the probabilities that a set of inputs belongs to a particular output class .",
    "this is achieved by applying the _ softmax _ transformation to the output values , so that they are all non - negative and sum to unity , namely @xmath50}{\\sum_{j=1}^n \\exp[y_j(\\bmath{x}^{(k)};\\bmath{a})]}.\\ ] ] the classification likelihood is then given by the _ cross - entropy _ of the targets and softmaxed output values  @xcite , @xmath51 in this scenario , the true and predicted output values are both probabilities ( which lie in @xmath34 $ ] ) . for the true outputs , all are zero except for the correct output which has a value of unity . for classification networks",
    ", the @xmath39 hyper - parameters do not appear in the log - likelihood .",
    "the training of the nn can be started from some random initial state , or from a state determined from a ` pre - training ' procedure discussed below . in the former case ,",
    "the network training begins by setting random values for the network parameters , sampled from a normal distribution with zero mean and variance of @xmath52 ( this value can be modified by the user ) .    in the latter case",
    ", skynet makes use of the pre - training approach developed by @xcite , which obtains a set of network weights and biases close to a good solution of the network objective function .",
    "this method was originally devised with autoencoders in mind and is based on the model of restricted boltzmann machines ( rbms ) .",
    "an rbm is a generative model that can learn a probability distribution over a set of inputs .",
    "it consists of a layer of input nodes and a layer of hidden nodes , as shown in figure  [ fig : rbm ] . in this case , the map from the inputs to the hidden layer and then back is treated symmetrically and the weights are adjusted through a number of ` epochs ' , gradually reducing the reproduction error . to model an autoencoder , rbms",
    "are ` stacked ' , with each rbm s hidden layer being the input for the next .",
    "the initial case is the nn s inputs to the first hidden layer ; this is repeated for the first to second hidden layer and so on until the central layer is reached .",
    "the network weights can then be ` unfolded ' by using the transpose for the symmetric connections in the decoding half to provide a decent starting point for the full training to begin .",
    "this is shown in fig .",
    "[ fig : autoencoder ] , where the @xmath20 and @xmath21 weights matrices are defined by pre - training .     visible nodes and @xmath53 hidden nodes .",
    "bias nodes are not shown .",
    "image courtesy wikimedia commons . ]",
    "the training is then performed using _ contrastive divergence _  @xcite .",
    "this procedure can be summarised in the following steps , where sampling indicates setting the value of the node to @xmath54 with the probability calculated and @xmath55 otherwise .    1 .",
    "take a training sample @xmath56 and compute the probabilities of the hidden nodes ( their values using a sigmoid activation function ) and sample a hidden vector @xmath57 from this distribution .",
    "2 .   let @xmath58 , where @xmath59 is used to indicate the outer product .",
    "3 .   using @xmath57 ,",
    "compute the probabilties of the visible nodes and sample @xmath60 from this distribution .",
    "resample the hidden vector from this to obtain @xmath61 .",
    "4 .   let @xmath62 . 5 .",
    "@xmath63 for some learning rate @xmath64 .",
    "more details can be found in  @xcite and  @xcite has useful diagrams and explanations .",
    "this pre - training approach can also be used for more general feed - forward networks .",
    "all layers of weights , except for the final one that connects the last hidden layer to the outputs , are pre - trained as if they were the first half of a symmetric autoencoder .",
    "however , the network weights are not unfolded ; instead the final layer of weights is initialised randomly as would have been done without pre - training . in this way",
    ", the network ` learns the inputs ' before mapping to a set of outputs .",
    "this has been shown to greatly reduce the training time on multiple problems by  @xcite .",
    "we note that when an autoencoder is pre - trained , the activation function to the central hidden layer is made linear and the activation function from the final hidden layer to the outputs is made sigmoidal .",
    "general feed - forward networks that are pre - trained continue to use the original activation functions .",
    "both of these are simply the default settings and the user has the freedom to alter them to suit their specific problem .",
    "once the initial set of network parameters have been obtained , either by assigning them randomly or through pre - training , the network is then trained ( further ) by iterative optimisation of the objective function .",
    "first , initial values of the hyperparameters @xmath39 ( for regression networks ) and @xmath42 are set .",
    "the values @xmath39 are set by the user and can be set on either the true output values themselves or on their whitened values ( as defined in section  [ sec : nntrain_whiten ] ) .",
    "the only difference between these two settings is the magnitude of the error used .",
    "the algorithm then calculates a large initial estimate for @xmath42 , @xmath65 where @xmath66 is the total number of weights and biases ( nn parameters ) and @xmath67 is a rate set by the user ( @xmath64 , default @xmath68 ) that defines the size of the ` confidence region ' for the gradient .",
    "this expression for @xmath42 sets larger regularisation ( or ` damping ' ) when the magnitude of the gradient of the likelihood is larger .",
    "this relates the amount of ` smoothing ' required to the steepness of the function being smoothed .",
    "the rate factor in the denominator allows us to increase the damping for smaller confidence regions on the value of the gradient .",
    "this results in smaller , more conservative steps that are more likely to result in an increase in the function value but results in more steps being required to reach the optimal weights .",
    "nn training then proceeds using an adapted form of a truncated newton ( or ` hessian - free ' ) optimisation algorithm as described below , to calculate the step , @xmath69 , that should be taken at each iteration .",
    "following each such step , adjustments to @xmath42 and @xmath39 may be made before another step is calculated .",
    "first , @xmath39 can be updated by multiplying it by a value @xmath70 such that @xmath71 .",
    "this serves to assure that at convergence , the @xmath44 value equals the number of unconstrained data points of the problem .",
    "similarly , @xmath42 is then updated such that the probability @xmath72 is maximised for the current set of nn parameters @xmath36 .",
    "these procedures are described in detail by  ( * ? ? ?",
    "2.3 & 2.6 ) and  ( * ? ? ?",
    "3.6 & appendix b ) .    to obtain the step @xmath69 at each iteration",
    ", we first note that one may approximate a general function @xmath73 up to second - order in its taylor expansion by @xmath74 where @xmath75 is the gradient and @xmath76 is the hessian matrix of second derivatives , both evaluated at @xmath36 . for our purposes , the function @xmath73 is the log - posterior distribution of the nn parameters and hence   represents a gaussian approximation to the posterior .",
    "the hessian of the log - posterior is the regularised ( ` damped ' ) hessian of the log - likelihood function , where the prior , whose magnitude is set by @xmath42 , provides the regularisation .",
    "if we define the hessian matrix of the log - likelihood as @xmath77 , then @xmath78 , where @xmath79 is the identity matrix . the regularisation parameter @xmath42 can be interpreted as controlling the level of ` conservatism ' in the gaussian approximation to the posterior . in particular",
    ", regularisation helps prevent the optimisation becoming trapped in small local maxima by smoothing out the function being explored .",
    "it also aids in reducing the region of confidence for the gradient information which will make it less likely that a step results in a worse set of parameters .",
    "ideally , we seek a step @xmath69 , such that @xmath80 . using the approximation ,",
    "one thus requires @xmath81 in the standard newton s method of optimisation one simply solves this equation directly for @xmath82 to obtain @xmath83 in principle , iterating this stepping procedure will eventually bring us to a local maximum of @xmath73 .",
    "moreover , newton s method has the important property of being scale - invariant , namely its behaviour is unchanged under any linear rescaling of the parameters .",
    "methods without this property often have problems optimising poorly scaled parameters .",
    "there are , however , some major practical difficulties with the standard newton s method .",
    "first , the hessian @xmath77 of the log - likelihood is not guaranteed to be positive semi - definite .",
    "thus , even after the addition of the damping term @xmath84 derived from the log - prior , the full hessian @xmath85 of the log - posterior may also not be invertible .",
    "second , even if @xmath85 is invertible , the inversion is prohibitively expensive if the number of parameters is large , as is the case even for modestly - sized neural networks .    to address the first issue",
    ", we replace the hessian @xmath77 with a form of gauss ",
    "newton approximation @xmath86 , which is guaranteed to be positive semi - definite and can be defined both for the regression likelihood and the classification likelihood , respectively @xcite . in particular , the approximation used differs from the classical gauss  newton matrix in that it retains some second derivative information .",
    "second , to avoid the prohibitive expense of calculating the inverse in , we instead solve ( with @xmath77 replaced by @xmath86 in @xmath85 ) for @xmath69 iteratively using a conjugate - gradient algorithm , which requires only matrix - vector products @xmath87 for some vector @xmath88 .",
    "one can avoid even the computational burden of calculating and storing the hessian @xmath85 . in principle",
    ", products of the form @xmath87 can be easily computed using finite differences at the cost of a single extra gradient evaluation using the identity @xmath89 this approach is , however , subject to numerical problems .",
    "therefore , we instead calculate @xmath87 products using a stable and efficient procedure applicable to nns @xcite .",
    "this involves an additional forward and backward pass through the network beyond the initial ones required for a gradient calculation .",
    "the combination of all the above methods makes practical the use of second - order derivative information even for large networks and significantly improves the rate of convergence of nn training over standard backpropagation methods .",
    "it has been noted that this method for quasi - newton second - order descent is equivalent to the first - order ` natural gradient ' by  @xcite .      following each iteration of the optmisation algorithm ,",
    "the posterior , likelihood , correlation , and error squared values are calculated both for the training data and for the validation data ( which were not used in calculating the steps in the optimisation ) .",
    "the correlation of the network outputs is defined for each output @xmath10 as @xmath90 where @xmath91 and @xmath92 are the means of these output variables over all the training data ; the functional dependencies of @xmath93 have been dropped for brevity .",
    "the correlation provides a relative measure of how well the predicted outputs match the true ones . in practice",
    ", the correlations from each output can be averaged together to give an average correlation for the network s predictions .",
    "the average error - squared of the network outputs is defined by @xmath94 ^ 2 } , \\label{eq : errsqr_defn}\\ ] ] and is complementary to their correlation , since it is an absolute measure of accuracy .",
    "as one might expect , as the optimisation proceeds , there is a steady increase in the values of the posterior , likelihood , correlation , and negative of the error squared , evaluated both for the training and validation data .",
    "eventually , however , the algorithm will begin to overfit , resulting in the continued increase of these quantities when evaluated on the training data , but a decrease in them when evaluated on the validation data .",
    "this divergence in behaviour is taken as indicating that the algorithm has converged and the optimisation in terminated .",
    "the user may choose which of the four quantities listed above is used to determine convergence , although the default is to use the error squared , since it does not include the hyperparameters @xmath95 and @xmath42 in its calculation and is less prone to problems with zeros than the correlation .",
    "we note that the correlation and the error - squared functions discussed above also provide quantitative measures with which to compare the performance of different network architectures , both in terms of the number of hidden nodes and how they are distributed into layers . as network size and complexity",
    "is increased , a point will be reached at which minimal or no further gains may be achieved in increasing correlation or reducing error - squared .",
    "therefore , any network architecture that can achieve this peak performance is equally well - suited . in practice ,",
    "we will wish to find the smallest or simplest network that does so as this minimizes the risk of overfitting and the time required for training .",
    "after training a network , in particular a regression network , one may want to calculate the accuracy of the network s predicted outputs .",
    "a computationally cheap method of estimating this was suggested by  @xcite , whereby one adds gaussian noise to the true outputs of the training data and trains a new network on this noisy data . after performing many realisations",
    ", the networks predictions will average to the predictions in the absence of the added noise .",
    "moreover , the standard deviation of their predictions will provide a good estimate of the accuracy of the original network s predictions . since one can train the new networks using the original trained network as a starting point ,",
    "the re - training on noisy data is very fast .",
    "additionally , evaluating the ensemble of predictions to measure the accuracy is not very computationally intensive as network evaluations are simple to perform and can be done in less than a millisecond .",
    "explicitly , the steps of this method are :    1 .   start with the converged network with parameters @xmath96 , trained on the original data set @xmath97 .",
    "estimate the noise on the residuals using @xmath98 ^ 2/k$ ] .",
    "2 .   define a new data set @xmath99 by adding gaussian noise of zero mean and variance @xmath100 to the outputs ( targets ) in @xmath101 .",
    "3 .   train a nn on @xmath99 using the parameters @xmath96 as a starting point .",
    "training should converge rapidly as the new data set is only slightly different from the original .",
    "denote the new network parameters by @xmath102 .",
    "4 .   repeat steps ( ii ) and ( iii ) multiple times to obtain an ensemble of networks with parameters @xmath103 .",
    "5 .   use each of the networks @xmath103 to make a prediction for a given set of inputs .",
    "the accuracy of the original network s outputs can be estimated as the standard deviation of the outputs of these networks .",
    "in addition to these steps , skynet includes the option for the user to add random gaussian offsets to the parameters @xmath96 before training is performed on the new data set ( step iii ) .",
    "this offset will aid the training in moving the optimisation from a potential local maximum in the posterior distribution of the network parameters , but the size of the offset must be chosen for each problem ; for this , we recommend using a value @xmath104 .",
    "we thus add noise to both the training data and the saved network parameters before training a new network whose posterior maximum will be near to , but not exactly the same as , the original network s .    this method may be compared with that described in  @xcite for determining the accuracy of the nn predictions for the likelihood used in bambi .",
    "although the method described here requires the overhead time of training additional networks , this is small compared the speed gains possible .",
    "indeed , the new method s accuracy computations require less than a millisecond , as opposed to tenths of a second for the method used previously . consequently , the faster method described here is now incorporated into our new public release version of bambi , leading to around two orders of magnitude increase in speed over that reported in  @xcite .",
    "as our first toy example , we consider a simple regression problem . we generate @xmath105 points randomly in the range",
    "@xmath106 $ ] , for which we evaluate the ramped sinc function , @xmath107 and then add gaussian noise with zero mean and a standard deviation of @xmath108 .",
    "the addition of noise makes the regression problem more difficult and prevents any exact solution being possible .    to perform the regression , the @xmath105 data items",
    "@xmath109 are divided randomly into @xmath110 items for training and @xmath111 for validation . for this simple problem",
    ", we use a network with a single hidden layer containing @xmath29 nodes ( we denote the full network by @xmath112 ) , and we whiten the input and output data using . the network was not pre - trained .",
    "the optimal value for @xmath29 is determined by comparing the correlation and error - squared for networks with different numbers of hidden nodes .",
    "these results are presented in fig .",
    "[ fig : sincevidence ] , which shows that the correlation increases and the error - squared decreases until we reach @xmath113 hidden nodes , after which both measures level off .",
    "obtained from converged nns with architecture @xmath112 for the ramped sinc function regression problem.,height=215 ]    thus , adding additional nodes beyond this number does not improve the accuracy of the network .",
    "for the network with @xmath114 hidden nodes , we obtain a correlation of greater than @xmath115 per cent ; a comparison of the true and predicted outputs in this case is shown in figure  [ fig : sincplot ] .",
    "on the training data ( left ) and validation data ( right ) for the ramped sinc function regression problem . ]",
    "we now consider a toy classification problem based on the three - way classification data set created by radford neal for testing his own algorithms for nn training . in this data",
    "set , each of four variables @xmath116 , @xmath117 , @xmath118 , and @xmath119 is drawn @xmath120 times from the standard uniform distribution @xmath121 $ ] .",
    "if the two - dimensional euclidean distance between @xmath122 and @xmath123 is less than @xmath124 , the point is placed in class @xmath55 ; otherwise , if @xmath125 , the class was set to @xmath54 ; and if neither of these conditions is true , the class was set to @xmath126 . note that the values of @xmath118 and @xmath119 play no part in the classification .",
    "gaussian noise with zero mean and standard deviation @xmath127 is then added to the input values .",
    "approximately @xmath32 percent of the data was used for training and the remaining @xmath128 per cent for validation .",
    "we again use a network with a single hidden layer containing @xmath29 nodes , and we whiten the input and output data using . the network was not pre - trained .",
    "the full network thus has the architecture @xmath129 , where the three output nodes give the probabilities ( which sum to unity ) that the input data belong to class 0 , 1 , or 2 , respectively .",
    "the final class assigned is that having the largest probability .    the optimal value for @xmath29 is again determined by comparing the correlation and error - squared for networks with different numbers of hidden nodes .",
    "these results are shown in fig .",
    "[ fig : classprobstruct ] , from which we see that the correlation increases and the error - squared decreases until we reach @xmath130 hidden nodes , after which both measures level off . for the network with @xmath131 hidden nodes ,",
    "a total of @xmath132 per cent of training data points and @xmath133 per cent of validation points were correctly classified .",
    "a summary of the classification results for this network is given table  [ tab : cdatatrain ] .",
    ".classification results for the converged nn with architecture @xmath134 for the neal data set . [ cols=\"^,^,^,^,^,^ \" , ]     these results show that the extra information given to the regression networks trained on @xmath135 feature values from the autoencoder acted as a disadvantage in predicting the galaxy ellipticities . for networks trained on @xmath111 or @xmath136 features , however , the accuracies of the predicted ellipticities were better even than those obtained using the full original images in some cases .",
    "this demonstrates the power of being able to eliminate redundant information and noise , and thereby improve the accuracy of the analysis .",
    "we also observe that adding unnecessary complexity to the nn structure makes it more difficult for the algorithm to find the global maximum .",
    "this same method for dimensionality reduction  which also eliminates noise  before performing measurements can clearly be applied to a wide range of other astronomical applications .",
    "examples include classification of supernovae by type , or measurements of galaxies and stars by their spectra .",
    "we have described an efficient and robust neural network training algorithm , called skynet , which we have now made freely available for academic purposes .",
    "this generic tool is capable of training large and deep feed - forward networks , including autoencoders , and may be applied to supervised and unsupervised machine learning tasks in astronomy , such as regression , classification , density estimation , clustering and dimensionality reduction .",
    "skynet employs a combination of ( optional ) pre - training followed by iterative refinement of the network parameters using a regularised variant of newton s optimisation algorithm that incorporates second - order derivative information without the need even to compute or store the hessian matrix .",
    "linear and sigmoidal activation functions are provided for the user to choose between .",
    "skynet adopts convergence criteria that naturally prevent overfitting , and it also includes a fast algorithm for estimating the accuracy of network outputs .",
    "we first demonstrate the capabilities of skynet on toy examples of regression , classification , and dimensionality reduction using autoencoder networks , and then apply it to the classic machine learning problem of handwriting classification for determining digits from the mnist database . in an astronomical context , skynet is applied to : the regression problem of measuring the ellipticity of noisy and convolved galaxy images in the mapping dark matter challenge ; the classification problem of identifying gamma - ray bursters that are detectable by the swift satellite ; and the dimensionality reduction problem of compressing and denoising images of galaxies . in each case",
    ", the straightforward use of skynet produces networks that perform the desired task quickly and accurately , and typically achieve results that are competitive with machine learning approaches that have been tailored to the required task .",
    "future development of skynet will expand upon many of the current features and introduce new ones .",
    "we are working to include more activation functions ( e.g. @xmath15 , softsign , and rectified linear ) , pooling of nodes , convolutional nns , diversity in outputs ( i.e. mixing regression and classification ) , and more robust support of recursive nns .",
    "this is all in addition to further improving the speed and efficiency of the training algorithm itself .",
    "however , skynet in its current state is already a useful tool for performing machine learning in astronomy .",
    "the authors thank john skilling for providing very useful advice in the early stages of algorithm development .",
    "we also thank amy lien for providing the data used in seciton  [ sec : grb ] .",
    "this work utilized three different high - performance computing facilities at different times : initial work was performed on cosmos viii , an sgi altix uv1000 supercomputer , funded by sgi / intel , hefce and pparc , and the authors thank andrey kaliazin for assistance ; early work also utilized the darwin supercomputer of the university of cambridge high performance computing service ( ` http://www.hpc.cam.ac.uk/ ` ) , provided by dell inc . using strategic research infrastructure funding from the higher education funding council for england",
    "; later work utilised the discover system of the nasa center for climate simulation at nasa goddard space flight center .",
    "pg is currently supported by a nasa postdoctoral fellowship from the oak ridge associated universities and completed a portion of this work while funded by a gates cambridge scholarship at the university of cambridge .",
    "ff is supported by a research fellowship from the leverhulme and newton trusts .    1 andreon s. , gargiulo g. , longo g. , tagliaferri r. , & capuano n. , 1999 , arxiv : astro - ph/9906099 andreon s. , gargiulo g. , longo g. , tagliaferri r. , & capuano n. , 2000 , mnras , 319 , 700716 auld t. , bridges m. , hobson m.p .",
    ", gull s.f . , 2008 , mnras , 376 , l11 auld t. , bridges m. , hobson m.p . , 2008 , mnras , 387 , 1575 ball n.m .",
    ", brunner r.j . , 2010 , int",
    ".  j.  mod .",
    ", 19 , 1049 bergstra j. , desjardins g. , lamblin p. , & bengio y. , 2009 , technical report 1337 , dpartement dinformatique et de recherche oprationnelle , universit de montral .",
    "bertin e. , arnouts s. , 1996 , a&as supplement , 117 , 393 bridges m. , cranmer k. , feroz f. , hobson m.p . , ruiz de austri r. , trotta r. , 2011 , jhep , 03 , 012 bonnett c. , 2013 , arxiv:1312.1287 [ astro-ph.co ] carreira - perpignan m. a. & hinton . g. e. , 2005 , proceedings of the tenth international workshop on artificial intelligence and statistics , eds . cowell r. g. & ghahramani z. , 3340 ciresan d. c. , meier u. , gambardella l. m. , & schmidhuber j. , 2010 , neural comput . , 22 , 32073220 cybenko g. , 1989 , mathematics of control , signals , and systems , 2 , 303314 erhan d. et al . , 2010 ,",
    "journal of machine learning research , 11 , 625660 fawcett t. , 2006 , pattern recogn .",
    "lett . , 27 , 861 fendt w.a . ,",
    "wandelt b.d . , 2007 , apj , 654 , 2 feroz f. , hobson m.p . , 2008 , mnras , 384 , 449 feroz f. , hobson m.p . ,",
    "bridges m. , 2009 , mnras , 398 , 1601 feroz f. , hobson m. p. , cameron e. , & pettitt a. n. , 2013 , arxiv:1306.2144 [ astro-ph.im ] feroz f. , marshall p. j. , hobson m. p. , 2008 ,",
    "[ astro - ph ] fynbo j. et al . , 2009 ,",
    "apjs , 185 , 526 gehrels n. et al . , 2004 ,",
    "apj , 611 , 1005 geva s. , sitte j. , ieee , 3 , 621 glorot x. & bengio y , 2010 , proceedings of the thirteenth international conference on artificial intelligence and statistics , journal of machine learning research , eds .",
    "teh y. w. & titterington m. , 249256 glorot x. , bordes a. , & bengio y. , 2011 , proceedings of the fourteenth international conference on artificial intelligence and statistics , journal of machine learning research , eds .",
    "gordon g. & dunson d. , 315323 graff p. , feroz f. , hobson m.p .",
    ", lasenby a.n . , 2012 , mnras , 421 , 169 gull s.f .",
    "& skilling j. , 1999 , quantified maximum entropy : memsys 5 users manual .",
    "maximum entropy data consultants ltd .",
    "edmunds , suffolk , uk . ` http://www.maxent.co.uk/ ` hinton g.e . ,",
    "osindero s. , & teh y .- w . , 2006 , neural comput . , 18 , 15271554 hinton g.e . &",
    "salakhutdinov r.r . , 2006 ,",
    "science , 313 , 504 - 507 hobson m. p. , jones a. w. , lasenby a. n. , & bouchet f. r. , 1998 , mnras , 300 , 129 hornik k. , stinchcombe m. & white h. , 1990 , neural networks , 3 , 359 hyvrinen a. , oja e. , 2000 , neural networks , 13 , 411 karpenka n.v . , feroz f. , hobson m.p . , 2013 , mnras , 429 , 12781285 kendall m.g .",
    ", 1957 , a course in multivariate analysis .",
    "griffin , london kitching t. et al .",
    ", 2011 , annals of applied statistics , 5 , 22312263 kitching t. et al . , 2012 , new astronomy reviews , submitted lecun y. , bottou l. , bengio y. , & haffner p. , 1998",
    ", proc . of the ieee , 86 , 22782324 lien a. , sakamoto t. , gehrels n. , palmer d. , graziani c. , 2012 , proceedings of the international astronomical union , 279 , 347 longo g. , tagliaferri r. , & andreon s. , 2001 , mining the sky : proceedings of the mpa / eso / mpe workshop , eds .",
    "banday a. j. , zaroubi s. , bartelmann m. , 379385 mackay d.j.c .",
    ", 1992 , neural computation , 4 , 415447 mackay d.j.c . , 1995 ,",
    "network : computation in neural systems , 6 , 469 mackay d.j.c , 2003 , information theory , inference , and learning algorithms .",
    "cambridge univ . press . ` www.inference.phy.cam.ac.uk/mackay/itila/ ` mandic d. , chambers j. , 2001 , recurrent neural networks for prediction : learning algorithms , architectures and stability .",
    "wiley , new york .",
    "martens j. , 2010 , in frnkranz j. , joachims t. , eds , proc .",
    "machine learning .",
    "omnipress , haifa , p. 735 murtagh f. , 1991 , neural comput . , 2 , 183 pascanu r. & bengio y. , 2013 , arxiv:1301.3584 [ cs.lg ] pearlmutter b.a . , 1994 , neural comput . , 6 , 147 sanger t.d . ,",
    "1989 , neural networks , 2 , 459 schraudolph n.n . , 2002 , neural comput .",
    ", 14 , 1723 serra - ricart m. , calbet x. , garrido l. , & gaitan v. , 1993 , aj , 106 , 1685 skilling j. , 2004 , aip conference series , 735 , 395 tagliaferri r. et al . , 2003a , neural networks , 16 , 297 tagliaferri r. , longo g. , andreon s. , capozziello s. , donalek c. , & giordano g. , 2003b , neural nets : 14th italian workshop on neural nets , eds .",
    "apolloni b. , marinaro m. , & tagliaferri r. , 226234 wanderman d. , piran t. , 2010 , mnras , 406 , 1944 way m.j .",
    ", scargle j.d . , ali k.m . ,",
    "srivastava a.n . , 2012 ,",
    "advances in machine learning and data mining for astronomy .",
    "crc press ."
  ],
  "abstract_text": [
    "<S> we present the first public release of our generic neural network training algorithm , called skynet . </S>",
    "<S> this efficient and robust machine learning tool is able to train large and deep feed - forward neural networks , including autoencoders , for use in a wide range of supervised and unsupervised learning applications , such as regression , classification , density estimation , clustering and dimensionality reduction . </S>",
    "<S> skynet uses a ` pre - training ' method to obtain a set of network parameters that has empirically been shown to be close to a good solution , followed by further optimisation using a regularised variant of newton s method , where the level of regularisation is determined and adjusted automatically ; the latter uses second - order derivative information to improve convergence , but without the need to evaluate or store the full hessian matrix , by using a fast approximate method to calculate hessian - vector products . </S>",
    "<S> this combination of methods allows for the training of complicated networks that are difficult to optimise using standard backpropagation techniques . </S>",
    "<S> skynet employs convergence criteria that naturally prevent overfitting , and also includes a fast algorithm for estimating the accuracy of network outputs . </S>",
    "<S> the utility and flexibility of skynet are demonstrated by application to a number of toy problems , and to astronomical problems focusing on the recovery of structure from blurred and noisy images , the identification of gamma - ray bursters , and the compression and denoising of galaxy images . </S>",
    "<S> the skynet software , which is implemented in standard ansi c and fully parallelised using mpi , is available at http://www.mrao.cam.ac.uk / software / skynet/.    [ firstpage ]    methods : data analysis  methods : statistical </S>"
  ]
}