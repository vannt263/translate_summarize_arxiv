{
  "article_text": [
    "diffusion tensor imaging ( dti ) is a powerful tool to detect , in vivo , the white matter anatomy and structures of the brain .",
    "the raw mr - data are collected by a magnetic resonance scanner and consist of spectral measurement from the displacement distribution of water molecules constrained into cellular structures .",
    "diffusion anisotropy characterizes the nervous fibers .",
    "after the fourier inversion , the mr - signals are corrupted by a complex gaussian noise , and consequently , the recorded measurement magnitudes , referred as diffusion weighted magnetic resonance imaging ( dw - mri ) data , will follow the rician distribution . the noise distribution , however , will still stay gaussian in both real and imaginary components .",
    "the simplest method for diffusion tensor estimation ( dte ) is based on the linearized log - normal regression model , where the residual variance is assumed to be either constant ( least squares ) or depending on the signal amplitude ( weighted least squares ) .",
    "these gaussian noise models fail to fit the high frequency data , which carry information about the higher order diffusion characteristics . in the existing literature @xcite on the ml - estimation of diffusion tensors under the rician noise ,",
    "the maximization algorithm involves repeated computation of modified bessel functions . by using data augmentation we are able to replace the rician likelihood by a poisson likelihood which is standard in the framework of glm .",
    "such simplification reduces dramatically the computational burden of the fisher - scoring maximization algorithm .",
    "this applies also at high @xmath0-amplitudes , where in the low signal regime measurements below a threshold are customarily coded as zeros . in the standard ls or wls approaches ,",
    "zero - measurements are problematic since they can not be fitted by a log - normal distribution , and simply discarding them induces selection bias .",
    "the appropriately modeled noise level provides capability of data correction in further insights , e.g. removing artefacts from the raw data .",
    "this paper is structured as follows .",
    "section [ sec : theo ] describes data augmentation and specifies the statistical model for dte . in section [ sec : method ] we discuss the implementation of the em and the fisher - scoring algorithms in the dti context .",
    "in addition , we also specify priors for the parameters and discuss the computation of the maximum a posteriori estimator ( mape ) under the same scheme .",
    "section [ sec : res ] illustrates the results from both synthetic and real data . in section [ sec : dis ] we conclude with an overview of the methods and the undergoing developments .",
    "in magnetic resonance imaging ( mri ) , we usually need to take the noise in the raw mr - acquisitions into account .",
    "the complex valued noise @xmath2 is composed of two @xmath3 gaussian random variables with zero mean and variance @xmath4 , one for the real and the other one for the imaginary component .",
    "after the fourier inversion , the signal intensity @xmath5 is corrupted by the the complex gaussian noise , and @xmath6 will be observed .",
    "consequently , the observed mr - signal magnitudes follow a rician distribution resulting in the likelihood function @xmath7 where @xmath8 is the @xmath9-order modified bessel function of first kind . for @xmath10",
    "it has also the following representation in terms of gaussian hypergeometric series @xcite : @xmath11 let @xmath12 , then eq . ( [ ( 1 ) ] )",
    "gives @xmath13 with @xmath14 .",
    "we follow the strategy presented in @xcite introducing an augmented data @xmath15 from a poisson distribution with mean @xmath16 .",
    "the likelihood of the observed data can be transformed from the rician likelihood eq .",
    "( [ ( 2 ) ] ) to a joint augmented density @xmath17 where @xmath18 is from the conditional distribution gamma(@xmath19 ) given @xmath15 .",
    "( [ eq : joint ] ) provides a transformation from a non - linear regression problem to the glm framework @xmath20 with @xmath21 corresponding to the response in general , see @xcite for more details .",
    "in dw - mri , the signal is modeled as the first equality @xmath22 where the control vector @xmath23 is determined by the sequence of gradient pulses , @xmath24 , and @xmath25 is a vector of unit length .",
    "the mr - signal decays exponentially with respect to the @xmath0-amplitude . depending on the gradient direction @xmath26 the decay",
    "is modeled by the reflection symmetric diffusivity function @xmath27 .",
    "great efforts have been devoted to modeling the diffusivity , and in general we can have parametrization as the second equality . in the simplest model",
    "the diffusivity is expressed by a symmetric and positive definite rank-2 tensor @xmath28 , giving @xmath29 where in the left hand side the diffusion tensor is parametrized as @xmath30 with a design matrix @xmath31 in high angular resolution models ( hardi ) ( see e.g. @xcite ) , the diffusivity is modeled with a totally symmetric cartesian tensor @xmath32 of order @xmath33 , as @xmath34      in the optimization of the likelihood , we employ the em ( expectation - maximization ) algorithm , which is one among the iterative methods in the mle or in the maximum a posterior estimation ( mape ) .",
    "the em algorithm proceeds in two steps and shortens the computational complexity by using augmented data . in terms of our case ,",
    "in the e - step we calculate the expectation of the log likelihood w.r.t the conditional distribution of @xmath15 given by the observations and other parameters with fixed values . in the m - step ,",
    "we find the ml parameter of @xmath35 and @xmath4 by maximizing the augmented log likelihood quantities .",
    "the computational details are listed in appendix [ ape : a ] .    the log likelihood from eq .",
    "( [ eq : joint ] ) is expressed as @xmath36 where @xmath37 does not depend on @xmath38 which will be omitted in the m - step . from section [ subsec :",
    "para ] , we have @xmath39 .    in the em - iteration , given the current parameter estimates @xmath40 , we update the conditional expectation of the augmented data by @xmath41 in the m - step we update @xmath4 and @xmath35 by the recursions @xmath42 and @xmath43 where @xmath44 is the number of acquisitions at each voxel . for the tensor parameter @xmath45 , we employ a stabilized fisher scoring method : given the stabilizing parameter @xmath46 $ ] , we iterate the recursion @xmath47 until convergence to a fixed point @xcite . in eq .",
    "the score @xmath48 is given by @xmath49 and the corresponding fisher information is @xmath50    the initials of the em algorithm can be obtained through the least square ( ls ) from a truncated dataset with the diffusion weighting ranging from @xmath51 in order to fit the gaussian model ( see @xcite , @xcite ) . to pursue higher quality of the initials",
    ", we could further apply the weighted least square ( wls ) described in @xcite . in the appendix [ ape : b ]",
    "we compare the differences between our em algorithm and the direct optimization of the rician likelihood in eq .",
    ", which is commonly used to compute the mle in dti .",
    "it should be noted that the em algorithm is needed because of the latent augmented variables ; it does not decrease the marginal likelihood of the data , see appendix [ ape : aa ] for the proof .      in the bayesian framework",
    ", the maximum a posterior estimation ( mape ) aims to obtain the point estimates by maximizing the posterior density .",
    "the difference between mle and mape in this scenario is in the prior probability @xmath52 . given the data @xmath53 , the normalizing constant in the posterior density @xmath54 does not depend on the parameter @xmath55 .",
    "we find the mape by maximizing the joint density @xmath56 , and this is achieved by iterating the em - recursion with the penalization @xmath57 @xmath58 until convergence to a fixed point .",
    "the log - prior penalization term has a regularizing effect , which vanishes asymptotically as the sample size grows @xcite .    in dte",
    ", we can assign conjugate priors in light of section [ subsec : mle ] for @xmath4 and @xmath35 .",
    "since we have little knowledge of the tensor parameter @xmath45 , we may choose non - informative priors which are either scale- or shift - invariant @xcite .",
    "a simple bayesian hierarchical model is obtained after the following choices :    * @xmath4 has scale invariant improper prior with density @xmath59 , * @xmath60 , where @xmath61 are very small .",
    "* @xmath62 has the isotropic centered gaussian prior @xmath63 , where @xmath64 is a @xmath65 precision matrix .",
    "the penalized em - updates for mape are given by @xmath66 and @xmath67 additionally , this gives the modified score and fisher scoring @xmath68    under our bayesian model with weak priors the map estimation eq .",
    ". are similar as the ml updates eq .",
    "indeed , usually @xmath69 , and we can omit the difference between eq.([eq : sigma ] ) and eq.([eq : sigmamap ] ) .",
    "then when @xmath70 and @xmath71 are small enough , the difference between the likelihood and posterior mode of @xmath72 , expressed in eq . and",
    "eq . respectively , can also be ignored .",
    "the only difference when updating @xmath45 , is that we have considered the correction between the elements of a tensor represented by the prior distribution , the inverse covariance matrix , @xmath64",
    ". such correction may be ignorable .",
    "remark : sometimes the mle can be treated as a special case of the mape where the precision of the parameters depend on the chosen prior",
    ". if the effects of the priors are weak enough to be ignored , then the posterior distribution is asymptotically approximated by the likelihood .",
    "the consequence is that numerically the map tend to the ml estimates numerically .",
    "such remark is not unusual ( see @xcite ) but nearly has never appeared in the dti literature .",
    "synthetic data sets were simulated by choosing a positive tensor of 2nd - order and of 4th - order with fixed @xmath72 and the noise variance @xmath4 .",
    "the simulated data sets in the experiments arise from models with parameter values resembling the real scenario .",
    "every dataset contains 1440 measurements which were sampled from 32 distinct gradients and 15 distinct increasing @xmath0 values ( knots ) up to 14000@xmath1 , each repeated three times .",
    "the ground truth ( gt ) of high ( h- ) and low ( l- ) rician noise , @xmath4 , are 93,0405 and 12,8821 , respectively . to compare the performance , we first plot the ml estimated rician signals and the gt shown in fig .",
    "[ fig : sfit ] .",
    "fig.[fig : snrwh ] shows the empirical signal to noise ratio ( snr ) of the distinct @xmath0 values from the first 480 measurements as an illustration .",
    "for comparison of the methods , we simulate 100 datasets from high noise case under the 4th - order tensor and compare the first 480 measurements of the sample means of snr and the gt under different methods in fig.[fig : snr ] , where `` @xmath73 '' denotes that only the low frequencies ( @xmath0 values less than 1000@xmath1 ) are considered in the estimation .",
    "this figure reveals that our mle and the wls under a truncated dataset fit the gt well .",
    "however , when comparing the mean square errors ( mse ) of the noise variance , the wls * has a huge bias , 54.777 , compared with the mse of the ml estimates , 10.358 .",
    "the fitted rician signals are depicted in fig .",
    "[ fig : s ] , where the estimated signals are retrieved from a low b and a high b -value cases estimated from the first 480 measurements .",
    "it reveals that in the low b - value case both the wls * and the mle perform well .",
    "but our ml estimates show advantages in the high b - value case .",
    "the reason is that for the wls * we use data information which do not consider the high frequencies , but only fit back to fetch reliable estimates of signals .",
    "we further check the mse on tensor coefficients from the 15 distinct @xmath0 values .",
    "the results are described in fig.[fig : mse ] , where we can conclude that the mle is the best one compared with other methods .",
    "the average computational time of the aforementioned mle method under the 4th - order tensor model is 0.4868 seconds , which is extremely shorter than the minutes running time per voexl from the current standard methods such as matlab nelder - mead based or gradient - based estimators ( see @xcite ) .",
    "the gt of the upper curve is from the high snr corresponding the lower noise level with @xmath74 .",
    "the bottom one has the high noise level with @xmath75 .",
    "the red curves are fitted snr under the 2nd - order tensor model . while the green stars represent the empirical snr under the 4th - order tensor model.,width=321 ]         values .",
    "the plots illustrate the means of the signal intensities at @xmath76 , respectively , estimated by by different methods.,width=245 ]    \\(a ) low @xmath0 value , @xmath77     values .",
    "the plots illustrate the means of the signal intensities at @xmath76 , respectively , estimated by by different methods.,width=234 ]    \\(b ) high @xmath0 value , @xmath78          the data consist of @xmath79 diffusion mr - images of the brain of an healthy human volunteer , taken from four @xmath80-thick consecutive axial slices , and measured using a philips achieva @xmath81 tesla mr - scanner .",
    "the image resolution is @xmath82 pixels of size @xmath83 @xmath84 . after masking out the skull and the ventricles , we remain with a region of interest ( roi ) containing @xmath85 voxels . in the protocol",
    ", we used all the combinations of the @xmath86 gradient directions with the @xmath0-values varying in the range @xmath87 , with @xmath88 repetitions , for a total of 23323644 data points .",
    "the average computational cost per voxel by our method the 4th - order tensor model from this dataset is 1.8331 seconds .",
    "we illustrate the results mainly under the 4th - order tensor model .",
    "fig.[fig : famd ] shows the mean diffusivity ( md ) and the fractional anisotropy ( fa ) of diffusion from two consecutive slices , where fa is computed from the results under 2nd - order tensor model , which is given by @xmath89)^2+(\\lambda_2- e[\\lambda])^2+(\\lambda_3- e[\\lambda])^2 ) } } { \\sqrt{2 ( \\lambda_1 ^ 2+\\lambda_2 ^ 2+\\lambda_3 ^ 2 ) } } .\\end{aligned}\\ ] ] the average values of fa from these two roi are @xmath90 and @xmath91 , respectively .",
    "the color in fa represents the orientations of the fibers . under the 4th - order tensor model , md",
    "is expressed as @xmath92 the average values of md from slice 3 and 4 are 6.248e-03 @xmath93 , 6.045e-03 @xmath93 , respectively , and we have the same estimated values of md under 2nd - order tensor model .",
    "-value.,width=340 ]    \\(a ) mean diffusivity ( md )    -value.,width=302 ]    \\(b ) fractional anisotropy ( fa )    we also plot the rician noise map of @xmath94 from the two consecutive slices shown in fig .",
    "[ fig : rnoise ] , where the artefacts are clearly depicted by white color representing very high noise , which reveals the true scenario from the raw mr images .",
    "visualization of angular resolution of dti data under different tensor models from the region of interest ( roi ) of two consecutive slices are displayed in fig .",
    "[ fig : tensor ] , where the roi is near the hippocampus and the empty spaces inside of left parts of the diffusion profiles ( dp ) are the masked ventricle .",
    "dp depict under the 4th - order tensors providing much angular information of diffusion , where the colors represents the principle orientations of diffusion at each voxel .",
    "these tensor profiles are plotted by matlab fandtasia toolbox @xcite .",
    "our method substantially differs from the previous ones in the literature and the advantages are summarized by the following points : 1 ) we introduce a novel data augmentation , which allows the non - linear regression problem to be transformed into the glm framework in dte .",
    "2 ) subsequently , the computation is dramatically reduced due to the tractable modes of parameters of interest in the sense of point estimation .",
    "in addition , when employing fisher - scoring scheme we simplify the complexity of the fisher information .",
    "3 ) our rician noise model can be combined with any tensor model in different representation , such as spheric harmonic expansion , by reparametrization .",
    "4 ) either ml or map estimation yields more accurate estimates than the ls and wls do .",
    "in addition , high frequencies from the low snr data and the zero measurements are also included into the estimation .",
    "these data are known to contain detailed anatomical information of the complex tissue in vivo .",
    "5 ) our method leads to significantly less biased estimates of the noise level , which plays key role in denoising the mri and cleaning the artefacts .",
    "_ positive constraints . _",
    "the physical feature of diffusion requires the tensor to be positive definite .",
    "our model allows to check the positivity of diffusivity in the tensor updates under the scheme of fisher - scoring method . for the rank-2 tensor model ,",
    "the constraining is fairly easy to do by computing the eigenvalues of the tensor matrix @xmath32 . for hardi , barmpoutis et al .",
    "@xcite propose the gram matrix approach , using the quartic form to guarantee the positivity .",
    "other methods such as @xcite address the constraint by calculating the z - eigenvalue polynomials .",
    "_ mle vs mape_. in this work , we did not list the results from mape but we emphasize the differences between these two methods .",
    "bayesian methods have advantages in the learning process , meaning that they may gain extra information from the prior knowledge .",
    "when the prior is weak , like in our case , we learn things from the data , what we actually do when approaching the problem through frequentist statistical modeling . in order to learn the uncertainty of the diffusion parameters , a fully bayesian approach",
    "is highly recommended to characterize the posterior parameter distributions rather than point estimation .",
    "we thank professor antti penttinen for reviewing the manuscript and providing insightful comments .",
    "we would also like to thank the radiology unit of helsinki university hospital for the data collection .",
    "this work was funded by doctoral program in computing and mathematical sciences ( comas ) , university of jyv\"skyl .",
    "we acknowledge the finnish doctoral programme in stochastics and statistics ( fdpss ) provided travel funds for this research .",
    "consider a statistical model @xmath95 , where @xmath96 , and the likelihood of the observed data @xmath97 is expressed as the marginal of an integrated joint likelihood @xmath98 here @xmath99 and @xmath100 are interpreted as latent variables .",
    "when @xmath101 is discrete , we replace integrals by sums . in the em algorithm @xcite , starting with an inital value @xmath102 , we iterate the maximization step @xmath103 where the integration is with respect to the conditional density @xmath104 by jensen inequality , the kullback relative entropy of the conditional distribution @xmath105 related to @xmath106 , given by @xmath107 is non - negative , which implies @xmath108 and consequently @xmath109 i.e. the em - step does not decrease the marginal likelihood of @xmath53 .",
    "it follows also from , that fixing a @xmath45-subvector and maximizing with respect to the remaining @xmath45-coordinates does not decrease the marginal likelihood of @xmath53 .",
    "the em algorithm is iterated until convergence to a fixed point @xmath110 , a local maximum of the marginal likelihood @xmath111 .",
    "when the local maximum is the global one , @xmath112 is the maximum likelihood estimator of the parameter .",
    "the advantage of the em algorithm is that , for some smart choices of the data augmentation @xmath21 and the joint density @xmath113 , the maximization step can be simpler than maximizing directly the marginal likelihood @xmath111 , especially in cases where the latter is hard to evaluate .",
    "appendix [ ape : a ] gives details of the expectation - maximization ( em ) algorithm in dte .",
    "we consider the rician noise model with the poissonian data augmentation of section [ sec : theo ] .",
    "the latent augmented variable @xmath15 conditionally on @xmath114 is given by @xmath115 it follows @xcite that this discrete distribution is referred as reinforced poisson distribution with parameter @xmath116 .    in the em algorithm we need to compute the conditional expectation of @xmath15 conditionally on @xmath18 and the design matrix @xmath117 . given the current values @xmath118",
    "@xmath119 with @xmath120 note that @xmath121 , where @xmath122 is the zero - order bessel function of the first kind , @xmath123 is the zero - order modified bessel function of first kind , which satisfies @xmath124 and @xmath125    in the m - step , we maximize the parameters of the augmented log likelihood @xmath126 from eq .",
    "w.r.t @xmath127 . omitting the items not depending on these parameters ,",
    "@xmath126 can be expressed as @xmath128 it is easy to see in eq . that the log likelihood w.r.t @xmath4 and @xmath129 are inverse gamma and gamma distributions , respectively",
    ". hence , we update these two parameters by their modes : @xmath130 and @xmath131    to apply the fisher scoring method , we have the score of @xmath45 is @xmath132 and the fisher - information is given by @xmath133 =   \\frac{\\widehat{s_0 ^ 2}_{ml}}{\\widehat{\\sigma^2}_{ml } } \\sum_{i=1}^m \\exp ( 2 z_i \\theta ) z_i z_i^t.\\end{aligned}\\ ] ]      without data agumentation , we have to directly maximize the rician log likelihood @xmath134 , in short @xmath135 thereafter , by using some typical mle method , such as gradient descent",
    ". then the first ( the score ) and second derivatives of @xmath135 are usually required .",
    "the loglikelihood @xmath135 is @xmath136 where @xmath137 are modified bessel functions of first kind satisfying @xmath138    the score of @xmath4 and @xmath35 are respectively given by @xmath139 and @xmath140 the score of @xmath45 is given by @xmath141 the hessian of @xmath45 is given by @xmath142 where we denote @xmath143 for snr @xmath144 , the corresponding fisher - information matrix is approximated by@xmath145   \\approx \\sum\\limits_{i=1}^m z_{ih } z_{ik } \\biggl ( \\frac{s_0 ^ 2}{\\sigma^2 } \\exp(2 z_i \\theta ) - \\frac 1 2 \\biggr),\\end{aligned}\\ ] ] where ( see@xcite ) @xmath146",
    "^ 2}{\\sigma^2 } \\exp(2 z_i \\theta ) \\biggr)^2   + \\frac{s_0 ^ 2}{\\sigma^2 } \\exp(2 z_i \\theta ) - \\frac 1 2 .\\end{aligned}\\ ] ]        1 .   we do not need to calculate all the elements of the hessian as we can directly find the modes of @xmath35 and @xmath4 by data augmentation .",
    "a small improvement appears in the reparametrization of @xmath72 or @xmath147 by @xmath35 .",
    "2 .   in the e - step",
    "we compute @xmath148 which does not depend on the parameters @xmath149 and @xmath35 . in the m - step we use eq .",
    "[ expections ] , the recursive values from @xmath150 , instead of solving the intractable formula w.r.t those parameters .",
    "that dramatically reduces the computation of the score from eq.([score_s0r],[score_sigmar],[score_thetar ] ) to eq.([score_s02],[score_sigma],[score_theta ] ) , respectively .",
    "3 .   the em algorithm allows us to use empirical values from eq . to compute the fisher information .",
    "our fisher information @xmath151 which fits the whole range of snr and is slightly bigger than the approximated one , @xmath152 , expressed in ( eq . ) , which requires heavy mathematical calculations to deal with different expectations ( see @xcite for more details ) .",
    "in addition , when computing the score of @xmath45 in eq .",
    "[ eq : theta ] , we do not need to update the items containing @xmath153 as they are fixed values from eq .. all those lead reduced computation in practice .",
    "barmpoutis , a. and vemuri , b.c.and shepherd , t. m. and forder , j.r . , 2007 .",
    "tensor splines for interpolation and approximation of dt - mri with applications to segmentation of isolated rat hippocampi .",
    "_ medical imaging , ieee transactions on _ , 26(11 ) , 1537 - 1546 .",
    "barber , p.a .",
    "and darby , d.g . and desmond , p.m. and yang , q. and gerraty , r.p . and jolley , d. and donnan , g.a . and tress , b.m .and davis , s.m . , 1998 barmpoutis , a. and hwang , m.s . and howland , d. and forder , j.r . and vemuri , b.c . , 2009 .",
    "prediction of stroke outcome with echoplanar perfusion - and diffusion - weighted mri .",
    "_ aan enterprises _ , 52(2 ) , 418426 .",
    "leemans , a. , jeurissen , b. , sijbers , j. , jones , d.k . , 2009 .",
    "exploredti : a graphical toolbox for processing , analyzing , and visualizing diffusion mr data .",
    "intl soc . mag .",
    "reson . med _",
    "3537 hawaii , usa .        and jeurissen , b. and verhoye , m. and van a. j. and sijbers , j. , 2011 .",
    "maximum likelihood estimation - based denoising of magnetic resonance images using restricted local neighborhoods . _ physics in medicine and biology _",
    ", 56(16 ) , 5221    sparacino , g. and tombolato , c. and cobelli , c. , 2000 .",
    "maximum - likelihood versus maximum a posteriori parameter estimation of physiological system models : the c - peptide impulse response case study .",
    "_ biomedical engineering , ieee transactions on _ , 47(6 ) , 801811 ."
  ],
  "abstract_text": [
    "<S> diffusion tensor imaging ( dti ) is widely used to characterize , in vivo , the white matter of the central nerve system ( cns ) . </S>",
    "<S> this biological tissue contains much anatomic , structural and orientational information of fibers in human brain . </S>",
    "<S> spectral data from the displacement distribution of water molecules located in the brain tissue are collected by a magnetic resonance scanner and acquired in the fourier domain . </S>",
    "<S> after the fourier inversion , the noise distribution is gaussian in both real and imaginary parts and , as a consequence , the recorded magnitude data are corrupted by rician noise </S>",
    "<S> .    statistical estimation of diffusion leads a non - linear regression problem . in this paper </S>",
    "<S> , we present a fast computational method for maximum likelihood estimation ( mle ) of diffusivities under the rician noise model , based on the expectation maximization ( em ) algorithm . by using data augmentation </S>",
    "<S> , we are able to transform a non - linear regression problem into the generalized linear modeling ( glm ) framework , reducing dramatically the computational cost . the fisher - scoring method is used for achieving fast convergence of the tensor parameter . the new method is implemented and applied using both synthetic and real data in a wide range of @xmath0-amplitudes up to 14000 @xmath1 . higher accuracy and precision of the rician estimates </S>",
    "<S> are achieved compared with other log - normal based methods . </S>",
    "<S> in addition , we extend the ml framework to the maximum a posterior ( map ) estimation in dti under the aforementioned scheme by specifying the priors . we will describe how close numerically are the estimators of model parameters obtained through ml and map estimation .    </S>",
    "<S> [ [ keywords ] ] keywords + + + + + + + +    data augmentation , fisher scoring , maximum likelihood estimator , maximum a posterior estimator , rician likelihood , reduced computation </S>"
  ]
}