{
  "article_text": [
    "in this work we consider time - independent schrdinger equation to calculate vibrational spectra of molecules . the goal is to find smallest eigenvalues and corresponding eigenfunctions of the hamiltonian operator .",
    "the key assumption that we use is that potential energy surface ( pes ) can be approximated by a small number of sum - of - product functions .",
    "this holds , e.g. if pes is a polynomial .",
    "we discretize the problem using the discrete variable representation ( dvr ) scheme @xcite .",
    "the problem is that the storage required for each eigenvector grows exponentially with dimension @xmath2 as @xmath3 , where @xmath4 is number of grid points in each dimension . even for the dvr scheme where @xmath5 grid points in each dimension is often sufficient to provide very accurate results we would get @xmath6  pb of storage for a 12 dimensional problem .",
    "this issue is often referred to as the _ curse of dimensionality_. to avoid exponential growth of storage we use _ tensor train _ ( tt ) decomposition @xcite to approximate the operator and the eigenfunctions in the dvr representation , which is known to have exponential convergence rate .",
    "it is important to note that the tt - format is algebraically equivalent to the matrix product state format ( mps ) , which has been used for a long time in quantum information theory and solid state physics to approximate certain wavefunctions @xcite ( dmrg method ) , see the review @xcite for more details .",
    "prior research @xcite has shown that the eigenfunctions often can be well approximated in the tt - format , i.e. they lie on a certain low - dimensional non - linear manifold .",
    "the key question is how to utilize this a priori knowledge in computations .",
    "we propose to use well - established iterative methods that utilize matrix inversion and solve corresponding linear systems inexactly along the manifold to accelerate the convergence .",
    "our main contributions  are :    * we propose a concept of a _ manifold preconditioner _ that explicitly utilizes information about the size of the tt - representation .",
    "we use the manifold preconditioner for a tensor version of _ locally optimal block preconditioned conjugate gradient _ method ( lobpcg ) @xcite .",
    "we will refer to this approach as _ manifold - preconditioned lobpcg _",
    "( mp lobpcg ) .",
    "the approach is first illustrated on computation of a single eigenvector ( section [ sec : mp1 ] ) and then extended to the block case ( section [ sec : mpb ] ) .",
    "* we propose tensor version of simultaneous inverse iteration ( also known as block inverse power method ) , which significantly improves accuracy of the proposed mp lobpcg .",
    "similarly to the manifold preconditioner the inversion is done using the a priori information that the solution belongs to a certain low - parametric manifold .",
    "we will refer to this method as _ manifold - projected simultaneous inverse iteration _",
    "( mp sii ) .",
    "the approach is first illustrated on computation of a single eigenvector ( section [ sec : iii1 ] ) and then extended to the block case ( section [ sec : iib ] ) .",
    "* we calculate vibrational spectra of acetonitrile molecule ch@xmath0cn using the proposed approach ( section [ sec:12 ] ) .",
    "the results are more accurate than those of the smolyak grid approach @xcite but with much less storage requirements , and more accurate than the recent memory - efficient h - rrbpm method @xcite , which is also based on tensor decompositions .",
    "we note that the smolyak grid approach does not require pes to be approximated by a small number of sum - of - product functions .",
    "we follow @xcite and consider schrdinger equation with omitted @xmath7 cross terms and the potential - like term in the normal coordinate kinetic energy operator .",
    "the hamiltonian in this case looks as @xmath8 where @xmath9 denotes potential energy surface ( pes ) .",
    "we discretize the problem using the discrete variable representation ( dvr ) scheme on the tensor product of hermite meshes @xcite such that each unknown eigenfunction is represented as @xmath10 where @xmath11 denotes one - dimensional dvr basis function .",
    "we call arising multidimensional arrays _ tensors_. the hamiltonian consists of two terms : the laplacian - like part and the pes .",
    "it is well - known that the laplacian - like term can be written in the kronecker product form @xmath12 where @xmath13 is the one dimensional discretization of the @xmath14-th mode .",
    "the dvr discretization of the pes is represented as a tensor @xmath15 .",
    "the operator corresponding to the multiplication by @xmath15 is diagonal .",
    "finally the hamiltonian is written as @xmath16    for our purposes it is convenient to treat  @xmath17 not as a 2d @xmath18 matrix , but as a multidimensional operator @xmath19 . in this case",
    "the discretized schrdinger equation has the form @xmath20 hereinafter we use notation @xmath21 implying matrix - by - vector product from . using this notation can be equivalently written as @xmath22",
    "in this section we discuss how to solve the schrdinger equation numerically and present our approach for finding a single eigenvector .",
    "the case of multiple eigenvalues is discussed in section [ sec : block ] .    the standard way to find required",
    "number of smallest eigenvalues is to use iterative methods .",
    "the simplest iterative method of finding one smallest eigenvalue is the shifted power iteration @xmath23 where the shift @xmath24 is an approximation to the largest eigenvalue of  @xmath25 .",
    "the matrix - by - vector product is the bottleneck operation in this procedure .",
    "this method was successfully applied to the calculation of vibrational spectra in @xcite .",
    "the eigenvectors in this work are represented as sum - of - products , which allows for fast matrix - by - vector multiplications . despite the ease of implementation and efficiency of each iteration ,",
    "the convergence of this method requires thousands of iterations .    instead of power iteration",
    "we use inverse iteration  @xcite @xmath26 which is known to have much faster convergence if a good approximation @xmath24 to the required eigenvalue @xmath27 is known .",
    "question related the solution of linear systems in the tt - format will be discussed in section [ sec : iii1 ] .",
    "convergence of the inverse iteration is determined by ratio @xmath28 where @xmath29 is the next closest to @xmath27 eigenvalue .",
    "thus , @xmath24 has to be closer to @xmath27 than to @xmath29 for the method to converge .",
    "however , the closer @xmath24 to @xmath27 is , the more difficult to solve the linear system with matrix @xmath30 it is .",
    "therefore , typically this system is solved inexactly  @xcite .",
    "parameter @xmath24 can also depend on the iteration number ( rayleigh quotient iteration ) , however in our experiments ( section [ sec : num ] ) constant choice of @xmath24 yields convergence in 5 iterations .    as it follows from for the inverse iteration to converge fast a good initial approximation has to be found .",
    "to get initial approximation we propose to use locally optimal block preconditioned conjugate gradient ( lobpcg ) method as it is relatively easy to implement in tensor formats , and a preconditioner can be explicitly utilized .",
    "the problem with straightforward usage of the iterative processes above is that we need to store an eigenvector  @xmath31 .",
    "the storage of this vector is @xmath32 , which is prohibitive even for @xmath33 and @xmath34 .",
    "therefore we need a compact representation of an eigenfunction which allows to do inversion and basic linear algebra operations in a fast way . for this purpose",
    "we use the tensor train ( tt , mps ) format @xcite .",
    "tensor @xmath35 is said to be in the tt - format if it is represented as @xmath36 where @xmath37 , @xmath38 , @xmath39 .",
    "matrices @xmath40 are called tt - cores and @xmath41 are called tt - ranks . for simplicity",
    "we assume that @xmath42 , @xmath43 and call @xmath44 the tt - rank . in numerical experiments we use different mode sizes @xmath45 , @xmath46 , but for simplicity we will use notation @xmath47 in complexity estimates .",
    "compared to @xmath32 parameters of the whole tensor , tt decomposition contains @xmath48 parameters as each @xmath49 , @xmath50 has size @xmath51 .",
    "the definition of the tt - format of an operator is similar to the tt representation of tensors @xmath52 where tt - cores @xmath53 , @xmath54 .",
    "if @xmath55 , @xmath43 then this representation contains @xmath56 degrees of freedom .",
    "tt - format can be considered as a multidimensional generalization of svd . other alternatives to generalize the svd to many dimensions are the canonical , tucker and hierarchical tucker formats .",
    "we refer the reader to @xcite for detailed survey .",
    "the important point why we use the tt decomposition is that it can be computed in a stable way and it does not suffer from the `` curse of dimensionality '' .",
    "moreover , there exists well - developed software packages to work with the tt - decomposition  @xcite .      for the inverse iteration",
    "we need to find tt - representation of @xmath57 by approximately solving a linear system @xmath58 assume that both the exact eigenvector @xmath35 and the current approximation @xmath31 belong to the manifold @xmath59 of tensors with tt - rank @xmath44 .",
    "the solution of may have ranks larger than @xmath44 and therefore be out of @xmath59 . in the present work we suggest exploiting the information that @xmath35 belongs to @xmath59 and",
    "retract the solution of back to the manifold .",
    "we refer to this concept as a _ manifold - projected inverse iteration _ ( mp ii ) . in this work",
    "we pose the following optimization problem with a rank constraint @xmath60 problem is hard to solve as operator @xmath61 is close to singular .",
    "similarly to the inexact inverse iteration framework we are not searching for the solution that finds global minima of , but utilize several sweeps of the alternating least squares ( als ) method with the initial guess @xmath31 @xcite .",
    "the als procedure alternately fixes all but one tt - core and solves minimization problem with respect to this tt - core .",
    "for instance , an update of a core @xmath62 when all other cores are fixed is @xmath63 found from @xmath64 the minimization over a single core ( see appendix [ sec : als ] ) is a standard linear least squares problem with the unknown vector that has the size @xmath65  size of the corresponding tt - core and is very cheap .",
    "moreover these systems can be also solved iteratively .",
    "the described minimization over all cores in the tt - representation is referred to as one sweep of the als .",
    "the total computational cost of one sweep is then  @xmath66 , where @xmath67 is maximum tt - rank of the operator @xmath17 , see appendix [ sec : als ] . _",
    "according to the proposed concept we start from @xmath68 and use only a few sweeps ( one or two ) of the als method rather than running the method till convergence .",
    "moreover , we found that one can solve local linear systems inexactly either with fixed number of iterations or fixed low accuracy .",
    "_ such low requirements for solution of local linear systems and number of als sweeps results in a very efficient method .",
    "however , for this method to converge , a good initial approximation to both eigenvector and eigenvalue has to be found .      to get initial approximation we use lobpcg method . the lobpcg algorithm for one eigenvalue looks as follows @xmath69 where @xmath70 denotes preconditioner and vector of coefficients @xmath71",
    "is chosen from minimization of the rayleigh quotient @xmath72 finding @xmath73 is equivalent to solving the following @xmath74 eigenvalue problem @xmath75\\ ,      \\alpha = \\lambda \\begin{bmatrix } { \\mathcal{x}}_{k } \\\\ { \\mathcal{r}}_k \\\\ { \\mathcal{p}}_k \\end{bmatrix } [ { \\mathcal{x}}_{k } , { \\mathcal{r}}_k , { \\mathcal{p}}_k]\\ , \\alpha.\\ ] ] let us discuss tt version of the lobpcg .",
    "operations required to implement the lobpcg are presented below :    [ [ preconditioning . ] ] preconditioning .",
    "+ + + + + + + + + + + + + + + +    the key part of the lobpcg iteration is multiplication by a preconditioner @xmath70 . in this work",
    "we use @xmath76 as a preconditioner .",
    "this preconditioner works well if the density of states is low , see  @xcite . to make a preconditioner more efficient",
    "one can project it to the orthogonal complement of current approximation of the solution , see jacobi - davidson method  @xcite .",
    "instead of forming @xmath77 we calculate matrix - by - vector multiplication @xmath78 . hence , similarly to the inverse iteration ( section [ sec : iii1 ] ) we propose solving a minimization problem @xmath79 we also use only several sweeps of als for this problem .",
    "we refer to this construction of preconditioner as a _ manifold preconditioner _ as it retracts the residual on a manifold of tensors with fixed rank @xmath44 .",
    "note that if @xmath30 is known to be positive definite , then minimization of energy functional @xmath80 can be used instead of minimization of the residual .",
    "[ [ summation - of - two - tensors . ] ] summation of two tensors .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    given two tensors @xmath35 and @xmath81 with ranks @xmath44 in the tt - format @xmath82 the cores of the sum @xmath83 are defined as @xcite @xmath84 and @xmath85 thus , tensor @xmath86 is explicitly represented with ranks  @xmath87 .",
    "[ [ inner - product - and - norm . ] ] inner product and norm .",
    "+ + + + + + + + + + + + + + + + + + + + + + +     to find inner product of two tensors @xmath35 and @xmath81 in the tt - format we first need to calculate the hadamard product , which can calculated as @xmath88 therefore , @xmath89 where @xmath90 using special structure of matrices @xmath91 the inner product can be calculated in @xmath92 complexity @xcite .",
    "the norm can be computed using inner product as @xmath93 .",
    "[ [ reducing - rank - rounding . ] ] reducing rank ( rounding ) .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    as we have seen , after summation ranks grow . to avoid rank growth there exists special _ rounding _ operation .",
    "it suboptimally reduces rank of a tensor with a given accuracy . in 2d",
    "the rounding procedure of @xmath94 looks as follows .",
    "first we calculate qr decompositions of matrices @xmath95 and @xmath9 : @xmath96 hence @xmath97 finally , to reduce the rank we calculate the svd decomposition of @xmath98 and truncate singular values up to required accuracy . in @xcite",
    "this idea is generalized to the tt case .",
    "the complexity is @xmath92 .",
    "* @xmath99 : calculates @xmath100 via the cross approximation algorithm ( section [ sec : iib ] ) .",
    "* @xmath101 : block multiplication of a vector of tt tensors @xmath102 by real - valued matrix @xmath103 using @xmath104 function , see .",
    "* @xmath105 : orthogonalizes tt tensors @xmath106 : @xmath107 via cholesky ( section  [ sec : iib ] ) or modified gram - schmidt procedure .",
    "matrix - by - vector multiplications are done using using @xmath108 . *",
    "@xmath109 : solves @xmath110 , @xmath111`length`@xmath112 using @xmath113 sweeps of als method to minimize @xmath114 with a rank constraint .",
    "* @xmath115 : orthogonalizes tt tensors @xmath106 : @xmath107 with respect to @xmath116 : @xmath117 .",
    "to avoid rank growth we use rounding if ` length`@xmath118 is small and via ` multifuncrs ` if ` length`@xmath118 is large . *",
    "@xmath119 : truncates each tensor @xmath106 : @xmath107 with rank  @xmath44 using rounding procedure .",
    "[ [ matrix - by - vector - multiplication . ] ] matrix - by - vector multiplication .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to calculate matrix - by - vector operation @xmath120 in it is convenient to have hamiltonian @xmath25 represented in the tt - format . in this case",
    "there exists explicit representation of matrix - by - vector multiplication @xmath121 when both @xmath17 and @xmath35 are represented in the tt - format @xcite @xmath122 which gives representation with tt - rank  @xmath123 . to reduce the rank one can either use the rounding procedure or use als minimization of the following optimization problem @xmath124 which is faster than rounding for large ranks",
    "in the previous section we discussed the algorithm for a single eigenvector . in this section",
    "we extend the algorithm to a block case .",
    "the difference is that we need to make additional block operations such as othogonalization and block multiplication .      in this section we present manifold - projected version of the simultaneous inverse iteration , which is also known as block inverse power method .",
    "assume we are given a good initial approximation to eigenvalues and eigenvectors of linear operator @xmath17 given by its tt - representation .",
    "in this case inverse iteration yields fast convergence rate with low memory requirements .",
    "[ alg : lobpcg ]    group close eigenvalues in clusters @xmath125 $ ] , @xmath126 $ ] compute shift @xmath127  average eigenvalue in cluster @xmath128 compute @xmath129  corresponding subvector of @xmath130 @xmath131 ) @xmath132 $ ] , @xmath133 $ ] @xmath134    given initial approximation we first split eigenvalues into clusters of eigenvalues .",
    "proximity of eigenvalues is defined using a threshold parameter .",
    "if a cluster consists of one eigenvalue , then we run a version described in section  [ sec : iii1 ] .",
    "otherwise we need additional orthogonalization on each iteration step .",
    "the orthogonalization is done using the qr decomposition via cholesky factorization .",
    "let us consider orthogonalization procedure for vector @xmath135 $ ] in more details .",
    "first we calculate gram matrix @xmath136 where we used notation @xmath137 the calculation of gram matrix can be done in @xmath138 operations as calculation of a single inner product requires @xmath139 operations .",
    "then we calculate the cholesky factorization of the @xmath140 gram matrix @xmath141 .",
    "we note that @xmath142 consists of numbers , so the standard cholesky factorization of matrices is used .",
    "the final and the most time consuming step is to find block matrix - by - vector operation denoted by ` block_matvec`@xmath143 . for a general matrix @xmath144 function ` block_matvec`@xmath145 produces block vector @xmath146 $ ] such that @xmath147 if @xmath148 is small , say @xmath149 , then the summation can be done using summation and rounding .",
    "the typical value of @xmath148 in numerical experiments is @xmath150 or @xmath151 , so to reduce the constant in complexity we use the so - called _ cross approximation method _ , which is able to calculate the tt - decomposition of a tensor using only few of its elements .",
    "namely , the cross approximation is able to build tt - representation of a tensor of exact rank  @xmath44 using only @xmath152 elements ( number of parameters in the tt - format ) using interpolation formula @xcite with @xmath153 complexity .",
    "if tensor is approximately of low rank then tensor can be approximated with given accuracy using the same approach and quasioptimality estimates exist @xcite .",
    "to find tt - representation of @xmath154 we calculate @xmath152 elements of tensor @xmath154 by explicitly calculating elements in @xmath155 and summing them up with coefficients @xmath156 .",
    "this approach allows to calculate general type of functions of a block vector @xmath157 and is referred to as ` multifuncrs ` .",
    "it is typically used when either the number of input tensors is large or some nonlinear function of input tensors must be computed .",
    "the similar approach was used in @xcite for a fast computation of convolution .",
    "the result of solving the block system @xmath158 using @xmath113 of optimization procedure is denoted as @xmath159 , where @xmath160.\\ ] ] the overall algorithm is summarized in algorithm [ alg : inverse ] .",
    "all discussed auxiliary functions are presented in algorithm [ alg : functions ] . if the cluster size is much smaller than the number of required eigenvalues , the complexity of finding each cluster is fully defined by the inversion operation which costs @xmath161 .",
    "thus , the overall complexity of the inverse iteration scales linearly with @xmath162 : @xmath163 .",
    "we note that each cluster can be processed independently in parallel .",
    "we use the lobpcg method to get initial approximation for the manifold - preconditioned simultaneous inverse iteration .",
    "the problem is that each iteration of the lobpcg is much slower compared to the inverse iteration for large number of eigenvalues .",
    "we hence only run lobpcg with small ranks and then correct its solution with larger ranks using the inverse iteration .",
    "[ alg : lobpcg ]    [ alg : lobpcg ]    @xmath164 @xmath165 @xmath166 @xmath167 @xmath168 augment @xmath116 with corresponding tensors from @xmath169 restart the algorithm with new @xmath130 ( optionally ) increase truncation rank @xmath170    the lobpcg algorithm is summarized in algorithm  [ alg : lobpcg ] .",
    "auxiliary functions used in the algorithm are presented in algorithm [ alg : functions ] .",
    "we also use matlab like notation for submatrices , e.g. @xmath171 is the corresponding submatrix in matrix @xmath172 .",
    "block matrix - by - vector multiplication arises when multiplying by @xmath173 matrix , where @xmath162 is the number of eigenvalues to be found .",
    "when @xmath174 is a large number and we use ` multifuncrs ` for block matrix - by - vector product instead of straightforward truncation .",
    "this is the most time consuming step in the algorithm and it costs @xmath175 .",
    "another time - consuming part is matrix - by - vector multiplication which costs @xmath176 .",
    "thus , the overall complexity of each iteration is @xmath177 .    in order to accelerate the method",
    ", we consider a version of lobpcg with deflation .",
    "deflation technique is that we stop iterating converged eigenvectors . in this case",
    "the residual must be orthogonalized with respect to the converged eigenvectors .",
    "this procedure is denoted as ` ortho ` and is described in more details in algorithm [ alg : functions ] .",
    "it might be also useful to increase rank of vectors that did not converged after previous deflation step .",
    "the prototype is implemented in python using the ` ttpy ` library https://github.com/oseledets/ttpy .",
    "the code of the proposed algorithm can be found at https://bitbucket.org/rakhuba/ttvibr . for the basic linear algebra tasks the mkl library is used .",
    "python and mkl are from the anaconda python distribution https://www.continuum.io .",
    "python version is 2.7.11 .",
    "mkl version is 11.1 - 1 .",
    "tests were performed on a single intel core i7 2.6 ghz processor with 8 gb of ram .",
    "however , only 1 thread was used .",
    "first of all , we test our approach on a model hamiltonian when the solution is known analytically . following  @xcite",
    "we choose bilinearly coupled 64 dimensional hamiltonian @xmath178 with @xmath179 and @xmath180 .",
    "the tt - rank of this hamiltonian is @xmath181 for all inner modes independently of @xmath2 or @xmath4 .    to solve this problem we first use manifold - preconditioned lobpcg method ( section  [ sec : mpb ] ) with rank @xmath182 and then correct it with the mp inverse iteration .",
    "the thresholding parameter @xmath183 for separation of energy levels into clusters in the inverse iteration is @xmath184 ( @xmath185 and @xmath186 are in the same cluster if @xmath187 ) .",
    "the mode size @xmath33 is constant for each mode . as it follows from figure",
    "[ fig : berror ] the mp inverse iteration significantly improves the accuracy of the solution .",
    "we used 10 iterations of the lobpcg .",
    "the mp lobpcg computations took approximately 3 hours of cpu time and the mp inverse iteration took additionally 30 minutes .",
    "we also tested tensor version of the preconditioned inverse iteration ( pinvit ) @xcite that in case of a single eigenvector looks as @xmath188 where @xmath189 is selected to minimize the rayleigh quotient .",
    "figure [ fig : conv_sd ] illustrates convergence behavior of last 10 eigenvalues for different methods .",
    "the pinvit which also allows for explicit preconditioner converged to wrong eigenvalues .",
    "lobpcg method without preconditioner is unstable due to rank thresholding .",
    "we note that all these iterations converged to correct eigenvalues if the number of eigenvalues to be found was less than  @xmath190 .          in this part we present calculations of vibrational spectra of acetonitrile ( ch@xmath0cn ) molecule .",
    "the hamiltonian used is described in @xcite and looks as follows @xmath191 it contains @xmath192 terms : @xmath193 kinetic energy terms , @xmath193 quadratic , @xmath194 cubic , and @xmath195 quartic potential terms .",
    "we chose the same basis size that was used in @xcite , namely the mode sizes were @xmath196 corresponding to the order described in that work .",
    "we found that ranks of the hamiltonian for this particular molecule do not strongly depend on the permutation of indices , namely the largest rank we observed among random permutations was @xmath197 , while the maximum rank of the best permutation was @xmath198 . in computations we permuted indices such that array of @xmath199",
    "is sorted in a decaying order .",
    "table [ tab : hamranks ] contains ranks of the hamiltonian in the tt - representation .",
    "we note that total ranks are ranks of a sum of potentials after rounding , and hence they are not equal to the sum of ranks of potentials in table [ tab : hamranks ] .     to assemble the potential @xmath9 one needs to add rank-1 terms @xmath200 and rank-1 terms @xmath201 .",
    "each rank-1 term can be expressed analytically in the tt - format . as was mentioned in section [ sec : mp1 ] after each summation the rank grows , so the rounding procedure is used . recall that the rounding procedure requires @xmath202 operations .",
    "thus , the complexity of assembling the hamiltonian is @xmath203 where ` nnz ` stands for number of nonzeros , @xmath4 is the maximum mode size and @xmath67 is the maximum rank of @xmath25 .",
    "the total time of assembling the hamiltonian was less than @xmath204 second .",
    ".tt - ranks of the parts of the hamiltonian with @xmath205 threshold .",
    "+ [ cols=\"<,^,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     we ran the lobpcg method with tt - rank equal to 12 and used manifold preconditioner .",
    "initial guess is chosen from the solution of the harmonic part of the hamiltonian .",
    "eigenvectors of multidimensional quantum harmonic oscillator are tensor product of 1d oscillator eigenvectors and therefore can be represented analytically in the tt - format with rank @xmath204 .",
    "shift for the preconditioner is set to be the lowest energy of the harmonic part .",
    "the convergence of each eigenvalue is presented on figure [ fig : conv_ch3cn ] .",
    "the obtained eigenfunctions were used as an initial approximation to the inverse iterations with ranks equal to  @xmath206 .",
    "shifts were chosen to be lobpcg energies .",
    "the thresholding parameter @xmath183 for separation of energy levels into clusters in the inverse iteration is @xmath207 these results were further corrected with the inverse iteration with rank @xmath208 . as it follows from table",
    "[ tab : energies ] and figure  [ fig : ch3cn_error ] both corrections are more accurate than the h - rrbpm method .",
    "the latter correction with rank @xmath208 yields energy levels lower than those of the smolyak quadrature method @xcite which means that our energy levels are more accurate .",
    "we note that storage of a solution with @xmath208 is less than the storage of the smolyak method ( 180  mb compared with 1.5  gb ) .        timings and storage of the h - rrbpm method were taken from @xcite . on this example",
    "the state - of - the - art method ` eigb ` @xcite method converges within approximately several days .",
    "the problem is that all basis functions are considered in one basis , which leads to large ranks .",
    "nevertheless , the ` eigb ` becomes very efficient and accurate when small number of eigenpairs are required .     of eigenvalues of the acetonitrile molecule with respect to the eigenvalue number .",
    "are energies obtained by smolyak quadratures @xcite . negative value of error stand for more accurate than smolyak quadratures energies .",
    "black line denotes zero value of the error .",
    "mp sii stands for manifold - projected inverse iteration.,width=340 ]",
    "the simplest basis set for representing unknown eigenfunctions is the direct product ( dp ) of one - dimensional basis functions . if a fast matrix - by - vector operation is given , then krylov iterative methods are available and the only problem is the exponential storage requirements  @xmath3 . alternatively one can prune a direct product basis @xcite or use a basis that is a product of functions with more than one variable @xcite . in this work ,",
    "we focus on dp basis and further reduce  @xmath3 storage by approximating unknown eigenvectors in the tt - format .",
    "we refer the reader to @xcite for detailed surveys on tensor decompositions .",
    "canonical tensor decomposition ( also called cp decomposition or candecomp / parafac model ) of the eigenvectors of vibrational problems was considered in work by leclerc and carrington @xcite .",
    "the authors used rank - reduced block power method ( rrbpm ) .",
    "each iteration of this method involves matrix - by - vector product , which can be efficiently implemented in tensor formats .",
    "the problem is that this method has poor convergence .",
    "moreover , canonical decomposition is known to have stability issues @xcite .",
    "the hierarchical rrbpm ( h - rrbpm ) proposed in  @xcite by thomas and carrington is a significant improvement of the rrbpm .",
    "this method also utilizes sum - of - products representation , but treats strongly coupled coordinates together .",
    "coupled coordinates are further decomposed hierarchically .",
    "the multi configuration time dependent hartree ( mctdh ) approach @xcite also uses tensor format , namely the tucker decomposition .",
    "this approach reduces complexity , but suffers from the curse of dimensionality .",
    "this problem was solved in the multilayer mctdh @xcite which is similar to the hierarchical tucker representation @xcite .",
    "we would also like to discuss tensor algorithms for solving eigenvalue problems developed in mathematical community .",
    "there are two natural directions of solving eigenvalue problems in tensor formats .",
    "one direction is straightforward generalization of iterative methods to tensor arithmetics with rank reduction on each iteration . for the canonical decomposition power method with shifts",
    "was generalized in @xcite and used in the rrbpm method .",
    "the preconditioned inverse iteration ( pinvit ) for tensor formats was considered in @xcite .",
    "the inverse iteration used in this paper differs from the pinvit , which is basically preconditioned steepest descent .",
    "tensor version of the inverse iteration based on iterative solution of arising linear systems was considered in @xcite .",
    "the pinvit iteration can explicitly utilize a preconditioner .",
    "the construction of preconditioners in tensor formats for eigenvalue problems was considered in @xcite .",
    "the approach for a general operator  @xcite uses newton - schulz iteration in order to find a good preconditioner .",
    "however , due to high amount of matrix multiplications , this approach becomes time - consuming . in order to construct a preconditioner one can use approximate inverse matrix or approximate solution of linear systems .",
    "see @xcite for solvers in tensor formats .",
    "the more advanced lobpcg method was for the first time generalized in @xcite .",
    "we utilize this method and construct preconditioner based on optimization procedure .",
    "the pinvit method with the proposed preconditioner and the lobpcg with and without preconditioning were tested in section  [ sec : num ] .",
    "although rank - truncated pinvit method is fast and is able to accurately calculate small amount of eigenvalues , it fails to converge when a lot of eigenvalues are required .",
    "alternatively to iterative methods , one can pose an optimization problem  minimization of the rayleigh quotient with the constraint on rank .",
    "this approach was recently considered in the matrix product state ( mps ) community @xcite and independently in @xcite .",
    "the only disadvantages is that all eigenfunctions are treated in one basis , which leads to large ranks and the method becomes slow ( calculation of the acetonitrile took several days ) .",
    "nevertheless , this approach becomes very efficient and accurate when small number of eigenpairs are required .",
    "one of the most interesting missing bits is the theory of the proposed approach .",
    "first , why the eigenfunctions can be well - approximated in the tt - format and what are the restrictions on the pes .",
    "second , the convergence properties of the manifold preconditioner have to be established . these questions will be addressed in future works .     from practical point of view",
    ", the applicability of the proposed approach for general molecules has to be studied .",
    "currently , it requires the explicit knowledge of sum - of - product representation .",
    "obtaining such a representation is a separate issue , which can be done by using existing methods : potfit @xcite or more general tt - cross approximation approach @xcite .",
    "if pes has large ranks , coordinate transformation can be helpful @xcite .",
    "we would like to thank prof .",
    "tucker carrington jr . and",
    "his research group for providing data for the numerical experiment section .",
    "the authors also would like to thank alexander novikov for his help in improving the manuscript .",
    "we also thank anonymous referees for their comments and constructive suggestions .    this work has been supported by russian science foundation grant 14 - 11 - 00659 .",
    "let us discuss technical details of solving the problem . to illustrate the idea",
    "let us start from the skeleton decomposition in 2d . in this case",
    "@xmath210 , where @xmath211 .",
    "this representation is equivalent to the tt - format in 2d . indeed , by defining cores @xmath212 and @xmath213 we get .",
    "the als procedure starts with fixing one core .",
    "let us fix @xmath9 with orthonormal columns ( can always be done by qr decomposition ) and find the updated @xmath95 from the minimization problem @xmath214 thanks to well - known property of kronecker products @xmath215 the problem can be equivalently represented as a minimization problem on unknown vector @xmath216 @xmath217 where @xmath218 denotes vectorization of @xmath219 by reshaping it into a column vector .",
    "finally we get small linear system @xmath220 where matrix @xmath221 is of size @xmath222 , while the initial matrix @xmath223 is of size @xmath224 . to avoid squared condition number",
    "one can formally use the projectional approach @xmath225 which corresponds to the zero gradient of the energy functional @xmath226 the problem is that @xmath223 is not positive definite unless the smallest eigenvalue is required .",
    "nevertheless , we found that this approach also works if @xmath227 is close enough to the eigenvector corresponding to the eigenvalue closest to  @xmath24 .",
    "this assumption holds as before running mp ii we find a good initial guess for both @xmath24 and eigenvector , see section [ sec : mp1 ] .",
    "note that operator @xmath228 is also given in the low - rank matrix format ( also corresponds to the tt - matrix format in 2d ) : @xmath229 where @xmath230 .",
    "hence , the matrix of a local system can be represented as @xmath231 and one can do matrix - vector products with  @xmath232 complexity .",
    "indeed , matrices @xmath233 are of size @xmath234 and @xmath235 are @xmath236 .",
    "hence multiplication of @xmath237 by a vector requires @xmath238 operations .",
    "similarly to the 2d case in arbitrary dimension we get the following linear system on the vectorized @xmath239-th core  @xmath240 @xmath241 where @xmath242 matrix @xmath243 is a multiplication of first @xmath244 cores reshaped into @xmath245 matrix : @xmath246 matrix @xmath247 is defined by analogy .",
    "typically additional orthogonalization of @xmath243 and @xmath247 is done @xcite .",
    "since operator @xmath223 is given by its tt - representation , matrix - vector multiplication requires @xmath248 operations , where @xmath44 is tt - rank of the tensor @xmath35 and @xmath67 is the maximum rank of the operator @xmath223 .",
    "compared with @xmath249 of the 2d case tt - cores between the first and last dimensions are of size @xmath250 for tt - tensor and @xmath251 for tt - matrix , so squared ranks appear .",
    "we note that if dimension and/or mode size are large : @xmath252 , then the total complexity is @xmath253 .         + & & & & & reference + & & & & & & & & smolyak @xcite + & & & & & & & & @xmath254 gb + & & & & & & & & + level & sym . & @xmath255 & @xmath256 & @xmath255 & @xmath256 & @xmath255 & @xmath256 & @xmath255 & @xmath256 & @xmath255 & @xmath256 & @xmath256 & @xmath255 & @xmath209 + zpve & & 9837.893 & 0.485 & 9837.525 & 0.118 & 9837.429 & 0.022 & 9837.463 & 0.056 & 9837.408 & 0.001 & & @xmath257 & 9837.4073 + @xmath258 & @xmath255 & 361.24 & 0.25 & 361.04 & 0.04 & 361.00 & 0.01 & 361.016 & 0.03 & 360.991 & 0.000 & -0.001 & 360.990 & 360.991 + & & 361.24 & 0.25 & 361.06 & 0.07 & 361.00 & 0.01 & 361.029 & 0.04 & 360.991 & 0.000 & -0.001 & 360.990 & 360.991 + @xmath259 & @xmath255 & 723.60 & 0.42 & 723.41 & 0.23 & 723.22 & 0.04 & 723.274 & 0.09 & 723.180 & -0.001 & -0.001 & 723.180 & 723.181 + & & 723.60 & 0.42 & 723.41 & 0.23 & 723.22 & 0.04 & 723.276 & 0.09 & 723.180 & -0.001 & -0.001 & 723.180 & 723.181 + @xmath259 & @xmath260 & 724.27 & 0.44 & 724.05 & 0.23 & 723.87 & 0.04 & 723.919 & 0.09 & 723.827 & 0.000 & -0.001 & 723.826 & 723.827 + @xmath261 & @xmath260 & 901.25 & 0.59 & 900.83 & 0.16 & 900.71 & 0.05 & 900.722 & 0.06 & 900.660 & -0.002 & -0.004 & 900.658 & 900.662 + @xmath262 & @xmath255 & 1035.33 & 1.21 & 1034.23 & 0.10 & 1034.19 & 0.07 & 1034.211 & 0.08 & 1034.127 & 0.001 & -0.002 & 1034.124 & 1034.126 + & & 1035.34 & 1.21 & 1034.23 & 0.10 & 1034.20 & 0.07 & 1034.241 & 0.11 & 1034.127 & 0.001 &",
    "-0.002 & 1034.124 & 1034.126 + @xmath263 & @xmath264 & 1087.23 & 0.67 & 1086.86 & 0.31 & 1086.66 & 0.11 & 1086.720 & 0.17 & 1086.554 & 0.000 & -0.002 & 1086.552 & 1086.554 + @xmath263 & @xmath260 & 1087.23 & 0.67 & 1086.86 & 0.31 & 1086.66 & 0.11 & 1086.734 & 0.18 & 1086.554 & 0.000 & -0.001 & 1086.553 & 1086.554 + @xmath263 & @xmath255 & 1088.52 & 0.74 & 1088.09 & 0.31 & 1087.88 & 0.11 & 1087.960 & 0.18 & 1087.776 & 0.000 & -0.001 & 1087.775 & 1087.776 + & & 1088.52 & 0.75 & 1088.09 & 0.31 & 1087.88 & 0.11 & 1088.005 & 0.23 & 1087.776 & 0.000 & -0.001 & 1087.775 & 1087.776 + @xmath265 & @xmath255 & 1260.70 & 0.82 & 1260.04 & 0.16 & 1259.90 & 0.02 & 1259.947 & 0.07 & 1259.811 & -0.071 & -0.073 & 1259.809 & 1259.882 + & & 1260.70 & 0.82 & 1260.14 & 0.26 & 1259.90 & 0.02 & 1260.035 & 0.15 & 1259.811 & -0.071 & -0.073 & 1259.809 & 1259.882 + @xmath266 & @xmath260 & 1390.32 & 1.34 & 1389.44 & 0.47 & 1389.16 & 0.18 & 1390.003 & 1.03 & 1389.007 & 0.034 & -0.002 & 1388.971 & 1388.973 + @xmath267 & @xmath255 & 1396.40 & 1.71 & 1394.90 & 0.21 & 1394.79 & 0.10 & 1395.349 & 0.66 & 1394.701 & 0.012 & -0.007 & 1394.682 & 1394.689 + & & 1396.40 & 1.72 & 1394.90 & 0.21 & 1394.80 & 0.11 & 1395.519 & 0.83 & 1394.709 & 0.020 & -0.007 & 1394.682 & 1394.689 + @xmath267 & @xmath264 & 1396.56 & 1.65 & 1395.10 & 0.19 & 1395.01 & 0.10 & 1395.745 & 0.84 & 1394.916 & 0.009 & -0.007 & 1394.900 & 1394.907 + @xmath267 & @xmath260 & 1399.39 & 1.70 & 1398.07 & 0.38 & 1397.85 & 0.16 & 1398.570 & 0.88 & 1397.727 & 0.040 & -0.003 & 1397.684 & 1397.687 + @xmath268 & @xmath255 & 1452.27 & 1.17 & 1451.65 & 0.55 & 1451.35 & 0.25 & 1451.409 & 0.31 & 1451.095 & -0.006 & -0.008 & 1451.093 & 1451.101 + & & 1452.27 & 1.17 & 1451.65 & 0.55 & 1451.35 & 0.25 & 1451.501 & 0.40 & 1451.095 & -0.006 & -0.008 & 1451.093 & 1451.101 + @xmath268 & @xmath255 & 1454.09 & 1.26 & 1453.37 & 0.55 & 1453.08 & 0.25 & 1453.179 & 0.35 & 1452.821 & -0.006 & -0.008 & 1452.819 & 1452.827 + & & 1454.24 & 1.41 & 1453.37 & 0.55 & 1453.08 & 0.25 & 1453.231 & 0.40 & 1452.821 & -0.006 & -0.008 & 1452.819 & 1452.827 + @xmath268 & @xmath260 & 1454.85 & 1.45 & 1453.95 & 0.55 & 1453.65 & 0.25 & 1453.739 & 0.34 & 1453.397 & -0.006 & -0.008 & 1453.395 & 1453.403 + @xmath269 & @xmath255 & 1484.53 & 1.30 & 1483.33 & 0.10 & 1483.30 & 0.07 & 1483.455 & 0.23 & 1483.225 & -0.004 & -0.009 & 1483.220 & 1483.229 + & & 1484.54 & 1.31 & 1483.34 & 0.11 & 1483.30 & 0.07 & 1483.545 & 0.32 & 1483.226 & -0.003 & -0.008 & 1483.221 & 1483.229 + @xmath270 & @xmath255 & 1621.95 & 1.73 & 1620.86 & 0.64 & 1620.45 & 0.22 & 1620.540 & 0.32 & 1620.202 & -0.020 & -0.024 & 1620.198 & 1620.222 + & & 1621.96 & 1.74 & 1620.86 & 0.64 & 1620.45 & 0.22 & 1620.729 & 0.51 & 1620.202 & -0.020 & -0.024 & 1620.198 & 1620.222 + @xmath270 & @xmath260 & 1622.56 & 1.79 & 1621.40 & 0.64 & 1620.99 & 0.22 & 1621.281 & 0.51 & 1620.748 & -0.019 & -0.024 & 1620.743 & 1620.767 + @xmath271 & @xmath255 & 1751.15 & 1.62 & 1750.18 & 0.65 & 1749.76 & 0.23 & 1750.918 & 1.39 & 1749.591 & 0.061 & -0.005 & 1749.525 & 1749.53 + & & 1751.15 & 1.62 & 1750.43 & 0.90 & 1749.76 & 0.23 & 1751.229 & 1.70 & 1749.612 & 0.082 & -0.003 & 1749.527 & 1749.53 + @xmath272 & @xmath260 & 1758.64 & 2.22 & 1756.85 & 0.42 & 1756.60 & 0.18 & 1757.736 & 1.31 & 1756.475 & 0.049 & -0.007 & 1756.419 & 1756.426 + @xmath272 & @xmath264 & 1758.65 & 2.22 & 1756.85 & 0.42 & 1756.60 & 0.18 & 1757.756 & 1.33 & 1756.476 & 0.050 & -0.007 & 1756.419 & 1756.426 + @xmath272 & @xmath255 & 1759.26 & 2.13 & 1757.53 & 0.39 & 1757.30 & 0.17 & 1758.023 & 0.89 & 1757.148 & 0.015 & -0.010 & 1757.123 & 1757.133 + & & 1759.26 & 2.13 & 1757.53 & 0.40 & 1757.31 & 0.18 & 1758.263 & 1.13 & 1757.154 & 0.021 & -0.009 & 1757.124 & 1757.133 + @xmath272 & @xmath255 & 1762.20 & 2.43 & 1760.37 & 0.59 & 1759.98 & 0.21 & 1761.112 & 1.34 & 1759.848 & 0.076 & -0.004 & 1759.768 & 1759.772 + & & 1762.20 & 2.43 & 1760.56 & 0.78 & 1759.98 & 0.21 & 1761.381 & 1.61 & 1759.859 & 0.087 & -0.002 & 1759.770 & 1759.772 + @xmath273 & @xmath260 & 1787.24 & 2.04 & 1785.52 & 0.31 & 1785.37 & 0.16 & 1785.338 & 0.13 & 1785.128 & -0.079 & -0.087 & 1785.120 & 1785.207 + @xmath274 & @xmath255 & 1818.50 & 1.70 & 1817.82 & 1.02 & 1817.07 & 0.27 & 1817.207 & 0.41 & 1816.789 & -0.010 & -0.012 & 1816.787 & 1816.799 + & & 1818.50 & 1.70 & 1817.82 & 1.02 & 1817.07 & 0.27 & 1817.219 & 0.42 & 1816.789 & -0.010 & -0.012 & 1816.787 & 1816.799 + @xmath274 & @xmath260 & 1820.78 & 1.83 & 1819.98 & 1.03 & 1819.22 & 0.27 & 1819.635 & 0.68 & 1818.943 & -0.009 & -0.012 & 1818.940 & 1818.952 + @xmath274 & @xmath264 & 1820.78 & 1.83 & 1819.98 & 1.03 & 1819.22 & 0.27 & 1819.644 & 0.69 & 1818.944 & -0.008 & -0.011 & 1818.941 & 1818.952 + @xmath274 & @xmath255 & 1821.92 & 1.89 & 1821.07 & 1.04 & 1820.30 & 0.27 & 1820.482 & 0.45 & 1820.021 & -0.010 & -0.014 & 1820.017 & 1820.031 + & & 1821.92 & 1.89 & 1821.07 & 1.04 & 1820.30 & 0.27 & 1820.575 & 0.54 & 1820.021 & -0.010 & -0.014 & 1820.017 & 1820.031 + @xmath275 & @xmath264 & 1845.94 & 1.68 & 1844.45 & 0.19 & 1844.36 & 0.10 & 1845.160 & 0.90 & 1844.312 & 0.054 & -0.008 & 1844.250 & 1844.258 + @xmath275 & @xmath255 & 1846.03 & 1.70 & 1844.53 & 0.20 & 1844.43 & 0.10 & 1845.309 & 0.98 & 1844.387 & 0.057 & -0.008 & 1844.322 & 1844.33 + & & 1846.04 & 1.71 & 1844.53 & 0.20 & 1844.43 & 0.10 & 1845.610 & 1.28 & 1844.389 & 0.059 & -0.008 & 1844.322 & 1844.33 + @xmath275 & @xmath260 & 1846.41 & 1.72 & 1844.89 & 0.20 & 1844.79 & 0.10 & 1845.985 & 1.30 & 1844.745 & 0.055 & -0.009 & 1844.681 & 1844.69 + @xmath276 & @xmath255 & 1934.56 & 3.01 & 1931.84 & 0.29 & 1931.70 & 0.15 & 1931.849 & 0.30 & 1931.523 & -0.024 & -0.033 & 1931.514 & 1931.547 + & & 1934.58 & 3.03 & 1931.84 & 0.29 & 1931.70 & 0.16 & 1931.897 & 0.35 & 1931.523 & -0.024 & -0.032 & 1931.515 & 1931.547 + @xmath277 & @xmath260 & 1984.05 & 2.20 & 1983.51 & 1.66 & 1982.14 & 0.29 & 1982.577 & 0.73 & 1981.821 & -0.028 & -0.034 & 1981.815 & 1981.849 + @xmath277 & @xmath264 & 1984.05 & 2.20 & 1983.51 & 1.66 & 1982.14 & 0.29 & 1982.783 & 0.93 & 1981.821 & -0.029 & -0.035 & 1981.815 & 1981.85 + @xmath277 & @xmath255 & 1985.33 & 2.48 & 1984.51 & 1.65 & 1983.14 & 0.28 & 1983.483 & 0.63 & 1982.823 & -0.034 & -0.041 & 1982.816 & 1982.857 + & & 1985.33 & 2.48 & 1984.51 & 1.66 & 1983.14 & 0.29 & 1983.575 & 0.72 & 1982.823 & -0.034 & -0.041 & 1982.816 & 1982.857 + @xmath278 & @xmath260 & 2062.65 & 5.58 & 2058.66 & 1.59 & 2057.17 & 0.10 & 2058.000 & 0.93 & 2057.071 & 0.003 & -0.020 & 2057.048 & 2057.068 + @xmath278 & @xmath255 & 2069.96 & 4.67 & 2066.78 & 1.49 & 2065.38 & 0.10 & 2065.811 & 0.53 & 2065.272 & -0.014 & -0.019 & 2065.267 & 2065.286 + & & 2070.21 & 4.92 & 2066.80 & 1.51 & 2065.38 & 0.10 & 2066.355 & 1.07 & 2065.297 & 0.011 & -0.018 & 2065.268 & 2065.286 + @xmath279 & @xmath255 & 2118.65 & 7.27 & 2112.50 & 1.12 & 2111.75 & 0.37 & 2113.788 & 2.41 & 2111.511 & 0.131 & -0.003 & 2111.377 & 2111.38 + & & 2118.67 & 7.29 & 2112.50 & 1.12 & 2111.75 & 0.37 & 2113.928 & 2.55 & 2111.565 & 0.185 & -0.001 & 2111.379 & 2111.38 + @xmath279 & @xmath260 & 2120.01 & 7.71 & 2113.35 & 1.05 & 2112.67 & 0.38 & 2114.145 & 1.85 & 2112.467 & 0.170 & -0.003 & 2112.294 & 2112.297 + @xmath280 & @xmath255 & 2120.94 & 1.61 & 2119.96 & 0.63 & 2119.66 & 0.33 & 2121.397 & 2.07 & 2119.396 & 0.069 & -0.010 & 2119.317 & 2119.327 + & & 2121.14 & 1.81 & 2119.96 & 0.63 & 2119.66 & 0.33 & 2122.058 & 2.73 & 2119.422 & 0.095 & -0.010 & 2119.317 & 2119.327 + @xmath280 & @xmath255 & 2121.54 & 1.00 & 2121.10 & 0.56 & 2120.84 & 0.30 & 2122.132 & 1.59 & 2120.583 & 0.042 & -0.012 & 2120.529 & 2120.541 + & & 2122.07 & 1.53 & 2121.10 & 0.56 & 2120.85 & 0.31 & 2122.481 & 1.94 & 2120.586 & 0.045 & -0.011 & 2120.530 & 2120.541 + @xmath280 & @xmath264 & 2122.07 & 1.16 & 2121.46 & 0.55 & 2121.21 & 0.30 & 2123.442 & 2.53 & 2120.943 & 0.033 & -0.013 & 2120.897 & 2120.91 + @xmath280 & @xmath255 & 2123.23 & 0.40 & 2124.17 & 1.34 & 2123.16 & 0.32 & 2124.548 & 1.71 & 2122.845 & 0.011 & 0.001 & 2122.835 & 2122.834 + & & 2123.32 & 0.49 & 2124.18 & 1.34 & 2123.16 & 0.33 & 2125.200 & 2.37 & 2122.997 & 0.163 & 0.004 & 2122.838 & 2122.834 + @xmath280 & @xmath260 & 2123.59 & 0.29 & 2124.72 & 1.42 & 2123.63 & 0.33 & 2126.877 & 3.58 & 2123.419 & 0.118 & -0.001 & 2123.300 & 2123.301 + @xmath281 & @xmath255 & 2145.31 & 2.70 & 2144.46 & 1.85 & 2142.68 & 0.06 & 2142.585 & -0.03 & 2142.390 & -0.224 & -0.235 & 2142.379 & 2142.614 + & & 2145.31 & 2.70 & 2144.60 & 1.99 & 2142.68 & 0.06 & 2143.580 & 0.97 & 2142.390 & -0.224 & -0.235 & 2142.379 & 2142.614 + @xmath282 & @xmath255 & 2186.73 & 3.09 & 2185.33 & 1.70 & 2184.00 & 0.37 & 2184.164 & 0.53 & 2183.622 & -0.013 & -0.016 & 2183.619 & 2183.635 + & & 2186.73 & 3.09 & 2185.33 & 1.70 & 2184.00 & 0.37 & 2184.254 & 0.62 & 2183.622 & -0.013 & -0.016 & 2183.619 & 2183.635 + @xmath282 & @xmath255 & 2189.99 & 3.86 & 2187.88 & 1.74 & 2186.50 & 0.36 & 2187.113 & 0.98 & 2186.124 & -0.014 & -0.019 & 2186.119 & 2186.138 + & & 2190.00 & 3.86 & 2187.88 & 1.74 & 2186.50 & 0.36 & 2187.230 & 1.09 & 2186.124 & -0.014 & -0.019 & 2186.119 & 2186.138 + @xmath282 & @xmath255 & 2191.88 & 4.24 & 2189.41 & 1.77 & 2188.00 & 0.36 & 2188.710 & 1.07 & 2187.626 & -0.016 & -0.021 & 2187.621 & 2187.642 + & & 2192.02 & 4.38 & 2189.42 & 1.78 & 2188.00 & 0.36 & 2188.820 & 1.18 & 2187.626 & -0.016 & -0.021 & 2187.621 & 2187.642 + @xmath282 & @xmath260 & 2192.61 & 4.46 & 2189.93 & 1.78 & 2188.50 & 0.36 & 2189.111 & 0.97 & 2188.127 & -0.017 & -0.022 & 2188.122 & 2188.144 + @xmath283 & @xmath260 & 2208.60 & 1.98 & 2207.08 & 0.45 & 2206.79 & 0.17 & 2208.281 & 1.65 & 2206.740 & 0.114 & -0.011 & 2206.615 & 2206.626 + @xmath283 & @xmath264 & 2208.61 & 1.98 & 2207.09 & 0.45 & 2206.80 & 0.17 & 2208.938 & 2.31 & 2206.772 & 0.139 & -0.009 & 2206.624 & 2206.633 + @xmath283 & @xmath255 & 2208.73 & 1.97 & 2207.23 & 0.46 & 2206.95 & 0.18 & 2208.948 & 2.18 & 2206.810 & 0.044 & 0.001 & 2206.767 & 2206.766 + & & 2208.73 & 1.97 & 2207.23 & 0.46 & 2206.95 & 0.18 & 2209.447 & 2.68 & 2206.876 & 0.110 & 0.002 & 2206.768 & 2206.766 + @xmath283 & @xmath255 & 2209.57 & 2.01 & 2208.01 & 0.45 & 2207.72 & 0.16 & 2209.486 & 1.93 & 2207.604 & 0.045 & -0.010 & 2207.549 & 2207.559 + & & 2209.59 & 2.03 & 2208.02 & 0.46 & 2207.72 & 0.16 & 2209.739 & 2.18 & 2207.615 & 0.056 & -0.009 & 2207.550 & 2207.559 +                                        bill poirier and tucker carrington  jr .",
    "accelerating the calculation of energy levels and wave functions using an efficient preconditioner with the inexact spectral transform method .",
    ", 114(21):92549264 , 2001 .",
    "gustavo avila and tucker carrington  jr .",
    "using a pruned basis , a non - product quadrature grid , and the exact watson normal - coordinate kinetic energy operator to solve the vibrational schrdinger equation for c2h4 .",
    "135(6):064101 , 2011 .      richard dawes and tucker carrington  jr . how to choose one - dimensional basis functions so that a very efficient multidimensional basis may be extracted from a direct product of the one - dimensional functions : energy levels of coupled systems with as many as 16 coordinates . , 122(13):134101 , 2005 ."
  ],
  "abstract_text": [
    "<S> we propose a new algorithm for calculation of vibrational spectra of molecules using tensor train decomposition . under the assumption that eigenfunctions lie on a low - parametric manifold of low - rank tensors we suggest using well - known iterative methods that utilize matrix inversion ( lobpcg , inverse iteration ) and </S>",
    "<S> solve corresponding linear systems inexactly along this manifold . as an application </S>",
    "<S> , we accurately compute vibrational spectra ( 84 states ) of acetonitrile molecule ch@xmath0cn on a laptop in one hour using only @xmath1 mb of memory to represent all computed eigenfunctions . </S>"
  ]
}