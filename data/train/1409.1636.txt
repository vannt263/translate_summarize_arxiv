{
  "article_text": [
    "a data warehouse ( dw ) is the decision - making database which holds the data extracted from transaction systems , operational data stores and external sources .",
    "the data are processed by the etl from the source systems into the dw , traditionally referred as to data warehousing @xcite .",
    "the transformed data in the dw @xcite are usually structured according to star schemas and accessed by decision support systems , such as online analytical processing ( olap ) tools and business intelligence ( bi ) applications @xcite .",
    "data warehousing systems run etl jobs at a regular interval , such as daily , weekly or monthly .",
    "the data source systems such as transnational databases typically deal with the dynamic data , which means that the data are changed frequently , such as an online shopping system .",
    "the data in the transaction systems are changed during everyday s business operations , e.g. , it can be adding a new order , or updating records as simple as an address change of a registered member , or more complex purchase data .",
    "the data warehouse of keeping the detailed transaction history is called the _ system of record _ ( sor ) , which can date back the changes of the data in the transaction processing systems @xcite .",
    "for example , business users can retrieve each of the orders , as well as its status , including ordering , delivery , payment , and after - sale services ; or cancellation of orders .",
    "the changes in the transaction processing systems are reflected in the sor data through the etl processes , which involve the operations such as new records addition , update , deletion , lookup , and loading .",
    "the etl process that supports the high frequent changing data is not trivial , and worth the further study .    in addition , when an etl process runs , the data fed to the etl process usually are required to follow a certain order .",
    "a typical example is that dimension processing is prior to the fact processing due to the foreign - key relationship between fact and dimension tables .",
    "a fact record in the fact table consists of dimension key values and measures ( or the facts ) .",
    "when the etl process processes a fact record , it gets the dimension key values by _ looking up _ the dimension tables where all the dimension values have been loaded .",
    "if the fact records arrive at the data warehouse earlier than the dimension records , the _ lookup _ operation will fail . to ensure the integrity of early - arriving data , the parent tables ( referenced tables ) are loaded first , followed by the child tables ( the tables with the foreign keys ) . using this approach , however , the downside is obvious .",
    "for example , we have to use some extra space to keep the early - arriving data temporarily .",
    "the waiting wastes the time , and might mess up with the overall schedule plan in a busy it platform .    in this paper , we use two - sequential staging databases , in combination with different operation codes , to process transaction data .",
    "specifically , this paper makes the following contributions : 1 ) we propose the two - level data staging etl method for handling the high frequent changes of data , i.e. , with insertion , update and deletion ; 2 ) we propose the method of using different operation codes to support etl actions , i.e. , the method can identify data changes into different operation codes , and based on the codes for doing the foreign - key validation , transformation , and loading .",
    "3 ) we propose the argumentation process for handling early - arriving data ; and  one - stop \" method for fast- and slowly - changing data processing .",
    "this paper is structured as follows .",
    "section  [ sec : overview ] gives the overview of the two - level data staging etl method .",
    "section  [ sec : dwdataprocess ] details the proposed method , including data change detection , transformation , key validation and loading .",
    "section  [ sec : relatedwork ] presents the related work , and the state - of - the - art of data warehousing technologies .",
    "section  [ sec : conclusion ] concludes the paper .",
    "we now describe the two - level data staging etl for handling transaction data ( see figure  [ fig : architecture ] ) .",
    "the etl process makes use of two databases for data staging , and completes the necessary data transformation upon the two staging databases .",
    "the whole process consists of change detection , transformation , key validation and loading .",
    "the etl process first extracts the data from source systems ( e.g. , a transaction processing system ) into the level-1 staging database ( ssa lv1 ) , then transposes the data from ssl lv1 to the level-2 staging database ( ssa lv2 ) .",
    "ssa lv1 maintains a similar database schema as the source system , while ssa lv2 maintains a similar schema as the data warehouse ( i.e. , sor ) . in the change detection phase ,",
    "the data changes that occurred in the transaction processing systems are identified into different operation codes for guiding the etl actions of data transformation and loading . in the key validation phase ,",
    "the transposed data in the ssa lv2 tables are validated based on the foreign - key relationship between tables . during the loading phase ,",
    "the data in ssa lv2 tables are well - prepared , and loaded into the sor tables .",
    "the sor keeps the finest granular records , and the change history of the data .",
    "the data in sor are used to create data marts or cubes through the dimensional data store ( dds ) process of aggregation . in the following sections",
    ", we focus on the discussion of the two - level data staging etl process of extracting transaction data from the source systems to the sor , and briefly introduce the dds data process .",
    "ssa lv1 is used because it is better to extract all the required source data to a staging area . in case",
    "the etl process needs to be re - run , it can still base on the data in ssa lv1 which was previously extracted from the source tables .",
    "ssa lv2 is used to simplify the etl process for handling the changes of the data , which might include fast - changing , slowly - changing , early - arriving , and static data . as such",
    ", the etl process could simply move the data from ssa lv2 to sor without needing any further data transformation , which greatly simplifies the loading process .",
    "we now describe how to process the records from an operational source system into sor using the two - level data staging etl .",
    "a record from the source system is extracted to the ssa lv1 , and marked with the transaction ( tx ) type code ,  i \" ,  u \" or  d \" , which indicates the action of insertion , update , or deletion in the source system . however , we can not directly insert or update the record into a sor table based on the tx type code . instead , we need to detect the change of the data ( or the tx type ) in the source system , and apply _ a new operation code _ for the change .",
    "the reasons are as follows .",
    "first , the transaction type code only represents the type of the action happened in the source table , it does not fully apply to the detailed records in the sor .",
    "for example , for the many - to - one mapping as shown in figure  [ fig : changedetect ] , in the source system the records in the tables , @xmath0 and @xmath1 , with the transaction type code  i `` are mapped to a single record in @xmath2 under the same business key value ' ' address 123 \" . if the record from @xmath0 is first inserted into the target table @xmath2 .",
    "then , if we solely follow the last tx type code in @xmath1 to insert the record , we will receive an error since it should be  update \" , instead of  insert \" .",
    "the second reason is that the daily etl job will only extract the latest action applied to the source table .",
    "if there is more than one action applied to the source table at the same day , the extraction will miss all the previous actions .",
    "for example , if the tx type codes ,  i \" and  u \" , are applied to the source table at the same date ( i.e. , a record is inserted , and updated subsequently ) , when the etl job runs at the end of the day , and updates the sor table based on the last tx type code  u \" , errors may occur because it should do the insertion , instead of updating the record .",
    ".the operation codes [ cols=\"<,<\",options=\"header \" , ]     [ tab : ssalv2 ]      after the processes of transformation and key validation , the records in ssa lv2 are ready to be loaded into sor tables .",
    "we classify the data to be loaded into two categories , static data and dynamic data .",
    "static data refer to those unlikely change over time , such as customer personal information including name , birthday , sex , etc . , while dynamic data refer to those more likely to change , such as customer address , and phone , etc . in sor ,",
    "the static data are simply stored in the static table while the dynamic data are stored in the history table with begin date and end date for tracking the changes . during the transformation and key validation processes , static and dynamic data both",
    "were moved to the same ssa lv2 table , which will be moved to the corresponding static and history tables in sor by the loading process .",
    "recall that we have identified different operation codes during transformation and key validation processes , and stored the codes in the ssa lv2 table . in the loading process",
    ", we move the data into sor tables based on these operation codes . the loading process is described in algorithm  [ alg : loading ]    after the data being processed by the transformation and key validation , we have the data in the ssa lv2 table , which are ready for the loading ( see table  [ tab : ssalv2 ] ) .",
    "the loading process involves the source table in ssa lv2 , and the target tables in the sor .",
    "compared with the transformation and key validation , the loading process is much simpler , which no lookup action is required .",
    "we identify the following five typical loading scenarios , which load static and dynamic data into sor tables based on the operation codes",
    ".    * scenario 1 .",
    "* figure  [ fig : load1 ] shows the loading process for the records with the operation code  b \" .",
    "for the static data from the ssa lv2 table , we insert a new record with the surrogate key  005 \" , business key  bk5 \" into sor static table . for the dynamic data from the ssa lv2 table ,",
    "we insert a new record into the history table with surrogate key ",
    "005 \" , business key  bk5 \" , begin date  20041008 \" , and end date  99991231 \" ( representing the latest record ) .",
    "@xmath3 read a record from the ssa lv2 table    insert into static table insert into history table update static table for the changed values .",
    "update history table , and set the end date .",
    "insert the record into history table : with same surrogate key as the previous version ; with new begin date ; with  99991231 \" as the end date ; and with the latest values for the other fields . update static table , and set the last transaction type code as ",
    "d \" ; update history table , and set the end date . update static table with the latest static values , and turn off the augment flag .",
    "insert the record into history table : with same surrogate key as static table ; with a new begin date ; with  99991231 \" as the end date ; and with the latest values for the other fields .",
    "update static table with the latest static values , and turn off the augment flag .",
    "insert the record into history table : with same surrogate key as static table ; with a new begin date ; with  9999 - 12 - 31 \" as the end date ; and with the latest values for the other fields .",
    "insert the record into static table : with a new surrogate key generated ; with a new begin date ; with  9999 - 12 - 31 \" as the end date ; and with the augment flag turned on .",
    "* scenario 2 .",
    "* figure  [ fig : load2 ] shows the loading process for the record with surrogate key  001 \" and the operation code  eb \" . since the sor static table has not kept any history record , we can directly insert the static data into the sor static table . for the sor history table",
    ", we have to first look up the record using the surrogate key  001 \" and sor begin date  20141001 \" .",
    "when the record is found , we first update the end date of the history record to  20041007 \" .",
    "then , we insert into a new record with the identical surrogate key  001 \" and business key  bk1 \" , fill the begin date  20141008 \" and the dynamic data from the ssa lv2 table , and set the end date to  99991231 \" , which represents the latest record .",
    "* scenario 3 .",
    "* figure  [ fig : load3 ] shows the loading process for the record with surrogate key  004 \" and operation code ",
    "we first locate the record in the sor static table using the surrogate key , then set the last record transaction type code as ",
    "d \" , representing that the record has been deleted . in the sor history table",
    ", we locate the record using the surrogate key  004 \" and sor begin date  20141006 \" . if the record is found , we update the end date  20141007 \" that is from the ssa lv2 table .",
    "* scenario 4 .",
    "* figure  [ fig : load4 ] shows the loading process for the record with surrogate key  003 \" and operation code  da \" . in the sor static table",
    ", we locate the augment record using the surrogate key .",
    "since the augment record only contains surrogate key , business key , and augment flag , we update the blank attributes , and turn off the augment flag . in the sor history table , we insert a new record with surrogate key  003 \" , business key  bk3 \" , begin date  20141008 \" , and the dynamic data from the ssa lv2 table .",
    "the end date is set to  99991231 \" to represent the latest record .    * scenario 5 .",
    "* figure  [ fig : load5 ] shows the loading process for the record with surrogate key  613 \" and operation code  a \" . in the sor static table",
    ", we insert a new record with surrogate key  613 \" , business key  z \" , and augment flag  1 \" .",
    "the other fields are left blank .",
    "since it is for augmentation , we do not need to insert any record into the sor history table .",
    "the argument record will be updated again when the record with  da \" from ssa lv2 arrives next time .",
    "according to the etl process shown in figure  [ fig : architecture ] , it is the dds data process after the two - level data staging etl process .",
    "dds data process summarizes and aggregates the data from sor , and inserts into the dimension and fact tables in dds , or directly transforms the data to multi - dimensional olap cubes ( molap ) .",
    "the dds data process does dimension and fact extraction .",
    "the dimension extraction process extracts dimension records from sor based on the last transaction date , and only extracts the records whose last transaction dates are equal to or greater than the starting date of the current etl job .",
    "if a record in sor with the last transaction type code  d \" , the record will not be extracted to dds if the target is a static dimension table ; but , if the target is a slowly changing dimension ( scd ) table , the record will be extracted to keep track of changes .",
    "compared with the dimension extraction , fact extraction is more complicated .",
    "we have to determine which record to be extracted based on the actual affected date and the last transaction date .",
    "to identify which field in sor is the actual affected date , we need to examine what information the fact table is going to store .",
    "for example , for the sales data mart , if the fact table is for holding order data , the actual affected date should be the transaction date in the sor transaction order table .",
    "after identifying the actual affected date , we select all the records in the sor table with the actual affected date , and process them into the corresponding dds fact table . if the fact table contains the measures calculated from the measures of the previous date , all the records in the sor table at or after the earliest affected date need to be retrieved to rebuild the fact table .",
    "etl has been the subject of extensive research in the academic community .",
    "the recent papers @xcite present a survey of the research in etl .",
    "the etl design has been discussed from different perspectives .",
    "the papers @xcite use uml - based method to design etl processes , and @xcite use er model in the etl design .",
    "akkaoui and e.  zimnyi @xcite propose a platform - independent etl conceptual model based on the business process model notation ( bpmn ) standard , and implement etl using business process execution language ( bpel ) .",
    "skotas and smitsis @xcite introduce the semantic web technologies , _ ontology _ , to the etl conceptual modeling . in @xcite , the author presents the requirement - driven approach for the etl design , where the etl can be produced separately for each requirement , and consolidated incrementally . thomsen and pederson propose a python - based etl programing framework to address etl programming efficiency , e.g. , only a few code lines are needed to implement an etl program for a specific dimensional schema @xcite . in this paper , we , on the other hand , use the bottom - up approach for the etl design .",
    "the proposed etl method aims at simplifying processing transaction data , and provides ",
    "one - stop \" approach for handling early - arriving , fast- and slowly - changing data .",
    "optimizing etl is a time - consuming process , but it is essential to ensure etl jobs to complete within specific time frames . for etl optimization , simitsis et al . propose a theoretical framework @xcite , which formalizes etl state spaces into a directed acyclic graph ( dag ) , then searches the best execution plan with regards to the time of the state spaces .",
    "tziovara et al .",
    "propose the approach of optimizing etl based on an input logical etl template @xcite . in @xcite , li and zhan",
    "analyze the task dependencies in an etl workflow , and optimize etl workflow by applying the parallelization technique to the tasks without dependency .",
    "behrend and jrg use rule - based approach to optimize etl flows @xcite , where the rules are generated based on the algebraic equivalences .",
    "our approach is an etl method , but can also be used to optimize the etl in solving the early - arriving data problem , in which no dependencies need to be considered .",
    "the latest trend of data warehousing is to support big data , and offer real - time / right - time capability , e.g. , @xcite .",
    "the emergence of the cloud computing technologies , such as mapreduce @xcite , makes it feasible for etl to process large - scale data on many nodes . as the evidence ,",
    "the two open source mapreduce - based systems , pig @xcite and hive @xcite , become increasingly used in data warehousing .",
    "but , they both are designed for the generic purpose for big data analytics , with limited etl capabilities , somewhat like dbmss other than full - fledged etl tools . to complement this ,",
    "our previous work , etlmr @xcite , extends the etl programming framework @xcite using mapreduce , but maintains its simplicity in implementing a parallel dimensional etl program .",
    "besides , the framework @xcite is proposed to improve the dimensional etl capability of hive .",
    "the difference is that etlmr aims at the traditional rdbms - based data warehousing system , while the latter uses hive for more scalability .",
    "in data warehousing , it is challenging to process transaction data featured with high frequent changes . in this paper , we have proposed a two - level data staging etl method for handing the transaction data .",
    "the proposed method identifies the changes of source data into different operation codes , then pre - processes the data from the staging area of the level - one to the level - two , and finally loads the data into a data warehouse based on the operation codes . in the paper , we used a running example to illustrate how to use the proposed etl method , including detecting the changes , identifying the operation codes , verifying the key , and loading .",
    "the proposed etl provides the  one - stop \" approach for processing fast - changing , slowly - changing , and early - arriving data ."
  ],
  "abstract_text": [
    "<S> in data warehousing , extract - transform - load ( etl ) extracts the data from data sources into a central data warehouse regularly for the support of business decision - makings . </S>",
    "<S> the data from transaction processing systems are featured with the high frequent changes of insertion , update , and deletion . </S>",
    "<S> it is challenging for etl to propagate the changes to the data warehouse , and maintain the change history . </S>",
    "<S> moreover , etl jobs typically run in a sequential order when processing the data with dependencies , which is not optimal , e.g. , when processing early - arriving data . in this paper </S>",
    "<S> , we propose a two - level data staging etl for handling transaction data . </S>",
    "<S> the proposed method detects the changes of the data from transactional processing systems , identifies the corresponding operation codes for the changes , and uses two staging databases to facilitate the data processing in an etl process . </S>",
    "<S> the proposed etl provides the  one - stop \" method for fast - changing , slowly - changing and early - arriving data processing . </S>"
  ]
}