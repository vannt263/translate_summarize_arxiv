{
  "article_text": [
    "subspace clustering is the problem of clustering a collection of points drawn approximately from a union of linear subspaces .",
    "this is an important problem in pattern recognition with diverse applications from computer vision @xcite to genomics @xcite .",
    "* related work . *",
    "early subspace clustering methods were based on alternating between finding the subspaces given the clustering and vice versa @xcite , and were very sensitive to initialization . the need for good initialization motivated the development of an algebraic technique called generalized principal component analysis ( gpca ) @xcite , which solves the problem in closed form . the key idea behind gpca",
    "is that a union of @xmath0 subspaces can be represented by a collection of polynomials of degree @xmath0 , with the property that their gradients at a data point give the normals to the subspace passing through that point .",
    "this is exploited in @xcite and @xcite for clustering a known number of subspaces .",
    "the recent abstract algebraic subspace clustering ( aasc ) method of @xcite , unifies the ideas of @xcite , into a provably correct method for the decomposition of a union of subspaces to its constituent subspaces .",
    "however , while in theory gpca and aasc are applicable to subspaces of any dimensions , in practice the estimation of the subspaces is sensitive to data corruptions .",
    "the need for methods that can handle high - dimensional data corrupted by noise and outliers motivated the quest for better subspace clustering affinities .",
    "state - of - the - art methods , such as _ sparse subspace clustering _ @xcite and _ low rank subspace clustering _",
    "@xcite , exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces .",
    "sparse and low rank representation techniques are then used to compute the coefficients , which are then used to build a subspace clustering affinity .",
    "these methods perform very well when the subspace dimensions are much smaller than the dimension of the ambient space , the subspaces are sufficiently separated and the data are well distributed inside the subspaces @xcite .",
    "however , these methods fail when the dimensions of the subspaces are large , e.g. a union of hyperplanes , which is the case where gpca , to be henceforth called algebraic subspace clustering ( asc ) , performs best .",
    "in addition , sparse methods produce low inter - class connectivity , but the intra - class connectivity is also low due to sparsity , leading to over - segmentation issues .",
    "conversely , low - rank and @xmath1 methods produce high intra - class connectivity ( since they are less sparse ) but this also leads to high inter - class conectivity .",
    "consequently , there is a strong need for developing methods that produce high intra - class and low inter - class connectivity .    * paper contributions . *",
    "the main contribution of this paper is to propose a new subspace clustering algorithm that can handle noisy data drawn from a union of subspaces of different dimensions .",
    "the key idea is to construct for each data point ( the _ reference point _ ) a sequence of projections onto hyperplanes that contain the _ reference subspace _ ( the subspace associated to the reference point ) .",
    "the norms of the projected data points are used to define their affinity with the reference point .",
    "this process leads to an affinity matrix of high intra - class and low cross - class connectivity , upon which spectral clustering is applied .",
    "we provide a theorem of correctness of the proposed algorithm in the absence of noise as well as a variation suitable for noisy data . as a secondary contribution",
    ", we propose to replace the angle - based affinity proposed in @xcite by a superior distance - based affinity .",
    "this modification is motivated by the fact that the angle - based affinity is theoretically correct only in the case of hyperplanes , and is not a good affinity for subspaces of varying dimensions .",
    "our experiments demonstrate that the proposed method outperforms other subspace clustering algorithms on the hopkins 155 motion segmentation database as well as on synthetic experiments for arbitrary - dimensional subspaces of a low - dimensional ambient space .",
    "we begin with a brief overview of the asc theory and algorithms .",
    "we refer the reader to @xcite for details .    * subspace clustering problem . *",
    "let @xmath2 be a set of points that lie in an unknown union of @xmath3 subspaces @xmath4 , where @xmath5 a linear subspace of @xmath6 of dimension @xmath7 .",
    "the goal of subspace clustering is to find the number of subspaces , a basis for each subspace , and cluster the data points based on their subspace membership , , find the correct decomposition or _ clustering _ of @xmath8 as @xmath9 , where @xmath10 . to make the subspace clustering problem well - defined",
    ", we need to make certain assumptions on the geometry of both the subspaces @xmath5 and the data @xmath8 . in this work",
    "we assume that the underlying union of subspaces @xmath11 is _ transversal _ @xcite , which in particular implies that there are no inclusions between subspaces .",
    "moreover , we assume that @xmath12 for @xmath13 , , each of the given points is associated to a unique subspace .",
    "this guarantees that the above decomposition of @xmath8 is in fact a partition , and it is unique .",
    "a final assumption that we need is that the data @xmath8 are rich enough and in general position ( see definition [ def : general - position ] ) .",
    "* unions of subspaces as algebraic varieties .",
    "* a key idea behind asc is that a union of @xmath0 subspaces @xmath4 of @xmath14 is the _",
    "zero set _ of a finite set of homogeneous polynomials of degree @xmath0 with real coefficients in @xmath15 indeterminates @xmath16 .",
    "such a set is called an _ algebraic variety",
    "_  @xcite .",
    "for example , a union of @xmath0 hyperplanes @xmath17 , where the @xmath18th hyperplane @xmath19 is defined by its normal vector @xmath20 , is the zero set of @xmath21 likewise , the union of a plane with normal @xmath22 and a line with normals @xmath23 is the zero set of the two polynomials @xmath24 and @xmath25 .",
    "observe that these _ vanishing polynomials _ are homogeneous of degree @xmath0 , where @xmath0 is the number of subspaces .",
    "moreover , they are factorizable into linear forms , with each subspace contributing a linear form to the product .",
    "each such linear form is in turn defined by a normal vector to the subspace .    *",
    "finding vanishing polynomials . *",
    "note that the coefficients of the polynomials associated with a union of subspaces @xmath11 can be obtained from sufficiently many samples @xmath26 in _ general position _ by solving a linear system of equations .",
    "[ def : general - position ] we say that the data @xmath27 is in general position if a degree @xmath0 polynomial vanishes on @xmath8 if and only if it vanishes on the underlying union of subspaces @xmath11 .    for example , if @xmath11 is a union of two planes in @xmath28 with normals @xmath29 , @xmath30 , then we can write @xmath31 as @xmath32 where @xmath33 and @xmath34 .",
    "thus , we can find the vector of coefficients @xmath35 by solving the set of linear equations @xmath36 for @xmath37 .",
    "more generally , each polynomial of degree @xmath0 can be written as @xmath38 , where @xmath39 is the veronese embedding of degree @xmath0 that maps a point @xmath40 to all @xmath41 distinct monomials of degree @xmath0 in the entries of @xmath42 .",
    "consequently , a basis for the set of polynomials of degree @xmath0 that vanishes in @xmath8 can be found by computing a basis for the right nullspace of the embedded data matrix , , by solving the linear system : @xmath43^\\transpose\\c = \\0.\\ ] ] however , the polynomials obtained by the above procedure may not factorize into a product of linear forms because the space of factorizable polynomials is not a linear space , e.g @xmath44 is not factorizable .",
    "* polynomial differentiation algorithm .",
    "* even though an elegant solution based on polynomial factorization exists for the case of hyperplanes @xcite , it has not been generalized for subspaces of different dimensions",
    ". however , an alternative solution has been achieved by observing that given _ any _ degree @xmath0 vanishing polynomial @xmath31 on @xmath11 , and a point @xmath42 in @xmath11 , the gradient of @xmath31 evaluated at @xmath42 will be orthogonal to the subspace associated with point @xmath42 ( see @xcite and @xcite for a geometric and algebraic argument respectively ) .",
    "consequently , for the purpose of computing normal vectors to the subspaces , it is enough to compute general vanishing polynomials of degree @xmath0 .",
    "the set of all such polynomials , denoted @xmath45 , is a finite - dimensional vector space and a basis can be computed as a basis of the right nullspace of the veronese matrix @xmath46^\\transpose$ ] , where @xmath39 is the veronese embedding of degree @xmath0 that maps a point @xmath40 to all @xmath41 distinct monomials of degree @xmath0 in the entries of @xmath42 . having a basis @xmath47 for @xmath45",
    ", it can be shown that the subspace associated to a point @xmath48 can be identified as the orthogonal complement of the subspace spanned by the vectors @xmath49 @xcite .",
    "then we can remove the points that lie in the same subspace as @xmath42 and iterate the procedure with the remaining points until all subspaces have been identified .",
    "it is remarkable that this procedure is provably correct for a known number of subspaces of arbitrary dimensions . even though this result is general and insightful , algorithms that are directly based on it",
    "are extremely sensitive to noise .",
    "the main reason is that any procedure for estimating the dimension of the nullspace will unavoidably involve thresholding the singular values of @xmath50 , which will in turn yield very unstable estimates of the subspaces and subsequently poor clustering of the points .",
    "* spectral algebraic subspace clustering algorithm . * in the interest of enhancing the robustness of asc in the presence of noise and obtaining a working algebraic algorithm , the standard practice has been to apply a variation of the polynomial differentiation algorithm based on spectral clustering . more specifically , given noisy data",
    "@xmath8 lying close to a union of @xmath0 subspaces @xmath11 , one computes an approximate vanishing polynomial @xmath31 whose coefficients are given by the right singular vector of @xmath51 corresponding to its smallest singular value .",
    "given @xmath31 , one computes the gradient of @xmath31 at each point in @xmath8 ( which gives a normal vector associated with each point in @xmath52 , and builds an affinity matrix between points @xmath53 and @xmath54 as the cosine of the angle between their corresponding normal vectors , , @xmath55 this affinity is then used as input to any spectral clustering algorithm to obtain the clustering @xmath56 .",
    "we call this spectral asc method with _ angle - based affinity _ as sasc - a . to gain some intuition on @xmath57 , suppose @xmath11 is a union of @xmath0 hyperplanes and that there is no noise",
    ". then @xmath31 must be of the form in . in that case",
    "@xmath58 is simply the cosine of the angle between the normals to the hyperplanes that are associated with points @xmath53 and @xmath54 .",
    "if both points lie in the same hyperplane , their normals must be equal , and hence @xmath59 . otherwise , @xmath60 is the cosine of the angles between the hyperplanes .",
    "thus , assuming that these angles are not small , and that the points are well distributed on the union of the hyperplanes , spectral clustering on the affinity matrix @xmath57 will in general yield the correct clustering .",
    "even though sasc - a is much more robust in the presence of noise than purely algebraic methods for the case of a union of hyperplanes , it is fundamentally limited by the fact that it applies only to unions of hyperplanes . indeed , if the orthogonal complement of a subspace @xmath61 has dimension greater than @xmath62 , there may be points @xmath63 inside @xmath61 such that the angle between @xmath64 and @xmath65 is as large as @xmath66 .",
    "in such instances , points associated to the same subspace may be weakly connected and thus there is no guarantee for the success of spectral clustering .",
    "* abstract filtration scheme . * motivated by the limitation of the polynomial differentiation algorithm to a known number of subspaces , and the association of undesired _ ghost - subspaces _ with the recursive method of @xcite , an alternative algebraic subspace clustering procedure based on _ filtrations of subspace arrangements _ was proposed in @xcite .",
    "the procedure is abstract in the sense that it receives as input a union @xmath67 of an unknown number of subspaces of arbitrary dimensions , and it decomposes it to the list of its constituent subspaces .",
    "this is done recursively by identifying a single subspace each time : @xmath11 is intersected with the hyperplane @xmath68 , whose normal vector is the gradient of a vanishing polynomial at a point @xmath69 .",
    "then @xmath68 contains the subspace @xmath61 associated to @xmath42 and so does the new _ smaller _ union of subspaces @xmath70 .",
    "next @xmath71 is intersected with a hyperplane @xmath72 of @xmath68 , whose normal is the gradient of a vanishing polynomial of @xmath71 evaluated at @xmath42 .",
    "as before , @xmath73 contains @xmath61 and the process repeats until no non - zero vanishing polynomial exists , in which case @xmath61 is precisely @xmath74 . by picking a point",
    "@xmath75 a new subspace @xmath76 is identified and so on .",
    "this method has very strong theoretical guarantees ( for noiseless data ) but is fairly abstract in nature .",
    "it is the very purpose of the remaining of this paper to adapt the work of @xcite to a numerical algorithm and to experimentally demonstrate its merit .",
    "in this section , we propose a new subspace clustering procedure which addresses the robustness of asc with respect to noise and unknown subspace dimensions , especially in the case of subspaces of varying dimensions .",
    "our first contribution is to replace the angle - based affinity in by a distance - based - affinity and to show that the new affinity possesses superior theoretical guarantees",
    ".    given unit norm data points @xmath77 lying close to an unknown union of @xmath0 subspaces , let @xmath31 be an approximate vanishing polynomial whose coefficients are given by the right singular vector of @xmath51 associated with its smallest singular value .",
    "we define the _ distance - based - affinity _ as @xmath78 where the gradient vectors are assumed to be normalized to unit euclidean norm",
    ". we will refer to this spectral asc method with the _ distance - based affinity _ in sasc - d .",
    "the denomination _ distance - based _ comes from the fact that the euclidean distance from point @xmath54 to the hyperplane @xmath79 defined by the unit normal vector @xmath80 is precisely @xmath81 .",
    "moreover , @xmath79 contains the subspace passing through @xmath53 .",
    "thus , if @xmath53 and @xmath54 are in the same subspace , then the distance from @xmath54 to @xmath79 is zero and so is the distance from @xmath53 to @xmath82 .",
    "this implies that @xmath83 .",
    "of course , it may be the case that @xmath84 for points @xmath53 and @xmath54 coming from distinct subspaces .",
    "for instance , consider a union of two lines in @xmath28 and choose a plane containing one of the lines . if the plane happens to contain the two lines , then @xmath84 for all pairs of points in the two lines .",
    "[ thm : dba ] let @xmath85 be points of @xmath6 lying in a union of @xmath0 subspaces @xmath11 .",
    "let @xmath31 be a homogeneous polynomial of any degree vanishing on @xmath11 .",
    "then the distance - based affinity in is such that if points @xmath86 lie in the same subspace , then @xmath87 .",
    "the converse is not true in general .",
    "theorem [ thm : dba ] shows the superiority of the distance - based affinity in over the angle - based affinity in because it ensures that points from the same subspace will be given an affinity of maximal value @xmath62 .",
    "what still limits the theoretical guarantees of is the fact that points from distinct subspaces may also have a maximal affinity @xmath62 . in this section ,",
    "we show that it is possible to further refine by a filtration process illustrated in figure [ fig : filtration ] .",
    "let @xmath88 be a set of points in @xmath6 in general position in a union of @xmath0 transversal subspaces @xmath89 .",
    "assume that each point lies in only one of the @xmath0 subspaces and is normalized to have unit norm .",
    "the key idea behind the filtration process is that , given an arbitrary _ reference point _",
    "@xmath90 in one of the subspaces , say @xmath61 , we can identify all other data points in the same subspace as @xmath42 by 1 ) projecting all data points in @xmath8 onto @xmath61 and 2 ) finding the points in @xmath8 whose norm after projection remains equal to one .",
    "@xmath91    the fundamental challenge , however , is that we do not know @xmath61 . the filtration process in figure [ fig : filtration ] is designed , precisely , to perform a sequence of projections , which ultimately give the projection onto @xmath61 without knowing @xmath61 .",
    "at step 1 of the filtration , choose a vanishing polynomial @xmath92 of @xmath11 of degree @xmath0 from the nullspace of @xmath50 such that @xmath93 .",
    "one can show that such a @xmath92 always exists .",
    "let @xmath94 and let @xmath95 be the hyperplane of @xmath6 defined by @xmath96 . if @xmath97 , then by theorem [ thm : dba ] we know that point @xmath53 is not in @xmath61 .",
    "consequently , we can filter the set @xmath8 to obtain a subset @xmath98 .",
    "geometrically , @xmath99 is precisely the subset of @xmath8 that lies inside the hyperplane @xmath95 , , @xmath100 .",
    "the key observation now is that @xmath99 is a set of points of @xmath6 drawn from the union of subspaces @xmath101 .",
    "but @xmath71 is up to isomorphism a union of subspaces of @xmath102 , since it is embedded in the hyperplane @xmath95 .",
    "in particular , consider the composite linear transformation @xmath103 , where the first arrow is the orthogonal projection of @xmath6 onto @xmath95 and the second arrow maps a basis of @xmath95 to the standard basis of @xmath102 .",
    "we can replace the redundant representation @xmath99 by @xmath104 .",
    "it is important to note that the norm of every point in @xmath99 remains unchanged and equal to @xmath62 under the transformation @xmath105 .",
    "note also that now @xmath106 may be actually a subset of a union of @xmath107 subspaces of @xmath102 , as it is quite possible that all points in @xmath8 lying in some subspace @xmath108 were filtered out .",
    "now , since @xmath8 is in general position inside @xmath11 , @xmath109 will be in general position inside @xmath110 , and from this one can deduce that every vanishing polynomial of @xmath106 has gradient orthogonal to @xmath111 at @xmath112 , and that there is a vanishing polynomial @xmath113 of degree @xmath114 such that @xmath115 .",
    "let @xmath116 be a hyperplane of @xmath102 defined by the normal vector @xmath117 .",
    "note that @xmath116 contains all the points of @xmath106 that correspond to @xmath61 .",
    "as before , we can filter the set @xmath106 to obtain a new set @xmath118 .",
    "once again , @xmath119 lies in a union of at most @xmath120 subspaces of @xmath102 , which is however embedded in the hyperplane @xmath116 , and thus we can replace @xmath119 by its image @xmath121 under the composite linear transformation @xmath122 , in which the first arrow is the orthogonal projection of @xmath102 onto @xmath116 , and the second arrow maps a basis of @xmath116 to the standard basis of @xmath123 . proceeding inductively , this process will terminate precisely after @xmath124 steps , where @xmath125 is the codimension of @xmath61 . more specifically",
    ", there will be no non - zero vanishing polynomials on @xmath126 and @xmath127 will be isomorphic to @xmath61 .",
    "thus @xmath128 will consist of the images of the points of @xmath129 under the sequence of transformations @xmath130 .",
    "we note that the norm of these points remains unchanged and equal to @xmath62 under @xmath130 .",
    "once the points of @xmath8 that lie in @xmath61 have been identified , we can remove them and repeat the process starting with the set @xmath131 , which lies in general position inside @xmath132 .",
    "this leads to algorithm [ alg : fasc ] , which we term _ filtrated - algebraic - subspace - clustering _ ( fasc ) , and is guaranteed to return the correct clustering :    @xmath133 ; @xmath134 ; @xmath135 ; take any @xmath136 , @xmath137 ; @xmath138 ; find @xmath139 s.t .",
    "@xmath140 ; @xmath141 $ ] ; @xmath142 ; @xmath143 ; @xmath144 ; @xmath145 ; @xmath146 ; @xmath147 ; @xmath148 ; @xmath149 ;    [ thm : fasc ] let @xmath150 be points of @xmath6 lying in a transversal union of subspaces @xmath151 .",
    "let @xmath152 $ ] .",
    "assuming that the points of @xmath8 are in general position inside @xmath11 , algorithm [ alg : fasc ] returns a set @xmath153 such that @xmath154 , where @xmath155 is a permutation on @xmath0 symbols .",
    "let us now consider the case where the data @xmath8 are corrupted by noise . in this case ,",
    "algorithm [ alg : fasc ] ( fasc ) is not applicable because the noisy embedded data matrix @xmath51 is in general full rank .",
    "nonetheless , we will show next that we can still exploit the insights revealed by the theoretical guarantees of fasc to construct a robust - asc algorithm .    to begin with ,",
    "note that algorithm [ alg : fasc ] requires a single vanishing polynomial at each step of each filtration .",
    "we can use any approximate vanishing polynomial at step @xmath156 .",
    "for example , letting @xmath157 be the points that have passed through the filtration at step @xmath158 , we can let @xmath159 be the polynomial whose coefficients are given by the right singular vector of @xmath160 corresponding to its smallest singular value .",
    "notice that no thresholding is required to choose such a @xmath159 .",
    "this is in sharp contrast to the polynomial differentiation algorithm described in section [ section : asc - overview ] , which requires a thresholding on the singular values of @xmath51 in order to estimate a basis of @xmath45 .",
    "now , for any point @xmath48 , @xmath159 gives a hyperplane @xmath161 that approximately contains the subspace associated to point @xmath42 .",
    "however , we can not go to the next step due to the following problems .",
    "[ prb : whichpoint ] in general , two points lying approximately in the same subspace @xmath61 will produce different hyperplanes that approximately contain @xmath61 with different levels of accuracy . in the noiseless case",
    "any point would be equally good . in the presence of noise though",
    ", the choice of the reference point @xmath42 becomes significant . how should @xmath42 be chosen ?",
    "[ prb : threshold ] given a hyperplane produced by a point @xmath42 , we need to determine which other points in @xmath8 lie approximately in the hyperplane and filter out the remaining points .",
    "a simple approach is to filter out a point if its distance to the hyperplane is above a threshold @xmath162 , or if the relative change in its norm is more than @xmath162 .",
    "clearly the choice of @xmath162 will affect the performance of the algorithm . how should @xmath162 be chosen ?",
    "[ prb : codimension ] finally , we also need to determine the number of steps needed to stop the filtration .",
    "this is equivalent to determining the codimension of the subspace associated to the reference point of that filtration . in the noiseless case ,",
    "one stops when the norm of the reference point becomes less than @xmath62 . in the noisy case , because the hyperplanes used to construct the filtration are only approximate , the norm of the reference point could drop at every step of the filtration .",
    "hence a suitable stopping criterion needs to be devised .",
    "inspired be the sasc - d algorithm , which handles noise by computing a normal vector for each data point and uses the normal vectors to define a distance - based affinity , we propose to address problem [ prb : whichpoint ] by constructing a filtration for each data point @xmath163 with reference point @xmath53 and using the norms of the data points to construct the affinity .",
    "let @xmath164 be the projection at step @xmath156 of the filtration for point @xmath53 . recall that at step @xmath156 , only a subset of the original points will remain , while others will be filtered out .",
    "we can define an affinity matrix as @xmath165 this affinity captures the fact that if points @xmath53 and @xmath54 are in the same subspace , then the norm of @xmath54 should not change from step @xmath166 to step @xmath156 of the filtration computed with reference point @xmath53 .",
    "otherwise , if @xmath53 and @xmath54 are in different subspaces , the norm of @xmath54 is expected to be reduced by the time the filtration reaches step @xmath167 , where @xmath61 is the reference subspace associated to @xmath53 . in the case of noiseless data , only the points in the correct subspace survive step @xmath124 and their norms are precisely equal to one . therefore , @xmath168 if points @xmath53 and @xmath54 are in the same subspace and @xmath169 otherwise . in the case of noisy data",
    ", the above affinity will not be perfect due to problems [ prb : threshold ] and [ prb : codimension ] , which we address next .    to address problem [ prb : threshold ] , let @xmath31 be the approximate vanishing polynomial whose coefficients are the right singular vector of @xmath51 corresponding to the smallest singular value .",
    "let @xmath170 notice that @xmath171 in the noiseless case . in the presence of noise , @xmath172 is the average over all points of the distance of a point from the hyperplane that it produces .",
    "evidently , small levels of noise will correspond to small values of @xmath172 .",
    "thus , we propose to define @xmath173 , where @xmath174 is a user defined parameter . to determine @xmath175",
    ", we propose to construct multiple filtrations for different values @xmath176 of @xmath175 .",
    "each filtration will result in a different affinity matrix .",
    "suppose we have defined a stopping criterion to terminate each filtration so that we can use the affinity matrix at the last step of the filtration ( see below for stopping criteria ) .",
    "given these affinity matrices , we choose the one whose normalized laplacian has the largest eigengap @xmath177 , where the eigenvalues are ordered increasingly .",
    "to address problem [ prb : codimension ] , we stop the filtration at step @xmath156 if 1 ) the number of points is less than the ambient dimension of the veronese - embedded points ; 2 ) the reference point @xmath53 is filtered out at the @xmath178th step ; or 3 ) the number of points that passed through the filtration at step @xmath179 is less than some integer @xmath180 .",
    "this integer is the smallest number of points that our algorithm is allowed to consider as a cluster .",
    "finally , the resulting affinity is symmetrized , and used for spectral clustering , as described in algorithm [ alg : fsasc ] .",
    "denotes the spectrum of the normalized laplacian matrix of @xmath181 , @xmath182 denotes spectral clustering being applied to @xmath183 to obtain @xmath0 clusters , and @xmath184 is the polynomial whose coefficients are the right singular vector of @xmath51 corresponding to the smallest singular value . ]",
    "0.8    [ 1 ] ( not enough points ) ; eigengap @xmath185 ; @xmath186 ; @xmath187 $ ] ; @xmath188 ; @xmath189 ; @xmath190 ; @xmath191 ; @xmath192 ; eigengap @xmath193 ; @xmath194 ; @xmath195 ; @xmath196 ;    @xmath197 ,   q \\gets p , \\boldsymbol{c } \\gets 0_{1 \\times n}$ ] ; flag @xmath198 ; @xmath199 $ ] ; @xmath200 $ ] ; flag @xmath185 ; @xmath201 : \\frac{||\\x_{j'}|| - || \\pi(\\x_{j'})||}{||\\x_{j'}|| } \\le \\delta \\right\\}$ ] flag @xmath185 ; @xmath202 ; @xmath203- \\j$ ] ; flag @xmath185 ; @xmath204 ; @xmath205 ; @xmath206 ; @xmath207 ;    ( @xmath208 ) ;",
    "* synthetic data .",
    "* we randomly generate @xmath209 subspaces of dimensions @xmath210 in @xmath211 . for each choice of @xmath212 ,",
    "we randomly generate @xmath213 unit norm points per subspace and add zero - mean gaussian noise with standard deviation @xmath214 in the direction orthogonal to the subspace . for each choice of @xmath212 and @xmath215 , we perform @xmath216 independent subspace clustering experiments using the algebraic methods fsasc , sasc - d and sasc - a , and compare to state of the art methods such as ssc @xcite , lrr @xcite , lrsc @xcite and lsr using equation ( 16 ) in @xcite .",
    "we also use the heuristic post processing of the affinity for lrr ( lrr - h ) and lsr ( lsr - h ) . for fsasc",
    "we use @xmath217 and @xmath218 , for ssc @xmath219 and @xmath220 , for lrr @xmath221 , for lrsc @xmath222 and for lsr @xmath223 .",
    "we report average clustering errors , intra - cluster connectivities of the affinity matrices @xmath57 produced by the methods ( defined to be the minimum _",
    "_ algebraic connectivity _ _ among the subgraphs corresponding to each of the three subspaces ) and inter - cluster connectivities ( @xmath224 ) .",
    "due to lack of space , we report errors on all methods only for @xmath225 and @xmath226 noise .",
    "table [ table : synthetic - error ] reports the mean clustering errors .",
    "observe that fsasc is the only method that gives @xmath166 error for noiseless data for all dimension configurations , thus verifying experimentally its strong theoretical guarantees : no restrictions on the dimensions of subspaces are required for correctness .",
    "as expected , ssc , lrr , lrsc and lsr yield perfect clustering when @xmath227 is small , but their performance degrades significantly for large @xmath227 .",
    "observe also that , although sasc - d is much simpler than fsasc and has similar complexity to sasc - a , its performance is very close to that of fsasc , and much better than sasc - a .",
    "we attribute this phenomenon to the correctness theorem [ thm : dba ] of sasc - d . as the noise level increases ,",
    "fsasc remains stable across all dimension configurations with superior behavior among all compared methods .",
    "sasc - d is less robust in the presence of noise , except for the case of hyperplanes , in which it is the best method .",
    "this phenomenon is expected , since sasc - d is essentially equivalent to fsasc if the latter is configured to take only one step in each filtration in figure [ fig : filtration ] , and this is precisely the optimal stopping point in every filtration when the subspaces are hyperplanes . in this case , if data are noisy , the criterion for stopping fsasc filtrations is determined by the parameter @xmath175 and by the level of noise via the quantity @xmath228 , leading to suboptimal values ( , more than one step may be taken in the filtration ) .",
    "tables [ table : synthetic - intra ] and [ table : synthetic - inter ] indicate that fsasc yields higher quality affinity graphs for the purpose of clustering . to see why this is the case , observe that except for fsasc , we can distinguish two kinds of behavior in the remaining methods : the first kind gives high intra - cluster connectivity at the cost of high inter - cluster connectivity .",
    "such methods are sasc - d , sasc - a , lrr , lrsc and lsr .",
    "the second kind gives low inter - cluster connectivity at the expense of low intra - cluster connectivity leading to unstable clustering results by the spectral clustering method .",
    "such methods are ssc , lrr - h and lsr - h .",
    "this is expected because these methods use sparse affinities . on the other hand",
    ", fsasc circumvents this trade - off by giving high intra - cluster connectivity and low inter - cluster connectivity , thus enhancing the success of the spectral clustering step .",
    ".mean clustering error in @xmath229 over @xmath216 independent experiments on synthetic data for @xmath230 subspaces of @xmath211 of varying dimensions @xmath231 and varying levels of noise @xmath232 . [ cols=\"<,^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "we presented a novel algebraic subspace clustering method based on the geometric idea of filtrations and we experimentally demonstrated its robustness to noise using synthetic and real data and its superiority to the state - of - the - art algorithms on several occasions .",
    "overall , the method works very well for subspaces of arbitrary dimensions in a low - dimensional ambient space , and it can handle higher dimensions via a projection .",
    "the main weakness of the method is its high computational complexity , which comes from the large number of filtrations required , as well as the exponential cost of fitting polynomials to @xmath0 subspaces .",
    "future research will be concerned with reducing the complexity , as well as dealing with outliers and missing entries ."
  ],
  "abstract_text": [
    "<S> algebraic subspace clustering ( asc ) is a simple and elegant method based on polynomial fitting and differentiation for clustering noiseless data drawn from an arbitrary union of subspaces . in practice , however , asc is limited to equi - dimensional subspaces because the estimation of the subspace dimension via algebraic methods is sensitive to noise . </S>",
    "<S> this paper proposes a new asc algorithm that can handle noisy data drawn from subspaces of arbitrary dimensions . </S>",
    "<S> the key ideas are ( 1 ) to construct , at each point , a decreasing sequence of subspaces containing the subspace passing through that point ; ( 2 ) to use the distances from any other point to each subspace in the sequence to construct a subspace clustering affinity , which is superior to alternative affinities both in theory and in practice . </S>",
    "<S> experiments on the hopkins 155 dataset demonstrate the superiority of the proposed method with respect to sparse and low rank subspace clustering methods . </S>"
  ]
}