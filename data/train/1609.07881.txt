{
  "article_text": [
    "here we show again the trajectories taken by different algorithms as in fig .  1 in the main text , but now plotted against time rather than iterative steps .    the deviation @xmath44 versus time taken for different algorithms , for the experimental data of @xcite . , height=238 ]",
    "we discuss various technical details pertaining to the apg and/or cg - apg algorithms described in the main text .",
    "as explained in the main text , the apg algorithm relies on a projection @xmath73 to enforce the quantum constraints after each gradient step .",
    "the argument of @xmath73 is a hermitian operator @xmath126 with eigenvalues @xmath127 ( in descending order ) and eigenvectors @xmath128 .",
    "one projects @xmath129 onto the probability simplex so that @xmath130 with @xmath131 and @xmath132 , and then rebuilds the operator with @xmath133 , i.e. , @xmath134 . the projection of @xmath127 onto the simplex is done as follows @xcite : find @xmath135 , then define @xmath136 .",
    "finally we have @xmath137 .      during the gradient step of apg",
    ", one can wind up outside the physical state space , i.e. , @xmath138 at each iterative step need not be a valid state .",
    "it can even happen that not all @xmath139s needed in the iterative step are positive , for which @xmath140 is ill - defined because of the logarithm .",
    "we can prevent this by checking whether any @xmath141 is negative after @xmath138 is computed , and set @xmath142 if this happens to be the case .",
    "empirically , we observe such cases to occur only very rarely .",
    "we also incorporated a few small adjustments to apg recommended in ref .",
    "@xcite , as well as the barzilai - borwein method for computing step sizes @xcite , for better step - size estimation and improved performance in the implementation of the cg - apg algorithm used to produce the figures in the main text .",
    "we list those adjustments here .",
    "first , for iterative step @xmath143 , rather than fixing the step size as @xmath144 , we set @xmath145 if there was no restart in the previous iteration and the denominator is nonzero ; otherwise we set @xmath146 , for some pre - chosen constant @xmath10 .",
    "we used @xmath147 and @xmath148 ( see main text ) as recommended in @xcite .",
    "we also use the following update on @xmath149 and @xmath138 for @xmath143 to prevent changes in @xmath150 from affecting convergence :    @xmath151    the rules eqs .   and are exactly those stated in the apg algorithm in the main text , but with @xmath152 replaced by @xmath153 for the step - size adjustment .",
    "sometimes we observe that standard apg as prescribed by @xcite fails to restart early enough for good performance .",
    "we hence use a stricter restart criterion : restart when @xmath154 with @xmath155 set to a small positive value ( @xmath156 for the graphs in the main text ) .      for the cg - apg algorithm , as explained in the main text",
    ", one would like to start with cg iterations and switch to apg when the hessian stabilizes , i.e. , it changes only by a little with further apg steps .",
    "this happens when the trajectory is sufficiently close to the mle .",
    "here , we explain the technical details of this switchover .",
    "the hessian of @xmath28its curvature  characterizes its local quadratic structure .",
    "it is the  second derivative \" of @xmath28 , and comes from considering the second - order variation of @xmath27 : @xmath157 , where @xmath158 , the first - order variation of @xmath27 , with @xmath159 and @xmath160 as in the main text . here ,",
    "@xmath31 and @xmath161 are independent infinitesimal variations of @xmath11 .",
    "a little algebra gives @xmath162 and we identify the linear operator [ on @xmath23 ] @xmath164 as the hessian of @xmath27 at @xmath11",
    ".    the eigenvalues of @xmath165 give the local quadratic structure of @xmath28 .",
    "ideally , determining the right time during cg to switch to apg requires computing how much @xmath165 changes across successive apg steps from the current value of @xmath11 .",
    "however , this would be very costly : it is as if one is running apg alongside cg , and the hessian is a large matrix ( @xmath166 in size ) and hence expensive to compute .",
    "instead , we adopt a compromise that works well in practice : ( 1 ) we treat the @xmath9s as if they were all mutually orthogonal so that the eigenvalues of @xmath165 would be equal to @xmath167 , and ( 2 ) we look at the change in @xmath165 between iterations of cg instead of between iterations of apg .",
    "the @xmath9s are never exactly mutually orthogonal for informationally complete measurements , but a good tomographic design would seek to spread out the @xmath9 directions , and for large dimensional situations , their mutual overlaps will be small and @xmath167 is a good enough proxy for the eigenvalues of the hessian .",
    "while looking at the change in @xmath165 across iterations of cg would not always guarantee a similar change for apg , a small change with cg iterations signals closeness to the mle , or that cg has stagnated . in either case",
    ", one should switch to apg .",
    "thus , in our implementation of cg - apg , we first initialize cg with the maximally mixed state , and switch to apg at the first iteration when the overlap , @xmath168 exceeds @xmath169 for some chosen small @xmath170 value . here , @xmath171 , where @xmath172 for state @xmath43 of the @xmath173th cg iteration .",
    "the switchover thus occurs when the angle between the @xmath174s for subsequent iterations is small enough .",
    "we find that @xmath175 radians works well in practice .",
    "figure 1 in the main text shows the trajectories taken by different algorithms for the experimental data of ref .",
    "@xcite for a noisy @xmath50-state .",
    "there , we saw a long initial slow phase of apg , which is in fact atypical of the behavior seen for generic states .",
    "figure [ fig : suppmat2 ] shows the more representative behavior for a random 8-qubit pure state with 10% added white noise .",
    "as in fig .  1 , @xmath47 , with 100 copies for each of the @xmath48 settings of the 8-qubit product - pauli pom .",
    "observe the significantly shorter length of the initial slow phase of apg than for the noisy @xmath50-state in fig .  1 .",
    "the plot shows the trajectories taken by different algorithms as in fig .  1 in the main text , but for simulated data generated from a random 8-qubit pure state with 10% added white noise .",
    ", height=238 ]",
    "here , we present the counting argument that gives @xmath94 as the computational cost of evaluating a full set of born probabilities after making use of the product structure of the pom . to remind the reader of the notation :",
    "the system comprises @xmath0 registers each of dimension @xmath86 ; the pom on each register is @xmath87 ; the @xmath0-register pom outcome is @xmath88 , with @xmath89 and @xmath90 ; and @xmath176 .",
    "we also need the following basic fact : evaluating @xmath177 for @xmath34 an @xmath178 matrix and @xmath179 an @xmath180 matrix requires @xmath181 operations ( elementary addition / multiplication ) .    in each step of the procedure described in the main text",
    ", one needs to evaluate @xmath182 for given @xmath183 .",
    "one such evaluation requires the computation of the trace of @xmath108 with each of the @xmath115 submatrices of @xmath107 . @xmath108 and each submatrix are @xmath114 in size , so the computational cost of evaluating @xmath106 is @xmath184 operations .",
    "one incurs this cost for every choice of @xmath183 , so the total cost of evaluating @xmath106 for all @xmath183 , for given @xmath185 , is @xmath186 . adding up this cost over all values of @xmath187 gives",
    "the total cost for evaluating a full set of born probabilities as @xmath188}.\\end{aligned}\\ ] ] for @xmath189 , as is usually the case , this gives the dominant computational cost of @xmath94 ; for @xmath190 , one has instead the cost of @xmath191 .",
    "m.  grant and s.  boyd , _ graph implementations for nonsmooth convex programs _",
    ", in v.  blondel , s.  boyd , and h.  kimura , eds .",
    ", _ recent advances in learning and control _ , lecture notes in control and information sciences , ( springer , 2008 ) .",
    "h.  hffner , w.  hnsel , c.  f.  roos , j.  benhelm , d.  chek - al - kar , m.  chwalla  , t.  krber , u.  d.  rapol , m.  riebe , p.  o.  schmidt , c.  becher , o.  ghne , w.  dr , and r.  blatt , nature ( london ) * 438 * , 643 ( 2005 ) .",
    "r.  j.  bruck , j.  math .",
    "appl .  * 61 * , 159 ( 1977 ) .",
    "g.  b.  passty , j.  math .",
    "appl .  * 72 * , 383 ( 1979 ) .",
    "y.  nesterov , _ introductory lectures on convex optimization : a basic course _ , kluwer academic , dordrecht ( 2004 ) ."
  ],
  "abstract_text": [
    "<S> conventional methods for computing maximum - likelihood estimators ( mle ) often converge slowly in practical situations , leading to a search for simplifying methods that rely on additional assumptions for their validity . in this work </S>",
    "<S> , we provide a fast and reliable algorithm for maximum likelihood reconstruction that avoids this slow convergence . </S>",
    "<S> our method utilizes an accelerated projected gradient scheme that allows one to accommodate the quantum nature of the problem in a different way than in the standard methods . </S>",
    "<S> we demonstrate the power of our approach by comparing its performance with other algorithms for @xmath0-qubit state tomography . in particular , an 8-qubit situation that purportedly took weeks of computation time in 2005 can now be completed in under a minute for a single set of data , with far higher accuracy than previously possible . </S>",
    "<S> this refutes the common claim that mle reconstruction is slow , and reduces the need for alternative methods that often come with difficult - to - verify assumptions . </S>",
    "<S> the same algorithm can be applied to general optimization problems over the quantum state space ; the philosophy of projected gradients can further be utilized for optimization contexts with general constraints .    _ </S>",
    "<S> introduction. _ efficient and reliable characterization of properties of a quantum system , for example , its state or the process it is undergoing , is needed for the success of any quantum information processing task . </S>",
    "<S> such are the goals of quantum tomography @xcite , broadly classified into state tomography and process tomography . </S>",
    "<S> process tomography can be recast as state tomography via the well - known choi - jamiolkowski isomorphism @xcite ; we hence restrict our attention to state tomography . </S>",
    "<S> tomography is a two - step process : the first is data gathering via appropriate measurements of the quantum system ; the second is the estimation of the state from the gathered data . </S>",
    "<S> this second step is the focus of this article .    </S>",
    "<S> a popular estimation strategy is that of the maximum - likelihood estimator ( mle ) @xcite from standard statistics , a matter of convex optimization . </S>",
    "<S> computing the mle for quantum tomography is , however , not straightforward due to the constraints imposed by quantum mechanics . while general - purpose and easy - to - use convex optimization toolboxes ( e.g. , cvx @xcite ) are available for small - sized problems , it is clear that specially adapted mle algorithms are needed for tackling useful system sizes . </S>",
    "<S> past mle algorithms @xcite incorporate the quantum constraints by going to the _ factored space _ </S>",
    "<S> ( see definition later ) where the quantum constraints are satisfied by construction via a many - to - one map back to the state space . </S>",
    "<S> gradient methods can then be straightforwardly employed in the now - unconstrained factored space . </S>",
    "<S> these algorithms can be slow in practice , with an extreme example @xcite of an 8-qubit situation purportedly ( see refs .  </S>",
    "<S> @xcite ) requiring _ weeks _ of computation time , to find the mle , together with bootstrapped error bars ( 10 mle reconstruction in all ) , for the measured data @xcite . </S>",
    "<S> this has triggered a search for alternative approaches to mle reconstruction @xcite , specializing to circumstances in which certain assumptions about the system are applicable , permitting simpler and hence , faster , reconstruction .    yet </S>",
    "<S> , the mle approach provides a principled estimation strategy , and is still one of the most popular methods for experimenters . </S>",
    "<S> the mle gives a justifiable point estimate for the state @xcite . </S>",
    "<S> it is the natural starting point for different ways of quantifying the uncertainty in the estimate : one can bootstrap the measured data @xcite and quantify the scatter in the mles for simulated data ; confidence regions can be established starting from the mle point estimator ( this is standard in statistics , but a recent discussion can be found in @xcite ) ; credible regions for the actual data are such that the mle is the unique state contained in every error region @xcite . </S>",
    "<S> it is thus worthwhile to pursue better methods for finding the mle .    here </S>",
    "<S> , we present a fast algorithm to accurately compute the mle from tomographic data . the computation of the mle for a single set of data for the 8-qubit situation mentioned above now takes less than a minute , and returns a far more accurate answer than previous algorithms in the same amount of time . </S>",
    "<S> the speedup and accuracy originate from two features introduced here : ( i ) the  cg - apg \" algorithm that combines an accelerated projected - gradient ( apg ) approach , which overcomes convergence issues of previous methods , with the existing conjugate - gradient ( cg ) algorithm ; ( ii ) the use of the product structure ( if present ) of the tomographic measurements to speed up each iterative step . </S>",
    "<S> the cg - apg algorithm gives faster and more accurate reconstruction whether or not the tomographic measurements are of product structure ; the product structure , if present , can also be employed to speed up previous mle algorithms .    </S>",
    "<S> _ the problem setup_. in a typical quantum tomography scenario , @xmath1 independently and identically prepared copies of the quantum state are measured one - by - one via a set of measurement outcomes @xmath2 , with @xmath3 @xmath4 and @xmath5 . </S>",
    "<S> @xmath2 is formally known as a povm ( positive operator - valued measure ) or a pom ( probability - operator measurement ) . </S>",
    "<S> the measured data @xmath6 consist of a sequence of detection events @xmath7 , where @xmath8 records the click of the detector for outcome @xmath9 for the @xmath10th copy measured . </S>",
    "<S> the likelihood for data @xmath6 given state @xmath11 is @xmath12}^{f_k}\\right\\}}^n\\,,\\ ] ] where @xmath13 is the probability for outcome @xmath9 , @xmath14 is the total number of clicks in detector @xmath15 , and @xmath16 is the relative frequency .    </S>",
    "<S> the mle strategy views the likelihood as a function of @xmath11 for the obtained @xmath6 , and identifies the quantum state @xmath11 ( the statistical operator or density matrix ) , with @xmath17 and @xmath18 , that maximizes @xmath19 as the best guess  the mle @xmath20 . </S>",
    "<S> this can be phrased as an optimization problem for the normalized negative log - likelihood , @xmath21 :    @xmath22    the domain here is the space of bounded operators @xmath23 on the @xmath24-dimensional hilbert space @xmath25 . </S>",
    "<S> we refer to as the quantum constraints . </S>",
    "<S> any @xmath26 satisfying is a valid state ; the convex set of all valid states is the quantum state space . </S>",
    "<S> @xmath27 is convex , and hence has a unique minimum value , on the quantum state space . </S>",
    "<S> furthermore , @xmath28 is differentiable ( except at isolated points ) with gradient @xmath29 , so that @xmath30 for infinitesimal unconstrained @xmath31 .    </S>",
    "<S> _ the problem of slow convergence_. previous mle algorithms @xcite converge slowly to the mle because of the  by - construction \" incorporation of the quantum constraints : one writes @xmath32 for @xmath33 , and performs gradient descent in the _ factored space _ of unconstrained @xmath34 operators , for @xmath35 . straightforward algebra yields @xmath36 to linear order in @xmath37 . </S>",
    "<S> @xmath38 is negative  hence walking downhill  for @xmath39 , for a suitably chosen small @xmath40 . </S>",
    "<S> this choice of @xmath37 prescribes a @xmath11-update of the form @xmath41\\ ] ] to linear order in @xmath40 . </S>",
    "<S> @xmath42 comprises two terms , each with @xmath43 as a factor . when the mle is close to the boundary of the state space </S>",
    "<S>  a typical situation when there are limited data ( unavoidable in high dimensions ) for nearly pure true states@xmath43 eventually gets close to a rank - deficient state and has at least one small eigenvalue . </S>",
    "<S> yet , @xmath43 has unit trace , so its spectrum must be highly asymmetric . </S>",
    "<S> @xmath42 inherits this asymmetry , leading to a locally ill - conditioned problem and slow convergence .    </S>",
    "<S> the deviation @xmath44 at each step of the iteration for different algorithms , for the experimental data of @xcite . </S>",
    "<S> @xmath45 is the smallest @xmath27 value attained among the algorithms ( reached by the apg and cg - apg algorithms when run till further progress is hindered by machine precision ) ; @xmath46 is the corresponding likelihood value . </S>",
    "<S> here , @xmath47 , for 100 copies for each of the @xmath48 settings of the 8-qubit product - pauli pom . </S>",
    "<S> the dash - dotted line indicates the @xmath27 value obtained in @xcite with the dg algorithm . </S>",
    "<S> , height=238 ]    to illustrate , consider the situation of @xcite : tomography of a ( target ) @xmath49-qubit @xmath50-state via product - pauli measurements . </S>",
    "<S> figure [ fig:8qubit ] shows the trajectories taken by different algorithms from the maximally mixed state to the mle  the minimum of @xmath27for the experimental data of @xcite . </S>",
    "<S> the red and blue lines are for commonly used mle methods : the diluted direct - gradient ( dg ) algorithm @xcite and the cg algorithm with step - size optimization via line search @xcite . </S>",
    "<S> both algorithms walk in the factored space , with dg performing straightforward descent according to eq .  , while cg walks along the conjugate - gradient direction . </S>",
    "<S> the plot shows the dg and cg iterations initially decreasing @xmath27 quickly , but the advances soon stall , with @xmath27 stagnating at values significantly larger than attainable by the apg and cg - apg algorithms ( explained below ) . </S>",
    "<S> note that on average the cg - apg and dg algorithms take about the same time per iterative step ; see appendix [ app0 ] for a graph similar to fig .  [ fig:8qubit ] but plotted against time rather than steps .    _ </S>",
    "<S> the cg - apg algorithm._ the slowdown in convergence for dg and cg puts a severe limit on the accuracy of the mle reconstruction : the analysis of @xcite stopped  after a long wait @xcite  at a state with likelihood @xmath51 . </S>",
    "<S> that was sufficient for the purpose of @xcite to show the establishment of entanglement , but can hardly be considered useful for further mle analysis . </S>",
    "<S> the ill - conditioning in the factored space , which leads to the slowdown in dg and cg , can be avoided by walking in the @xmath11-space . </S>",
    "<S> there , @xmath28 has gradient @xmath52 which , unlike that of @xmath53 , is not proportional to @xmath11 . </S>",
    "<S> walking in the @xmath11-space , however , does not ensure the quantum constraints are satisfied . </S>",
    "<S> the constraints are instead enforced by projecting the unconstrained @xmath11 operator back into the quantum state space after each gradient step . </S>",
    "<S> this is an example of the well - studied and often - used  projected - gradient \" methods in numerical optimization @xcite .    in steepest - descent methods , </S>",
    "<S> the local condition number of the merit function [ @xmath28 or @xmath53 here ] affects convergence . </S>",
    "<S> poor conditioning leads to a steepest - descent direction that oscillates back and forth . </S>",
    "<S> one smooths out the approach to the minimum by giving each step some  momentum \" from the previous step . </S>",
    "<S> the cg method implements this for quadratic merit functions ; for projected gradients , accelerated gradient schemes @xcite are instead the focus . coupled with adaptive restart @xcite </S>",
    "<S> , the apg method can be thought of as indirectly probing the local condition number by gradually increasing the amount of momentum preserved ( controlled by @xmath54 in the algorithm below ) , and resetting ( @xmath55 ) whenever the momentum causes the current step to point too far from the steepest - descent direction . </S>",
    "<S> the apg algorithm of refs .  </S>",
    "<S> @xcite , in @xmath11-space , thus proceeds as follows :    given @xmath56 , @xmath57 , and @xmath58 . </S>",
    "<S> initialize @xmath59 , @xmath60 . </S>",
    "<S> set @xmath61 , @xmath62 , @xmath63 . </S>",
    "<S> ( choose step size via backtracking ) set @xmath64 . </S>",
    "<S> update @xmath65 , @xmath66 . </S>",
    "<S> set @xmath67 ; termination criterion . </S>",
    "<S> ( restart ) @xmath68 , @xmath69 , @xmath70 ; ( accelerate ) set @xmath71 , @xmath72 .    </S>",
    "<S> the operation @xmath73 above projects the hermitian argument to the nearest state [ satisfying constraints ] as measured by the euclidean distance @xcite . </S>",
    "<S> one can also modify the backtracking portion of the algorithm for better performance ; see appendix [ appa ] for further details .    </S>",
    "<S> applying the apg algorithm to the 8-qubit example above , one indeed finds fast convergence to the mle ( see fig .  </S>",
    "<S> [ fig:8qubit ] ) once the walk brings us sufficiently close ; no slowdown of convergence as seen in dg and cg is observed . </S>",
    "<S> apg with adaptive restart exhibits linear convergence ( i.e. , the deviation from the optimal value decreases exponentially ) in areas of strong convexity @xcite sufficiently close to the minimum point .    </S>",
    "<S> far from the minimum , apg can descend slowly , as is clearly visible in fig .  </S>",
    "<S> [ fig:8qubit ] . </S>",
    "<S> cg descent in the factored space , on the other hand , is rapid in this initial phase . </S>",
    "<S> similar behavior is observed for other states ( see a representative example in appendix [ appb ] ) , although the initial slow apg phase is usually markedly shorter than in the @xmath50-state example here . thus , a practical strategy is to start with cg in the factored space to capitalize on its initial rapid descent , and switch over to apg in the @xmath11-space when the fast convergence of apg sets in , _ provided _ one can determine cheaply when the switch should occur .    </S>",
    "<S> both the apg and cg algorithms use a local quadratic approximation at each step , the accuracy of which relies on the local curvature , measured by the hessian of the merit function . </S>",
    "<S> the advance is quick if the hessian changes slowly from step to step so that prior - step information provides good guidance for the next step . </S>",
    "<S> empirically , for nearly pure true states , we observe that the hessian of @xmath28 changes a lot initially in the apg algorithm but settles down close to the mle . </S>",
    "<S> this is likely a consequence of the fact that the apg trajectory comes very quickly close to the boundary of the state space , so that some @xmath74 values , which occur in the hessian of @xmath28 as @xmath75 , can be very small and unchecked by the @xmath76 values away from the mle . </S>",
    "<S> on the other hand , the hessian of @xmath53 relevant for the cg algorithm is initially slowly changing , but starts fluctuating closer to the mle , likely due to the ill - conditioning in the factored - space gradient discussed previously . with this understanding , </S>",
    "<S> the proposal is then to start with cg in the factored space , perform a test along the way to detect when the hessian of @xmath28 settles down , at which point one switches over to apg in the @xmath11-space for rapid convergence to the minimum . </S>",
    "<S> the hessian itself is , however , expensive to compute ; one can instead get a good gauge by monitoring the different @xmath77 values , cheaply computable from the @xmath74s already used in the algorithm ; see appendix a. this then is finally our cg - apg algorithm , with a superfast approach to the mle that outperforms all other algorithms ; see fig .  </S>",
    "<S> [ fig:8qubit ] .    </S>",
    "<S> _ exploiting the product structure._ part of the speed in the computation of the mle in the 8-qubit example above stems from exploiting the product structure of the situation . for the four algorithms </S>",
    "<S> compared , the most expensive part of the computation is the evaluation of the probabilities @xmath78 needed in @xmath27 and @xmath79 , for @xmath11 at each iterative step . for a @xmath24-dimensional system and @xmath80 pom outcomes , </S>",
    "<S> the computational cost for obtaining the full set of @xmath74s is @xmath81 [ there are @xmath80 probabilities , each requiring @xmath82 operations for the trace of a product of two @xmath83 matrices ] . for the 8-qubit example , @xmath84 , and the pom has @xmath85 outcomes .    </S>",
    "<S> the computational cost can be greatly reduced if one has a product structure : the system comprises @xmath0 registers , and the pom is a product of individual poms on each register . for simplicity , we assume the @xmath0 registers each have dimension @xmath86 , and the pom on each register is the same , written as @xmath87 . </S>",
    "<S> the @xmath0-register pom outcome is then @xmath88 , with @xmath89 and @xmath90 . </S>",
    "<S> the generalization to non - identical registers and poms is obvious . </S>",
    "<S> the total dimension is @xmath91 and @xmath92 . </S>",
    "<S> exploiting this product structure reduces the computational cost of evaluating the probabilities from @xmath93 to @xmath94 ( for @xmath95 ) . for @xmath0 qubits with product - pauli measurements ( @xmath96 , @xmath97 ) , </S>",
    "<S> this is a huge reduction from @xmath98 to @xmath99 .    </S>",
    "<S> the computational savings arise because parts of the evaluation of the probabilities can be re - used . </S>",
    "<S> let @xmath100 , the partial trace on the @xmath0th register , for a given @xmath101 . </S>",
    "<S> this same @xmath102 can be used to evaluate @xmath103 for any @xmath104 . </S>",
    "<S> one does this repeatedly , partial - tracing out the last register each time , until one arrives at the probabilities @xmath105 . at each stage , evaluating @xmath106 from @xmath107 involves computing the trace of @xmath108 with submatrices of @xmath107 . </S>",
    "<S> specifically,@xmath109 where @xmath110 with @xmath111 ( similarly for @xmath112 ) , @xmath113 is a @xmath114 submatrix , and the full @xmath107 is a @xmath115 array of these submatrices . getting @xmath106 from @xmath107 simply requires replacing each submatrix in @xmath107 by the number @xmath116 , which takes @xmath117 computations . since each @xmath107 </S>",
    "<S> need only be computed once for all subsequent @xmath118 , simple counting ( see appendix [ appc ] ) yields a total computational cost of @xmath94 ( for @xmath95 ) to evaluate the full set of probabilities .    </S>",
    "<S> ( color only . ) time taken , for a convergence criterion of @xmath119 , for different algorithms on a varying number of qubits @xmath0 . for each @xmath0 , </S>",
    "<S> 50 states are used , each a haar - random pure state with 10% added white noise to emulate a noisy preparation . for each state </S>",
    "<S> , the different algorithms are run for @xmath120 , where @xmath121 are the born probabilities for the state on the @xmath0-qubit product - pauli pom . </S>",
    "<S> the mle is hence the actual state . </S>",
    "<S> the lines labeled  np \" indicate runs _ without _ using the product structure . </S>",
    "<S> these stop at six qubits due to the long time taken . </S>",
    "<S> the lines are drawn through the average time taken for each algorithm over the 50 states ; the scatter of the timings are shown only for the algorithms using the product structure . for @xmath122 </S>",
    "<S> , cg did not converge within the maximum alloted time ( @xmath123 times that taken by cg - apg / apg ) in 3 out of the 50 states ; these points are plotted with that maximum alloted time ( circled points ) , and the average time taken is hence a lower bound on the actual average for cg . </S>",
    "<S> cg failed to converge in a reasonable time for all states beyond 8 qubits ; dg failed to converge beyond 7 qubits . </S>",
    "<S> , height=245 ]    figure [ fig : nqb ] shows the performance of the different algorithms for a varying number of qubits with and without exploiting the product structure , for the product - pauli measurement . </S>",
    "<S> a significant speedup is visible when the product structure is incorporated . </S>",
    "<S> similar behavior is observed for the commonly used alternative pom , the product - tetrahedron measurement @xcite . for comparison </S>",
    "<S> , we also display the runtime for the general - purpose cvx toolbox for convex optimization @xcite ; the clear disadvantage there is the inability to capitalize on the product structure . </S>",
    "<S> note that cvx does not allow direct specification of a convergence criterion on the @xmath124 value as we have done in the other algorithms ; the plotted points are instead verified after the fact to have an average @xmath124 value much less than @xmath125 . all computations are conducted with _ </S>",
    "<S> matlab _ on a desktop computer ( 3 ghz intel xeon cpu e5 - 1660 ) .    </S>",
    "<S> it is important to note that incorporating the product structure in the mle reconstruction is very different from putting in assumptions about the state or the noise : in the former , one knows the structure by design of the tomographic experiment ; the latter assumptions require additional checks of compliance , which can not be guaranteed to be easy or even possible to do . </S>",
    "<S> one should also note that tomography experiments with systems larger than a couple of qubits typically employ poms with a product structure , because of the comparative ease in design and construction , so this product assumption is very often satisfied in practice .    _ </S>",
    "<S> conclusion._ we have demonstrated that , with the right algorithm , mle reconstruction can be done quickly and reliably , with no latent restriction on the accuracy of the mle obtained . as the dimension increases , </S>",
    "<S> there is no getting around the fact that any tomographic reconstruction will become very expensive , but our algorithm slows the onset of that point beyond the system size currently accessible in experiments . </S>",
    "<S> we note here that our method can be immediately applied to the reconstruction of the mle for process tomography . </S>",
    "<S> furthermore , it is a general method for optimization in the quantum state space or other types of constraints , and hence can also be used in other such problems .    </S>",
    "<S> this work is funded by the singapore ministry of education ( partly through the academic research fund tier 3 moe2012-t3 - 1 - 009 ) and the national research foundation of singapore . </S>",
    "<S> the research is also supported by the national research foundation ( nrf ) , prime minister s office , singapore , under its create programme , singapore - mit alliance for research and technology ( smart ) biosystems and micromechanics ( biosym ) irg . </S>",
    "<S> hkn is partly funded by a yale - nus college start - up grant . </S>",
    "<S> the authors thank c.  roos and o.  ghne for sharing the experimental data of ref .  </S>",
    "<S> @xcite and information about the mle reconstruction used in that work . </S>",
    "<S> zz thanks chenglong bao for his discussions regarding apg and george barbastathis for general discussions .    </S>",
    "<S> j.  shang and z.  zhang contributed equally to this work . </S>"
  ]
}