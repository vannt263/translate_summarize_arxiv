{
  "article_text": [
    "we consider a convex separable linearly constrained optimization problem @xmath0 where @xmath1 and @xmath2 is @xmath3 by @xmath4 . by a separable convex problem ,",
    "we mean that the objective function is a sum of @xmath5 independent parts , and the matrix is partitioned compatibly as in @xmath6 here @xmath7 is convex and lipschitz continuously differentiable , @xmath8 is a proper closed convex function ( possibly nonsmooth ) , @xmath9 is @xmath3 by @xmath10 with @xmath11 , and the columns of @xmath9 are linearly independent for @xmath12",
    ". constraints of the form @xmath13 , where @xmath14 is a closed convex set , can be incorporated in the optimization problem by setting @xmath15 when @xmath16 .",
    "the problem ( [ prob])([probm ] ) has attracted extensive research due to its importance in areas such as image processing , statistical learning and compressed sensing .",
    "see the recent survey @xcite and its references .",
    "let @xmath17 be the lagrangian given by @xmath18 where @xmath19 is the lagrange multiplier for the linear constraint and @xmath20 denotes the euclidean inner product .",
    "it is assumed that there exists a solution @xmath21 to ( [ prob])([probm ] ) and an associated lagrange multiplier @xmath22 such that @xmath23 attains a minimum at @xmath21 , or equivalently , the following first - order optimality conditions hold : @xmath24 and for @xmath25 @xmath26 and for all @xmath27 , we have @xmath28 where @xmath29 denotes the gradient .",
    "a popular strategy for solving ( [ prob])([probm ] ) is the alternating direction multiplier method ( admm ) @xcite given by @xmath30 where @xmath31 , the augmented lagrangian , is defined by @xmath32 here @xmath33 is the penalty parameter .",
    "early admms only consider problem ( [ prob])([probm ] ) with @xmath34 corresponding to a @xmath35-block structure . in this case",
    ", the global convergence and complexity can be found in @xcite . when @xmath36 the admm strategy ( [ adm ] ) , a natural extension of the @xmath35-block admm , is not necessarily convergent @xcite ,",
    "although its practical efficiency has been observed in many recent applications @xcite .",
    "recently , much research has focused on modifications to admm to ensure convergence when @xmath36 .",
    "references include @xcite .",
    "one approach @xcite assumes @xmath37 of the functions in the objective are strongly convex and the penalty parameter is sufficiently small .",
    "linear convergence results under additional conditions are obtained in @xcite .",
    "analysis of a randomly permuted admm which allows for nonseparated variables is given in @xcite .",
    "another approach , first developed in @xcite , involves a back substitution step to complement the admm forward substitution step .",
    "the algorithms developed in our paper utilize this back substitution step .",
    "the dominant computation in an iteration of admm is the solution of the subproblems in ( [ adm ] ) .",
    "the efficiency depends on our ability to solve these subproblems inexactly while still maintaining the global convergence , especially when no closed formula exists for the subproblems @xcite .",
    "one line of research is to solve the subproblems to an accuracy based on an absolute summable error criterion @xcite . in @xcite ,",
    "the authors combine an adaptive error criterion with the absolute summable error criterion for 2-block admm with logarithmic - quadratic proximal regularization and further correction steps to modify the solutions generated from the admm subproblems . in @xcite ,",
    "the authors develop a 2-block admm with a relative error stopping condition for the subproblems , motivated by @xcite , based on the total subgradient error .",
    "another line of research is to add proximal terms to make the subproblems strongly convex @xcite and relatively easy to solve .",
    "however , this approach often requires accurate solution of the proximal subproblems .",
    "when @xmath38 , admm reduces to the standard augmented lagrangian method ( alm ) , for which practical relative error criteria for solving the subproblems have been developed and encouraging numerical results have been obtained @xcite . in this paper , motivated by our recent work on variable stepsize bregman operator splitting methods ( bosvs ) , by recent complexity results for gradient and accelerated methods for convex optimization , and by the adaptive relative error strategy used in alm , we develop new _ inexact _",
    "approaches for solving the admm subproblems . to the best of our knowledge ,",
    "these are the first admms for solving the general separable convex optimization problem ( [ prob])([probm ] ) based on an adaptive accuracy condition that does not employ an absolute summable error criterion and that guarantees global convergence , even when @xmath36 .",
    "to guarantee global convergence , a block gaussian backward substitution strategy is used to make corrections to the approximate subproblem solutions . in the special case @xmath39 , the method will reduces to a 2-block admm without back substitution .",
    "this idea of using block gaussian back substitution was first proposed in @xcite .",
    "the method in this earlier work requires the exact solution of the subproblems to obtain global convergence , while our new approach allows an inexact solution .",
    "more recently , a linearly convergent admm was developed in @xcite .",
    "this algorithm linearizes the subproblems to achieve an inexact solution , and requires that the functions @xmath7 and @xmath8 in the objective function satisfy certain `` local error bound '' conditions .",
    "in addition , to ensure linear convergence , the stepsize @xmath40 in ( [ adm ] ) must be sufficiently small , which could significantly deteriorate the practical performance .    in this paper , we focus on problems where the minimization of the dual function over one or more of the primal variable @xmath41 is nontrial , and the accuracy of an inexact minimizer needs to be taken into account . on the other hand ,",
    "when these minimizations are simple enough , it is practical to minimize the lagrangian over @xmath42 and compute the dual function .",
    "this leads to a possibly nonsmooth dual function which can be approached through smoothing techniques as in @xcite , or through active set techniques as in @xcite .",
    "our paper is organized as follows . in the section [ bosvs ] ,",
    "we first generalize the bosvs algorithm @xcite to handle multiple blocks .",
    "the original bosvs algorithm was tailored to the two block case , but used an adaptive stepsize when solving the subproblem , and consequently , it achieved much better overall efficiency when compared to the bregman operator splitting ( bos ) type algorithms based on a fixed smaller stepsize . in sections  [ mbosvs ] and [ abosvs ] ,",
    "more adaptive stopping criteria for the subproblems are proposed .",
    "the adaptive criteria for bounding the accuracy in the admm subproblems are based on both the current and accumulated iteration change in the subproblem .",
    "these novel stopping criteria are motivated by the complexity analysis of gradient methods for convex optimization , and by the relative accuracy strategy often used in an inexact augmented lagrangian method for nonlinear programming .",
    "although our analysis is carried out with vector variables , these results could be extended to matrix variables which could have more potential applications .",
    "the set of solution / multiplier pairs for ( [ prob ] ) is denoted @xmath43 , while @xmath44 is a generic solution / multiplier pair . for @xmath42 and @xmath45 , @xmath46 is the standard inner product , where the superscript @xmath47 denotes transpose .",
    "the euclidean norm , denoted @xmath48 , is defined by @xmath49 and @xmath50 for a positive definite matrix @xmath51 .",
    "@xmath52 denotes the set of nonnegative real numbers , while @xmath53 denotes the set of positive real numbers . for a differentiable function @xmath54",
    ", @xmath55 is the gradient of @xmath56 at @xmath42 , a column vector .",
    "more generally , @xmath57 denotes the subdifferential at @xmath42 .",
    "if @xmath42 is a vector , then @xmath58 denotes the subvector obtained by dropping the first block of variables from @xmath42 . thus if @xmath59 with @xmath60 for @xmath61 $ ] , then @xmath62 @xmath63 .",
    "three related inexact admms are developed called generalized , multistep , and accelerated bosvs .",
    "they differ in the details of the formula for the new iterate @xmath64 , but the overall structure of the algorithms is the same .",
    "both multistep and accelerated bosvs typically represent a more exact admm iteration when compared to generalized bosvs , while the accelerated bosvs subiterations often converge more rapidly than those of multistep bosvs .",
    "the common elements of these three algorithms appear in algorithm  [ admmcommon ] .",
    "l * parameters : * @xmath65 , @xmath66 , @xmath67 , @xmath68 . + * starting guess : * @xmath69 and @xmath70 . + * initialize : * @xmath71 , @xmath72 , @xmath73 and @xmath74 , @xmath75 , @xmath76 +    [ cols= \" < , < \" , ]      + * next * + 1c .",
    "set @xmath77 , @xmath78 , @xmath79 , and @xmath80 .",
    "+    two parameter sequences appear in the accelerated bosvs scheme , the @xmath81 and @xmath82 sequences .",
    "they must be chosen so that the line search condition of step  1a is satisfied for each value of @xmath83 , and the stopping condition of step  1b is satisfied for @xmath83 sufficiently large .",
    "if the lipschitz constant @xmath84 of @xmath7 is known , then we could take @xmath85,\\ ] ] in which case , we have @xmath86 this relation along with a taylor series expansion of @xmath7 around @xmath87 implies that the line search condition in step  1a of accelerated bosvs is satisfied for each @xmath83 .",
    "moreover , we show ( after lemma  [ lem - ag - conv ] ) that with these choices for @xmath81 and @xmath82 , the stopping condition of step  1b is also satisfied eventually .    a different , adaptive way to choose the parameters , that does not require",
    "knowledge of the lipschitz constant for @xmath7 , is the following : choose @xmath88 $ ] , where @xmath89 are safeguard parameters , and set @xmath90 after some algebra , it can be shown that @xmath91 hence , the ratio @xmath92 appearing in the line search condition of step  1a tends to infinity as @xmath93 tends to infinity since @xmath94 .",
    "we take @xmath95 to be the smallest integer for which the line search condition is satisfied .",
    "based on the identity ( [ delta / alpha ] ) , the expression @xmath92 has exactly the same effect as @xmath96 in generalized bosvs .",
    "consequently , it satisfies exactly the same inequality ( [ delta_bound ] ) .",
    "let us first observe that when @xmath97 , we have reached a solution of ( [ prob])([probm ] ) .",
    "[ l - stop - cond3 ] if @xmath98 in the accelerated bosvs algorithm , then @xmath99 solves @xmath100@xmath101 and @xmath102 .",
    "if @xmath103 , then @xmath104 for each @xmath105 .",
    "it follows that @xmath106 by step  1c , @xmath107 . by the definitions @xmath108 and @xmath109 where @xmath110 , we have @xmath111 for each @xmath83 due to ( [ equals ] ) . again , by step  1c , @xmath112 .",
    "consequently , we have @xmath113 . since all three algorithms in this paper share algorithm  [ admmcommon ] , the remainder of the proof is exactly as in lemma  [ l - stop - cond ] .",
    "we now establish the following analogue of lemma  [ lem - gd - conv ] .",
    "[ lem - ag - conv ] if the inner loop sequence @xmath114 associated with accelerated bosvs is nonincreasing as a function of @xmath83 , then for each @xmath115 $ ] , we have @xmath116 where @xmath117 is the terminating value of @xmath83 at iteration @xmath118 , @xmath119 is the minimizer of the function @xmath120 defined in @xmath121 , and @xmath122 is the smallest eigenvalue of @xmath123 .    by the definition @xmath124 ,",
    "we have @xmath125 add to this the identity @xmath126 to obtain @xmath127 & ( 1 - \\alpha^l ) \\left [ f_i({\\bar{{{\\bf{a}}}}}_i^l ) + \\langle \\nabla   f_i({\\bar{{{\\bf{a}}}}}_i^l ) , { { \\bf{a}}}_i^{l-1 }   - { \\bar{{{\\bf{a}}}}}_i^l \\rangle \\right ] + \\alpha^l \\left [ f_i({\\bar{{{\\bf{a}}}}}_i^l ) + \\langle \\nabla   f_i({\\bar{{{\\bf{a}}}}}_i^l ) , { { \\bf{u}}}_i^l - { \\bar{{{\\bf{a}}}}}_i^l \\rangle \\right].&\\end{aligned}\\ ] ] by the convexity of @xmath7 , it follows that @xmath128 hence , we have @xmath129 .\\ ] ] adding and subtracting any @xmath27 in the last term , and then exploiting the convexity of @xmath7 gives @xmath130 + \\langle \\nabla   f_i({\\bar{{{\\bf{a}}}}}_i^l ) , { { \\bf{u}}}_i^l - { { \\bf{u } } } \\rangle \\\\ & \\le & f_i({{\\bf{u } } } ) + \\langle \\nabla   f_i({\\bar{{{\\bf{a}}}}}_i^l ) , { { \\bf{u}}}_i^l - { { \\bf{u } } } \\rangle .\\end{aligned}\\ ] ] therefore , @xmath131.\\ ] ]    now by the line search condition in step  1a of accelerated bosvs and then by ( [ 1234 ] ) , we have @xmath132 & & \\quad \\quad \\quad + \\frac{\\rho}{2 } \\|{{\\bf{a}}}_i { { \\bf{a}}}_i^l - { { \\bf{b}}}_i^k + { \\bm \\lambda}^k/\\rho\\|^2 + h_i   ( { { \\bf{a}}}_i^l ) \\nonumber \\\\[.05 in ] & \\le & ( 1 - \\alpha^l ) f_i ( { { \\bf{a}}}_i^{l-1 } ) + \\alpha^l f_i({{\\bf{u } } } ) + \\alpha^l \\langle \\nabla f_i({\\bar{{{\\bf{a}}}}}_i^l ) , { { \\bf{u}}}_i^l   - { { \\bf{u } } } \\rangle + \\frac{(1-\\sigma)\\delta^l}{2 \\alpha^l } \\|{{\\bf{a}}}_i^l - { \\bar{{{\\bf{a}}}}}_i^l\\|^2 \\nonumber \\\\[.05 in ] & & \\quad \\quad",
    "\\quad + \\frac{\\rho}{2 } \\|{{\\bf{a}}}_i { { \\bf{a}}}_i^l - { { \\bf{b}}}_i^k + { \\bm \\lambda}^k/\\rho\\|^2 + h_i   ( { { \\bf{a}}}_i^l ) .",
    "\\label{1235}\\end{aligned}\\ ] ] next , we utilize the definitions of @xmath133 and @xmath134 and the convexity of both @xmath8 and the norm term to obtain @xmath135 + \\frac{(1-\\sigma)\\delta^l}{2 \\alpha^l } \\|{{\\bf{a}}}_i^l - { \\bar{{{\\bf{a}}}}}_i^l\\|^2 \\nonumber \\\\ & &   \\quad + ( 1-\\alpha^l ) \\left(\\frac{\\rho}{2 } \\|{{\\bf{a}}}_i { { \\bf{a}}}_i^{l-1 } - { { \\bf{b}}}_i^k   + { \\bm \\lambda}^k/\\rho \\|^2   + h_i   ( { { \\bf{a}}}_i^{l-1 } ) \\right ) \\nonumber \\\\ & & \\quad + \\alpha^l \\left(\\frac { \\rho}{2 } \\|{{\\bf{a}}}_i { { \\bf{u}}}_i^l - { { \\bf{b}}}_i^k + { \\bm \\lambda}^k/\\rho \\|^2 + h_i   ( { { \\bf{u}}}_i^l ) \\right ) \\nonumber\\\\ & = &   ( 1-\\alpha^l ) \\left ( f_i ( { { \\bf{a}}}_i^{l-1 } ) + \\frac{\\rho}{2 } \\|{{\\bf{a}}}_i { { \\bf{a}}}_i^{l-1 } - { { \\bf{b}}}_i^k + { \\bm \\lambda}^k/\\rho \\|^2 +   h_i   ( { { \\bf{a}}}_i^{l-1 } )   \\right ) \\nonumber\\\\   & & \\quad + \\alpha^l [ f_i({{\\bf{u } } } ) + \\langle \\nabla   f_i({\\bar{{{\\bf{a}}}}}_i^l ) , { { \\bf{u}}}_i^l   - { { \\bf{u } } } \\rangle ] + \\frac{(1-\\sigma)\\delta^l \\alpha^l}{2 } \\|{{\\bf{u}}}_i^l - { { \\bf{u}}}_i^{l-1}\\|^2    \\nonumber\\\\   & & \\quad + \\alpha^l \\left(\\frac { \\rho}{2 } \\|{{\\bf{a}}}_i { { \\bf{u}}}_i^l - { { \\bf{b}}}_i^k + { \\bm \\lambda}^k/\\rho \\|^2 + h_i   ( { { \\bf{u}}}_i^l ) \\right ) \\nonumber \\\\ & = &   ( 1-\\alpha^l ) l_i^k ( { { \\bf{a}}}_i^{l-1 } )   + \\alpha^l [ f_i({{\\bf{u } } } ) + \\langle \\nabla   f_i({\\bar{{{\\bf{a}}}}}_i^l ) , { { \\bf{u}}}_i^l   - { { \\bf{u } } } \\rangle ] \\nonumber \\\\ & & \\quad + \\frac{(1-\\sigma)\\delta^l \\alpha^l}{2 } \\|{{\\bf{u}}}_i^l - { { \\bf{u}}}_i^{l-1}\\|^2 +   \\alpha^l \\left(\\frac { \\rho}{2 } \\|{{\\bf{a}}}_i { { \\bf{u}}}_i^l - { { \\bf{b}}}_i^k + { \\bm \\lambda}^k/\\rho\\|^2 + h_i   ( { { \\bf{u}}}_i^l ) \\right ) .   \\label{qwer}\\end{aligned}\\ ] ] since @xmath8 is convex , we have @xmath136 for any @xmath137 .",
    "the expansion of the quadratic @xmath138 in step  1a of accelerated bosvs around @xmath139 can be written @xmath140 since @xmath139 minimizes @xmath141 in step  1a , the first - order optimality conditions imply that @xmath142 for some @xmath137 .",
    "we choose @xmath143 , and then multiply ( [ hineq ] ) and ( [ qeq ] ) by @xmath82 and add to ( [ qwer ] ) to obtain @xmath144 hence , for any @xmath27 we have @xmath145    from the definition of @xmath146 in accelerated bosvs , it follows that @xmath147 with the convention that @xmath148 ( since @xmath149 .",
    "hence , for any sequence @xmath150 , @xmath151 , we have @xmath152 suppose that @xmath153 for each @xmath83 . by assumption , @xmath154 is nonincreasing ; since @xmath155 and @xmath156 , it follows that @xmath157 , and we have @xmath158 we now multiply ( [ asdf ] ) by @xmath146 and sum over @xmath83 between 1 and @xmath117 . exploiting the identity ( [ id1 ] ) with @xmath159 and ( [ id2 ] ) with @xmath160 , we obtain @xmath161 where @xmath162 denotes the final @xmath146 in accelerated bosvs .",
    "next , we multiply the definition @xmath163 by @xmath164 and sum over @xmath93 between 1 and @xmath83 . again , exploiting the identity @xmath165 yields @xmath166 since @xmath167 , it follows that @xmath168 consequently , @xmath133 is a convex combination of @xmath169 through @xmath170 . since @xmath171 is a convex function of @xmath172 , jensen s inequality yields @xmath173 we apply this inequality to the last term in ( [ zxcv ] ) and substitute @xmath174 , @xmath175 , and @xmath176 to obtain @xmath177 finally , take @xmath178 . since the left side of ( [ generalu ] ) is nonpositive for this choice of @xmath179 , the proof is complete .",
    "let us now examine the assumptions and consequences of lemma  [ lem - ag - conv ] in the context of the choices ( [ ag_constant ] ) and ( [ ag_linesearch ] ) for the parameters @xmath81 and @xmath82 . for the choice ( [ ag_constant ] ) and for @xmath180 , we have @xmath181 hence , @xmath146 is @xmath182 . since @xmath183 , it follows that for @xmath180 , @xmath184 in the special case @xmath185 , @xmath186 . since the sequence @xmath187 is identically one , it is nonincreasing and the assumption of lemma  [ lem - ag - conv ] is satisfied . since @xmath162 is the final value for @xmath146 in step  1 of accelerated bosvs , it follows from ( [ ag - converge ] ) that @xmath188 .    for the choice ( [ ag_linesearch ] ) and for @xmath180",
    ", we have @xmath189 and @xmath190 .",
    "it follows that @xmath191 and for @xmath180 , we have @xmath192 hence , @xmath193 in the special case @xmath185 , we also have @xmath157 . again , the sequence @xmath187 is identically one , which satisfies the requirement of lemma  [ lem - ag - conv ] ; consequently , the speed with which @xmath194 converges to @xmath119 depends on the growth rate of @xmath146 . by the definition of @xmath146 in accelerated bosvs , @xmath195 since @xmath196",
    ", it follows from ( [ delta / alpha ] ) that @xmath197 @xmath198 , which implies that @xmath199 by ( [ gammarecur ] ) , we have @xmath200 as noted beneath ( [ delta / alpha ] ) , @xmath201 satisfies the inequality ( [ delta_bound ] ) for @xmath96 , which implies that @xmath202 hence , ( [ recur ] ) yields @xmath203 since @xmath204 , it follows that @xmath205 which implies that @xmath206 . in summary , for either of the choices ( [ ag_constant ] ) or ( [ ag_linesearch ] ) , we have @xmath207 for each @xmath83 , and @xmath188 .",
    "moreover , by the inequality ( [ generalu ] ) with @xmath178 , the objective value satisfies @xmath208 .",
    "although lemma  [ lem - ag - conv ] was stated in terms of the terminating iteration @xmath117 of the inner iteration , it applies to any of the inner iterations ; that is , for each @xmath105 and @xmath83 , we have @xmath209 whenever @xmath146 approaches infinity , as it does with the choices ( [ ag_constant ] ) and ( [ ag_linesearch ] ) , the right side approach zero and @xmath210 converges to @xmath119 .",
    "hence , the stopping conditions in step  1b of accelerated bosvs are satisfied for @xmath83 sufficiently large when @xmath211 .",
    "the convergence of accelerated bosvs , like the other algorithms , relies on a decay property for the iterates , which we now give .",
    "[ l - key - lemma3 ] if the accelerated bosvs parameters @xmath146 tend infinity as @xmath83 grows and @xmath196 for each @xmath83 , then lemma  @xmath212 holds for the accelerated scheme .",
    "we substitute @xmath213 and @xmath207 in ( [ generalu ] ) to obtain @xmath214 where @xmath215 .",
    "this is exactly the same as ( [ jumppoint ] ) in the proof of lemma  [ l - key - lemma2 ] .",
    "the remainder of the proof is exactly as in the proof of lemma  [ l - key - lemma2 ] .",
    "using the decay property of lemmas  [ l - key - lemma2 ] and [ l - key - lemma3 ] , we now obtain the convergence of accelerated bosvs .",
    "[ l - glob - thm3 ] suppose that for the inner loop sequence @xmath114 associated with accelerated bosvs we have @xmath207 for each @xmath83 , @xmath146 tends to infinity as @xmath83 grows , and there exists a constant @xmath216 such that @xmath217 for all @xmath83 .",
    "if accelerated bosvs performs an infinite number of iterations generating iterates @xmath218 , @xmath219 , and @xmath220 , then the sequences @xmath218 and @xmath219 both approach a common limit @xmath21 and @xmath220 approaches a limit @xmath221 where @xmath44 .",
    "the proof is identical to that of theorem  [ l - glob - thm2 ] through the end of case  1 . for accelerated bosvs",
    ", the fact that @xmath194 is a convex combination of @xmath222 is shown in ( [ convex1])([convex2 ] ) .",
    "the treatment of accelerated bosvs first differs from that of multistep bosvs in the second paragraph of case  2 ( @xmath162 tends to @xmath223 ) where the multistep bosvs stopping condition @xmath224 , is used to show that @xmath225 approaches zero .",
    "since accelerated bosvs uses the new stopping condition @xmath226 , a new analysis is needed in case  2 .    by the definition of @xmath210",
    ", we have @xmath227 if @xmath228 denotes @xmath229 and @xmath230 so that @xmath210 satisfies the stopping criterion @xmath231 , then @xmath232 since @xmath233 and @xmath234 when @xmath230 .",
    "squaring this , dividing by @xmath235 , and utilizing the assumption that @xmath217 for all @xmath83 , we deduce that @xmath236 since @xmath228 approach zero by ( [ eklim ] ) , it follows that @xmath237 approaches zero as @xmath118 tends to infinity .",
    "since @xmath162 is nondecreasing , @xmath238 also approaches zero as @xmath118 tends to infinity .",
    "since @xmath194 is a bounded sequence and @xmath162 tends to infinity in case  2 , we can replace @xmath194 by any other bounded sequence and reach the same conclusion . in particular , since the sequence @xmath119 is bounded we conclude that @xmath239 approaches zero as @xmath118 tends to infinity , the same conclusion we reached in multistep bosvs scheme .",
    "the rest of the proof is exactly as in theorem  [ l - glob - thm2 ] .",
    "this completes the proof .",
    "the parameter choices given in both @xmath240 and @xmath241 satisfy the assumption of theorem  @xmath242 that @xmath243 for some constant @xmath244 .",
    "in particular , for @xmath240 , we show in @xmath245 that @xmath246 .",
    "this is combined with the definition of @xmath82 in @xmath240 to obtain @xmath247 for @xmath248 . for the choice @xmath241",
    ", it follows from @xmath249 and @xmath250 that @xmath251    in this paper , we have focused on algorithms based on an inexact minimization of @xmath120 in step  @xmath252 of algorithm  @xmath253 . in cases where @xmath7 and @xmath8 are simple enough that the exact minimizer @xmath119 of @xmath120 can be quickly evaluated , we could simply set @xmath254 and @xmath255 in step  @xmath252 .",
    "the analysis of this exact algorithm is very similar to the analysis in theorems  @xmath256 and @xmath242 . in the analysis of the inexact algorithms , a key inequality",
    "@xmath257 was @xmath258 where @xmath259 for an exact minimizer of @xmath120 , the same inequality can be established but with @xmath260 replaced by zero .",
    "this follows directly from the first - order optimality conditions for a minimizer of @xmath120 and for a minimizer of ( [ prob])([probm ] ) .",
    "since @xmath260 disappears , then so do the @xmath261 and @xmath222 terms in lemma  [ l - key - lemma2 ] ; consequently , the analysis becomes simpler when the minimizer of @xmath120 is exact .",
    "in this section , we investigate the performance of the algorithms for an image reconstructed problem that can be formulated as @xmath262 where @xmath263 is the given image data , @xmath264 is a matrix describing the imaging device , @xmath265 is the total variation norm , @xmath266 is the @xmath267 norm , @xmath268 is a wavelet transform , and @xmath269 and @xmath270 are weights .",
    "the first term in the objective is the data fidelity term , while the next two terms are for regularization ; they are designed to enhance edges and increase image sparsity . in our experiments",
    ", @xmath268 is a normalized haar wavelet with four levels and @xmath271 .",
    "the problem ( [ 3block - obj ] ) is equivalent to @xmath272 where @xmath273 and @xmath274 is the vector of finite differences in the image along the coordinate directions at the i - th pixel in the image , while @xmath275 where @xmath3 is the total number of pixels in the image .",
    "the problem ( [ 3block - equiv ] ) has the structure appearing in ( [ prob])([probm ] ) with @xmath276 f_2 : = 0 , & h_2({{\\bf{w } } } ) = \\|{{\\bf{w}}}\\|_{1,2 } , \\\\[.05 in ] f_3 : = 0 , & h_3({{\\bf{z } } } ) = \\|{{\\bf{z}}}\\|_1 , \\\\[.05 in ] \\end{array } \\\\ { { \\bf{a}}}_1 = \\left (   \\begin{array}{l }   { { \\bf{b } } } \\\\   { \\bm \\psi } { ^{\\sf t}}\\end{array }   \\right ) , \\quad { { \\bf{a}}}_2 = \\left (   \\begin{array}{r } -{{\\bf{i } } } \\\\   { { \\bf{0 } } } \\end{array } \\right ) , \\quad { { \\bf{a}}}_3 = \\left (   \\begin{array}{r }   { { \\bf{0 } } } \\\\ -{{\\bf{i } } } \\end{array } \\right ) , \\quad \\mbox{and } \\quad { { \\bf{b } } } = \\left (   \\begin{array}{r } { { \\bf{0 } } } \\\\ { { \\bf{0 } } } \\end{array } \\right ) . \\end{array}\\ ] ] when solving the test problems using accelerated bosvs , we use choose @xmath82 and @xmath81 as in ( [ ag_linesearch ] ) . since @xmath277 , the line search condition holds automatically , and the second and third subproblems are solved in closed form , due to the simple structure of @xmath278 and @xmath279 . only the first subproblem is solved inexactly . at iteration @xmath118 , the solution of this subproblem approximates the solution of @xmath280 where @xmath220 and @xmath281 are the lagrange multipliers at iteration @xmath118 for the constraints @xmath282 @xmath172 and @xmath283 @xmath284 respectively .    the stopping condition for the inner loop of either multistep or accelerated bosvs required that @xmath285 . to improve efficiency , we replaced this condition by @xmath286 where @xmath117 is the number of iterations performed by the inner loop for block @xmath105 at iteration @xmath118 .",
    "for all the algorithm , we chose the initial @xmath287 in the line search using the bb approximation , which is given in step  1a of generalized bosvs .",
    "moreover , when @xmath288 , we increase @xmath289 by setting @xmath290 where @xmath291 in our numerical experiments .",
    "when @xmath289 is sufficiently large , we have @xmath292 and the line search condition in the algorithms is satisfied by @xmath287 ; that is , @xmath293 .",
    "consequently , when @xmath289 is sufficiently large , we have @xmath294 and the relaxed stopping condition @xmath295 implies that @xmath285 , the original stopping condition . since @xmath296 , it follows that @xmath288 for only a finite number of iterations , and hence , @xmath285 for @xmath118 sufficiently large .",
    "this ensures the global convergence of the algorithms .    another improvement to efficiency",
    "was achieved by further relaxing the line search criterion . in particular , for the line search in generalized bosvs ( step  1b ) , we replaced the right side @xmath297 by @xmath298 where @xmath299 is a summable sequence . in the line search of multistep bosvs ( step  1b ) ,",
    "@xmath300 was replaced by @xmath301 , where @xmath302 with @xmath303 a summable sequence . in the line search of accelerated bosvs ( step  1a )",
    ", we replaced @xmath304 by @xmath305 , where @xmath306 .",
    "it can be proved that when the line search is relaxed in this way using summable sequences , there is no effect on the global convergence theory ; these @xmath307 and @xmath308 terms need to be inserted in each inequality in the analysis , but in the end , the steps and the conclusions are unchanged . on the other hand ,",
    "when the line search is relaxed , it can terminate sooner , and the algorithms can be more efficient . for the numerical experiments",
    ", we took @xmath309 . for multistep bosvs , @xmath310 , while for accelerated bosvs , @xmath311 . since @xmath312 for multistep bosvs and @xmath313 for accelerated bosvs , the @xmath303 sequences are summable .",
    "in all the algorithms , we use the following parameters : @xmath314 for the inner loop stopping condition , we took @xmath315 in multistep bosvs , and @xmath316 in accelerated bosvs , while in step  2 of the admm template algorithm  [ admmcommon ] , we took @xmath317 @xmath318 , @xmath319 , and @xmath320 . for comparison ,",
    "we provide numerical results based on the algorithm in @xcite where we use matlab s conjugate gradient routine cgs to solve the subproblem ( [ u - subprob ] ) almost exactly , stopping when @xmath321 .",
    "all the codes were implemented in matlab ( version r2014a ) .",
    "the following figures show the relative objective error @xmath322 versus cpu time , where @xmath323 is the optimal function value of ( [ 3block - obj ] ) obtained by applying accelerated bosvs until the eighth digit of the relative objective value did not change in four consecutive iterations .",
    "the first experiment employs an image deblurring problem from @xcite .",
    "the original image is the well - known cameraman image of size @xmath324 and the observed data @xmath263 in ( [ 3block - obj ] ) is a blurred image obtained by imposing a uniform blur of size @xmath325 with gaussian noise and snr of @xmath326db .",
    "the weights in ( [ 3block - obj ] ) are @xmath327 and @xmath328 , and the penalty parameter @xmath329 .",
    "figure  [ error_plots](a ) shows the base-10 logarithm of the relative objective error versus cpu time . in this problem where the subproblems are relatively easy",
    ", generalized bosvs is significantly slower than the exact , multistep , and accelerated algorithms , while both multistep and accelerated bosvs were faster than the exact scheme .",
    "the second set of test problems , which arise in partially parallel imaging ( ppi ) , are found in @xcite .",
    "the observed data , corresponding to 3 different images , are denoted data  1 , data  2 , and data  3 . for these test problems ,",
    "the weights in ( [ 3block - obj ] ) are @xmath330 and @xmath331 , and the penalty parameter @xmath332 .",
    "the performance of the algorithms is shown in figure  [ error_plots](b)(d ) .",
    "these test problems are much more difficult than the first problem since @xmath264 is large , relatively dense , and ill conditioned . in this case , all the inexact algorithms are faster than the exact algorithm initially . the exact algorithm becomes faster than generalized bosvs when the relative error is around @xmath333 or @xmath334 .",
    "accelerated bosvs is always significantly faster than the exact algorithm .",
    "\\(a )   ( b )   + ( c )",
    "three inexact alternating direction multiplier methods were presented for solving separable convex linearly constrained optimization problems , where the objective function is the sum of smooth and relatively simple nonsmooth terms .",
    "the nonsmooth terms could be infinite , so the algorithms and analysis included problems with additional convex constraints .",
    "these algorithms all originate from the 2-block variable stepsize bosvs scheme of @xcite which employs indefinite proximal terms and linearized subproblems .",
    "the 2-block scheme was generalized to a multiblock scheme using a back substitution process to generate an auxiliary sequence @xmath218 that played the role of @xmath335 in the original , potentially divergent @xcite , multiblock admm ( [ adm ] ) .",
    "the three new methods , called generalized , multistep , and accelerated bosvs , correspond to different accuracy levels when solving the admm subproblems .",
    "generalized bosvs employed only one iteration in the subproblems , while multistep and accelerated bosvs performed multiple iterations until the iteration change was sufficiently small .",
    "the multistep and accelerated schemes differed in the rate with which they solved the the subproblems .",
    "if @xmath83 was the number of iterations in the subproblem , then multistep bosvs had a convergence rate of @xmath336 , while accelerated bosvs had a convergence rate of @xmath337 .",
    "global convergence was established for all the methods .",
    "numerical experiments were performed using image reconstruction problems .",
    "the accelerated bosvs algorithm had the best performance when compared with either the other inexact algorithms , or the exact algorithm of @xcite .",
    ", _ modified lagrangians in convex programming and their generalizations _ , in point - to - set maps and mathematical programming , p.  huard , ed . , vol .  10 of mathematical programming studies , springer berlin heidelberg , 1979 , pp"
  ],
  "abstract_text": [
    "<S> inexact alternating direction multiplier methods ( admms ) are developed for solving general separable convex optimization problems with a linear constraint and with an objective that is the sum of smooth and nonsmooth terms . </S>",
    "<S> the approach involves linearized subproblems , a back substitution step , and either gradient or accelerated gradient techniques . </S>",
    "<S> global convergence is established . </S>",
    "<S> the methods are particularly useful when the admm subproblems do not have closed form solution or when the solution of the subproblems is expensive . </S>",
    "<S> numerical experiments based on image reconstruction problems show the effectiveness of the proposed methods .    </S>",
    "<S> separable convex optimization , alternating direction method of multipliers , admm , multiple blocks , inexact solve , global convergence    90c06 , 90c25 , 65y20 </S>"
  ]
}