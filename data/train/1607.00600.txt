{
  "article_text": [
    "this paper addresses optimization in multi - agent networks where each agent aims at optimizing a local performance criterion possibly subject to local constraints , but yet needs to agree with the other agents in the network on the value of some decision variables that refer to the usage of some shared resources .",
    "cooperative multi - agent decision making problems have been studied recently by many researchers , mainly within the control and operational research communities , and are found in various application domains such as power systems @xcite , wireless and social networks @xcite , robotics @xcite , to name a few .    a possible approach to cooperative multi - agent optimization consists in formulating and solving a mathematical program involving the decision variables , objective functions , and constraints of the entire network .",
    "though this centralized perspective appears sensible , it may end up being impractical for large scale systems for which the computational effort involved in the program solution can be prohibitive . also ,",
    "privacy of information is not preserved since agents are required either to share among them or to provide to a central entity their performance criteria and constraints .",
    "distributed optimization represents a valid alternative to centralized optimization and , in particular , it overcomes the above limitations by allowing agents to keep their information private , while distributing the computational effort .",
    "typically , an iterative procedure is conceived , where at each iteration agents perform some local computation based on their own information and on the outcome of the local computations of their neighboring agents at the previous iteration , till convergence to some solution , possibly an optimal one for the centralized optimization problem counterpart .",
    "effective distributed optimization algorithms have been proposed in the literature for a general class of convex problems over time - varying , multi - agent networks .",
    "in particular , consensus algorithms are formulated in @xcite and in our recent paper @xcite for problems where agents have their own objective functions and constraints but decision variables are common . in this paper",
    ", we address a specific class of convex optimization problems over time - varying , multi - agent networks , which we refer to as _ inequality - coupled problems _ for short - hand notation . in this class of problems ,",
    "each agent has its own decision vector , objective function , and constraint set , and is coupled to the others via a constraint expressed as the non - negativity of the sum of convex functions , each function corresponding to one agent .",
    "we propose a novel distributed iterative scheme based on a combination of dual decomposition and proximal minimization to deal with inequality - coupled problems . under convexity assumptions and suitable connectivity properties of the communication network ,",
    "agents reach consensus with respect to the dual variables , without disclosing information about their local objective and constraint functions , nor about the function encoding their contribution to the coupling constraint .",
    "the proposed algorithm converges to some optimal dual solution of the centralized problem counterpart , while for the primal variables , we show convergence to the set of optimal primal solutions .    the contributions of our paper versus the existing literature can be summarized as follows :    * _ we extend dual decomposition based algorithms to a distributed setting , accounting for possibly time - varying network connectivity . _",
    "+ if the communication networks were time - invariant and connected , then , dual decomposition techniques ( see @xcite , and references therein ) as well as approaches based on the alternating direction method of multipliers @xcite could be applied to our set - up .",
    "indeed , after dualizing the coupling constraint , the problem assumes a separable structure and a central update step can be taken for the dual variables .",
    "however , this latter step involves communication among all agents that are coupled via the constraints , which may not be possible in a time - varying connectivity set - up . *",
    "_ we provide a proximal minimization perspective to gradient / subgradient algorithms , which does not require differentiability assumptions on the primal objective functions , and/or gradient / subgradient computation .",
    "also , privacy is preserved since agents do not have to share their local information . _ + the incremental gradient / subgradient algorithms in @xcite could be adopted as an alternative to dual decomposition . however , they require agents to perform updates sequentially , in a cyclic or randomized order , and hence do not allow for parallel computations .",
    "one could then head for primal - dual subgradient based consensus algorithms as in @xcite and @xcite .",
    "however , in the former the coupling constraint is assumed to be known to all agents , thus violating privacy , whereas in the latter this is not the case , but each agents objective function has to be differentiable .",
    "note that the approaches to distributed optimization in @xcite could be applied to inequality - coupled problems by introducing a common decision vector collecting all local decision variables .",
    "however , the computational and communication effort would unnecessarily increase , since each agent would need to compute , maintain , and also communicate to its neighbors an estimate of the decision vectors of all agents , not just of its own . + in our approach agents need to exchange the estimate of the dual variables associated with the coupling constraints .",
    "our approach is , hence , particularly appealing in the case when the number of coupling constraints is low compared to the dimension of each decision vector , since the amount of information exchanged is limited compared to primal - based distributed optimization algorithms .",
    "the rest of the paper is organized as follows . a formal statement of the problem , along with the proposed algorithm",
    "is given in section  [ sec : problem ] .",
    "convergence and optimality of our algorithm are studied in section  [ sec : analysis ] , while section  [ sec : prep_results ] prove some preliminary relations that are instrumental to the subsequent analysis . a numerical example along with a suggestion on how to speed up numerical convergence is presented in section  [ sec : example ] , while some final remarks are drawn in section  [ sec : conclusion ] . to make the paper self - contained , an auxiliary result from the literature is reported in the appendix .",
    "consider the following optimization program @xmath0 involving @xmath1 agents that communicate over a time - varying network .",
    "each agent @xmath2 , @xmath3 , has its own vector @xmath4 of @xmath5 decision variables , its local constraint set @xmath6 and objective function @xmath7 , and it is contributing to the coupling constraint @xmath8 via function @xmath9 .",
    "note that equality linear coupling constraints can be also dealt with by means of @xmath10 , as shown in the numerical example of section  [ sec : example ] .",
    "problem @xmath10 could be solved , in principle , in a centralized fashion . however , if the number @xmath1 of agents is large , this may turn out to be computationally prohibitive .",
    "in addition , each agent would be required to share its own information ( coded via @xmath11 , @xmath12 , and @xmath13 ) either with the other agents or with a central unit collecting all information , which may be undesirable in some cases , due to privacy issues .",
    "we next formulate a distributed strategy that overcomes both the privacy and computational issues just outlined .",
    "let us consider the lagrangian function @xmath14 given by @xmath15 where @xmath16\\,{\\!^\\top\\!}\\in{\\mathbb{r}}^n$ ] , with @xmath17 , belongs to @xmath18 , whereas @xmath19 is the vector of lagrange multipliers ( @xmath20 denotes the @xmath21-th dimensional non - negative orthant ; in the sequel we shall sometimes write @xmath22 in place of @xmath19 ) .",
    "correspondingly , we can define the dual function as @xmath23 which , due to the separable structure of objective and constraint functions in problem @xmath10 ( see ) , can be expressed as @xmath24 where each @xmath25 is a concave function representing the dual function of agent @xmath2 . given these definitions , the dual of problem @xmath10 in",
    "can be expressed as : @xmath26 or , equivalently , as @xmath27 the coupling between agents is represented by the fact that @xmath28 is a common decision vector and the agents should agree on its value .",
    "1.1    * initialization * +  @xmath29 .",
    "+  consider @xmath30 , for all @xmath31 .",
    "+  consider @xmath32 , for all @xmath31 .",
    "+ * for @xmath33 repeat until convergence * +  @xmath34 .",
    "+  @xmath35",
    ". +  @xmath36 +    @xmath37    @xmath38 .",
    "@xmath39 .",
    "[ alg : alg1 ]    the optimization program in resembles the optimization program formulated in @xcite , since it exhibits the same structure with the dual variables in place of the primal ones .",
    "one could then apply the distributed algorithm in @xcite to solve .",
    "this however would involve computing the solution of a @xmath40 program at each iteration , for each agent , which is computationally challenging , in general .",
    "our distributed solution is thus inspired by that in @xcite , but it largely deviates from it to avoid that challenge .",
    "the basic steps of our distributed iteration scheme are summarized in algorithm  [ alg : alg1 ] and are described hereafter .",
    "each agent @xmath2 , @xmath31 , initializes the estimate of its local decision vector with @xmath30 ( step  3 of algorithm  [ alg : alg1 ] ) , and the estimate of the common dual variables vector with a @xmath32 that is feasible for problem @xmath41 ( step  4 of algorithm  [ alg : alg1 ] ) .",
    "a sensible choice is to set @xmath42 , and @xmath43 , @xmath33 , which corresponds to the solution of problem when coupling constraints are neglected .    at every iteration @xmath44 , @xmath45 , each agent @xmath2 computes a weighted average @xmath46 of the dual variables vector based on the estimates @xmath47 , @xmath48 , of the other agents and its own estimate ( step  6 ) . the weight @xmath49 that agent @xmath2 attributes to the estimate of agent @xmath50 at iteration @xmath44",
    "is set equal to zero if agent @xmath2 does not communicate with agent @xmath50 at iteration @xmath44 .",
    "if we were to follow the algorithmic solution in @xcite , agent @xmath2 should perform the following proximal maximization step to update its local estimate @xmath51 : @xmath52 as anticipated , solving a @xmath40 program is not easy , and for this reason algorithm  [ alg : alg1 ] alternates between a primal and a dual update step ( ( step  7 and step  8 , respectively ) . in particular , step  7 performs an update of the local primal vector @xmath53 as in dual decomposition , whereas , differently from dual decomposition , the update of the dual vector in step  8 involves also a proximal term to facilitate consensus among the agents .    finally",
    ", the auxiliary primal iterates @xmath54 , defined as the following weighted running average of @xmath55 : @xmath56 is computed in step 9 of algorithm  [ alg : alg1 ] through a recursive version of equation .",
    "such an auxiliary variable shows better convergence properties compared to @xmath57 , and is often constructed in the so - called primal recovery procedure of dual decomposition methods , @xcite .",
    "it is worth noticing that step  8 in algorithm  [ alg : alg1 ] is equivalent to a projected subgradient step .",
    "indeed the maximization of a quadratic function in step  8 can be explicitly solved , leading to @xmath58_+$ ] , where @xmath59_+$ ] denotes the projection of its argument onto @xmath20 .",
    "note that @xmath60 constitutes a subgradient of @xmath25 evaluated at @xmath46 , and @xmath61 can be thought of as the gradient step - size .",
    "both formulations of step  8 can then be adopted in algorithm  [ alg : alg1 ] , but we prefer to maintain the proximal formulation since it is more convenient for convergence analysis , and also enables the extension of the approach in @xcite to the distributed setting , while overcoming at the same time the requirement in @xcite that all agents should know the coupling constraint and not only their own contribution @xmath13 as in our algorithm .",
    "the proposed distributed algorithm shows properties of convergence and optimality , which hold under the following assumptions on the structure of the problem and on the communication features of the time - varying multi - agent network .",
    "[ convexity ] [ ass : convex ] for each @xmath31 , the function @xmath62 and each component of @xmath63 are convex , and the set @xmath64 is also convex .",
    "[ compactness ] [ ass : compact ] for each @xmath33 , the set @xmath64 is compact .",
    "note that , under assumptions  [ ass : convex ] and [ ass : compact ] , @xmath65 is finite for any @xmath66 : @xmath67 , @xmath68 , where @xmath69 .",
    "[ slater s condition ] [ ass : slater ] there exists @xmath70{\\!^\\top\\!}\\in x$ ] and @xmath71 with @xmath72 , such that @xmath73 and @xmath74 , with the inequality interpreted component - wise . for those components which are linear we could also have equality .    as a consequence of assumptions  [ ass : convex]-[ass : slater ] , we have that strong duality holds and an optimal primal - dual pair @xmath75 exists , where @xmath76{\\!^\\top\\!}$ ] .",
    "moreover , the saddle - point theorem @xcite holds , i.e. , given an optimal pair @xmath75 , we have that @xmath77    in the following we will denote by @xmath78 the set of all primal minimizers , and by @xmath79 the set of all dual maximizers . under assumptions  [ ass : convex]-[ass : slater ] , it was shown in @xcite that @xmath79 is bounded , so that @xmath80 is finite .",
    "as for the time - varying coefficient @xmath61 , we impose the following assumptions that are similar to those in @xcite .",
    "[ coefficient @xmath61 ] [ ass : ck_coefficient ] @xmath82 is a non - increasing sequence of positive reals such that @xmath83 for all @xmath84 , with @xmath85 .",
    "moreover ,    1 .",
    "@xmath86 , 2 .",
    "@xmath87 .",
    "one possible choice for @xmath82 satisfying assumption  [ ass : ck_coefficient ] is @xmath88 for some @xmath89 .    as in @xcite ,",
    "the communication network satisfies the following connectivity conditions .",
    "[ weight coefficients ] [ ass : weights ] there exists @xmath90 such that for all @xmath91 and all @xmath92 , @xmath93 , @xmath94 , and @xmath95 implies that @xmath96 .",
    "moreover , for all @xmath92 ,    1 .",
    "@xmath97 for all @xmath33 , 2 .",
    "@xmath98 for all @xmath99 .",
    "note that , if we fix @xmath92 , the information exchange between the @xmath1 agents can be coded via a directed graph @xmath100 , where nodes in @xmath101 represent the agents , and the set @xmath102 of directed edges is defined as @xmath103 i.e. , at time @xmath44 the link @xmath104 is present if agent @xmath50 communicates with agent @xmath2 and agent @xmath2 weights the information received from agent @xmath50 with @xmath49 .",
    "if the communication link is not active , then @xmath105 ; if @xmath95 then agent @xmath50 is said to be neighbor of agent @xmath2 at time @xmath44 .",
    "let @xmath106 denote the set of edges @xmath104 representing pairs of agents that communicate directly infinitely often .",
    "we then impose the following connectivity and communication assumption .",
    "[ connectivity and communication ] [ ass : network ] graph @xmath107 is strongly connected , i.e. , for any two nodes there exists a path of directed edges that connects them",
    ". moreover , there exists @xmath108 such that for every @xmath109 , agent @xmath2 receives information from a neighboring agent @xmath50 at least once every consecutive @xmath110 iterations .",
    "details on the interpretation of assumptions  [ ass : weights ] and [ ass : network ] can be found in @xcite .",
    "if assumptions [ ass : convex]-[ass : network ] are satisfied , then algorithm [ alg : alg1 ] converges and agents agree to a common vector of lagrange multipliers . specifically , their local estimates @xmath111 converge to some optimal vector of lagrange multipliers , while the vector @xmath112{\\!^\\top\\!}$ ] approaches @xmath78 , the set of minimizers of the primal problem .",
    "these results are formally stated in the following theorems .",
    "[ dual optimality ] [ thm : dual_optimality ] consider assumptions [ ass : convex]-[ass : network ] .",
    "we have that , for some @xmath113 , @xmath114    [ primal optimality ] [ thm : primal_optimality ] consider assumptions [ ass : convex]-[ass : network ] .",
    "we have that @xmath115 where @xmath116 denotes the distance between @xmath117 and the set @xmath118 , i.e. , @xmath119 .",
    "in this section we prove several relations that link the dual variables local estimate @xmath111 of agent @xmath2 , @xmath33 , with their arithmetic average @xmath120    the following lemma establishes a relation between @xmath121 and @xmath122 , @xmath33 , where the consensus error @xmath123 , related to agent @xmath2 , is given by @xmath124    [ lemma : error_vk ] consider assumptions [ ass : weights ] and [ ass : network ] . for all @xmath125 with @xmath126 , @xmath127 , and for all @xmath31 , @xmath128 where @xmath129 , and @xmath130 .",
    "the proof is omitted since it follows closely the one of lemma  2 in @xcite , where primal quantities appear in place of all relevant dual quantities .",
    "[ lemma : sum_error ] consider assumptions  [ ass : ck_coefficient]-[ass : network ] . fix any @xmath131 .",
    "we then have that for any @xmath132 , @xmath133 where @xmath134    see the proof of lemma  3 in @xcite .",
    "it should be noted that for all @xmath31 , @xmath135 is finite as the initial value of the algorithm , @xmath136 is finite as the convex combination of finite values , and @xmath137 is finite due to assumption [ ass : compact ] . by , and due to the fact that @xmath58_+$ ] as discussed in section [ sec : setup ] , we then have that @xmath138 , and hence @xmath139 is also finite .",
    "we then have the following lemma , which is fundamental for the analysis of section  [ sec : solution ] .",
    "[ lemma : relation ] consider assumptions  [ ass : convex]-[ass : slater ] and assumption [ ass : weights ] .",
    "fix any @xmath131 . for any @xmath140 , and for any @xmath141 and @xmath19",
    "we have , @xmath142    consider the quantity @xmath143 . adding and subtracting @xmath51 inside the norm and then expanding the square , we have that + @xmath144 where the second equality is obtained by adding and subtracting @xmath145 .",
    "consider now step  8 of algorithm  [ alg : alg1 ] . by the optimality condition ( proposition 3.1 in @xcite ) , we have that , for any @xmath19 , @xmath146 where the first term in the inner product above constitutes the gradient of the objective function that appears at step 8 of algorithm [ alg : alg1 ] ( it is quadratic , hence differentiable ) , multiplied by @xmath147 .",
    "recalling the definition of @xmath123 and after rearranging some terms , we have that @xmath148 for any @xmath19 . by adding and subtracting @xmath149 in the right - hand side of the inequality",
    "above we obtain @xmath150 for any @xmath19 .",
    "consider now step  7 of algorithm  [ alg : alg1 ] . by the optimality of @xmath53 we have that @xmath151 for any @xmath66 . combining the previous statement with , and by adding and subtracting @xmath152 and @xmath153 , we have that @xmath154 for any @xmath19 and any @xmath66 . by summing across @xmath2 , @xmath33 , rearranging some terms , and recalling the definition of the lagrangian function , we obtain @xmath155 for any @xmath19 and for any @xmath141 .    by the definition of @xmath46 ( step  6 of algorithm  [ alg : alg1 ] ) , by the fact that , under assumption  [ ass : weights ] , @xmath156 , and by convexity of @xmath157 , we have that @xmath158 let @xmath159 .",
    "using the fact that @xmath160 , setting @xmath161 and @xmath162 , we obtain @xmath163 where the second inequality is given by the fact that @xmath164 . by the cauchy - schwarz inequality we have that @xmath165 finally , by using , , together with",
    ", inequality follows , thus concluding the proof .    the relations established in lemmas  [ lemma : error_vk ]",
    ", [ lemma : sum_error ] , and [ lemma : relation ] can be exploited to prove the following proposition .",
    "[ prop : conv_error ] consider assumptions  [ ass : convex]-[ass : network ] .",
    "we have that    1 .",
    "@xmath166 , 2 .",
    "@xmath167 , for all @xmath31 .",
    "consider with @xmath168 and @xmath169 , where @xmath75 is an optimal primal - dual pair .",
    "by we have that @xmath170 , and hence we can drop this term from the right - hand side of . fixing @xmath171 and summing across @xmath44 , @xmath172 ,",
    "we have that @xmath173 by lemma  [ lemma : sum_error ] , after some cancellations , and after neglecting some negative terms in the right - hand side , we obtain @xmath174 since holds for any @xmath175 , one can always choose @xmath176 such that @xmath177 .",
    "let then @xmath178 . by assumption  [ ass : ck_coefficient].2 , and",
    "since @xmath179 is finite as an effect of @xmath180 and @xmath181 being finite ( see discussion below and below , respectively ) , the right - hand side of is finite , leading to the the first part of the proposition .",
    "the second part follows then directly from the first one , thus concluding the proof .",
    "we are now in a position to show that , as @xmath44 tends to infinity , the agents dual estimates @xmath111 , @xmath31 , reach consensus to their arithmetic average @xmath182 .",
    "the proof of the following proposition is analogous to the one of proposition  2 in @xcite and proposition  1 in @xcite , but it is included here for completeness .",
    "[ consensus ] [ prop : conv_consensus ] consider assumptions [ ass : convex]-[ass : network ] .",
    "we have that @xmath183    under assumptions  [ ass : convex]-[ass : network ] , by the second part of proposition  [ prop : conv_error ] we have that @xmath167 , for all @xmath31 .",
    "then , for any @xmath184 we can choose @xmath185 such that @xmath186 for all @xmath127 , for all @xmath31 .    by of lemma  [ lemma : error_vk ]",
    ", we then have that for all @xmath31 , @xmath187 where the equality is due to exchanging the summation order , and the last inequality is due to corollary  [ cor : boundedness ] and the fact that @xmath188 .    taking limit superior in both sides of as @xmath189 , we have that @xmath190 but since @xmath191 is arbitrary , the last statement implies that @xmath192 , for all @xmath33 , thus concluding the proof .",
    "the section is devoted to the analysis of algorithm  [ alg : alg1 ] and its optimality .",
    "specifically , we will first prove three preliminary results , and then we will give the proofs of theorems  [ thm : dual_optimality ] and [ thm : primal_optimality ] .",
    "[ thm : alg_conv ] consider assumptions  [ ass : convex]-[ass : network ] .",
    "we have that for any @xmath193 , for all @xmath33 , the sequence @xmath194 is convergent .",
    "consider of lemma  [ lemma : relation ] .",
    "let @xmath159 , @xmath168 , and @xmath169 , with @xmath195 . by neglecting some negative terms in the right - hand side",
    "we obtain @xmath196 for any @xmath113 .",
    "since all terms in are positive , we can apply the deterministic supermartingale convergence theorem ( see theorem  [ thm : supermartingale ] in the appendix ) . to this end ,",
    "set @xmath197 , @xmath198 , and @xmath199 . by lemma  [ lemma :",
    "sum_error ] , and due to the first part of proposition [ prop : conv_error ] , we have that @xmath200 the last statement together with assumption  [ ass : ck_coefficient].2 implies that @xmath201 . by the supermartingale convergence theorem we then have that @xmath202 is a convergent sequence .    by proposition  [ prop : conv_consensus ] ,",
    "@xmath111 , @xmath31 , converges to @xmath182 .",
    "the last statement , together with the fact that the sequence @xmath203 converges , implies that @xmath204 , and hence also @xmath205 , converge for any @xmath113 , for all @xmath31 , thus concluding the proof .",
    "[ cor : boundedness ] consider assumptions  [ ass : convex]-[ass : network ] . the sequence @xmath206 is bounded .    by theorem  [ thm : alg_conv ] , for all @xmath31 , the sequence @xmath207 is convergent , and it is therefore bounded by a finite value @xmath208 . by the triangle inequality we have that @xmath209 .",
    "we then have that @xmath210 , with @xmath211 as an effect of @xmath79 being bounded ( see lemma 1 in @xcite , as recalled below ) , thus concluding the proof .    in view of the previous corollary",
    ", the following summability result holds true .",
    "[ lemma : dual_summability ] consider assumptions  [ ass : convex]-[ass : network ] .",
    "we have that @xmath212    by corollary  [ cor : boundedness ] , @xmath206 is bounded , whereas by the definition of @xmath46 ( step  6 of algorithm  [ alg : alg1 ] ) and @xmath182 in , it follows that @xmath213 and @xmath214 are also bounded , for all @xmath31 .",
    "let @xmath215 denote a uniform upper bound for these sequences .",
    "due to compactness of @xmath12 , @xmath31 , @xmath216 is finite for any @xmath217 , with @xmath218 .",
    "therefore , @xmath25 is concave ( being a dual function ) on the compact set @xmath219 , hence it will also be lipschitz continuous on @xmath219 with lipschitz constant @xmath220 , i.e. , @xmath221    by the definition of @xmath222 we have that @xmath223 , for all @xmath224 , for all @xmath31 , hence @xmath225 where @xmath226 . multiplying both sides by @xmath147 , fixing @xmath171 and summing across @xmath44 , @xmath172 , we have that @xmath227 where the second inequality follows from @xmath228 where the first equality is obtained by the definition of @xmath46 ( step  6 of algorithm  [ alg : alg1 ] ) and by assumption  [ ass : weights ] , the inequality is due to the triangle inequality for @xmath229 , and the last equality is obtained exchanging the two summations and using assumption  [ ass : weights ] . letting @xmath178 , and due to , follows , thus concluding the proof .",
    "we are now in a position to prove the two main results of the paper .",
    "theorem  [ thm : dual_optimality ] by the definition of dual function in , for any @xmath19 @xmath230 where the inequality is due to the fact that @xmath53 does not necessarily minimize @xmath231 , the first equality is obtained by adding and subtracting @xmath232 and @xmath233 , and the second equality is obtained by using the definition of @xmath25 , the fact that @xmath53 minimizes @xmath234 ( see step 7 of algorithm [ alg : alg1 ] ) , and the fact that @xmath235 .    to upper bound the last term in , consider the quantity @xmath143 . adding and subtracting @xmath51 inside the norm and then expanding the square , we have that @xmath236 where the second equality is obtained by adding and subtracting @xmath145 .",
    "consider now step  8 of algorithm  [ alg : alg1 ] . by the optimality condition ( proposition 3.1 in @xcite )",
    ", we have that , for any @xmath19 , @xmath237 where the first term in the inner product in constitutes the gradient of the objective function that appears at step  8 of algorithm  [ alg : alg1 ] ( it is quadratic , hence differentiable ) , multiplied by @xmath147 . using in , recalling the definition of @xmath123 , and after rearranging some terms",
    ", we have that @xmath238 for any @xmath19 . by subtracting @xmath239 to both sides of , after rearranging some terms",
    ", we obtain @xmath240 for any @xmath19 . by the definition of @xmath46 ( step  6 of algorithm  [ alg : alg1 ] ) , by the fact that , under assumption  [ ass : weights ] , @xmath156 , and by the convexity of @xmath157 , we have that @xmath241    consider now . summing across @xmath2 , @xmath31 , and using ,",
    "we have that @xmath242 by , multiplying both sides by @xmath147 and using to bound the last term , we obtain @xmath243 let @xmath159 .",
    "using the fact that @xmath160 , setting @xmath161 and @xmath244 , we obtain @xmath245 by in , together with the fact that @xmath246 , we then have that @xmath247 for any @xmath159 .",
    "substitute @xmath168 and neglect the negative term @xmath248 . fixing @xmath171 and summing across @xmath44 , @xmath172 , after some cancellations , we have that @xmath249 letting @xmath178 and using lemma  [ lemma : dual_summability ] , we then have that @xmath250 however , since @xmath251 due to assumption [ ass : ck_coefficient ] , and @xmath252 due to the fact that @xmath181 is an optimal dual solution for @xmath253 , we have that @xmath254 due to the fact that @xmath255 is continuous since it is concave , and that @xmath256 belongs to a compact set ( see discussion below ) , the last statement implies that there exists some @xmath257 such that @xmath258 or in other words @xmath259 converges to @xmath260 across a subsequence . by proposition  [ prop : conv_consensus ] and theorem  [ thm : alg_conv ]",
    "@xmath204 converges for all @xmath113 , and hence also for @xmath257 .",
    "since this sequence is convergent , implies that @xmath259 converges to @xmath260 across all subsequences , therefore @xmath259 has a unique limit point , and hence @xmath261 .",
    "the last statement together with proposition  [ prop : conv_consensus ] implies that @xmath262 , for all @xmath31 , thus concluding the proof .",
    "the proof of theorem  [ thm : primal_optimality ] below is inspired by @xcite , where the convergence of a running average sequence similar to @xmath263 is studied in a non distributed context .",
    "theorem  [ thm : primal_optimality ] note that @xmath54 is a convex combination of past values of @xmath53 , therefore , for all @xmath224 we have that @xmath264 .",
    "consider the quantity @xmath265 . by , under the convexity requirement of assumption  [ ass : convex ] , we have @xmath266 where the inequality ( as well as the subsequent ones ) is to be interpreted component - wise . step  8 of algorithm  [ alg : alg1 ] can be equivalently written as @xmath58_+$ ] , where @xmath59_+$ ] denotes the projection of its argument on @xmath20 .",
    "( see also discussion at the end of section [ sec : setup ] ) .",
    "therefore , @xmath267 by , with @xmath268 in place of @xmath44 , leads to @xmath269 where the first equality follows from the definition of @xmath270 , the second inequality involves an exchange on the summation order , the third equality is due to assumption  [ ass : weights ] , and the last one is obtained after some term cancellations . since @xmath271 is a bounded sequence , and due to the fact that @xmath272 , taking the limit superior in we obtain that @xmath273    consider now the quantity @xmath274 for any @xmath113 . by , under assumption",
    "[ ass : convex ] , we have that @xmath275 by in lemma  [ lemma : relation ] with @xmath169 and @xmath168 , for any @xmath195 , with @xmath268 in place of @xmath44 , and after neglecting the negative term @xmath276 , we have that @xmath277 where the second inequality follows from the fact that @xmath278 due to . substituting in",
    "we have that @xmath279 by assumption  [ ass : ck_coefficient].2 , corollary  [ cor : boundedness ] , and the summability result in , we know that all terms inside the parentheses are finite as @xmath280 .",
    "therefore , @xmath281 however , by we have that @xmath282 , hence @xmath283    by and , and due to the fact that @xmath284 is continuous as an effect of @xmath11 and @xmath13 being convex under assumption [ ass : convex ] , we have that all limit points of @xmath285 are feasible and achieve the optimal value .",
    "this implies that they are optimal for the primal problem , thus concluding the proof .",
    "in this section we provide a numerical example to demonstrate the proposed approach .",
    "consider a network of @xmath286 agents connected as in figure  [ fig : graph ] ; for simplicity we assumed that the network topology is fixed .     agents . ]",
    "each agent @xmath2 , @xmath31 , has @xmath287 decision variables , and a local constraint set @xmath288{\\times}[-10,10]$ ] , its objective function defined as @xmath289 , where the coefficients @xmath290 are independently extracted at random from a gaussian distribution with zero mean and covariance matrix equal to @xmath291 , @xmath292 being the identity matrix of size @xmath293 .",
    "the coupling constraint is given by @xmath294 , where , for all @xmath31 , @xmath295 is a @xmath296 matrix whose entries are independent realizations of a normal random variable with zero mean and unitary variance , and @xmath297{\\!^\\top\\!}$ ] .",
    "the resulting optimization problem is given by @xmath298 to put in the form of @xmath10 it suffices to represent equality constraints by double - sided inequalities . to this end ,",
    "set @xmath299 \\displaystyle \\frac{1}{m}b - a_i x_i \\end{bmatrix},\\ ] ] for all @xmath31 , which is linear in @xmath300 and therefore convex .",
    "using this reformulation we have a vector of lagrange multipliers @xmath301{\\!^\\top\\!}\\in \\mathbb{r}^4_+$ ] .",
    "the lagrange multipliers associated with the equality constraints of are then given by @xmath302{\\!^\\top\\!}$ ] .",
    "we ran the algorithm  [ alg : alg1 ] for 10000 iterations .",
    "figure  [ fig : multipliers ] shows the evolution of the agents estimates @xmath303 , @xmath31 , of the vector @xmath304 across iterations . as expected , all agents gradually reach consensus on the optimal lagrange multipliers of ( red triangles ) .    , @xmath31 , of the vector @xmath304 .",
    "red triangles represent the optimal dual solution . ]",
    "figure  [ fig : obj_constr ] shows the evolution of the primal objective value @xmath305 ( upper plot ) , and constraint violation in terms of @xmath306 ( lower plot ) , where @xmath300 is replaced by two different sequences : @xmath307 ( dashed lines ) , and @xmath308 ( solid lines ) , @xmath308 being defined as @xmath309 where @xmath310 is the iteration index related to a specific event , namely , the `` practical '' convergence of the lagrange multipliers .",
    "( upper plot ) and constraint violation @xmath306 ( lower plot ) as a function of @xmath307 ( dashed lines ) , and @xmath308 ( solid lines ) . ]    in the proposed example @xmath311 corresponds to the first iteration index @xmath44 where @xmath312 , with @xmath313 .",
    "it can be shown that theorem  [ thm : primal_optimality ] holds also for @xmath314 , @xmath31 .",
    "although the rate of asymptotic convergence of the @xmath315 sequences is the same of the @xmath316 sequences , @xmath317 shows an improved numerical behavior .    for the sake of completeness",
    "we also show in figure  [ fig : primal_ag2 ] the evolution of the sequences @xmath316 ( dashed lines ) , and @xmath315 ( solid lines ) for agent @xmath318 ; the evolution for other agents is similar . following the previous discussion ,",
    "the convergence rate properties of @xmath319 seem to be superior compared to @xmath320 .",
    "( dashed lines ) and @xmath321 ( solid lines ) .",
    "red triangles represent the optimal primal solution for agent @xmath322 . ]    from an implementation point of view , we observed that the algorithm would result in the same solution even if we do not represent equality constraints with double - sided inequalities , but allow @xmath323 in step  8 of algorithm  [ alg : alg1 ] to be also negative .",
    "in this paper we proposed a novel distributed algorithm for a certain class of convex optimization programs , over time - varying multi - agent networks .",
    "more precisely , an iterative scheme combining dual decomposition and proximal minimization was conceived , which converges to some optimal dual solution of the centralized problem counterpart , while primal iterates generated by the algorithm converge to the set of primal minimizers .",
    "a numerical example was also provided to better illustrate the features of the proposed methodology .",
    "current work focuses on the analysis of the convergence rate , and on the relaxation of the convexity assumption by extending the results of @xcite to a distributed set - up , and quantifying the duality gap incurred in case of mixed - integer programs .    from an application point of view , our goal is applying the proposed algorithm to the problem of optimal energy management of a building network @xcite .",
    "to ease the reader , we report here the deterministic version of the supermartingale convergence theorem @xcite ( proposition 8.2.10 , p. 489 ) , which is used in theorem  [ thm : alg_conv ] .",
    "[ thm : supermartingale ] let @xmath324 , @xmath325 , and @xmath326 be sequences of non - negative vectors , the latter satisfying @xmath201 . if @xmath327 we then have that @xmath328 , and that the sequence @xmath329 is convergent .",
    "d.  ioli , a.  falsone , and m.  prandini .",
    "an iterative scheme to hierarchically structured optimal energy management of a microgrid . in _",
    "ieee 54th annual conference on decision and control ( cdc ) _ , pages 52275232 , 2015 ."
  ],
  "abstract_text": [
    "<S> we study distributed optimization in a cooperative multi - agent setting , where agents need to agree on the usage of shared resources and can communicate via a time - varying network to this purpose . </S>",
    "<S> each agent has its own decision variables that should be set so as to minimize its individual objective function subject to local constraints . </S>",
    "<S> resource sharing is modeled via coupling constraints that involve the non - negativity of the sum of agents individual functions , each one depending on the decision variables of one single agent . </S>",
    "<S> we propose a novel distributed algorithm to minimize the sum of the agents objective functions subject to both local and coupling constraints , where dual decomposition and proximal minimization are combined in an iterative scheme . </S>",
    "<S> notably , privacy of information is guaranteed since only the dual optimization variables associated with the coupling constraints are exchanged by the agents . under convexity assumptions , jointly with suitable connectivity properties of the communication network , we are able to prove that agents reach consensus to some optimal solution of the centralized dual problem counterpart , while primal variables converge to the set of optimizers of the centralized primal problem . </S>",
    "<S> a numerical example shows the efficacy of the proposed approach .        ,    ,    ,    distributed optimization , consensus , dual decomposition , proximal minimization . </S>"
  ]
}