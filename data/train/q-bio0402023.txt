{
  "article_text": [
    "neocortical circuits are highly connected : a typical neuron receives synaptic input from of the order of 10000 other neurons .",
    "this fact immediately suggests that mean field theory should be useful in describing cortical network dynamics .",
    "furthermore , a good fraction , perhaps half , of the synaptic connections are local , from neurons not more than half a millimeter away , and on this length scale ( i.e. , within a  cortical column  ) the connectivity appears to be highly random , with a connection probability of the order of 10% .",
    "this requires a level of mean field theory a step beyond the kind used for uniform systems in condensed matter physics like ferromagnets .",
    "it has to describe correctly the fluctuations in the inputs to a given network element as well as their mean values , as in spin glasses .",
    "the theory we use here is , in fact , adapted directly from that for spin glasses .",
    "a generic feature of mean field theory for spin glasses and other random systems is that the `` quenched disorder '' in the connections ( the connection strengths in the network do not vary in time ) leads to an effectively noisy input to a single unit that one studies : spatial disorder is converted to temporal .",
    "the presence of this noise offers a fundamental explanation for the strong irregularity of firing observed experimentally in cortical neurons . for high connectivity ,",
    "the noise is gaussian , and the correct solution of the problem requires its correlation function to be found self - consistently . in this paper we summarize how to do this for some simple models for cortical networks .",
    "we focus particularly on the neuronal firing statistics .",
    "there is a long history of experimental investigations of the apparent noisiness of cortical neurons @xcite , but very little in the way of theoretical work based on network models .",
    "our work begins to fill that gap in a natural way , since the full mean field theory of a random network is based on self - consistently calculating the correlation function .",
    "in particular , we are able to identify the features of the neurons and synapses in the network that control the firing correlations .    the basic ideas developed here were introduced in a short paper @xcite , and these models are treated in greater detail in several other papers @xcite .",
    "here we just want to give a quick overview of the mean field approach and what we can learn from it .",
    "in all the work described here , our neurons are of the leaky integrate - and - fire kind , though it is straightforward to extend the method to other neuronal models , based , for example , on hodgkin - huxley equations . in our simplest model , we consider networks of excitatory and inhibitory neurons , each of which receives a synaptic connection from every other neuron with the same probability .",
    "each such connection has a  strength , \" the amount by which a presynaptic spike changes the postsynaptic potential . in this model , these strengths are independent of the postsynaptic membrane potential ( `` current - based synapses '' ) . all excitatory  to - excitatory connections that are present are taken to have the same strength , and analogously for the three other classes of connections ( excitatory - to - inhibitory , etc . ) .",
    "however , the strengths for the different classes are not the same . in addition , excitatory and inhibitory neurons both receive excitation from an external population , representing `` the rest of the brain '' .",
    "( for primary sensory cortices , this excitation includes the sensory input from the thalamus . )",
    "this is probably the simplest generic model for a generic `` cortical column '' of spiking neurons .",
    "the network is taken to have @xmath0 excitatory and @xmath1 inhibitory neurons . a given neuron ( of either kind )",
    "receives synaptic input from every excitatory ( resp .",
    "inhibitory ) neuron with probability @xmath2 ( resp .",
    "@xmath3 , with @xmath4 independent of @xmath5 . in our calculations we take the connection density @xmath4 to be 10% , but the results are not very sensitive to its value as long as it is fairly small .",
    "each nonzero synapse from a neuron in population @xmath6 to one in population @xmath5 is taken to have the value @xmath7 .",
    "synapses from the external population are treated in the same way , with strengths @xmath8 . for simplicity",
    ", neurons in the external population are assumed to fire like stationary independent poisson processes .",
    "we consider the limit @xmath9 , @xmath10 , with @xmath4 fixed , where mean field theory is exact .",
    "the subthreshold dynamics of the membrane potential of neuron @xmath11 in population @xmath5 obey @xmath12 where @xmath13 is the spike train of neuron @xmath14 in population @xmath6 .",
    "the membrane time constant is taken to have the same value @xmath15 for all neurons .",
    "we give the firing thresholds a narrow distribution of values ( 10% of the mean value , 1 ) .",
    "we take the firing thresholds @xmath16 and the postfiring reset levels to be 0 .",
    "we ignore transmission delays .",
    "the essential point of mean field theory is that for such a large , homogeneously random network , as for an infinite - range spin glass @xcite , we can treat the net input to a neuron as a gaussian random process .",
    "this reduces the network problem to a single - neuron one , with the feature that the statistics of the input have to be determined self - consistently from the firing statistics of the single neurons .",
    "this reduction was proved formally for a network of spiking neurons by fulvi mari @xcite .",
    "explicitly , the effective input current to neuron @xmath11 in population @xmath5 can be written @xmath17 .",
    "{ \\label}{eq : decrec}\\ ] ] here @xmath18 is the average rate in population @xmath6 , @xmath19 @xmath20 is a unit - variance gaussian random number , and @xmath21 is a ( zero - mean ) gaussian noise with correlation function equal to @xmath22 , the average correlation function in population @xmath6 . for the contribution from a single population , labeled by @xmath6 , the first term in ( [ eq : decrec ] ) , which represents the mean input current , is larger than the other two , which represent fluctuations , by a factor of @xmath23 : averaging over many independent input neurons reduces fluctuations relative to those in a single neuron by the square root of the number of terms in the sum .",
    "( for our way of scaling the synapse strengths , the factor @xmath24 in the first term arises formally from adding @xmath25 terms , each of which is proportional to @xmath26 . )    however , while the fluctuation terms are small in comparison to the mean for a given input population @xmath6 , small compared to the population - averaged input ( the first term in ( [ eq : decrec ] ) ) , we will see that when we sum over all populations the first term will vanish to leading order .",
    "what remains of it is only of the same order as the fluctuations .",
    "therefore fluctuations can not be neglected .",
    "the fact that the fluctuation terms are gaussian variables is just a consequence of the central limit theorem , since we consider the limit @xmath27 .",
    "note that one fluctuation term is static and the other dynamic .",
    "the origin of the static one is the fact that the network is inhomogeneous , so different neurons will have different number of synapses and therefore different strengths of net time - averaged inputs .",
    "it is perhaps not immediately obvious , but the formal derivation ( @xcite , see also @xcite for an analogous case ) shows that the dynamic noise also originates from the random inhomogeneity in the network .",
    "it would be absent if there were no randomness in the connections , as , for example , in a model like ours but with full connectivity .",
    "the presence of the factor @xmath28 in the third term in ( [ eq : decrec ] ) makes this point evident ; in the general case the noise variance is proportional to the variance of the connection strengths .      in any mean field theory ,",
    "whether it is for a ferromagnet , a superconductor , electroweak interactions , or a neural network , one has to make an _ ansatz _ describing the state in question .",
    "this ansatz contains some parameters ( generally called  order parameters \" ) , the values of which are then determined self - consistently . here",
    ", our  order parameters \" are the mean rates @xmath29 , their mean square values ( which appear in ( [ eq : bb ] ) , and the correlation functions @xmath22 .",
    "we make an _ ansatz _ for the correlation functions that describes an asynchronous irregular firing state : we take @xmath29 to be time - independent and @xmath22 to have a delta - function peak ( of strength equal to @xmath29 ) at @xmath30 , plus a continuous part that falls off toward zero as @xmath31 .",
    "we could also look , for example , for solutions in which @xmath29 was time - dependent and/or @xmath22 had extra delta - function peaks ( these might describe oscillating population activity ) , but we have not done so .",
    "thus , we can not exclude the existence of such exotic states , but we can at least check whether our asynchronous , irregularly - firing states exist and are stable .",
    "we can find the mean rates , at least when they are low , independently of their fluctuations and the correlation functions : in an irregularly - firing state the membrane potential should fluctuate around a stationary value , with the noise occasionally and irregularly driving it up to threshold . in mean field theory , we have @xmath32 where @xmath33 is given by ( [ eq : decrec ] ) .",
    "( we have dropped the neuron index @xmath11 , since we are now doing a one - neuron problem . ) from ( [ eq : decrec ] ) , we see that the leading terms in @xmath33 are large ( @xmath34 ) , so if the membrane potential is to be stationary they must nearly cancel : @xmath35 that is , the mean excitatory ( @xmath36 ) and inhibitory ( @xmath37 ) currents must nearly balance . therefore we call ( [ eq : balance ] ) the balance condition . defining @xmath38 , we can also write it in the form @xmath39 the external rate @xmath40 is assumed known , so these two linear equations can be solved for @xmath29 , @xmath41 .",
    "we can write the solution as @xmath42_{ab } j_{b0}r_0 , { \\label}{eq : balsoln}\\ ] ] where by @xmath43 we mean the inverse of the @xmath44 matrix with elements @xmath45 , @xmath46 .",
    "this result was obtained some time ago by amit and brunel @xcite and , for a nonspiking neuron model , by van vreeswijk and sompolinsky @xcite .",
    "however , a complete mean field theory involves the rate fluctuations within the populations and the correlation functions , and it is clear that if we want to understand something quantitative about the degree of irregularity of the neuronal firing , it is necessary to do the full theory .",
    "this can not be done analytically , so we resort to numerical calculation .",
    "our method was inspired by the work of eisfeller and opper @xcite on spin glasses .",
    "they , too , had a mean field problem that could not be solved analytically , so they solved numerically the single - spin problem to which mean field theory reduced their system . in our case , we have to solve numerically , the problem of a single neuron driven by gaussian random noise , and the crucial part is to make the input noise statistics consistent with the output firing statistics .",
    "this requires an iterative procedure .",
    "we have to start with a guess about the mean rates , the rate fluctuations , and the correlation functions for the neurons in the two populations .",
    "we then generate noise according to ( [ eq : decrec ] ) and simulate many trials of neurons driven by realizations of this noise . in these trials ,",
    "the effective numbers of inputs @xmath25 are varied randomly from trial to trial , with a gaussian distribution of width @xmath24 , to capture the effects of the random connectivity in the network .",
    "we compute the firing statistics for these trials and use the result to improve our estimate of the input noise statistics .",
    "we then repeat the trials and iterate the loop until the input and output statistics agree .",
    "we can get a good initial estimate of the mean rates from the balance condition equation ( [ eq : balsoln ] ) , but this is harder to do for the rate fluctuations and correlation function .",
    "the method we have used is to do the initial trials with white noise input ( of a strength determined by the mean rates ) .",
    "there seems to be no problem converging to a solution with self consistent rates , rate fluctuations and firing correlations from this starting point .",
    "more details of the procedure can be found in @xcite .      as a measure of the firing irregularity , we consider the fano factor @xmath47 .",
    "it is defined as the ratio of the variance of the spike count to its average , where both statistics are computed over a large number of trials .",
    "it is easy to relate it to the correlation function , as follows .",
    "if @xmath48 is a spike train as in ( [ eq : model1 ] ) , the spike count in an interval from 0 to @xmath49 is @xmath50 its mean is just @xmath51 , and its variance is @xmath52 \\rangle        { \\label}{eq : varcount}\\ ] ] the quantity in the averaging brackets in ( [ eq : varcount ] ) is just the correlation function .",
    "changing integration variables from @xmath53 to @xmath54 and taking @xmath55 gives @xmath56 for a poisson process , @xmath57 , leading to @xmath58 .",
    "thus , a fano factor greater than 1 is not really `` more irregular than a poisson process '' , since any deviation of @xmath47 from 1 comes from some kind of firing correlations .",
    ", for 3 values of the relative inhibition parameter @xmath59.,width=453,height=302 ]    for this model we have studied how the magnitude of the synaptic strengths affects the fano factor .",
    "we have used @xmath60 in fig .",
    "[ fig : js - ff ] we plot f as a function of the overall scaling factor @xmath61 for three different values of the relative inhibition strength @xmath59 .",
    "evidently , increasing synaptic strength in either way increases @xmath47 .",
    "how can we understand this result ?",
    "let us think of the stochastic dynamics of the membrane potential @xmath62 after a spike and reset , as described , for example , by a fokker - planck equation .",
    "right after reset , the distribution of @xmath62 is a delta - function at the reset level .",
    "then it spreads out diffusively and its center drifts toward a quasi - equilibrium level .",
    "the speed of the spread and the width of the quasi - equilibrium distribution reached after a time @xmath63 are both proportional to the synaptic strength .",
    "this distribution is only `` quasi - equilibrium '' because on the somewhat longer timescale of the typical interspike interval , significant weight will reach the absorbing boundary at threshold .",
    "nevertheless , we can regard it as nearly stationary if the rate is much less than @xmath64 .",
    "the center of the quasi - equilibrium distribution has to be at least a standard deviation or so below threshold if the neuron is going to fire at a low - to - moderate rate .",
    "thus , since this width is proportional to the synaptic strengths , if we fix the reset at zero and the threshold at 1 the drift of the distribution after reset will be _ upward _ for sufficiently weak strengths and _ downward _ for strong enough ones .",
    "hence , in the weak case , there is a reduced probability of spikes ( relative to a poisson process ) for times shorter than @xmath15 , leading to a refractory dip in the correlation function and a fano factor bigger than 1 . in the strong - synapse case , the rapid initial spread of the membrane potential distribution before it has a chance to drift very far downward leads to excess early spikes , a positive correlation function at short times , and a fano factor bigger than 1 .",
    "the relevant ratio is the width of the quasi - equilibrium membrane potential distribution ( for this model , roughly speaking , @xmath61 ) divided by the different between reset and threshold .",
    "the above argument applies even for neurons with white noise input .",
    "but in the mean field description the firing correlation induced by this effect lead to correlations in the input current , which amplify the effects .",
    "in a second model , we add a touch of realism , replacing the current - based synapses by conductance - based ones .",
    "then the postsynaptic potential change produced by a presynaptic spike is equal to a strength parameter multiplied by the difference between the postsynaptic membrane potential and the reversal potential for the class of synapse in question .",
    "in addition , we include a simple model for synaptic dynamics : we need no longer assume that the postsynaptic potential changes instantaneously in response to the postsynaptic spike .    now the subthreshold membrane potential dynamics become @xmath65 here @xmath66 is a nonspecific leakage conductance ( taken in units of inverse time ) ; it corresponds to @xmath64 in ( [ eq : model1 ] ) .",
    "the @xmath67 are the reversal potentials for the synapses from population @xmath6 ; they are above threshold for excitatory synapses and below 0 for inhibitory ones , so the synaptic currents are positive ( i.e. , inward ) and negative ( outward ) , respectively , in these cases . the time - dependent synaptic conductances @xmath68 reflect the firing of presynaptic neuron @xmath14 in population @xmath6 , filtered at its synapse to postsynaptic neuron @xmath11 in population @xmath5 : @xmath69 when a connection between these neurons is present ; otherwise it is zero .",
    "( we assume the same random connectivity distribution as in the previous model . )",
    "we have taken the synaptic filter kernel @xmath70 to have the simple form @xmath71 representing an average temporal conductance profile following a presynaptic spike , with characteristic opening and closing times @xmath72 and @xmath73 .",
    "this kernel is normalized so that @xmath74 ; thus , the total time integral of the conductance over the period after an isolated spike is equal to @xmath75 .",
    "hence , for very short synaptic filtering times , this model looks like ( [ eq : model1 ] ) with a membrane potential - dependent @xmath76 equal to @xmath77 .",
    "we take the ( dimensionless ) parameters @xmath78 , like the @xmath79 in the previous model , to be of order 1 , so we anticipate a large ( @xmath80 ) mean current input from each population @xmath6 and , in the asynchronously - firing steady state , a near cancellation of these separately large currents .    in mean field theory , we have the effective single - neuron equation of motion @xmath81 in which the total effect of population @xmath6 on a neuron in population @xmath5 is a time - dependent conductance @xmath82 consisting of a population mean @xmath83 static noise of variance @xmath84 and dynamic noise with correlation function @xmath85 where @xmath86 is the correlation function of the synaptically filtered spike trains of population @xmath6 .      as for the model with current - based synapses",
    ", we can argue that in an irregularly , asynchronously - firing state the average @xmath87 should vanish . from ( [ eq : dumf ] ) we obtain @xmath88 again , for large connectivity the leakage term can be ignored .",
    "in contrast to what we found in the current - based case , now the balance condition requires knowing the mean membrane potential @xmath89 .",
    "however , we will see that in the mean field limit the membrane potential has a narrow distribution centered just below threshold .",
    "since the fluctuations are very small , the factor @xmath90 in ( [ eq : dufull ] ) can be regarded as constant , and we are effectively back to the current - based model .",
    "thus , defining @xmath91 we can just apply the analysis from the current - based case .",
    "it is useful to measure the membrane potential relative to @xmath89 .",
    "so , writing @xmath92 and using the balance condition ( [ eq : condbal ] ) , we find @xmath93 where @xmath94 ,                        { \\label}{eq : gtot}\\end{aligned}\\ ] ] with @xmath95 the fluctuating parts of @xmath82 , the statistics of which are given by ( [ eq : fluctg ] ) and ( [ eq : covarg ] ) .",
    "this looks like a simple leaky integrator with current input @xmath96 and a time - dependent effective membrane time constant equal to @xmath97 .",
    "following shelley _",
    "@xcite , ( [ eq : ddumf ] ) can be further rearranged into the form @xmath98 ,                                   { \\label}{eq : chaseus}\\ ] ] with the `` instantaneous reversal potential '' ( here measured relative to @xmath89 ) given by @xmath99 eq .",
    "( [ eq : chaseus ] ) says that at any instant of time , @xmath100 is approaching @xmath101 at a rate @xmath102",
    ".     follows the effective reversal potential @xmath103 closely , except when @xmath104 is above threshold . here",
    ", the threshold is 1 and the reset 0.94.,width=453,height=302 ]    for large @xmath25 , @xmath105 is large ( @xmath80 ) , so the effective membrane time constant is very small and the membrane potential follows the fluctuating @xmath101 very closely . fig .  [",
    "fig : vvs ] shows an example from one of our simulations .",
    "this is the main qualitative difference between mean field theory for the model with current - based synapses and the one with conductance - based ones .",
    "it is also the reason we introduced synaptic filtering into the present model . in the current - based one ,",
    "the membrane potential filtered the input current with a time constant @xmath15 which we could assume to be long compared with synaptic time constants , so we could safely ignore the latter .",
    "but here the effective membrane time constant becomes shorter than the synaptic filtering times , so we have to retain the kernel @xmath70 ( [ eq : synfilter ] ) . here",
    "we have argued this solely from the fact that we are dealing with the mean field limit , but shelley _ et al . _",
    "argue that it actually applies to primary visual cortex ( see also @xcite ) .",
    "we also observe that in the mean field limit , both the leakage conductance and the fluctuation term @xmath95 are small in comparision with the mean , so we can approximate @xmath106 by a constant : @xmath107 furthermore , the fluctuations @xmath101 in the instantaneous reversal potential ( [ eq : instrevpot ] ) are then of order @xmath26 : membrane potential fluctuations can not go far from @xmath89 .",
    "but @xmath108 must go above threshold frequently enough to produce firing at the self - consistent rates .",
    "thus , @xmath89 must lie just a little below threshold , as promised above . hence , at fixed firing rates , the conductance - based problem effectively reduces to a current - based one with a very small effective membrane time constant @xmath109 and synaptic coupling parameters @xmath110 given by ( [ eq : jeff ] ) .",
    "of course , as we increase the firing rates of the external population and thereby increase the rates in the network , we will change @xmath105 , making both @xmath111 and the fluctuations @xmath112 correspondingly smaller .",
    "if we neglected synaptic filtering , the resulting dynamics would be rather trivial",
    ". it would be self - consistent to take the input current as essentially white noise , for then excursions of @xmath112 above threshold would be be uncorrelated , and , since the membrane potential could react instantaneously to follow it up to threshold , so would the firing be .",
    "( simulations confirm this argument . )    therefore , the synaptic filtering is essential .",
    "it imparts a positive correlation time to the fluctuations @xmath101 , so if it rises above threshold it can stay there for a while . during this time , the neuron will fire repeatedly , leading to a positive tail in the correlation function for times of the order of the synaptic time constants .",
    "this broader the kernel @xmath113 , the stronger this effect . in the self - consistent description ,",
    "this effect feeds back on itself : @xmath101 acquires even longer correlations , and these lead to even stronger bursty firing correlations .",
    "thus , the mean field limit @xmath27 can be pathological in the conductance - based model with synaptic filtering",
    ". however , here we take the view that mean field theoretical calculations may still give a useful description of real cortical dynamics , despite that fact that real cortex is not described by the @xmath27 limit .",
    "for example , the true effective membrane time constant is not zero , but , according to experiment @xcite , it is significantly reduced from its _ in vitro _",
    "value by synaptic input , probably @xcite to a value less than characteristic synaptic filtering times .",
    "doing mean field theory with moderately , but not extremely large connectivities can describe such a state in a natural and transparent way .     for 3 reset values.,width=453,height=302 ]    as in the current - based model , the fano factor grows with the ratio of the membrane potential distribution to the threshold ",
    "reset difference .",
    "it also grows with increasing synaptic filtering time , as argued above .",
    "[ fig : tau - ff ] shows @xmath47 plotted as a function of @xmath73 , with @xmath72 fixed at 1 ms , for a set of reset values .",
    "finally , we try to take a step beyond homogeneously random models to describe networks with systematic structure in their connections .",
    "we consider the example of a hypercolumn in primary visual cortex : a collection of @xmath114 orientation columns , within which each neuron responds most strongly to a stimulus of a particular orientation .",
    "the hypercolumn contains columns that cover the full range of possible stimulus orientations from 0 to @xmath115 .",
    "it is known that columns with similar orientation selectivities interact more strongly than those with dissimilar ones ( because they tend to lie closer to each other on the cortical surface ) .",
    "we build this structure into an extended version of our model , which can be treated with essentially the same mean field methods as the simpler , homogeneously random one . in the version we present here",
    ", we revert to current - based synapses , but it is straightforward to construct a corresponding model with conductance - based ones .",
    "a study of a similar model , for non - spiking neurons , was reported by wolf _ et al .",
    "those authors have also simulated a network of spiking neurons like the one described here @xcite , complementing the mean - field analysis we give here .",
    "each neuron now acquires an extra index @xmath116 labeling the stimulus orientation to which it responds most strongly , so the equations of motion for the membrane potentials become @xmath117 the term @xmath118 represents the external input current for a stimulus with orientation @xmath119 .",
    "( in this section we set all thresholds equal to 1 , and @xmath116 refers only to orientation . )",
    "we assume it comes through diluted connections from a population of @xmath120 poisson neurons which fire at a constant rate @xmath40 : @xmath121 as in the single - column model , we take the nonzero connections to have the value @xmath122 but now we take the connection probability to depend on the difference between @xmath116 and @xmath119 , according to @xmath123 .                           {",
    "\\label}{eq : tunedin}\\ ] ] this tuning is assumed to come from a hubel - wiesel feed - forward connectivity mechanism .",
    "the general form has to be periodic with period @xmath115 and so would have terms proportional to @xmath124 for all @xmath125 , but here , following ben - yishai _ et al . _",
    "@xcite , we use the simplest form possible .",
    "we have also assumed that the degree of tuning , measured by the anisotropy parameter @xmath126 , is the same for inhibitory and excitatory postsynaptic neurons .",
    "assuming isotropy , we are free to measure all orientations relative to @xmath119 , so we set @xmath127 from now on .    similarly , we take the nonzero intracortical interactions @xmath128 to be @xmath129 and take the probability of connection to be @xmath130 .",
    "{ \\label}{eq : jtuning}\\ ] ] analogously to ( [ eq : tunedin ] ) , @xmath131 is independent of both the population indices @xmath5 and @xmath6 , since we always take @xmath132 independent of @xmath6 .    in real cortex ,",
    "cells are arranged so they generally lie closer to ones of similar than to ones of dissimilar orientation preference , and they are more likely to have connections with nearby cells than with distant ones .",
    "this is the anatomy that we model with ( [ eq : jtuning ] ) . in @xcite and @xcite a slightly different model was used , in which the connection probability was taken constant but the strength of the connections was varied like ( [ eq : jtuning ] ) .",
    "the equations below for the balance condition and the column population rates are the same for both models , but the fluctuations are a little different .    as for the input tuning ,",
    "the form ( [ eq : jtuning ] ) is just the first two terms in a fourier series , but again we use the simplest possible form for simplicity .      the balance condition for the hypercolumn model is simply that the net synaptic current should vanish for each column @xmath116 .",
    "going over to a continuum notation by writing the sum on columns as an integral , we get @xmath133\\sqrt{k_b } r_b(\\theta ' ) = 0 .",
    "{ \\label}{eq : meanorient}\\ ] ] we have to distinguish two cases : broad and narrow tuning . in the broad case , the rates @xmath134 are positive for all @xmath116 .",
    "in the narrowly - tuned case ( the physiologically realistic one ) , @xmath134 is zero for @xmath135 greater than some @xmath136 , which we call the tuning width .",
    "( in general @xmath136 could be different for excitatory and inhibitory neurons , but with our @xmath5-independent @xmath126 in ( [ eq : tunedin ] ) and @xmath5- and @xmath6-independent @xmath137 in ( [ eq : jtuning ] ) , it turns out not to . )    in the broad case the integral over @xmath138 can be done trivially with the help of the trigonometric identity @xmath139 and expanding @xmath140 .",
    "we find that the higher fourier components @xmath141 , @xmath142 , do not enter the result : @xmath143 if ( [ eq : broadbalance ] ) is to hold for every @xmath116 , the constant piece and the part proportional to @xmath144 both have to vanish : for each fourier component we have an equation like ( [ eq : balance ] ) .",
    "thus we get a pair of equations like ( [ eq : balsoln ] ) : @xmath145_{ab } j_{b0}r_0 & \\;\\;\\;\\ ; & r_{a,2 } = -\\frac{2\\epsilon}{\\gamma}\\sum_{b=1}^2 [ { \\sf \\hat j}^{-1}]_{ab } j_{b0}r_0   \\ ; ( = \\frac{2\\epsilon}{\\gamma}r_{a,0 } ) , { \\label}{eq : broadsoln}\\end{aligned}\\ ] ] where @xmath146 , as in the simple model .",
    "this solution is acceptable only if @xmath147 , since otherwise @xmath148 will be negative for @xmath149 .",
    "therefore , for @xmath150 , we make the _ ansatz _ @xmath151 ( i.e. , we write @xmath152 as @xmath153 ) for @xmath154 and @xmath155 for @xmath156 .",
    "we put this into the balance condition ( [ eq : meanorient ] ) .",
    "now the integrals run from @xmath157 to @xmath136 , so they are as trivial as in the broadly - tuned case , but the _ ansatz _ works and we find @xmath158 where @xmath159 ( the algebra here is essentially the same as that in a different kind of model studied by ben - yishai _ et al .",
    "_ @xcite ; see also @xcite . )    eqns .",
    "( [ eq : balnarrow ] ) can be solved for @xmath136 and @xmath160 , @xmath161 .",
    "dividing one equation by the other leads to the following equation for @xmath136 : @xmath162 then one can use either of the pairs of equations ( [ eq : balnarrow ] ) to find the remaining unknowns @xmath160 : @xmath163_{ab } j_{b0}r_0 .",
    "{ \\label}{eq : r2}\\ ] ]    the function @xmath164 takes the value 1 at @xmath165 and falls monotonically to @xmath166 at @xmath167 .",
    "thus , a solution can be found for @xmath168 . for @xmath169 , @xmath170 and we go back to the broad solution . for @xmath171 ,",
    "@xmath172 : the tuning of the rates becomes infinitely narrow .",
    "note that stronger tuning of the cortical interactions ( bigger @xmath137 ) leads to broader orientation tuning of the cortical rates .",
    "this possibly surprising result can be understood if one remembers that the cortical interactions ( which are essentially inhibitory ) act divisively ( see , e.g. , ( [ eq : broadsoln ] ) and ( [ eq : r2 ] ) ) .",
    "another feature of the solution is that , from ( [ eq : findtc ] ) , the tuning width does not depend on the input rate @xmath40 , which we identify with the contrast of the stimulus .",
    "thus , in the narrowly - tuned case , the population rates in this model automatically exhibit contrast - invariant tuning , in agreement with experimental findings @xcite .",
    "we can see that this result is a direct consequence of the balance condition .",
    "however , we should note that individual neurons in a column will exhibit fluctuations around the mean tuning curves which are not negligible , even in the mean - field limit . these come from the static part of the fluctuations in the input current ( like the second term in ( [ eq : decrec ] ) for the single - column model ) , which originate from the random inhomogeneity of the connectivity in the network .    as for the single - column model , the full solution , including the determination of the rate fluctuations and correlation functions , has to be done numerically .",
    "this only needs a straightforward extension of the iterative procedure described above for the simple model .",
    "we now consider the tuning of the dynamic input noise .",
    "using the continuum notation , we get input and recurrent contributions adding up to @xmath173c_b(\\theta ' , t - t ' ) , { \\label}{eq : noise}\\end{aligned}\\ ] ] where @xmath174 is the correlation function for population @xmath6 in column @xmath138 . we can not proceed further analytically for @xmath175 , since this correlation function has to be determined numerically .",
    "but we know that for an irregularly firing state @xmath176 always has a piece proportional to @xmath177 .",
    "this , together with the external input noise , gives a flat contribution to the noise spectrum of @xmath178r_b(\\theta ' ) .",
    "{ \\label}{eq : white}\\ ] ] the integrals on @xmath138 are of the same kind we did in the calculation of the rates above , so we get @xmath179 \\nonumber \\\\ \\ ; & = & r_0(1+\\epsilon \\cos 2\\theta)(j_{a0}^2 - \\sum_{bc } j_{ab}^2 [ { \\sf \\hat j}^{-1}]_{bc } j_{c0 } ) , { \\label}{eq : finalnoise}\\end{aligned}\\ ] ] where we have used ( [ eq : findtc ] ) and ( [ eq : r2 ] ) to obtain the last line .",
    "thus , the recurrent synaptic noise has the same orientation tuning as that from the external input , unlike the population firing rates , which are narrowed by the cortical interactions .",
    "to study the tuning of the noise in the neuronal firing , we have to carry out the full numerical mean field computation .",
    "[ fig : fanotuning ] shows results for the tuning of the fano factor with @xmath116 , for three values of the overall synaptic strength factor @xmath61 . for small @xmath61",
    "there is a minimum in @xmath47 at the optimal orientation ( 0 ) , while for large @xmath61 there is a maximum .",
    "it seems that for any @xmath61 , @xmath47 is either less than 1 at all angles or greater than 1 at all angles ; we have not found any cases where the firing changes from subpoissonian to superpoissonian as the orientation is varied .     for 3 values of @xmath61.,width=453,height=302 ]",
    "these examples show the power of mean field theory in studying the dynamics of dense , randomly - connected cortical circuits , in particular , their firing statistics , described by the autocorrelation function and quantities derived from it , such as the fano factor .",
    "one should be careful to distinguish this kind of mean field theory from ones based on `` rate models '' , where a function giving the firing rate as a function of the input current is given by hand as part of the model . by construction",
    ", those models can not say anything about firing statistics . here",
    "we are working at a more microscopic level , and both the equations for the firing rates and the firing statistics emerge from a self - consistent calculation .",
    "we think it is important to do a calculation that can tell us something about firing statistics and correlations , since the irregular firing of cortical neurons is a basic experimental fact that should be explained , preferably quantitatively , not just assumed .",
    "we were able to see in the simplest model described here how this irregularity emerges in mean field theory , provided cortical inhibition is strong enough .",
    "this confirms results of @xcite , but extends the description to a fully self - consistent one , including correlations .",
    "it became apparent how the strength of synapses and the post - spike reset level controlled the gross characteristics of the firing statistics , as measured by the fano factor . a high reset level and/or strong synapses result in an enhanced probability of a spike immediately after reset , leading to a tendency toward bursting .",
    "low reset and/or weak synapses have the opposite effect .",
    "visual cortical neurons seem to show typical fano factors generally somewhat above the poisson value of 1 .",
    "they have also been shown to have very high synaptic conductances under visual stimulation .",
    "our mean - field analysis of the model with conductance - based synapses shows how these two observed properties may be connected .    in view of the large variation of fano factors that there could be , it is perhaps remarkable that observed values do not vary more than they do .",
    "we like to speculate about this as a coding issue : any constrained firing correlations imply reduced freedom to encode input variations , so information transmission capacity is maximized when correlations are minimized .",
    "thus , plausibly , evolutionary pressure can be expected to keep synaptic strengths in the right range .",
    "finally , extending the description from a single `` cortical column '' to an array or orientation columns forming a hypercolumn provided a way of understanding the intracortical contribution to orientation tuning , consistent with the basic constraints of dominant inhibition , irregular firing and high synaptic conductance .",
    "these issues can also be addressed directly with large - scale simulations , as in @xcite .",
    "however mean field theory can give some clearer insight ( into the results of such simulations , as well as of experiments ) , since it reduces the network problem to a single - neuron one , which we have a better chance of understanding .",
    "so we think it is worth doing mean field theory even when it becomes as computationally demanding as direct network simulation ( as it does in the case of the orientation hypercolumn @xcite ) . at the least",
    ", comparison between the two approaches can allow one to identify clearly any true correlation effects , which are , by definition , not present in mean field theory .",
    "much more can be done with mean field theory than we have described here .",
    "first , as mentioned above , it can be done for any kind of neuron , even a multi - compartment one , not just a point integrate - and - fire one . at the level of the connectivity model ,",
    "the simple model described by ( [ eq : jtuning ] ) can also be extended to include more details of known functional architecture ( orientation pinwheels , layered structure , etc . ) .",
    "it is also fairly straightforward to add synaptic dynamics ( not just the average description of opening and closing of the channels on the postsynaptic side described by the kernel @xmath113 in ( [ eq : conductance ] ) ) .",
    "one just has to add a synaptic model which takes the spikes produced by our single effective neuron as input to a synaptic model .",
    "thus , the path toward including more potentially relevant biological detail is open , at non - prohibitive computational cost ."
  ],
  "abstract_text": [
    "<S> we review the use of mean field theory for describing the dynamics of dense , randomly connected cortical circuits . for a simple network of excitatory and inhibitory leaky integrate - and - fire neurons </S>",
    "<S> , we can show how the firing irregularity , as measured by the fano factor , increases with the strength of the synapses in the network and with the value to which the membrane potential is reset after a spike . generalizing the model to include conductance - based synapses </S>",
    "<S> gives insight into the connection between the firing statistics and the high - conductance state observed experimentally in visual cortex . </S>",
    "<S> finally , an extension of the model to describe an orientation hypercolumn provides understanding of how cortical interactions sharpen orientation tuning , in a way that is consistent with observed firing statistics . </S>"
  ]
}