{
  "article_text": [
    "modeling non - homogeneous and multi - component data is a problem that challenges scientific researchers in several fields , as e.g. in climatology , finance & insurance , meteorology , neuroscience , ...",
    "( see e.g. @xcite ) .",
    "in general , it is not possible to find a simple and closed form probabilistic model to describe such data .",
    "that is why one often resorts to non - parametric approaches , such as e.g. kernel density estimation ones ( see e.g. @xcite ) or non - parametric bayesian methods ( see e.g. @xcite ) , just to name a few .",
    "however , when the multiple components are separable , parametric modeling becomes again tractable .",
    "several hybrid models have been proposed in such context , combining two or more densities ( see e.g. @xcite ) .    in this study , we tackle the general problem in a specific case , when data exhibit heavy tails .",
    "extreme behaviors that are described by heavy tail modeling , can be observed for a large number of phenomena , natural ( from the big dutch floods of 1952 to the recent earthquake in italy ) , financial ( e.g. the sub - prime crisis in north america or the sovereign debt crisis in europe ) , medical ( e.g. the avian influenza ) , technological ( e.g. fukushima ) or others .",
    "one mathematical field , extreme value theory ( evt ) , which started with tippett and fisher ( 1928 ) , is totally devoted to the analysis and modeling of the extremes ( see @xcite for general references ) .",
    "studies on extremes were developed in many fields , as , for instance , in financial markets and actuarial mathematics ( see e.g. @xcite ) , in epidemiology ( see e.g. @xcite for the first introduction of evt in epidemiologic methods ) , in signal processing ( see e.g. @xcite when considering the general problem of false - alarms probability determination , or @xcite for the spike detection in neural signals in biomedicine , or @xcite for the detection of a binary signal in additive noise in telecommunication , or @xcite for the damage detection in machine diagnostics ) .",
    "introducing evt helps managing the many catastrophes that our society is facing with , unfortunately , an observed increasing trend of occurrence of extreme events since the beginning of the 20th century ( see @xcite ) , but also helps improving the standard data processing by taking into account the tail information .    whereas evt focuses on how to study and model extremes using the information in the tail of the distribution only ( which is the strength of this theory , even if sometimes also its weakness in practice as tail data are scarce by definition ) , it is also very useful to combine it with standard statistics developed for the main information given in the data . to extract the important information given by extremes and to highlight as well the information contained in the entire underlying distribution , it is natural to take into account the dissymmetry of the data weights above a high threshold ( tail ) and below it ( around the mean ) .",
    "different methods have been proposed so far to do it ( see e.g. @xcite ) .",
    "the main goal of this paper is to develop a self - calibrating method to model heavy tailed data , the choice of this class of unsupervised procedures being clearly to ease practical implementations ( in particular when complexity burden and/or delay processing are critical ) and to enlarge its applicability .",
    "indeed , the difficulty faced when applying standard methods of evt as the peaks over threshold ( pot ) approach ( first introduced by davison and smith in the 90 s ; see @xcite ) , the hill method ( see @xcite ) , or the qq - estimator one ( see @xcite ) , is that they are graphical ad hoc approaches .",
    "this self - calibrating method may be seen as two - folds : when ( i ) looking for a full modeling for non - homogeneous , multi - component and heavy tailed data , ( ii ) focusing on the tail and evaluating in an unsupervised way the high threshold over which the tail will be modeled ; it might then constitute an alternative evt method to standard ones as e.g. the pot approach .    in this paper",
    ", we introduce a hybrid model with several components including a generalized pareto distribution ( gpd ) , to take into account the heavy tail present in the data . without loss of generality ,",
    "we assume continuous and asymmetric right heavy tailed data , a similar treatment being possible on the left tail ( see @xcite ) .",
    "how many components of the hybrid model to consider and how to choose them ? since we are interested in fitting the whole distribution underlying asymmetric heavy tailed data , the idea is to consider both the mean and tail behaviors , and to use limit theorems for each one ( as suggested and developed analytically in @xcite ) , in order to make the model as general as possible .",
    "therefore , we introduce a gaussian distribution for the mean behavior , justified by the central limit theorem ( clt ) , and a gpd for the tail , since the pickands theorem ( see @xcite ) tells us that the tail of the distribution may be evaluated through a gpd above a high threshold . to bridge the gap between mean and asymptotic behaviors , we use an exponential distribution .",
    "a different weight has been assigned to each component in order to have a better handling of the extremes .",
    "the resulting three - components hybrid model is called g - e - gpd model .",
    "note that the gpd is the fixed component of this heavy tailed model , but the two other components could be chosen differently , depending on the data , and even reduced to one component ( as developed earlier in @xcite ) . indeed , specific treatment could be done to fit the exact distribution of the mean behavior for which we have much data , if we would like to avoid the use of the limiting normal distribution .",
    "for instance , when having skewed distribution near the mean , which is typical for insurance claims data , the normal distribution should be replaced by a lognormal without loss of generality ( see e.g. @xcite ) .",
    "it would not change the idea of the self - calibrating method . concerning the number of components , we point out that the model needs at least two - components , including the gpd , for the method to be workable .",
    "indeed , the threshold over which the gpd is fitted ( that we call the tail threshold ) , is determined in the algorithm as the junction point between the gpd and another distribution .",
    "contrary to standard evt approaches , it means that we need some information before the tail threshold to benefit from this self - calibrating method ( further investigation will be made to adapt the method when partial information is available before the tail threshold ) .",
    "moreover , the intermediate distribution ( here an exponential ) is used as a leverage to give full meaning of tail threshold to the last junction point between the gpd and its neighbour ( the intermediate distribution ) .",
    "the distance between two successive junction points will automatically tend to 0 when introducing unuseful components .",
    "an iterative unsupervised algorithm is developed for estimating the parameters of the three - components hybrid model .",
    "it starts by enforcing the continuity and the differentiability of the three components at the two junction points , then proceeds in an iterative way to determine successive thresholds and parameters of the involved distributions .",
    "it provides a judicious weighting of the three distributions as well as a good location for the junction points or thresholds , especially for the tail threshold that points out the presence of extremes .",
    "this algorithm is based , for each iteration , on the resolution of numerical optimisation problems in least squares sense , using the levenberg marquardt ( lm ) method ( e.g. @xcite ) .",
    "we study its convergence analytically and numerically .",
    "the performance of this self - calibrating method is studied in terms of goodness - of - fit on simulated data from g - e - gpd monte - carlo simulations . given the very good performance ,",
    "we apply the method on real data , considering neural data and the _ s&p500 _ log - returns . a comparison with other existing graphical approaches is also given .",
    "the paper is organized as follows . in section [ sec : model ] we introduce our hybrid model .",
    "the method and its unsupervised iterative algorithm are developed in section [ sec : algo ] .",
    "simulation results are presented in section [ sec : simul ] , and applications of the method on real data in section [ sec : applic ] .",
    "results are discussed , in both sections , accompanied with a comparison of those obtained via standard methods .",
    "conclusions follow in the last section .",
    "we consider a piecewise model where each component represents a different behavior of the data , which might be heterogeneous or not .",
    "we assume that the data admit a continuous ( non - degenerate ) distribution , and accordingly , we introduce a general hybrid probability density function ( pdf ) , with some smoothness constraints . without any loss of generality",
    ", we consider a three - components model .",
    "more precisely , the hybrid model we propose , links three different distributions to each other at two junction points , denoted by @xmath0 and @xmath1 : a gaussian distribution to model the mean behavior of the data , a gpd to represent the tail and an exponential distribution to bridge the gap between these two behaviors .",
    "this model , denoted by g - e - gpd ( gaussian - exponential - generalized pareto distribution ) , is characterized by its pdf @xmath2 expressed as : @xmath3 the different parameters are gathered in the vector denoted by @xmath4 and are described hereafter . to begin , @xmath5 , @xmath6 stand for the weights associated to each component .",
    "the parameters @xmath7 , and @xmath8 represent , respectively , the mean and the standard deviation of the gaussian pdf @xmath9 given by : @xmath10 , @xmath11 the parameters @xmath12 and @xmath13 denote , respectively , the tail index and the shape parameter of the gpd pdf @xmath14 , defined by : @xmath15 where @xmath16 & \\hbox{if}\\;\\;\\ ; \\xi<0                 \\end{array }               \\right .. \\ ] ] finally , @xmath17 indicates the intensity parameter of the exponential pdf @xmath18 defined by @xmath19 + in the sequel , we consider that the transitions from one behavior to another are smooth . as a consequence ,",
    "we constraint the resulting hybrid pdf @xmath2 to be @xmath20-regular .",
    "note that by combining this constraint and the assumption of heavy tailed data , the number of free parameters and hence the size of @xmath4 will be reduced .",
    "let us present these assumptions .",
    "the first two assumptions are part of the construction of the g - e - gpd model .    * first we assume , by construction , that the data distribution admits a pdf @xmath2 .",
    "this means that @xmath2 is non - negative and satisfies @xmath21 , _",
    "i.e. _ @xmath22 where @xmath23 denotes the cumulative distribution function ( cdf ) of the gaussian distribution .",
    "* we focus on heavy tailed data .",
    "this implies that @xmath2 belongs to the frchet maximum domain of attraction ( @xmath24 ) and therefore @xmath25 ( see _ e.g. _ @xcite , p. 159 ) .",
    "the main constraint is to require a smooth pdf and to further reduce the number of free - parameters , that is why we have imposed @xmath2 to be of class @xmath26 ;    * @xmath2 is continuous and differentiable at the two junctions points @xmath0 and @xmath1 .",
    "assumptions ( i)-(iii ) give rise to six equations relating all model parameters : @xmath27^{-1};\\\\    u_1=\\mu+\\lambda\\sigma^2 ; & \\gamma_3=\\beta\\,\\gamma_2 \\ ,",
    "e(u_2;\\lambda ) .",
    "\\end{array } \\right.\\ ] ] consequently , the parameter vector @xmath4 retains only the free parameters and we set @xmath28 $ ] . + it is then straightforward to deduce from @xmath2 the expression of the cdf and quantile function associated with the g - e - gpd model .",
    "the g - e - gpd cdf , denoted @xmath29 , is given by : @xmath30 and the corresponding quantile function by : @xmath31 where the notation @xmath32 denotes the inverse function of the function @xmath33 .    a classical problem that arises when dealing with parametric models concerns how to estimate the model parameters .",
    "to answer this problem , we develop an iterative algorithm for estimating the parameters vector @xmath4 .",
    "this algorithm is an extension of the one built in @xcite . for each iteration",
    ", it is based on the numerical resolution of optimization problems in least squares sense , using the levenberg marquardt ( lm ) method ( see @xcite ) .",
    "we describe it and study its convergence in the next session .",
    "here we describe the iterative algorithm suggested to estimate the g - e - gpd model parameters , which self - calibrates the model , in particular the threshold above which a frchet distribution fits the extremes .",
    "this algorithm follows the same logic as the one developed for two - components in @xcite .",
    "we will recall it in the appendix when studying its convergence . for each iteration , it breaks down the problem of the parameters vector @xmath4 estimation into two nested subproblems ; the parameters @xmath34 $ ] and @xmath35 are estimated alternatively . indeed ,",
    "for each iteration , we estimate the parameters vector @xmath36 by minimizing the squared error ( se ) between the empirical cdf and the estimated one , when considering the estimate of @xmath35 of the previous iteration . thereafter , @xmath35 will be estimated again , as well , by minimizing the se between the empirical cdf and the estimated one , using , this time , the estimated @xmath36 vector .",
    "evidently , this procedure starts by fixing initial parameters and ends when a stop condition is satisfied .",
    "small modifications on the parameters vector , the data scale , and the stop condition , are provided when going from the two- to the three - components algorithm , and will be detailed later on .",
    "these modifications have no influence on the functional principle of the algorithm , neither on how to study its convergence . in this convergence study , given in the appendix ,",
    "we prove analytically the existence of a stationary point , then show numerically that the stationary point is attractive and unique .",
    "this last part is still an open analytical question .",
    "let us describe this iterative algorithm .",
    "first , we consider an n - sample @xmath37 with a g - e - gpd parent distribution .",
    "we denote by @xmath38 an associated given realization . for the rest of this work ,",
    "@xmath39 and @xmath40 denote the initialization and the estimate of the parameter @xmath41 at the @xmath42 iteration , respectively .    to start its iterative process",
    ", the three - components algorithm needs the knowledge of @xmath43 , the initialization of @xmath35 .",
    "however , the only information we have about @xmath35 is that it is positive , which makes its initialization difficult .",
    "for that reason , we start initializing @xmath44 $ ] .",
    "to do so , we chose @xmath45 as the mode of the data , according to the fact that about @xmath46 of gaussian observations are bellow @xmath47 , we took @xmath48 , where @xmath49 represents the quantile of order @xmath46 associated to @xmath29 , and @xmath50 as a quantile of order sufficiently high ( above 80% , for this work )",
    ". then we use this initialization @xmath51 to determine @xmath43 , minimizing the se between the hybrid cdf given @xmath52 ( fixed ) , and the empirical cdf @xmath53 associated to the sample @xmath54 , defined , for all @xmath55 , by @xmath56 . to do so",
    ", we do not evaluate this se on the realizations @xmath57 only ( as there might be only a few observations in the tail ) , but on a generated sequence of synthetic increasing data @xmath58 , of size @xmath59 ( @xmath59 can be different from @xmath60 ) , with a logarithmic step , in order to increase the number of points above @xmath1 .",
    "more precisely , for any @xmath61 , @xmath62 is expressed as : @xmath63 notice that the introduction of new points between the observations of @xmath64 has an impact on @xmath29 by evaluating it on more points , but not on the step function @xmath53 .",
    "hence @xmath43 is now determined by solving the following minimization problem using the lm algorithm ( see @xcite ) : @xmath65 where @xmath66 represents @xmath4 for @xmath52 .",
    "note that this initialization step is the first modification we have introduced , compared with the two - components algorithm .",
    "once @xmath43 is determined , we can , thereafter , proceed iteratively . for all @xmath67 ,",
    "the @xmath42 iteration is splitted into two main minimization problems , which are solved alternatively , as described hereafter .",
    "* _ step @xmath68 : _ * determination of @xmath69 $ ] , minimizing the se between the hybrid cdf given @xmath70 , and the empirical one , as follows : @xmath71 where @xmath72 denotes @xmath4 for @xmath73 ( fixed ) . +",
    "this minimization problem is as well numerically resolved using the lm algorithm .    * _ step @xmath74 : _ * determination of @xmath75 , minimizing the se between the hybrid cdf given @xmath76 , and the empirical one , _",
    "i.e. _ by solving the following minimization problem via the lm algorithm : @xmath77 where @xmath78 , represents @xmath4 for @xmath79 ( fixed ) .",
    "* _ stop condition : _ * the algorithm iterates until it satisfies the following stop condition : @xmath80 where @xmath81 denotes the distance between @xmath41 and @xmath82 , chosen in this study as the mean squared error ( mse ) , @xmath83 $ ] , and @xmath84 represents the observations above the quantile @xmath85 of order @xmath86 associated with @xmath29 .",
    "the second modification with respect to the two - components algorithm initially developed , lies at the stop condition . indeed , to ensure a reliable fit of data not only for the main behavior but also for the tail",
    ", we force the algorithm to stop only when the mse between the hybrid cdf and the empirical one is small enough ( @xmath87 , for this work ) , using on one hand all data ( condition c1 ) , on the other hand only extreme order statistics above a desired @xmath88 ( condition c@xmath89 ) .",
    "otherwise , the algorithm stops when a fixed number @xmath90 of iterations ( @xmath91 , for this work ) is reached ( condition c3 ) .",
    "note that this algorithm can be adapted to different hybrid models according to the nature and the number of its components ( if larger than 2 ) , without any influence on the convergence study of the adapted algorithm .",
    "we point out that in this method , it is important to have at least two - components , among which a gpd to describe the extremes behavior , to be able to determine in an automatic way the threshold above which the gpd is fitted . for simplicity , we focus on the right tail , but it is straightforward to repeat the same procedure to consider the left tail too . the algorithm has been extended in an example of this type , when considering both tails ( see @xcite ) .",
    "we just follow the same logic : breaking down the problem of parameters estimation into two subproblems and then resolving them alternatively .    to summarize , let us provide a pseudo - code of our algorithm .",
    "initialization of @xmath51 , @xmath86 , @xmath92 , and @xmath90 , then initialization of @xmath43 : + @xmath93 + where @xmath53 is the empirical cdf of x. we note that this distance is computed on the points @xmath58 defined in .",
    "+ iterative process :    * @xmath94 * * estimation of @xmath76 : @xmath95 + * * estimation of @xmath75 : @xmath96 * @xmath97 + until @xmath98 or @xmath99",
    ".    return @xmath100.$ ]",
    "to study the performance of the algorithm to self - calibrate the g - e - gpd model , we build on monte - carlo simulations . to do so ,",
    "we proceed in @xmath101 steps :    * we consider @xmath102 training sets @xmath103 , of length @xmath60 and @xmath102 test sets @xmath104 , of length @xmath105 , with a g - e - gpd parent distribution admitting a fixed parameters vector @xmath4 . * on each training set @xmath106 , @xmath107 , we estimate @xmath4 , say @xmath108 $ ] , using the algorithm given in the previous section .",
    "we denote by @xmath109 the estimation of the parameter @xmath41 relative to the @xmath110 training set .",
    "* we compute the empirical mean and variance of estimates of each parameter over the @xmath102 training sets . for any parameter @xmath41 ,",
    "we denote by @xmath111 and @xmath112 its empirical mean and variance , respectively , defined as : @xmath113 and @xmath114 .",
    "we can check the relevance of @xmath111 using two criterions : 1 .",
    "the mse expressed for any parameter @xmath41 as : @xmath115 a small value of mse highlights the reliability of parameters estimation using the presented algorithm .",
    "2 .   test on the mean ( with unknown variance ) : @xmath116 . + for @xmath117 , we can use a normal test ( instead of a @xmath118-test ) of size @xmath86 , with a rejection region of @xmath119 at level @xmath86 described by @xmath120 , where the statistics @xmath121 is given by @xmath122 and @xmath123 is the quantile of order @xmath124 of the standard normal distribution .",
    "* we compare the hybrid pdf @xmath2 given @xmath4 with the pdf @xmath125 estimated on each test set @xmath126 , given @xmath127 . to do so , we compute the average of the log - likelihood ratio @xmath128 of @xmath129 by @xmath130 , over the @xmath102 simulations : @xmath131 it is obvious that the smallest the value of @xmath132 is , the most trustworthy is the algorithm .",
    "we present in table [ tab1 ] the results obtained when taking @xmath133 $ ] , @xmath134 , @xmath135 and @xmath136 .",
    "different values of @xmath60 have been considered to study its impact on the parameters estimation .",
    "the reliability of the three - components algorithm , in terms of goodness - of - fit , is pointed out through the two criterions described above , as well as _ via _ the average of the log - likelihood ratio .",
    "first , for each estimated parameter , we notice a small mse whenever the data size is large enough , with a variance of order @xmath137 ( except for @xmath1 , where it is much larger ) .",
    "this order being larger than standard ones ( @xmath138 , as _",
    "e.g. _ for the the hill and qq estimators ) , we resort to a statistical test as an additional criterion . for the @xmath102 training sets , we compute the test statistics denoted @xmath139 and the corresponding @xmath36-value @xmath140 , with respect to the parameter @xmath41 . for any @xmath141 and for any parameter @xmath142",
    ", we obtain @xmath143 , and @xmath144 ( it even remains greater than @xmath145 ) , which reveals a high acceptance ( @xmath146 level ) of @xmath119 ( @xmath147 ) _ i.e. _ a very high level of similarity between the values obtained via the algorithm and the fixed ones ( even for @xmath148 ) .",
    "finally , the accuracy of the parameters estimation is also highlighted via the average log - likelihood ratio .",
    "for the three different values of @xmath60 , this average is lower than @xmath149 , involving a good self - calibration of the g - e - gpd hybrid model .    a remaining question , which might be the object of another paper , is the study of the convergence rate of this algorithm .",
    "here , to have an idea of how fast it works , we indicate in table [ tab1 ] the average execution time and the average iterations number ( the floor function ) over the @xmath102 simulations .",
    "as shown in this table , they both increase with the data size , as expected .",
    "we notice that the average execution time is still small , even for @xmath150 , indicating a fast convergence of the algorithm .",
    "it could be even reduced by converting our programs from the r programming language to the c++ one .    besides the reliable estimation of the parameters ,",
    "we show in table [ tab2 ] , _ via _ the mse , that our algorithm enhances the gpd parameters estimation when compared with the maximum likelihood ( ml ) method and the probability weighted moments ( pwm ) ( see @xcite ) .",
    "we mention that the threshold we select for the comparison is the one obtained by the algorithm .",
    ".monte - carlo simulations results for @xmath133 $ ] , @xmath134 , @xmath136 , and + @xmath151 . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     the numerical results obtained for the threshold and tail index , as well as for the mse between the empirical tail distribution and the estimated gpd using the four methods respectively , are reported in table [ tab4 ] .",
    "we can notice that all methods offer a good fit of the tail distribution , with a slightly overestimation for the g - e - gdp and qq methods compared with the mep and hill ones . in figure [ qsp500 ]",
    ", we also give a comparison of the estimated quantile function using the g - e - gpd method and the graphical ( mep , hill and qq ) ones .",
    "+   +    in table [ tab4 ] and figure [ qsp500 ] , we observe once again similar results for the various methods .",
    "it confirms the good performance of the self - calibrating method to estimate the tail distribution .",
    "as already said , this latter method also provides a good modeling for the entire cdf .",
    "in this paper , we propose a self - calibrating method to model heavy tailed data that may be non - homogeneous and multi - components .",
    "we develop it introducing a general non - degenerate hybrid @xmath20 distribution for heavy tailed data modeling , which links a normal distribution to a gpd via an exponential distribution that bridges the gap between mean and asymptotic behaviors .",
    "the three distributions are connected to each other at junction points estimated by an iterative algorithm , as are the other parameters of the model .",
    "the convergence of the algorithm is studied analytically for one part and numerically for the other .",
    "the performance of the method is studied on simulated data .",
    "based on those results , we observe that the proposed unsupervised algorithm offers a judicious fit of the asymmetric right heavy tailed data with an accurate determination of the tail threshold indicating the presence of extremes , as well as of the parameters of the gpd that fits the extremes over this threshold .",
    "several applications of the method have been done on real data , in particular on insurance data .",
    "we give two of them on data coming from very different fields , neural data and financial ones ( s&p500 ) .",
    "a comparison follows with other existing methods .",
    "note that this method has been developed when considering asymmetric right heavy tailed data ; it can of course be applied in the same way when having the asymmetry on the left side , or when having a heavy tail on each side ( without requiring a symmetry ) .",
    "this method has many advantages and should become of great use in practice .",
    "the main advantage is to be unsupervised , avoiding the somehow arbitrary resort , when fitting the tail , to standard graphical methods ( e.g. mep , hill , qq methods ) in evt .",
    "a second advantage is to fit with the same iterative algorithm the full distribution of observed heavy tailed data , of any type whenever smooth enough ( @xmath20-distribution ) , providing an accurate estimation of the parameters for the mean and extreme behaviors .",
    "it certainly answers a big concern encountered by practitioners .",
    "moreover the method is quite general : besides the gpd needed when fitting the heavy tail , the other components might be chosen differently , not using limit behavior ( clt ) but distributions chosen specifically for the data that are worked out ( as e.g. lognormal for insurance claims )",
    ". it would not change at all the structure of the algorithm .",
    "it should be emphasized that determining in a unsupervised way the threshold over which we have extremes , requires to have information before the threshold .",
    "we suggest here an approach that avoids traditional graphical methods when fitting the entire distribution .",
    "further investigation will follow in order to make this method also available as a pure evt tool ( i.e. to fit the tail only ) .",
    "it means to determine the minimum information required to determine the neighbor distribution of the gpd to have a robust estimation for the tail threshold and the gpd parameters estimation .",
    "moreover , we plan also to tackle the analytical study of the convergence rate of the algorithm as a function of sample size .    finally , a r package should appear soon online .",
    "meantime , the r codes are available upon request .",
    "the first two authors acknowledge the support from the european union s seventh framework programme for research , technological development and demonstration under grant agreement no 318984 - rare .",
    "wiley series in probability and statistics .",
    "wiley , 2006 .",
    ", vol .  405 .",
    "john wiley & sons , 2009 .    extreme forex moves .",
    ", 2 ( 2003 ) , 6366 .",
    "adaptive threshold estimation via extreme value theory . , 2 ( 2010 ) , 490500 .    a hybrid pareto model for asymmetric",
    "fat - tailed data : the univariate case .",
    ", 1 ( 2009 ) , 5376 .    living in a stochastic world and managing complex risks .",
    "( 2015 ) .",
    "models for exceedances over high thresholds .",
    "( 1990 ) , 393442 .    .",
    "springer science & business media , 2006 .    .",
    "phd thesis , universit de reims champagne ardenne , france , 2015 .",
    "distribution hybride pour la modlisation de donnes  deux queues lourdes : application sur les donnes neuronales . in _",
    "groupe detudes du traitement du signal et des images , gretsi _ ( 2015 ) .    a new unsupervised threshold determination for hybrid models . in _",
    "2014 ieee international conference on acoustics , speech and signal processing ( icassp ) _ ( 2014 ) , pp .",
    "34403444 .    a hybrid distribution for highly right skewed data .",
    "2015 international conference on communications and networking ( comnet ) _ ( 2015 ) .",
    "unsupervised threshold determination for hybrid models .",
    ", university of reims , june 2013 .    combining algebraic approach with extreme value theory for spike detection . in _",
    "signal processing conference ( eusipco ) , 2012 proceedings of the 20th european _ ( 2012 ) , pp .  18361840 .    .",
    "springer - verlag , 1997 .    .",
    "springer science & business media , 1998 .",
    "extreme - value theory applied to false - alarm probabilities ( corresp . ) .",
    ", 3 ( 1962 ) , 259260 .    . academic press , 2001 .",
    "an extreme value theory approach for the early detection of time clusters . a simulation - based assessment and an illustration to the surveillance of salmonella . , 28 ( 2014 ) , 50155027 .",
    "columbia university press , 1958 .    a simple general approach to inference about the tail of a distribution .",
    "( 1975 ) , 11631174 .",
    "parameter and quantile estimation for the generalized pareto distribution funcrion .",
    "( 1987 ) , 339349 .    robust kernel density estimation .",
    "( 2012 ) , 25292565 .    a generalization of brouwer s fixed point theorem .",
    ", 3 ( 1941 ) , 457459 .    the czeledin distribution function . in _",
    "xxxiv astin colloquium , berlin _ ( 2003 ) .",
    "mixture probability distribution functions to model wind speed distributions .",
    ", 1 ( 2012 ) , 110 .    normex , a new method for evaluating the distribution of aggregated heavy tailed risks . , 4 ( 2014 )",
    ". special issue on _ extremes in finance _",
    "( guess ed .",
    "p. embrechts ) : 661 - 691 .",
    "the qq - estimator and heavy tails . , 4 ( 1996 ) , 699724 .    .",
    "3island press , 1983 .",
    "a method for the solution of certain nonlonear problems in least squares quart .",
    "( 1944 ) , 164168 .",
    "simulation of the entire range of daily precipitation using a hybrid probability distribution .",
    "( 2012 ) , 117 .",
    "reliability assessment of microarray data using fuzzy classification methods : a comparative study .",
    "( 2011 ) , 351360 .",
    "an algorithm for least - squares estimation of nonlinear parameters .",
    "( 1963 ) , 431441 .    neural spike detection and localisation via volterra filtering . in _ 2012 ieee international workshop on machine learning for signal processing ( mlsp ) _ ( 2012 ) .    .",
    "princeton series in finance .",
    "princeton university press , 2005 .",
    "robust detection using extreme - value theory .",
    ", 3 ( 1969 ) , 370375 .    new composite models for the danish fire insurance data . , 2 ( 2014 ) , 180187 .",
    "schauder fixed point theorem in spaces with global nonpositive curvature . ( 2009 ) , 18 .    .",
    "springer us , 2010 , pp .  8189 .    on estimation of a probability density function and mode .",
    ", 3 ( 1962 ) , 10651076",
    ".    statistical inference using extreme order statistics .",
    "( 1975 ) , 119131 .    ,",
    "http://dx.doi.org/10.5281/zenodo.14607 .",
    "using noise signature to optimize spike - sorting and to assess neuronal classification quality . , 1 ( 2002 ) , 4357 .    . .",
    "r foundation for statistical computing , vienna , austria , 2014 .",
    "parameter estimation for differential equations : a generalized smoothing approach . , 5 ( 2007 ) , 741796 .    statistical analysis of the non - homogeneity detector for \\{stap } applications . , 3 ( 2004 ) , 253267 .",
    "springer - verlag , 1987 .    .",
    "springer science & business media , 2007 .",
    "two - component extreme value distribution for flood frequency analysis .",
    ", 7 ( 1984 ) , 847856 .    , vol .  3 .",
    "mcgraw - hill new york , 1964 .",
    "modelling and analysis of communication traffic heterogeneity in opportunistic networks .",
    ", 11 ( 2015 ) , 23162331 .    , 2016 .",
    "r package version 0.10 - 35 .",
    "bayesian nonparametric inference for random distributions and related functions .",
    ", 3 ( 1999 ) , 485527 .",
    ". springer science & business media , 2013 .    a proof of the markov - kakutani fixed point theorem via the hahn - banach theorem .",
    "( 1993 ) , 3738 .",
    "damage detection in mechanical structures using extreme value statistics . in _",
    "spie s 9th annual international symposium on smart structures and materials _ ( 2002 ) , international society for optics and photonics , pp .",
    "289299 .    a generalization of generalized banach fixed - point theorem in a weak left small self - distance space . ,",
    "99 ( 2007 ) , 105 .",
    "as already commented , the algorithm convergence does not depend on the number ( @xmath152 ) of components .",
    "therefore , we develop its analysis when considering two - components ( _ i.e. _ @xmath153 ; no exponential component ) , with a unique weight associated to each one . in the following ,",
    "we denote by @xmath154 the junction point connecting the gaussian distribution to the gpd .",
    "we mention that for this two - components hybrid model , named g - gpd , the constraint @xmath155 can be relaxed .",
    "the parameters vector of the g - gpd model is @xmath156 $ ] .",
    "the two - components algorithm ( see @xcite ) estimates the parameters @xmath157 $ ] and @xmath154 alternatively .",
    "let us give its pseudo - code for more clarity .",
    "initialization of @xmath158 , @xmath159 , and @xmath90 .",
    "+ determination of the empirical cdf @xmath53 associated with our sample @xmath160 .",
    "+ iterative process :    * @xmath94 * * estimation of @xmath161 $ ] : + @xmath162 + where @xmath163 represents @xmath4 for a fixed @xmath164 , and @xmath165 is the domain of @xmath36 for the realization @xmath166 . + * * estimation of @xmath167 : @xmath168 + where @xmath78 means @xmath4 for @xmath79 ( fixed ) , and @xmath169 is the domain of @xmath154 according to @xmath166 . +",
    "* @xmath97 + until @xmath170 or @xmath171 .",
    "+    return @xmath172.$ ]    the convergence study is in two main steps .",
    "the first one gives the analytical proof of the existence of stationary points .",
    "indeed the algorithm , which consists of a sequence of minimization , does not rely on the optimization of a cost function by seeking a trajectory to reach an extremum of an error surface . as a consequence ,",
    "the existence of a stationary point , even the convergence towards such one , is not guaranteed , and has to be proved ( see appendix a ) .",
    "the second step consists in checking that the algorithm converges to a unique stationary point .",
    "it is done numerically , performing various simulations changing each time the initialization ( see appendix b ) .",
    "we observe that , whatever the initialization , the algorithm converges to the same stationary point .",
    "the analytical proof of this second step is still an open problem .",
    "we start this section by presenting the theoretical framework in which the existence of stationary points has been proved . for a given realization @xmath160 and given parameters @xmath173 with @xmath174",
    ", we consider the function :    @xmath175    where for @xmath176 denoting , as previously , @xmath4 for a given @xmath177 , @xmath178 is defined by : @xmath179 + to check that @xmath180 is a map , it is enough to show that @xmath181 admits a unique minimum , for any @xmath182 , with @xmath157\\in{\\mathbb{r}}\\times { \\mathbb{r}}_+^*$ ] and @xmath183 . since the expression of the hybrid cdf @xmath29 with respect to ( w.r.t . )",
    "@xmath177 ( see ) is rather complicated , we proceed _ via _ simulations . fixing @xmath177 , for instance @xmath184 ( it would be the same for @xmath185 ) , we draw , for a given sample @xmath186 , @xmath187 as a function of @xmath154 ( _ i.e. _ @xmath188 ) , observing if it admits a unique minimum .",
    "take the example of a g - gpd sample @xmath186 with @xmath189 , generated with @xmath190 , @xmath191 and @xmath192 , and consider different scenarios varying the value of the gaussian parameter @xmath36 . in figure",
    "[ cvx ] , we present the different curves of @xmath187 depending on @xmath36 , which all exhibit a unique minimum . note that for @xmath193 $ ] which corresponds to the parameter of the generated sample , the minimum of @xmath187 ( see the green curve of the right plot ) coincides with the exact threshold @xmath192 of the data ( represented by a vertical red dashed line in both plots of figure [ cvx ] ) , as expected .",
    ".[cvx ]    using @xmath194 and @xmath195 , the two steps of the first iteration of the algorithm can be given , for a fixed @xmath158 , by the following relations : @xmath196 more generally , for any @xmath197 , we can write @xmath198 where the function @xmath199 is defined from @xmath169 to @xmath169 by : @xmath200 .",
    "consequently , the algorithm can also be expressed as :    initialization of @xmath158 , @xmath92 , and @xmath90 .",
    "+ determination of the empirical cdf @xmath53 according to @xmath166 .",
    "+ iterative process :    @xmath94 @xmath201 @xmath97 + while @xmath202 and @xmath203 .",
    "+ return @xmath204    a way to prove the existence of stationary points of algorithm [ algo2 ] is to demonstrate the existence of fixed - points of the function @xmath199 . to do so",
    ", we build on the fixed - point theorem .",
    "several versions of this theorem exist in the literature _",
    "e.g. _ the version of banach ( see @xcite ) , or of markov - kakutani ( ses @xcite ) , or of schauder ( see @xcite ) , or of brouwer ( see @xcite ) . in this work ,",
    "we consider the latter one , as its hypotheses are , in our case , more straightforward to check .",
    "this theorem states that _ every continuous function from a closed ball of a euclidean space into itself has a fixed point_. it implies that the functional @xmath199 admits at least one fixed point if the following two conditions , ( @xmath205 ) and ( @xmath206 ) , are satisfied :    ( @xmath205 ) : : :    @xmath169 is a closed ball of a euclidean space .",
    "( @xmath206 ) : : :    @xmath199 is continuous on @xmath169 .",
    "the conditions ( @xmath205 ) is clearly satisfied : for a realization @xmath166 , @xmath207 $ ] is a closed ball of @xmath208 that is a euclidian space .",
    "now , to verify ( @xmath206 ) , we prove that @xmath209 and @xmath210 are both continuous on their domains ( since @xmath199 is the composite function : @xmath211 ) using the heine - cantor theorem ( see e.g. @xcite ) and the ramsay et al.s one that we recall here .    _",
    "theorem ( @xcite ) + let @xmath212 and @xmath213 be metric spaces with @xmath212 closed and bounded",
    ". let @xmath214   + be uniformly continuous in @xmath166 and @xmath86 , such that @xmath215 is well defined for all @xmath216",
    ". then the function @xmath217 is continuous .",
    "_    the proof of the continuity of the two functions @xmath210 and @xmath209 being the same , let us consider for instance the function @xmath210 . using ramsay s theorem",
    ", we need to check that @xmath165 is a compact and that @xmath218 is uniformly continuous on @xmath165 , to conclude to the continuity of @xmath210 .",
    "the first condition , @xmath165 is a compact of @xmath219 , is satisfied when noticing that we are working with a gaussian density , with finite mean and variance , hence which is bounded .",
    "now , as @xmath165 is a compact , it is sufficient to show that @xmath218 is continuous on @xmath165 to deduce , by the heine - cantor theorem , its uniform continuity . since @xmath220",
    ", we just need to study the continuity of @xmath29 w.r.t .",
    "@xmath36 to deduce the continuity of @xmath221 w.r.t .",
    "we recall here that , by construction , @xmath29 is continuous w.r.t . @xmath166 and not to its parameters . hence",
    ", its continuity according to @xmath36 remains to be proved .",
    "since @xmath29 is composed of two functions ( see for @xmath153 ) , the gaussian cdf and the gpd , we will study the continuity of each one w.r.t .",
    "the continuity of the gaussian cdf @xmath23 as a function of @xmath36 is immediate since it means to look at the continuity of its likelihood w.r.t .",
    "@xmath157 $ ] .",
    "now , for the gpd @xmath222 , its parameters @xmath35 and @xmath223 are expressed as fonctions of @xmath36 : @xmath224 , and are both continuous in @xmath36 .",
    "hence @xmath222 is continuous in @xmath36 as the composition of continuous functions w.r.t .",
    "@xmath36 .    finally , we can deduce the continuity of the function @xmath218 on @xmath165 as a composition , sum and products , of continuous functions on @xmath165 , from which we conclude to the continuity of @xmath210 on @xmath169 .",
    "_ conclusion : _ we can conclude that the functional @xmath199 is continuous on @xmath169 as a composition of two continuous functions : @xmath210 and @xmath209 .",
    "hence the existence of at least one fixed - point according to the brouwer fixed - point theorem .",
    "consequently , the algorithm admits at least one stationary point . since the method does not follow a path on an error surface , it is free from local minima traps as are the classical gradient search based methods . in the next section ,",
    "we perform simulations to check if the algorithm converges to a unique stationary point regardless to the initialization .",
    "to study numerically the convergence of the algorithm to a unique attractive stationary point , we consider the recurrent sequence @xmath225 , obtained when applying algorithm [ algo1 ] on a generated g - gpd distributed data with a fixed parameter @xmath4 .",
    "different initial values of this sequence are considered and for each one we represent graphically the associated recurrent sequence . to ensure the algorithm to be on the right track , all initial values are selected in the interval to which @xmath154 belongs , namely @xmath226 $ ] ( see @xcite ) . for illustration , we report here two examples among all those performed to test the convergence .      for @xmath227 $ ] with @xmath229 and @xmath230 , we present in figure [ cv1 ] the recurrent sequence @xmath225 , where the initial value @xmath231 .",
    "as shown in this figure , regardless the choice of @xmath158 in @xmath232 , the algorithm converges to the fixed value of @xmath233 ( represented by a continuous horizontal line ) , denoted by @xmath234 .",
    "we observe that :    1 .   if @xmath235 , the associated recurrent sequence is non decreasing , as for instance for the gray cercles curve with @xmath236 and the red triangles ( upwards oriented ) one with @xmath237 ; 2 .   if @xmath238 , the associated recurrent sequence is non increasing , _",
    "e.g. _ the blue diamonds curve for @xmath239 and the pink triangles ( downwards oriented ) curve for @xmath240 .    consequently , based on figure [ cv1 ] ,",
    "regardless the choice of @xmath241 , the recurrent sequence @xmath225 is monotone on @xmath169 and converges to a unique attractive stationary point that corresponds to @xmath234",
    ".         + an additional remark concerns the number of iterations .",
    "we could observe in the simulation study that the closest to @xmath234 is @xmath158 , the fastest is the convergence , as expected .",
    "it appears clearly on the two reported examples ( see the green @xmath246 marks curve in both figures ) .    to conclude ,",
    "let us comment that extending this convergence study to three - components is straightforward and follows the same logic as for two components .",
    "the estimation of @xmath4 is also broken down into the estimation of @xmath36 and @xmath35 alternately .",
    "the associated algorithm can be , as well as for the two - components model , represented by a functional of @xmath35 .",
    "hence , we can prove as previously that this functional is continuous on a closed ball of a euclidean space , according to the brouwer theorem , to infer the existence of stationary points of algorithm [ algo3 ] .",
    "the difference between the two- and three - components algorithms concerns only the data scale , the stop condition , and the condition on @xmath35 to be positive ( frchet distribution ) , so does not interfere in the convergence of the three - components algorithm .",
    "these three conditions have only been introduced to enhance the parameters estimation ."
  ],
  "abstract_text": [
    "<S> one of the main issues in the statistical literature of extremes concerns the tail index estimation , closely linked to the determination of a threshold above which a generalized pareto distribution ( gpd ) can be fitted . </S>",
    "<S> approaches to this estimation may be classified into two classes , one using standard peak over threshold ( pot ) methods , in which the threshold to estimate the tail is chosen graphically according to the problem , the other suggesting self - calibrating methods , where the threshold is algorithmically determined . </S>",
    "<S> our approach belongs to this second class proposing a hybrid distribution for heavy tailed data modeling , which links a normal ( or lognormal ) distribution to a gpd via an exponential distribution that bridges the gap between mean and asymptotic behaviors . a new unsupervised algorithm is then developed for estimating the parameters of this model . </S>",
    "<S> the effectiveness of our self - calibrating method is studied in terms of goodness - of - fit on simulated data . </S>",
    "<S> then , it is applied to real data from neuroscience and finance , respectively . </S>",
    "<S> a comparison with other more standard extreme approaches follows .    </S>",
    "<S> _ keywords : _ algorithm ; extreme value theory ; gaussian distribution ; generalized pareto distribution ; heavy tailed data ; hybrid model ; least squares optimization ; levenberg marquardt algorithm ; neural data ; s&p 500 index    _ 2010 ams classification : _ 60g70 ; 62e20 ; 62f35 ; 62p05 ; 62p10 ; 65d15 ; 68w40 </S>"
  ]
}