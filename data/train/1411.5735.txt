{
  "article_text": [
    "compressed sensing @xcite has generated enormous interest and research activities in mathematics , statistics , signal processing , imaging and information sciences , among numerous other areas .",
    "one of the basic problems is to reconstruct a sparse signal under a few linear measurements ( linear constraints ) far less than the dimension of the ambient space of the signal . consider a sparse signal @xmath13 , an @xmath14 sensing matrix a and an observation @xmath15 , @xmath16 , such that : @xmath17 where @xmath18 is an @xmath19-dimensional observation error .",
    "if @xmath20 is sparse enough , it can be reconstructed exactly in the noise - free case and in stable manner in the noisy case provided that the sensing matrix @xmath5 satisfies certain incoherence or the restricted isometry property ( @xmath21 ) @xcite .",
    "the direct approach is @xmath1 optimization , including constrained formulation : @xmath22 and the unconstrained @xmath1 regularized optimization : @xmath23 with positive regularization parameter @xmath24 . since minimizing @xmath1 norm",
    "is np - hard @xcite , many viable alternatives are available . greedy methods ( matching pursuit @xcite , othogonal matching pursuits ( omp ) @xcite , and regularized omp ( romp ) @xcite ) work well if the dimension @xmath19 is not too large . for the unconstrained problem ( [ eq : l0 uncons ] ) , the penalty decomposition method @xcite replaces the term @xmath25 by @xmath26 , and minimizes over @xmath27 for a diverging sequence @xmath28 .",
    "the variable @xmath29 allows the iterative hard thresholding procedure .",
    "the relaxation approach is to replace @xmath1 norm @xmath30 by a continuous sparsity promoting penalty functions @xmath31 .",
    "convex relaxation uniquely selects @xmath32 as the @xmath0 norm @xmath33 .",
    "the resulting problems are known as basis pursuit ( lasso in the over - determined regime @xcite ) .",
    "the @xmath0 algorithms include @xmath0-magic @xcite , bregman and split bregman methods @xcite and yall1 @xcite . theoretically , cands and tao introduced rip condition and used it to establish the equivalent and unique global solution to @xmath1 minimization via @xmath0 relaxation among other stable recovery results @xcite .",
    "there are also many choices of @xmath32 for non - convex relaxation .",
    "one is the @xmath3 norm ( @xmath34 ) with @xmath1 equivalence under rip @xcite .",
    "the @xmath35 norm is representative of this class of functions , with the reweighted least squares and half - thresholding algorithms for computation @xcite . near the rip regime",
    ", @xmath35 penalty tends to have higher success rate of sparse reconstruction than @xmath0 .",
    "however , it is not as good as @xmath0 if the sensing matrix is far away from rip @xcite as we shall see later as well . in the highly non - rip ( coherent ) regime",
    ", it is recently found that the difference of @xmath0 and @xmath36 norm minimization gives the best sparse recovery results @xcite .",
    "it is therefore of both theoretical and practical interest to find a non - convex penalty that is consistently better than @xmath0 and always ranks among the top in sparse recovery whether the sensing matrix satisfies rip or not .    in the statistics literature of variable selection ,",
    "fan and li @xcite advocated for classes of penalty functions with three desired properties : * unbiasedness , sparsity * and * continuity*. to help identify such a penalty function denoted by @xmath37 , fan and lv @xcite proposed the following condition for characterizing unbiasedness and sparsity promoting properties .    the penalty function @xmath38 satisfies :    1 .",
    "@xmath39 is increasing and concave in @xmath40 ; 2 .",
    "@xmath41 is continuous with @xmath42 ; 3 .",
    "if @xmath39 depends on a positive parameter @xmath24 , then @xmath43 is increasing in @xmath44 and @xmath45 is independent of @xmath24 .",
    "it follows that @xmath41 is positive and decreasing , and @xmath45 is the upper bound of @xmath41 .",
    "it is shown in @xcite that penalties satisfying condition 1 and @xmath46 enjoy both unbiasedness and sparsity .",
    "though continuity does not generally hold for this class of penalty functions , a special one parameter family of functions , the so called * transformed @xmath0 functions ( tl1 ) @xmath47 * , where @xmath48 with @xmath49 , satisfies all three desired properties @xcite .",
    "we shall study the minimization of tl1 functions for cs problems , in terms of theory , algorithms and computation .",
    "we proposed the algorithms of tl1 via dc approximation @xcite and implemented numerical tests based on two classes of coherent random sensing matrices .",
    "same as @xmath10 regularization @xcite , there also exists thresholding algorithm for tl1 , which is studied in the companion paper @xcite .",
    "the rest of the paper is organized as follows . in section 2",
    ", we study the properties of tl1 penalty and its regularization models .",
    "one rip condition is given for the consistency of tl1 constrained model with original @xmath1 model .",
    "we also prove that the local minimizers of the tl1 constrained model extract independent columns from the sensing matrix @xmath5 , as well as the local minimizers of the unconstrained model . in section 3 ,",
    "we present two dc algorithms for tl1 optimization ( dcatl1 ) . in section 4 , we compare the performance of dcatl1 with some state - of - the - art methods using two classes of matrices : the gaussian and the oversampled discrete cosine transform ( dct ) .",
    "numerical experiments indicate that dcatl1 is robust and consistently top ranked while maintaining high sparse recovery rates across all sensing matrices .",
    "concluding remarks are in section 5 .",
    "[ cols= \" < , > \" , ]",
    "we have studied compressed sensing problem with the transformed @xmath0 penalty function for both the unconstrained and constrained models .",
    "we presented a theory on the uniqueness and @xmath1 equivalence of the global minimizer of the unconstrained model under rip and analyzed properties of local minimizers .",
    "we showed two dc algorithms along with a convergence theory .    in numerical experiments",
    ", dcatl1 is on par with the best method reweighted @xmath35 ( @xmath50 ) in the unconstrained ( constrained ) model , using incoherent gaussian matrices @xmath5 . for highly coherent over - sampled dct matrices ,",
    "dcatl1 is comparable to the best method dca @xmath51 algorithm . for random matrices of varying degree of coherence",
    ", we tested gaussian and over - sampled dct sensing matrices .",
    "the dcatl1 algorithm is the most robust for constrained and unconstrained models alike .    in future work , we plan to develop tl1 algorithms for imaging processing applications such as deconvolution and deblurring .",
    "the authors would like to thank professor wenjiang fu for suggesting reference @xcite and a helpful discussion .",
    "we also wish to thank the reviewers for their constructive comments .",
    "the proof generally follows the lines of arguments in @xcite and @xcite , while using special properties of the penalty function @xmath52 .",
    "+ for simplicity , we denote @xmath53 by @xmath54 and @xmath55 by @xmath56 .",
    "+ define the function : @xmath57 it is continuous and increasing in the parameter @xmath58 . note that at @xmath59 , @xmath60 , and as @xmath61 , @xmath62 by ( [ rip ] ) .",
    "there exists a constant @xmath7 , such that @xmath63 .",
    "the number @xmath7 depends on the rip of matrix @xmath5 only , and so it is independent of the scalar @xmath64 . + for @xmath65 : @xmath66    let @xmath67 , and we want to prove that the vector @xmath68 .",
    "it is clear that , @xmath69 , since @xmath70 is the support set of @xmath56 . by the triangular inequality of @xmath52 , we have : @xmath71 then @xmath72 it follows that : @xmath73    now let us arrange the components at @xmath74 in the order of decreasing magnitude of @xmath75 and partition into @xmath76 parts : @xmath77 , where each @xmath78 has @xmath79 elements ( except possibly @xmath80 with less ) .",
    "also denote @xmath81 and @xmath82 .",
    "since @xmath83 , it follows that @xmath84          now we estimate the @xmath36 norm of @xmath88 from above in terms of @xmath85 .",
    "it follows from @xmath54 being the minimizer of the problem ( [ eq : spar revised ] ) and the definition of @xmath89 ( [ para : scaled pa ] ) that @xmath90        by ( [ rip a ] ) , the factor @xmath100 is strictly positive , hence @xmath101 , and @xmath102 . also by inequality ( [ ineq : ttc ] ) , @xmath103 .",
    "we have proved that @xmath104 .",
    "the equivalence of ( [ eq : spar revised ] ) and ( [ eq : l0 revised ] ) holds .",
    "d. needell and r. vershynin , _ signal recovery from incomplete and inaccurate measurements via regularized orthogonal matching pursuit , _ ieee journal of selected topics in signal processing , 4(2):310 - 316 , 2010 .",
    "s. zhang and j. xin , _ minimization of transformed @xmath108 penalty : closed form representation and iterative thresholding algorithms _ , arxiv preprint , arxiv:1412.5240 ( 2014 ) , comm .",
    "math sci , to appear ."
  ],
  "abstract_text": [
    "<S> we study the minimization problem of a non - convex sparsity promoting penalty function , the transformed @xmath0 ( tl1 ) , and its application in compressed sensing ( cs ) . </S>",
    "<S> the tl1 penalty interpolates @xmath1 and @xmath0 norms through a nonnegative parameter @xmath2 , similar to @xmath3 with @xmath4 $ ] . </S>",
    "<S> tl1 is known in the statistics literature to enjoy three desired properties : unbiasedness , sparsity and lipschitz continuity . </S>",
    "<S> we first consider the constrained minimization problem and prove the uniqueness of global minimizer and its equivalence to @xmath1 norm minimization if the sensing matrix @xmath5 satisfies a restricted isometry property ( rip ) and if @xmath6 , where @xmath7 depends only on @xmath5 . though result contains the well - known equivalence of @xmath0 norm and @xmath1 norm , in the limit @xmath8 , the main difficulty is in treating the lack of scaling property of the tl1 penalty function . for a general sensing matrix @xmath5 </S>",
    "<S> , we show that the support set of a local minimizer corresponds to linearly independent columns of @xmath5 , and recall sufficient conditions for a critical point to be a local minimum . </S>",
    "<S> next , we present difference of convex algorithms for tl1 ( dcatl1 ) in computing tl1-regularized constrained and unconstrained problems in cs . </S>",
    "<S> the dcatl1 algorithm involves outer and inner loops of iterations , one time matrix inversion , repeated shrinkage operations and matrix - vector multiplications . for the unconstrained problem , we prove convergence of dcalt1 to a stationary point satisfying the first order optimality condition . finally in numerical experiments , </S>",
    "<S> we identify the optimal value @xmath9 , and compare dcatl1 with other cs algorithms on two classes of sensing matrices : gaussian random matrices and over - sampled discrete cosine transform matrices ( odct ) . among existing algorithms , </S>",
    "<S> the iterated reweighted least squares method based on @xmath10 norm is the best in sparse recovery for gaussian matrices , and the dca algorithm based on @xmath11 penalty is the best for odct matrices . </S>",
    "<S> we find that for both classes of sensing matrices , the performance of dcatl1 algorithm ( initiated with @xmath12 minimization ) always ranks near the top ( if not the top ) , and is the _ most robust choice _ insensitive to rip ( incoherence ) of the underlying cs problems .    </S>",
    "<S> * keywords : * transformed @xmath0 penalty , sparse signal recovery theory , difference of convex function algorithm , convergence analysis , coherent random matrices , compressed sensing , robust recovery    90c26 , 65k10 , 90c90 </S>"
  ]
}