{
  "article_text": [
    "deep artificial neural networks ( anns ) have recently been very successful at solving image categorization problems .",
    "early successes with the mnist database @xcite were expanded to the more difficult but similarly sized cifar-10 dataset @xcite and street - view house numbers dataset @xcite .",
    "more recently , many groups have achieved better results on these small datasets ( e.g. @xcite ) and as well as success on larger datasets ( e.g. @xcite ) .",
    "this work culminated with the application of deep neural networks to imagenet @xcite , a very large and challenging dataset .",
    "the relative success of deep anns in general  and convolutional neural networks in particular  on these datasets have put them well ahead of other methods in terms of image categorization by machines . given that deep anns are approaching human performance on some datasets ( or even passing it , for example on mnist )",
    "suggests that these models may be able to shed light on how the human visual system solves these same tasks .",
    "there has recently been considerable effort to take deep anns and make them more biologically plausible by introducing neural `` spiking '' @xcite , such that connected nodes in the network transmit information via instantaneous single bits ( spikes ) , rather than transmitting real - valued activities .",
    "while one goal of this work is to better understand the brain by trying to reverse engineer it @xcite , another goal is to build energy - efficient neuromorphic systems that use a similar communication method for image categorization @xcite .",
    "we first train a network on static images using traditional deep learning techniques ; we call this the",
    "_ static network_. we then take the parameters ( weights and biases ) from the static network and use them to connect spiking neurons , forming the _ dynamic network _ ( or spiking network ) .",
    "the challenge is to train the static network in such a way that a ) it _ can _ be transferred into a spiking network , and b ) the classification error of the dynamic network is as close to that of the static network as possible ( this means the error rate is as low as possible , since we do not expect the dynamic network to perform better than the static one ) .",
    "we base our network off that of krizhevsky et al .",
    "@xcite , which achieved  11% error on the cifar-10 dataset ( a larger variant of the model won the imagenet 2012 competition ) .",
    "the original network consists of five layers : two generalized convolutional layers , followed by two locally - connected non - convolutional layers , followed by a fully - connected softmax classifier .",
    "a generalized convolutional layer consists of a set of convolutional weights followed by a neural nonlinearity , then a pooling layer , and finally a local response normalization layer .",
    "the locally - connected non - convolutional layers are also followed by a neural nonlinearity . in the case of the original network , the nonlinearity is a rectified linear ( relu ) function , and both pooling layers perform overlapping max - pooling .",
    "code for the original network and details of the network architecture and training can be found at https://code.google.com / p / cuda - convnet2/.    to make the static network transferable to spiking neurons , a number of modifications are necessary .",
    "first , we remove the local response normalization layers .",
    "this computation would likely require some sort of lateral connections between neurons , which are difficult to add in the current framework since the resulting network would not be feedforward .",
    "second , we changed the pooling layers from max pooling to average pooling .",
    "again , computing max pooling would likely require lateral connections between neurons , making it difficult to implement without significant changes to the training software .",
    "while the neural engineering framework can be used to compute a max function in a feedforward manner @xcite , this method requires prohibitively many neurons to achieve reasonable accuracy .",
    "average pooling , on the other hand , is very easy to compute in spiking neurons , since it is simply a weighted sum .",
    "the other modifications  using leaky integrate - and - fire neurons and training with noise  are the main focus of this paper , and are described in detail below .",
    "our network uses a modified leaky integrate - and - fire ( lif ) neuron nonlinearity instead of the rectified linear nonlinearity .",
    "past work has kept the rectified linear nonlinearity for the static network and substituted in the spiking integrate - and - fire ( if ) neuron model in the dynamic network @xcite , since the static firing curve of the if neuron model is a rectified line .",
    "our motivations for using the lif neuron model are that a ) it is more biologically realistic than the if neuron model @xcite , and b ) it demonstrates that alternative models can be used in such networks .",
    "the methods applied here are transferable to other neuron types , and could be used to train a network for the idiosyncratic neuron types employed by some neuromorphic hardware ( e.g. @xcite ) .",
    "the lif neuron dynamics are given by the equation @xmath0 where @xmath1 is the membrane voltage , @xmath2 is the input current , and @xmath3 is the membrane time constant .",
    "when the voltage reaches @xmath4 , the neuron fires a spike , and the voltage is held at zero for a refractory period of @xmath5 .",
    "once the refractory period is finished , the neuron obeys equation  [ eqn : lifode ] until another spike occurs .",
    "given a constant input current @xmath6 , we can solve equation  [ eqn : lifode ] for the time it takes the voltage to rise from zero to one , and thereby find the steady - state firing rate @xmath7^{-1 } & \\text{if } j > v_{th } \\\\",
    "0 & \\text{otherwise }    \\end{cases}.    \\label{eqn : lifss}\\end{aligned}\\ ] ]    theoretically , we should be able to train a deep neural network using equation  [ eqn : lifss ] as the static nonlinearity and make a reasonable approximation of the network in spiking neurons , assuming that the spiking network has a synaptic filter that sufficiently smooths a spike train to give a good approximation of the firing rate .",
    "the lif steady state firing rate has the particular problem that the derivative approaches infinity as @xmath8 , which causes problems when employing backpropagation . to address this",
    ", we added smoothing to the lif rate equation .",
    "equation  [ eqn : lifss ] can be rewritten as @xmath9^{-1}\\end{aligned}\\ ] ] where @xmath10 .",
    "if we replace this hard maximum with a softer maximum @xmath11 , then the lif neuron loses its hard threshold and the derivative becomes bounded .",
    "further , we can use the substitution @xmath12    \\label{eqn : softrelusigma}\\end{aligned}\\ ] ] to allow us control over the amount of smoothing , where @xmath13 as @xmath14 .",
    "figure  [ fig : softlif ] shows the result of this substitution .",
    "; the soft lif function smooths this threshold .",
    "the right panel shows the derivatives of the response functions .",
    "the hard lif function has a discontinuous and unbounded derivative at @xmath15 ; the soft lif function has a continuous bounded derivative , making it amenable to use in backpropagation . ]      training neural networks with various types of noise on the inputs is not a new idea . denoising autoencoders @xcite have been successfully applied to datasets like mnist , learning more robust solutions with lower generalization error than their non - noisy counterparts .    in a spiking neural network ,",
    "the neuron receiving spikes in a connection ( called the post - synaptic neuron ) actually receives a filtered version of each spike .",
    "this filtered spike is called a post - synaptic current ( or potential ) , and the shape of this signal is determined by the combined dynamics of the pre - synaptic neuron ( e.g. how much neurotransmitter is released ) and the post - synaptic neuron ( e.g. how many ion channels are activated by the neurotransmitter and how they affect the current going into the neuron ) .",
    "this post - synaptic current dynamics can be characterized relatively well as a linear system with the impulse response given by the @xmath16-function @xcite : @xmath17    the filtered spike train can be viewed as an estimate of the neuron activity .",
    "for example , if the neuron is firing regularly at 200 hz , filtering spike train will result in a signal fluctuating around 200 hz .",
    "we can view the neuron output as being 200 hz , with some additional `` noise '' around this value . by training",
    "our static network with some random noise added to the output of each neuron for each training example , we can simulate the effects of using spikes on the signal received by the post - synaptic neuron .",
    "figure  [ fig : noise ] shows how the variability of filtered spike trains depends on input current for the lif neuron .",
    "since the impulse response of the @xmath16-filter has an integral of one , the mean of the filtered spike trains is equal to the analytical rate of equation  [ eqn : lifss ] .",
    "however , the statistics of the filtered signal vary significantly across the range of input currents . just above the firing threshold ,",
    "the distribution is skewed towards higher firing rates ( i.e. the median is below the mean ) , since spikes are infrequent so the filtered signal has time to return to near zero between spikes . at higher input currents , on the other hand ,",
    "the distribution is skewed towards lower firing rates ( i.e. the median is above the mean ) . in spite of this",
    ", we used a gaussian distribution to generate the additive noise during training , for simplicity .",
    "we found the average standard deviation to be approximately @xmath18 across all positive input currents for an @xmath16-filter with @xmath19 . the final steady - state soft lif curve used in training",
    "is given by @xmath20^{-1 }      + \\eta(j )    \\label{eqn : softlifnoise}\\end{aligned}\\ ] ] where @xmath21 and @xmath22 is given by equation  [ eqn : softrelusigma ] .    ) .",
    "the solid line shows the mean of the filtered spike train ( which matches the analytical rate of equation  [ eqn : lifss ] ) , the ` x'-points show the median , the solid error bars show the 25th and 75th percentiles , and the dotted error bars show the minimum and maximum .",
    "the spike train was filtered with an @xmath16-filter ( equation  [ eqn : alpha ] ) with @xmath23 s. ( note that this is different than the @xmath19 used in simulation , to better display the variation . ) ]      finally , we convert the trained static network to a dynamic spiking network . the parameters in the spiking network ( i.e. weights and biases ) are all identical to that of the static network .",
    "the convolution operation also remains the same , since convolution can be rewritten as simple connection weights ( synapses ) @xmath24 between pre - synaptic neuron @xmath25 and post - synaptic neuron @xmath26 .",
    "( how the brain might _ learn _ connection weight patterns , i.e. filters , that are repeated at various points in space , is a much more difficult problem that we will not address here . )",
    "similarly , the average pooling operation can be written as a simple connection weight matrix , and this matrix can be multiplied by the convolutional weight matrix of the following layer to get direct connection weights between neurons .",
    "the only component of the network that actually changes , then , when moving from the static to the dynamic network , is the neurons themselves .",
    "the most significant change is that we replace the soft lif rate model ( equation  [ eqn : softlifnoise ] ) with the lif spiking model ( equation  [ eqn : lifode ] ) .",
    "we also remove the additive gaussian noise used in training .    additionally , we add post - synaptic filters to the neurons , which filter the incoming spikes before passing the resulting currents to the lif neuron equation .",
    "as stated previously , we use the @xmath16-filter for our synapse model , since it has both strong biological support @xcite , and removes a significant portion of the high - frequency variation produced by spikes .",
    "we pick the decay time constant @xmath27 ms , typical for excitatory ampa receptors in the brain @xcite .",
    "we tested our network on the cifar-10 dataset .",
    "this dataset is composed of 60000 @xmath28 pixel labelled images from ten categories .",
    "we used the first 50000 images for training and the last 10000 for testing , and augmented the dataset by taking random @xmath29 patches from the training images and then testing on the center patches from the testing images .",
    "this methodology is similar to krizhevsky et al .",
    "@xcite , except that they also used multiview testing where the classifier output is the average output of the classifier run on nine random patches from each testing image ( increasing the accuracy by about 2% ) .",
    "table  [ tab : mods ] shows the effect of each modification on the network classification error .",
    "our original static network based on the methods of @xcite achieved 14.63% error , which is higher than the  11% achieved by the original paper since a ) we are not using multiview testing , and b ) we used a shorter training time ( 160 epochs versus 520 epochs ) .",
    ".effects of successive modifications to cifar-10 error .",
    "we first show the original static ( non - spiking ) network based on @xcite .",
    "modifications 1 - 5 are cumulative , which each one applied in addition to the previous ones .",
    "rows 6 - 8 show the results of running static networks 3 - 5 in spiking neurons , respectively .",
    "row 9 shows the best architecture for spiking implementation , network 4 , trained for additional epochs , and row 10 shows this highly - trained network in spiking neurons .",
    "this is the best spiking - network result on cifar-10 to date . [",
    "cols=\"^,<,^\",options=\"header \" , ]",
    "our results demonstrate that it is possible to train accurate deep convolutional networks for image classification using more biologically accurate leaky integrate - and - fire ( lif ) neurons , as opposed to the traditional rectified - linear or sigmoid neurons .",
    "such a network can be run in spiking neurons , and training with noise decreases the amount of error introduced when running in spiking versus rate neurons .    the first main contribution of this paper is to demonstrate that state - of - the - art deep spiking networks can be trained with lif neurons .",
    "other state - of - the - art methods use integrate - and - fire ( if ) neurons @xcite , which are easier to fit to the rectified linear units commonly used in deep convolutional networks , but are biologically implausible . by smoothing the lif response function",
    "so that its derivative remains bounded , we are able to use this more biologically plausible neuron with a standard convolutional network trained by backpropagation .",
    "this idea of smoothing the neuron response function is applicable to other neuron types , too .",
    "many other neuron types have discontinuous response functions ( e.g. the fitzhugh - nagumo neuron ) , and our smoothing method allows such neurons to be used in deep convolutional networks .",
    "we found that there was very little error introduced by switching from the soft response function to the hard response function with lif neurons for the amount of smoothing that we used .",
    "however , for neurons with harsh discontinuities that require more smoothing , it may be necessary to slowly relax the smoothing over the course of the training so that , by the end of the training , the smooth response function is arbitrarily close to the hard response function .",
    "the other main contribution of this paper is to demonstrate that training with noise on neuron outputs can decrease the error introduced when transitioning to spiking neurons .",
    "training with noise on neuron outputs improved the performance of the spiking network considerably ( the error decreased by 3.4% ) .",
    "this is because noise on the output of the neuron simulates the variability that a spiking network encounters when filtering a spike train .",
    "there is a tradeoff between too little training noise , where the resultant dynamic network is not robust enough against spiking variability , and too much noise , where the accuracy of the static network is decreased .",
    "since the variability produced by spiking neurons is not gaussian ( figure  [ fig : noise ] ) , our additive gaussian noise is a rough approximation of the variability that the spiking network will encounter .",
    "future work includes training with noise that is more representative of the variability seen in spiking networks , to accommodate both the non - gaussian statistics at any particular input current , and the changing statistics across input currents .",
    "direct comparison with other spiking neural networks is difficult , since the amount of error introduced when converting from a static to a spiking network is heavily dependent on the firing rates of the neurons .",
    "nevertheless , we found our network to perform favourably with other spiking networks , achieving the best published result for a spiking network on cifar-10 , and the best result for a lif neuron spiking network on mnist .",
    "we also report our average firing rates for each layer and for the entire network , to facilitate comparison with future networks .",
    "the firing rates for the convolutional layers of our network are higher than typical in visual cortex @xcite .",
    "future work includes looking at methods to lower firing rates , though this may involve sparsification of neural firing  having fewer neurons fire for a particular stimulus  which can be difficult in convolutional networks .",
    "other future work includes implementing max - pooling and local contrast normalization layers in spiking networks .",
    "networks could also be trained offline as described here and then fine - tuned online using an stdp rule , such as the one described in @xcite , to help further reduce errors associated with converting from rate - based to spike - based networks , while avoiding difficulties with training a network in spiking neurons from scratch .",
    "y.  cao , y.  chen , and d.  khosla , `` spiking deep convolutional neural networks for energy - efficient object recognition , '' _ international journal of computer vision _",
    "113 , no .  1 ,",
    "pp . 5466 , nov 2014 .",
    "p.  u. diehl , d.  neil , j.  binas , m.  cook , s .- c .",
    "liu , and m.  pfeiffer , `` fast - classifying , high - accuracy spiking deep networks through weight and threshold balancing , '' in _ ieee international joint conference on neural networks ( ijcnn ) _",
    ", 2015 .",
    "b.  v. benjamin , p.  gao , e.  mcquinn , s.  choudhary , a.  r. chandrasekaran , j .-",
    "bussat , r.  alvarez - icaza , j.  v. arthur , p.  a. merolla , and k.  boahen , `` neurogrid : a mixed - analog - digital multichip system for large - scale neural simulations , '' _ proceedings of the ieee _ , vol .",
    "102 , no .  5 , pp . 699716 , 2014 .",
    "p.  vincent , h.  larochelle , y.  bengio , and p .- a .",
    "manzagol , `` extracting and composing robust features with denoising autoencoders , '' in _ international conference on machine learning ( icml ) _ , 2008 , pp .",
    "10961103 .      p.",
    "jonas , g.  major , and b.  sakmann , `` quantal components of unitary epscs at the mossy fibre synapse on ca3 pyramidal cells of rat hippocampus . ''",
    "_ the journal of physiology _ , vol .",
    "472 , pp . 615663 , 1993 .",
    "d.  garbin , o.  bichler , e.  vianello , q.  rafhay , c.  gamrat , l.  perniola , g.  ghibaudo , and b.  desalvo , `` variability - tolerant convolutional neural network for pattern recognition applications based on oxram synapses , '' in _ ieee international electron devices meeting ( iedm ) _ , 2014 , pp .",
    "28.4.128.4.4 .",
    "b.  nessler , m.  pfeiffer , l.  buesing , and w.  maass , `` bayesian computation emerges in generic cortical microcircuits through spike - timing - dependent plasticity . ''",
    "_ plos computational biology _ , vol .  9 , no .  4 , p. e1003037 , apr 2013"
  ],
  "abstract_text": [
    "<S> we train spiking deep networks using leaky integrate - and - fire ( lif ) neurons , and achieve state - of - the - art results for spiking networks on the cifar-10 and mnist datasets . </S>",
    "<S> this demonstrates that biologically - plausible spiking lif neurons can be integrated into deep networks can perform as well as other spiking models ( e.g. integrate - and - fire ) . </S>",
    "<S> we achieved this result by softening the lif response function , such that its derivative remains bounded , and by training the network with noise to provide robustness against the variability introduced by spikes . </S>",
    "<S> our method is general and could be applied to other neuron types , including those used on modern neuromorphic hardware . </S>",
    "<S> our work brings more biological realism into modern image classification models , with the hope that these models can inform how the brain performs this difficult task . </S>",
    "<S> it also provides new methods for training deep networks to run on neuromorphic hardware , with the aim of fast , power - efficient image classification for robotics applications . </S>"
  ]
}