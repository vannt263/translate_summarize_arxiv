{
  "article_text": [
    "the determination of parton distributions ( pdfs ) and their uncertainties  @xcite poses a difficult problem because one is trying to determine the probability distribution for a set of functions .",
    "given that this is necessarily done from a finite set of data it requires some assumptions : some of these , such as a certain degree of smoothness , may be physically motivated , but it is important to check that they do not bias the result and in particular that they do not destroy its statistical interpretation . the most common way of implementing these assumptions is to assume a functional form for the pdfs , each parametrized by a small number of parameters ( typically between two and five ) which are determined by fitting a suitable set of data . the nnpdf collaboration has developed an alternative approach  @xcite which tries to avoid the bias associated to this procedure .    the nnpdf approach is based on four main ingredients :    * _ monte carlo by importance sampling . _ nnpdf produces a monte carlo sampling of the probability density in the ( function ) space of pdfs . to adequately sample this space by simple binning",
    "would be simply impossible : for example assuming seven pdfs ( the three light quarks and antiquarks and the gluon ) sampled at ten points , binning the probability distribution in each direction with five bins one would end up with @xmath0 bins .",
    "the problem is solved by importance sampling : most bins are empty and only those with data are relevant . hence , one starts by constructing a set of data replicas , which reproduces the statistical features of the original data .",
    "it then turns out that a sample of 1000 pseudo - data replicas is large enough to reproduce central values , uncertainty and correlations of the starting data to a few percent accuracy * _ neural networks as universal unbiased interpolants .",
    "_ each of the underlying functions is parametrized with a feed - forward multilayer neural network .",
    "the architecture chosen corresponds to 37 free parameters for each of the seven pdfs .",
    "it can then be checked that results do not depend on the parametrization by verifying that they are unchanged if the size of the neural network is reduced . *",
    "_ genetic algorithms for neural network training . _",
    "the best fit is determined by using a genetic algorithm , and starting from a random initialization of parameters .",
    "this ensures that the presumably wide space of equivalent minima can be adequately explored . *",
    "_ determination of the best fit by cross - validation .",
    "_ because the parametrization is very large , the best fit is not the minimum of the @xmath1 , which would correspond to fitting noise .",
    "the best fit is then found by dividing randomly data in two sets ( training and validation ) for each experiment , minimizing the @xmath1 of the training set while monitoring the @xmath1 of both sets .",
    "the best - fit is obtained when the @xmath1 of the validation set starts increasing despite the fact that the @xmath1 of the training set still decreases .",
    "our starting point is the nnpdf2.1 nlo  @xcite pdf set : we would like to test that it behaves in a statistically consistent way . for a start , in table  [ tab : estfit1 ]",
    "we show the statistical estimators for this pdf fit : @xmath2 is the result of the comparison to data of the best - fit pdfs ( defined as the average over the @xmath3 replicas of the monte carlo sample ) ; @xmath4 is the average of the values obtained by comparing each pdf replica to the data , and @xmath5 is the value of the same figure of merit , but obtained obtained comparing each pdf replica to the corresponding data replica . for the latter , the training and validation values",
    "are also shown .",
    "all figures of merit are computed using the full covariance matrix , with normalization uncertainties included using the so - called @xmath6 method of ref .",
    "@xcite ; they are all normalized to the number of data points @xmath7 .",
    "the fact that @xmath8 while @xmath9 , and also that @xmath10 are both consistent with the fact that the fit is `` learning '' an underlying law : the fitted pdfs are closer to the data than the data replicas ( despite being fitted to the latter ) , and the best fit ( obtained averaging replicas ) is yet closer to the data than any of the individual replicas .",
    ".[tab : estfit1 ] table of statistical estimators for nnpdf2.1 with @xmath11 replicas ( first columns ) .",
    "the subsequent columns show the corresponding results , to be discussed in sect .",
    "[ sec : funcdat ] , for fits to central data and with fixed partitions , with @xmath12 replicas each .",
    "all entries in the last column are obtained repeating the procedure for five random choices of fixed partition and averaging the final results .",
    "all values are normalized to the number of data points . [ cols=\"^,^,^,^\",options=\"header \" , ]     however , the most striking result is given by the pdf uncertainties in the fp case : these uncertainties , though somewhat smaller , are still of the same order of magnitude as those of the the standard fit .",
    "this means that different replicas constructed by refitting exactly the same data over and over again still have a non - negligible spread and thus uncertainty .",
    "this is only possible because of the random nature of the fitting algorithm , and it shows that indeed there is a nontrivial space of almost equivalent minima",
    ". it should be noticed that indeed the fluctuation of @xmath4 for this replica set is significantly smaller than for the reference and cv sets , consistent with the hypothesis that one is now exploring a space of equivalent or almost equivalent minima .",
    "a more quantitative insight on the relative size of various contributions to the uncertainties can be obtained by computing the average uncertainty on the prediction for the fitted observables obtained using each pdf set .",
    "these are shown , both for the global and individual dataset , in table  [ tab : nnpdf - fixedparts ] , where the starting data uncertainty is also shown for comparison .",
    "the uncertainties obtained fitting to central data or to pseudodata replicas are almost identical : as already noticed , one might as well fit to central data .",
    "both are significantly smaller than the original data uncertainty , thereby showing that an underlying law has been learnt .",
    "the residual uncertainty in the fp case is still sizable .",
    "if one assumes that the uncertainty in the fp case is the functional uncertainty , while in the cv case it is the sum in quadrature of data and functional uncertainty , then one concludes that the functional uncertainty is rather more than half the total uncertainty .",
    "having verified that pdfs determined with the nnpdf methodology are consistent with statistical expectations and free of parametrization bias , it is natural to think that some of the statistical tools discussed here , as well as more refined statistical tests , may be used to guide and validate further improvements .",
    "two aspects of the methodology may be may be amenable to improvement . the first has to do with the underlying functional form . at present",
    ", pdfs are parametrized as a neural network , multiplied by a preprocessing function of the form @xmath13 .",
    "the exponents are then randomly varied in a reasonable range .",
    "the preprocessing speeds up the fitting of the neural network , and ensures that outside the data region the behaviour of the pdf does not fluctuate too wildly .",
    "this procedure is much more general and unbiased than that used in fits such as mstw or cteq , in which the functional form also incorporates the same small- and large-@xmath14 behaviour , but the exponents @xmath15 and @xmath16 are fitted ( instead of being varied in a range around their best fit ) and the residual number of parameters is smaller by more than one order of magnitude .",
    "but the preprocessing could still be a source of residual bias , so one should check whether results are stable upon completely different choices of preprocessing .",
    "the second has to do with the determination of the best fit .",
    "while cross - validation is quite efficient on average , it could still lead to some specific dataset being under- or overlearnt ; it involves some arbitrariness , for instance in deciding the precise form of the stopping criteria ; and it could lead to an excessively wide and thus sub - optimal space of minima . hence alternative methods to determine the optimal fit should be explored .",
    "correspondingly , two sets of statistical investigations may be worth pursuing in order to guide and validate these improvements .",
    "on the one hand , it may be interesting to study the form of the probability distributions of pdf replicas : for instance , this could allow one to directly address the question of what in a conventional procedure is the @xmath17 range which correponds to a 68% confidence interval",
    ". on the other hand , it may be useful to investigate systematically the statistical impact of each dataset , with the aim of arriving at a full `` closure test ''  a proof that there is no information loss in extracting pdfs from data .",
    "these improvements may be useful and even necessary for precision phenomenology at the lhc .",
    "* acknowledgments : * we thank g.  cowan , l.  lyons and h.  prosper for discussions and encouragement .",
    "is supported by the bundesministerium fr bildung and forschung ( bmbf ) of the federal republic of germany ( project code 05h09pae ) .",
    "this work was partly supported by the spanish mec fis2007 - 60350 grant ."
  ],
  "abstract_text": [
    "<S> we discuss the statistical properties of parton distributions within the framework of the nnpdf methodology . </S>",
    "<S> we present various tests of statistical consistency , in particular that the distribution of results does not depend on the underlying parametrization and that it behaves according to bayes theorem upon the addition of new data . </S>",
    "<S> we then study the dependence of results on consistent or inconsistent datasets and present tools to assess the consistency of new data . </S>",
    "<S> finally we estimate the relative size of the pdf uncertainty due to data uncertainties , and that due to the need to infer a functional form from a finite set of data . </S>"
  ]
}