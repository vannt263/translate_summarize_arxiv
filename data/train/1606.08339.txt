{
  "article_text": [
    "applied time series analysis , forecasting and accompanying methods of decision analysis using increasingly sophisticated stochastic models of time series is nowadays central to many companies , non - profit organizations , research groups and individuals in the business of investment management , as well as in the broader financial services industries . among frontier applied research questions is a central challenge of scaling statistical analysis addressing dynamics in cross - series relationships of multiple time varying indices i.e. , of usefully characterizing complex patterns of multivariate volatility to apply to forecasting and decisions with higher - dimensional time series .",
    "this is the focus of this paper , addressed in terms of modelling and methodological advances coupled with a detailed case study in finance .",
    "dynamic dependence network models are extensions of multiregression dynamic models ( mdms  @xcite ) .",
    "mdms incorporate directed graphical model structure into a multivariate time series , allowing contemporaneous values of some univariate series to appear as predictors of other series .",
    "originally introduced to preserve certain conditional independence structures related to causality over time  @xcite , mdms have been developed and applied to multivariate time series in areas such as forecasting of brand sales and traffic flows  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and as empirical models of dynamic network structures generating inter - related time series in areas such as neuroscience , engineering signal processing and financial econometrics  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "while these previous works have illustrated the effectiveness of some particular mdms in inference and forecasting of multivariate time series , our interests here are defined by needs for several extensions of the modelling ideas and methodology .",
    "beginning with the basic mdm framework , we are motivated to extend and explore more general methodology to capture and quantify time - variations in patterns of conditional independence structures .",
    "we do this via the concept of sparsity in conditional dependence networks and develop analysis to enable dynamic modelling of these sparse networks over time .",
    "this is overlaid with innovations in bayesian model uncertainty analysis relative to conditional independence structure , in a dynamic / adaptive strategy that also deals with model parameter uncertainty .",
    "further , we link this extended mdm framework to the increasingly adopted cholesky - style approach to modelling multivariate stochastic volatility  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "then , we are interested in extensions to include time - varying autoregressions in predictive model components . such natural extensions of mdms have not , to date , been exploited , in part due to the lack of extension of existing theoretical results for forecast distributions more than one - step ahead ; we address this in the context of an overall simulation - based analysis that immediately allows forecasting multi - steps ahead as required in many applications including our portfolio studies .",
    "finally , ddnms inherit the mdm feature that sequential time series analysis and forecasting can be _ decoupled _ into that of a set of univariate dynamic linear models ( dlms) so enabling fast , parallel processing and then _ recoupled _ for forecasting and decisions .",
    "section  [ sec : ddms ] discusses mdms and links to cholesky - style multivariate volatility models , then develops the decouple / recouple feature of sequential analysis that enables parallel processing of multivariate time series .",
    "this section notes some new and practically relevant technical developments that are detailed in the appendix .",
    "section  [ sec : ddnms ] defines a class of ddnms that extend mdms to include predictive dynamic model components with time - varying autoregressive ( tvar ) structure .",
    "the forward - filtering and forecasting analysis is discussed , with required extensions to the existing mdm theory . in this context",
    ", we develop model structure uncertainty analysis via sequential bayesian mixture modelling with implied model averaging for inference and forecasting . developed in detail in section  [ sec : modeluncertainty ] , this addresses uncertainty about , and learning on , structural model components including the predictor variable uncertainty that defines the network \" structure ; that is , the patterns of sparsity in contemporaneous relationships among series as well as potential links to lagged values of the series .",
    "embedding these in an overall framework of bayesian model uncertainty and model averaging leads to inference on the time - varying structure of the implicit network of interconnections .",
    "the analysis also includes uncertainty about key model hyper - parameters , including tvar lags and discount factors defining rates of change of state vectors and volatility processes .",
    "a practically important element of the work is the use of annealed structure learning via power discounting that acts to limit the degeneracy of posterior model probabilities over time , and so enhance model adaptability to new data and changing circumstances .",
    "discussed in section  [ sec : modeluncertainty ] , this is shown in the case study of section  [ sec : casestudy ] to be both statistically supported in terms of enhancing model fit and forecast performance and to underlie improved decisions in resulting financial portfolio evaluations .    with brief background on bayesian decision analysis in dynamic portfolio allocation in section  [ sec : portfoliotheory ] , the case study in section  [ sec : casestudy ] concerns a @xmath0dimensional time series of daily prices ( in $ us ) of several international currencies , commodities and stock indices over a time - span of 11 years .",
    "the section summarizes key aspects of ddnm specification , assessment and use in both @xmath1 and @xmath2day ahead forecasting , and explores the outcomes of a range of portfolio studies .",
    "key technical details are in the series of sections of the appendix .",
    "section  [ sec : conc ] concludes the paper with some summary comments .",
    "the framework is that of structured state - space modelling and general notation follows that of standard bayesian dynamic linear models  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "the @xmath3-vector time series @xmath4 is observed over time @xmath5 .",
    "denote by @xmath6 information available at @xmath7 and by @xmath8 the time @xmath9 information set ; the latter are sequentially updated as observations are made over time .",
    "consider each univariate series @xmath10 ; for @xmath11 let @xmath12 be a subset of indices of those series higher than @xmath13 in the selected order , and set @xmath14 the empty set .",
    "then @xmath15 is the @xmath16vector of time @xmath9 values on the series in the _ parental set _ @xmath17 the @xmath18 independent , univariate dlms of define a triangular system that , by composition , yields a full multivariate model for @xmath19 assuming sparsity of parental sets i.e. , that some or all of the @xmath20 contain fewer than the full number of potential parental indicators this is a dynamic _ graphical model _",
    "@xcite ; the graphical modelling terminology reflects the construction of the model from a set of conditional distributions in a directed , acyclic graph format resulting from the triangular / cholesky - style specification .",
    "this is a general example of a multiregression dynamic model .",
    "the coupled set of univariate dlms is , over times @xmath21 @xmath22 with components as follows :    * @xmath23 , a known column vector of predictors or constants , with corresponding dynamic regression coefficients in the column state vector @xmath24 each of dimension @xmath25 * @xmath26 , a vector of dynamic regression coefficients @xmath27 @xmath28 linking contemporaneous values of some of the other series to series @xmath13 ; the number of parents and dimension of @xmath26 is @xmath29 * the observation errors are conditionally independent over @xmath13 with @xmath30 independently of @xmath31 with possibly time - varying precisions @xmath32 define @xmath33 * the full dynamic state and regression vectors , each of dimension @xmath34 are @xmath35    complete model specification involves time evolution models for the @xmath36 and @xmath32 we develop these below , building on traditional bayesian dynamic linear model specifications  ( e.g. * ? ? ?",
    "* ) .    with notation @xmath37 and @xmath38 for",
    "@xmath39 collect the effective coefficients @xmath26 and implicit zero values in the matrix @xmath40 the set of @xmath18 coupled models of can then be written as @xmath41 where @xmath42 with @xmath43 and @xmath44 with precision matrix @xmath45 then @xmath46 hence @xmath47 is the cholesky of @xmath48 subject to row - scaling by the square roots of the diagonal entries @xmath32    the parental sets reflect the _ contemporaneous conditional dependence structure _ across the series ; conditional on state parameters and predictors , for any @xmath49 we see that @xmath50 if @xmath51 this class of mdms naturally yields a path to flexible modelling of multivariate volatility , as we can use any state - space evolution for the @xmath26 and univariate volatility models for the @xmath52 , and they together induce the stochastic dynamics of the implied @xmath53 also , sparse structuring will be based on small , parsimonious choices of parental sets , so yielding structure and , typically , sparsity in the resulting dynamic graphical model .",
    "the desirability of this and benefits in terms of potential to improve forecasts and resulting decisions in areas such as financial portfolio analysis has been highlighted in earlier uses of mdms and other approaches to dynamic graphical models  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "the models of are completed by specifying dynamic model forms for the state vectors @xmath36 and precisions @xmath52 over time .",
    "standard dlm classes provide ranges of models for structured , linear and conditionally gaussian evolutions of the @xmath36 coupled with tractable discount specifications for evolution noise levels as well as for the residual volatilities @xmath52  @xcite .",
    "we use one of the simplest such specifications in our case study below multivariate random walk evolutions for each of the @xmath36 over time coupled with discount specifications and so restrict discussion here to that specific model form .",
    "specifically , we adopt random walk state evolution models @xmath54 where the evolution error @xmath55 is zero - mean normal , independent over time and across series , and has a time - dependent evolution error variance matrix defined via a single discount factor @xmath56.$ ] coupled with this is a standard random walk volatility model @xmath57 where the @xmath58 are independent beta random variates with time - dependent beta parameters defined via the single discount factor @xmath59 $ ] for series @xmath60 again , these are standard models and full details appear in the above references . we include summary details in appendix a of this paper , together with summaries of the resulting prior , posterior and forecast distributions as they are updated over time . critically , these analyses apply in parallel , the series being decoupled for forward - filtering and forecasting within - series ; forecast distributions are then coupled together for multivariate forecasting , as summarized below .",
    "key elements of the forward - filtering analyses are sequentially updated versions of the following distributions .",
    "these are specific to each series @xmath13 and conditionally independent across @xmath60 see appendix a for the full technical and notational details .",
    "[ [ posteriors - and - priors - at - t-1 ] ] posteriors and priors at @xmath61 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    at each time @xmath61 information @xmath62 is sufficiently summarized in terms current normal / gamma posteriors @xmath63 where the notation represents the conditional normal and marginal gamma @xmath64 and where @xmath65 is a current point estimate of the residual variance @xmath66 these imply @xmath1step ahead prior distributions for states of the same normal / gamma form @xmath67 where @xmath68 and @xmath69 based on the specified discount factors @xmath70",
    "[ [ step - forecasts - at - time - t-1 ] ] @xmath1step forecasts at time @xmath61 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the implied predictive distribution is t with @xmath71 degrees of freedom , @xmath72 where @xmath73 appears linearly in @xmath74 and quadratically @xmath75    [ [ k - step - forecasts ] ] @xmath76step forecasts : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    more than @xmath1step ahead forecast distributions are similarly given by conditional t distributions ; the conditioning requires known values of future independent predictor variables and explicitly involves future values of parental series for each @xmath60 the practical approach to utilizing this theory for @xmath76step forecasting is detailed below .",
    "the compositional nature of the triangular / cholesky - style mdm yields access to nice analytics in evaluating @xmath1step ahead forecasts and various relevant aspects of the full multivariate predictive distribution . applied work",
    "will require computation of predictive means and variance matrices , and other summaries , as well as evaluations of the joint density function .",
    "some specific comments are given here , with full theoretical details in appendix b of the paper .",
    "first , note that the full @xmath1step ahead predictive density for @xmath77 is , via composition , simply @xmath78 as the univariate conditionals @xmath79 are t densities noted above , the multivariate distribution is a product of ts .",
    "the p.d.f .",
    "is easily evaluated based on observed data .",
    "this point is important in model assessment , comparison and combination , as the product over time of the joint predictive densities under any chosen model defines the model marginal likelihood . in extended models below",
    ", we use this in comparing model structures in terms of ranges of parental sets as well as model hyper - parameters , including discount factors and other aspects of model specification .",
    "second , we typically require @xmath1step ahead predictive moments as well as , in some cases , other summaries . for portfolio applications based on mean - variance optimisations and trade - offs such as in our case study of this paper we are interested in forecast mean vectors , variance matrices and precision matrices @xmath80 under  .",
    "assume that , for all @xmath81 the degrees - of - freedom parameters @xmath71 exceed 2 , so that the variances exist .",
    "the recursive form of the compositional model means that we have access to analytically tractable recursions to enable the calculations of @xmath82 and @xmath83 full details appear in appendix b. this explicitly recognizes the appearance of contemporaneous values of the @xmath73 in the conditioning of forecasts for @xmath84 further , in some applied work we are also interested in the predictive precision matrix @xmath85 .",
    "this directly reflects the conditional dependence structure between variables and thus plays a crucial component in various types of analysis .",
    "in particular , the precision matrix has a determining effect on the allocation portfolio weights in financial portfolio studies .",
    "it turns out that the recursive evaluation of joint predictive means and variance matrices has an analytically nice parallel for recursive computation of the precision matrix . as well as a new theoretical result for mdms ,",
    "this is a key practical note since it allows us to avoid direct matrix inversion .",
    "again , details are given below in appendix b.    finally , we note that the computation of moments and precision matrices of @xmath76step forecast distributions for @xmath86 follow very similar lines , so details are omitted here .",
    "we use the term _ dynamic dependence network model _ ( ddnm ) for an mdm that has been extended to allow for time - varying autoregressive ( tvar ) components in each univariate series .",
    "this broader model class extends the practical utility of mdms , while requiring extensions of the methodology to enable bayesian forecasting in the resulting time - varying , vector autoregressions ( tv - vars ) coupled with cholesky - style multivariate volatility .",
    "the network \" terminology is relevant in that ddnms have dynamic linkages both across series and at lagged values that can represent and be interpreted as both contemporaneous and lagged network interconnections .",
    "indeed , variants of these models that utilize latent thresholding concepts for the dynamics of state vectors have recently been explored in contexts where network structure is a key interest  @xcite .",
    "here that is not a key focus , but the inclusion of tvar model components is of central interest in terms of improving forecast accuracy , and resulting characterizations of cross - series patterns in multivariate volatility .",
    "the ddnm class modifies the basic mdm of as follows . for each @xmath87 @xmath88 where @xmath89 is a time - varying intercept , each @xmath90 is a @xmath91vector of tv - var coefficients for lag @xmath92 for some maximum lag @xmath93 and @xmath94 the observation noise .",
    "it is clear that we could add dynamic effects of additional independent variables to enrich the class of models ; our case study does not do that , but the methodological details are directly extensible .",
    "let @xmath95 and write @xmath96 for the @xmath97-vector that extends @xmath90 with zeros for the elements of subscript larger than @xmath98 then   can be written in the vector form @xmath99 where @xmath100 and @xmath101 is as in  . in our portfolio application , we consider the simple but practically central tv - var model where the autoregressive predictor variables of series @xmath13 only contains its own lags and the intercept ; that is , @xmath102 where @xmath103 is the time - varying autoregressive coefficient of series @xmath13 at lag @xmath104 @xmath105 this can be written as an mdm in which @xmath106 and @xmath107 as a result , much of the theory and methodology of mdms applies . in particular , the general results on forward - filtering and @xmath1step ahead forecasting of section  [ sec : mdmanalysis ] hold for these ddnms .",
    "however , for forecasting more than one - step ahead , the mdm theory is inapplicable .",
    "this arises as the existing theoretical results for forecasting in the mdm framework require knowledge of the future predictor variables ; in autoregressive contexts , the future predictors include lagged values of the @xmath10 which are unknown at the time of forecasting . for any @xmath108 forecasting @xmath76steps",
    "ahead requires an ability to deal with uncertainty about the then - required predictors that are values of @xmath109 now in the dynamic linear regressions representing both lagged values of the current series @xmath13 as well as the parental predictors .",
    "the solution to this is simulation : given a current , time @xmath9 set of posterior for state vectors and volatilities across the @xmath18 series , we can trivially simulate each model to time @xmath110 and conditional on the value of the sampled @xmath111 vector , continue to sample @xmath112 and so forth up to whatever lead time required . repeating this",
    "independently will define a monte carlo sample from the full set of posterior predictive distributions over each @xmath113 and hence from the full predictive distribution @xmath114 .",
    "direct summarization then leads to monte carlo approximations to predictive mean vectors , variance matrices and other quantities of interest .",
    "critically , we can simulate samples as large as desired very efficiently , since this uses the analytically tractable set of @xmath18 dlms analyzed and simulated in parallel .",
    "the forward - filtering updates and simulation computations are standard and technically / computationally trivial .",
    "application of ddnms requires addressing questions of model uncertainty about key defining parameters : the structural parental sets @xmath20 , and the hyperparameters comprising tvar model orders @xmath115 and discount factors @xmath116 for each @xmath117 we address this using multiple ddnms , each defined by selected parameters , evaluating and sequentially revising posterior model probabilities across this discrete set of models , and then averaging over models for inferences and predictions .",
    "this basic mixture modelling approach has been central to bayesian forecasting and dynamic models for decades , predating its more recent popularization in static models under the name bayesian model averaging ( bma )  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* chapt 12 , and references therein ) .      for each @xmath87 define @xmath118 for any specific choice of these parameters . here : ( i ) @xmath119 can take any of the @xmath120 possible values ( though we may decide to restrict the possibilities based on exploratory analysis of initial training data or on substantive grounds ) ; ( ii ) @xmath121 for some specified maximum lag @xmath122 ( iii ) the discount factor pair @xmath116 takes a value from a discrete set of @xmath123 points on a grid in @xmath124 ^ 2.$ ] allowing the maximum set of possible parents , this defines a class of @xmath125 possible ddnms for series @xmath60    given specific values for each @xmath126 we have one ddnm for @xmath77 whose parameters are denoted by @xmath127 the number of such models is @xmath128 ; in any realistic application , this will be too large a class of models to evaluate .",
    "for example , our case study has @xmath129 @xmath130 and @xmath131 , yielding more than @xmath132 possibilities .",
    "fortunately , the compositional structure of ddnms leads to a massive reduction based on assumption of independent priors over model structures across the set of @xmath18 univariate models .",
    "see this as follows .    at any given model @xmath133 ,",
    "extend our earlier notation for @xmath1step forecast densities of to explicitly note the dependence on the values in @xmath133 ; at time @xmath9 , the p.d.f .",
    "is @xmath134 then , under any prior distribution giving initial probabilities @xmath135 to each of the large set of possible models , the posterior model probability based on observed data @xmath136 is @xmath137 where the last step inserts the product of univariate t densities of at each time @xmath138 now suppose that prior at @xmath139 has model parameters independent across series @xmath13 , with @xmath140 this implies that the full model posterior in is the product of @xmath18 independent model posteriors @xmath141 this last equation also shows how these model @xmath13 probabilities are sequentially updated over time as successive observations are made .",
    "each of the @xmath1step conditional densities is a univariate t , so the computations are trivial for each @xmath142 .",
    "the factorization over series @xmath13 implies that the analyses can be run in parallel .",
    "this exploitation of the decoupling inherent in ddnms thus reduces the computation to that of evaluating @xmath143 univariate dlms in parallel and then combining the results across series to deliver model probabilities over all @xmath133 . in our case study with @xmath129 @xmath130 and @xmath131 ,",
    "this yields a very manageable number of just over 300,000 models .",
    "then , as a result , we have overall model probabilities updated sequentially via the resulting dynamic / time @xmath9 version of , namely @xmath144 the analysis requires specification of model uncertainty priors .",
    "we have prior independence over @xmath145 as noted above , and take each series @xmath146specific model prior as @xmath147 with independent components as follows : _ ( i ) _ our interest in sparse models favors priors on the parental sets @xmath20 that penalize large values of @xmath148 thus , a parental set @xmath20 with @xmath149 elements has prior probability @xmath150 with an expected parental set size of @xmath151 .",
    "the latter provides insight into prior specification of @xmath152 _ ( ii ) _ discount factors @xmath116 are selected from a rectangular grid of @xmath123 specified values with a uniform discrete prior on the grid . _",
    "( iii ) _ the tvar model order @xmath153 is assigned a uniform prior on the range @xmath154 , given the chosen maximum possible lag @xmath155 .",
    "we make one additional extension of the model uncertainty framework .",
    "it is well known that , with sufficient data accrued , posterior model probabilities concentrate around a smaller set of models , eventually favouring a single model  ( e.g. * ? ? ?",
    "* chapt 12 ) .",
    "this theoretically guaranteed behaviour can often lead to significantly down - weighting many models that may be of possible future interest , and degrade predictive performance as a result .",
    "this has led to interest in discounting past data to allow model probabilities to depend more on recent and current behaviour of the time series , and to adapt more adequately to incoming observations . a particular method of _ power discounting _",
    ", used historically in bayesian forecasting  @xcite has shown promise in portfolio studies  @xcite and has recently received attention in other applied areas  @xcite ( linking to a parallel historical literature where discount factors are called forgetting \" factors ) . the basic idea and resulting",
    "implementation is simple .",
    "extend the discrete set of models to @xmath156 where @xmath157 $ ] is a _ model probability power discount factor_. then the computation of posterior model probabilities is modified from the standard bayesian update of to the extended form @xmath158 then being normalized to sum to 1 over all possibilities @xmath159 the @xmath160power applied to the time @xmath61 model probabilities acts to increase the dispersion of this time @xmath61 posterior , somewhat down - weighting the information content of past data .",
    "smaller values of @xmath161 discount history to a greater extent , flattening \" the time @xmath61 posterior relative to the standard update when @xmath162    the prior specification now extends to add a prior @xmath163 .",
    "we take @xmath163 to be discrete uniform on a chosen grid of points in @xmath124.$ ] this extends the analysis to include the power discount , so extends the model size in one additional dimension . resulting conditional posterior model probabilities",
    "are still computed as above and then combined for evaluation of at each time @xmath9 .    finally note that , given the full set of model probabilities at each time",
    "@xmath164 we simply marginalize via summation to deduce implied marginal posteriors on any subset of elements of @xmath159 this is the route to evaluating over time the posterior support for different values of each discount factor , now including the power discount factor , as well as parental set membership and tvar model orders .",
    "we use this extensively in the case study below .      under the general discrete model space ,",
    "@xmath1step ahead forecast distributions are discrete mixtures over models @xmath133 of the product - form ddnm forecast distributions whose structure is discussed in section  [ sec : mvpmdm ] .",
    "the @xmath1step ahead forecast mean vectors and variance matrices required for portfolio studies can then be evaluated by bayesian model averaging using extensions of the nice , analytic recursions of that section and appendix b. several changes are needed to account for the model averaging , and a new theoretical element for computing covariance terms between pairs of series @xmath165 detailed in appendix c of this paper is key .",
    "readers can refer to that appendix for additional details .",
    "for forecasting more than @xmath1step ahead , the involvement of tvar terms in ddnms means that we do not have easily implemented analytic forms for forecast moments .",
    "hence , for @xmath86 we explore the @xmath123-step - ahead predictive distribution @xmath166 via direct and straightforward monte carlo simulation .",
    "drawing a large monte carlo sample from this distribution as detailed in appendix c yields relevant monte carlo estimates of the predictive mean and variance matrix .",
    "the simulations can be run in parallel and computationally cheap per sample .    in our case study , as in other financial applications , we adopt models in which the @xmath10 are logged values of fx prices , commodity prices , or stock prices .",
    "hence , even if we had access to analytic forms of predictive moments , they would be of limited interest as our portfolio decision analyses require as inputs at each decision making stage the predictive mean vectors and variance matrices of the implied returns , i.e. , non - linear transformations of differences of log prices . here",
    "the use of monte carlo simulations comes into play positively , as we can of course simply transform all simulated price series to returns , and hence directly compute sample estimates of the forecast means and variance matrices on the returns scale .",
    "our case study concerns bayesian forecasting and decision analysis for financial time series , and follows standard approaches in utilizing extensions of traditional markowitz portfolio optimisation  @xcite .",
    "the basic methodology for our portfolio decision analysis is summarized here .",
    "we consider @xmath76step ahead portfolio optimisation with on ddnm - based forecasts ; in the study below we evaluate cases with @xmath167 and @xmath168 on daily data .",
    "focus here on the @xmath1step case ; the development for @xmath86 is the same but for the fact that it uses @xmath76step ahead predictive distributions and assessments of portfolio characteristics with rolling @xmath2day horizons ; readers can impute the omitted details .",
    "as we are interested in financial _",
    "returns _ from investment decisions , any model of financial series that does not directly uses observed returns as the time series @xmath169 will have to enable computation of the implied mean vectors and variance matrices of future returns themselves . as noted earlier , our case study adopts our generally preferred approach of modelling _ log prices _ of fx series , commodities , and stock market indices . given observed or simulated log prices @xmath10 for one series @xmath13 over any period of time , the returns are simply evaluated via exponentiating the differences of log prices .",
    "hence , in our simulation - based analysis of ddnms , it is trivial to map monte carlo samples of predictive distributions for future log prices to those of future returns , and then compute monte carlo approximations of the required moments and other aspects of the distributions .    for our development here",
    ", we drop the time and forecast horizon in the notation for clarity .",
    "whatever the model form , step ahead desired , and nature of computation , we will use the running notation but now explicitly for returns : looking ahead our desired horizon , @xmath169 is the @xmath91vector of future returns , and we suppose that at the current time point we have evaluated the forecast mean vector @xmath170 and variance matrix @xmath171 , denoted by @xmath172 we now reallocate existing investments at the current time according to a portfolio weight vector @xmath173 that redistributes investments among the @xmath18 indices ; the eventual return will then be @xmath174 when we move ahead in time and learn @xmath175 the weight vector is chosen via bayesian decision analysis to optimise a specific portfolio loss function : generally , minimizing expected portfolio risk while aiming for good realized returns , subject to additional constraints . under the forecast moments @xmath176",
    ", the implied return on the portfolio for any given portfolio weight vector @xmath173 then has mean and variance @xmath177 portfolio risk is taken as the standard deviation @xmath178 in financial terminology this is referred to as the _ projected risk ( pr ) _ of the portfolio .",
    "we take the traditional closed portfolio approach in which the portfolio weights sum to 1 , i.e. , @xmath179 , so that we are simply reinvesting existing resources ( not increasing from external sources or reducing the overall investment level ) in order to make fair comparisons across models and utility functions .",
    "we examine variants of three commonly used portfolio allocation rules , as follows .",
    "these all depend critically on the forecast precision matrix , denoted by @xmath180    1 .",
    "_ target portfolio : _ given a specified return target @xmath181 , optimise the portfolio weights by minimizing the ex ante portfolio variance among the restricted set portfolios with expected return @xmath182 .",
    "the investor decision problem reduces to choosing the vector of portfolio weights @xmath173 to minimize @xmath183 , subject to constraints @xmath182 and @xmath179 . direct analysis using lagrange multipliers",
    "yields analytic forms of the optimising vector @xmath184  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* chapt 10 ) .",
    "target constraint portfolio : _ this modifies the first rule by adding the constraints that each element of @xmath173 must be nonnegative .",
    "the solution @xmath185 has no closed analytic form , but can easily computed using one of many standard non - linear optimisation algorithms ; we use the quadratic programming tools in matlab in our case study .",
    "benchmark uncorrelated target portfolio : _ this strategy involves an additional benchmark \" time series @xmath186 .",
    "the forecasting model is then fitted to the extended @xmath187dimensional series , with the benchmark now included .",
    "this will lead to forecast moments for the returns on the original @xmath18 series together with the benchmark . at our current time",
    "point , suppose this leads to forecast benchmark mean and variance @xmath188 and with forecast covariance vector between our original series and the benchmark of @xmath189 for some covariance vector @xmath190 this strategy modifies the target portfolio with the additional constraints @xmath191 and @xmath192 that is , we aim at a return that exceeds that of the benchmark by the specified target @xmath193 while being uncorrelated with the benchmark .",
    "direct analysis using lagrange multipliers yields an analytic form for the optimising vector @xmath194 .",
    "the traditional target portfolio defines a trade - off between risk and expected return .",
    "adding non - negativity constraints will naturally increase the risk for a given target return , and is a rule that comes closer to representing realistic constraints on individual investors and some mutual funds , for example , which are inherently constrained to be long - only .",
    "the benchmark uncorrelated rule defines a decoupling and risk diversification strategy ; a core idea is that it aims to exceed the benchmark whether it rises or falls .",
    "the benchmark can be any asset ; we choose the s&p 500 index as benchmark in our case study .",
    "one main goal is to examine and illustrate the utility of ddnms , using our extended model uncertainty framework . within that , a key applied interest is the utility in practical portfolio decision making .",
    "it has been empirically demonstrated by many that more accurate forecasts do not necessarily lead to better investment performance ; hence , in addition to describing aspects of the bayesian analysis in terms of inference on model structure and forecasting , we evaluate a number of performance measures of more practical relevance to the financial investment management context",
    ".    for higher - dimensional portfolios , appropriately structuring and constraining forecast distributions is critical ; even very small changes in variances and covariances among assets can have important consequences on resulting portfolios  ( e.g. * ? ? ?",
    "if conditional dependence structures can be appropriately captured by sparse ddnms , the reduced parameter dimension can improve accuracy and stability of estimation and hence forecasting , and so lead to improved portfolios .",
    "the extended ddnm uncertainty analysis offers the ability to focus on classes of sparse models , and additional stability may then arise via the bayesian averaging over multiple sparse models for prediction .",
    "we analyze @xmath195 financial time series with @xmath77 being logged values of daily closing prices of 9 currency exchange rates relative to the us dollar , 2 commodities prices , and 2 u.s .",
    "stock indices ; see table  [ table : abb ] .",
    "the time period of 2,979 working - week days from august 1st 2000 to december 31st 2011 includes periods of major growth as well as recession in the us and worldwide economies .",
    "the series represent major liquid benchmark securities across 3 asset classes to approximately reflect the global macroeconomic conditions .",
    "we model log prices directly , building on our experience that this is a surer route to useful predictive models than the traditional approach using returns  ( e.g. * ? ? ?",
    "* ) ; the basic point here is that moving to returns can difference away \" small changes in time that can be important in influencing short - term predictions if allowed via models on prices ( or log prices ) that have some opportunity to capture them .",
    "we split the data into a training and test data period : from august 1st 2000 to april 14th 2006 ( 1,489 observations ) as the training data set , and then from april 17th 2006 to december 31st 2011 ( 1,490 log - prices ) as the test date set to evaluate step ahead forecasting and portfolio outcomes .",
    "rll crll j&name & asset & & j&name & asset +   + 1&chf & swiss franc & & 7&aud & australian dollar + 2&eur & euro & & 8&nzd & new zealand dollar + 3&nsd & nasdaq composite index & & 9&zar & south african rand + 4&s&p & s&p 500 index & & 10&gol & gold + 5&nok & norwegian krone & & 11&cad & canadian dollar + 6&gbp & british pound & & 12&jpy & japanese yen + & & & & 13&oil & crude oil +      we first apply the ddnm with model uncertainty to analyze the training data . with a little over 300,000 possible models @xmath196",
    "this is computationally accessible .",
    "priors are as discussed above , with specific settings shown in the first 5 rows of table  [ table : par ] .",
    "this defines a symmetric bivariate grid of @xmath197 values of the model discount factors @xmath116 for each @xmath113 a grid of values for the power discount factor @xmath161 over @xmath198 $ ] ( following experiences in  @xcite ) , maximum tvar model order @xmath199 and prior parental set inclusion probability of @xmath200    following this analysis , we then modify the candidate model set prior to the sequential analysis over the test data period .",
    "the point here is simply to recognise that many models in the full set of models have such low posterior probability based on the training data that it is justifiable and then computationally efficient to cut - back to a smaller space of potential models for further use in sequential forecasting and portfolio studies over the test data period .",
    "we remove models whose posterior probabilities after the training period are lower than @xmath201 , a small threshold ; this study takes @xmath202 as per table  [ table : par ] .",
    "cl c cl parameter & value & & setting & value +   + @xmath203 & 0.975 : 0.005 : 0.995 & & @xmath201 & 0.001 + @xmath204 & 0.975 : 0.005 : 0.995 & & @xmath205 & 10,000 + @xmath161 & 0.950 : 0.005 : 1.000 & & target return & 0.5% ( daily ) + @xmath155 & 2 + @xmath206 & 0.3    for the test data analysis period , we run the extended ddnm analyses with model uncertainty , sequentially filtering and updating posterior model probabilities and conditional posteriors for model state vectors and volatilities within each model , and then evaluating @xmath1 to @xmath2step ahead forecast distributions at each time point .",
    "each forecasting exercise generates samples of size @xmath207 for evaluation of forecast moments , giving point forecasts and variance matrices that feed into the portfolio decision analyses under each of the three classes of portfolio loss functions .",
    "this decision analysis uses a daily base target return on portfolios of 0.1% , i.e. , a @xmath2day target of 0.5% ; this corresponds to an appropriately aggressive annual target return of 30% .",
    "we evaluate several risk characteristics of the optimised portfolios , as well as realised portfolio returns , over the test data period .",
    "figure  [ fig : ldlkh ] displays the time trajectories of the marginal posterior probabilities @xmath208 , covering both training and testing periods .",
    "we can see that models with @xmath209 are clearly dominated by those with @xmath210 confirming the relevance of power discounting of model probabilities .",
    "also , the posterior probabilities of @xmath211 stay relatively stable over time with insignificant differences over the range of values less than 1 specified here ; that is , @xmath162 is ruled out , but values on the @xmath212 ranges are otherwise hardly discriminated .       and",
    "@xmath2step ahead point forecasts plotted against the power discount factor @xmath215 this shows the dominance of smaller values of @xmath161 at both forecast horizons on these two raw forecast accuracy measures , consistent with the preference for values less than 1 based on the posterior model probabilities in figure  [ fig : ldlkh ] . note that use of much lower values of @xmath161 than explored here would lead to low posterior probabilities and increasing rmse and mad measures ; the range @xmath216 appears to be a sweet - spot \" for this key parameter .",
    ", width=528 ]    figure  [ fig : test ] shows overall forecast accuracy measured by out - of - sample root mean squared error ( rmse ) and mean absolute deviation ( mad ) of the @xmath1 and @xmath2step ahead point forecasts under model - averaged predictive distributions conditional on @xmath161 over the test data period . as shown in both figures , higher @xmath161 over this range",
    "leads to lower prediction accuracy .",
    "the relevance of @xmath211 is reinforced here . in some later summaries ,",
    "we look at outcomes based on models with @xmath217 as an example ; note that both the forecast accuracy and model posterior probabilities are relatively high at this power discounting level .",
    "these substantial improvements in forecasting at both @xmath1 and @xmath2days ahead strongly support the strategy of power discounting in sequential updating of probabilities over models on purely statistical grounds ; later we show additional support for values @xmath211 in terms of portfolio decision outcomes .",
    "some insights into the adequacy of model structure can be gained by viewing plots such as in figure  [ fig : err ] .",
    "this shows the trajectories of log prices with the @xmath2day ahead forecast mean and 90% credible intervals for the oil series .",
    "the lower frames show scatter and qq plots of the corresponding standardized @xmath2step forecast errors .",
    "there is evidence of slightly heavy - tailed departure from normal  as expected and overall excellent conformance to the model .",
    "this typifies exploratory residual plots across the series overall indicating no strong evidence of model inadequacies .",
    "figure  [ fig : avglag2 ] shows the time trajectories of posterior means @xmath218 for the effective tvar lags for each series @xmath219 together with the probabilities that @xmath220 note variation over time across all series as the model adapts to time - varying patterns in the data . to key out some example features , note that the posterior on lag 1 is high and stable over time for chf , but for a burst of volatility during the early months of the global recession in early 2008 ; s&p shows somewhat more volatile patterns over time and favours higher lagged structure .",
    "the trajectories over time of posterior means of discount factors for the model state vectors , @xmath222 ( upper frame ) , and of those for the residual volatility in observations , @xmath223 ( lower frame ) .",
    ", title=\"fig : \" ] +      figure  [ fig : avgdelbe ] shows trajectories of posterior means for the discount factors , @xmath222 and @xmath223 for each series @xmath13 over the time frame .",
    "there are notable changes over time on each , reflecting adaptation of the underlying posterior model probabilities .",
    "for instance , we see clear shifts to favouring lower volatility discount factors @xmath204 across basically all series beginning around september 2008 onwards , i.e. as the global recession escalates.the model recognizes the need for increased volatility across the entire system , and appropriately adapts to the major changes experienced at that time .",
    "later , in early 2010 as global markets are stabilizing , the posterior shifts mass towards higher @xmath204 values as global and series - specific patterns lead to reduced volatility levels .",
    "two other highlights are that the posterior for the s&p state discount factor @xmath224 favours high values throughout , reflecting the innate stability of relationships of this major aggregate index with predictors , and the posterior for the volatility discount factor @xmath225 on nok also naturally reflects lower volatility in price fluctuations of this strong and stable currency relative to the other series",
    ".      trajectories of posterior probabilities of parental set membership for each series are shown in figure  [ fig : avgedge1 ] .",
    "that is , for each series @xmath13 and potential parental series @xmath226 the posterior probability that @xmath227 conditional on @xmath228 over time @xmath9 .",
    "these figures exemplify the abilities of the model to : _",
    "( i ) _ focus on data - supported sparse models , as many such posterior probabilities are low across the entire time period , or for major time periods ; _ ( ii ) _ identify strong predictive relationships through evaluation of high posterior probabilities of some contemporaneous parents being included ; and _ ( iii ) _ adapt to changing circumstances , with probabilities showing more dynamics during the recessionary years in some cases .",
    "inherently also , the variations over time formally accommodate patterns of collinearity among potential parental predictors for each series , and time variation in such patterns .",
    "keying out some examples , the posterior probability that s&p is a parent of nsd is high throughout the whole sample period , a relationship in accord with the common market sense of strong and sustained relationships among the two stock indices .",
    "similarly , nzd is naturally a sustained parental predictor for aud , with posterior probability close to 1 throughout the whole sample period .",
    "one example of changing parental structure is the case of oil as a potential parent for cad ; the posterior probability is generally low during the later years of the great moderation , up to 2008 when it increased to higher levels in the global recession , and then maintains generally higher levels to the end of the time period .",
    "one summary of the complexity / sparsity of model structure is the size of each parental size @xmath29 by averaging across models with respect to model probabilities at any time , we can evaluate summaries such as the posterior mean @xmath229 such calculations show that while there are clear dynamics over time in the posterior probabilities of individual parents there is strong stability in terms of the effective parent set sizes . this stability in part",
    "reflects collinearities among potential parental predictors , and hence the positive relevance of a sparsity - inducing prior for parent inclusion . on the latter ,",
    "the posterior selects out \" many potential parental predictors across the series for much of the time , again reflecting data - based support for relative sparsity .",
    "rough summaries of posterior mean parental set sizes are that chf , eur , nsd have around 5 - 6 parents ; oil ( of course , as the last in the order ) has none while s&p , much higher in the order , also has just around 0 - 1 ; nok , gbp have around 2 - 3 , aud has about 4 - 5 , and the remaining indices have around 1 .",
    "we evaluate portfolio characteristics and outcomes , comparing results based on the three portfolio utility structures described in section  [ sec : portfoliotheory ] . for each ,",
    "we evaluate separately in the contexts of the ddnm - based forecasts for both @xmath1 and @xmath2day ahead portfolio rebalancing .",
    "the quantitative measures we use are the following standard performance indicators :    * _ cumulative return ( cr ) . _",
    "write @xmath230 for the realized return of the portfolio at each period @xmath231 the cumulative return over a time period @xmath232 is then @xmath233 * _ mean realized return ( mrr ) : _ over any time period @xmath234 this is simply @xmath235 * _ risk ( r ) : _ the realized risk over any period @xmath236 is simply the sample standard deviation @xmath237 of realized returns @xmath238 * _ projected risk ( pr ) : _ projected risk is the finance term for the theoretical standard deviation of the forecast distribution of the optimised portfolio , as noted in section  [ sec : portfoliotheory ] . at any time @xmath9 ,",
    "if @xmath239 are the mean vector and variance matrix of the forecast returns and @xmath240 the optimal weight vector , then @xmath241 * _ sharpe ratio ( sr ) : _ this compares realized returns to realized risk via @xmath242 over any period @xmath234 typically converted to and reported on an annualized basis . * _ projected sharpe ratio .",
    "_ this is the theoretical analogue of the realized sharp ratio , given by @xmath243 for forecasts made at time @xmath9 and with resulting optimal weight vector @xmath244    further comparisons are made with a professional investment community benchmark , the _ newedge cta index _  @xcite .",
    "professional money managers and commodity trading advisors ( cta ) typically monitor managed futures accounts , which generally have exposure to a number of markets such as stocks , derivatives , commodities , energy , agriculture and currency .",
    "the cta index is designed to track the largest 20 ctas ( largest in terms of assets under management ) and to be representative of the managed futures markets broadly . since the portfolio of the 13 assets here is comparable to that managed by ctas ,",
    "we compare the performance of our portfolios with the publicly available _ newedge cta index _ as a key benchmark .",
    "we set daily target returns to 0.1% corresponding to a target of 30% on an annual basis .",
    "figure  [ fig : cr1 ] shows results across the test data period based on @xmath1day ahead forecasts for model averaged ddnms using differing values of the model probability power discount @xmath215 the plot shows cumulative returns , sharpe ratios and risks from each of the three portfolios , together with the the cta index .",
    "it can be seen that the target and benchmark neutral portfolios perform similarly well and beat the cta index consistently in terms of both raw ( cr ) and risk - adjusted ( sr ) returns .",
    "relative to the cta index , the excess risk incurred by these strategies are modest and outweighed by the improved returns .",
    "the target constraint portfolio , in contrast , has substantially poorer performance on all three metrics on this short - term , @xmath1day ahead basis .",
    "day ahead forecasts from models differing only through the value of the power discount @xmath161 .",
    "this shows similar performance of the target and benchmark neutral portfolios on all measures , and their dominance over the cta index in terms of raw and risk - adjusted returns .",
    ", scaledwidth=90.0% ]      with a @xmath2day target return of 0.5% , consistent with the 30% annual return , the parallel @xmath2day results appear in figure  [ fig : cr5 ] .",
    "it is evident that when @xmath245 both return measures are higher than for @xmath161 outside this range , while the risk is only slightly higher .",
    "forecasting and decisions using standard model uncertainty analysis @xmath246 ) is evidently dominated in terms of portfolio performance by models with @xmath161 in this range , while models with smaller @xmath161 clearly suffer degraded performance ( due to over - discounting historical data and hence over - fitting more recent data ) .",
    "we also now see that target constraint portfolio shows generally superior performance in this longer - term , @xmath2day ahead analysis than at the shorter @xmath1day horizon , achieving generally higher returns at the cost of higher risk incurred by its no - shorting \" constraints .",
    "day ahead forecasts from models differing only through the value of the power discount @xmath161 , with conclusions generally paralleling those under the @xmath1day analysis .",
    "one specific point to note is that the target constraint portfolio outperforms others at @xmath2days ahead but under - performs relatively at the @xmath1day horizon , a point linked to reversal and momentum effects in assets : long - only portfolios like the target constraint here tend to show reversal effects at shorter horizons while benefiting from momentum effects at longer horizons . ]",
    "compared with the cta index , all portfolios with @xmath248 have higher raw and risk - adjusted returns , while the risks are also larger but better - compensated by higher returns .",
    "if we take @xmath217 as an example , more detailed performance comparisons are shown in figure  [ fig : cumrtn ] and table  [ table : perf5day ] .",
    "the target constraint portfolio empirically realizes largest cumulative return over the full test data period , as a trade - off from also incurring the largest risk among the three portfolios .",
    "a point of practical interest is that , in terms of cumulative return , the portfolio with target constraint outperforms others at @xmath2days ahead but under - performs relatively at the @xmath1day horizon .",
    "this is consistent with the increased role of reversal rather than momentum effects on assets in the short - term , and vice versa in the long - term .",
    "that is , long - only portfolios such as our target constraint portfolios tend to show reversal effects at shorter horizons while benefiting from momentum effects at longer horizons .",
    "for @xmath76step ahead forecasting at any time @xmath9 , we clearly need the future values of parental variables for each model , so make that explicit in notation . at time",
    "@xmath164 assuming knowledge of future @xmath249 and being explicit about the need for values of the parental series , we have the following .",
    "first , the @xmath76step ahead prior at time @xmath9 is @xmath250 where @xmath251 are updated inductively from @xmath252 and @xmath253 . based on this , we have forecast distribution @xmath254 where @xmath255 and @xmath256 write @xmath257 then @xmath258    [ [ appendix - b - joint - predictive - moments - and - precision - matrices - in - mdms ] ] appendix b : joint predictive moments and precision matrices in mdms ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~      as discussed in section  [ sec : mvpmdm ] , we are interested in the moments @xmath259 under  .",
    "assume that , for all @xmath81 @xmath260 so that these moments exist .",
    "the compositional model form allows for recursive moment computation that recognizes the appearance of contemporaneous values of the @xmath73 in the conditioning of forecasts for @xmath84 details now follow ( note also that @xmath76step ahead computations are very similar , and so details are omitted here ) .",
    "compute the univariate mean and variance of @xmath265 @xmath266 using the implied simplified forms of   with all terms in @xmath267 set to zero .",
    "insert @xmath268 as the @xmath91the element of @xmath82 and @xmath269 as the @xmath270element of @xmath271 .",
    "* at this point , we have already computed the values of the moments of from the previous steps .",
    "these are used in the following calculations .",
    "first , write @xmath273 for the subvector of means in @xmath274 on elements in @xmath20 only , and @xmath275 for the corresponding submatrix of @xmath276 then , the marginal mean @xmath277 and variance @xmath278 of @xmath10 are computed as follows .",
    "@xmath279 where @xmath280 insert @xmath277 as the @xmath146th element of @xmath82 and @xmath278 as the @xmath281element of @xmath282 respectively .",
    "* compute the covariance vector @xmath283 as follows .",
    "write @xmath284 for the @xmath261vector that extends @xmath285 with zeros for elements @xmath286 then @xmath287 insert element @xmath288 of this vector as the @xmath289 and @xmath290 entries of @xmath282 for @xmath291          consider now the predictive precision matrix @xmath294",
    ". for each @xmath11 denote the precision matrix of the @xmath261vector @xmath262 by @xmath295 in parallel to the subvector means and variance matrices in above ; again , at @xmath296 these are scalars .    to compute the precision matrix at each time @xmath9",
    ", we can avoid matrix inversion by utilizing the interim products , namely the covariance vectors @xmath297 that have been already calculated above .",
    "this block - wise inversion decreases computational instability and reduce complexity to @xmath298 and is especially efficient under sparse models for larger @xmath299 the computation of @xmath300 is as follows .          *",
    "given the computed @xmath297 , compute the precision matrix @xmath302 via its partition as @xmath303 with entries @xmath304 here @xmath305 is a scalar , so this recursive computation of @xmath300 avoids matrix inversions .",
    "[ [ c1.-1-step - ahead - forecast - moments - in - discrete - mixtures - of - ddnms ] ] c1 .",
    "@xmath1step ahead forecast moments in discrete mixtures of ddnms ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    with reference to @xmath1step ahead forecasting in mixtures of ddnms arising via bayesian model uncertainty analysis as in section  [ sec : forecastmix - of - ddns ] , basic technical details are noted here .",
    "this gives the analytic forms for @xmath1step ahead joint predictive means , covariance and precision matrices , via direct extension of the analytic results in appendix b.    standing at time @xmath61 , we introduce the following notation for marginal and model - conditional forecast means and variances within each univariate series @xmath117 first , label the full set of possible series @xmath13 models as @xmath307 ( in an abitrary order ) , and denote the number of such models by @xmath308 so that @xmath309 indexes the set .",
    "now consider two series @xmath165 where @xmath313 the @xmath1step forecast covariance at time @xmath9 can be evaluated as @xmath314 this can be seen as follows .",
    "first , since @xmath315 we have @xmath316 and thus @xmath317 then @xmath318\\ p(\\mathcal{m}_h=\\mu|\\cd_{t-1 } ) } \\\\ & = \\sum_{\\mu=1:m_h } { [ \\ e(y_{ht}y_{jt}|\\cd_{t-1 } , \\mathcal{m}_h=\\mu)-e(y_{ht}|\\cd_{t-1},\\mathcal{m}_h=\\mu ) e(y_{jt}|\\cd_{t-1 } , \\mathcal{m}_h=\\mu)\\ ] \\",
    "p(\\mathcal{m}_h=\\mu|\\cd_{t-1})}\\\\ & = \\sum_{\\mu=1:m_h}{\\cov(y_{ht},y_{jt}|\\cd_{t-1 } , \\mathcal{m}_h=\\mu)\\ p(\\mathcal{m}_h=\\mu|\\cd_{t-1})},\\end{aligned}\\ ] ] as stated .",
    "for each @xmath321 compute the univariate mean and variance of @xmath322 under each @xmath323 namely @xmath324 using the implied simplified forms of   with all terms in @xmath267 set to zero . then calculate the marginal predictive mean and variance of @xmath325 using  .",
    "insert @xmath268 as the @xmath91th element of @xmath82 and @xmath269 as the @xmath270element of @xmath271 .",
    "visit each model @xmath307 in @xmath326 .",
    "make explicit in the notation that the parental set for series @xmath13 generally depends on the model by writing @xmath327 here .",
    "then , compute the following quantities in parallel : @xmath328 \\\\ & = \\x_{j\\mu t}'\\a_{j\\mu\\phi t } + \\f_{pa(j|\\mu),t}'\\a_{j\\mu\\gamma t } , \\\\",
    "q_{j\\mu t } & = ( s_{j\\mu , t-1 } + u_{j\\mu t } )   r_{j\\mu t}/(r_{j\\mu t}-2 ) + \\a_{j\\mu\\gamma t}'\\q_{pa(j|\\mu),t}\\a_{j\\mu\\gamma t } , \\end{split}\\end{aligned}\\ ] ] where @xmath329 these two moments are obtained from the conditional distribution given by  .",
    "then calculate @xmath277 and @xmath278 according to  , and insert @xmath277 as the @xmath146th element of @xmath82 and @xmath278 as the @xmath281element of @xmath282 respectively .",
    "write @xmath330 for the @xmath261vector that extends @xmath331 with zeros for elements @xmath332 then @xmath333 then calculate @xmath334 according to  , and insert element @xmath288 of this vector as the @xmath289 and @xmath290 entries of @xmath282 for @xmath291      at this point , @xmath292 and we have filled in all elements of the @xmath91vector @xmath82 , @xmath293 matrix @xmath271 .",
    "the precision matrix can be calculated recursively in a similar fashion , paralleling section  [ sec : precmat ] .",
    "the @xmath123-step - ahead predictive mean and variance matrix at any time @xmath9 can not be directly evaluated analytically unless @xmath167 ( above ) .",
    "hence we utilize direct and straightforward monte carlo simulations .",
    "specify a monte carlo sample size @xmath205 and proceed with the following steps .",
    "this generates random samples from the full predictive @xmath335 , i.e. , giving synthetic future trajectories over all time points @xmath336 for @xmath337 monte carlo averaging at each time point then provides approximations to predictive means and variance matrices , and any other quantities of interest .",
    "these can be tuned for accuracy by simply increasing @xmath205 as the simulations are both parallelizable and computationally cheap per sample .    in the details below , we again extend the parental set notation so that , for each series @xmath13 and any specific model @xmath307 in @xmath326 , the parental set is now denoted by @xmath338 notice that when @xmath339 we have @xmath340 for all @xmath341    for each series @xmath342 in turn , we simulate a monte carlo sample of size @xmath205 of values of @xmath343 over @xmath344 . at each series index",
    "@xmath345 the values of any required parental predictors @xmath346 for the generated model @xmath307 will have been simulated at previous step @xmath347 and so are available as conditional predictors for series @xmath60 the process is as follows .        1 .   for the current series index @xmath113 sample the discrete posterior @xmath349 over models",
    "@xmath350 to generate a sample of @xmath205 models @xmath351 , @xmath352 * for @xmath353step ahead , generate samples @xmath354 by simulating from the @xmath1step ahead t distribution @xmath355 ; this is just at @xmath167 .",
    "each sample @xmath356 is generated from this conditional based on the @xmath357th sample value of the parental set .",
    "* for each of @xmath358steps ahead in sequence , repeat the above computation to generate sample values @xmath359 ; at step @xmath360 this again simulates t distributions as in at @xmath361 .",
    "each sample @xmath356 is based on such a t distribution whose parameters involve the recently simulated values of all needed predictors to evaluate both the parental vector @xmath362 and the vector @xmath363 if it includes lagged values of any of the series .",
    "2 .   based on these monte carlo samples",
    "now running over series @xmath364 compute series @xmath13 moments and update the saved monte carlo information , as follows .",
    "+ for each step ahead",
    "@xmath365 in this order : * compute the sample mean and variance of @xmath366 as the monte carlo approximations to the predictive moments of @xmath367 i.e. , the values @xmath368 and @xmath369 insert @xmath368 as the @xmath13-th element of @xmath370 and @xmath371 as the @xmath372 element of @xmath373 * compute the sample covariance vector of @xmath374 as the monte carlo approximation to predictive covariance vector @xmath375 insert element @xmath376 of this vector as the @xmath377 and @xmath378 entries of @xmath379 for @xmath380 3 .   if @xmath381 move to the next series @xmath382 and repeat ; otherwise , stop and save the complete set of monte carlo samples , if desired , as well as the complete predictive mean vectors @xmath370 and variance matrices @xmath383 over @xmath337"
  ],
  "abstract_text": [
    "<S> we discuss bayesian forecasting of increasingly high - dimensional time series , a key area of application of stochastic dynamic models in the financial industry and allied areas of business . </S>",
    "<S> novel state - space models characterizing sparse patterns of dependence among multiple time series extend existing multivariate volatility models to enable scaling to higher numbers of individual time series . </S>",
    "<S> the theory of these _ dynamic dependence network _ models shows how the individual series can be _ decoupled _ for sequential analysis , and then _ recoupled _ for applied forecasting and decision analysis . </S>",
    "<S> decoupling allows fast , efficient analysis of each of the series in individual univariate models that are linked for later recoupling through a theoretical multivariate volatility structure defined by a sparse underlying graphical model . </S>",
    "<S> computational advances are especially significant in connection with model uncertainty about the sparsity patterns among series that define this graphical model ; bayesian model averaging using discounting of historical information builds substantially on this computational advance . </S>",
    "<S> an extensive , detailed case study showcases the use of these models , and the improvements in forecasting and financial portfolio investment decisions that are achievable . using a long series of daily international currency , stock indices and commodity prices , the case study includes evaluations of multi - day forecasts and bayesian portfolio analysis with a variety of practical utility functions , as well as comparisons against commodity trading advisor benchmarks .    _ </S>",
    "<S> keywords : _ bayesian forecasting ; discount model averaging ; dynamic graphical model ; graphical model uncertainty ; multiregression dynamic model ; portfolio optimization ; sparse dynamics </S>"
  ]
}