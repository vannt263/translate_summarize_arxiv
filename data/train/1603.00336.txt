{
  "article_text": [
    "this paper is concerned with the numerical solution of linear equations of the form @xmath0 where the operator @xmath1 and right - hand side @xmath2 depend on a parameter @xmath3 which takes values in some parameter set @xmath4 .",
    "such equations arise in many contexts such as uncertainty quantification , optimization or control , where the solution of have to be evaluated with many instances of the parameters ( multi - query context ) . for large systems of equations ( e.g. arising from a fine discretization of a parameter - dependent partial differential equation ) , solving for one instance of the parameter can be very expensive , which leads to intractable computations in a multi - query context .",
    "model order reduction methods aim at constructing an approximation of the solution map @xmath5 whose evaluation for a certain value of @xmath3 is cheaper than solving .",
    "standard approaches rely on galerkin - type projections of @xmath6 on a low - dimensional subspace @xmath7 of the solution space @xmath8 , a so - called _",
    "reduced space_. the reduced space can be generated from evaluations ( snapshots ) of the solution @xmath6 at some selected ( or randomly chosen ) values of the parameter @xmath3 , see @xcite .",
    "the proper orthogonal decomposition method aims at constructing an optimal subspace for the approximation of the set of solutions @xmath9 in a mean - square sense ( see @xcite ) .",
    "reduced basis ( rb ) methods ( see @xcite for a survey ) aim at controlling the approximation uniformly over the parameter set . in this context , reduced spaces are usually constructed using greedy algorithms .    in many applications",
    "one is not interested in the solution @xmath6 itself , but only in a variable of interest @xmath10 which is a functional of @xmath6 . here",
    "we assume that @xmath10 depends linearly on @xmath6 .",
    "efficient goal - oriented methods have been proposed for the estimation of a scalar - valued variable of interest @xmath10 .",
    "a standard method consists in computing an approximation of the solution of the so - called dual problem associated to which is used to correct the estimation of @xmath10 .",
    "we refer to @xcite for a general survey on primal - dual methods and to @xcite for the application in the context of rb methods .   +   in this paper , we propose projection based model order reduction methods for the estimation of a variable of interest @xmath10 taking values in a vector space of finite or infinite dimension .",
    "we consider the case where @xmath11 with @xmath12 a parameter - dependent linear operator .",
    "for example , for boundary value problems , @xmath12 can be defined as the trace operator providing the restriction of the solution to the boundary of the domain . in this case",
    "the variable of interest belongs to an infinite dimensional space or , after discretization , to a finite but possibly high dimensional space .",
    "the standard approach , which consists in treating @xmath10 as a collection of scalar - valued variables of interest and in building one reduced dual space for each of them , has a complexity which grows proportionally to the dimension of @xmath10 .",
    "our approach circumvents this issue by constructing a single reduced dual space , thus allowing to handle variables of interest with high and potentially infinite dimension .",
    "a similar approach can be found for parametric dynamical systems , see the monograph @xcite for a general introduction . in this framework ,",
    "projection - based model order reduction methods are used for the approximation of @xmath10 which is an output of the dynamical system .",
    "petrov - galerkin methods have been proposed with different ways of constructing the reduced basis for the test and trial space , such as the balanced truncation methods , ( balanced ) proper orthogonal decomposition method , moment matching methods , etc .",
    "we refer to @xcite for a recent review on these methods . in the present paper",
    ", we aim at exploring other possibilities than the petrov - galerkin projection .",
    "+ in a first part , we introduce and analyze different methods for computing projections of the solution and approximations of the variable of interest .",
    "we first present a non goal - oriented petrov - galerkin approach to compute an approximation of @xmath6 from which an estimation of @xmath10 is deduced .",
    "then , we introduce a generalization of the standard primal - dual method to the case of a vector - valued variable of interest , which relies on the approximation of the primal variable @xmath6 and of the solution @xmath13 of the dual problem @xmath14 where @xmath15 and @xmath16 are the adjoints of operators @xmath1 and @xmath12 respectively .",
    "we show that the error on the variable of interest depends on three reduced spaces : the approximation space @xmath7 for the primal variable @xmath6 , the test space @xmath17 which is used for the petrov - galerkin projection of @xmath6 , and an approximation space @xmath18 for the dual variable @xmath13 which is projected on the space of @xmath18-valued linear operators .",
    "finally , we present a petrov - galerkin method where the projection is obtained by solving a saddle point problem which involves an approximation space @xmath7 for @xmath6 and an approximation space @xmath19 for an auxiliary variable .",
    "we show that if @xmath19 is defined by @xmath20 , then error bounds for both the projection of the primal variable on @xmath7 and the approximation of the variable of interest can be improved compared to error bounds of a primal - dual approach using the same spaces @xmath7 , @xmath17 and @xmath18 .",
    "the proposed approach is a goal - oriented extension of the method proposed in @xcite .    in a second part , we derive ( for both approaches ) computable error estimates for the approximation of the variable of interest .",
    "then , we propose greedy algorithms based on these error estimates for the construction of the reduced spaces @xmath7 and @xmath21 .",
    "we discuss different choices for the reduced space @xmath17 .",
    "in particular , we introduce a parameter - dependent space depending on a preconditioner obtained by means of an interpolation of the inverse of the operator @xmath1 proposed in @xcite .",
    "+    this paper is organized as follows . in section [ sec : analysis ] , we introduce and analyze the different projection methods for the estimation of vector - valued variables of interest for general linear equations of the form formulated in a hilbert setting .",
    "then , in section [ sec : application2mor ] , we derive error estimates for the approximation of the variable of interest and we propose practical greedy algorithms for the construction of reduced spaces .",
    "finally , in section [ sec : numerical ] , numerical experiments illustrate the properties of the projection methods and of the greedy algorithms . in particular , we provide a simplified complexity analysis for the so - called _ offline _ phase ( _ i.e. _ the construction of the reduced spaces ) and for the _ online _ phase ( _ i.e. _ the evaluation of @xmath10 for a particular instance of @xmath3 ) .",
    "let @xmath8 , @xmath22 and @xmath23 be three hilbert spaces . for a hilbert space @xmath24 equipped with a norm @xmath25 ,",
    "we denote by @xmath26 the topological dual space of @xmath24 .",
    "we consider the linear equation @xmath27 with @xmath28 and @xmath29 , and a variable of interest @xmath30 where @xmath31 .",
    "we assume that @xmath32 is a norm - isomorphism is a norm - isomorphism if it is a continuous and weakly coercive operator satisfying the assumptions of the neas theorem ( * ? ? ?",
    "* chapter 2 ) . ] such that for all @xmath33 , @xmath34 where    [ infsup ] @xmath35    which ensures the well - posedness of . in this section ,",
    "we present different methods for constructing an approximation @xmath36 of @xmath37 .",
    "first , in section [ sec : pg ] , we present a standard approach which consists in estimating the variable of interest from a petrov - galerkin projection of @xmath38 . in section [ sec : pd ] , we present an extension of the primal - dual approach to the case of vector - valued variables of interest , where the variable of interest is estimated from a standard petrov - galerkin projection of the primal variable and a projection of the solution of a dual problem .",
    "finally , in section [ sec : saddlepoint ] , we introduce a goal - oriented projection method based on a saddle - point formulation .    before going further ,",
    "let us introduce some additional notations .",
    "for a hilbert space @xmath24 , we denote by @xmath39 the riesz map such that @xmath40 , where @xmath41 denotes the duality pairing .",
    "the dual norm @xmath42 on @xmath26 is such that @xmath43 .",
    "then @xmath44 and @xmath45 hold for any @xmath46 and @xmath47 .",
    "for any operator @xmath48 , with @xmath49 and @xmath50 two hilbert spaces , @xmath51 denotes the adjoint of @xmath52 , such that @xmath53 for any @xmath54 and @xmath55 .",
    "suppose that we are given a subspace @xmath56 of finite dimension @xmath57 in which we seek an approximation of @xmath38 .",
    "the orthogonal projection @xmath58 of @xmath38 on @xmath7 , given by @xmath59 , is characterized by @xmath60 in practice , an approximation @xmath61 can be defined as a petrov - galerkin projection of @xmath38 characterized by @xmath62 where @xmath63 is a test space of dimension @xmath57 . under the assumption that @xmath64 the next proposition provides a quasi - optimality result for @xmath65 and gives an error bound for the approximation of the variable of interest . in",
    "what follows , notation @xmath66 ( resp .",
    "@xmath67 ) is used in place of @xmath68 ( resp .",
    "@xmath69 ) when the minimum ( resp .",
    "the maximum ) is reached .",
    "[ prop:1 ] under assumption , the solution @xmath65 of equation satisfies @xmath70 where @xmath71 furthermore , @xmath72 with @xmath73    with @xmath58 the orthogonal projection of @xmath38 on @xmath7 , for any @xmath74 and @xmath75 , we have @xmath76 taking the minimum over @xmath77 , dividing by @xmath78 and taking the maximum over @xmath79 , we obtain @xmath80 , where @xmath81 is defined by .",
    "thanks to the orthogonality condition we have @xmath82 , from which we deduce that @xmath83 . to prove",
    ", it remains to prove that @xmath84 .",
    "noting that @xmath85 we obtain @xmath86 let introduce @xmath87 which , from assumption , satifies @xmath88",
    ". then using assumption we obtain @xmath89 furthermore for any @xmath90 and @xmath75 , we have @xmath91 taking the infimum over @xmath75 , dividing by @xmath92 and taking the supremum over @xmath93 , we obtain thanks to .    the error bound for the approximation of the variable of interest @xmath37 is the product of three terms :    * @xmath94 , which suggests that the approximation space @xmath7 should be defined such that @xmath38 can be well approximated in @xmath7 , * @xmath95 , which suggests that the test space @xmath17 should be chosen such that any element of @xmath7 can be well approximated by an element of @xmath96 , and * @xmath97 , which suggests that any element of @xmath98 should be well approximated by an element of @xmath99 .    as already noticed in ( * ? ? ?",
    "* section 11.1 ) , @xmath17 plays a double role : a test space for the definition of @xmath65 ( point ( b ) ) and an approximation space for the range of @xmath100 ( point ( c ) ) .",
    "the proposed petrov - galerkin projection method coincides with the interpolatory projection method used in the context of parametric dynamical systems ( see @xcite ) .",
    "our analysis provides quasi - optimality results on @xmath10 for any parameter value @xmath3 .",
    "also , the condition @xmath101 ensures the invertibility of the reduced operator @xmath102 defined by @xmath103 for all @xmath104 and @xmath105 . in @xcite ,",
    "the invertibility of @xmath106 is not discussed in the time - independent case .    under assumption ,",
    "the classical ca s lemma states that @xmath107 it has been shown in @xcite that this can be improved to @xmath108 noting that equation yields @xmath109 we observe that provides a sharper bound than in , where the constants differ by a factor @xmath110 .",
    "[ rmk : spd_1 ] we suppose that @xmath32 is a symmetric coercive operator , with @xmath111 and @xmath112 the norm induced by the operator @xmath32 such that @xmath113 . then @xmath81 defined by admits the following simple expression @xmath114 if the test space @xmath17 is defined by @xmath115 , we obtain @xmath116 , and from , we obtain @xmath117 . in other words , the standard galerkin projection coincides with the orthogonal projection .    in the case where the variable of interest @xmath37 is scalar - valued , we have @xmath118 and @xmath119 .",
    "the so - called _ compliant case _ corresponds to @xmath120 for any @xmath121 .",
    "then , by definition , we have @xmath122 and thanks to , we recover the so - called `` squared effect '' @xmath123      we now extend the classical primal - dual approach @xcite for the estimation of a vector - valued variable of interest",
    ". + let us introduce the dual variable @xmath124 defined by @xmath125 .",
    "the relation @xmath126 shows that the variable of interest can be exactly determined if either the primal variable @xmath38 or the dual variable @xmath127 is known .",
    "+ now , for given approximations @xmath128 of @xmath38 and @xmath129 of @xmath127 , we define the approximation @xmath130 of @xmath37 by @xmath131 where @xmath132 is the standard estimation of the variable of interest and where @xmath133 is a correction using the approximation of the dual variable .",
    "the following proposition provides an error bound on the variable of interest , which is a generalization of the classical error bound for scalar - valued variables of interest ( see @xcite ) to vector - valued variables of interest .",
    "+    [ prop : classicalbounds ] the approximation @xmath130 of @xmath37 defined by satisfies @xmath134 where @xmath135    for any @xmath136 , we have @xmath137 dividing by @xmath92 and taking the supremum over @xmath138 , we obtain .",
    "+ in practice , the approximation @xmath139 can be defined as the petrov - galerkin projection @xmath65 of @xmath38 on a given approximation space @xmath7 with a given test space @xmath17 , see equation . for the approximation @xmath129 of @xmath140",
    ", the bound suggests that @xmath141 should be small .",
    "we then propose to choose @xmath129 as a solution of @xmath142 where @xmath143 is a given approximation space ( different from @xmath17 ) .",
    "the next proposition shows how to construct a solution of .",
    "+    [ def : qk ] the operator @xmath144 defined for @xmath136 by @xmath145 is linear and is a solution of .",
    "moreover @xmath146 is characterized by @xmath147    we easily prove that the optimization problem admits a unique solution which depends linearly and continuously on @xmath148 , so that @xmath149 defined by is a linear operator in @xmath150 .",
    "equation is the euler equation associated to the minimization problem .",
    "furthermore for any @xmath151 and @xmath138 , we have @xmath152 taking the supremum over @xmath153 and then the infimum over @xmath151 , we obtain that @xmath154 , which means that @xmath155 is a solution of .",
    "+ in practice , for computing the approximation of the variable of interest with @xmath156 , we only need to compute @xmath157 .",
    "the following lemma shows how this can be performed without computing the operator @xmath149 .",
    "+    [ lem : compute_qtr ] let @xmath149 be defined by .",
    "then for @xmath158 , @xmath159 where @xmath160 is defined by @xmath161    for any @xmath136 , since @xmath162 , we have @xmath163 furthermore , by definition of @xmath149 we have @xmath164 combining and , we obtain @xmath165 for all @xmath136 , which concludes the proof .     + we give now a new bound of the error on the variable of interest .",
    "[ prop : newbounds ] let @xmath166 be the petrov - galerkin projection defined by and let @xmath156 be defined by .",
    "then the approximation @xmath36 defined by satisfies @xmath167 where @xmath168 moreover , @xmath169    for any @xmath136 , and for any @xmath170 we have @xmath171 from ,",
    "we have @xmath172 dividing by @xmath173 and taking the supremum over @xmath138 in , we obtain @xmath174 then , taking the minimum over @xmath170 , we obtain .",
    "finally , taking @xmath175 in , we obtain from .",
    "observing that @xmath176 and @xmath177 , proposition [ prop : newbounds ] provides a sharper bound of the error on the variable of interest by taking advantage of the orthogonality property .      in this section",
    "we extend the method proposed in @xcite for the approximation of ( vector - valued ) variables of interest .",
    "the idea is to define the projection of @xmath38 on the reduced space @xmath7 by means of a saddle point problem .",
    "we first define and analyze this saddle point problem .",
    "then we use the solution of this problem for the estimation of the variable of interest .",
    "\" let us equip @xmath22 with a norm @xmath178 such that the relation @xmath179 holds for any @xmath180 , which is equivalent to the following relation between the riesz maps @xmath181 and @xmath182 : @xmath183 the orthogonal projection @xmath58 of @xmath38 on @xmath7 satisfies @xmath184 starting from this observation , we introduce a subspace @xmath185 of dimension @xmath186 and we define the projection @xmath187 in @xmath7 as the solution of the saddle point problem @xmath188 in the following proposition , we prove the well - posedness of under the condition ( discrete inf - sup condition ) @xmath189 and we provide a practical characterization of @xmath187 .    [ prop : saddle_solution ] under assumption , there exists a unique solution @xmath190 in @xmath191 to    [ eq : pgd ] @xmath192    and @xmath193 is the unique solution of .    since the riesz map @xmath181 defined by is coercive and under the discrete inf - sup condition on operator @xmath32 , theorem 2.34 of @xcite gives that is a well - posed problem whose solution @xmath190 is the unique solution of the saddle - point problem @xmath194 denoting @xmath195 with @xmath196 , this saddle point problem is equivalent to @xmath197 which coincides with problem .     + the following proposition provides a quasi - optimality result for the projection @xmath198 of @xmath38 onto @xmath7 .    [ prop : bounds_saddle_point ] under assumption , the solution @xmath187 of is such that @xmath199 where @xmath200 is such that @xmath201    let @xmath202 be the solution of . for any @xmath104 and @xmath203",
    ", we have @xmath204 equation implies that @xmath205 so that @xmath206 . using , it follows @xmath207 using in , taking the infimum over @xmath203 , dividing by @xmath78 and taking the supremum over @xmath208 , we obtain @xmath209 from ( with @xmath17 replaced by @xmath19 ) and ( which implies @xmath210 ) , we obtain",
    ". then , from the definition of @xmath58 , we have @xmath211 from which we deduce .",
    "+ from the definition of @xmath212 , we easily deduce the following corollary .",
    "[ cor : idealtestspace ]  if  @xmath19 is   such that   @xmath213 ( or equivalently @xmath214 ) then @xmath215 and @xmath187 coincides with the best approximation @xmath58 of @xmath38 in @xmath7 .",
    "[ rmk : weakcoercive ] note that and give @xmath216 which is sharper than the classical error bound obtained by the cea s lemma @xmath217    now , we consider the approximation @xmath36 of @xmath37 defined by @xmath218 where @xmath219 is the solution of the saddle point problem .",
    "the following proposition provides an error bound for the approximation of the variable of interest .",
    "+    [ prop : bounds_qoi_saddle_point ] the approximation @xmath36 defined by satisfies @xmath220 with @xmath221 and @xmath222    for any @xmath136 and @xmath203 , we have @xmath223 taking the minimum over @xmath203 , dividing by @xmath92 and taking the supremum over @xmath153 , we obtain .",
    "finally , thanks to , and , we obtain .",
    "we observe that @xmath19 impacts both the quality of the projection of @xmath38 ( via the constant @xmath224 in ) and the quality of the approximation of the variable of interest ( via constants @xmath224 and @xmath225 in )",
    ". then , we will consider for @xmath19 spaces of the form @xmath226 with @xmath227 .",
    "this implies @xmath228 so that the error bound for the variable of interest is better than the error bound of the primal - dual method with primal approximation space @xmath7 , primal test space @xmath229 and dual approximation space @xmath230 .",
    "therefore , we expect the approximation @xmath187 to be closer to the solution @xmath38 than the petrov - galerkin projection @xmath65 . also , the approximation of the quantity of interest is expected to be improved .",
    "[ rmk : spd_2 ] let us consider the case where @xmath32 is symmetric and coercive , @xmath231 and @xmath115 .",
    "the choice implies that @xmath232 , so that @xmath19 admits the orthogonal decomposition @xmath233 .",
    "equation implies that @xmath234 .",
    "let @xmath235 . equation gives @xmath236 for all @xmath203 , which implies that @xmath237 is the orthogonal projection of @xmath38 on @xmath19 , where @xmath187 and @xmath238 are the orthogonal projections of @xmath38 on @xmath7 and @xmath239 respectively .",
    "furthermore , the approximation of the variable of interest is given by @xmath240 .",
    "we conclude that in this particular setting , the saddle point approach can be simply interpreted as an orthogonal projection of @xmath38 on the enriched space @xmath241 , followed by a standard estimation of the variable of interest .",
    "we now consider a parameter - dependent equation @xmath242 where @xmath3 denotes a parameter taking values in a set @xmath243 , @xmath244 and @xmath245 .",
    "the variable of interest is defined by @xmath246 , with @xmath247 .",
    "+ in section [ sec : analysis ] , we have presented different projection methods for the estimation of the variable of interest which rely on the introduction of three spaces : the primal approximation space @xmath7 , the primal test space @xmath17 and the dual approximation space @xmath18 .",
    "we recall that for the saddle point approach , we introduce the space @xmath248 .",
    "we adopt an _ offline / online strategy_. reduced ( low - dimensional ) spaces @xmath7 , @xmath17 and @xmath18 are constructed during the _ offline phase_. then , the projections on these reduced spaces and the evaluations of the variable of interest are rapidly computed for any parameter value @xmath249 during the _ online phase_. + in section [ sec : constructionwr ] , we will first consider the construction of the test space @xmath17",
    ". for scalar - valued variables of interest , reduced spaces @xmath7 and @xmath18 are classically defined as the span of snapshots of the primal and dual solutions @xmath6 and @xmath13 .",
    "these snapshots can be selected at random , using samples drawn according a certain probability measure over @xmath4 , see _",
    "e.g. _ @xcite .",
    "another popular method is to select the snapshots in a greedy way @xcite , with a uniform control of the error @xmath250 over @xmath4 .",
    "this method requires an estimation of the error on the variable of interest . in the same lines , we introduce error estimates for vector - valued variables of interest in section [ sec : errorestimate ] , and we propose greedy algorithms for the construction of @xmath7 and @xmath18 in section [ sec : offline ] .",
    "assuming that the primal approximation space @xmath7 is given , we know from the previous section that @xmath17 should be chosen such that @xmath81 is as close to zero as possible ( see propositions [ prop:1 ] , [ prop : newbounds ] , [ prop : bounds_saddle_point ] and [ prop : bounds_qoi_saddle_point ] ) . in the literature , @xmath115 is a common choice ( standard galerkin projection ) .",
    "when the operator @xmath1 is symmetric and coercive , we can choose @xmath115 which is the optimal test space with respect to the norm induced by @xmath1 ( see remark [ rmk : spd_1 ] ) .",
    "however , this choice may lead to an inaccurate projection of the primal variable when the operator is ill - conditioned ( i.e. @xmath251 ) . in the case of non coercive operators ,",
    "a parameter - dependent test space is generally defined by @xmath252 , where @xmath253 is called the `` supremizer operator '' ( see e.g. @xcite ) .",
    "this approach is no more than a minimal residual method since the resulting petrov - galerkin projection defined by is @xmath254 . in section [ sec : pg ]",
    ", we have seen that the petrov - galerkin projection with an ideal test space @xmath255 coincides with the best approximation . having a basis @xmath256 of @xmath7 , the computation of this ideal parameter - dependent test space would require the computation of @xmath257 for all @xmath258 for each parameter s value @xmath3 , which is unfeasible in practice . up to our knowledge , the only attempt to construct quasi - optimal test spaces for non symmetric and",
    "weakly coercive operators can be found in @xcite , where the authors proposed a greedy algorithm for the construction of a ( parameter independent ) test space which ensures the quasi - optimality constant to be uniformly bounded by an arbitrarily small constant . here , we adopt an alternative approach where the ( parameter - dependent ) test space is defined by @xmath259 where @xmath260 is an interpolation of the inverse of @xmath1 using @xmath261 interpolation points in the parameter set @xmath4 . in practice , when @xmath1 is a matrix , algorithms developed in @xcite can be used .",
    "this will be detailed later on .",
    "the underlying idea is to obtain a test space as close as possible to the ideal test space @xmath262 defined in . for @xmath263 , with @xmath264 by convention , we have @xmath115 , which yields the standard galerkin projection .      in this section ,",
    "we propose practical error estimates for the variable of interest , first for the primal - dual approach and then for the saddle point method .      given approximations @xmath139 and @xmath129 of the primal solution @xmath38 and the dual solution @xmath127 respectively , a standard approach is to start from the error bound @xmath265 which is provided by proposition [ prop : classicalbounds ] .",
    "this suggests to measure the norm of the residuals associated to the primal and dual variables . in practice , we distinguish two cases . + in the case where the operator @xmath1 is symmetric and coercive ,",
    "it is natural to choose the parameter - dependent norm @xmath266 as the one induced by the operator , i.e. @xmath267 . however , neither the primal error @xmath268 nor the dual residual norm @xmath269 can be computed without computing the primal and dual solutions @xmath6 and @xmath13 .",
    "the classical way to circumvent this issue is to introduce a parameter - independent norm @xmath270 , which is in general the `` natural '' norm associated to the space @xmath8 , and to measure residuals with the associated dual norm @xmath271 .",
    "here we assume that the operator @xmath1 satisfies @xmath272 for all @xmath121 , where @xmath273 .",
    "by definition of the norm @xmath274 , we can write @xmath275 then we have @xmath276 . in the same way",
    ", we can prove that @xmath277 .",
    "finally , we obtain @xmath278 where @xmath279 is a certified error bound for the variable of interest , which involves computable primal and dual residual norms .",
    "+ in the general case , we consider for @xmath280 the natural norm on @xmath8 , i.e. @xmath281 . as a consequence , the norm of the dual residual is computable , but the computation of the error @xmath282 requires the primal solution @xmath6 which is not available in practice .",
    "once again , we assume that the operator satisfies the property so that we can write @xmath283",
    ". then we end up with the same error bound for the variable of interest .",
    "we now derive new error bounds in the case where the approximation @xmath284 is obtained by the saddle point method introduced in section [ sec : saddlepoint ] .",
    "let us start from the error bound @xmath285 provided by proposition [ prop : bounds_qoi_saddle_point ] .",
    "once again , we distinguish two cases . + for the case where the operator @xmath1 is symmetric and coercive , we consider for @xmath266 the norm induced by the operator , i.e. @xmath113 .",
    "according to remark [ rmk : spd_2 ] , the quantity @xmath286 is nothing but the orthogonal projection of @xmath6 onto @xmath20 , with @xmath115 .",
    "then for any @xmath287 we have @xmath288 where the norm @xmath270 is the natural norm on @xmath8 such that holds .",
    "then , taking the infimum over @xmath287 we obtain @xmath289 finally , we obtain that @xmath290 note that the main difference between this error estimate and the previous one is the minimization problem over @xmath19 in both primal and dual residuals .",
    "the solution of those minimization problems lead to additional computational costs , but sharper error bounds will be obtained , as illustrated by the numerical examples in the next section . + for the general case , we consider @xmath291 .",
    "starting from and using the relation to bound the primal error by the primal residual norm , we obtain the following error estimate @xmath292 where @xmath293 .",
    "+    all the proposed error estimates rely on the knowledge of @xmath294 . in the case",
    "where @xmath294 can not be easily computed , we can replace it by a lower bound @xmath295 , e.g. provided by a scm procedure @xcite .",
    "this option will not be considered here .",
    "another option is to remove @xmath294 from the definitions of @xmath279 , therefore leading to error estimates which are no more certified error bounds .      here , we propose different greedy algorithms for the construction of the reduced spaces @xmath7 and @xmath18 . at each iteration",
    ", we search for a parameter value @xmath296 where the error estimate @xmath279 is maximum , i.e. @xmath297 a first strategy is to simultaneously enrich both the primal approximation space @xmath298 and the dual approximation space @xmath299 at each iteration .",
    "this strategy is referred as the _ simultaneous construction _ , as opposed to the _ alternate construction _ which consists in enriching @xmath18 ( resp .",
    "@xmath7 ) if @xmath7 ( resp .",
    "@xmath18 ) were enriched at the previous greedy iteration step .",
    "in the literature , and for scalar - valued variables of interest , the classical approaches are either a _ separated construction _ of @xmath7 and @xmath300 ( using two independent greedy algorithms , see for e.g. @xcite ) , or a _ simultaneous construction _ ( see _ e.g. _ @xcite ) .",
    "the latter option can take advantage of a single factorization of the operator @xmath301 to compute both the primal and dual variables . the _",
    "alternate construction _ proposed here is not usual .",
    "this possibility is mentioned in remark 2.47 of the tutorial @xcite .",
    "for vector - valued variables of interest ( @xmath302 ) , the enrichment strategy makes sense only if @xmath303 , in which case @xmath304 .",
    "however , if @xmath305 is finite but very high , the enrichment strategy may lead to a rapid increase of the dimension of the dual approximation space .",
    "therefore , when @xmath306 is infinite or very high , we propose to replace the enrichment strategy by @xmath307 where the space @xmath230 is enriched with a single vector @xmath308 , with @xmath136 such that @xmath309 contrarily to the full enrichment , this partial enrichment does not necessarily lead to a zero error at the point @xmath310 for the next iterations .",
    "then we expect that will deteriorate the convergence properties of the algorithm , but for @xmath311 , the space @xmath312 defined by will have a much lower dimension than the space @xmath313 defined by .",
    "it is worth mentioning that in @xcite , the authors propose a similar partial enrichment strategy for the test space @xmath19 but not in a goal - oriented framework .",
    "the definition of the test space @xmath17 requires the definition of a preconditioner @xmath260 which is here constructed by interpolation of the inverse of @xmath1 .",
    "following the idea of @xcite , the interpolation points for the preconditioner are chosen as the points where solutions ( primal and dual ) have already been computed , i.e. the points given by .",
    "the resulting algorithms are summarized in algorithm [ alg : simultaneous ] and algorithm [ alg : alternated ] respectively for the simultaneous and the alternate constructions of @xmath7 and @xmath18 .",
    "error estimator @xmath314 , a samples set @xmath4 , maximum iteration @xmath315 initialize @xmath316 and the spaces @xmath317 and @xmath318 find @xmath319 compute a factorization of @xmath320 and update the preconditioner if needed solve @xmath321 update @xmath322 , and @xmath323 solve @xmath324 update @xmath325 , and @xmath326 find @xmath148 according to or solve @xmath327 update @xmath328 , and @xmath329    error estimator @xmath314 , a samples set @xmath4 , maximum iteration @xmath315 initialize @xmath316 and the spaces @xmath317 and @xmath318 find @xmath319 compute a factorization of @xmath320 and update the preconditioner if needed solve @xmath321 update @xmath322 , and @xmath323 solve @xmath324 update @xmath325 , and @xmath326 find @xmath148 according to or solve @xmath327 update @xmath328 , and @xmath329",
    "in this section , we present numerical applications of the methods proposed in sections [ sec : analysis ] and [ sec : application2mor ] .",
    "we first describe the applications in section [ applis ] .",
    "then we compare the projection methods for the estimation of a variable of interest in section [ compproj ] . finally , we study the behavior of the proposed greedy algorithms for the construction of the reduced spaces in section [ appligreedy ] .        we consider a linear elasticity problem @xmath330 over a domain @xmath331 ( represented in figure [ fig : pont_settings_a ] ) , where @xmath332 is the displacement field and @xmath333 is the strain tensor associated to the displacement field @xmath334 .",
    "the hooke tensor @xmath335 is such that @xmath336 where @xmath337 is the poisson coefficient and @xmath338 is the young modulus defined by @xmath339 , @xmath340 being the indicator function of the subdomain @xmath341 , see figure [ fig : pont_settings_b ] .",
    "the components of @xmath342 are independent and log - uniformly distributed over @xmath343 $ ] .",
    "we impose homogeneous dirichlet boundary condition @xmath344 on @xmath345 ( red lines ) , a unit vertical surface load on @xmath346 ( green square ) , and a zero surface load on the complementary part of the boundary ( see figure [ fig : pont_settings_a ] ) .",
    "we consider the galerkin approximation @xmath347 of @xmath348 on a @xmath349 finite element approximation space @xmath350 of dimension @xmath351 associated to the mesh plotted in figure [ fig : pont_settings_b ] .",
    "the vector @xmath352 such that @xmath353 is the solution of the linear system @xmath354 of size @xmath355 , with @xmath356 and @xmath357 , where @xmath358 denotes the hooke tensor with the young modulus @xmath359 .",
    "the norm @xmath274 on the space @xmath8 is chosen such that @xmath360 , that means @xmath361 .",
    "we also consider the parameter - independent norm @xmath270 defined by @xmath362 with @xmath363 .",
    "it corresponds to the norm induced by the operator associated with the hooke tensor @xmath358 instead of @xmath335 .",
    "+ let us consider @xmath364 which is the vertical displacement of the galerkin approximation on the blue line @xmath365 , see figure [ fig : pont_settings_a ]",
    ". we can write @xmath366 where @xmath367 is a basis of the space @xmath368 of dimension @xmath369",
    ". then there exists @xmath370 such that @xmath371 where @xmath372 is the variable of interest .",
    "the norm @xmath373 is defined as the canonical norm of @xmath374 .",
    "+      we consider the benchmark problem of the cooling of electronic components proposed in the opus project .",
    "the equation to solve is an advection - diffusion equation over the domain @xmath375 @xmath376 whose solution @xmath377 is the temperature field . here",
    "@xmath378 and @xmath379 denote respectively the diffusion coefficient and the advection field , which are parameter - dependent coefficients of the operator .",
    "the full description of this problem is given in @xcite .",
    "here , we only focus on the resulting algebraic parameter - dependent equation coming from stabilized finite element discretization of , that is @xmath380 , where @xmath381 are the coefficients of the finite element approximation @xmath382 of @xmath383 , and where @xmath384 is a 4-dimensional random vector .",
    "the space @xmath385 with @xmath386 is endowed with the norm @xmath281 which corresponds to the @xmath387-norm for all @xmath388 , where @xmath389 . ] .",
    "the variable of interest @xmath390 is the mean temperature of both electronic components , with @xmath391 where @xmath392 ( @xmath393 ) are two subdomains of @xmath375 ( see ( * ? ? ? *",
    "fig.7 ) ) .",
    "then we can write @xmath394 for an appropriate @xmath370 , with @xmath395 . here",
    "we have @xmath396 , which we equip with the canonical norm on @xmath397 .",
    "the goal of this section is to compare the projection methods proposed in section [ sec : analysis ] for the estimation of @xmath10 . here",
    "the approximation spaces @xmath7 , @xmath18 and the test space @xmath17 are given .",
    "we denote by @xmath398 , @xmath399 and @xmath400 the matrices containing the basis vectors of the corresponding subspaces . in order to improve condition numbers of reduced systems of equations",
    ", these bases are orthogonalized using a gram - schmidt procedure .",
    "we first detail how we build @xmath398 , @xmath399 and @xmath400 .",
    "the matrix @xmath401 contains @xmath402 snapshots of the solution : @xmath403 .",
    "the test space is @xmath115 , which corresponds to a standard galerkin projection method .",
    "the matrix @xmath404 contains @xmath405 snapshots of the dual variable @xmath406 .",
    "then @xmath407 .",
    "finally , according to the matrix @xmath408 is the concatenation of the matrices @xmath409 and @xmath404 .",
    "we consider a samples set @xmath410 of size @xmath411 . for each @xmath412",
    "we compute the exact quantity of interest @xmath10 and the approximation @xmath284 by the following methods .",
    "* _ primal only _ : solve the linear system @xmath413 of size @xmath57 and compute @xmath414 . *",
    "_ dual only _ : solve the linear system @xmath415 of size @xmath416 and compute @xmath417 .. ] * _ primal - dual _ : solve the linear system of the _ primal only _ method , solve the linear system @xmath418 of size @xmath416 and compute @xmath419 . * _ saddle point _ : according to remark [ rmk : spd_2 ] , solve the linear system @xmath420 of size @xmath421 , and compute @xmath422 .",
    "the affine decomposition of matrix @xmath1 allows for a rapid solution of the reduced systems for any parameter @xmath3 .",
    "+ figure [ fig : pont_proj ] gives the probability density function ( pdf ) , the @xmath423 norm and @xmath424 norm of the error @xmath425 estimated over the samples set @xmath426 .",
    "we see that the _ primal - dual _ method provides errors for the quantity of interest which correspond to the product of the errors of the _ primal only _ and _ dual only _ methods .",
    "this reflects the `` squared effect '' .",
    "moreover the _ saddle point _",
    "method provides errors that are almost 10 times lower than the _ primal - dual _ method .",
    "this impressive improvement can be explained by the fact that the proposed problem is `` almost compliant '' , in the sense that the primal and dual solutions are similar : the primal solution is associated to a vertical force on the green square of figure [ fig : pont_settings_a ] , and the dual solution is associated to a vertical loading on @xmath365 . to illustrate this , let us consider a `` less compliant '' application where the variable of interest is defined as the horizontal displacement ( in the direction @xmath427 , see figure [ fig : pont_settings_a ] ) of the solution on the blue line @xmath365 , i.e. @xmath428 ( instead of @xmath429 ) .",
    "the results are given in figure [ fig : pont_proj_l2 ] .",
    "for this new setting , we can draw similar conclusions but the _ saddle point _",
    "method provides a solution which is `` only '' 2 times better ( instead of 10 times ) than the _ primal - dual _ method .",
    "now we consider the effectivity index @xmath430 associated to the primal - dual error estimate defined by and to the saddle - point error estimate defined by .",
    "for the considered application , the coercivity constant @xmath294 can be obtained by the _ min - theta _ method ( * ? ? ?",
    "* proposition 2.35 ) .",
    "figure [ fig : pont_proj_errorest ] presents statistical information on @xmath431 : the pdf , the mean , the max - min ratio and the normalized standard deviation estimated on a samples set of size @xmath432 .",
    "we first observe in figure [ fig : pont_proj_errorest_a ] that the effectivity index is always greater than @xmath433 : this illustrates the fact that the error estimates are certified .",
    "moreover , the error estimate of the saddle point method is much better than the one of the primal - dual method .",
    "the max - min ratio and the standard deviation of the corresponding effectivity index are much smaller and the mean value is much closer to one for the saddle point method .         for this second application , @xmath434 contains @xmath435 snapshots of the primal solution ( @xmath436 ) , and @xmath437 contains @xmath438 snapshots of the dual solution so that the dimension of @xmath18 is @xmath439 .",
    "the test space @xmath17 is defined according to , where @xmath260 is an interpolation of @xmath440 using @xmath261 interpolation points selected by a greedy procedure based on the residual @xmath441 ( where @xmath442 denotes the matrix frobenius norm ) , see @xcite .",
    "the interpolation is defined by a frobenius semi - norm projection ( with positivity constraint ) using a random matrix with @xmath443 columns .",
    "the matrix associated to the test space is given by @xmath444 .",
    "once again , we consider a samples set @xmath426 of size @xmath411 . for any @xmath412 we compute the exact quantity of interest @xmath10 and the approximation @xmath284 by the following methods .    * _ primal only _ : solve the linear system @xmath445 of size @xmath57 and compute @xmath446 . *",
    "_ dual only _ : solve the linear system @xmath447 of size @xmath416 and compute @xmath448 . *",
    "_ primal - dual _ : solve the linear system of the primal only method , solve the linear system @xmath449 of size @xmath416 , and compute @xmath450 . * _ saddle point _ : solve the linear system of size @xmath451 @xmath452 with @xmath453 , and compute @xmath454     + the numerical results are given in figure [ fig : opus_proj ] .",
    "once again , the saddle point method leads to the lowest error on the variable of interest .",
    "also , we see that a good preconditioner ( for example with @xmath455 ) improves the accuracy for the saddle point method , the primal only method and the primal - dual method . however , this improvement is not really significant for the considered application : the errors are barely divided by @xmath405 compared to the non preconditioned galerkin projection ( @xmath263 ) .",
    "in fact , the preconditioner improves the quality of the test space , and the choice @xmath115 ( yielding the standard galerkin projection ) is sufficiently accurate for this example and for the chosen norm on @xmath8 .",
    "we discuss now the quality of the error estimate @xmath279 for the variable of interest .",
    "since in this application the constant @xmath294 can not be easily computed , we consider surrogates for and using a preconditoner @xmath260 .",
    "we consider @xmath456 for the primal - dual method , and @xmath457 for the saddle point method .",
    "figure [ fig : opus_proj_errorest ] shows statistics of the effectivity index @xmath458 for different numbers @xmath261 of interpolation points for the preconditioner .",
    "we see that the max - min ratio and the normalized standard deviation are decreasing with @xmath261 : this indicates an improvement of the error estimate .",
    "furthermore , the mean value of @xmath431 seems to converge ( with @xmath261 ) to 19.5 for the primal - dual method , and to 13.8 for the saddle point method .",
    "in fact , with a good preconditioner , @xmath459 ( or @xmath460 ) is expected to be a good approximation of the primal error @xmath461 ( or @xmath462 ) , but this does not ensure that the effectivity index @xmath431 will converge to 1 .         in both numerical examples ,",
    "the saddle point method provides the most accurate estimation for the variable of interest .",
    "let us note that the saddle point problem requires the solution of a dense linear system of size @xmath463 for the symmetric and coercive case , and of size @xmath464 for the general case . when using gauss elimination method for the solution of those systems ,",
    "the complexity is either in @xmath465 or @xmath466 ( with @xmath467 ) , which is larger than the complexity of the primal - dual method @xmath468 .",
    "however , in the case where the primal and dual approximation spaces have the same dimension @xmath469 , the saddle point method is only @xmath470 times ( in the symmetric and coercive case ) or @xmath471 times ( in the general case ) more expensive .    for the present applications",
    ", we showed that the preconditioner slightly improves the quality of the estimation @xmath284 , and of the error estimate @xmath279 .",
    "since the construction of the preconditionner yields a significant increase in computational and memory costs ( see @xcite ) , the preconditioning is not mandatory here .",
    "nevertheless , these results revealed the important role of the test space @xmath472 to reduce the projection error .",
    "the preconditioner used for constructing @xmath472 can be improved , for example with a better selection of the interpolation point for @xmath260 , see equation .",
    "note also that alternative methods can be also applied for constructing @xmath472 , such as the subspace interpolation method proposed in @xcite .",
    "we now consider the greedy construction of the reduced spaces by algorithms [ alg : simultaneous ] or [ alg : alternated ] .",
    "for the two considered applications , we show the convergence of the error estimate with respect to the complexity of the offline and of the online phase . for the sake of simplicity",
    ", we measure the complexity of the offline phase with the number of operator factorizations ( this corresponds to the number of iterations @xmath315 of algorithms [ alg : simultaneous ] and [ alg : alternated ] ) .",
    "of course exact estimation of the offline complexity should take into account many other steps ( for example , the computation of @xmath279 , of the preconditioner , etc ) , but the operator factorization is considered , for large scale applications , as the main source of computation cost . for the online complexity , we only consider the computation cost for the solution of one reduced system , see section [ sec : numerical_proj_conclusion ] . here",
    "we do not take into account the complexity for assembling the reduced systems although it may be a significant part of the complexity for `` not so reduced '' systems of equations .",
    "figure [ fig : bridge_rb ] shows the convergence of @xmath473 with respect to the offline and online complexities ( as defined above ) . in figure",
    "[ fig : bridge_rb_offline ] , we see that the saddle point method ( dashed lines ) always provides lower values for the error estimate compared to the primal - dual method ( continuous lines ) .",
    "however , as already mentioned , the saddle point method requires the solution of larger reduced systems during the _ online _ phase .",
    "therefore , the primal - dual method can sometimes provide lower error estimates ( see the blue and red curves of figure [ fig : bridge_rb_online ] ) for the same _ online _ complexity .",
    "the simultaneous construction of @xmath7 and @xmath18 with full dual enrichment ( green curves ) yields a very fast convergence of the error estimate during the _ offline _ phase , see figure [ fig : bridge_rb_offline ] ) .",
    "but the rapid increase of @xmath474 leads to high _ online _ complexity , so that this strategy becomes non competitive during the _ online _ phase , see figure [ fig : bridge_rb_online ] .",
    "we compare now the alternate and the simultaneous construction of @xmath7 and @xmath18 with partial dual enrichment ( red and blue curves in figure [ fig : bridge_rb ] ) .",
    "the initial idea of the alternate construction is to build reduced spaces of better quality . indeed , since the evaluation points of the primal solution are different from the one of the dual solution , the reduced spaces are expected to contain `` complementary information '' for the approximation of the variable of interest . in practice , we observe in figure [ fig : bridge_rb_offline ] that the alternate construction is ( two times ) more expensive during the _ offline _ phase , but the resulting error estimate behaves very similarly to the simultaneous strategy , see figure [ fig : bridge_rb_online ] .",
    "we conclude that the alternate strategy is not relevant for this application .",
    "furthermore , let us note that after iteration 50 of the greedy algorithm , the rate of convergence of the dashed red curve of figure [ fig : bridge_rb_offline ] ( i.e. the simultaneous construction with partial dual enrichment using the saddle point method ) rapidly increases .",
    "a possible explanation is that the dimension of the dual approximation space is large enough to reproduce correctly the dual variable , which requires a dimension higher than @xmath369 .",
    "the same observation can be done for the alternative strategy ( the dashed blue curve ) after iteration @xmath475 ( which corresponds to @xmath476 ) . also , we note that the primal - dual method does not present this behavior .         for the application 2 , we first test algorithms [ alg : simultaneous ] and [ alg : alternated ] with the use of a preconditioner ( defined in section [ sec : appli22 ] ) .",
    "the interpolation points for the preconditioner are the ones where the solutions ( primal and dual ) have been computed , see algorithms [ alg : simultaneous ] and [ alg : alternated ] .",
    "the preconditioner is used for the definition of the test space @xmath472 , see equation , and for the error estimate @xmath279 , see equation for the primal - dual method and for the saddle point method .",
    "the numerical results are given in figure [ fig : opus_rb ] .",
    "we can draw the same conclusions as for application 1 .",
    "* in the offline phase , the saddle point method provides lower errors ( figure [ fig : opus_rb_offline ] ) .",
    "however , the corresponding reduced systems are larger , and we see that the primal - dual method provides lower errors for the same online complexity , see figure [ fig : opus_rb_online ] . for this test case , the benefits ( in term of accuracy ) of the saddle point method does not compensate the additional online computational costs . *",
    "the full dual enrichment yields a fast convergence during the offline phase , but the rapid increase of @xmath18 is disadvantageous regarding the online complexity .",
    "however , since the dimension of the variable of interest is `` only '' @xmath395 , the full dual enrichment is still an acceptable strategy ( compared to the previous application ) . * here , the alternate strategy ( blue curves ) seems to yield slightly better reduced spaces compared to the simultaneous strategy , see figure [ fig : opus_rb_online ] .",
    "but this leads to higher offline costs , see figure [ fig : opus_rb_offline ] .",
    "we also run numerical tests without using the preconditioner . in that case , we replace @xmath260 by @xmath477 . figure [ fig : opus_rb_withoutprecond ] shows numerical results which are very similar to those of figure [ fig : opus_rb ] . to illustrate the benefits of using the preconditioner , let us consider the effectivity index @xmath478 associated to the error estimate for the variable of interest .",
    "figure [ fig : rb_opus_eta ] shows the confidence interval @xmath479 of probability @xmath186 for @xmath431 defined as the smallest interval which satisfies @xmath480 where @xmath481 for @xmath482 .",
    "when using the preconditioner , we see in figure [ fig : rb_opus_eta ] that the effectivity index is improved during the greedy iterations in the sense that the confidence intervals are getting smaller and smaller . also , we note that after the iteration @xmath483 , the effectivity index is always above @xmath433 : this indicates that the error estimate tends to be certified . furthermore ,",
    "after iteration @xmath484 we do not observe any further improvement , so that is seems not useful to continue enriching the preconditioner .",
    "let us finally note that the use of the preconditioner yields significant computational costs .",
    "indeed , we have to store operator factorizations ( in our current implementation of the method ) , and the computation of the interpolation of the inverse operator requires additional problems to solve ( see @xcite ) . for the present application , even if the effectivity index of the error estimate is improved , the benefits of using the preconditioner remains questionable .",
    "we have proposed and analyzed projection based methods for the estimation of vector - valued variables of interest in the context of parameter - dependent equations .",
    "this includes a generalization of the classical primal - dual method to the case of vector - valued variables of interest , and also a petrov - galerkin method based on a saddle point problem .",
    "numerical results showed that the saddle point method always improves the quality of the approximation compared to the primal - dual method using the same reduced spaces .",
    "we have also derived computable error estimates and greedy algorithms for the goal - oriented construction of the reduced spaces .",
    "the performances of these approaches have been compared on numerical examples , with an analysis of both the offline complexity ( construction of the reduced spaces ) and the online complexity ( evaluation of the reduced order model and estimation of the variable of interest for one instance of the parameter ) .",
    "this complexity analysis revealed that the saddle point method is preferable to the primal - dual method regarding the offline costs .",
    "however , in the situation where the reduction of the online costs matter more than the reduction of offline costs , then the primal - dual method seems to be a better option ( at least for the considered applications ) . for the considered applications ,",
    "the use of preconditioners allows the construction of better reduced test spaces and also better error estimates . even if the additional computational costs for building the preconditioner is significant",
    ", this has demonstrated the importance of having a suitable test space and good residual based error estimates .",
    "the proposed error estimates , which involve the use of cauchy - schwarz inequalities , are clearly not optimal . extending probabilistic error bounds proposed in @xcite to the case of vector - valued variables could improve these error estimates .                                ,",
    "_ reduced basis approximation and a posteriori error estimation for affinely parametrized elliptic coercive partial differential equations : application to transport and continuum mechanics_. arch . comput . methods eng . , 15(3):229275 , may 2008 ."
  ],
  "abstract_text": [
    "<S> we propose and compare goal - oriented projection based model order reduction methods for the estimation of vector - valued functionals of the solution of parameter - dependent equations . </S>",
    "<S> the first projection method is a generalization of the classical primal - dual method to the case of vector - valued variables of interest . </S>",
    "<S> we highlight the role played by three reduced spaces : the approximation space and the test space associated to the primal variable , and the approximation space associated to the dual variable . </S>",
    "<S> then we propose a petrov - galerkin projection method based on a saddle point problem involving an approximation space for the primal variable and an approximation space for an auxiliary variable . a goal - oriented choice of the latter space , defined as the sum of two spaces , allows us to improve the approximation of the variable of interest compared to a primal - dual method using the same reduced spaces . </S>",
    "<S> then , for both approaches , we derive computable error estimates for the approximations of the variable of interest and we propose greedy algorithms for the goal - oriented construction of reduced spaces . </S>",
    "<S> the performance of the algorithms are illustrated on numerical examples and compared to standard ( non goal - oriented ) algorithms . </S>"
  ]
}