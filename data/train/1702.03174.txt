{
  "article_text": [
    "suppose we are given a sufficiently smooth nonlinear function @xmath0 and we are asked to solve the equation @xmath1 this archetypical problem is ubiquitous in all fields of mathematics , science and engineering .",
    "for example , ray tracing techniques in optics and computer graphics need to accurately calculate intersection points between straight lines , rays , and objects of varying shapes and sizes @xcite .",
    "implicit ode solvers are often formulated like , after which a root - finder of some kind is applied @xcite .",
    "depending on the properties of the function @xmath2 , there are several methods that present themselves .",
    "sometimes the derivative is not available for various reasons , in which case the secant method will prove useful .",
    "if higher - order convergence is desired , inverse quadratic interpolation may be used @xcite .",
    "if the derivative of @xmath2 exists and is available , newton s method is a solid choice , especially if @xmath2 is also convex .",
    "recently , a new interpretation of root - finding methods in terms of odes has been introduce by grau - snchez et al",
    "their idea is to consider the inverse function derivative rule as an ode , so that any explicit ode solver may be converted to a root - finding method . indeed , grau - snchez et al .",
    "have successfully introduced root - finders based on adams - type multistep and runge - kutta integrators .",
    "it goes without saying that only explicit ode solvers can be usefully converted to root - finding methods .",
    "however , predictor - corrector pairs are possible , as those methods are indeed explicit .",
    "we argue that the ode approach can be interpreted as inverse interpolation with ( higher ) derivatives .",
    "indeed , any linear integration method is based on polynomial interpolation .",
    "thus , the ode approach can be seen as a generalisation of inverse interpolation methods such as the secant method or inverse quadratic interpolation .",
    "the analysis can thus be combined into a single approach based on inverse polynomial interpolation .",
    "our main theoretical result is a theorem on the convergence rate of root - finders based on explicit linear multistep methods .",
    "we furthermore prove a barrier on the convergence rate of lmm - based root - finders .",
    "it turns out that adding a few history points quickly boosts the convergence rate close to the theoretical bound .",
    "however , adding many history points ultimately proves an exercise in futility due to diminishing returns in the convergence rate .",
    "two lmm - based methods are constructed explicitly , one using two history points with a convergence rate of @xmath3 and another with three history points that converges with rate @xmath4 .",
    "using several numerical examples , we show that the lmm - based methods indeed achieve this higher convergence rate .",
    "furthermore , pathological examples where newton s method fails to converge are used to show increased stability .",
    "we also construct a robust lmm - based method combined with bisection to produce a method that can been seen as an extension of brent s @xcite .",
    "similar to brent s method , whenever an enclosing starting bracket is provided , an interval @xmath5 $ ] with @xmath6 , our method is guaranteed to converge .",
    "this article is organised in the following way .",
    "first , we find the convergence rate of a wide class of root - finders in section  [ sec : barriers ] and prove a barrier on the convergence rates .",
    "next , in section  [ sec : new_finders ] we derive new root - finders based on full linear multistep methods and show that such methods are stable when the initial guess is sufficiently close to the root .",
    "after this , some results are presented in section  [ sec : results ] that verify our earlier theoretical treatment .",
    "finally , we present our robust implementation in section  [ sec : robust ] , after which we give our conclusions in section  [ sec : conclusions ] .",
    "root - finding methods based on the ode approach of grau - snchez et al . can be derived by assuming that the function @xmath2 is sufficiently smooth and invertible in the vicinity of the root . under these assumptions",
    ", the chain rule gives @xmath7^\\prime ( y ) = \\frac{1}{f^\\prime\\big(x\\big ) } = f(x),\\ ] ] which we may interpret as an autonomous ode for the inverse .",
    "integrating from an initial guess @xmath8 to @xmath9 yields @xmath10 immediately , we see that applying the forward euler method to gives newton s method . from ,",
    "we see that the step size of the integrator should be taken as @xmath11 .",
    "however , newton s method may also be interpreted as an inverse linear taylor method , i.e. , a method where the inverse function is approximated by a first - order taylor polynomial .",
    "indeed , any linear numerical integration method applied to can be interpreted as an inverse polynomial interpolation .    as such , explicit linear multistep methods applied to will also produce a polynomial approximation to the inverse function .",
    "such a method has the form @xmath12 where indeed @xmath13 , otherwise we end up with an implicit root - finder , which would not be very useful .",
    "the coefficients of the method , @xmath14 and @xmath15 , will depend on the previous step sizes and will therefore be different each step .",
    "the step sizes are given by @xmath16 , the differences in @xmath17-values .",
    "since we wish to find the root , we set @xmath18 , leading to @xmath19 .",
    "furthermore , the @xmath17-values are of course given by the function values of the root estimates , i.e. , @xmath20 like an ode solver , we may use an implicit lmm in tandem with an explicit lmm to form a predictor - corrector pair , the whole forming an explicit method . unlike an ode solver",
    ", we may construct derivative - free root - finders based on the lmm approach by setting all @xmath21 for @xmath22 and for all @xmath23 , e.g. , the secant method .",
    "for an ode solver this would obviously not make sense .",
    "similar to ode solvers , we may introduce higher derivatives by using @xmath24 the following theorem provides the maximal convergence rate for any method of the form .",
    "furthermore , it provides a fundamental barrier on the convergence rate of lmm - based root - finders .",
    "[ thm : convergence_bound ] for simple roots , the maximal convergence rate @xmath25 for any method of the form , where the coefficients are chosen so as to give the highest order of convergence , is given by the largest real root of @xmath26 where @xmath27 is the number of derivatives of @xmath28 used in the method .",
    "thus , @xmath29 for methods defined by .",
    "the coefficients @xmath30 indicate whether the coefficients @xmath31 are arbitrarily fixed from the outset or left free to maximise the order of convergence , i.e. , @xmath32 if @xmath33 is free and @xmath34 otherwise .",
    "moreover , the limiting convergence rate for any method using @xmath27 derivatives is @xmath35 .",
    "any method of the form implicitly uses inverse polynomial ( hermite ) interpolation applied to the inverse function @xmath28 , let us call the resulting interpolation @xmath36 .",
    "let @xmath37 , @xmath38 be the interpolation points .",
    "at each point @xmath37 , there are @xmath39 values are interpolated , the inverse function value @xmath40 if @xmath32 and @xmath27 derivative values .",
    "thus , the polynomial interpolation error formula gives @xmath41^{(n+1)}(\\upsilon)}{(n+1 ) ! } \\prod_{k=0}^{s-1 } ( y - y_{n+k})^{d+\\sigma_k},\\ ] ] where @xmath42 is in the interval spanned by the interpolation points and @xmath43 .",
    "the approximation to the root is then computed as @xmath44 .",
    "let us denote the exact value of the root as @xmath45 , then @xmath46^{(n+1)}(\\upsilon)|}{(n+1 ) ! } \\prod_{k=0}^{s-1 } | y_{n+k}|^{d+\\sigma_k}.\\ ] ] define @xmath47 and recognise that @xmath48 , where @xmath49 .",
    "thus , we find @xmath50 where @xmath51 is a constant depending on @xmath52^{(n+1)}(\\upsilon ) $ ] , @xmath53 and @xmath54 .",
    "the error behaviour is of the form @xmath55 asymptotically as @xmath56 . here",
    ", @xmath57 is an arbitrary constant .",
    "applying @xmath58 @xmath53 times on the left and @xmath59 times on the right - hand side leads to @xmath60 where all the constants have been absorbed into @xmath61 .",
    "thus , is established .",
    "finally , the highest convergence rate can be achieved by leaving all @xmath31 free .",
    "this way , we obtain @xmath62 simplifying , we obtain @xmath63 note that @xmath64 is always a solution of this equation .",
    "however , the maximal convergence rate is given by the largest real root , so that we look for solutions @xmath65 .",
    "dividing by @xmath66 yields @xmath67 hence if @xmath68 , we obtain @xmath69 .    from theorem  [ thm : convergence_bound ] , we find several special cases , such as the derivative - free interpolation root - finders .",
    "using @xmath70 , we find the following result .",
    "inverse polynomial interpolation root - finders , i.e. , @xmath70 resulting in all @xmath21 in , can attain at most a convergence rate that is quadratic .",
    "their convergence rates are given by the largest real root of @xmath71    the coefficients @xmath14 are chosen to maximise the order of convergence , so that @xmath32 for all @xmath72 , while @xmath70 , leading to @xmath73 simplifying yields .",
    "furthermore , the convergence rate is bounded by @xmath74 , since @xmath70 .    inverse polynomial root - finders such as the secant method ( @xmath75 ) or inverse quadratic interpolation ( @xmath76 ) are derivative - free , so that their highest convergence rate is @xmath77 according to theorem [ thm : convergence_bound ] .",
    "the first few convergence rates for derivative - free inverse polynomial interpolation methods are presented in table  [ tab : convergence_rates_interpolation ] .",
    "the well - known convergence rates for the secant method and the inverse quadratic interpolation method are indeed reproduced .",
    "as becomes clear from the table , the rates quickly approach @xmath77 but never quite get there .",
    "the increase in convergence rate becomes smaller and smaller as we increase the number of interpolation points .",
    ".the first few convergence rates for @xmath53 points using only function values . [ cols=\"<,<\",options=\"header \" , ]     table  [ tab : examples_brent_hybrid ] shows that for most functions , both brent s method and the lmm - based method take a comparable number of iterations .",
    "however , in some cases , the difference is considerable . in the worst case considered here",
    ", brent s method takes @xmath78 times as many iterations to converge . in terms of efficiency index ,",
    "brent s method should be superior with an efficiency index of @xmath79 against @xmath80 of the lmm - based method . taken over the whole set of test functions , however , brent s method takes more than three times as many iterations in total , leading to a significant increase in function evaluations .",
    "we conclude therefore that practically , the lmm - based root - finder is a better choice .",
    "we have discussed root - finders based on full linear multistep methods .",
    "such lmm - based methods may be interpreted as inverse polynomial ( hermite ) interpolation methods , resulting in a simple and general convergence analysis .",
    "furthermore , we have proven a fundamental barrier for lmm - based root - finders : their convergence rate can not exceed @xmath35 , where @xmath27 is the number of derivatives used in the method .",
    "the results indicate that compared to the adams - bashforth root - finder methods of grau - snchez et al .",
    "@xcite , any full lmm - based method with @xmath81 has a higher convergence rate . as ode solvers ,",
    "full lmms are typically not zero - stable and special choices of the coefficients have to be made .",
    "employed as root - finders on the other hand , it turns out that lmms are stable , due to the rapid decrease of the step size .",
    "this allows the usage of full lmms that are otherwise not zero - stable .",
    "contrary to the adams - type methods , the full lmm - based root - finders can achieve the convergence rate of @xmath82 in the limit that all history points are used .",
    "the @xmath75 and @xmath76 methods , @xmath53 being the number of history points , were explicitly constructed and provide a convergence rate of @xmath83 and @xmath4 , respectively .",
    "numerical experiments confirm these predicted convergence rates .",
    "furthermore , application to pathological functions where newton s method diverges show that the lmm - based methods also have enhanced stability properties .",
    "finally , we have implemented a robust lmm - based method that is guaranteed to converge when provided with an enclosing bracket of the root .",
    "the resulting robust lmm root - finder algorithm is a cascade of twelve root - finders increasing in convergence rate but decreasing in reliability . at the base",
    "sits bisection , so that the method is indeed guaranteed to converge to the root . at the top",
    "resides the @xmath76 lmm - based root - finder , providing a maximal convergence rate of @xmath4 .    in terms of efficiency index ,",
    "brent s method is theoretically the preferred choice with @xmath79 compared to @xmath80 for the lmm - based method .",
    "however , numerical examples show that the increased convergence rate leads to a significant decrease in the total number of function evaluations over a range of test functions .",
    "therefore , in practical situations , provided the derivative is available , the lmm - based method performs better .",
    "this work was generously supported by philips lighting and the intelligent lighting institute .",
    "l.  shengguo , l.  xiangke , and c.  lizhi , `` a new fourth - order iterative method for finding multiple roots of nonlinear equations , '' _ applied mathematics and computation _ , vol .  215 , no .  3 , pp .",
    "1288  1292 , 2009 ."
  ],
  "abstract_text": [
    "<S> root - finders based on full linear multistep methods ( lmms ) use previous function values , derivatives and root estimates to iteratively find a root of a nonlinear function . as ode solvers , full lmms are typically not zero - stable . </S>",
    "<S> however , used as root - finders , the interpolation points are convergent so that such stability issues are circumvented . </S>",
    "<S> a general analysis is provided based on inverse polynomial interpolation , which is used to prove a fundamental barrier on the convergence rate of any lmm - based method . </S>",
    "<S> we show , using numerical examples , that full lmm - based methods perform excellently . </S>",
    "<S> finally , we also provide a robust implementation based on brent s method that is guaranteed to converge .    </S>",
    "<S> root - finder , nonlinear equation , linear multistep methods , iterative methods , convergence rate . </S>"
  ]
}