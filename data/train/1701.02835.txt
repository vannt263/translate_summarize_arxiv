{
  "article_text": [
    "the quadratic eigenvalue problem ( qep ) is to find scalars @xmath0 and nonzero vectors @xmath1 satisfying @xmath2 @xmath3 where @xmath4 are @xmath5 complex matrices , @xmath1 are the right and left eigenvectors , respectively , corresponding to the eigenvalue @xmath0 .",
    "it has many important applications including least squares problem with constraints , fluid mechanics , circuit simulation , and structural mechanics .",
    "in@xcite , tisseur systematically summarized and reviewed the qep .",
    "there are two major classes of numerical methods to solve large qeps .",
    "the first method is to linearize the qep into an equivalent generalized eigenvalue problem ( gep ) such as@xcite    @xmath6     + where @xmath7 when @xmath8 is reversible , we can transform it to an equivalent standard eigenvalue problem ( sep ) + @xmath9    then we can obtain the eigenpair @xmath10 from the eigenpair of @xmath11 or @xmath12 .",
    "there are also other linearization forms @xcite .",
    "the biggest advantage of this approach is that it can use all the theoretical and numerical results of standard and generalized eigenvalue problems .",
    "there are many well developed methods available , for example , rational krylov method @xcite , displacement inverse arnoldi method@xcite , jacobi - davidson method@xcite .",
    "the disadvantages of this method are also obvious .",
    "first , the size of the linearization is twice of the original qep .",
    "this results in a significant increase in computational and storage requirements .",
    "second , this method encounters stability problems @xcite .    the second class of methods to solve large qep are direct projection methods .",
    "these methods project the large qep directly to a subspace @xmath13 to obtain a smaller qep .",
    "they can preserve the structure iteration of the original problem and has better numerical stability .",
    "some direct projection methods , are the residual iteration method@xcite , and the jacobi - davidson method@xcite .",
    "the main difficulty with these methods is the lack of theoretical basis .",
    "qeps are an important type of nonlinear eigenvalue problems that are less familiar and less routinely solved than the sep and the gep .",
    "a @xmath14 dimension qep can have @xmath15 eigenvalues ( when @xmath16 is singular , the number of the eigenvalue is less than @xmath15 ) and eigenvectors .",
    "sep has schur decomposition form , gep has generalized schur decomposition form , but qep does not have such forms . for the direct projection methods , the properties of the projection subspace @xmath13 has not been thoroughly analyzed .",
    "we are not very familiar with the information contained in the subspace . therefore , there are no perfect convergence analysis of these algorithms .",
    "another disadvantage is that most methods require matrix inversion at each iteration .",
    "while the convergence speed is fast , the computational costs are larger .    in this paper",
    ", we first analyze the convergence of the residual iteration method .",
    "we show the property of the subspace which is constructed by the residual iteration process .",
    "we have established the relationship between the subspace and the desired eigenvectors .",
    "we analyze the impact of shift selection and subspace expansion on the capacity of the subspace containing desired eigenvectors . in the process of expanding subspace",
    ", this method needs to solve a linear system at every step . for large scale problems in which the equations can not be solved directly ,",
    "then we propose an inner and outer iteration version of the residual iteration method .",
    "the new method uses iterative method to solve the equations and uses the approximate solution to expand the subspace .",
    "we establish the relationship between inner and outer iteration and give a quantitative criterion for inner iteration which can ensure the convergence of outer iteration .",
    "the remainder of this paper is arranged as follows . in section 2",
    ", we introduce the quadratic residual iteration method and analyze the convergence of this method . in section 3 , we propose an inexact residual iteration method and give a quantitative convergence analysis of the method . in section 4 ,",
    "several numerical experiments are presented to verify the results in this paper .    throughout the paper , we denote by @xmath17 the 2-norm of a vector or matrix , by @xmath18 the identity matrix with the order clear from the context , by the superscript @xmath19 the conjugate transpose of a vector or matrix .",
    "we measure the distance between a nonzero vector @xmath20 and a subspace @xmath21 by @xmath22 where @xmath23 is the orthogonal projector onto @xmath21 and the columns of @xmath24 form an orthonormal basis of the orthogonal complement of @xmath21 .",
    "in this section , we first introduce the quadratic residual iteration method .",
    "then we analyze the convergence property of the method . this provides a theoretical basis to further improve this type of methods .    for the most original residual iteration method",
    ", we can find inspiration from newton iterative method .",
    "if we transform the quadratic eigenvalue problem into an equation @xmath25 and solve it using the newton s method , then we can obtain the following method .",
    "* method 1 : * one step quadratic residual iteration method    1 .   for @xmath26",
    "3 .   @xmath28 .",
    "4 .   @xmath29 .",
    "end for    [ newton ]    where @xmath30 .",
    "this is a very elementary method which can only find one eigenvalue and eigenvector .",
    "it is similar to the rayleigh quotient iteration method for sep .",
    "depending on the characteristics of the newton iteration method , it can have a faster convergence rate when it begins to converge .",
    "but the convergence result is affected by the initial value @xmath31 . under certain conditions ,",
    "this method may have error convergence . in @xcite ,",
    "some deficiencies are addressed resulting in an improved method . in process method 1",
    ", we need one matrix inversion and one more matrix - vector product @xmath32 .",
    "when we use the subspace projection method , we use the residual iteration process to expand the subspace@xcite as follows :    * method 2 : * subspace quadratic residual iteration method ( qri )    1 .   set shift @xmath33 and initial vector @xmath34 , @xmath35 and convergence tolerance @xmath36 .",
    "2 .   for @xmath37",
    ".   set @xmath38 $ ] .",
    "4 .   project the large qep onto subspace @xmath39 and solve the smaller qep + @xmath40 to get the eigenvalue @xmath41 and eigenvector @xmath42 .",
    "compute ritz vector @xmath43 as approximate eigenvector .",
    "compute the residual @xmath44 .",
    "select the first non - convergence @xmath45 .",
    "compute @xmath46 .",
    "get @xmath47 from @xmath48 .",
    "end for    this method projects the large qep to the subspace directly at step 4 .",
    "it constructs the subspace by using the residual iteration method to generate new vector @xmath47 at each step . at step 7 , if the first residual @xmath49 , we can use the second residual to expand the subspace .",
    "so we can compute more than one eigenpairs using this method . in method 1 , the new vector is @xmath50 .",
    "if we use it to expand the subspace in method 2 , it needs one more matrix - vector product . we can remove the term @xmath51 and use a fixed value @xmath33 in method 2 . in order to get an orthogonal basis of the projection subspace , we should orthogonalize @xmath48 with @xmath52 at step 9 . at step 8 ,",
    "when the matrix size is small , the expanding vector @xmath48 can be computed directly .",
    "usually , @xmath48 can be obtained by solving the linear equations @xmath53 .",
    "jia and sun proposed a refined residual iteration method @xcite .",
    "they introduced the idea of refined projection to quadratic eigenvalue problem . for computing the approximate eigenvector of ritz value @xmath54 at step 5 ,",
    "the refined vector @xmath48 satisfies @xmath55    this method also has some changes and modifications , for example , we can use different shift @xmath33 at each step .",
    "we can use vector @xmath56 to expand subspace .",
    "so this method has a close relationship with jacobi - davidson method @xcite .",
    "all methods have shown good convergence properties . however",
    "the convergence of these methods have not thoroughly analyzed . according to the property of the residual iteration method for sep , with this method , it is easy to determine the eigenvalues which are close to the target point @xmath33 .",
    "so , beneficial shift can accelerate the convergence of the method .",
    "for a given target point @xmath33 , @xmath0 is the closest eigenvalue and @xmath20 is the corresponding eigenvector .",
    "then @xmath57 is the desired eigenpair of the subspace @xmath58 .",
    "we can use the distance between @xmath20 and @xmath58 or the angle @xmath59 to measure the convergence of the method .",
    "let @xmath58 be a subspace produced by method 2 .",
    "we use @xmath58 to denote the projection subspace and its orthogonal base .",
    "let @xmath60 be an orthogonal projection operator onto subspace @xmath58 .",
    "when we get @xmath61 with @xmath62 and @xmath63 is residual , define @xmath64 $ ] , the expanded subspace can be written as @xmath65 , where @xmath66 .    to improve subspace expansion , we have the following demonstration :    * theorem 1 * let @xmath58 and @xmath67 where the subspace is computed by method 2 , @xmath57 is the desired eigenpair .",
    "suppose @xmath68 , and @xmath69 , then we have @xmath70    because @xmath71 and @xmath72 .",
    "then we have @xmath73 finally , we get @xmath70    since @xmath74 , so we know that the value @xmath75 represents the ability of the method to obtain information from the outside subspace . at the @xmath76th step",
    ", we set @xmath77 then we can get the following result @xmath78 and @xmath79    this means that the subspace @xmath80 can contain more information of the desired eigenvector through expanding .",
    "the value @xmath75 determines the effect of each extension .",
    "so it should be as small as possible and we can use the upper bound to estimate the convergence rate . to give this bound , we first need the following lemma .",
    "assume that @xmath81 is a generalized linear form of @xmath82 , then we have @xcite @xmath83    * lemma@xcite * let the diagonal elements of @xmath84 are the eigenvalues of @xmath82 .",
    "@xmath85 $ ] , @xmath86 $ ] are the corresponding right and left eigenvectors .",
    "assume that @xmath87 is not an eigenvalue of @xmath11 and @xmath82 , then we have @xmath88    for the convergence rate of the method 2 , we have the following result .",
    "* theorem 2 * let @xmath47 , @xmath89 , @xmath90 , @xmath91,(@xmath92 ) be the same as the earlier definitions . for a given shift @xmath33 , @xmath93 is the closest eigenvalue and @xmath94 is the corresponding eigenvector , @xmath45 is the residual , and @xmath95 let @xmath96 then we have @xmath97[th2.1 ]    from @xmath98    we know @xmath99 then @xmath100    so @xmath101 since @xmath66 , @xmath102 , then it holds that + @xmath103 finally we get @xmath104    we can use this result to show the convergence of both method 1 and method 2 .",
    "when we use a fixed shift @xmath33 in method 1 , we get a constant matrix @xmath105 and method 1 becomes to the power method of matrix @xmath106 .",
    "let @xmath107 are the eigenvalues of @xmath108 .",
    "then we can get the eiagenpair @xmath109 by the power method .",
    "if @xmath93 is the nearest eigenvalue to @xmath33 , @xmath110 is near to @xmath111 but there must be a constant gap between @xmath110 and @xmath111 .",
    "we can not find the exact eigenvector @xmath110 from constant matrix @xmath108 . in method 1 , we use constantly changing shift @xmath112 to get changing matrix @xmath113 .",
    "we have two ways to achieve the convergence .",
    "the first is to use better shift at each iteration .",
    "method 1 is one such way . at each iteration",
    ", we use new shift @xmath114 to get new approximate eigenvector @xmath115 . from , we have @xmath116 when @xmath114 is converging to @xmath93 , @xmath117 .",
    "we can get better @xmath115 from @xmath114 , similarly , better @xmath115 can give better @xmath114 .",
    "the second way to achieve the convergence is subspace expanding .",
    "now , we use fixed shift @xmath33 in the iterative process and we obtain the better approximate eigenvector in the expanded subspace with factor @xmath118 . combining and , we obtain an estimation of total convergence rate for method 2 , @xmath119 assume @xmath120 a moderate value at each iteration then the convergence rate is decided by the value @xmath121 .",
    "the size of this value depends on the distribution of eigenvalues and selection of shift @xmath33 .",
    "so a good shift can accelerate the convergence of the method .",
    "the good convergence of the quadratic residual iteration method mainly benefit from the special expanding vector @xmath48 .",
    "when we compute the expanding vector at step 8 of method 2 , we do not compute the inverse matrix directly .",
    "we can compute @xmath48 by solving the corresponding linear equations .",
    "usually , this is one of the most resource intensive part of the method . for large scale problems ,",
    "it is hard to compute the expanding vector , even by solving linear equations . for linear eigenvalue problems , one way to solve this problem is to compute an inexact solution of the linear equations .",
    "this kind of methods are called inexact method or outer inner iteration methods@xcite .    based on this principle ,",
    "we propose the inexact iteration method for quadratic eigenvalue problems .",
    "* method 3 : * inexact quadratic residual iteration method    1 .   set shift @xmath33 and initial vector @xmath34 , @xmath35 and convergence tolerance @xmath36 .",
    "2 .   for @xmath37",
    "do : 3 .   set @xmath38 $ ] .",
    "4 .   project the large qep onto subspace @xmath39 and solve the smaller qep + @xmath40 to get the eigenvalue @xmath41 and eigenvector @xmath42 .",
    "compute ritz vector @xmath43 as approximate eigenvector .",
    "compute the residual @xmath44 .",
    "select the first non - convergence @xmath45 .",
    "solve the equations @xmath122 9 .",
    "get @xmath47 from @xmath48 .",
    "end for    the difference between method 2 and method 3 is at step 8 .",
    "method 2 computes an exact solution , but method 3 only needs to calculate an approximate solution . for large scale problems , it is difficult to solve the equations exactly .",
    "it is feasible to use a method to compute an approximate solution .",
    "normally , we use an iterative method to compute the approximate solution of the equations .",
    "so , we call it inner iteration and call the iterative of method 3 the outer iteration",
    ". the main difficulty of the method is to determine the accuracy of the approximate solution .",
    "we must to find the balance between outer and inner iteration .",
    "let @xmath48 be the exact solution of and @xmath123 be the approximate solution .",
    "then the relative error between them is @xmath124 then we can write @xmath125 where @xmath126 is the normalized error direction vector .",
    "+ so we get : + @xmath127 where @xmath128    define @xmath129 and @xmath130 where @xmath131 and @xmath132 are the normalized subspace expansion vectors in the inexact and exact methods , respectively . we can measure the difference between @xmath131 and @xmath132 by @xmath133 or @xmath134 .",
    "the relationship between @xmath133 and @xmath134 is @xcite , @xmath135 the relationship between @xmath133 and @xmath136 is , @xmath137 then we obtain : @xmath138    for the standard eigenvalue problem , we have obtained the convergence properties of the exact method .",
    "we can prove the convergence of the inexact method by proving that @xmath131 can mimic @xmath132 . according to the above relationships",
    ", we can show that @xmath131 can be a good imitation of @xmath132 under a moderately precise inner iteration .",
    "but these convergence results are more or less based on prior knowledge of eigenvalues . for the quadratic eigenvalue problem",
    ", we need to analyze the requirements of the inner iteration directly from the convergence of the inexact method .",
    "the convergence condition of method 3 is @xmath139 , @xmath140 is the desired eigenpair . according to the conclusion @xmath141",
    ", it is equivalent to @xmath142 according to their relationships of @xmath143 , the convergence can be analyzed by the value @xmath144 or @xmath145 .    for method 3 , we can take the approximate solution @xmath123 as an exact solution of the following perturbed equation @xmath146 here @xmath147 is the perturbation matrix of @xmath148 .",
    "* lemma 1 * if @xmath149 , the approximate solution @xmath123 and the exact solution @xmath48 have the following relationship @xmath150    for a matrix @xmath151 and the corresponding unit matrix @xmath18 , if @xmath152 , then @xmath153 is invertible@xcite and @xmath154    now , we can get @xmath155^{-1 } ( \\sigma^{2}m+\\sigma c+k)^{-1 } r. \\end{aligned}\\ ] ] we use formula to @xmath156^{-1}$ ] and ignore higher order terms to obtain @xmath157 ( \\sigma^{2}m+\\sigma c+k)^{-1 } r\\\\ & = [ i-(\\sigma^{2}m+\\sigma c+k)^{-1}\\delta h ] u\\\\ & = u-(\\sigma^{2}m+\\sigma c+k)^{-1}\\delta h u. \\end{aligned}\\ ] ] then we obtain the relationship about @xmath123 and @xmath48 .    for the convergence condition of method 3 , we have the following result .",
    "* theorem 4 * suppose @xmath140 is the desired eigenpair , and @xmath158 , then the angle between the inexact solution and the desired eigenvector satisfies @xmath159    from @xmath160 we can get @xmath161    when @xmath162 , we can ignore the small value @xmath163 .",
    "then the formula can be written as @xmath164 we can rewrite this formula as @xmath165 similarly , the formula also can be written as @xmath166 therefore , combining the last relation with establishes @xmath167    then we can get @xmath168    from the following relationship @xmath169 we can finish the proof .    from theorem 4 , we know that the inexact method can have fast convergence when the difference between @xmath170 and @xmath171 is very small . because @xmath172 and @xmath173 , these two values are always close , regardless of whether they are large or small .    when @xmath170 is a very small value , it means that both angles between @xmath48 and @xmath20 , @xmath132 and @xmath174 are small .",
    "so the exact residual iteration method can have fast convergence rate . for the inexact method ,",
    "if the inner iteration is not very precise , there is a large error between @xmath123 and @xmath48 . at first glance",
    ", the convergence rate of the inexact method may not be very fast .",
    "but small @xmath170 means small @xmath171 .",
    "so the angle between @xmath123 and @xmath20 is also small .",
    "that is to say , the inexact method can have fast convergence rate in spite of the moderate accuracy of inner iteration .",
    "when the value @xmath170 is not too small , this means that the inexact method can not get fast convergence speed using high accuracy in inner iteration .",
    "but the convergence of the inexact method can be guaranteed by @xmath175 , where @xmath176 is a constant value less than one .",
    "this condition is relatively easy to be satisfied and is independent of the inner iteration .",
    "the analysis shows that the convergence of the inexact method is mainly determined by the shift selection and subspace expansion",
    ". however , there are limitations to improving the precision of the inner iteration .",
    "several numerical experiments are presented in this section to demonstrate the effectiveness of method 3 and the analysis results . for all examples , dim stands for the dimension of matrix",
    ", tol denotes the convergent precision of outer iteration , tol is the precision of inner iteration .",
    "we use gmres to solve the inner iteration .",
    "we show the cpu time of each part of the method and unit is in seconds .",
    "total expresses the total cpu time , cgmres stands for the steps of gmres , tgmres stands for the cpu time of gmres , unit is in seconds .",
    "iter is the number of iteration .",
    "we select three different tol in our experiments , which respective denoted by `` inexact1 '' , `` inexact2 '' , `` inexact3 '' . in following figures , the horizontal axis is the dimension of subspace and the vertical axis is the relative largest of the six residuals norm at each subspace expansion .",
    "* example 1 * for a fixed shift @xmath33 , let @xmath177 and @xmath178 .",
    "we show that the eigenvector of @xmath108 is different from the eigenvector of the corresponding quadratic eigenvalue problem .",
    "so we can not use the power method of @xmath108 to compute the eigenvector of @xmath82 .         for large scale problem",
    ", it needs more large subspace to find the desired eigenvalues .",
    "all of the three methods with different inner precision need more outer iteration .",
    "but , we can find out from figure 2 that they have the same convergence history .",
    "there are no advantages if we use higher precision for inner iteration .",
    "we can see from the table 2 that for the inner iteration with the lower precision , the less time it will use at each outer iteration . then the total time saving is very considerable .",
    "so the method with lowest inner iteration is more superiority than that using highest inner precision .",
    "this is consistent with our theoretical analysis . in many practical applications , it is impossible , even if we want to find a very accurate solution fot the inner iteration .",
    "so both theory and experiments show that this new method is a feasible and efficient method ."
  ],
  "abstract_text": [
    "<S> in this paper , we first establish the convergence criteria of the residual iteration method for solving quadratic eigenvalue problems . </S>",
    "<S> we analyze the impact of shift point and the subspace expansion on the convergence of this method . in the process of expanding subspace </S>",
    "<S> , this method needs to solve a linear system at every step . for large scale problems in which the equations can not be solved directly </S>",
    "<S> , we propose an inner and outer iteration version of the residual iteration method . </S>",
    "<S> the new method uses the iterative method to solve the equations and uses the approximate solution to expand the subspace . </S>",
    "<S> we analyze the relationship between inner and outer iterations and provide a quantitative criterion for the inner iteration which can ensure the convergence of the outer iteration . </S>",
    "<S> finally , our numerical experiments provide proof of our analysis and demonstrate the effectiveness of the inexact residual iteration method . </S>"
  ]
}