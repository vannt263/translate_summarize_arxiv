{
  "article_text": [
    "-0.25 cm    let us consider the linear regression model , which assumes the linear relationship between the predictors @xmath0 and the predictand @xmath1 , @xmath2 where @xmath3 are the regression coefficients and @xmath4 is the error term assumed to have standard normal distribution . for simplicity",
    "we assume that @xmath5 is centered to mean zero , and the columns of @xmath6 are mean - centered and scaled to variance one .",
    "the ordinary least squares ( ols ) regression estimator is the common choice in situations where the number of observations , @xmath7 , in the data set is greater than the number of predictor variables , @xmath8 .",
    "however , in presence of multicollinearity among predictors , the ols estimator becomes unreliable , and if @xmath8 exceeds @xmath7 it can not even be computed .",
    "several alternatives have been proposed in this case ; here we focus on the class of shrinkage estimators which penalize the residual sum - of - squares .",
    "the ridge estimator uses an @xmath9 penalty on the regression coefficients @xcite , while the lasso estimator takes an @xmath10 penalty instead @xcite .",
    "although this does no longer allow for a closed form solution for the estimated regression coefficients , the lasso estimator gets _ sparse _",
    ", which means that some of the regression coefficients are shrunken to zero .",
    "this means that lasso acts like a variable selection method by returning a smaller subset of variables being relevant for the model .",
    "this is appropriate in particular for high dimensional low sample size data sets ( @xmath11 ) , arising from applications in chemometrics , biometrics , econometrics , social sciences and many other fields , where the data include many uninformative variables which have no effect on the predictand or have very small contribution to the model .",
    "there is also a limitation of the lasso estimator , since it is able to select only at most @xmath7 variables when @xmath12 . if @xmath7 is very small , or if the number of informative variables ( variables which are relevant for the model ) is expected to be greater than @xmath7 , the model performance can become poor . as a way out , the elastic net ( _ enet _ ) estimator has been introduced @xcite , which combines both @xmath10 and @xmath9 penalties : @xmath13 here , @xmath5 , the observations @xmath14 form the rows of @xmath6 , and the penalty term @xmath15 is defined as @xmath16.\\ ] ] the entire strength of the penalty is controlled by the tuning parameter @xmath17 .",
    "the other tuning parameter @xmath18 is the mixing proportion of the ridge and lasso penalties and takes value in @xmath19 $ ] .",
    "the elastic net estimator is able to select variables like in lasso regression , and shrink the coefficients according to ridge . for an overview of sparse methods ,",
    "see @xcite .    a further limitation of the previously mentioned estimators is their lack of robustness against data outliers . in practice ,",
    "the presence of outliers in data is quite common , and thus robust statistical methods are frequently used , see , for example  @xcite . in the linear regression setting , outliers may appear in the space of the predictand ( so - called vertical outliers ) , or in the space of the predictor variables ( leverage points ) @xcite .",
    "the least trimmed squares ( lts ) estimator has been among the first proposals of a regression estimator being fully robust against both types of outliers @xcite . it is defined as @xmath20 where the @xmath21 are the ordered absolute residuals @xmath22 , and @xmath23 @xcite . the number @xmath24 is chosen between @xmath25 and @xmath7 , where @xmath26 refers to the largest integer @xmath27 , and it determines the robustness properties of the estimator @xcite .",
    "the lts estimator also became popular due to the proposal of a quick algorithm for its computation , the so - called fast - lts algorithm @xcite .",
    "the key feature of this algorithm is the `` concentration step '' or c - step , which is an efficient way to arrive at outlier - free data subsets where the ols estimator can be applied .",
    "this only works for @xmath28 , but recently the sparse lts regression estimator has been proposed for high dimensional problems @xcite : @xmath29 this estimator adds an @xmath10 penalty to the objective function of the lts estimator , and it can thus be seen as a robust counterpart of the lasso estimator .",
    "the sparse lts estimator is robust to both vertical outliers and leverage points , and also a fast algorithm has been developed for its computation @xcite .",
    "the contribution of this work is twofold : a new sparse and robust regression estimator is proposed with combined @xmath10 and @xmath9 penalties .",
    "this robustified elastic net regression estimator overcomes the limitations of lasso type estimators concerning the low number of variables in the models , and concerning the instability of the estimator in case of high multicollinearity among the predictors @xcite . as a second contribution , a robust elastic net version of logistic regression",
    "is introduced for problems where the response @xmath30 is a binary variable , encoded with @xmath31 referring to the class memberships of two groups .",
    "the logistic regression model is @xmath32 , for @xmath33 , where @xmath34 denotes the conditional probability for the @xmath35th observation , @xmath36 and @xmath37 is the error term assumed to have binomial distribution .",
    "the most popular way to estimate the model parameters is the maximum likelihood ( ml ) estimator which is based on maximizing the log - likelihood function or , equivalently , minimizing the negative log - likelihood function , @xmath38 with the deviances @xmath39    the estimation of the model parameters with this method is not reliable when there is multicollinearity among the predictors and is not feasible when @xmath40 . to solve these problems , friedman et al .",
    "@xcite suggested to minimize a penalized negative log - likelihood function , @xmath41 here , @xmath42 is the elastic net penalty as given in equation ( [ penalty ] ) , and thus this estimator extends ( [ elnet ] ) to the logistic regression setting . using the elastic net penalty",
    "also solves the non - existence problem of the estimator in case of non - overlapping groups @xcite .",
    "robustness can be achieved by trimming the penalized log - likelihood function , and using weights as proposed in the context of robust logistic regression @xcite .",
    "these weights can also be applied in a reweighting step which increases the efficiency of the robust elastic net logistic regression estimator .",
    "the outline of this paper is as follows . in section [ sec : enetlts ]",
    ", we introduce the robust and sparse linear regression estimator and provide a detailed algorithm for its computation .",
    "section [ sec : enetlogit ] presents the robust elastic net logistic regression estimator .",
    "some important details which are different from the linear regression algorithm are mentioned here .",
    "section [ sec : tuningpara ] explains how the tuning parameters for the proposed estimators can be selected ; we prefer an approach based on cross - validation .",
    "since lts estimators possess a rather low statistical efficiency , a reweighting step is introduced in section  [ sec : rewight ] to increase the efficiency .",
    "the properties of the proposed estimators are investigated in simulation studies in section [ sec : simulations ] , and section [ sec : applications ] shows the performance on real data examples .",
    "section [ sec : comptime ] provides some insight into the computation time of the algorithms , and the final section [ sec : conclude ] concludes .",
    "-0.25 cm    a robust and sparse elastic net estimator in linear regression can be defined with the objective function @xmath43 where @xmath44 with @xmath45 , @xmath46 , and @xmath15 indicates the elastic net penalty with @xmath47 as in equation ( [ penalty ] ) .",
    "we call this estimator the _ enet - lts _ estimator , since it uses a trimmed sum of squared residuals , like the sparse lts estimator ( [ splts ] ) .",
    "the minimum of the objective function ( [ objectlinear ] ) determines the optimal subset of size @xmath24 , @xmath48 which is supposed to be outlier - free",
    ". the coefficient estimates @xmath49 depend on the subset @xmath50 .",
    "the enet - lts estimator is given for this subset @xmath51 by @xmath52    it is not trivial to identify this optimal subset , and practically one has to use an algorithm to approximate the solution .",
    "this algorithm uses c - steps : suppose that the current @xmath24-subset in the @xmath53th iteration of the algorithm is denoted by @xmath54 , and the resulting estimator by @xmath55 .",
    "then the next subset @xmath56 is formed by the indexes of those observations which correspond to the smallest @xmath24 squared residuals @xmath57 if @xmath58 denotes the estimator based on @xmath56 , then by construction of the @xmath24-subsets it follows immediately : @xmath59 this means that the c - steps decrease the objective function ( [ objectlinear ] ) successively , and lead to a local optimum after convergence .",
    "the global optimum is approximated by performing the c - steps with several initial subsets .",
    "however , in order to keep the runtime of the algorithm low , it is crucial that the initial subsets are chosen carefully .",
    "as motivated in @xcite , for a certain combination of the penalty parameters @xmath18 and @xmath60 , elemental subsets are created consisting of the indexes of three randomly selected observations .",
    "using only three observations increases the possibility of having no outliers in the elemental subsets .",
    "let us denote these elemental subsets by @xmath61 where @xmath62 .",
    "the resulting estimators based on the three observations are denoted by @xmath63 .",
    "now the squared residuals @xmath64 can be computed for all observations @xmath33 , and two c - steps are carried out , starting with the @xmath24-subset defined by the indexes of the smallest squared residuals . then only those @xmath65 @xmath24-subsets with the smallest values of the objective function ( [ objectlinear ] ) are kept as candidates . with these candidate subsets , the c - steps are performed until convergence ( no further decrease ) , and the best subset is defined as that one with the smallest value of the objective function .",
    "best subset _ also defines the estimator for this particular combination of @xmath18 and @xmath60 .",
    "basically , one can apply this procedure now for a grid of values in the interval @xmath47 and @xmath46 .",
    "practically , this may still be quite time consuming , and therefore , for a new parameter combination , the best subset of the neighboring grid value of @xmath18 and/or @xmath60 , is taken , and the c - steps are started from this best subset until convergence .",
    "this technique , called _",
    "warm starts _",
    ", is repeated for each combination over the grid of @xmath18 and @xmath60 values , and thus the start based on the elemental subsets is carried out only once .    the choice of the optimal tuning parameters @xmath66 and @xmath67 is detailed in section [ sec : tuningpara ] .",
    "the subset corresponding to the optimal tuning parameters is the optimal subset of size @xmath24 .",
    "the enet - lts estimator is then calculated on the optimal subset with @xmath66 and @xmath67 .",
    "-0.25 cm    based on the definition ( [ enetlog ] ) of the elastic net logistic regression estimator , it is straightforward to define the objective function of its robust counterpart based on trimming , @xmath68 where again @xmath44 with @xmath45 , and @xmath15 is the elastic net penalty as defined in equation ( [ penalty ] ) . as outlined in the last section [ sec : enetlts ] , the task is to find the optimal subset which minimizes the objective function and defines the robust sparse elastic net estimator for logistic regression .",
    "it turns out that the algorithm explained previously in the linear regression setting can be successfully used to find the approximative solution . in the following we will explain the modifications that need to be carried out .",
    "c - steps : : :    in the linear regression case , the c - steps were based on the squared    residuals ( [ eq : cstepreg ] ) . now the @xmath24-subsets are    determined according to the indexes of those observations with the    smallest values of the deviances    @xmath69 .",
    "however ,    here it needs to be made sure that the original group sizes are in the    same proportion .",
    "denote @xmath70 and @xmath71 the    number of observations in both groups , with @xmath72 .",
    "then @xmath73 and    @xmath74 define the group sizes in each    @xmath24-subset .",
    "a new @xmath24-subset is created with    the @xmath75 indexes of the smallest deviances    @xmath76 and    with the @xmath77 indexes of the smallest deviances    @xmath78 .",
    "elemental subsets : : :    in the linear regression case , the elemental subsets consisted of the    indexes of three randomly selected observations , see ( [ 3obs ] ) .",
    "now    four observations are randomly selected to form the elemental subsets ,    two from each group .",
    "this allows to compute the estimator , and the two    c - steps are based on the @xmath24 smallest values of the    deviances . as before , this is carried out for 500 elemental subsets ,    and only the `` best '' 10 @xmath24-subsets are kept . here ,",
    "`` best '' refers to an evaluation that is borrowed from a robustified    deviance measure proposed in croux and haesbroeck @xcite in the    context of robust logistic regression ( but not in high dimension ) .",
    "these authors replace the deviance function ( [ deviances ] ) used in    ( [ mllog ] ) by a function @xmath79 to define the    so - called bianco yohai ( by ) estimator    @xmath80 a highly robust logistic regression estimator , see also    @xcite .",
    "the form of the function @xmath79 is shown    in figure  [ fig : phifunc ] , see @xcite for details .",
    "+    we use this function as follows : positive scores    @xmath81 of group 1 , i.e.    @xmath82 , refer to correct classification and receive the    highest values for @xmath79 , while negative scores    refer to misclassification , with small or zero    @xmath79 values . for the scores of group 0 we have    the reverse behavior , see figure  [ fig : phifunc ] .",
    "when evaluating an    @xmath24-subset , the sum over the @xmath24 values of    @xmath83 for    @xmath84 is computed , and this sum should be as large as    possible .",
    "this means that we aim at identifying an    @xmath24-subset where the groups are separated as much as    possible",
    ". points on the wrong side have almost no contribution , but    also the contribution of outliers on the correct side is bounded . in    this way , outliers will not dominate the sum .",
    "+    with the best 10 @xmath24-subsets we continue the c - steps until    convergence . finally , the subset with the largest sum    @xmath83 over all    @xmath84 forms the best index set .",
    "+    function @xmath79 used    for evaluating an @xmath24-subset , based on the scores    @xmath81 for the two    groups.,scaledwidth=70.0% ]    the selection of the optimal parameters @xmath66 and @xmath67 is discussed in section [ sec : tuningpara ] .",
    "the subset corresponding to these optimal tuning parameters is defined as the optimal subset of size @xmath24 .",
    "the enet - lts logistic regression estimator is then calculated on the optimal subset with @xmath66 and @xmath67 .",
    "note that at the beginning of the algorithm for linear regression , the predictand is centered , and the predictor variables are centered robustly by the median and scaled by the mad . within the c",
    "- steps of the algorithm ,",
    "we additionally mean - center the response variable and scale the predictors by their arithmetic means and standard deviations , calculated on each current subset , see also @xcite .",
    "the same procedure is applied for logistic regression , except for centering the predictand . in the end , the coefficients are back - transformed to the original scale .",
    "-0.25 cm    sections [ sec : enetlts ] and [ sec : enetlogit ] outlined the algorithms to arrive at a best subset for robust elastic net linear and logistic regression , for each combination of the tuning parameters @xmath85 $ ] and @xmath86 $ ] . in this section",
    "we define the strategy to select the optimal combination @xmath66 and @xmath67 , leading to the optimal subset . for this purpose",
    "we are using @xmath53-fold cross - validation ( cv ) on those best subsets of size @xmath24 , with @xmath87 . in more detail , for @xmath53-fold cv ,",
    "the data are randomly split into @xmath53 blocks of approximately equal size . in case of logistic regression , each block needs to consist of observations from both classes with approximately the same class proportions as in the complete data set .",
    "each block is left out once , the model is fitted to the `` training data '' contained in the @xmath88 blocks , using a fixed parameter combination for @xmath18 and @xmath60 , and it is applied to the left - out block with the `` test data '' . in this way , @xmath24 fitted values are obtained from @xmath53 models , and they are compared to the corresponding original response by using the following evaluation criteria :    * for linear regression we take the root mean squared prediction error ( rmspe ) @xmath89 where @xmath90 presents the test set residuals from the models estimated on the training sets with a specific @xmath18 and @xmath60 ( for simplicity we omitted here the index @xmath53 denoting the models where the @xmath53-th block was left out and the corresponding test data from this block ) .",
    "* for logistic regression we use the mean of the negative log - likelihoods or deviances ( mnll ) @xmath91 where @xmath92 presents the test set deviances from the models estimated on the training sets with a specific @xmath18 and @xmath60 .    note that the evaluation criteria given by ( [ eq : cvrmspe ] ) and ( [ eq : cvlog ] ) are robust against outliers , because they are based on the best subsets of size @xmath24 , which are supposed to be outlier free .    in order to obtain more stable results , we repeat the @xmath53-fold cv five times and take the average of the corresponding evaluation measure . finally , the optimal parameters @xmath66 and @xmath67 are defined as that couple for which the evaluation criterion gives the minimal value .",
    "the corresponding best subset is determined as the optimal subset .",
    "note that the optimal couple @xmath66 and @xmath67 is searched on a grid of values @xmath85 $ ] and @xmath86 $ ] . in our experiments we used 41 equally spaced values for @xmath18 , and @xmath60 was varied in steps of size @xmath93 . for determining @xmath94 in the linear regression case we used the same approach as in alfons et al .",
    "@xcite which is based on the pearson correlation between @xmath95 and the @xmath96th predictor variable @xmath97 on winsorized data . for logistic regression we replaced the pearson correlation by a robustified point - biserial correlation : denote by @xmath70 and @xmath71 the group sizes of the two groups , and by @xmath98 and @xmath99 the medians of the @xmath96th predictor variable for the data from the two groups , respectively .",
    "then the robustified point - biserial correlation between @xmath95 and @xmath97 is defined as @xmath100 where @xmath101 is the mad of @xmath97 , and @xmath102 .",
    "-0.25 cm    the lts estimator has a low efficiency , and thus it is common to use a reweighting step @xcite .",
    "this idea is also used for the estimators introduced here . generally , in a reweighting step the outliers according to the current model are identified and downweighted . for the linear regression model we will use the same reweighting scheme as proposed in alfons et al .",
    "@xcite , which is based on standardized residuals . in case of logistic regression",
    "we compute the pearson residuals which are approximately standard normally distributed and given by @xmath103 with @xmath34 the conditional probabilities from ( [ eq : pi ] )",
    ".    for simplicity , denote the standardized residuals from the linear regression case also by @xmath104 .",
    "then the weights are defined by @xmath105 where @xmath106 , such that @xmath107 of the observations are flagged as outliers in the normal model .",
    "the reweighted enet - lts estimator is defined as @xmath108 where @xmath109 , @xmath110 stands for the vector of binary weights ( according to the current model ) , @xmath111 , and @xmath112 corresponds to squared residuals for linear regression or to the deviances in case of logistic regression . since @xmath113 , and because the optimal parameters @xmath66 and @xmath67 have been derived with @xmath24 observations , the penalty can act ( slightly ) differently in ( [ eq : rewest ] ) than for the raw estimator . for this reason ,",
    "the parameter @xmath67 has to be updated , while the @xmath66 regulating the tradeoff between the @xmath10 and @xmath9 penalty is kept the same .",
    "the updated parameter @xmath114 is determined by @xmath115-fold cv , with the simplification that @xmath66 is already fixed .",
    "-0.25 cm    in this section , the performance of the new estimators is compared with different sparse estimators in different scenarios .",
    "we consider both the raw and the reweighted versions of the enet - lts estimators , and therefore aim to show how the reweighting step improves the methods .",
    "the raw and reweighted enet - lts estimators are compared with their classical , non - robust counterparts , the linear and logistic regression estimators with elastic net penalty @xcite . in case of linear regression we also compare with the reweighted sparse lts estimator of @xcite .",
    "all robust estimators are calculated taking the subset size @xmath116 such that their performances are directly comparable .    for each replication , we choose the optimal tuning parameters @xmath66 and @xmath67 over the grids @xmath18 and @xmath60 with 5-times repeated @xmath115-fold cv as described in section [ sec : tuningpara ]",
    ". to select the tuning parameters for the classical estimators with elastic net penalty , we first draw the same grid for @xmath18 , namely @xmath85 $ ] , with 41 equally spaced grid points",
    ". then we use @xmath115-fold cv as provided by the r package _ glmnet _ , which automatically checks the model quality for a sequence of values for @xmath60 , taking the mean squared error as an evaluation criterion .",
    "finally , the tuning parameters corresponding to the smallest value of the minimum cross - validated error are determined as the optimal tuning parameters . in order to be coherent with our evaluation ,",
    "the tuning parameters for the sparse lts estimator are determined in the same way as for the enet - lts estimator .",
    "all simulations are carried out in r @xcite .",
    "note that we simulated the data sets with intercept .",
    "as described at the end of section [ sec : enetlogit ] , the data are centered and scaled at the beginning of the algorithm and only in the final step the coefficients are back - transformed to the original scale , where also the estimate of the intercept is computed .    * _ sampling schemes for linear regression _ :",
    "* let us consider two different scenarios by means of generating a `` low dimensional '' data set with @xmath117 and @xmath118 and a `` high dimensional '' data set with @xmath119 and @xmath120 .",
    "we generate a data matrix where the variables are forming correlated blocks , @xmath121 , where @xmath122 , @xmath123 and @xmath124 have the dimensions @xmath125,@xmath126 and @xmath127 , with @xmath128",
    ". such a block structure can be assumed in many application , and it mimics different underlying hidden processes .",
    "the observations of the blocks are generated independently from each other , from a multivariate normal distribution @xmath129 with @xmath130 , @xmath131 , @xmath132 , @xmath133 with @xmath134 , @xmath131 , @xmath135 , and @xmath136 with @xmath137 , @xmath131 , @xmath138 , respectively . while the first two blocks belong to the informative variables with sizes of @xmath139 and @xmath140 , the third block represents uninformative variables with @xmath141 .",
    "furthermore , we take @xmath142 to allow for a high correlation among the informative variables , and @xmath143 to have low correlation among the uninformative variables .    to create sparsity ,",
    "the true parameter vector @xmath144 consists of zeros for the last 90% of the entries referring to the uninformative variables , while the first 10% of the entries are assigned to one .",
    "the response variable is calculated by @xmath145 where the error term @xmath37 is distributed according to a standard normal distribution @xmath146 , for @xmath110 .",
    "this is the design for the simulations with clean data . for the simulation scenarios with outliers",
    "we replace the first @xmath147 of the observations of the block of informative variables by values coming from independent normal distributions @xmath148 for each variable .",
    "further , the error terms for these @xmath147 outliers are replaced by values from @xmath149 instead of @xmath146 , where @xmath150 represents the estimated standard deviation of the clean predictand vector . in this way",
    ", the contaminated data consist of both vertical outliers and leverage points . * _ sampling schemes for logistic regression _ : * we also consider two different scenarios for logistic regression , a `` low dimensional '' data set with @xmath117 and @xmath151 and a `` high dimensional '' data set with @xmath119 and @xmath120 .",
    "the data matrix is @xmath152 , where @xmath153 has the dimension @xmath154 and @xmath124 is of dimension @xmath127 , with @xmath155 .",
    "the data matrices are generated independently from @xmath156 with @xmath157 , @xmath131 , @xmath158 , and @xmath136 with @xmath137 , @xmath131 , @xmath138 , respectively . while the first block consists of the informative variables with @xmath159 , the second block represents uninformative variables with @xmath141 .",
    "we take @xmath160 for a high correlation among the informative variables , and @xmath161 for moderate correlation among the uninformative variables .",
    "the coefficient vector @xmath144 consists of ones for the first 10% of the entries , and zeros for the remaining uninformative block .",
    "the elements of the error term @xmath37 are generated independently from @xmath146 .",
    "the grouping variable is then generated according to the model @xmath162 with this setting , both groups are of approximately the same size .",
    "contamination is introduced by adding outliers only to the informative variables .",
    "denote @xmath70 the number of observations in class 0 .",
    "then the first @xmath163 observations of group 0 are replaced by values generated from @xmath148 . in order to create `` vertical '' outliers in addition to leverage points , we assign those first @xmath164 observations of class 0 a wrong class membership .    * _ performance measures _ : * for the evaluation of the different estimators , training and test data sets are generated according to the explained sampling schemes .",
    "the models are fit to the training data and evaluated on the test data .",
    "the test data are always generated without outliers .",
    "as performance measures we use the root mean squared prediction error ( rmspe ) for linear regression , @xmath165 and the mean of the negative log - likelihoods or deviances ( mnll ) for logistic regression , @xmath166 where @xmath167 and @xmath168 , @xmath110 , indicate the observations of the test data set , @xmath169 denotes the coefficient vector and @xmath170 stands for the estimated intercept term obtained from the training data set . in logistic regression",
    "we also calculate the misclassification rate ( mcr ) , defined as @xmath171 where @xmath172 is the number of misclassified observations from the test data after fitting the model on the training data .",
    "further , we consider the precision of the coefficient estimate as a quality criterion , defined by @xmath173 in order to compare the sparsity of the coefficient estimators , we evaluate the false positive rate ( fpr ) and the false negative rate ( fnr ) , defined as @xmath174 @xmath175 the fpr is the proportion of non - informative variables that are incorrectly included in the model . on the other hand , the fnr is the proportion of informative variables that are incorrectly excluded from the model .",
    "a high fnr usually has a bad effect on the prediction performance since it inflates the variance of the estimator .",
    "these evaluation measures are calculated for the generated data in each of 100 simulation replications separately , and then summarized by boxplots .",
    "the smaller the value for these criteria , the better the performance of the method .",
    "* _ results for linear regression _",
    ": * the outcome of the simulations for linear regression is summarized in figures [ fig : rmspe_lin][fig : fnr_lin ] .",
    "the left plots in these figures are for the simulations with low dimensional data , and the right plots for the high dimensional configuration .",
    "figure [ fig : rmspe_lin ] compares the rmspe .",
    "all methods yield similar results in the low dimensional non - contaminated case , while in the high dimensional clean data case the elastic net method is clearly better",
    ". however , in the contaminated case , elastic net leads to poor performance , which is also the case for sparse lts .",
    "enet - lts performs even slightly better with contaminated data , and there is also a slight improvement visible in the reweighted version of this estimator .",
    "the precision in figure [ fig : bias_lin ] shows essentially the same behavior .",
    "the fpr in figure [ fig : fpr_lin ] , reflecting the proportion of incorrectly added noise variables to the models , shows a very low rate for sparse lts . here",
    ", the elastic net even improves in the contaminated setting , and the same is true for enet - lts .",
    "a quite different picture is shown in figure [ fig : fnr_lin ] with the fnr . sparse lts and elastic net miss a high proportion of informative variables in the contaminated data scenario , which is the reason for their poor overall performance .",
    "note that the outliers are placed in the informative variables , which seems to be particularly difficult for sparse lts .     and",
    "@xmath118 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]    and @xmath118 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]     and @xmath118 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]   and @xmath118 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]     and @xmath118 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]   and @xmath118 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]     and @xmath118 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]   and @xmath118 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]    * _ results for logistic regression _ : * figures [ fig : misclas_log][fig : fnr_log ] summarize the simulation results for logistic regression . as before , the left plots refer to the low dimensional case , and the right plots to the high dimensional data . within one plot ,",
    "the results for uncontaminated and contaminated data are directly compared .",
    "the misclassification rate in figure [ fig : misclas_log ] is around 10% for all methods , and it is slightly higher in the high dimensional situation . in case of contamination , however , this rate increases enormously for the classical method elastic net .    the average deviances in figure [ fig : mnll_log ] show that the reweighting of the enet - lts estimator clearly improves the raw estimate in both the low and high dimensional cases .",
    "it can also be seen that elastic net is sensitive to the outliers .",
    "the precision of the parameter estimates in figure [ fig : bias_log ] reveal a remarkable improvement for the reweighted enet - lts estimator compared to the raw version , while there is not any clear effect of the contamination on the classical elastic net estimator .",
    "the fpr in figure [ fig : fpr_log ] shows a certain difference between uncontaminated and contaminated data for the elastic net , but otherwise the results are quite comparable .",
    "a different picture is visible from the fnr in figure [ fig : fnr_log ] , where especially in the low dimensional case the elastic net is very sensitive to the outliers .",
    "overall we conclude that the enet - lts performs very well in case of contamination even though this was not clearly visible in the precision , and it also yields reasonable results for clean data .     and @xmath151 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]   and @xmath151 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]     and @xmath151 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]    and @xmath151 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]     and @xmath151 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]   and @xmath151 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]     and @xmath151 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]   and @xmath151 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]     and @xmath151 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]   and @xmath151 ) ; right : high dimensional data set ( @xmath119 and @xmath120).,title=\"fig:\",scaledwidth=50.0% ]",
    "-0.25 cm    in this section we focus on applications with logistic regression , and compare the non - robust elastic net estimator with the robust enet - lts method .",
    "the model selection is conducted as described in section [ sec : tuningpara ] .",
    "model evaluation is done with leave - one - out cross validation , i.e.  each observation is used as test observation once , a model is estimated on the remaining observations , and the negative log - likelihood is calculated for the test observation . in these real data examples it is unknown if outliers are present . in order to avoid an influence of potential outliers on the evaluation of a model , the 25% trimmed mean of the negative log - likelihoods",
    "is calculated to compare the models .",
    "the time - of - flight secondary iron mass spectroscope cosima @xcite was sent to the comet churyumov - gerasimenko in the rosetta space mission by the esa to analyze the elemental composition of comet particles which were collected there @xcite . as reference measurements , samples of meteorites provided by the natural history museum vienna",
    "were analyzed with the same type of spectroscope at max planck institute for solar system research in gttingen .",
    "here we apply our proposed method for logistic regression to the measurements from particles from the meteorites ochansk and renazzo with 160 and 110 spectra , respectively .",
    "we restrict the mass range to 1 - 100mu , consider only mass windows where inorganic and organic ions can be expected as described in @xcite and variables with positive median absolute deviation .",
    "so we obtain @xmath176 variables .",
    "further , the data is normalized to have constant row sum 100 .",
    "table [ tab : meteorite ] summarizes the results for the comparison of the methods .",
    "the trimmed mnll is much smaller for the enet - lts estimator than for the classical elastic net method .",
    "the reweighting step improves the quality of the model further .",
    "the selected tuning parameter @xmath66 is much smaller for enet - lts than for the classical elastic net method which strongly influences the number of variables in the models .",
    ".renazzo and ochansk : number of variables in the optimal models and trimmed mean negative log - likelihood from leave - one - out cross validation of the optimal models . [ cols=\"<,>,>\",options=\"header \" , ]     different behavior of the coefficient estimates can be expected .",
    "figure  [ fig : glassvessel ] ( left ) shows coefficients of the reweighted enet - lts model corresponding to variables associated with potassium and calcium .",
    "the band which is associated with potassium has positive coefficients , i.e. high values of these variables correspond to the potassic group which is coded with ones in the response .",
    "high values of the variables in the band which is associated with calcium will favor a classification to the potasso - calcic group ( coded with zero ) , since the coefficients for these variables are negative .",
    "further , it can be observed that neighboring variables , which are correlated , have similar coefficients .",
    "this is favored by the @xmath9 term in the elastic net penalty . in figure",
    "[ fig : glassvessel ] ( right ) the coefficient estimates of the elastic net model are visualized .",
    "fewer coefficients are non - zero than for enet - lts which was favored by the @xmath10 term in the elastic net penalty , but in the second block of non - zero coefficients neighboring variables receive very different coefficient estimates .",
    "-0.25 cm    for our algorithm we employ the classical elastic net estimator as it is implemented in the r package @xmath177 @xcite .",
    "so , it is natural to compare the computation time of our algorithm with this method . in the linear regression case",
    "we also compare with the sparse lts estimator implemented in the r package @xmath178 @xcite . for calculating the estimators we take a grid of five values for both tuning parameters @xmath18 and @xmath60 .",
    "the data sets are simulated as in section [ sec : simulations ] for a fixed number of observations @xmath117 , but for a varying number of variables @xmath8 in a range from @xmath179 to @xmath180 . in figure",
    "[ fig : compt_time ] ( left : linear regression , right : logistic regression ) , the cpu time is reported in seconds , as an average over @xmath115 replications . in order to show the dependency on the number of observations @xmath7 , we also simulated data sets for a fixed number of variables @xmath120 with a varying number of observations @xmath181 .",
    "the results for linear and logistic regression are summarized in figure  [ fig : compt_time_n ] .",
    "the computations have been performed on an intel core 2 q9650 @ @xmath182 ghz@xmath1834 processor .     and varying @xmath8 ; left : for linear regression ; right : for logistic regression.,title=\"fig:\",scaledwidth=50.0% ]   and varying @xmath8 ; left : for linear regression ; right : for logistic regression.,title=\"fig:\",scaledwidth=50.0% ]     and varying @xmath7 ; left : for linear regression ; right : for logistic regression.,title=\"fig:\",scaledwidth=50.0% ]   and varying @xmath7 ; left : for linear regression ; right : for logistic regression.,title=\"fig:\",scaledwidth=50.0% ]    let us first consider the dependency of the computation time on the number of variables @xmath8 for linear regression , shown in the left plot of figure  [ fig : compt_time ] .",
    "sparse lts increases strongly with the number of variables @xmath8 since it is based on the lars algorithm which has a computational complexity of @xmath184 @xcite .",
    "also for the smallest number of considered variables , the computation time is considerably higher than for the other two methods .",
    "the reason is that for each value of @xmath60 and each step in the cv the best subset is determined starting with 500 elemental subsets . in this setting at least 25,000 estimations of a lasso model",
    "are needed , because for each cross validation step at each of the 5 values of @xmath60 , two c - steps for 500 elemental subsets are carried out , and for the 10 subsamples with lowest value of the objective function , further c - steps are performed .",
    "in contrast , the enet - lts estimator starts with 500 elemental subsets only for one combination of @xmath18 and @xmath60 , and takes the _ warm start _",
    "strategy for subsequent combinations .",
    "this saves computation time , and there is indeed only a slight increase with @xmath8 visible when compared to the elastic net estimator . in total approximately 1,700 elastic net models",
    "are estimated in this procedure , which are considerably fewer than for the sparse lts approach .",
    "the computation time of sparse lts also increases with @xmath7 due to the computational complexity of lars , while the increase is only minor for enet - lts , see figure  [ fig : compt_time_n ] ( left ) .",
    "the results for the computation time in logistic regression are presented in figure [ fig : compt_time ] ( right ) and [ fig : compt_time_n ] ( right ) . here",
    "we can only compare the classical elastic net estimator and the proposed robustified enet - lts version .",
    "the difference in computation time between elastic net and enet - lts is again due to the many calls of the glmnet function within enet - lts .",
    "the robust estimator is considerably slower in logistic regression when compared to linear regression for the same number of explanatory variables or observations .",
    "the reason is that more c - steps are necessary to identify the optimal subset for each parameter combination of @xmath18 and @xmath60 .",
    "-0.25 cm    in this paper , robust methods for linear and logistic regression using the elastic net penalty were introduced .",
    "this penalty allows for variable selection , can deal with high multicollinearity among the variables , and is thus very appropriate in high dimensional sparse settings .",
    "robustness has been achieved by using trimming .",
    "this usually leads to a loss in efficiency , and therefore a reweighting step was introduced .",
    "overall , the outlined algorithms for linear and logistic regression turned out to yield good performance in different simulation settings , but also with respect to computation time .",
    "particularly , it was shown that the idea of using `` warm starts '' for parameter tuning allows to save computation time , while the precision is still preserved . a competing method for robust high dimensional linear regression , the sparse lts estimator @xcite , does not use this idea , and is thus much less attractive concerning computation time , especially in case of many explanatory variables .",
    "we should also admit that for other simulation settings ( not shown here ) , the difference between sparse lts and the enet - lts estimator is not so big , or even marginal , depending on the exact setting .    for this reason ,",
    "a further focus was on the robust high dimensional logistic regression case .",
    "we consider such a method as highly relevant , since in many modern applications in chemometrics or bio - informatics , one is confronted with data information from two groups , with the task to find a classification rule and to identify marker variables which support the rules .",
    "outliers in the data are frequently a problem , and they can affect the identification of the marker variables as well as the performance of the classifier .",
    "for this reason it is desirable to treat outliers appropriately .",
    "it was shown in simulation studies as well as in data examples , that in presence of outliers the new proposal still works well , while its classical non - robust counterpart can lead to poor performance .",
    "note that in @xcite a logistic regression method with elastic net penalty is proposed using weights to reduce the influence of outliers .",
    "their approach is to perform outlier detection in a pca space , obtain weights based on robust mahalanobis distances in the pca score space and derive weights from these distances .",
    "these weights are then used to down - weight the negative log likelihoods in the penalized objective function to reduce the influence of outliers .",
    "however , it is not guaranteed that outliers can be detected in the pca score space",
    ". an increasing number of uninformative variables will disguise observations deviating from the majority only in few informative variables , but these hidden outlying observations can still distort the model . therefore , model based outlier detection is highly recommended as proposed in our algorithm .    the algorithms to compute the proposed estimators are implemented in r functions , which are available upon request from the authors .",
    "the basis for the computation of the robust estimator is the r package @xmath177 @xcite .",
    "this package also implements the case of multinomial and poisson regression .",
    "naturally , a further extension of the algorithms introduced here could go into these directions .",
    "further work will be devoted to the theoretical properties of the family of enet - lts estimators .",
    "this work is partly supported by the austrian science fund ( fwf ) , project p 26871-n20 and by grant tubitak 2214/a from the scientific and technological research council of turkey ( tubitak ) .",
    "the authors thank f. brandsttter , l. ferrire , and c. koeberl ( natural history museum vienna , austria ) for providing meteorite samples , c. engrand ( centre de sciences nuclaires et de sciences de la matire , orsay , france ) for sample preparation , and m. hilchenbach ( max planck institute for solar system research , gttingen , germany ) for tof - sims measurements .",
    "the authors are grateful to kurt varmuza for valuable feedback on the results of the meteorite data .",
    "10 url # 1`#1`urlprefixhref # 1#2#2 # 1#1                          a.  alfons , http://cran.r-project.org/package=robusthd[robusthd : robust methods for high dimensional data ] , r foundation for statistical computing , vienna , austria , r package version 0.4.0 ( 2013 ) .",
    "http://cran.r-project.org/package=robusthd        j.  friedman , t.  hastie , n.  simon , r.  tibshirani , http://cran.r-project.org/package=glmnet[glmnet : lasso and elastic net regularized generalized linear models ] , r foundation for statistical computing , vienna , austria , r package version 2.0 - 5 ( 2016 ) .",
    "http://cran.r-project.org/package=glmnet      v.  bianco , a.m.  yohai , robust estimation in logistic regression model , in robust statistics , data analysis , and computer intensive methods , 1734 ; lecture notes in statistics * 109 * , springer verlag , ed .",
    "h. , rieder : new york , 1996 .    , http://www.r-project.org[r : a language and environment for statistical computing , vienna , austria ] , r foundation for statistical computing , vienna , austria , isbn 3 - 900051 - 07 - 0 ( 2017 ) .",
    "j.  kissel , k.  altwegg , b.  clark , l.  colangeli , h.  cottin , s.  czempiel , j.  eibl , c.  engrand , h.  fehringer , b.  feuerbacher , et  al .",
    ", cosima  high resolution time - of - flight secondary ion mass spectrometer for the analysis of cometary dust particles onboard rosetta , space science reviews 1280 ( 1 - 4 ) ( 2007 ) 823867 .",
    "r.  schulz , m.  hilchenbach , y.  langevin , j.  kissel , j.  silen , c.  briois , c.  engrand , k.  hornung , d.  baklouti , a.  bardyn , et  al .",
    ", comet 67p / churyumov - gerasimenko sheds dust coat accumulated over the past four years , nature 5180 ( 7538 ) ( 2015 ) 216218 .",
    "k.  varmuza , c.  engrand , p.  filzmoser , m.  hilchenbach , j.  kissel , h.  krger , j.  siln , m.  trieloff , random projection for dimensionality reduction - applied to time - of - flight secondary ion mass spectrometry data , analytica chimica acta 705(1 ) ( 2011 ) 4855 ."
  ],
  "abstract_text": [
    "<S> fully robust versions of the elastic net estimator are introduced for linear and logistic regression . the algorithms to compute the estimators </S>",
    "<S> are based on the idea of repeatedly applying the non - robust classical estimators to data subsets only . </S>",
    "<S> it is shown how outlier - free subsets can be identified efficiently , and how appropriate tuning parameters for the elastic net penalties can be selected . a final reweighting step improves the efficiency of the estimators . </S>",
    "<S> simulation studies compare with non - robust and other competing robust estimators and reveal the superiority of the newly proposed methods . </S>",
    "<S> this is also supported by a reasonable computation time and by good performance in real data examples .    </S>",
    "<S> elastic net penalty , least trimmed squares , c - step algorithm , high dimensional data , robustness , sparse estimation </S>"
  ]
}