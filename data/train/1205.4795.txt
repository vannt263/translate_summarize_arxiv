{
  "article_text": [
    "the advent of modern technology makes it easier to collect massive , large - scale data - sets .",
    "a common feature of these data - sets is that the number of covariates greatly exceeds the number of observations , a regime opposite to conventional statistical settings .",
    "for example , portfolio allocation with hundreds of stocks in finance involves a covariance matrix of about tens of thousands of parameters , but the sample sizes are often only in the order of hundreds ( e.g. , daily data over a year period [ @xcite ] ) .",
    "genome - wide association studies in biology involve hundreds of thousands of single - nucleotide polymorphisms ( snps ) , but the available sample size is usually in hundreds , also .",
    "data - sets with large number of variables but relatively small sample size pose great unprecedented challenges and opportunities for statistical analysis .",
    "regularization methods have been widely used for high - dimensional variable selection [ @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ] . yet , most existing methods such as penalized least - squares or penalized likelihood [ @xcite ] are designed for light - tailed distributions .",
    "@xcite established the irrepresentability conditions for the model selection consistency of the lasso estimator .",
    "@xcite studied the oracle properties of nonconcave penalized likelihood estimators for fixed dimensionality .",
    "@xcite investigated the penalized least - squares estimator with folded - concave penalty functions in the ultra - high dimensional setting and established a nonasymptotic weak oracle property .",
    "@xcite proposed and investigated the sure independence screening method in the setting of light - tailed distributions .",
    "the robustness of the aforementioned methods have not yet been thoroughly studied and well understood .",
    "robust regularization methods such as the least absolute deviation ( lad ) regression and quantile regression have been used for variable selection in the case of fixed dimensionality .",
    "see , for example , @xcite , @xcite , @xcite , @xcite .",
    "the penalized composite likelihood method was proposed in @xcite for robust estimation in ultra - high dimensions with focus on the efficiency of the method .",
    "they still assumed sub - gaussian tails .",
    "@xcite studied the @xmath0-penalized quantile regression in high - dimensional sparse models where the dimensionality could be larger than the sample size .",
    "we refer to their method as robust lasso ( r - lasso ) .",
    "they showed that the r - lasso estimate is consistent at the near - oracle rate , and gave conditions under which the selected model includes the true model , and derived bounds on the size of the selected model , uniformly in a compact set of quantile indices .",
    "@xcite studied the @xmath0-penalized lad regression and showed that the estimate achieves near oracle risk performance with a nearly universal penalty parameter and established also a sure screening property for such an estimator .",
    "@xcite obtained bounds on the prediction error of a large class of @xmath0-penalized estimators , including quantile regression . @xcite",
    "considered the nonconvex penalized quantile regression in the ultra - high dimensional setting and showed that the oracle estimate belongs to the set of local minima of the nonconvex penalized quantile regression , under mild assumptions on the error distribution . in this paper , we introduce the penalized quantile regression with the weighted @xmath0-penalty ( wr - lasso ) for robust regularization , as in @xcite . the weights are introduced to reduce the bias problem induced by the penalty .",
    "the flexibility of the choice of the weights provides flexibility in shrinkage estimation of the regression coefficient .",
    "wr - lasso shares a similar spirit to the folded - concave penalized quantile - regression [ @xcite , @xcite ] , but avoids the nonconvex optimization problem .",
    "we establish conditions on the error distribution in order for the wr - lasso to successfully recover the true underlying sparse model with asymptotic probability one .",
    "it turns out that the required condition is much weaker than the sub - gaussian assumption in @xcite .",
    "the only conditions we impose is that the density function of error has lipschitz property in a neighborhood around 0 .",
    "this includes a large class of heavy - tailed distributions such as the stable distributions , including the cauchy distribution .",
    "it also covers the double exponential distribution whose density function is nondifferentiable at the origin .    unfortunately , because of the penalized nature of the estimator , wr - lasso estimate has a bias . in order to reduce the bias ,",
    "the weights in wr - lasso need to be chosen adaptively according to the magnitudes of the unknown true regression coefficients , which makes the bias reduction infeasible for practical applications .    to make the bias reduction feasible",
    ", we introduce the adaptive robust lasso ( ar - lasso )",
    ". the ar - lasso first runs r - lasso to obtain an initial estimate , and then computes the weight vector of the weighted @xmath0-penalty according to a decreasing function of the magnitude of the initial estimate .",
    "after that , ar - lasso runs wr - lasso with the computed weights .",
    "we formally establish the model selection oracle property of ar - lasso in the context of @xcite with no assumptions made on the tail distribution of the model error .",
    "in particular , the asymptotic normality of the ar - lasso is formally established .",
    "this paper is organized as follows .",
    "first , we introduce our robust estimators in section  [ secnotation ] .",
    "then , to demonstrate the advantages of our estimator , we show in section  [ seclasso ] with a simple example that lasso behaves suboptimally when noise has heavy tails . in section  [ secoracle ] ,",
    "we study the performance of the oracle - assisted regularization estimator .",
    "then in section  [ secselection ] , we show that when the weights are adaptively chosen , wr - lasso has the model selection oracle property , and performs as well as the oracle - assisted regularization estimate . in section  [ secan ] , we prove the asymptotic normality of our proposed estimator .",
    "the feasible estimator , ar - lasso , is investigated in section  [ secpenalty ] .",
    "section  [ secsimul ] presents the results of the simulation studies .",
    "finally , in section  [ secproof ] , we present the proofs of the main theorems .",
    "additional proofs , as well as the results of a genome - wide association study , are provided in the supplementary appendix [ @xcite ] .",
    "consider the linear regression model @xmath1 where @xmath2 is an @xmath3-dimensional response vector , @xmath4 is an @xmath5 fixed design matrix , @xmath6 is a @xmath7-dimensional regression coefficient vector , and @xmath8 is an @xmath3-dimensional error vector whose components are independently distributed and satisfy @xmath9 for some known constant @xmath10 . under this model , @xmath11 is the conditional quantile of @xmath12 given @xmath13 .",
    "we impose no conditions on the heaviness of the tail probability or the homoscedasticity of @xmath14 .",
    "we consider a challenging setting in which @xmath15 with some constant @xmath16 . to ensure the model identifiability and to enhance the model fitting accuracy and interpretability , the true regression coefficient vector @xmath17 is commonly imposed to be sparse with only a small proportion of nonzeros [ @xcite , @xcite ] .",
    "denoting the number of nonzero elements of the true regression coefficients by @xmath18 , we allow @xmath18 to slowly diverge with the sample size @xmath3 and assume that @xmath19 . to ease the presentation",
    ", we suppress the dependence of @xmath18 on @xmath3 whenever there is no confusion . without loss of generality , we write @xmath20 , that is , only the first @xmath21 entries are nonvanishing .",
    "the true model is denoted by @xmath22 and its complement , @xmath23 , represents the set of noise variables .",
    "we consider a fixed design matrix in this paper and denote by @xmath24 the submatrix of @xmath25 corresponding to the covariates whose coefficients are nonvanishing .",
    "these variables will be referred to as the signal covariates and the rest will be called noise covariates .",
    "the set of columns that correspond to the noise covariates is denoted by @xmath26 .",
    "we standardize each column of @xmath25 to have @xmath27-norm @xmath28 .",
    "to recover the true model and estimate @xmath29 , we consider the following regularization problem : @xmath30 where @xmath31 is the quantile loss function , and @xmath32 is a nonnegative penalty function on @xmath33 with a regularization parameter @xmath34 .",
    "the use of quantile loss function in ( [ genpenreg ] ) is to overcome the difficulty of heavy tails of the error distribution . since @xmath35 , ( [ genpenreg ] ) can be interpreted as the sparse estimation of the conditional @xmath36th quantile . regarding the choice of @xmath32 ,",
    "it was demonstrated in @xcite and @xcite that folded - concave penalties are more advantageous for variable selection in high dimensions than the convex ones such as the @xmath0-penalty .",
    "it is , however , computationally more challenging to minimize the objective function in ( [ genpenreg ] ) when @xmath37 is folded - concave . noting that with a good initial estimate @xmath38 of the true coefficient vector , we have @xmath39 thus , instead of ( [ genpenreg ] ) we consider the following weighted @xmath0-regularized quantile regression : @xmath40 where @xmath41 is the vector of nonnegative weights , and @xmath42 is the hadamard product , that is , the componentwise product of two vectors .",
    "this motivates us to define the weighted robust lasso ( wr - lasso ) estimate as the global minimizer of the convex function @xmath43 for a given nonstochastic weight vector : @xmath44 the uniqueness of the global minimizer is easily guaranteed by adding a negligible @xmath27-regularization in implementation .",
    "in particular , when @xmath45 for all @xmath46 , the method will be referred to as robust lasso ( r - lasso ) .",
    "the adaptive robust lasso ( ar - lasso ) refers specifically to the two - stage procedure in which the stochastic weights @xmath47 for @xmath48 are used in the second step for wr - lasso and are constructed using a concave penalty @xmath32 and the initial estimates , @xmath49 , from the first step . in practice",
    ", we recommend using r - lasso as the initial estimate and then using scad to compute the weights in ar - lasso .",
    "the asymptotic result of this specific ar - lasso is summarized in corollary  [ cor1 ] in section  [ secpenalty ] for the ultra - high dimensional robust regression problem .",
    "this is a main contribution of the paper .",
    "in this section , we use a specific example to illustrate that , in the case of heavy - tailed error distribution , lasso fails at model selection unless the nonzero coefficients , @xmath50 , have a very large magnitude .",
    "we assume that the errors @xmath51 have the identical symmetric stable distribution and the characteristic function of @xmath52 is given by @xmath53 = \\exp\\bigl(-|u|^{\\alpha } \\bigr),\\ ] ] where @xmath54 .",
    "by @xcite , @xmath55 is finite for @xmath56 , and @xmath57 for @xmath58 .",
    "furthermore , as @xmath59 , @xmath60 where @xmath61 is a constant depending only on @xmath62 , and we use the notation @xmath63 to denote that two terms are equivalent up to some constant .",
    "moreover , for any constant vector @xmath64 , the linear combination @xmath65 has the following tail behavior : @xmath66 with @xmath67 denoting the @xmath68-norm of a vector .    to demonstrate the suboptimality of lasso",
    ", we consider a simple case in which the design matrix satisfies the conditions that @xmath69 , @xmath70 , the columns of @xmath71 satisfy @xmath72 and @xmath73 for any @xmath74 and @xmath75 . here , @xmath76 is a positive integer measuring the sparsity level of the columns of @xmath71 .",
    "we assume that there are only fixed number of true variables , that is , @xmath21 is finite , and that @xmath77 . thus , it is easy to see that @xmath78 .",
    "in addition , we assume further that all nonzero regression coefficients are the same and @xmath79 .",
    "we first consider r - lasso , which is the global minimizer of ( [ eql1reg ] ) .",
    "we will later see in theorem  [ t2 ] that by choosing the tuning parameter @xmath80 r - lasso can recover the true support @xmath81 with probability tending to  1 .",
    "moreover , the signs of the true regression coefficients can also be recovered with asymptotic probability one as long as the following condition on signal strength is satisfied : @xmath82    now , consider lasso , which minimizes @xmath83 we will see that for ( [ eqlasso ] ) to recover the true model and the correct signs of coefficients , we need a much stronger signal level than that is given in ( [ eql1sigstrength ] ) . by results in optimization theory , the karush ",
    "tucker ( kkt ) conditions guaranteeing the necessary and sufficient conditions for @xmath84 with @xmath85 being a minimizer to  ( [ eqlasso ] ) are @xmath86 where @xmath87 is the complement of @xmath88 , @xmath89 is the subvector formed by entries of  @xmath90 with indices in @xmath88 , and @xmath91 and @xmath92 are the submatrices formed by columns of @xmath93 with indices in @xmath88 and @xmath87 , respectively .",
    "it is easy to see from the above two conditions that for lasso to enjoy the sign consistency , @xmath94 with asymptotic probability one , we must have these two conditions satisfied with @xmath95 with probability tending to 1 .",
    "since we have assumed that @xmath96 and @xmath97 , the above sufficient and necessary conditions can also be written as @xmath98 conditions ( [ eqkkt1a ] ) and ( [ eqkkt2a ] ) are hard for lasso to hold simultaneously .",
    "the following proposition summarizes the necessary condition , whose proof is given in the supplementary material [ @xcite ] .",
    "[ prop1 ] in the above model , with probability at least @xmath99 , where @xmath100 is some positive constant , lasso does not have sign consistency , unless the following signal condition is satisfied @xmath101    comparing this with ( [ eql1sigstrength ] ) , it is easy to see that even in this simple case , lasso needs much stronger signal levels than r - lasso in order to have a sign consistency in the presence of a heavy - tailed distribution .",
    "in this section , we establish the model selection oracle property of wr - lasso .",
    "the study enables us to see the bias due to penalization , and that an adaptive weighting scheme is needed in order to eliminate such a bias .",
    "we need the following condition on the distribution of noise .",
    "[ assp1 ] there exist universal constants @xmath102 and @xmath103 such that for any @xmath104 satisfying @xmath105 , @xmath106 s are uniformly bounded away from 0 and @xmath107 and @xmath108 where @xmath106 and @xmath109 are the density function and distribution function of the error @xmath14 , respectively .",
    "condition  [ assp1 ] implies basically that each @xmath106 is lipschitz around the origin .",
    "commonly used distributions such as the double - exponential distribution and stable distributions including the cauchy distribution all satisfy this condition .",
    "denote by @xmath110 .",
    "the next condition is on the submatrix of @xmath25 that corresponds to signal covariates and the magnitude of the entries of @xmath25 .",
    "[ assp2 ] the eigenvalues of @xmath111 are bounded from below and above by some positive constants @xmath112 and @xmath113 , respectively .",
    "furthermore , @xmath114    although condition  [ assp2 ] is on the fixed design matrix , we note that the above condition on @xmath115 is satisfied with asymptotic probability one when the design matrix is generated from some distributions .",
    "for instance , if the entries of @xmath25 are independent copies from a subexponential distribution , the bound on @xmath115 is satisfied with asymptotic probability one as long as @xmath116 ; if the components are generated from sub - gaussian distribution , then the condition on @xmath115 is satisfied with probability tending to one when @xmath117 .      to evaluate our newly proposed method ,",
    "we first study how well one can do with the assistance of the oracle information on the locations of signal covariates .",
    "then we use this to establish the asymptotic property of our estimator without the oracle assistance . denote by @xmath118 the oracle regularized estimator ( ore ) with @xmath119 and @xmath120 being the vector of all zeros , which minimizes @xmath43 over the space @xmath121 .",
    "the next theorem shows that ore is consistent , and estimates the correct sign of the true coefficient vector with probability tending to one .",
    "we use @xmath122 to denote the first @xmath21 elements of  @xmath123 .",
    "[ t1 ] let @xmath124 with @xmath125 a constant .",
    "if conditions  [ assp1 ] and  [ assp2 ] hold and @xmath126 , then there exists some constant @xmath127 such that @xmath128 if in addition @xmath129 , then with probability at least @xmath130 , @xmath131 where the above equation should be understood componentwisely .",
    "as shown in theorem  [ t1 ] , the consistency rate of @xmath132 in terms of the vector norm is given by @xmath133 .",
    "the first component of @xmath133 , @xmath134 , is the oracle rate within a factor of @xmath135 , and the second component @xmath136 reflects the bias due to penalization .",
    "if no prior information is available , one may choose equal weights @xmath137 , which corresponds to r - lasso .",
    "thus , for r - lasso , with probability at least @xmath138 , it holds that @xmath139      in this section , we show that even without the oracle information , wr - lasso enjoys the same asymptotic property as in theorem  [ t1 ] when the weight vector is appropriately chosen . since the regularized estimator @xmath140 in ( [ eql1reg ] ) depends on the full design matrix @xmath25 , we need to impose the following conditions on the design matrix to control the correlation of columns in @xmath71 and @xmath141 .",
    "[ assp3 ] with @xmath133 defined in theorem  [ t1 ] , it holds that @xmath142 where @xmath143 for a matrix @xmath144 and vector @xmath145 , and @xmath146 .",
    "furthermore , @xmath147 for some constant @xmath148 .",
    "to understand the implications of condition  [ assp3 ] , we consider the case of @xmath149 . in the special case of @xmath150 , condition  [ assp3 ]",
    "is satisfied automatically . in the case of equal correlation ,",
    "that is , @xmath151 having off - diagonal elements all equal to @xmath152 , the above condition  [ assp3 ] reduces to @xmath153 this puts an upper bound on the correlation coefficient @xmath152 for such a dense matrix .",
    "it is well known that for gaussian errors , the optimal choice of regularization parameter @xmath154 has the order @xmath155 [ @xcite ] .",
    "the distribution of the model noise with heavy tails demands a larger choice of @xmath154 to filter the noise for r - lasso . when @xmath156 , @xmath133 given in ( [ eq12a ] ) is in the order of @xmath157 .",
    "in this case , condition  [ assp3 ] reduces to @xmath158 for wr - lasso , if the weights are chosen such that @xmath159 and @xmath160 , then @xmath133 is in the order of @xmath134 , and correspondingly , condition  [ assp3 ] becomes @xmath161 this is a more relaxed condition than ( [ eq13 ] ) , since with heavy - tailed errors , the optimal @xmath154 should be larger than @xmath162 . in other words , wr - lasso not only reduces the bias of the estimate , but also allows for stronger correlations among the signal and noise covariates .",
    "however , the above choice of weights depends on unknown locations of signals .",
    "a data - driven choice will be given in section  [ secpenalty ] , in which the resulting ar - lasso estimator will be studied .",
    "the following theorem shows the model selection oracle property of the wr - lasso estimator .",
    "[ t2 ] suppose conditions  [ assp1][assp3 ] hold .",
    "in addition , assume that @xmath163 with some constant @xmath164 , @xmath165 and @xmath166 , where @xmath115 is defined in condition  [ assp2 ] , @xmath133 is defined in theorem  [ t1 ] , and @xmath167 is some positive constant .",
    "then , with probability at least @xmath168 , there exists a global minimizer @xmath169 of @xmath43 which satisfies    @xmath170 ;    @xmath171 .",
    "theorem  [ t2 ] shows that the wr - lasso estimator enjoys the same property as ore with probability tending to one .",
    "however , we impose nonadaptive assumptions on the weight vector @xmath172 . for noise covariates , we assume @xmath173 , which implies that each coordinate needs to be penalized . for the signal covariates , we impose ( [ eq12 ] ) , which requires @xmath174 to be small . when studying the nonconvex penalized quantile regression",
    ", @xcite assumed that @xmath115 is bounded and the density functions of @xmath14 s are uniformly bounded away from 0 and @xmath107 in a small neighborhood of 0 .",
    "their assumption on the error distribution is weaker than our condition  [ assp1 ] .",
    "we remark that the difference is because we have weaker conditions on @xmath115 and the penalty function [ see condition  [ assp2 ] and ( [ eq12 ] ) ] .",
    "in fact , our condition  [ assp1 ] can be weakened to the same condition as that in @xcite at the cost of imposing stronger assumptions on @xmath115 and the weight vector  @xmath123 .",
    "@xcite and @xcite imposed the restricted eigenvalue assumption of the design matrix and studied the @xmath0-penalized quantile regression and lad regression , respectively .",
    "we impose different conditions on the design matrix and allow flexible shrinkage by choosing @xmath123 .",
    "in addition , our theorem  [ t2 ] provides a stronger result than consistency ; we establish model selection oracle property of the estimator .",
    "we now present the asymptotic normality of our estimator . define @xmath175 and @xmath176 with for @xmath177 .",
    "[ t3 ] assume the conditions of theorem  [ t2 ] hold , the first and second order derivatives @xmath178 and @xmath179 are uniformly bounded in a small neighborhood of  0 for all @xmath180 , and that @xmath181,@xmath182 , and @xmath183 .",
    "then , with probability tending to 1 there exists a global minimizer @xmath184 of @xmath43 such that @xmath170 .",
    "moreover , @xmath185 { \\stackrel}{\\mathscr{d } } { \\longrightarrow}n \\bigl(0 , \\tau(1-\\tau ) \\bigr),\\ ] ] where @xmath186 is an arbitrary @xmath21-dimensional vector satisfying @xmath187 , and @xmath188 is an dimensional vector with the @xmath46th element @xmath189 .",
    "the proof of theorem  [ t3 ] is an extension of the proof on the asymptotic normality theorem for the lad estimator in @xcite , in which the theorem is proved for fixed dimensionality .",
    "the idea is to approximate @xmath190 in ( [ eql1reg ] ) by a sequence of quadratic functions , whose minimizers converge to normal distribution . since @xmath191 and the quadratic approximation are close , their minimizers are also close , which results in the asymptotic normality in theorem  [ t3 ] .",
    "theorem  [ t3 ] assumes that @xmath192 .",
    "since by definition @xmath193 , it is seen that the condition implies @xmath194 .",
    "this assumption is made to guarantee that the quadratic approximation is close enough to @xmath195 .",
    "when @xmath21 is finite , the condition becomes @xmath196 , as in @xcite .",
    "another important assumption is @xmath197 , which is imposed to make sure that the bias @xmath198 caused by the penalty term does not diverge .",
    "for instance , using r - lasso will create a nondiminishing bias , and thus can not be guaranteed to have asymptotic normality .",
    "note that we do not assume a parametric form of the error distribution .",
    "thus , our oracle estimator is in fact a semiparametric estimator with the error density as the nuisance parameter .",
    "heuristically speaking , theorem  [ t3 ] shows that the asymptotic variance of @xmath199 is @xmath200 . since @xmath201 and @xmath202 , if the model errors @xmath14 are i.i.d . with density function @xmath203",
    ", then this asymptotic variance reduces to @xmath204 . in the random design case where the true covariate vectors @xmath205",
    "are i.i.d .",
    "observations , @xmath206 converges to @xmath207 $ ] as @xmath208 , and the asymptotic variance reduces to @xmath209)^{-1}$ ] .",
    "this is the semiparametric efficiency bound derived by @xcite for random designs . in fact , if we assume that @xmath210 are i.i.d .",
    ", then the conditions of theorem  [ t3 ] can hold with asymptotic probability one . using similar arguments",
    ", it can be formally shown that @xmath199 is asymptotically normal with covariance matrix equal to the aforementioned semiparametric efficiency bound .",
    "hence , our oracle estimator is semiparametric efficient .",
    "in previous sections , we have seen that the choice of the weight vector @xmath123 plays a pivotal role for the wr - lasso estimate to enjoy the model selection oracle property and asymptotic normality .",
    "in fact , conditions in theorem  [ t2 ] require that @xmath211 and that @xmath212 does not diverge too fast .",
    "theorem [ t3 ] imposes an even more stringent condition , @xmath213 , on the weight vector @xmath122 . for r - lasso , @xmath214 and",
    "these conditions become very restrictive . for example , the condition in theorem  [ t3 ] becomes @xmath215 , which is too low for a thresholding level even for gaussian errors . hence",
    ", an adaptive choice of weights is needed to ensure that those conditions are satisfied . to this end",
    ", we propose a two - step procedure .    in the first step , we use r - lasso , which gives the estimate @xmath216 .",
    "as has been shown in @xcite and @xcite , r - lasso is consistent at a near - oracle rate @xmath217 and selects the true model @xmath218 as a submodel [ in other words , r - lasso has the sure screening property using the terminology of @xcite ] with asymptotic probability one , namely , @xmath219 we remark that our theorem  [ t2 ] also ensures the consistency of r - lasso .",
    "compared to @xcite , theorem  [ t2 ] presents stronger results but also needs more restrictive conditions for r - lasso .",
    "as will be shown in latter theorems , only the consistency of r - lasso is needed in the study of ar - lasso , so we quote the results and conditions on r - lasso in @xcite with the mind of imposing weaker conditions .    in the second step , we set @xmath220 with @xmath221 where @xmath222 is a folded - concave penalty function , and then solve the regularization problem ( [ eql1reg ] ) with a newly computed weight vector .",
    "thus , vector @xmath223 is expected to be close to the vector @xmath224 under @xmath27-norm .",
    "if a folded - concave penalty such as scad is used , then @xmath225 will be close , or even equal , to zero for @xmath226 , and thus the magnitude of @xmath227 is negligible .",
    "now , we formally establish the asymptotic properties of ar - lasso .",
    "we first present a more general result and then highlight our recommended procedure , which uses r - lasso as the initial estimate and then uses scad to compute the stochastic weights , in corollary  [ cor1 ] . denote by @xmath228 with @xmath229 . using the weight vector @xmath230 , ar - lasso minimizes the following objective function : @xmath231 we also need the following conditions to show the model selection oracle property of the two - step procedure .",
    "[ assp5 ] with asymptotic probability one , the initial estimate satisfies @xmath232 with some constant @xmath233 .    as discussed above ,",
    "if r - lasso is used to obtain the initial estimate , it satisfies the above condition .",
    "our second condition is on the penalty function .",
    "[ assp4 ] @xmath234 is nonincreasing in @xmath235 and is lipschitz with constant @xmath236 , that is , @xmath237 for any @xmath238 .",
    "moreover , @xmath239 for large enough  @xmath3 , where @xmath240 is defined in condition  [ assp5 ] .    for the scad [ @xcite ]",
    "penalty , @xmath241 is given by @xmath242 for a given constant @xmath243 , and it can be easily verified that condition  [ assp4 ] holds if @xmath244 .",
    "[ t4 ] assume conditions of theorem  [ t2 ] hold with @xmath245 and @xmath246 , where @xmath247 with some constant @xmath248 and @xmath249 .",
    "then , under conditions  [ assp5 ] and  [ assp4 ] , with probability tending to one , there exists a global minimizer @xmath250 of ( [ eql1reghat ] ) such that @xmath251 and @xmath252 .",
    "the results in theorem  [ t4 ] are analogous to those in theorem  [ t2 ] .",
    "the extra term @xmath253 in the convergence rate @xmath254 , compared to the convergence rate @xmath133 in theorem [ t2 ] , is caused by the bias of the initial estimate @xmath216 .",
    "since the regularization parameter @xmath154 goes to zero , the bias of ar - lasso is much smaller than that of the initial estimator @xmath255 .",
    "moreover , the ar - lasso @xmath140 possesses the model selection oracle property .",
    "now we present the asymptotic normality of the ar - lasso estimate .",
    "[ assp6 ] the smallest signal satisfies @xmath256 .",
    "moreover , it holds that @xmath257 for any @xmath258 .",
    "the above condition on the penalty function is satisfied when the scad penalty is used and @xmath259 where @xmath260 is the parameter in the scad penalty  ( [ eqscad ] ) .",
    "[ t5 ] assume conditions of theorem  [ t3 ] hold with @xmath261 and @xmath246 , where @xmath254 is defined in theorem  [ t4 ] .",
    "then , under conditions  [ assp5][assp6 ] , with asymptotic probability one , there exists a global minimizer @xmath140 of ( [ eql1reghat ] ) having the same asymptotic properties as those in theorem  [ t3 ] .    with the scad penalty ,",
    "conditions in theorems  [ t4 ] and  [ t5 ] can be simplified and ar - lasso still enjoys the same asymptotic properties , as presented in the following corollary .",
    "[ cor1 ] assume @xmath262 , @xmath263 , @xmath264 with @xmath260 the parameter in the scad penalty and @xmath265 .",
    "further assume that @xmath266 with @xmath267 some positive constant .",
    "then , under conditions  [ assp1 ] and  [ assp2 ] , with asymptotic probability one , there exists a global minimizer @xmath268 of @xmath269 such that @xmath270 if in addition , @xmath271 , then we also have @xmath272 where @xmath186 is an arbitrary @xmath21-dimensional vector satisfying @xmath187 .",
    "corollary  [ cor1 ] provides sufficient conditions for ensuring the variable selection sign consistency of ar - lasso .",
    "these conditions require that r - lasso in the initial step has the sure screening property .",
    "we remark that in implementation , lasso is able to select the variables missed by r - lasso , as demonstrated in our numerical studies in the next section .",
    "the theoretical comparison of the variable selection results of r - lasso and ar - lasso would be an interesting topic for future study .",
    "one set of @xmath273 satisfying conditions in corollary  [ cor1 ] is @xmath274 and @xmath275 with @xmath276 some constant .",
    "corollary  [ cor1 ] gives one specific choice of @xmath154 , not necessarily the smallest @xmath154 , which makes our procedure work .",
    "in fact , the condition on @xmath154 can be weakened to @xmath277 .",
    "currently , we use the @xmath27-norm @xmath278 to bound this @xmath279-norm , which is too crude .",
    "if one can establish @xmath280 for an initial estimator @xmath281 , then the choice of  @xmath154 can be as small as @xmath282 , the same order as that used in @xcite .",
    "on the other hand , since we are using ar - lasso , the choice of @xmath154 is not as sensitive as r - lasso .",
    "in this section , we evaluate the finite sample property of our proposed estimator with synthetic data .",
    "please see the supplementary material [ @xcite ] for a real life data - set analysis , where we provide results of an eqtl study on the _ chrna6 _ gene .    to assess the performance of the proposed estimator and compare it with other methods , we simulated data from the high - dimensional linear regression model @xmath283 where the data had @xmath284 observations and the number of parameters was chosen as @xmath285 .",
    "we fixed the true regression coefficient vector as @xmath286 for the distribution of the noise , @xmath287 , we considered six symmetric distributions : normal with variance 2 ( @xmath288 ) , a scale mixture of normals for which @xmath289 with probability 0.9 and @xmath290 otherwise ( @xmath291 ) , a different scale mixture model where @xmath292 and @xmath293 ( @xmath294 ) , laplace , student s @xmath295 with degrees of freedom 4 with doubled variance ( @xmath296 ) and cauchy .",
    "we take @xmath297 , corresponding to @xmath0-regression , throughout the simulation .",
    "correlation of the covariates , @xmath298 were either chosen to be identity ( i.e. , @xmath299 ) or they were generated from an ar(1 ) model with correlation 0.5 , that is @xmath300 .",
    "we implemented five methods for each setting :    1 .   _",
    "@xmath27-oracle _ , which is the least squares estimator based on the signal covariates .",
    "lasso _ , the penalized least - squares estimator with @xmath0-penalty as in @xcite .",
    "_ scad _ , the penalized least - squares estimator with scad penalty as in @xcite .",
    "r - lasso _ , the robust lasso defined as the minimizer of ( [ eql1reg ] ) with @xmath301 .",
    "_ ar - lasso _ , which is the adaptive robust lasso whose adaptive weights on the penalty function were computed based on the scad penalty using the r - lasso estimate as an initial value .",
    "the tuning parameter , @xmath154 , was chosen optimally based on 100 validation data - sets .",
    "for each of these data - sets , we ran a grid search to find the best @xmath154 ( with the lowest @xmath27 error for @xmath302 ) for the particular setting .",
    "this optimal @xmath154 was recorded for each of the 100 validation data - sets .",
    "the median of these 100 optimal @xmath154 were used in the simulation studies .",
    "we preferred this procedure over cross - validation because of the instability of the @xmath27 loss under heavy tails .",
    "the following four performance measures were calculated :    1 .",
    "@xmath27 loss , which is defined as @xmath303 .",
    "@xmath0 loss , which is defined as @xmath304 .",
    "3 .   number of noise covariates that are included in the model , that is the number of false positives ( fp ) .",
    "4 .   number of signal covariates that are not included , that is , the number of false negatives ( fn ) .    for each setting",
    ", we present the average of the performance measure based on 100 simulations .",
    "the results are depicted in tables  [ simul1 ] and  [ simul2 ] .",
    "a boxplot of the @xmath27 losses under different noise settings is also given in figure  [ figboxplot2 ] ( the @xmath27 loss boxplot for the independent covariate setting is similar and omitted ) .",
    "for the results in tables  [ simul1 ] and  [ simul2 ] , one should compare the performance between lasso and r - lasso and that between scad and ar - lasso .",
    "this comparison reflects the effectiveness of @xmath0-regression in dealing with heavy - tail distributions .",
    "furthermore , comparing lasso with scad , and r - lasso with ar - lasso , shows the effectiveness of using adaptive weights in the penalty function .",
    "@@lcd2.3d2.3d2.3cc@ & & & & & & + @xmath288 & @xmath305 loss & 0.833 & 4.114 & 3.412 & 5.342 & 2.662 + & @xmath306 loss & 0.380 & 1.047 & 0.819 & 1.169 & 0.785 + & fp , fn & & & & & 17.27 , 0.70 + @xmath291 & @xmath305 loss & 0.977 & 5.232 & 4.736 & 4.525 & 2.039 + & @xmath306 loss & 0.446 & 1.304 & 1.113 & 1.028 & 0.598 + & fp , fn & & & & & 16.76 , 0.51 + @xmath294 & @xmath305 loss & 1.886 & 7.563 & 7.583 & 8.121 & 5.647 + & @xmath306 loss & 0.861 & 2.085 & 2.007 & 2.083 & 1.845 + & fp , fn & & & & & 11.97 , 2.57 + laplace & @xmath305 loss & 0.795 & 4.056 & 3.395 & 4.610 & 2.025 + & @xmath306 loss & 0.366 & 1.016 & 0.799 & 1.039 & 0.573 + & fp , fn & & & & & 18.81 , 0.40 + @xmath307 & @xmath305 loss & 1.087 & 5.303 & 5.859 & 6.185 & 3.266 + & @xmath306 loss & 0.502 & 1.378 & 1.256 & 1.403 & 0.951 + & fp , fn & & & & & 18.53 , 0.82 + cauchy & @xmath305 loss & 37.451 & 211.699 & 266.088 & 6.647 & 3.587 + & @xmath306 loss & 17.136 & 30.052 & 40.041 & 1.646 & 1.081 + & fp , fn & & & & & 17.28 , 1.10 +    @@lcd2.3d3.3d3.3cc@ & & & & & & + @xmath288 & @xmath305 loss & 0.836 & 3.440 & 3.003 & 4.185 & 2.580 + & @xmath306 loss & 0.375 & 0.943 & 0.803 & 1.079 & 0.806 + & fp , fn & & & & & 14.49 , 0.74 + @xmath291 & @xmath305 loss & 1.081 & 4.415 & 3.589 & 3.652 & 1.829 + & @xmath306 loss & 0.495 & 1.211 & 1.055 & 0.901 & 0.593 + & fp , fn & & & & & 13.29 , 0.51 + @xmath294 & @xmath305 loss & 1.858 & 6.427 & 6.249 & 6.882 & 4.890 + & @xmath306 loss & 0.844 & 1.899 & 1.876 & 1.916 & 1.785 + & fp , fn & & & & & 7.86 , 2.71 + laplace & @xmath305 loss & 0.803 & 3.341 & 2.909 & 3.606 & 1.785 + & @xmath306 loss & 0.371 & 0.931 & 0.781 & 0.927 & 0.573 + & fp , fn & & & & & 12.90 , 0.55 + @xmath307 & @xmath305 loss & 1.122 & 4.474 & 4.259 & 4.980 & 2.855 + & @xmath306 loss & 0.518 & 1.222 & 1.201 & 1.299 & 0.946 + & fp , fn & & & & & 13.40 , 1.05 + cauchy & @xmath305 loss & 31.095 & 217.395 & 243.141 & 5.388 & 3.286 + & @xmath306 loss & 13.978 & 31.361 & 36.624 & 1.461 & 1.074 + & fp , fn & & & & & 12.45 , 1.17 +     loss with correlated covariates . ]",
    "our simulation results reveal the following facts .",
    "the quantile based estimators were more robust in dealing with the outliers .",
    "for example , for the first mixture model ( @xmath291 ) and cauchy , r - lasso outperformed lasso , and ar - lasso outperformed scad in all of the four metrics , and significantly so when the error distribution is the cauchy distribution . on the other hand , for the light - tail distributions such as the normal distribution ,",
    "the efficiency loss was limited . when the tails get heavier , for instance , for the laplace distribution , quantile based methods started to outperform the least - squares based approaches , more so when the tails got heavier .",
    "the effectiveness of weights in ar - lasso is self - evident .",
    "scad outperformed lasso and ar - lasso outperformed r - lasso in almost all of the settings .",
    "furthermore , for all of the error settings ar - lasso had significantly lower @xmath27 and @xmath0 loss as well as a smaller model size compared to other estimators .",
    "it is seen that when the noise does not have heavy tails , that is for the normal and the laplace distribution , all the estimators are comparable in terms of @xmath0 loss .",
    "as expected , estimators that minimize squared loss worked better than r - lasso and ar - lasso estimators under gaussian noise , but their performances deteriorated as the tails got heavier .",
    "in addition , in the two heteroscedastic settings , ar - lasso had the best performance among others .    for cauchy noise",
    ", least squares methods could only recover 1 or 2 of the true variables on average .",
    "on the other hand , @xmath0-estimators ( r - lasso and ar - lasso ) had very few false negatives , and as evident from @xmath27 loss values , these estimators only missed variables with smaller magnitudes .",
    "in addition , ar - lasso consistently selected a smaller set of variables than lasso .",
    "for instance , for the setting with independent covariates , under the laplace distribution , r - lasso and ar - lasso had on average 34.76 and 18.81 false positives , respectively .",
    "also note that ar - lasso consistently outperformed r - lasso : it estimated @xmath308 ( lower @xmath0 and @xmath27 losses ) , and the support of @xmath308 ( lower averages for the number of false positives ) more efficiently .",
    "in this section , we prove theorems  [ t1 ] ,  [ t2 ] and  [ t4 ] and provide the lemmas used in these proofs .",
    "the proofs of theorems  [ t3 ] and  [ t5 ] and proposition  [ prop1 ] are given in the supplementary appendix [ @xcite ] .",
    "we use techniques from empirical process theory to prove the theoretical results .",
    ". then @xmath310 . for a  given deterministic @xmath311 , define the set @xmath312 then define the function @xmath313 lemma  [ l3 ] in section  [ lemmas ] gives the rate of convergence for @xmath314 .",
    "we first show that for any @xmath315 with @xmath316 , @xmath317 \\geq\\tfrac { 1}{2}c_0cn\\bigl\\|\\bolds{\\beta } _",
    "1-\\bolds{\\beta}_1^*\\bigr\\|_2 ^ 2\\ ] ] for sufficiently large",
    "@xmath3 , where @xmath167 is the lower bound for @xmath318 in the neighborhood of @xmath319 .",
    "the intuition follows from the fact that @xmath308 is the minimizer of the function @xmath320 , and hence in taylor s expansion of @xmath321 $ ] around @xmath308 , the first - order derivative is zero at the point @xmath322 .",
    "the left - hand side of ( [ e032 ] ) will be controlled by @xmath314 .",
    "this yields the @xmath27-rate of convergence in theorem  [ t1 ] .    to prove ( [ e032 ] ) , we set @xmath323 .",
    "then , for @xmath324 , @xmath325 thus , if @xmath326 , by @xmath327 , fubini s theorem , mean value theorem and condition [ assp1 ] it is easy to derive that @xmath328\\nonumber \\\\ & & \\qquad = e\\bigl[a_i \\bigl(1\\{\\varepsilon_i\\leq a_i\\}-\\tau\\bigr)- \\varepsilon_i1\\ { 0\\leq\\varepsilon_i\\leq a_i \\}\\bigr ] \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad = e\\biggl[\\int_0^{a_i}1\\{0\\leq \\varepsilon_i\\leq s\\}\\,ds\\biggr]\\nonumber \\\\ & & \\qquad = \\int_0^{a_i } \\bigl(f_i(s)-f_i(0)\\bigr)\\,ds = \\frac{1}{2}f_i(0)a_i^2 + o(1)a_i^2,\\nonumber\\end{aligned}\\ ] ] where the @xmath329 is uniformly over all @xmath330 . when @xmath331 , the same result can be obtained . furthermore , by condition  [ assp2 ] , @xmath332 this together with ( [ e014 ] ) and the definition of @xmath333 proves ( [ e032 ] ) .    the inequality ( [ e032 ] ) holds for any @xmath334 , yet @xmath335 may not be in the set .",
    "thus , we let @xmath336 , where @xmath337 which falls in the set @xmath338 . then , by the convexity and the definition of  @xmath132 , @xmath339 using this and the triangle inequality , we have @xmath340\\nonumber \\\\ & & \\qquad   =   \\bigl\\",
    "{ v_n\\bigl(\\bolds { \\beta}^*\\bigr ) - e v_n\\bigl(\\bolds{\\beta}^*\\bigr)\\bigr\\ } -\\bigl \\{v_n(\\tilde{\\bolds{\\beta } } ) - e v_n(\\tilde{\\bolds{\\beta } } ) \\bigr\\ } \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\quad\\qquad { } + l_n(\\tilde{\\bolds{\\beta } } ) - l_n \\bigl(\\bolds { \\beta}^*\\bigr ) + n\\lambda_n\\bigl\\|\\mathbf{d } _ 0\\circ\\bolds { \\beta}_1^*\\bigr\\|_1 - n\\lambda_n\\bigl\\| \\mathbf{d}_0\\circ\\tilde{\\bolds{\\beta } } _ 1 \\bigr\\|_1\\nonumber \\\\ & & \\qquad \\leq nz_n(m ) + n\\lambda_n\\bigl\\|\\mathbf{d}_0 \\circ\\bigl(\\bolds{\\beta}_1^ * -\\tilde{\\bolds{\\beta}}_1 \\bigr ) \\bigr\\|_1.\\nonumber\\end{aligned}\\ ] ] by the cauchy ",
    "schwarz inequality , the very last term is bounded by@xmath341 .",
    "define the event @xmath342 . then by lemma  [ l3 ] , @xmath343 on the event @xmath344 , by ( [ eq16 ] ) , we have @xmath345 \\leq2m\\sqrt{sn(\\log n)}+n\\lambda_n\\| \\mathbf{d}_0\\|_2m.\\ ] ] taking @xmath346 . by condition [ assp2 ] and the assumption@xmath347 ,",
    "it is easy to check that @xmath348 . combining these two results with ( [ e032 ] )",
    ", we obtain that on the event @xmath344 , @xmath349 which entails that @xmath350 note that @xmath351 implies @xmath352 .",
    "thus , on the event  @xmath344 , @xmath353    the second result follows trivially .",
    "since @xmath132 defined in theorem  [ t1 ] is a minimizer of @xmath195 , it satisfies the kkt conditions . to prove that @xmath354 is a global minimizer of @xmath43 in the original @xmath355 space",
    ", we only need to check the following condition : @xmath356 where @xmath357 for any @xmath3-vector @xmath358 with @xmath359 . here , @xmath360 denotes the vector @xmath361 .",
    "then the kkt conditions and the convexity of @xmath362 together ensure that @xmath140 is a global minimizer of @xmath363 .",
    "define events @xmath364 where @xmath133 is defined in theorem  [ t1 ] and @xmath365 then by theorem  [ t1 ] and lemma  [ l1 ] in section  [ lemmas ] , @xmath366 . since @xmath367 on the event @xmath368 , the inequality ( [ e005 ] ) holds on the event @xmath369 .",
    "this completes the proof of theorem  [ t2 ] .",
    "the idea of the proof follows those used in the proof of theorems  [ t1 ] and  [ t2 ] .",
    "we first consider the minimizer of @xmath370 in the subspace @xmath371 .",
    "let @xmath372 , where @xmath373 with @xmath374 , @xmath375 , and @xmath376 is some large enough constant . by the assumptions in the theorem",
    ", we have @xmath377 .",
    "note that @xmath378 where @xmath379 and @xmath380 with @xmath381 for any vector @xmath358 . by the results in the proof of theorem  [ t1 ] , @xmath382 \\geq2^{-1}c_0n\\|\\tilde",
    "a_n\\mathbf{v}_1\\|_2 ^ 2 $ ] , and moreover , with probability at least @xmath138 , @xmath383\\bigr| \\leq nz_n(ca_n)\\leq2 \\tilde a_n\\sqrt{s(\\log n)n } \\|\\mathbf{v}_1 \\|_2.\\ ] ] thus , by the triangle inequality , @xmath384 the second term on the right - hand side of ( [ e042 ] ) can be bounded as @xmath385 by triangle inequality and conditions  [ assp5 ] and  [ assp4 ] , it holds that @xmath386\\\\[-8pt ] & \\leq & c_5\\bigl\\|\\hat{\\bolds { \\beta}}{}^{\\mathrm{ini}}_1-\\bolds{\\beta}^*_1 \\bigr\\|_2 + \\bigl\\| \\mathbf{d}_0^*\\bigr\\|_2 \\leq c_2 c_5\\sqrt{s(\\log p)/n}+\\bigl\\|\\mathbf{d}_0^ * \\bigr\\|_2.\\nonumber\\end{aligned}\\ ] ] thus , combining ( [ e042])([e044 ] ) yields @xmath387 making @xmath388 large enough , we obtain that with probability tending to one , @xmath389",
    ". then it follows immediately that with asymptotic probability one , there exists a minimizer @xmath390 of @xmath391 such that @xmath392 with some constant @xmath248 .",
    "it remains to prove that with asymptotic probability one , @xmath393 then by kkt conditions , @xmath394 is a global minimizer of @xmath269 .",
    "now we proceed to prove ( [ e045 ] ) .",
    "since @xmath395 for all @xmath396 , we have that @xmath397 .",
    "furthermore , by condition  [ assp5 ] , it holds that @xmath398 with asymptotic probability one .",
    "then , it follows that @xmath399 therefore , by condition  [ assp4 ] we conclude that @xmath400    from the conditions of theorem  [ t2 ] with @xmath401 , it follows from lemma  [ l1 ] [ inequality ( [ e049 ] ) ] that , with probability at least @xmath402 , @xmath403 combining ( [ e048])([e046 ] ) and by the triangle inequality , it holds that with asymptotic probability one , @xmath404 since the minimizer @xmath405 satisfies @xmath406 with asymptotic probability one , the above inequality ensures that ( [ e045 ] ) holds with probability tending to one .",
    "this completes the proof .",
    "this subsection contains lemmas used in proofs of theorems  [ t1 ] ,  [ t2 ] and  [ t4 ] .",
    "[ l3 ] under condition  [ assp2 ] , for any @xmath407 , we have @xmath408    define @xmath409 .",
    "then @xmath410 in ( [ e050 ] ) can be rewritten as @xmath411 .",
    "note that the following lipschitz condition holds for @xmath412 : @xmath413 let @xmath414 be a rademacher sequence , independent of model errors @xmath415 . the lipschitz inequality ( [ e001 ] ) combined with the symmetrization theorem and concentration inequality",
    "[ see , e.g. , theorems 14.3 and 14.4 in @xcite ] yields that @xmath416&\\leq & 2 e\\sup _",
    "{ \\bolds{\\beta}\\in\\mathcal { b}_0(m ) } \\biggl|\\frac{1}{n}\\sum_{i=1}^n w_i \\bigl(\\rho\\bigl(\\mathbf{x}_i^t\\bolds { \\beta } , y_i\\bigr ) - \\rho\\bigl(\\mathbf{x}_i^t \\bolds{\\beta}^ * , y_i\\bigr ) \\bigr ) \\biggr| \\nonumber\\\\[-8pt]\\\\[-8pt ] & \\leq & 4e\\sup_{\\bolds{\\beta}\\in\\mathcal{b}_0(m ) } \\biggl|\\frac { 1}{n}\\sum _ { i=1}^n w_i \\bigl ( \\mathbf{x}_i^t\\bolds{\\beta}- \\mathbf{x}_i^t \\bolds{\\beta}^ * \\bigr ) \\biggr|.\\nonumber\\end{aligned}\\ ] ] on the other hand , by the cauchy ",
    "schwarz inequality @xmath417 by jensen s inequality and concavity of the square root function , @xmath418 for any nonnegative random variable @xmath419 .",
    "thus , these two inequalities ensure that the very right - hand side of ( [ e033 ] ) can be further bounded by @xmath420\\\\[-8pt ] & & \\qquad \\leq m \\biggl\\{\\sum_{j=1}^s e \\biggl| \\frac{1}{n}\\sum_{i=1}^nw_ix_{ij } \\biggr|^2 \\biggr\\}^{1/2}= m\\sqrt{s / n}.\\nonumber\\end{aligned}\\ ] ] therefore , it follows from ( [ e033 ] ) and ( [ e034 ] ) that @xmath421 \\leq4m\\sqrt{s / n}.\\ ] ]    next , since @xmath422 has bounded eigenvalues , for any @xmath423 , @xmath424 combining this with the lipschitz inequality ( [ e001 ] ) , ( [ e051 ] ) and applying massart s concentration theorem [ see theorem 14.2 in @xcite ] yields that for any @xmath425 , @xmath408 this proves the lemma .    [ l1 ]",
    "consider a ball in @xmath426 around @xmath427 with some sequence @xmath428 .",
    "assume that@xmath429 , @xmath430 , @xmath431 , and @xmath432 .",
    "then under conditions  [ assp1][assp3 ] , there exists some constant @xmath127 such that @xmath433 where @xmath434 .    for a fixed @xmath435 and @xmath436 , define @xmath437 \\bigr],\\ ] ] where @xmath438 is the @xmath439th row of the design matrix .",
    "the key for the proof is to use the following decomposition : @xmath440 \\biggr\\|_\\infty \\\\ & & \\quad\\qquad { } + \\biggl\\|\\frac{1}{n}\\mathbf{q}^t\\rho_\\tau ' ( \\bolds{\\varepsilon } ) \\biggr\\|_\\infty+\\max_{j > s}\\sup _ { \\bolds{\\beta}\\in\\mathcal{n}}\\frac{1}{n}\\sum_{i=1}^n \\bigl|\\gamma_{\\bolds{\\beta},j}(\\mathbf{x}_i , y_i ) \\bigr|.\\nonumber\\end{aligned}\\ ] ] we will prove that with probability at least @xmath402 , @xmath441 \\biggr\\| _",
    "\\infty < \\frac{\\lambda_n}{2\\|\\mathbf { d}_1^{-1}\\|_\\infty}+ o(\\lambda _ n),\\label{e049 } \\\\",
    "i_2&\\equiv & n^{-1}\\bigl\\|\\mathbf{q}^t \\rho_\\tau'(\\bolds{\\varepsilon})\\bigr\\| _ \\infty= o ( \\lambda_n),\\label{e013 } \\\\",
    "i_3 & \\equiv&\\max_{j > s}\\sup_{\\bolds{\\beta}\\in\\mathcal{n } } \\biggl|\\frac { 1}{n}\\sum_{i=1}^n \\gamma_{\\bolds{\\beta},j}(\\mathbf{x}_i , y_i ) \\biggr| = o_p(\\lambda_n).\\label{e038}\\end{aligned}\\ ] ] combining ( [ e039])([e038 ] ) with the assumption @xmath429 completes the proof of the lemma .",
    "now we proceed to prove ( [ e049 ] ) .",
    "note that @xmath442 can be rewritten as @xmath443 \\biggr|.\\ ] ] by condition  [ assp1 ] , @xmath444 \\\\ & & \\qquad = f_i\\bigl(\\mathbf{s}_i^t \\bigl(\\bolds{\\beta}_1-\\bolds{\\beta}_1^*\\bigr)\\bigr ) - f_i(0)= f_i(0)\\mathbf{s } _",
    "i^t \\bigl(\\bolds{\\beta}_1 - \\bolds{\\beta}_1^*\\bigr)+ \\widetilde i_{i},\\end{aligned}\\ ] ] where @xmath445 is the cumulative distribution function of @xmath446 , and @xmath447 .",
    "thus , for any @xmath448 , @xmath449&= & \\sum_{i=1}^n \\bigl(f_i(0)x_{ij}\\mathbf{s } _",
    "i^t \\bigr ) \\bigl(\\bolds{\\beta}_1-\\bolds{\\beta}_1^*\\bigr ) + \\sum_{i=1}^n x_{ij}\\widetilde i_{i}.\\end{aligned}\\ ] ] this together with ( [ e019 ] ) and cauchy  schwarz inequality entails that @xmath450 where @xmath451 .",
    "we consider the two terms on the right - hand side of ( [ e016 ] ) one by one . by condition  [ assp3 ] ,",
    "the first term can be bounded as @xmath452 by condition  [ assp1 ] , @xmath453 .",
    "this together with condition  [ assp2 ] ensures that the second term of ( [ e016 ] ) can be bounded as @xmath454 since @xmath455 , it follows from the assumption @xmath456 that @xmath457 plugging the above inequality and ( [ e017 ] ) into ( [ e016 ] ) completes the proof of ( [ e049 ] ) .",
    "next , we prove ( [ e013 ] ) . by hoeffding s inequality ,",
    "if @xmath458 with @xmath167 is some positive constant , then @xmath459 thus , with probability at least @xmath460 , ( [ e013 ] ) holds .",
    "we now apply corollary 14.4 in @xcite to prove  ( [ e038 ] ) . to this end , we need to check conditions of the corollary . for each fixed @xmath46 , define the functional space @xmath461 . first note that @xmath462=0 $ ] for any @xmath463 .",
    "second , since the @xmath464 function is bounded , we have @xmath465 thus , @xmath466 .",
    "third , we will calculate the covering number of the functional space @xmath467 , @xmath468 . for any @xmath469 and @xmath470 , by condition  [ assp1 ] and the mean value theorem , @xmath471-e \\bigl[\\rho_\\tau ' \\bigl(y_i-\\mathbf{x}_i^t\\tilde{\\bolds { \\beta}}\\bigr)-\\rho_\\tau'(\\varepsilon_i ) \\bigr]\\nonumber \\\\ & & \\qquad = f_i\\bigl(\\mathbf{s}_i^t\\bigl ( \\tilde{\\bolds{\\beta}}_{1}-\\bolds{\\beta}_1^*\\bigr ) \\bigr)-f_i\\bigl(\\mathbf{s } _ i^t\\bigl(\\bolds { \\beta}_{1}-\\bolds{\\beta}_1^*\\bigr)\\bigr ) \\\\ & & \\qquad = f_i(a_{1i})\\mathbf{s}_i^t ( \\bolds{\\beta}_1 - \\tilde{\\bolds{\\beta}}_1),\\nonumber\\end{aligned}\\ ] ] where @xmath445 is the cumulative distribution function of @xmath446 , and @xmath472 lies on the segment connecting @xmath473 and @xmath474 .",
    "let @xmath475 . since @xmath106 s are uniformly bounded , by ( [ e036 ] ) , @xmath476-x_{ij}e \\bigl[\\rho_\\tau ' \\bigl(y_i-\\mathbf{x}_i^t\\tilde{\\bolds { \\beta}}\\bigr)-\\rho_\\tau'(\\varepsilon_i)\\bigr]\\bigr|\\nonumber \\\\ & & \\qquad \\leq c\\bigl|x_{ij}\\mathbf{s}_i^t(\\bolds { \\beta}_1-\\tilde{\\bolds{\\beta}}_1)\\bigr| \\\\ & & \\qquad \\leq c \\|x_{ij}\\mathbf{s}_i\\|_2\\|\\bolds { \\beta}_1-\\tilde{\\bolds{\\beta}}_1\\|_2 \\leq c \\sqrt{s}\\kappa_n^2\\|\\bolds{\\beta}_1-\\tilde { \\bolds{\\beta}}_1\\|_2,\\nonumber\\end{aligned}\\ ] ] where @xmath376 is some generic constant .",
    "it is known [ see , e.g. , lemma 14.27 in @xcite ] that the ball @xmath477 in @xmath478 can be covered by @xmath479 balls with radius @xmath480 .",
    "since @xmath481 can only take 3 different values @xmath482 , it follows from ( [ e018 ] ) that the covering number of @xmath467 is @xmath483 .",
    "thus , by calculus , for any @xmath484 , @xmath485 hence , conditions of corollary 14.4 in @xcite are checked and we obtain that for any @xmath407 , @xmath486 taking @xmath487 with @xmath376 large enough constant , we obtain that @xmath488 thus , if @xmath430 , then with probability at least @xmath402 , ( [ e038 ] ) holds .",
    "this completes the proof of the lemma .",
    "the authors sincerely thank the editor , associate editor , and three referees for their constructive comments that led to substantial improvement of the paper ."
  ],
  "abstract_text": [
    "<S> heavy - tailed high - dimensional data are commonly encountered in various scientific fields and pose great challenges to modern statistical analysis . a  natural procedure to address this problem is to use penalized regression with weighted @xmath0-penalty , called weighted robust lasso , in which weights are introduced to ameliorate the bias problem induced by the @xmath0-penalty . in the ultra - high dimensional setting , where the can grow exponentially with the sample size , we investigate the model selection oracle property and establish the asymptotic normality of the wr - lasso . </S>",
    "<S> we show that only mild conditions on the model error distribution are needed . </S>",
    "<S> our theoretical results also reveal that adaptive choice of the weight vector is essential for the wr - lasso to enjoy these nice asymptotic properties . to make the wr - lasso practically feasible </S>",
    "<S> , we propose a two - step procedure , called adaptive robust lasso ( ar - lasso ) , in which the weight vector in the second step is constructed based on the @xmath0-penalized quantile regression estimate from the first step . </S>",
    "<S> this two - step procedure is justified theoretically to possess the oracle property and the asymptotic normality . </S>",
    "<S> numerical studies demonstrate the favorable finite - sample performance of the ar - lasso .    , </S>"
  ]
}