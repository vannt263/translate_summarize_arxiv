{
  "article_text": [
    "this work stems from an interest in bayesian portfolio decision problems with long - term , multi - step investment objectives that lead to the need for computational methods for portfolio optimization .",
    "methodological advances reflect the fact that some such optimization problems can be recast purely technically as problems of computing modes of marginal posterior distributions in synthetic \" statistical models .",
    "we then have access to analytic and computational machinery for exploring posterior distributions whose marginal modes represent target optima in originating optimization / decision problems .",
    "we refer to this as _ bayesian emulation for decisions _",
    ", with the synthetic statistical model regarded as an emulating framework for computational reasons .",
    "the use of decision analysis for portfolios coupled with dynamic models for forecasting financial time series continues to be a very active area of bayesian analysis in research and in broad application in personal and corporate gambling on markets of all kinds .",
    "forecasting with multivariate dynamic linear / volatility models coupled with extensions of traditional markowitz mean - variance optimization  @xcite define benchmark approaches  ( e.g. @xcite , chapter 10 of @xcite , @xcite , among others ) .",
    "much recent work has emphasised advances in forecasting ability based on increasingly structured multivariate models  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) with benefits in portfolio outcomes based , in part , on improved characterizations of dynamics in multivariate stochastic volatility",
    ". however , relatively little bayesian work addresses interests in more relevant utility / loss functions , especially in longer - term , multi - step portfolio contexts ; much of the cited work here employs standard myopic / one - step ahead decision rules .",
    "our emphasis is to complement these time series forecasting advances with bayesian decision analysis that explicitly reflects personal or commercial utilities for stable portfolios in a multi - step context .",
    "in stylized forecasting and decision problems , analysis involves computing portfolio weight vectors to minimize expected portfolio loss functions , and to repeatedly apply this sequentially over time .",
    "the solutions can be approximated numerically in a number of ways , depending on the form of the loss function , but typically need customization of the numerical techniques .",
    "the approach here emerging naturally in the specific context of multi - step portfolios is a general approach applicable to a variety of loss functions . at any one time point with",
    "decision variable @xmath0 and expected loss function @xmath1 , the bayesian emulation strategy is useful if / when there exists a purely synthetic statistical model involving hypothetical random vectors ( parameters , latent variables or states ) @xmath2 and generating a posterior density @xmath3 under which the marginal model of @xmath4 is theoretically equal to the optimal @xmath0 in the portfolio decision . minimizing @xmath1 can then be approached by exploring @xmath3 with standard analytic and numerical methods for posterior analysis . while novel in terms of our context and development , the basic idea here goes back ( at least ) to  @xcite . there , with discrete decision variables in non - sequential design contexts , optimization is solved using a similar synthetic posterior idea and combining optimization with estimation using mcmc .",
    "this approach has , surprisingly , seen limited development , although recent work by  @xcite represents extension and new application .",
    "our work takes a broader emulating perspective with complete separation of models / forecasting and decisions / optimization .",
    "we develop emulation of portfolio decisions using forecast information from state - of - the - art multivariate dependency network models  @xcite , treated as given .",
    "we then define the new multi - step decision strategy for computing and revising bayesian portfolios over time based on these forecasts .",
    "section  [ sec : dlm ] summarizes the multi - step portfolio set - up in sequential forecasting . to define and exemplify the emulation approach , we give summary details of its use in multi - step portfolios with extensions of standard ( myopic , constrained ) quadratic loss functions . here",
    "the emulating synthetic statistical models are conditionally linear and normal state - space models , i.e. , dynamic linear models ( dlms ) , amenable to evaluation using analytic forward filtering and backward smoothing ( ffbs ) methods .",
    "this is extended in section  [ sec : lasso ] to a class of portfolios with sparsity - inducing penalties on portfolio weights and turnover .",
    "the emulating models here also have state - space forms , but now with non - normal structure . with augmented state - spaces , we can convert these to conditional dlms in which posterior evaluation and mode search are efficiently performed by combining ffbs with a customized em method .",
    "section  [ sec : marginal ] discusses a fundamental question of definition of portfolio loss functions and objectives in multi - step contexts , and a strategy for marginal mode evaluation .",
    "a range of portfolio loss functions are then evaluated in sequential forecasting and portfolio construction with a @xmath5dimensional series of daily fx , commodity and market index prices .",
    "section  [ sec : ex ] discusses this , highlighting choices of portfolio loss functions and objectives , and practical benefits arising with sparsity - inducing , multi - step portfolio strategies .",
    "the latter shows the potential to improve portfolio outcomes , particularly in the presence of realistic transaction costs .",
    "comments in section  [ sec : conc ] conclude the main paper .",
    "appendices provide technical details on optimization and on dynamic dependency network models used for forecasting .    * notational remarks * : we use @xmath6 for a generic density of @xmath7 given @xmath8 normal , exponential and gamma distributions are written as @xmath9 @xmath10 with mean @xmath11 and @xmath12 with shape @xmath13 and mean @xmath14 ; the values of their density functions at a particular @xmath7 are denoted by @xmath15 and @xmath16 respectively .",
    "indices @xmath17 for @xmath18 are shortened as @xmath19 .",
    "the @xmath20-dimensional all - ones and all - zeros vectors are @xmath21 and @xmath22 , respectively , and @xmath23 represents a zero vector or matrix when dimensions are obvious .",
    "over times @xmath24 we observe a @xmath25vector asset price time series @xmath26 ; the returns vector @xmath27 has elements @xmath28 @xmath29 at time @xmath30 with current information set @xmath31 , a model defines a forecast distribution for returns at the next @xmath32 time points . with no loss of generality and to simplify notation , take current time @xmath33 with initial information set @xmath34 predicting ahead , the predictive mean vectors and precision ( inverse variance ) matrices are denoted by @xmath35 $ ] and @xmath36^{-1}$ ] over the @xmath37steps ahead @xmath38 the time @xmath30 portfolio weight vector @xmath39 has elements @xmath40 some of which may be negative reflecting short - selling .",
    "standing at @xmath33 with a current , known portfolio @xmath41 stylized myopic ( one - step ) markowitz analyses are bayesian decision problems focused on choosing @xmath42 subject to constraints .",
    "standard mean - variance portfolios minimize @xmath43 subject to a chosen expected return _",
    "@xmath44 and usually a sum - to - one constraint @xmath45 , i.e. , allowing only portfolios closed to draw - down or additional investment .    for multi - step portfolios , extend to consider the sequence of potential portfolio vectors @xmath46 over the next @xmath32 periods .",
    "the decision is to choose @xmath47 but we are interested in target returns and portfolio turnover control over multiple steps , and so must consider how the decision analysis might play - out up to time @xmath48 consider multi - step ( expected ) loss functions of the form @xmath49 where @xmath50 , @xmath51 and @xmath52 are specified positive weights defining relative contributions of the terms in this sum , while @xmath53 is the ( least - norm ) generalized inverse of a specified @xmath54 positive - semi - definite matrix @xmath55 , and will be the usual inverse in cases of positive - definiteness .",
    "also , @xmath56 now includes the current portfolio vector @xmath57    the first set of terms in the sum involve specified multi - step target returns @xmath58 .",
    "individual investors typically prefer realized portfolios to progress relatively smoothly towards an end - point target @xmath59 , rather than bouncing from high to low interim returns .",
    "the weights @xmath50 can be used to increasingly emphasize the importance of later - stage returns as @xmath30 approaches @xmath60 note that allowing @xmath61 theoretically implies the hard constraint on expected return , @xmath62 as in the standard myopic case .",
    "hence we refer to   as including soft target constraints ,  while having the ability to enforce the hard constraint at the terminal point via sending @xmath63 to zero .",
    "the second set of terms in   penalize portfolio uncertainty using the standard risk measures @xmath64 , again allowing differential weighting as a function of steps - ahead @xmath30 .",
    "the final set of terms relates to portfolio turnover . if @xmath65 these terms penalize changes in allocations across all assets .",
    "if trades are at a fixed rate , this is a direct transaction cost penalty ; otherwise , it still relates directly to transactions costs and so that terminology will be used . with a heavy emphasis on these terms as defined by the @xmath52 weights optimal portfolios will be more stable over time , providing less stress on investors ( including emotional as well as workload stress for individual investors ) . the @xmath55 can play several constraint - related roles , as we discuss below .",
    "there are , of course , no new computational challenges to simple quadratic optimization implied by  .",
    "key points are that it is easy to : ( i ) compute the joint optimizing values @xmath46 , and ( ii ) deduce the one - step optimizing @xmath42 for the bayesian decision .",
    "optimization with respect to @xmath42 alone can be immediately performed using a forward - backward dynamic programming algorithm .",
    "importantly , the optimizing value for @xmath42 ( or for any subset of the @xmath39 ) is as a result of the quadratic nature of   precisely that sub - vector ( or subset of vectors ) arising at the global / joint maximizer @xmath66    the emulation idea translates the above concepts into a synthetic bayesian model immediately interpretable by statisticians .",
    "rewrite   as @xmath67 where each @xmath68 term is a specific normal p.d.f .",
    ", the @xmath69 are interpreted as random quantities in a multivariate normal distribution underlying this density form , and where each @xmath70 is set at @xmath71 specifically , consider a dynamic linear model ( dlm ) generating pairs of observations @xmath72 with @xmath73 scalar and @xmath70 a @xmath25vector based on latent @xmath25vector states @xmath39 via @xmath74 with a known initial state ( the current portfolio ) @xmath75 and where the @xmath76 are independent and mutually independent innovations sequences . in this model",
    ", observing the sequence of synthetic observations @xmath77 with @xmath78 immediately implies the resulting posterior @xmath79 as given in  .",
    "observe that computing the minimizer of @xmath80 is equivalent to calculating the posterior mode for @xmath46 in the synthetic dlm .",
    "it is immediate that the required ( marginal ) optimizing value for @xmath42 is the marginal mode in this joint posterior . since the joint posterior is normal",
    ", marginal modes coincide with values at the joint mode , so we can regard the bayesian optimization as solved either way .",
    "we easily compute the mode of @xmath42 using the forward filtering , backward smoothing ( ffbs ) algorithm akin to a viterbi - style optimization algorithm  ( ( e.g .",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) )  widely used in applications of dlms  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "as noted above , some applications may desire a hard target @xmath59 at the terminal point , and this is formally achieved by setting the synthetic variance @xmath81 in  .",
    "the multivariate normal posterior is singular due to the resulting constraint @xmath82 , but this raises no new issues as the ffbs computations apply directly .",
    "the general framework also applies with singular matrices @xmath55 , now playing the roles of the variance matrices of state innovations in  .",
    "these arise to enforce linear portfolio constraints @xmath83 where @xmath13 is a given @xmath84vector and @xmath85 is a full - rank @xmath86 matrix with @xmath87 choose @xmath75 to satisfy these constraints and ensure that each @xmath55 is such that @xmath88 . then the priors and posteriors for the synthetic states @xmath39 are singular and constrained such that @xmath83 ( almost surely ) . again the ffbs analysis applies directly to generate the optimal portfolio vector @xmath42 and the sequence of interim optimizing values @xmath46 even though only @xmath42 is used at @xmath33 .",
    "this now involves propagating singular normal posteriors for states , as is standard in , for example , constrained seasonal dlms  ( e.g. * ? ? ?",
    "a key portfolio case is the sum - to - one constraint @xmath89 for all @xmath90 here we redefine @xmath55 beginning with the identity @xmath91 representing equal and independent penalization of turnover across assets and then condition on the constraints to give rank @xmath92 matrices @xmath93",
    "now consider modifications to ( i ) more aggressively limit switching in / out of specific assets between time points for both transaction and psychological cost considerations , and to ( ii ) limit the numbers of assets invested at any time point .",
    "several authors have considered absolute loss / penalties to encourage shrinkage - to - zero of optimizing portfolio vectors  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) and we build on this prior work .",
    "key points , however , are that such approaches have not been consistent with a bayesian decision analysis framework , while goals with respect to marginal versus joint optimization in the multi - step context have been poorly understood and explored , and require clarification .",
    "our fully bayesian emulation strategy adds to this literature while also clarifying this critical latter point and defining relevant methodology .",
    "the laplace loss `` terminology relates to novel synthetic statistical models that emulate portfolio optimization with absolute norm terms to penalize portfolio weight changes .",
    "modify   to the form @xmath94 where the final term now replaces the quadratic score with the sum of absolute changes of asset weights @xmath95 relative to  , this aims to more aggressively limit transaction costs , both monetary and psychological .",
    "optimizing globally over @xmath46 may / will encounter boundary values in which some portfolio weights are unchanged between times @xmath96 and @xmath90 this theoretical lasso - style fact is one reason for the interest in such loss functions , due to the implied expectation of reduced portfolio turnover or churn'' and hence reduced costs .      in parallel to section  [ sec : dlmmodel",
    "] , we identify a synthetic statistical model again a state - space model but now with non - normal evolution / transition components for the synthetic latent states @xmath39 of the form @xmath97 where @xmath98 denotes the laplace ( double exponential ) distribution the p.d.f . for each element",
    "is @xmath99 . also , the @xmath100 are independent and mutually independent across the ranges of all suffices",
    ".    one of the immediate benefits of the bayesian emulating model approach is that we can exploit latent variable constructs . in particular here , the laplace distributions are known to be scale mixtures of normals  @xcite .",
    "thus , there exist latent random quantities @xmath101 , @xmath102 such that @xmath103 independently over @xmath104 , and based on which each synthetic state evolution in   has the form @xmath105 augmenting by the vectors of latent scales @xmath106 , the evolutions in   become @xmath107 this defines a conditionally normal dlm and the above / standard ffbs algorithm can be used to evaluate the posterior mode of @xmath108 for any @xmath109 including that at zero . to maximize over portfolios @xmath110 in the implied marginal with respect to @xmath111 , bayesian",
    "em  ( e.g. * ? ? ?",
    "* ) is the obvious and easily implemented approach . here",
    "the e - step applies to the latent @xmath112 , while ffbs gives the exact m - step for @xmath46 at each iterate . in summary :    1 .",
    "initialization : set each @xmath113 arbitrarily .",
    "candidates for initial values are the current @xmath75 , or the trivially computed values that optimize the multi - step portfolios under the quadratic loss of section  [ sec : dlm ] .",
    "2 .   for em",
    "iterates @xmath114 under a chosen stopping rule , repeat the following : . *",
    "_ e - step : _ for @xmath115 and @xmath116 update @xmath117 via @xmath118 to give a new matrix @xmath119 * _ m - step : _ implement ffbs for the emulating model of   and ( [ eq : ldlmz ] ) at @xmath120 and with augmented evolution in .",
    "this yields the exact mode @xmath121 of the synthetic posterior conditional on current @xmath122    on stopping at iterate @xmath123 , use @xmath124 as the approximate optimizing portfolio vector .",
    "the addition of linear constraints modifies the @xmath55 matrices with details extending those of the normal model in section  [ sec : constraints ] .",
    "write @xmath125 .",
    "then for the full - rank set of @xmath126 constraints @xmath83 , the diagonal @xmath127 is replaced in   by singular @xmath128 in the key special case of sum - to - one constraints @xmath129 for all @xmath30 , this reduces to @xmath130      in the one - step , myopic context , penalizing portfolio variance @xmath131 with a term proportional to @xmath132 is an obvious strategy towards the goal of inducing shrinkage to zero of optimized portfolio weights .",
    "as noted earlier , a number of recent works have introduced such a lasso - style penalty directly on portfolio weights , rather than on changes in weights , and with standard convex optimization algorithms for solution  ( e.g. * ? ? ?",
    "* ) and demonstrating improved portfolio performance in some cases  ( e.g. * ? ? ?",
    "we now integrate such penalties as components of a more general class loss function embedded in the multi - step framework , and develop the bayesian emulation methodology for this novel context .",
    "the shrinkage - inducing penalty @xmath133 aims to drive some subset of weights to zero exactly in the one - step , myopic context when balanced only by portfolio risk .",
    "a key point to note is that , when the portfolio vector is also subject to the sum - to - one constraint , then the combined loss function also more aggressively penalizes negative weights , i.e. , short positions , and so is particularly of interest to personal investors and institutional funds that generally adopt long positions .",
    "that is , the absolute weight penalty operates as a soft constraint towards non - negative weights . in our broader context below",
    ", this does not theoretically imply non - negative optimal weights , but does often yield such solutions . modify   to the form @xmath134 with weights @xmath135 on the new absolute loss terms at each horizon @xmath136 extending the latent variable construction of double exponential distributions to these terms in addition to the turnover terms , we now see that optimizing   is equivalent to computing the mode over states @xmath46 in a correspondingly extended synthetic dlm .",
    "this emulating model is : @xmath137 with synthetic observations @xmath73 ( scalar ) and @xmath138 ( @xmath25vectors ) , and where latent scales @xmath139 are augmented with additional terms @xmath140 for each @xmath90 conditioning on @xmath141 converts the laplace term @xmath142 to a conditional normal . to incorporate exact linear constraints on each @xmath143 the above",
    "is modified only through the implied changes to the @xmath55 ; this is precisely as detailed at the end of section  [ sec : lassomodel ] above .",
    "extension of the ffbs / em algorithm of section  [ sec : lassomodel ] provides for computation of the optimizing @xmath144 .",
    "each e - step now applies to the latent @xmath145 as well as @xmath112 , while the m - step applies as before to @xmath46 at each iterate .",
    "following initialization at @xmath146 , the earlier details of iterates @xmath114 are modified as follows :    * _ e - step : _ * * update the @xmath117 via @xmath147 to give a new matrix @xmath119 * * update the @xmath141 via @xmath148 to give a new matrix @xmath149 * _ m - step : _ ffbs applied to the extended emulating model   yields the exact mode @xmath121 of the synthetic posterior conditional on current @xmath150    the resulting @xmath124 defines the optimizing portfolio vector .",
    "in multi - step portfolio analysis , the decision faced at time @xmath33 is to choose @xmath42 only .",
    "the future weights @xmath151 are involved in the initial specification of the _ joint _ loss function @xmath80 in order to weigh expected fluctuations in risk and costs up to the target horizon @xmath48 from the viewpoint of bayesian decision theory , this is perfectly correct in the context of the actual decision faced if the approach is understood to be minimizing @xmath152 joint optimization over @xmath46 to deliver the actionable vector @xmath42 is bayesian decision analysis with this implied loss as a function of @xmath42 alone .",
    "the emulation framework provides an approach to computation , but also now suggests an alternative loss specification . with emulating synthetic joint density @xmath153 , minimizing the loss @xmath154 above is equivalent to _ profiling out _ the future hypothetical vectors @xmath151 by conditioning on their ( joint ) modal values .",
    "it is then natural to consider the alternative of _ marginalization _ over @xmath151 ; that is , define the implied _ marginal _ loss function @xmath155 as @xmath156 call @xmath154 the _ profiled loss function _ and @xmath155 the _ marginal loss function_.    in general , the resulting optimal vectors @xmath157 ( profiled ) and @xmath158 ( marginal ) will differ .",
    "a key exception is the case of the quadratic loss function and normal synthetic models of section  [ sec : dlm ] where the joint posterior @xmath153 is multivariate normal . in that case ,",
    "joint modes are joint means , whose elements are marginal means , i.e. , @xmath159 the situation is different in cases of non - normal emulating models , such based on the laplace forms .",
    "these are now considered further for comparisons of marginal and profile approaches .",
    "return to the laplace loss framework of sections  [ sec : laplacebasic ] and [ sec : lassomodel ] ( i.e. , the extended laplace context with @xmath160 ) with sum - to - one constraints . here the key issues of profiled versus marginal losses are nicely illustrated .",
    "similar features arise in the extended laplace loss context of section  [ sec : nonneg ] , but with no new conceptual or practical issues so details of that extension are left to the reader .",
    "the ffbs / em algorithm easily computes the optimal profile portfolio @xmath161 but it does not immediately extend to evaluating the optimal marginal portfolio @xmath158",
    ". of several approaches explored , the most useful is based on markov chain monte carlo ( mcmc ) analysis of the synthetic dlm , coupled with iterative , gradient - based numerical evaluation of the mode of the resulting monte carlo approximation to the required marginal density function .",
    "summary details are given here and further explored in application in section  [ sec : ex ] .",
    "the density @xmath162 is the @xmath42 margin under the full joint posterior of @xmath163 where @xmath164 is the vector of @xmath165step ahead latent scales .",
    "the ffbs / em approach is enabled by the nice analytic forms of implied conditional posteriors ; these also enable mcmc analysis in this conditionally normal dlm with uncertain scale factors .",
    "this approach is nowadays standard and easily implemented  ( e.g. ( * ? ? ?",
    "* chapt .",
    "15 ) ; ( * ? ?",
    "now the ffbs is exploited to generate _",
    "backward sampling _",
    ", rather than the backward smoothing that evaluates posterior modes . at each mcmc iterate , ffbs applies to _ simulate _ one draw of the full trajectory of states @xmath46 from the retrospective posterior @xmath166 conditional on current values of the latent scales .",
    "then , conditional on this state trajectory , the conditional posterior @xmath167 is simulated to draw a new sample of the latent scales . in the emulating model of   this second step",
    "involves a set of conditionally independent univariate draws , each from a specific gig ( generalized inverse gaussian ) distribution . applying the sum - to - one constraint on each @xmath39",
    "vector changes this structure for the @xmath168 however , and direct sampling of the @xmath117 is then not facile . to address this ,",
    "we define a metropolis - hastings extension for these elements to allow use of the constraint .",
    "summary details of this , and of mcmc convergence diagnostics related to the real - data application in section  [ sec : ex ] , are given in appendix  [ sec : appm ] .",
    "the mcmc generate samples indexed by superscript @xmath169 , @xmath170 for some chosen sample size @xmath171 the rao - blackwellized monte carlo approximation to the required margin for @xmath42 is then @xmath172 importantly , this is the density of a mixture of @xmath173 normals : each conditional @xmath174 in the sum is the implied normal margin in the dlm defined by conditioning values of latent scales , with moments trivially computable via ffbs ( using backward smoothing ) , and the density values are easily evaluated at any @xmath175 thus the portfolio optimization problem reduces to mode - finding in a mixture of multivariate normals , and there are a number of numerical approaches to exploit for this .",
    "the most effective is really one of the simplest a newton - type updating rule based on the first order derivative of the density , with repeat searches based on multiple initial values for numerical iterates .",
    "relevant candidate initial values can be generated by evaluating the mixture at each of the normal component means , and selecting some of those with highest mixture density .",
    "further details are noted in appendix  [ sec : appm ] .",
    "evaluation of multi - step portfolios uses data on daily returns of @xmath176 financial series : exchange rates of 10 international currencies ( fx ) relative to the us dollar , two commodities and two asset market indices ; see table  [ tab : list ] .",
    "the time series runs from august 8 , 2000 to december 30 , 2011 . an initial period of this data",
    "is used for exploratory analysis , followed by formal sequential filtering using a multivariate dynamic model , as noted below .",
    "the main interest in portfolio evaluation is then explored over the period of @xmath177 days from january 1 , 2009 to december 30 , 2011 .",
    "lclc names & symbol & names & symbol +   + australian dollar & aud & swiss franc & chf + euro & eur & british pound & gbp + japanese yen & jpy & new zealand dollar & nzd + canadian dollar & cad & norwegian kroner & nok + south african rand & zar & oil price & oil + gold & gld & nasdaq index & nsd + s&p index & s&p & & +      forecasts are generated from a time - varying , vector auto - regressive model of order 2  ( tv - var(2 ) , e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) , with dynamic dependence network structure  ( ddn , * ? ? ?",
    ". exploratory analysis of the first 500 observations is used to define the sparsity structure of the dynamic precision matrix for the tv - var innovations , i.e. , a sparse representation of multivariate volatility , following examples in the above references . from day 501",
    ", the analysis is run sequentially in time , updating and forecasting each day .",
    "the ddn structure enables analytic filtering and one - step forecasting ; forecasting multiple steps ahead in a tv - var with ddn structure is performed by direct simulation into the future . for each day",
    "@xmath30 during the investment period , the model generates multiple - step ahead forecast mean vectors and variance matrices , @xmath178 , given as monte carlo averages of @xmath179 forecast trajectories of the return vectors @xmath180 simulated at time @xmath90 we take @xmath181 days as the portfolio horizon , and reset the time index so that @xmath182 represents the start of the investment period , january 1 , 2009 . appendix  [ sec : ddn ] provides detailed discussion of the ddn model , use of exploratory training data , filtering and simulation - based forecasting .",
    "comparisons use various values of portfolio parameters in the quadratic / normal and laplace loss frameworks . in all cases ,",
    "we take the target return schedule @xmath58 to be constant , with @xmath183 representing daily return targets of @xmath184 , annualized ( 261 trading days ) to about @xmath185 then , we have @xmath186 for @xmath187 rather than strictly enforcing the constraint by @xmath188 so that the interim targets are soft \" rather than strictly enforced . the initial portfolio @xmath75 is the myopic markowitz portfolio for comparison .",
    "parameters @xmath189 define relative weights of the four components of loss . in a long - term context ( e.g. , when @xmath30 indexes months or more ) some use of discounting into the future becomes relevant .",
    "for example , we may take @xmath190 may be chosen to increase with @xmath30 , but @xmath50 to decrease with @xmath30 to more aggressively target the soft targets as @xmath30 approaches @xmath191 given the accurate and reliable long - term predictions . in short - term contexts , such as with our @xmath192day context ,",
    "this is not relevant , so we take constant weights @xmath193 setting @xmath194 loses no loss of generality , as the remaining three weights are relative to @xmath195 examples use various values of @xmath196 to highlight their impact on portfolio outcomes .",
    "larger values of @xmath197 reduce the penalty for risk in terms of overall portfolio variance ; larger values of @xmath52 leads to more volatile portfolio dynamics due to reduced penalties on transaction costs ; and larger values of @xmath198 reduce shrinkage of portfolio weights , also relaxing the penalties on shorting .",
    "portfolios are compared in several ways , including realized returns . with a fixed transaction cost of @xmath199 time",
    "@xmath30 optimal portfolio vector @xmath39 and realized return vector @xmath200 cumulative return @xmath201 from the period @xmath202 is @xmath203 in our examples , we compare cases with @xmath204 and @xmath205 we also compare our multi - step portfolios with the standard one - step / myopic markowitz approach naturally expected to yield higher cumulative returns with no transaction costs as it then generates much more volatile changes in portfolio weights day - to - day .",
    "our portfolios constraining turnover are naturally expected to improve this in terms of both stability of portfolios and cumulative return in the presence of practically relevant , non - zero @xmath206 .",
    "additional metrics of interest are portfolio risk  as traditionally measured by the realized portfolio standard deviations @xmath207 , and patterns of volatility in trajectories of optimized portfolio weights over time .",
    "first examples use the normal loss framework of section  [ sec : dlm ] with @xmath208 figure  [ fig : dlm ] shows trajectories over time of optimized portfolio weight vectors using @xmath209 and @xmath210 as well as those from the standard , myopic markowitz analysis that corresponds to @xmath211 we see the increased smoothness of changes as @xmath212 decreases ; at @xmath213 the trajectories ( not shown ) are almost constant .    figure  [ fig : dlmr ] plots trajectories of cumulative returns for three normal loss portfolios @xmath214 and 10,000 ) and for the markowitz analysis .",
    "markowitz and larger @xmath212 normal loss portfolios performs best in this metric with no transaction costs ; but the markowitz approach is very substantially out - performed by the smoother , multi - step portfolios under even very modest transaction costs @xmath215 smaller @xmath212 induces portfolios more robust to transaction costs . of note here is that , during 2009 following the financial crisis , portfolios with larger @xmath212 benefit as they are less constrained in adapting ; but , later into 2010 and 2011 , portfolios with lower @xmath212 are more profitable as they define ideal allocations with less switching and therefore save on transaction costs .",
    "figure  [ fig : sd ] shows trajectories of realized sds of optimized portfolios , i.e. @xmath207 over time , for each of the portfolios in figure  [ fig : dlmr ] ; also plotted is the theoretical lower bound trajectory @xmath216 from the myopic , one - step , minimum variance portfolio .",
    "less constrained portfolios with larger @xmath212 have lower standard deviations , approaching those of the markowitz portfolio while also generating smoother changes in portfolio weights and higher cumulative returns .",
    "thus , these portfolios are improved in these latter metrics at the cost of only modest increases in traditional portfolio risk . \"",
    "interestingly , the relationship between @xmath212 and realized standard deviations is not monotone ; we see larger standard deviations in the case of @xmath217 than with @xmath218 , the latter , very low value yielding an almost constant portfolio over time that , in this study , turns out to control risk at a level not matched by modestly more adaptive portfolios .    , @xmath217 ( upper ) and @xmath219 ( center ) , compared to traditional markowitz weights ( lower).,title=\"fig:\",width=355 ] , @xmath217 ( upper ) and @xmath219 ( center ) , compared to traditional markowitz weights ( lower).,title=\"fig:\",width=355 ] , @xmath217 ( upper ) and @xmath219 ( center ) , compared to traditional markowitz weights ( lower).,title=\"fig:\",width=355 ]     and @xmath220 ( red ) , 100 ( blue ) , 10,000 ( green ) , together with markowitz portfolios ( pink ) .",
    "the transaction cost is @xmath204 ( upper ) and 0.001 ( lower ) .",
    ", title=\"fig:\",width=355 ]   and @xmath220 ( red ) , 100 ( blue ) , 10,000 ( green ) , together with markowitz portfolios ( pink ) .",
    "the transaction cost is @xmath204 ( upper ) and 0.001 ( lower ) . , title=\"fig:\",width=355 ]    , @xmath220 ( red ) , 100 ( blue ) , 10,000 ( green ) , the markowitz portfolio ( pink ) , and the minimum variance portfolio ( black ) .",
    ", width=355 ]    , @xmath217 and @xmath221.,title=\"fig:\",width=355 ] , @xmath217 and @xmath221.,title=\"fig:\",width=355 ]      we explore similar graphical summaries from analyses using the extended laplace loss framework of section  [ sec : nonneg ] , and with sum - to - one constraints .",
    "figure  [ fig : em2 ] shows optimal weight trajectories with @xmath222 and @xmath223 ( this change of @xmath224 is only in this example ) , and with the same level of penalization of turnover and absolute weights , i.e. , @xmath225 we see expected effects of the two types of shrinkage of changes in weights and in weights themselves .",
    "first , the hard shrinkage of changes induces much less switching in portfolio allocation over time , with longish periods of constants weights on subsets of equities .",
    "this occurs even with larger @xmath212 where the portfolio becomes more volatile and similar to the markowitz case .",
    "second , the penalty on absolute weights themselves , and implicitly on short positions as a result in this context of sum - to - one weights , yields trajectories that are basically non - negative on all equities over time .",
    "the joint optimization drives some of the weights exactly to zero at some periods of time , indicating a less than full portfolio over these periods .",
    "furthermore , it is evident that there are periods where some of the weights while not zero are shrunk to very small values , so that a practicable strategy of imposing a small threshold would yield sparser portfolios i.e. , a soft `` sparsity feature .",
    "values @xmath226 favor more stability / persistence in the portfolio allocations , and we see more stepwise '' allocation switches rather than more volatile turnover .",
    "conversely , @xmath227 more aggressively favors no - shorting and encourages soft \" sparsity of allocations , resulting in dynamically switching portfolio weights over , generally , fewer assets .",
    "figure  [ fig : em2r ] plots trajectories of cumulative returns for three extended laplace loss portfolios to show variation with the value of @xmath228 together with one highly adaptive normal loss portfolio and the markowitz analysis , for @xmath229 and @xmath230 fixed .",
    "again we compare cases with transaction cost @xmath204 and 0.001 . as with normal loss comparisons ,",
    "all multi - step cases dominate the traditional markowitz analysis under even modest transaction costs .",
    "in addition , we now see the ability of the increasingly constrained multi - step laplace portfolios to outperform unconstrained and hence more volatile markowitz as well as multi - step normal loss portfolios even when transactions costs are zero or ignored . then , cumulative returns with @xmath231 are essentially uniformly dominated by those with @xmath232 and @xmath233 , regardless of the existence of transaction costs in this example .",
    "this suggests values of @xmath198 smaller than or comparable to @xmath212 to appropriately balance the two degrees of shrinkage while maintaining relevant returns .",
    "one reason for this is the encouragement towards less volatile swings in weights to larger negative / positive values and towards no - shorting as part of that , features that can lead to increased risk and transaction costs .",
    "@xmath217 and @xmath23410 ( green ) , 100 ( yellow ) and 10,000 ( purple ) , together with those from the normal loss portfolio with @xmath219 ( blue ) and the markowitz outcomes ( pink ) .",
    "the transaction cost is @xmath204 ( upper ) and 0.001 ( lower ) .",
    ", title=\"fig:\",width=355 ]   @xmath217 and @xmath23410 ( green ) , 100 ( yellow ) and 10,000 ( purple ) , together with those from the normal loss portfolio with @xmath219 ( blue ) and the markowitz outcomes ( pink ) .",
    "the transaction cost is @xmath204 ( upper ) and 0.001 ( lower ) . ,",
    "title=\"fig:\",width=355 ] +      we now discuss some analysis summaries related to the discussion of profiled and marginal losses of section  [ sec : marginal ] . as discussed in section  [ sec : margprof ] we do this in the laplace loss framework of sections  [ sec : laplacebasic ] and [ sec : lassomodel ] ( i.e. , with @xmath160 in the extended context ) .",
    "first , figure  [ fig : ma ] shows optimal weight trajectories with @xmath235 and @xmath236 comparing the profiled laplace loss weights @xmath237 of section  [ sec : lassomodel ] with the marginal laplace loss weights @xmath238 of section  [ sec : marginal ] .",
    "both strategies generate positive weights on the jpy , gbp and cad fx rates , with a number of the other assets having quite small weights for longer periods of time , while the weights under profiled loss vary more widely to higher absolute values .",
    "we see constant weights for long periods on adaptively updated subsets of assets using the profiled weights , as expected ; these trajectories are effectively smoothed - out and shrunk towards zero under the marginal weights .",
    "the latter do not exhibit the exact zero values that the former can , as we now understand is theoretically implied by our representation via the emulating statistical model : marginal modes will not be exactly at zero even when joint modes have subsets at zero .",
    "figure  [ fig : mar ] plots trajectories of cumulative returns for both profiled and marginal portfolios in each of the cases with @xmath209 and @xmath239 without transaction cost , the profiled and marginal portfolios are similarly lucrative whatever the value of @xmath212 , whereas the profiled portfolios show greater differences .",
    "in contrast , both approaches are more substantially impacted by transaction costs and in a similar way ; the cumulative return performance of portfolios decreases drastically in the presence of transaction costs . not shown here ,",
    "portfolios with smaller @xmath212 values define far more stable weights while resulting in very similar cumulative returns under both profiled and marginal strategies , as the resulting portfolio weights are very stable over time ; this extends this observation as already noted in the normal loss context in section  [ sec : exnormal ] .",
    "the marginal strategy tends to be less sensitive to @xmath212 than the profiled strategy , suggesting relevance in a conservative \" investment context with respect to loss function misspecification .",
    "even with quite widely varying @xmath212 , resulting marginal loss portfolios will be more stable , and far less susceptible to substantial changes and potential deterioration in terms of cumulative returns , than profiled loss portfolios .     and",
    "@xmath217 , showing profiled weights ( upper ) and marginal weights ( lower ) . , title=\"fig:\",width=355 ]   and @xmath217 , showing profiled weights ( upper ) and marginal weights ( lower ) .",
    ", title=\"fig:\",width=355 ]    $ ] ( red , dashed ) , [ profiled , @xmath240 $ ] ( red , full ) , [ marginal , @xmath241 $ ] ( blue , dashed ) , and [ marginal , @xmath240 $ ] ( blue , full ) . the transaction cost is @xmath204 ( upper ) and 0.001 ( lower ) .",
    ", title=\"fig:\",width=307 ] $ ] ( red , dashed ) , [ profiled , @xmath240 $ ] ( red , full ) , [ marginal , @xmath241 $ ] ( blue , dashed ) , and [ marginal , @xmath240 $ ] ( blue , full ) . the transaction cost is @xmath204 ( upper ) and 0.001 ( lower ) .",
    ", title=\"fig:\",width=307 ] +",
    "the selected illustrations in our application to financial time series highlight key features and benefits of our bayesian emulation approach to computing optimal decisions , as well as our development of new multi - step portfolio decision analysis .",
    "versions of the laplace loss functions generate multi - step portfolios that consistently outperform traditional myopic approaches , both with and without transaction costs ; they define psychologically and practically attractive framework for investors concerned about portfolio stability over multiple periods with defined targets .",
    "examples show the opportunity through appropriate selection of loss function parameters for resulting portfolios to cushion the impacts of economically challenging times for the market , and enhance recovery afterwards , as highlighted in the examples using fx , commodity and market index data over 2009 - 2012 .",
    "in addition to showcasing the application of the concept of bayesian emulation for decisions `` , our interest in multi - step portfolios also highlights the central question of optimization for one - step decisions in the multi - step view ; while specific numerical methods using dynamic programming might be tuned and customized to a specific loss function in this context , the bayesian emulation approach opens up new approaches and suggests new avenues for development . as we begin in our discussion of",
    "_ marginal _ versus _ profiled _ loss functions , there is now opportunity to drive some part of the research agenda from synthetic statistical models as a starting point , exploring and evaluating the _ implied _ loss functions .",
    "one specific , current direction linked to this is to define classes of non - normal , state - space models with skewed innovation / error distributions that induce asymmetric loss functions ; a key idea here is to use discrete / mean - scale mixtures of normals for the innovation / error distributions , so maintaining the ability to use mcmc coupled with ffbs / em methods for mode - finding while generating a very rich class of resulting loss functions .",
    "one key and desirable feature of the latter , in particular , is to represent high penalties on portfolio short - fall relative to moderate or expected gains .",
    "this direction , and others opened - up by the bayesian emulation for decisions '' approach , offers potential for impact on research frontiers in statistics and decision theory as well as application in financial portfolio development and other areas .",
    "to construct the approximate density in , we need to sample from the ( joint ) posterior of the model in .",
    "the gibbs sampler for this model has components related to those of bayesian lasso regression  @xcite , but now in the extended context of dynamic models using ffbs methods .",
    "the mcmc proceeds over iterations @xmath242 as follows :    * sample each @xmath243 from its generalized inverse gaussian ( gig ) is @xmath244 , complete conditional posterior distribution , @xmath245 independently across @xmath246 and @xmath136 * sample @xmath247 using ffbs . * for later use ,",
    "record the means and variances of the marginal normal posterior @xmath248 generated by the above ffbs analysis .    with the samples @xmath249 and the by - products @xmath250 , the monte carlo approximation to @xmath162 is @xmath251 the next step is mode - finding in this mixture of normals .",
    "modes satisfy @xmath252 with @xmath253 and @xmath254 we iterate this fixed - point equation to compute approximate modes , with the strategy for multiple global `` starting values as noted in the main paper .",
    "normal mixtures can exhibit multiple modes , and our starting values using means @xmath255 prioritized by the resulting values of @xmath256 explicitly address this by defining a set of spanning '' mode searches .",
    "note that , when the sum - to - one constraint is imposed on the original model by setting @xmath257 , then the full conditional of the @xmath117 is no longer a product of univariate gig distributions . to sample each @xmath139",
    ", we therefore use a novel , independence chain metropolis - hastings algorithm . here the product of the initial gig distributions in the unconstrained model is used as the obvious proposal distribution .",
    "acceptance probabilities involve singular normal densities based on the generalized inverses @xmath258 where @xmath259 .",
    "the acceptance probability of each proposed sample @xmath260 conditional on its previous value @xmath261 and all other parameters is @xmath262 where @xmath263 and @xmath264 .",
    "we observe in our empirical studies and the application of the paper , in particular , that the acceptance probability is generally very high typically around 98% .",
    "we note also that , due to sum - to - one constraint , the conditional , @xmath54 variance matrices @xmath265 are rank - deficient , being of rank @xmath92 .",
    "the generalized inverse @xmath266 in   are based on singular value decompositions in this iterative numerical solver for the modes of the normal mixture .",
    "for time series analysis and forecasting , we adapt the framework of dynamic dependence network models ( ddnms ) introduced in  @xcite .",
    "this model framework builds on prior work in multivariate dynamic modelling and innovates in bringing formal and adaptive bayesian model uncertainty analysis to parsimonious , dynamic graphical structures of real practical relevance to financial ( and other ) forecasting contexts .",
    "specific classes of ddnms represent both lagged and cross - sectional dependencies in multivariate , time - varying autoregressive structures , with an ability to adapt over time to dynamics in cross - series relationships that advances the ability to characterize changing patterns of feed - forward relationships and of multivariate volatility , and to potentially improve forecasts as a result .",
    "denote the @xmath267 vector of assets by @xmath268 ; in our application , @xmath268 is the vector of log prices of the financial assets .",
    "the ddnm extension of a tv - var(2 ) model represents @xmath268 via @xmath269 where @xmath270 , @xmath271 is a @xmath272 matrix of time - varying intercept auto - regressive coefficients , @xmath273 is a time - varying , lower triangular matrix with diagonal zeros , and @xmath274 with time - varying univariate volatilities on the diagonal .",
    "the model can be written element - wise as @xmath275 where @xmath276 is the @xmath277-th row of @xmath271 , @xmath278 is the _ parental set _ of series @xmath277 defined as the indices of @xmath277-th row of @xmath273 with non - zero elements , and @xmath279 and @xmath280 are the corresponding subvectors with @xmath281 elements of @xmath268 and @xmath277-th row of @xmath273 .",
    "the state parameters @xmath282 are assumed to follow normal random walks with a discount factor method applied to define the state evolution variance matrices as is standard in univariate dlms  @xcite .",
    "the observational variance @xmath283 is modeled as a gamma - beta stochastic volatility process over time , again based on standard dlm methodology  ( * ? ? ?",
    "sparsity of the parental sets @xmath284 defines patterns of zeros below the diagonal in @xmath273 .",
    "this in turn defines the sparsity structure of the implied residual precision matrix ; by inversion , the conditional precision matrix of @xmath268 given the past values and all dynamic parameters is @xmath285 which has the form of sparse cholesky decomposition when @xmath273 is sparse .",
    "if the level of sparsity in parental sets is high , then this precision matrix will also have zeros in some of the off - diagonal elements , representing conditional independencies in the innovations ; since elements of @xmath273 and @xmath286 are time - varying , these conditional independencies represent an underlying dynamic graphical model for the innovations .    given the parental sets @xmath284 , the sequential , forward filtering analysis of the multivariate ddnm partitions into a parallel set of @xmath20 univariate models with standard , analytic computation of on - line prior - to - posterior updating and one - step ahead forecasting . for forecast distributions more",
    "than one - step ahead , simulation methods are used as the unknown future observations @xmath287 ( @xmath288 ) are required as conditional predictors . direct simulation from the exact predictive distributions is easily implemented recursively , as detailed in  @xcite    as in much of our past work in practical financial time series forecasting , we apply the ddnm to log prices in the vector @xmath268 , and returns @xmath27 are then inferred . for univariate series @xmath277 with price @xmath289",
    "at time @xmath290 the return is @xmath291 with @xmath292 .",
    "similar relationships define the @xmath20-step ahead returns at any time .",
    "our portfolio analyses require predictive mean vectors and variance matrices of returns , which can be directly computed by transformation of the predictive samples of log prices .",
    "the ddnm requires specification of the parental sets @xmath284 .",
    "we choose these based on exploratory analysis of preliminary data over first 500 days .",
    "filtering and forecasting with the defined ddnm then run from day 501 , redefined as @xmath293 in the formal sequential analysis .",
    "this exploratory analysis runs _ full _ models over the first 500 days , i.e. , ddnms using @xmath294 for each @xmath295 then , we simply compute the cholesky decomposition of the posterior mean of @xmath296 and threshold its off - diagonal elements using a threshold of @xmath297 those elements exceeding the threshold in row @xmath277 define the parental set @xmath284 ( with , of course , @xmath298 that we adopt for the forward filtering and forecasting analysis from then on .",
    "the choice of the threshold is naturally important here ; a higher threshold yields sparser parental sets and hence sparser @xmath273 matrices .",
    "exploratory analysis on the first 500 days is used to explore and evaluate this , and guide the choice informally . with a very low threshold , forecasts of returns in this training period tends to have very narrow credible intervals but show substantial biases , especially in multi - step ahead prediction .",
    "higher thresholds consistent with increased sparsity lead to wider credible intervals but less adaptive models . on a purely exploratory basis , we chose @xmath299 as a sweet - spot \" balancing forecast mean accuracy and uncertainty .",
    "this ad - hoc but practically rationale exploratory analysis defined a relevant , specific ddnm for use here .",
    "the resulting parental sets are displayed in table  [ tab : paj ] .",
    "our results are based on the resulting ddnm with additional parameters as follows : for the normal dlm state evolutions , we use discount factor of 0.98 for each series , and 0.97 for residual stochastic volatilities .",
    "direct simulation of multi - step ahead predictive distributions used a monte carlo sample size of 50,000 .",
    "lc parent @xmath277 & @xmath300 +   + oil & @xmath301 + gbp & @xmath301 + eur & @xmath301 + nok & eur + zar & gbp nok + cad & @xmath301 + aud & nok cad + nzd & aud + jpy & gbp eur cad aud + chf & @xmath301 + gld & gbp zar cad chf + s&p & gbp eur nok cad aud nzd + nsd & aud jpy chf s&p +"
  ],
  "abstract_text": [
    "<S> we discuss the bayesian emulation approach to computational solution of multi - step portfolio studies in financial time series . </S>",
    "<S> _ bayesian emulation for decisions _ involves mapping the technical structure of a decision analysis problem to that of bayesian inference in a purely synthetic emulating \" statistical model . </S>",
    "<S> this provides access to standard posterior analytic , simulation and optimization methods that yield indirect solutions of the decision problem . </S>",
    "<S> we develop this in time series portfolio analysis using classes of economically and psychologically relevant multi - step ahead portfolio utility functions . </S>",
    "<S> studies with multivariate currency , commodity and stock index time series illustrate the approach and show some of the practical utility and benefits of the bayesian emulation methodology . _ some key words and phrases : _ bayesian forecasting ; dynamic dependency network models ; marginal and joint modes ; multi - step forecasting ; portfolio decisions ; synthetic model </S>"
  ]
}