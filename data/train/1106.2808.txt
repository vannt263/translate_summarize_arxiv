{
  "article_text": [
    "generalized parton distributions ( gpds ) @xcite provide a detailed description of the nucleon in terms of partonic degrees of freedom , which allows , e.g. , to address the three - dimensional distribution of quarks and gluons and the partonic decomposition of the nucleon spin @xcite .",
    "their determination improves our understanding of non - perturbative qcd dynamics in general .",
    "more specifically , they are a necessary ( though not sufficient ) input for the theoretical description of multiple - hard reactions in proton - proton collisions at lhc collider @xcite and for other applications reaching beyond the limit of collinear qcd .",
    "information on gpds comes from many sides .",
    "there exists already an impressive experimental data base for exclusive reactions , which is continuously improved .",
    "a standard approach is either confrontation of model predictions @xcite or least - squares fit @xcite to this data .",
    "the latter approach , formulated in mellin space , is analogous to the well - established global fitting framework of parton distribution functions ( pdfs ) .",
    "complementary information on moments of gpds comes from lattice qcd @xcite .",
    "however , compared to the situation for pdfs , extracting gpds from data is a much more intricate task and model ambiguities are much larger .",
    "for instance , the theoretically cleanest process , used for the determination of gpds , is deeply virtual compton scattering ( dvcs ) , @xmath1 , which can be parametrized by twelve compton form factors ( cffs ) . at leading order in the ( inverse )",
    "photon virtuality squared @xmath2 they are expressed as convolutions of gpds with perturbatively calculable coefficient functions . at leading order in the qcd coupling constant and for one of the cffs , @xmath0 , we have @xmath3    h ( x,\\xi , t,{\\cal q}^2)\\ , , \\label{eq : calh}\\ ] ] where @xmath4 , @xmath5 is bjorken s scaling variable , @xmath6 is the momentum transfer squared , and @xmath7 is the gpd .",
    "formulae such as ( [ eq : calh ] ) suggest to parametrize gpds at some input scale @xmath8 , to employ qcd evolution equations to determine the gpds at the desired scale @xmath9 , then to make the momentum fraction convolution to obtain cffs that are used to calculate measured cross - sections and asymmetries via known formulae @xcite , and , finally , to confront the result with experimental data by means of the least - squares method . due to the facts that gpds can not be fully constrained even by ideal data and that they depend at the input scale @xmath8 on three variables , the space of possible functions , although restricted by gpd constraints , is huge . as a result , the theoretical uncertainty induced by",
    "the choice of the fitting model is much more serious than in the pdf case , where the model functions depend at the input scale only on one variable , namely , the longitudinal momentum fraction @xmath10 .",
    "the fact that exclusive data is typically much less precise than inclusive one exacerbates the situation further .",
    "in this paper we explore an alternative approach , in which _ neural networks _",
    "are used in place of specific models .",
    "this essentially eliminates the problem of model dependence and , as an additional advantage , facilitates a convenient method to propagate uncertainties from experimental measurements to the final result . in the context of nucleon structure studies ,",
    "neural networks have already been successfully applied to extract pdfs by the nnpdf group @xcite and to parametrize electromagnetic form factors @xcite .",
    "similarly , they have been employed for parametrization of spectral function of hadronic tau decays @xcite . in the case of gpds , because of the abovementioned reasons we expect that the advantages of this method should be even more pronounced .    while our long - term goal are global fits of gpds , the data presently available allows only a less ambitious analysis , namely the fit of the dominant cff @xmath0 using neural networks @xcite ] .",
    "this reduces the mathematical complexity of the problem ( increased also by gpd constraints ) , while still retaining the relevance for the study of hadron structure .",
    "the imaginary part of the cff ( [ eq : calh ] ) is at leading order @xmath11 and so , although we consider only extraction of cffs , @xmath12 provides us with direct information about the shape of gpd @xmath7 on the cross - over line @xmath13 .",
    "moreover , a  dispersion relation \" @xcite @xmath14 - { \\cal c}(t,{\\cal q}^2)\\ , , \\label{eq : recalh}\\ ] ] can then be used as a sum rule to pin down this gpd outside of the kinematically accessible region .",
    "extraction of cffs from dvcs data in a largely model - independent way was also performed in @xcite .",
    "in contrast to the work presented here , these analyses are local in the sense that they extract values of cffs separately at each measured kinematic point .",
    "we emphasize that in refs .",
    "@xcite the systematic uncertainties , induced by other non - dominant cffs , were explored with model - dependent constraints for the four twist - two related cffs .",
    "the paper is organized as follows . in section [ sec : nnmethod ] we describe salient features of the neural network approach . in section [ sec : fit ] we present fits to a well - defined subset of available dvcs data , so that we could study neural network method in a controlled situation , free of subtleties which surface in many fits involving data coming from a number of different experiments .",
    "in particular , we used hermes measurements of two observables , beam spin asymmetry ( bsa ) and beam charge asymmetry ( bca ) in leptoproduction of a real proton ( of which dvcs is a subprocess ) @xcite .",
    "this data belongs to a kinematic region where these two asymmetries are determined essentially by the imaginary and the real part of the compton form factor @xmath0 ( [ eq : calh ] ) , respectively , and where dependence of @xmath0 on @xmath9 is weak and , therefore , can be neglected for simplicity .",
    "the neural network parameterization of cff @xmath0 is then used to make a prediction for another observable , namely , the beam charge - spin asymmetry in the kinematic region to be explored by the compass ii experiment @xcite . in section [ sec : comparison ]",
    "we place particular emphasis on the determination of uncertainties .",
    "to do so we fit a simple model using the standard least - squares method and compare the resulting uncertainties with those obtained by neural networks .",
    "section [ sec : conclusion ] contains conclusions .",
    "each blob symbolizes a neuron and thickness of arrows represents the strengths of weights @xmath15 .",
    "( smaller blobs with no input are so - called _ biases _  they improve the network s properties @xcite . ) ]    neural networks have been applied to a variety of tasks , including optimization problems .",
    "one of the main attractions is their apparent ability to mimic the behavior of human brain : with their multi - connectedness and parallel processing of information they can be trained to perform relatively complex classification and pattern recognition tasks . to this end",
    "many kinds of neural networks and corresponding `` learning '' algorithms have been developed , see , e.g. , @xcite . in our study , we used one of the most popular neural network types , known as _ multilayer perceptron_. schematically shown on fig .",
    "[ fig : perceptron ] , it is a mathematical structure consisting of a number of interconnected `` neurons '' organized in several layers .",
    "each neuron has several inputs and one output .",
    "the value at the output is given as a function @xmath16 of a sum of values at inputs @xmath17 , each weighted by a a certain number @xmath15 .",
    "consequently , a multilayer perceptron is defined by its architecture ( number of layers and number of neurons in each layer ) , by the values of its weights , and by the shape of its _ activation function _ @xmath18 .",
    "again somewhat inspired by biological neuron properties , the activation function is often taken as a nonlinear function with saturating properties for large and small values of its argument .",
    "we employed a very popular logistic sigmoid function @xmath19 for neurons in inner ( `` hidden '' ) layer(s ) , while for input and output layers we used the identity function @xmath20 . by iterating over the following steps the network",
    "is then trained , i.e. , it `` learns '' how to describe a certain set of data points :    1 .",
    "kinematic values ( two in our case : @xmath5 and @xmath6 ) of the first data point are presented to two input - layer neurons 2 .",
    "these values are then propagated through the network according to weights and activation functions . in the first iteration , weights are set to some random value .",
    "3 .   as a result , the network produces some resulting values of output in its output - layer neurons . here",
    "we have two : @xmath12 and @xmath21 . obviously ,",
    "after the first iteration , these will be some random functions of the input kinematic values : @xmath22 and @xmath23 .",
    "4 .   using these values of cff(s ) ,",
    "the observable(s ) corresponding to the first data point is ( are ) calculated and it is ( they are ) compared to actually measured value(s ) , with squared error used for building the standard @xmath24 function .",
    "the obtained error is then used to modify the network : it is , possibly weighted by the inverse uncertainty of the experimental measurement , propagated backwards through the layers of the network and each weight is adjusted such that this error is decreased .",
    "the concrete algorithm for this weight adjustment is discussed at the end of this section .",
    "this procedure is then repeated with the next data point , until the whole training set is exhausted .",
    "this sequence of steps ( called training _ epoch _ ) is repeated until the network is capable to describe experimental data with a sufficient accuracy  the precise stopping criterion is specified below .    from what is",
    "said it should be clear that a neural network is nothing more but a complicated non - linear multi - parameter function and that the training procedure is equivalent to the least - squares fitting of this function to data .",
    "the actual power of the approach stems from the following :    * a neural network serves as _ unbiased _ interpolating function .",
    "its complicated dependence on parameters enables it to approximate any smooth function with comparable ease can be arbitrary accurately approximated by a multilayer perceptron .",
    "strictly spoken , one hidden layer is enough for this , but networks with more hidden layers can have fewer neurons and/or can be more efficiently trained . ] .",
    "* updating the weights of a given neuron by back - propagation of error uses only values and gradients of activation functions in the immediate network neighborhood . in this sense",
    "the training process is local and it is algorithmically efficient .",
    "* this approach enables the following convenient method for propagation of experimental uncertainties ( and even their correlations ) into the final result : in our application a `` replica data set '' is interpolated .",
    "it is obtained from original data by generating random artificial data points using gaussian probability distribution with a width defined by the error bar of experimental measurements . taking a large number @xmath25 of such replicas , the resulting family of trained neural networks @xmath26",
    "defines a probability distribution of the represented cff @xmath27 and of any functional @xmath28 $ ] thereof .",
    "thus , the mean value of such a functional and its variance are @xcite @xmath29 \\big\\rangle & =    \\frac{1}{n_{rep}}\\sum_{k=1}^{n_{rep } } \\mathcal{f}[\\mathcal{h}^{(k)}]\\ ; , \\label{eq : funcprob } \\\\",
    "\\big(\\delta \\mathcal{f}[\\mathcal{h}]\\big)^2 & = \\big\\langle \\mathcal{f}[\\mathcal{h}]^2 \\big\\rangle   - \\big\\langle \\mathcal{f}[\\mathcal{h } ] \\big\\rangle^2 \\;. \\label{eq : variance}\\end{aligned}\\ ] ] for details of this `` monte carlo '' procedure see refs .",
    "@xcite and note that this is a general method of error propagation , which is not related to neural networks themselves and can be used also for error propagation in standard least - squares fitting of model functions to data .",
    "the power of neural networks to approximate any continuous function implies also the danger of overfitting the data ( also known as _ overtraining _ ) .",
    "namely , after a certain number of iterations the network will not only describe the general dependence of observables on kinematic variables , but will also adjust to the random fluctuations of data .",
    "this unwanted behavior is prevented by the cross - validation procedure in which initial data is divided into two sets : _ training _ set and _ validation _ set .",
    "then performance of the network is continuously checked on the validation set . since this set",
    "is not used for training , after the onset of overfitting , the error of the network s description of validation sets will increase , see fig .  [",
    "fig : crossvalidation ] .",
    "this is the moment at which training is stopped .        for more informations on neural networks",
    "we refer the reader to the vast literature on the subject , e.g. @xcite , while the phd thesis @xcite is a good reference for the particular approach used also in this work . in the remainder of this section",
    ", we specify some details about the neural networks we used .    in our study we employed the pybrain software library for neural networks @xcite",
    ". this choice was motivated by ( i ) the great flexibility of this library , which allows to explore various types of neural networks , and ( ii ) it being written in the python programming language , such that it was easy to link with our already existing python code for dvcs analysis . like almost every other freely available neural network software ( at least to our knowledge )",
    ", pybrain expects that the network output is directly compared to training data samples for error calculation . in our case , however , the network output must be transformed , i.e. , the dvcs observables must be calculated from @xmath12 and @xmath21 ( see step no . 4 in the training procedure described above ) , before the error can be evaluated . for this",
    ", we had to make some modifications to the pybrain software .",
    "concerning the network architecture , the number of neurons in the input and output layers were fixed by the problem at hand to be two , see fig .",
    "[ fig : perceptron ] . to fix the number of hidden layers and their neurons",
    ", one has to consider a trade - off between two requirements : the number of neurons in the hidden layer(s ) must be large enough to ensure sufficient expressive power of the network but must not be too large with respect to the number of available data points , otherwise the training will become difficult . in our case , it is already known from previous studies , e.g. @xcite , that cff @xmath0 is a reasonably well - behaved function .",
    "thus , since we used a relatively small sample of few tens of data points , see next section , just one hidden layer with about ten neurons proved to be sufficient .",
    "the specific results presented in this paper were obtained by training series of neural networks , with 13 neurons in the hidden layer each .",
    "we checked that results do not change appreciably if a neuron is added or removed from the hidden layer , which is one of the standard tests in neural network applications .",
    "there are various training algorithms available for updating the network weights , from a simple back - propagation algorithm @xcite to genetic algorithms like the one used by the nnpdf group @xcite . after some experimentation we ended up using the so called _ resilient back - propagation _ algorithm @xcite .",
    "this algorithm is a modification of the usual back - propagation algorithm : only the signs of the partial derivatives of the error function , and not their magnitude are taken into account .",
    "resilient back - propagation is known to be very reliable , fast and insensitive to parameters of the network and of the learning process .",
    "more sophisticated algorithms are needed when correlations are included in the @xmath24 error function or in cases for which the connection between the network output and the experimental observables is very non - linear .",
    "the latter would , e.g. be the case when the network output would represent gpds rather than cffs , so that the connection would involve convolutions with qcd evolution operators and hard - scattering coefficient functions .",
    "let us finally note that besides perceptrons , there are attempts to apply other kinds of neural networks in nucleon structure studies , such as self - organizing maps @xcite .",
    "for the first application , to assess the power of the neural network approach , we took just two sets of measurements of leptoproduction of a real photon by scattering leptons off unpolarized protons .",
    "one set consisted of 18 measurements of the first sine harmonic @xmath30 of the beam spin asymmetry ( bsa ) ) and we disregard the difference of these two observables in this paper . ]",
    "@xmath31 ( where @xmath32 is the azimuthal angle in the so - called trento convention ) , while in the other set there were 18 measurements of the first cosine harmonic @xmath33 of the beam charge asymmetry ( bca ) @xmath34 both sets cover identical kinematic regions @xmath35 as mentioned in the introduction , we assumed that qcd evolution effects can be neglected , i.e. , that cffs are independent of @xmath9 .",
    "furthermore , it has been shown that the hypothesis of cff @xmath0 dominance leads to a successful description of this data @xcite and so we relied on this assumption ( with the understanding that it is only an approximation which should be given up , once sufficiently precise data is available ) .",
    "thus , at present , just a single cff @xmath27 , or two real - valued functions @xmath22 and @xmath23 , are extracted from data by our neural networks .    a comment about the relation between @xmath12 and @xmath21 is in order .",
    "analytic properties of cff @xmath0 relate these two functions via a `` dispersion relation '' , which reads in twist - two approximation : @xmath36 cf",
    ".  eqs .",
    "( [ eq : dvcslo ] ) and ( [ eq : recalh ] ) . as discussed in the introduction ,",
    "this could be used to simplify the fitting function set from @xmath37 however , for two reasons we treat here the imaginary and real part of cff @xmath0 as independent quantities .",
    "first , assumption of `` dispersion relation '' ( [ eq : dr ] ) by itself introduces a theoretical bias we want to avoid . in section [ sec : comparison ]",
    "we will see that model - dependent least - squares fits that are based on the  dispersion relation \" ( [ eq : dr ] ) link , e.g. , assumptions about the low @xmath10 behaviour of @xmath12 to that of @xmath21 which might lead to rather misleading results . instead",
    ", neural networks which do not make use of ( [ eq : dr ] ) give large uncertainties for both , which is probably more realistic .",
    "+ second , to implement the  dispersion relation \" approach in a neural network framework one would need to either ( i ) disconnect topologically the network output representing @xmath38 from the input representing @xmath5 , or ( ii ) to create a separate neural network with one input and one output ( 11 ) for @xmath38 and train it together with the two - inputs - one - output ( 21 ) network representing @xmath22 . certainly , taking in future all available fixed target data into account , such a strategy might be feasible , and if so , would allow to get rid of the @xmath0 dominance hypothesis and might thus improve the phenomenological value of the final result .",
    "( [ eq : bca ] ) and first sine harmonic of beam spin asymmetry @xmath30 ( [ eq : bsa ] ) resulting from neural network fit ( hatched areas ) , shown together with data @xcite , used for training .",
    "two model fits , km09a ( solid ) and km09b ( dashed ) from @xcite are also shown for comparison . ]    using the procedure described in section  [ sec : nnmethod ] , we created 50 replicas from the set of 36 measured data points , which were randomly divided into a training set of 25 points and a validation set of 11 points .",
    "then we trained one neural network on each training set , monitoring progress on the validation set , as in fig .",
    "[ fig : crossvalidation ] .",
    "training was performed either until the onset of overlearning , or for 2000 epochs , whichever came first .",
    "we ended up with 50 neural networks , each representing cff @xmath0 .",
    "most of the neural networks fit the original data set quite well ( 44 of our 50 neural networks have @xmath24 less than the number of data points , 36 ) , with few poorer fits as expected due to the stochastic nature of the set of data replicas .    using this set of neural networks as a probability distribution in a functional space of all",
    "@xmath0 one can predict any function of @xmath0 , together with its uncertainty , using ( [ eq : funcprob ] ) and ( [ eq : variance ] ) . as a consistency check we plot in fig .",
    "[ fig : hermes09 ] the result for the very observables that are used for training of networks .",
    "we observe that uncertainties of measurements have been correctly propagated into the corresponding uncertainties given by neural networks .",
    "this is particularly visible in the first two panels of fig .",
    "[ fig : hermes09 ] , where uncertainties of both data and neural networks increase with @xmath39 in the same fashion .    error bands here and in other figures denote the uncertainty that corresponds to one standard deviation ( one sigma ) .",
    "we checked for departures from a gaussian distribution of network results and found them to be small in the region where data is available .",
    "this means that a one - sigma error band corresponds indeed to a 68% confidence level .",
    "however , as observed also by the nnpdf group @xcite , in the extrapolation region , where there is no data , departures from gaussian distribution are significant , and 68% confidence level regions are generally smaller than the one - sigma regions we plot .    ) in compass ii kinematics ( @xmath40 , @xmath41 ) ( hatched areas ) . two model fits , km09a ( solid ) and km09b ( dashed ) from @xcite are also shown for comparison . ]    as an example of a proper prediction coming from our analysis we plot in fig .",
    "[ fig : compass ] the beam charge - spin asymmetry ( bcsa ) , @xmath42 as a function of @xmath6 , for several kinematic points that are characteristic for the compass ii experiment ( where the muon is taken to be massless and the polarization is set equal to 0.8 ) .",
    "this experiment was chosen because its kinematics overlaps with that of the hermes data used for neural network training .",
    "hence these predictions represent partly interpolation and partly extrapolation of hermes data , thus testing this framework in a nontrivial way .",
    "one of the main advantages of the neural network approach to hadron structure is that it offers a convenient way to assess the uncertainties of non - perturbative functions ( gpds , cffs , pdfs ,  ) .",
    "let us now emphasize this point further by making a comparison with the common method in which one chooses some model and makes a least - squares fit of it to the data . to quantify this comparison",
    ", we took a simple model of the cff @xmath0 and fitted it to the very same data to which neural networks were trained .",
    "we adopted a version of the model used in @xcite for which the partonic decomposition of @xmath12 is : @xmath43\\,.\\end{aligned}\\ ] ] we parametrized the gpds along the cross - over trajectory @xmath13 as : @xmath44 here @xmath45 is the normalization of pdf @xmath46 from pdf fits , @xmath47 is the skewness ratio at small @xmath10 ( the ratio of a gpd at some point on the cross - over trajectory and the corresponding pdf ) , @xmath48 is the `` regge trajectory '' , @xmath49 controls the large-@xmath10 behavior , and @xmath50 and @xmath51 control the @xmath6-dependence @xcite .",
    "the parameters of the sea - quark gpd @xmath52 were taken to be as in @xcite ( @xmath53 , @xmath54 , @xmath55 , @xmath56 , @xmath57 , @xmath58 ) .",
    "for the valence quark gpd @xmath59 , we also fixed @xmath60 , @xmath61 , and @xmath62 .",
    "this left @xmath63 , @xmath64 and @xmath65 as free parameters , to be determined by the fit to the data .",
    "we obtained @xmath21 from the  dispersion relation \" ( [ eq : dr ] ) , treating the subtraction constant @xmath66 as a fourth and final free parameter of the model ) to be @xmath6-independent , @xmath67 .",
    "this is motivated a posteriori by the fact that such a simple model is good enough to fit the data . additionally , when we parametrize @xmath68 , the parameter @xmath69 tends to be strongly correlated with other parameters of the model , making error analysis , which is our main task , more difficult . ] .",
    "fitting this model to the same 36 data points used for neural network fit , using the well - known minuit software package @xcite for minimization of the @xmath24 function , resulted in a good fit ( @xmath70 ) with parameter values @xmath71 the uncertainty of the resulting fit is commonly determined by the so - called hessian method .",
    "the hessian matrix is given by the second derivatives of @xmath24 with respect to the model parameters @xmath72 at the minimum @xmath73 : @xmath74 the uncertainty @xmath75 of the function @xmath76 of these parameters , including the uncertainty of the parameters themselves , is given by the error propagation formula @xmath77 in a textbook approach to statistics , one sigma uncertainty is obtained by taking @xmath78 in this formula .",
    "however , there are several problems with this simple procedure .",
    "first , @xmath24 tends to vary very differently in different directions of parameter space , yielding a hessian with large variations in the eigenvalue spectrum which might then imply numerical difficulties . in our case , where eigenvalues vary over three orders of magnitude , the problem was not so severe .",
    "however , in dvcs fits , involving more parameters , we have also observed much stronger variations . to improve the reliability of the hessian method",
    ", we followed an iterative procedure @xcite which allows to find natural directions in parameter space and natural step sizes for the finite - difference calculation of the hessian matrix .",
    "second , it is known that with global fits , i.e. , when one combines data from several different experiments , errors are not distributed in the expected way . often , reasoning strictly within textbook statistics , one would have to reject some data as incompatible either with itself ( i.e. , because its @xmath24 is too large ) or with other data sets ( i.e. , because likelihood curves poorly overlap ) .",
    "this is mainly due to systematic errors which are not understood . without entering further into the discussion of this complex issue",
    ", we can follow the cteq procedure @xcite , where @xmath79 in ( [ eq : errorprop ] ) is increased ad hoc to @xmath80 in order to accommodate all experimental measurements used for fitting .",
    "the tolerance parameter @xmath81 is determined by calculating the average distance from the best fit along eigenvectors of the hessian matrix at which the model is still compatible with some particular experiment at some confidence level ( c.l . ) . in global pdf fits , with many data sets , values of @xmath82 are common , see e.g. @xcite for a recent review .",
    "nevertheless , using this procedure , we arrived at a tolerance @xmath83 ( for 68% c.l . ; cteq use 90% ) .",
    "we conclude that in our case with just two sets of data , coming from the same experiment , there are no statistical inconsistencies and we can actually use the textbook formula ( [ eq : errorprop ] ) with @xmath84 , which we did will have to be increased for a true global fit . ] .",
    "we note in passing that we found that model parameters are significantly more constrained by the bca than by the bsa data .     and @xmath85 ( ascending hatches ) from hermes bca and bsa  @xcite data compared with three model fits , one of which with determined uncertainties ( descending hatches ) . [",
    "fig : cff ] ]    we can now compare the model fit ( [ eq : tdr3 ] ) , taking also into account uncertainties calculated by the procedure just described , with the neural network parameterization of the cff @xmath0 .",
    "both findings are displayed in fig .",
    "[ fig : cff ] , together with two models from @xcite , where @xmath12 ( upper panels ) and @xmath21 ( lower panels ) are separately plotted .",
    "we notice several interesting features :    * in the kinematic region of measured data ( this is roughly the middle vertical third of the left panels ) , there is a good agreement of values and uncertainties of neural networks ( ascending hatches ) and model fit ( descending hatches ) .",
    "this shows that both fitting methods correctly _ interpolate _ the data , and that two , quite different , statistical methods used for determination of uncertainties are mutually consistent . * as one starts to _ extrapolate _ the fitted cff @xmath86 outside of the data region ( left and right thirds of left panels and the whole of the right panels ) , the two methods predict markedly different shapes and uncertainties for the cff @xmath86 .",
    "all model fits show effects of theoretical bias by following the @xmath87 functional behavior at small @xmath10 and claiming a small uncertainty there , whereas in neural network parameterizations it is obvious that extrapolated functions are very unconstrained .",
    "this is particularly visible in the right panels , illustrating the difficulty of a model - independent extrapolation towards @xmath88 , which is a limit of particular interest for hadron structure studies . *",
    "the model fit and the corresponding uncertainties ( descending hatches ) show the effect of the dispersion relation integral constraint ( [ eq : dr ] )  @xmath12 is significantly constrained also outside of the data region .",
    "this is actually a welcome feature of the dispersion relation fits , offering the opportunity to access nucleon structure in regions not directly accessible in the experiment .",
    "however , its usefulness depends on the extent to which the functional forms used are firmly established . *",
    "the model km09b ( dashed line ) , which was obtained in @xcite by including additional data , and , more importantly , by extending the model to include contributions of the cffs @xmath89 and @xmath90 , disagrees for @xmath21 with the other three fits .",
    "this emphasizes that the assumed @xmath0 dominance might substantially affect the quality of our results .",
    "based on hermes measurements of lepton scattering off unpolarized protons , a reaction which should be dominated by the cff @xmath91 , we performed the first neural network analysis of deeply virtual compton scattering data and obtained a neural network representation of the cff @xmath0 .",
    "this was then used to make a prediction for the beam charge - spin asymmetry , measurable at compass ii .",
    "the monte carlo method of propagation of experimental errors enabled us to determine also the uncertainty of the resulting cff , and we have found that ( in the kinematic region where data is available ) it agrees well with the uncertainty determined by model fits and standard statistical procedures . however , neural networks combined with monte carlo error propagation are a conceptually cleaner and practically simpler method .",
    "the propagation of uncertainty from experimental data is intrinsic .",
    "most importantly , this provides essentially a model - independent approach to determine cffs and gpds .",
    "one could argue that the neural network approach , just because it is essentially model - independent , does nothing more than faithfully representing the information available in the experimental data by universal objects : cffs ( as presented here ) or gpds ( yet to be extracted by this approach ) .",
    "this is valuable in itself but can also be considered as a stepping stone towards improved model - dependent studies , that in principle offer the advantage that our detailed theoretical and phenomenological understanding of qcd dynamics can be taken into account .",
    "such a two - step procedure would be quite natural as models can be more directly compared to neural - network determined cffs or gpds than to actual measured observables .",
    "finally , let us stress that we consider the presented neural network study only as a first step in the analysis of dvcs data .",
    "the problem here is that the number of observables measured at a given kinematic point is in practice not sufficient to pin down all twelve cffs , some of which are kinematically suppressed .",
    "thanks juan rojo for many valuable comments and zvonimir vlah for discussions on neural networks . d.m . and k.k .",
    "are grateful to the institut fr theoretische physik , university of regensburg and to theory group of the nuclear science division at lawrence berkeley national laboratory for their warm hospitality .",
    "this work was supported by the bmbf grant under the contract no .",
    "06ry9191 , by eu fp7 grant hadronphysics2 , by dfg grant , contract no .",
    "436 kro 113/11/0 - 1 and by croatian ministry of science , education and sport , contract no .",
    "119 - 0982930 - 1016 .",
    "d.  mller , d.  robaschik , b.  geyer , f.  m. dittes and j.  hoeji , _ wave functions , evolution equations and evolution kernels from light - ray operators of qcd _ , _ fortschr .",
    "* 42 * ( 1994 ) 101 [ http://arxiv.org/abs/hep-ph/9812448[hep-ph/9812448 ] ] .",
    "a.  v. radyushkin , _ scaling limit of deeply virtual compton scattering _ , _ phys .",
    "* b380 * ( 1996 ) 417425 [ http://arxiv.org/abs/hep-ph/9604317[hep-ph/9604317 ] ] .",
    "ji , _ deeply - virtual compton scattering _ , _ phys .",
    "* d55 * ( 1997 ) 71147125 [ http://arxiv.org/abs/hep-ph/9609381 [ hep - ph/9609381 ] ] .",
    "ji , _ gauge invariant decomposition of nucleon spin _ , _ phys .",
    "* 78 * ( 1997 ) 610613 [ http://arxiv.org/abs/hep-ph/9603249[hep-ph/9603249 ] ] .",
    "l.  frankfurt , m.  strikman and c.  weiss , _ dijet production as a centrality trigger for @xmath92 collisions at cern lhc _ , _ phys.rev . _",
    "* d69 * ( 2004 ) 114010 [ http://arxiv.org/abs/hep-ph/0311231[hep-ph/0311231 ] ] .",
    "b.  blok , y.  dokshitzer , l.  frankfurt and m.  strikman , _ the four jet production at lhc and tevatron in qcd _ , _ phys .",
    "* d83 * ( 2011 ) 071501 [ http://arxiv.org/abs/1009.2714[1009.2714 ] ] .",
    "m.  diehl and a.  schfer , _ theoretical considerations on multiparton interactions in qcd _ , _ phys .",
    "* b698 * ( 2011 ) 389402 [ http://arxiv.org/abs/1102.3081[1102.3081 ] ] .",
    "s.  v. goloskokov and p.  kroll , _ the role of the quark and gluon gpds in hard vector - meson electroproduction _ , _ eur . phys . j. _",
    "* c53 * ( 2008 ) 367384 [ http://arxiv.org/abs/0708.3569[0708.3569 ] ] .",
    "k.  kumeriki and d.  mller , _ deeply virtual compton scattering at small @xmath5 and the access to the gpd h _ , _ nucl .",
    "_ * b841 * ( 2010 ) 158 [ http://arxiv.org/abs/0904.0458[0904.0458 ] ] .",
    "collaboration , j.  d. bratt _ et .",
    "_ , _ nucleon structure from mixed action calculations using 2 + 1 flavors of asqtad sea and domain wall valence fermions _ , _ phys .",
    "rev . _ * d82 * ( 2010 ) 094502 [ http://arxiv.org/abs/1001.3620[1001.3620 ] ] .",
    "a.  v. belitsky , d.  mller and a.  kirchner , _ theory of deeply virtual compton scattering on the nucleon _ , _ nucl .",
    "phys . _ * b629 * ( 2002 ) 323392 [ http://arxiv.org/abs/hep-ph/0112108[hep-ph/0112108 ] ] .",
    "a.  belitsky and d.  mller , _ exclusive electroproduction revisited : treating kinematical effects _ , _",
    "phys.rev . _ * d82 * ( 2010 ) 074010 [ http://arxiv.org/abs/1005.5209[1005.5209 ] ]",
    ".    collaboration , r.  d. ball _ et .",
    "al . _ , _ a determination of parton distributions with faithful uncertainty estimation _ , _ nucl .",
    "* b809 * ( 2009 ) 163 [ http://arxiv.org/abs/0808.1231 [ 0808.1231 ] ] .",
    "collaboration , r.  d. ball , l.  del  debbio , s.  forte , a.  guffanti , j.  i. latorre _ et .",
    "_ , _ a first unbiased global nlo determination of parton distributions and their uncertainties _ ,",
    "_ nucl.phys .",
    "_ * b838 * ( 2010 ) 136206 [ http://arxiv.org/abs/1002.4407 [ 1002.4407 ] ] .    r.  d. ball _ et .",
    "_ , _ impact of heavy quark masses on parton distributions and lhc phenomenology _ , http://arxiv.org/abs/1101.1300[1101.1300 ]",
    ". k.  m. graczyk , p.  plonski and r.  sulej , _ neural network parameterizations of electromagnetic nucleon form factors _ , _ jhep _ * 09 * ( 2010 ) 053 [ http://arxiv.org/abs/1006.0342[1006.0342 ] ] .",
    "j.  rojo and j.  i. latorre , _ neural network parametrization of spectral functions from hadronic tau decays and determination of qcd vacuum condensates _ , _ jhep _ * 0401 * ( 2004 ) 055 [ http://arxiv.org/abs/hep-ph/0401047[hep-ph/0401047 ] ] .",
    "s.  forte , l.  garrido , j.  i. latorre and a.  piccione , _ neural network parametrization of deep - inelastic structure functions _ , _ jhep _ * 05 * ( 2002 ) 062 [ http://arxiv.org/abs/hep-ph/0204232 [ hep - ph/0204232 ] ] .",
    "collaboration , l.  del  debbio , s.  forte , j.  i. latorre , a.  piccione and j.  rojo , _ unbiased determination of the proton structure function f(2)**p with faithful uncertainty estimation _ , _ jhep _ * 0503 * ( 2005 ) 080 [ http://arxiv.org/abs/hep-ph/0501067[hep-ph/0501067 ] ]",
    ".    o.  v. teryaev , _ analytic properties of hard exclusive amplitudes _ , http://arxiv.org/abs/hep-ph/0510031[hep-ph/0510031 ] .",
    "k.  kumeriki , d.  mller and k.  passek - kumeriki , _ towards a fitting procedure for deeply virtual compton scattering at next - to - leading order and beyond _ , _ nucl .",
    "* b794 * ( 2008 ) 244323 [ http://arxiv.org/abs/hep-ph/0703179[hep-ph/0703179 ] ] .",
    "m.  diehl and d.  y. ivanov , _",
    "dispersion representations for hard exclusive processes _ , _ eur .",
    "phys . j. _ * c52 * ( 2007 ) 919932 [ http://arxiv.org/abs/0707.0351[0707.0351 ] ]",
    ". k.  kumeriki , d.  mller and k.  passek - kumeriki , _ sum rules and dualities for generalized parton distributions : is there a holographic principle ? _ , _ eur .",
    "phys . j. _ * c58 * ( 2008 ) 193215 [ http://arxiv.org/abs/0805.0152[0805.0152 ] ] .",
    "m.  guidal , _ a fitter code for deep virtual compton scattering and generalized parton distributions _ , _ eur .",
    "phys . j. _ * a37 * ( 2008 ) 319332 [ http://arxiv.org/abs/0807.2355[0807.2355 ] ] .",
    "m.  guidal and h.  moutarde , _ generalized parton distributions from deeply virtual compton scattering at hermes _ , _ eur .",
    "j. _ * a42 * ( 2009 ) 7178 [ http://arxiv.org/abs/0905.1220[0905.1220 ] ]",
    ". m.  guidal , _ generalized parton distributions from deep virtual compton scattering at clas _ , _ phys .",
    "* b689 * ( 2010 ) 156162 [ http://arxiv.org/abs/1003.0307[1003.0307 ] ] .",
    "h.  moutarde , _ extraction of the compton form factor h from dvcs measurements at jefferson lab _ , _ phys .",
    "_ * d79 * ( 2009 ) 094021 [ http://arxiv.org/abs/0904.1648[0904.1648 ] ] .",
    "collaboration , a.",
    "airapetian _ et .  al .",
    "_ , _ separation of contributions from deeply virtual compton scattering and its interference with the bethe ",
    "heitler process in measurements on a hydrogen target _",
    ", _ jhep _ * 11 * ( 2009 ) 083 [ http://arxiv.org/abs/0909.3587 [ 0909.3587 ] ] . collaboration , f.  gautheron _ et .",
    "_ , _ compass - ii proposal _ , _ cern - spsc-2010 - 014 _ ( 2010 ) spsc",
    "[ http://wwwcompass.cern.ch / compass / proposal / compass - ii_proposal / compass% -ii_proposal.pdf[http://wwwcompass.cern.ch / compass / proposal / compass - ii_proposal / compass% -ii_proposal.pdf ] ] .",
    "w.  t. giele , s.  a. keller and d.  a. kosower , _ parton distribution function uncertainties _ , http://arxiv.org/abs/hep-ph/0104052 [ hep - ph/0104052 ] .",
    "j.  c. rojo , _ the neural network approach to parton distribution functions _ , http://arxiv.org/abs/hep-ph/0607122 [ hep - ph/0607122 ] .",
    "phd thesis .",
    "t.  schaul , j.  bayer , d.  wierstra , y.  sun , m.  felder , f.  sehnke , t.  rckstie  and j.  schmidhuber , _ pybrain _ , _ the journal of machine learning research _ * 11 * ( 2010 ) 743746 .",
    "h.  honkanen , s.  liuti , j.  carnahan , y.  loitiere and p.  reynolds , _ new avenue to the parton distribution functions : self - organizing maps _ , _ phys.rev . _ * d79 * ( 2009 ) 034022 [ http://arxiv.org/abs/0810.2598[0810.2598 ] ] .",
    "d.  s. hwang and d.  mller , _ implication of the overlap representation for modelling generalized parton distributions _ , _ phys .",
    "lett . _ * b660 * ( 2008 ) 350359 [ http://arxiv.org/abs/0710.1567 [ 0710.1567 ] ] .",
    "f.  james and m.  roos , _ minuit : a system for function minimization and analysis of the parameter errors and correlations _",
    "* 10 * ( 1975 ) 343367 .",
    "j.  pumplin , d.  r. stump and w.  k. tung , _ multivariate fitting and the error matrix in global analysis of data _ , _ phys .",
    "* d65 * ( 2001 ) 014011 [ http://arxiv.org/abs/hep-ph/0008191[hep-ph/0008191 ] ] .",
    "j.  pumplin , d.  stump , j.  huston , h.  lai , p.  m. nadolsky _ et .",
    "_ , _ new generation of parton distributions with uncertainties from global qcd analysis _ , _ jhep _ * 0207 * ( 2002 ) 012 [ http://arxiv.org/abs/hep-ph/0201195[hep-ph/0201195 ] ] . a.  de  roeck and r.  thorne , _ structure functions _ , http://arxiv.org/abs/1103.0555[1103.0555 ] ."
  ],
  "abstract_text": [
    "<S> we have generated a parametrization of the compton form factor ( cff ) @xmath0 based on data from deeply virtual compton scattering ( dvcs ) using neural networks . </S>",
    "<S> this approach offers an essentially model - independent fitting procedure , which provides realistic uncertainties . </S>",
    "<S> furthermore , it facilitates propagation of uncertainties from experimental data to cffs . </S>",
    "<S> we assumed dominance of the cff @xmath0 and used hermes data on dvcs off unpolarized protons . </S>",
    "<S> we predict the beam charge - spin asymmetry for a proton at the kinematics of the compass ii experiment . </S>"
  ]
}