{
  "article_text": [
    "the pattern recognition functionality found in the ventral visual pathway of the primate brain is supported by a multi - layer , spiking biological network which is able to learn to recognize new patterns .",
    "an early and highly influential model of this computation is found in @xcite .",
    "they introduced a hierarchy of simple / complex cells ( model neurons ) that incorporated feature extraction and location invariance pooling over larger and larger receptive field sizes .",
    "the design has been used in many subsequent deep networks and is a member of the class of convolutional networks , that dates back to @xcite and @xcite .",
    "the model in @xcite did not use spiking neurons and thus could not support spike - timing - dependent plasticity ( stdp ) learning , which is known to occur in many parts of the mammalian brain . furthermore , under some conditions the recognition process in primates is so rapid that there is barely enough time for even one spike to travel from the retina to the decision region in the inferior temporal cortex @xcite . the first artificial spiking neural network ( snn ) that included stdp and",
    "was designed to address the speed of recognition observed in the human and primate brain is that of masquelier and thorpe @xcite .",
    "empirical data imposes the constraint that the majority of information used in recognition is contained in the first neural spike for any neuron in the feedforward pipeline .",
    "the paper @xcite presented a model that demonstrated how this was computationally possible and the information provided by the first spike was sufficient to drive an stdp - based feature discovery algorithm to support the recognition process .",
    "the initial work was performed on the caltech data set which presented images in a canonical viewpoint .",
    "later work in @xcite showed that this algorithm would scale to larger data sets which showed the same object from multiple views and at multiple sizes .",
    "the recognition performance compared favorably with other approaches , specifically , @xcite and @xcite .",
    "one limitation of the work in @xcite is that , despite the key observations underlying the design of the model , it raises new questions about biological realism that beg to be answered .",
    "specifically , there are two limitations that the work presented here considers .",
    "first , spike generation in @xcite is performed by non - leaky integrate - and - fire ( if ) neurons .",
    "this may not be a serious limitation , given the short time scale under which the model operates .",
    "however , it would be prudent to examine the model s performance using a more realistic spike - generation mechanism .",
    "this is possible by replacing the if neuron with a simplified izhikevich model neuron @xcite .",
    "the second question is why the stdp version used in @xcite is effective .",
    "put differently , can we understand why this , or alternative stdp mechanisms , perform effective feature discovery within this framework ?",
    "the work in @xcite shows that indeed that the stdp used in @xcite is an effective feature discovery mechanism .",
    "furthermore , a theory of how global properties of the network emerge through stdp was developed in  @xcite .",
    "the present paper takes preliminary steps to develop a probabilistic or bayesian variant of the original model in @xcite .",
    "in particular , we are inspired by the papers of nessler et al . @xcite .",
    "those papers show that an appropriately chosen stdp rule with a particular probabilistic interpretation , when embedded in an appropriately designed winner - take - all ( wta ) network , is able to approximate online expectation maximization .",
    "their paper also includes proof - of - concept experiments showing that implementations of the model are able to perform pattern classification .",
    "one drawback of the model in @xcite is that its trainable ` excitatory ' connections use negative weights .",
    "this is because the trained weights converge to the log of a probability value ( explained later ) .",
    "this gives an elegant probabilistic interpretation to the weights but since the logarithm of a probability value is negative , the weights are necessarily negative .",
    "the authors note that theoretically the weights can be shifted to positive values by choosing a positive parameter , @xmath0 , equal to the magnitude of the largest negative weight in the implementation .",
    "the present paper seeks a solution where the ( excitatory ) weights are positive .",
    "although the network in @xcite has five layers , it can be viewed functionally as consisting of three feedforward - connected modules : feature extraction , feature discovery , and classification . the feature extraction and discovery components",
    "are duplicated across five spatial scales .",
    "the feature discovery module uses a simple form of spike - time - dependent plasticity ( stdp ) based learning .",
    "we compare the if neuron with a simplified izhikevich regular - spiking ( rs ) neuron which models the spike generation dynamics of one type of neocortical pyramidal cell , the principle neuron in the neocortex .",
    "the present research studies variants of this network when stdp is modified and when the model neurons are modified . specifically , we modified the original implementation in @xcite .",
    "the motivation with respect to stdp was to obtain some insight as to why the learning rule is able to support feature discovery and to assess the effectiveness of our new probabilistic stdp rule in feature discovery .",
    "the motivation with regard to using different model neurons was to incrementally improve the biological realism of the model and to examine the robustness of stdp - based feature acquisition with a different model neuron .",
    "finally , if our manipulations do not have an effect on the qualitative performance of the model , either positive or negative , then we have established that the model has some degree of robustness .",
    "stdp modifies synaptic weights in response to local timing differences between pre- and postsynaptic spikes . the existence of several variants of stdp within synapses is well established in biological networks @xcite . in most cases , however , there is little consensus about its larger role in learning for biological networks .",
    "idealized models of stdp have been studied extensively .    in recent literature ,",
    "there have been at least two approaches to addressing the question of why biological stdp might be effective in learning .",
    "the first and most established approach seeks to obtain an idea of what kinds of temporal spike codes stdp produces @xcite and what kind of codes stdp can learn to recognize @xcite .",
    "the second approach assumes that the relevant biological networks implement a bayesian computation .",
    "empirical studies have provided evidence that the some brain areas may perform a bayesian analysis of sensory stimuli  @xcite . also , it is claimed that the spike generation process is stochastic with exponential function upon the membrane potential  @xcite .",
    "probabilistic interpretations of stdp could form a theoretical link to the broader area of machine learning .",
    "nessler et al . ( 2009 , 2013 ) showed that a version of the stdp rule can perform bayesian computation in a spiking winner - take - all ( wta ) network  @xcite .",
    "based on the nessler et al .",
    "( 2013 ) stdp rule , kappel et al .",
    "( 2014 ) developed a generic cortical microcircuit to approximately implement the hidden markov model ( hmm ) learning  @xcite .",
    "in addition , a modified variant of this probability - based stdp has been employed in a gaussian mixture model ( gmm ) training of the hmm equipped by embedded snns in its states  @xcite .",
    "the present work is inspired by the last approach and seeks to study a probabilistic rule in the context of the network used in @xcite .",
    "the snn architecture in the previous study ( masquelier and thorpe , 2007  @xcite ) is shown in fig .",
    "[ fig : snnarchitecture ] .",
    "the network consists of four layers hierarchy of spiking neurons ( s1-c1-s2-c2 ) in temporal domain .",
    "simple cells ( s ) compute a linear sum ( integration ) while complex cells ( c ) compute nonlinear max pooling . the last layer ( layer 5 ) performs a classification over visual features received from c2 .",
    "layer s1 detects edges of the image with four orientations using intensity - to - latency conversion .",
    "the wta between s1 cells is implemented to find the best matching orientation .",
    "c1 maps subsample s1 maps by taking the maximum response over a square neighborhood .",
    "s2 is selective to intermediate complexity visual features , defined as a combination of oriented edges .",
    "c2 takes the maximum response of s2 over all positions and scales .",
    "the maximum operation of the complex cells propagates the first spike emitted by a given group of afferent neurons because of the time - to - first - spike framework supposed in this architecture .",
    "stdp learning occurs in the weights projecting from layer c1 to layer s2 .",
    "the stdp has the effect of concentrating large synaptic weights on afferent neurons that systematically fire early , while postsynaptic spike latencies decrease .",
    "the synaptic weights are updated and duplicated at all positions and scales .",
    "the synaptic weights are considered between 0 and 1 to ensure excitatory synapses .",
    "a simplified stdp has been used in this structure in which the time window is supposed to cover the whole spike wave .",
    "when a cell fires , local inhibition prevents other cells in that layer from firing at the same scale and within a square neighborhood of the firing position .",
    "the final membrane potential measured in the c2 cells ( and is contrast invariant ) is used for training the radial basis function ( rbf ) classifier .",
    "images are processed by the network one by one and the neurons in each layer are allowed to fire only once . finally , the classifier layer utilizes rbf classification to recognize the patterns according to the visual features received from c2 .    .",
    "in the current investigation , we change the stdp rule and neuron models to study the effect on performance .",
    "( from @xcite),width=340 ]",
    "in a later section , we will examine network classification performance using a novel stdp rule . here , we explain the original stdp rule used in @xcite , along with the new rule . neither rule needs to compute the exact time difference between pre- and postsynaptic spikes .      the original stdp rule given in @xcite",
    "is shown below .",
    "@xmath1 denotes the weight from presynaptic neuron @xmath2 to postsynaptic neuron @xmath3 .",
    "@xmath4    the first case describes ltp and the second describes ltd . @xmath5 and @xmath6 denote firing times of units @xmath2 and @xmath3 , respectively .",
    "weights fall in the range ( 0 , 1 ) .",
    "note that the quantity @xmath7 is the slope of the sigmoidal function .",
    "therefore , if @xmath8 and @xmath9 , weight changes will be largest when @xmath1 is @xmath10 and weight changes will become arbitrarily small as @xmath1 approaches the bounds 0 or 1 .",
    "in the most of simulations , the magnitudes of amplification parameters @xmath11 and @xmath12 are kept in a ratio of 4/3 .      the proposed new rule yields weights with a probabilistic interpretation after convergence .",
    "our new rule is modified from @xcite .",
    "the rule from @xcite , when embedded in an appropriate winner - take - all network with poisson spiking neurons can approximate expectation maximization learning .",
    "it is given below .",
    "ltp occurs if the presynaptic neuron fires briefly ( e.g. , within @xmath14 ms ) before the postsynaptic neuron .",
    "otherwise ltd occurs .",
    "the rule is unusual in that the magnitude of the ltp weight adjustment is exponentially related to the current value of the weight .",
    "additionally , weight values are always negative .",
    "after convergence , the weight value is equal to the log of the probability that the presynaptic neuron will have fired within the @xmath15 interval , given that the postsynaptic neuron has fired .",
    "our snn uses positive weights for the excitatory neurons in the feedforward layers and therefore the rule  ( [ nessler ] ) is not applicable .",
    "therefore , we modified ( [ nessler ] ) to obtain ( [ at_stdp ] ) below .",
    "@xmath16    where @xmath17 and @xmath18 are the amplification parameters used in ( [ masquelierthorpestdp ] ) .",
    "the synaptic weights are constrained to be positive during the training process .      to interpret the behavior of the stdp rule probabilistically , we follow the approach of nessler et al .",
    "( 2013 ) , p.  21",
    ", formula 28  @xcite . at equilibrium",
    ", the expected weight change will be zero .",
    "this can be written and then simplified as shown below .",
    "the notation @xmath19 means that the postsynaptic neuron fired on a given stimulus presentation .",
    "the notation @xmath20 means that the presynaptic neuron fired before the postsynaptic neuron on the same stimulus presentation .",
    "@xmath21=0 \\leftrightarrow\\\\ a^+ \\cdot p_\\mathbf{w}^{*}(y_i = 1 | z_k = 1 ) e^{-w_{ki } } - a^- \\cdot p_\\mathbf{w}^{*}(y_i = 0 | z_k = 1 ) = 0 \\leftrightarrow\\\\ a^+ \\cdot p_\\mathbf{w}^{*}(y_i = 1 | z_k = 1 ) e^{-w_{ki } } + a^-\\cdot p_\\mathbf{w}^{*}(y_i = 1 | z_k = 1 ) = a^-   \\leftrightarrow\\\\ e^{-w_{ki}}=\\frac{a^- -a^- \\cdot   p_\\mathbf{w}^{*}(y_i = 1 | z_k = 1)}{a^+ \\cdot   p_\\mathbf{w}^{*}(y_i = 1 | z_k = 1 ) } \\leftrightarrow\\\\ w_{ki}=\\ln \\big(\\frac{a^+}{a^- } \\cdot \\frac{p_\\mathbf{w}^{*}(y_i = 1 | z_k = 1)}{1-p_\\mathbf{w}^{*}(y_i = 1 | z_k = 1 ) } \\big ) \\leftrightarrow \\nonumber \\end{array}\\ ] ] @xmath22 where , @xmath23 denotes an equilibrium probability .    the synaptic weight , @xmath1 , in ( [ eqn : equilibrium ] ) specifies the log odds ( logit ) of the probability .",
    "the probability , @xmath23 , denotes the casual effect of presynaptic neuron in postsynaptic spike in which the stdp rule is triggered .",
    "the odds ratio is the ratio of the probability that the event of interest occurs to the probability that it does not  @xcite .",
    "the event of interest is the presynaptic activation right before the postsynaptic neuron fires .",
    "positive log odds ( @xmath24 ) specifies a probable condition in which the synaptic weight is subject to ltp .",
    "the parameter @xmath25 determines a stochastic threshold for postsynaptic firing .",
    "the negative log odds triggers ltd until the synaptic weight reaches its minimum efficacy .",
    "the minimum efficacy is when the presynaptic neuron does not have an effect on the postsynaptic membrane potential ( disconnected synapse ) .",
    "a number of previous empirical studies have provided evidence that bayesian analysis of sensory stimuli occurs in the brain  @xcite . in bayesian inference , the hidden causes are inferred using prior knowledge and the likelihood of the observations to obtain a posterior probability . in the case of two classes @xmath26 and @xmath27 , the posterior probability of class @xmath26 is introduced by the sigmoid function as follows  @xcite @xmath28 where @xmath29 is @xmath30 which is interpreted as log odds by @xmath31 equation  ( [ eq : oddbishop ] ) is analogous to the synaptic weight in ( [ eqn : equilibrium ] ) .",
    "we consider the maximum possible weight value after a series of the ltp events during training .",
    "does an excitatory synaptic weight converge to some constant or grow without bound as a function of ltp events ? to address this , we start with initial weight , @xmath32 .",
    "the next synaptic weights after performing @xmath33 ltp rules are @xmath34 . according to  ( [ at_stdp ] ) , @xmath35 the maximum weight change happens when @xmath36 .",
    "so , if we suppose @xmath37 ( maximum change ) , the final synaptic weight is obtained by : @xmath38 and to find an upper bound for the synaptic weight in @xmath33 ltp operations , we have : @xmath39 therefore , @xmath40 finally , a synaptic weight after infinite iterations of the ltp adjustments ( with @xmath41 ) will fall in range @xmath42 where , @xmath43 is an upper bound if @xmath44 .",
    "izhikevich s model for the rs spiking neuron concisely , but accurately , captures the spike - generation dynamics of one subtype of pyramidal neuron  @xcite .",
    "we used a simplified version of this so that it would be compatible with the snn implementation ( we only use the first generated spike ) .",
    "its spike - generation dynamics is specified in equation set  ( [ eqn : izhikevich1 ] ) using state variables @xmath45 and @xmath46 .",
    "[ eqn : izhikevich1 ] @xmath47%\\nonumber\\end{aligned}\\ ] ]    @xmath45 denotes the membrane potential and @xmath46 specifies a recovery factor and keeps the membrane potential near the resting value , @xmath48 .    the many different parameters : capacity ( @xmath49 ) , threshold ( @xmath50 ) , @xmath29 , @xmath51 , @xmath0 , @xmath52 , and @xmath3 customize the spike - generation dynamics .",
    "the model parameters to obtain an rs spiking neuron are : @xmath53 ; where @xmath54 and @xmath55 were changed according to the model requirements .",
    "the spike time occurs when @xmath45 reaches @xmath54 .",
    "@xmath56 is set to the net synaptic input .",
    "we performed two experiments . both experiments compared four network architectures . specifically , we studied four types of the 5-layer snns as follows :    * * snn 1 : * the original 5-layer convolutional network of if spiking neurons in  @xcite with the original parameters . *",
    "* snn 2 : * the snn equipped with our probabilistic stdp learning rule . *",
    "* snn 3 : * the original snn using izhikevich - like neurons instead of if neurons . * * snn 4 : * the snn using izhikevich - like neurons and equipped with the probabilistic stdp rule .    in all of the snns , the learning parameter @xmath57 at the beginning and",
    "is multiplied by @xmath58 every @xmath59 postsynaptic spikes , until a maximum value of @xmath60 .",
    "@xmath18 is adjusted such that @xmath61 .            to compare with the original work @xcite",
    ", we evaluated the snn using a subset of the caltech dataset ( http://www.vision.caltech.edu/archive.html ) containing faces ( positive class ) and background images ( negative class ) that was used in the original model .",
    "we also compared motorbikes to background images .",
    "images were converted to gray - scale as described in @xcite .",
    "examples images for face and background appear in fig .",
    "[ fig : nonoisesamples ] ( more examples , including motorbikes , are given in @xcite ) .",
    "the dataset was equally divided into training and testing sets .    in one simulation ,",
    "the snns were applied on the face / background ( non - face ) training samples . in a second simulation ,",
    "the bike / background data was used .",
    "these simulations produced ten class - specific c2 features .",
    "each c2 cell represents a combination of edges in respect to an input image .",
    "a representation of the input images can be reconstructed by convolving the weight matrix with a set of kernels representing oriented bars  @xcite . during the training process , initial iterations",
    "do not show a clear reconstructed preferred stimulus .",
    "after training , the c2 features are able to represent the face images .",
    "thus , the c2 features developed selectivity to face features .",
    "learning was stopped after 1000 iterations while the weight matrices were saved after each 100 iterations to track the learning process .",
    "the trained network was evaluated using the testing set .",
    "following @xcite , we used two measures for evaluating the model : 1 ) the accuracy rate when the false positive rate equals the missed rate ( equilibrium point ) ; and , 2 ) a term of area under the receiver operator characteristic ( roc ) .",
    "[ fig : barchartsacc ] shows the maximum accuracy rate obtained from the snn models ( * snn 1 * through * snn 4 * ) .",
    "it also includes standard errors of the mean .",
    "these were computed by running each simulation nine times ( @xmath62 ) with a different set of initial values for the weights .",
    "different subsamples for training and testing data were not used in this first experiment but they were used in second experiment .",
    "[ fig : barchartsacc ] shows a trend that the probabilistic stdp rule gives better accuracy regardless of whether the if neuron ( * snn1 * vs * snn2 * ) or the izhikevich - like neuron is used ( * snn3 * vs * snn4 * ) .",
    "the overall performance for the motorbikes data was lower .",
    "[ fig : barchartsroc ] illustrates the same pattern of results for the maximum roc obtained from the snn models for both face and motorbike datasets .",
    "two - tailed t - tests were performed to assess the statistical significance of the accuracy trends .",
    "the following comparisons were performed : * snn1 * versus * snn2 * , * snn3 * versus * snn4 * , and * snn1 * versus * snn4 * for both faces and motorbikes .",
    "the results are shown in figs .",
    "[ fig : barchartsacc ] and [ fig : barchartsroc ] in terms of @xmath63 ( * ) , @xmath64 ( * * ) , @xmath65 ( * * * ) , and @xmath66 ( * * * * ) .",
    "the differences reached more significance for the network that used izhikevich - like neurons .",
    "thus , we claim that our probabilistic variant stdp rule significantly improves performance when izhikevich - like neurons are used , either with faces or motorbikes .",
    "[ fig : reconstruction ] shows reconstructions for the ten c2 cells on the face data for the four snns .",
    "they indicate that face - like features were discovered by all snn variants .",
    "thus , that property of the original snn in @xcite is shown to be robust under the changes introduced in this experiment .        fig .  [ fig : reconstructioniter ] shows the reconstructions for a selected face feature during the training process ( 100 to 1000 iterations ) for each of the snns .",
    "qualitatively , the results are similar for all of the snns .        fig .  [",
    "fig : cmp3 ] shows the accuracy results for all of snns applied to the faces data .",
    "the most salient trend is that * snn 3 * and * snn 4 * , which both use the izhikevich - like units , perform better relatively early in the training , at iterations 200 and 300 , but the advantage is lost later in the training .",
    "also , * snn 3 * seems to perform slightly worse than the others .",
    "standard errors were not computed , but this was done in experiment 2 .",
    "the results in experiment  1 need further exploration because , first , the results are possibly compromised by ceiling effects ( especially for the faces data ) and , second , we did not perform cross - validation by using different samples of training and testing data .      to address the possible ceiling effect , we used noisy test images ( gaussian noise with zero mean and standard deviation of @xmath67 ) for the testing phase .",
    "the purpose was to reduce the accuracy rate so that ceiling effects were no longer an issue .",
    "five sets of runs ( @xmath68 ) were performed for both faces and motorbikes .",
    "both testing and training images sets had 218 items .",
    "for each simulation , 175 items were sampled from each set to be used in that simulation .",
    "standard errors were computed for each of 40 data points .",
    "[ fig : noisyfaces_all ] shows the face results for all of snns ( * snn 1 * through * snn 4 * ) including standard errors .",
    "[ fig : noisymotorbikes_all ] shows the results for motorbikes . in both figures the accuracy rates are below 97 percent , so using the noisy images",
    "eliminated the complication of a ceiling effect .",
    "for the faces ( fig .",
    "[ fig : noisyfaces_all ] ) , all of the trends shown in fig .",
    "[ fig : cmp3 ] are preserved in these results .",
    "most notably , the trend where the probabilistic stdp variant improves accuracy regardless of whether if units or izhikevich - like units are used still persists . for the motorbikes , the proposed probabilistic stdp performs better",
    "; however , the accuracy performances are close across all manipulations . from the data taken as a whole",
    ", we can also conclude that the original model is robust across the manipulations .",
    "experiments 1 and 2 strongly suggest that the proposed probabilistic stdp rule improves the network performance in comparison to the original one .",
    "we ran two additional control experiments to improve our confidence in this result . the first experiment controlled for weight selection and the second experiment tested several other object categories .    figs .  [ fig : reconstruction ] and  [ fig : reconstructioniter ] show more selected weights for probabilistic stdp ( lines in face features ) .",
    "hence , is the advantage of the probabilistic rule because it selected more weights ? to address this , ten experiments with different @xmath69 ratios were run on the face recognition task to assess the effect of the number of selected weights on accuracy .",
    "[ fig : weightsvsacc ] shows the accuracy rate versus average number of the selected weights .",
    "this figure shows that the probabilistic rule consistently outperforms the classic rule for a given number of selected weights .     and @xmath70 .",
    "the trend lines show better performance of the probabilistic variation of the stdp introduced in this paper.,width=321 ]    additionally , a subset of the 3d - object images provided by savarese et al . at cvglab",
    ", stanford university  @xcite was used for new experiments .",
    "the accuracy rates of * snn 1 * through * snn 4 * are shown in fig .",
    "[ fig:3dres ] .",
    "this figure shows better performance of probabilistic stdp ( * snn 2 * and * snn 4 * ) than original one ( * snn 1 * and * snn 3 * respectively ) in 12 out of 16 cases .",
    "this paper studied the performance of the spiking network given in @xcite when the original stdp learning rule was replaced with a novel rule that had a probabilistic interpretation and also when the model neuron was changed from an if neuron to an izhikevich - like neuron .",
    "our findings are the following .",
    "first , our experiments introduced a probabilistic stdp variant which improved the accuracy rate .",
    "most notably , the magnitude of the weight adjustments for ltp was an exponential function of the weight magnitude .",
    "this is an entirely different type of rule .",
    "second , when we replaced the non - leaky if neurons with izhikevich - like neurons , accuracy performance remained robust .",
    "this is significant because the if neuron is the simplest possible spike generator whereas the izhikevich neuron has a much more complicated spike generation mechanism .",
    "our main conclusion is that our probabilistic variant of stdp improved performance for instances of the model that used either if neurons or izhikevich - like neurons .",
    "a secondary conclusion is that the original model is robust against more biologically realistic neurons ( izhikevich - like rs neurons ) .",
    "although izhikevich - like neuron did not improve the performance , considering the nature of the manipulations , new knowledge has in fact been obtained .",
    "we are not aware of other work that has studied variations of this model .    a priori",
    ", one would expect that the most likely effect of any drastic change , like those listed above , would impair network performance , but instead , the performance remained robust .",
    "future researchers experimenting with this model , for instance to build a deep spiking network , can have some measure of confidence that the performance of the individual s / c modules will remain robust when placed in a larger context .",
    "serre , t. , wolf , w. , bileschi , s. riesenhuber , m. , poggio , t. , robust object recognition with cortex - like mechanisms , _ ieee transactions on pattern analysis and machine intelligence _ , 29(3 ) , 411 - 426 , 2007 .",
    "kheradpisheh , s. r. , ganjtabesh , m. , masquelier , t. , bio - inspired unsupervised learning of visual features leads to robust invariant object recognition .",
    "arxiv preprint arxiv:1504.03871 , 2015 , to appear in _",
    "masquelier , timothe , and simon j. thorpe , learning to recognize objects using waves of spikes and spike - timing - dependent plasticity . _",
    "the 2010 international ieee joint conference on neural networks ( ijcnn ) _ , 2010 .",
    "nessler , b. , pfeiffer , m. , buesing l. , maass , w. , bayesian computation emerges in generic cortical microcircuits through spike - timing - dependent plasticity . _",
    "plos computational biology _ , 9(4 ) : e1003037 , 1 - 30 , 2013 .",
    "kappel , david , bernhard nessler , and wolfgang maass , stdp installs in winner - take - all circuits an online approximation to hidden markov model learning .",
    "_ plos computational biology _ , 10(3 ) : e1003511 , 1 - 22 , 2014 .",
    "tavanaei , amirhossein , and anthony s. maida , studying the interaction of a hidden markov model with a bayesian spiking neural network .",
    "_ ieee 25th international workshop on machine learning for signal processing ( mlsp ) _ , boston , ma , usa , 2015 ."
  ],
  "abstract_text": [
    "<S> this paper explores modifications to a feedforward five - layer spiking convolutional network ( scn ) of the ventral visual stream [ masquelier , t. , thorpe , s. , unsupervised learning of visual features through spike timing dependent plasticity . </S>",
    "<S> plos computational biology , 3(2 ) , 247 - 257 ] . </S>",
    "<S> the original model showed that a spike - timing - dependent plasticity ( stdp ) learning algorithm embedded in an appropriately selected scn could perform unsupervised feature discovery . </S>",
    "<S> the discovered features where interpretable and could effectively be used to perform rapid binary decisions in a classifier .    in order to study the robustness of the previous results </S>",
    "<S> , the present research examines the effects of modifying some of the components of the original model . for improved biological realism </S>",
    "<S> , we replace the original non - leaky integrate - and - fire neurons with izhikevich - like neurons . </S>",
    "<S> we also replace the original stdp rule with a novel rule that has a probabilistic interpretation . </S>",
    "<S> the probabilistic stdp slightly but significantly improves the performance for both types of model neurons . </S>",
    "<S> use of the izhikevich - like neuron was not found to improve performance although performance was still comparable to the if neuron . </S>",
    "<S> this shows that the model is robust enough to handle more biologically realistic neurons . </S>",
    "<S> we also conclude that the underlying reasons for stable performance in the model are preserved despite the overt changes to the explicit components of the model . </S>"
  ]
}