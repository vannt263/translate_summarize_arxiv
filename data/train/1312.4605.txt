{
  "article_text": [
    "the explosion in the collection and interest in big data in recent years has brought new challenges to modern statistics .",
    "bayesian analysis , which has benefited from the ease and generality of sampling - based inference , is now suffering as a result of the huge computational demand of posterior sampling algorithms , such as markov chain monte carlo ( mcmc ) , in large scale settings .",
    "mcmc faces several bottlenecks in big data problems due to the increasing expense in likelihood calculations , to the need for updating latent variables specific to each sampling unit , and to increasing mixing problems as the posterior becomes more concentrated in large samples .    to accelerate computation ,",
    "efforts have been focused in three main directions .",
    "the first is to parallelize computation of the likelihood at each mcmc iteration @xcite . under conditional independence ,",
    "calculations can be conducted separately for mini batches of data stored on different machines , with results fed back to a central processor .",
    "this approach requires communication within each iteration , which limits overall speed .",
    "a second strategy focuses on accelerating expensive gradient calculations in langevin and hamiltonian monte carlo @xcite using stochastic approximation based on random mini batches of data @xcite .",
    "a third direction is motivated by the independent product equation from @xcite .",
    "the data are partitioned into mini batches , mcmc is run independently for each batch without communication , and the chains are then combined to mimic draws from the full data posterior distribution . by running independent mcmc chains , this approach bypasses communication costs until the combining step and increases mcmc mixing rate , as the subset posteriors are based on smaller sample sizes and hence effectively annealed .",
    "the main open question for this approach is how to combine the chains to obtain an accurate approximation ?    to address this question , @xcite suggested to use averaged posterior draws to approximate the true posterior samples . @xcite and @xcite instead make use of kernel density estimation to approximate the subset posterior densities , and then approximate the true posterior density following .",
    "based on our experiments , these methods have adequate performance only in specialized settings , and exhibit poor performance in many other cases , such as when parameter dimensionality increases and large sample gaussian approximations are inadequate .",
    "we propose a new combining method based on the independent product equation , which attempts to address these problems .    in section 2 ,",
    "we first describe problems that arise in using in a naive way , briefly state the motivation for our method , and provide theoretic justification for the approximation error . in section 3 we then describe specific algorithms and discuss tuning parameter choice .",
    "section 4 assesses the method using extensive examples .",
    "section 5 contains a discussion , and proofs are included in the appendix .",
    "the fundamental idea for the new method is as follows . for a parametric model @xmath0 , assume the data contain @xmath1 conditionally independent observations , which are partitioned into @xmath2 non - overlapping subsets , @xmath3 .",
    "the following relationship holds between the posterior distribution for the full data set and posterior distributions for each subset , @xmath4 where @xmath5 is the prior distribution for the full data set and @xmath6 is that for subset @xmath7 . as we will only be obtaining draws from the subset posteriors to approximate draws from @xmath8 , we have flexibility in the choice of @xmath6 . if we further require @xmath9 , the above equation can be reformulated as @xmath10 which we refer to as the independent product equation .",
    "this equation indicates that under the independence assumption , the posterior density of the full data can be represented by the product of subset posterior densities if the subsets together form a partition of the original set .",
    "however , despite this concise relationship , sampling from the product of densities remains a difficult issue .",
    "@xcite use a convolution product to approximate this equation , resulting in an averaging method .",
    "this approximation is adequate when the subset posteriors are close to gaussian , as is expected to hold in many parametric models for sufficiently large subset sample sizes due to the bernstein - von mises theorem @xcite .    another intuitive way to apply is to use kernel based non - parametric density estimation ( kernel smoothing ) .",
    "using kernel smoothing , one obtains a closed form approximation to each subset posterior , with these approximations multiplied together to approximate the full data posterior .",
    "this idea has been implemented recently by @xcite , but suffers from several drawbacks .    1",
    ".   _ curse of dimensionality in the number of parameters @xmath11_. it is well known that kernel density estimation breaks down as @xmath11 increases , with the sample size ( number of posterior samples in this case ) needing to increase exponentially in @xmath11 to maintain the same level of approximation accuracy .",
    "subset posterior disconnection_. because of the product form , the performance of the approximation to the full data posterior depends on the overlapping area of the subset posteriors , which is most likely to be the tail area of a subset posterior distribution .",
    "hence , slight deviations in light - tailed approximations to the different subset posteriors might lead to poor approximation of the full data posterior .",
    "( see the right figure in fig.[fig:0 ] ) 3 .",
    "_ mode misspecification_. for a multimodal posterior , averaging ( noticing that the component mean of the kernel smoothing method is the average of subset draws ) can collapse different modes , leading to unreliable estimates .    to ameliorate these problems ,",
    "we propose a different method for parallelizing mcmc .",
    "this new method , designated as the _ weierstrass sampler _ , is motivated by the weierstrass transform , which is related to kernel smoothing but from a different perspective .",
    "our approach has good performance including in cases in which large sample normality of the posterior does not hold . in the rest of the article",
    ", we will use the term @xmath12 to denote general posterior distributions and @xmath13 for subset posteriors , in order to match the typical notation used with weierstrass transform .",
    "the key of our weierstrass sampler lies in the weierstrass transform , @xmath14 which was initially introduced by @xcite .",
    "the original proof shows that @xmath15 converges to @xmath12 pointwise as @xmath16 tends to zero .",
    "our method approximates the subset densities via the weierstrass transformation .",
    "we avoid inheriting the problems of the kernel smoothing method by directly modifying the targeted sampling distribution instead of estimating it from the subset posterior draws .",
    "in particular , we attempt to directly sample the approximated draws from the transformed densities instead of the original subset posterior distributions . applying the weierstrass transform to all subset posteriors and denoting @xmath17 by @xmath13 ,",
    "the full set posterior can be approximated as @xmath18 where @xmath19 , @xmath20 , @xmath21 and @xmath22 .",
    "the above equation can be viewed as the marginal distribution of the random variable @xmath23 , derived from its joint distribution with the random variables @xmath24 with joint density @xmath25 the original subset posteriors appear as components of this joint distribution , enabling subset - based posterior sampling . moreover , the conditional distribution of the target random variable @xmath23 given the subset random variables is simply gaussian .",
    "the original weierstrass transformation was stated in terms of the gaussian kernel . for flexibility , we relax this restriction by broadening the choice of kernel functions , justifying the generalizations through lemma 1 ( see appendix ) . following from the previous section , the posterior density can be approximated as @xmath26 where the last term can be viewed as the marginal distribution of random variable @xmath23 , derived from its joint distribution with the random variables @xmath27 and a joint density @xmath28 .",
    "if this joint density is proper ( illustrated in theorem [ thm:1 ] ) , one may sample @xmath23 from this distribution , utilizing the subset samplers as components .",
    "the details will be discussed in the next section .",
    "this section will focus on quantifying the approximation error of .",
    "the results will be stated in terms of both one - dimensional and multivariate models .",
    "the detailed derivation is only provided in the one - dimensional case in the appendix , as the multivariate derivation proceeds along identical lines .",
    "a hlder @xmath29 smooth density function ( for definition see lemma 1 ) is always bounded on @xmath30 for @xmath31 .",
    "let @xmath32 denote the maximum value of the subset posterior densities .",
    "we have the following result ( for the one - dimensional case ) .    [ thm:1 ] if the posterior densities and the kernel functions satisfy the condition in lemma 1 with @xmath33 and @xmath34 , i.e. , the posterior density is at least second - order differentiable and the kernel function has finite variance , then the distribution defined in is proper and there exists a positive constant @xmath35 such that when @xmath36 , the total variation distance between the posterior distribution and the approximation follows @xmath37 where @xmath38 and @xmath39 are the normalizing constants , and @xmath40 are defined as @xmath41     for multivariate distributions , the kernel variance @xmath42 should be substituted by the kernel covariance @xmath43 and we have a similar result .    [ corollary1 ] ( multivariate case )",
    "if the p - variate posterior densities @xmath44 and the kernel functions satisfy the conditions in lemma 2 with @xmath33 and @xmath45 , then for sufficiently small @xmath46 , where @xmath47 , the total variation distance between the posterior and the approximated density follows @xmath48 where @xmath49 are normalizing constants , and @xmath50 are defined in theorem 1 .    in the error bound in theorem 1 , the constants @xmath51 and @xmath35 do not vary much even as the sample size increases to infinity .",
    "in fact , they will converge to constants in probability ( see discussions in the appendix after the proof of theorem [ thm:1 ] ) . as a result ,",
    "the choice of the tuning parameters @xmath42 is independent of the sample size , which is a desirable property .",
    "as the error characterized by theorem [ thm:1 ] maintains a reasonable level , the approximation will be effective . in this section , a refinement sampler is proposed . for convenience",
    "we will focus on the gaussian kernel if not specified otherwise , but modifications to other kernels are straightforward .",
    "the algorithm is referred to as a _ weierstrass refinement sampler _ , because samples from an initial rough approximation to the posterior ( obtained via laplace , variational approximations or other methods ) are _ refined _ using information obtained from parallel draws from the different subset posteriors within a weierstrass approximation .",
    "typically , the initial rough approximations can be obtained using parallel computing algorithms ; for example , laplace and variational algorithms are parallelizable .",
    "equation can be used to obtain a gibbs sampler . for the gaussian kernel",
    ", the density in can be reformulated into , which can then be used to construct a gibbs sampler as follows ( univariate case ) : @xmath52 this approach takes the parameters in each subset as latent variables , and updates @xmath23 via the average of all latent variables .",
    "the gibbs updating is used as a refinement tool ; that is , the parameters @xmath23 are initially drawn from a rough approximation ( laplace , variational ) and then plugged into the gibbs sampler for one step updating , known as a refinement step .",
    "theorem 2 shows refinement leads to geometric improvement .",
    "[ thm2 ] assume @xmath53 which is an initial approximation to the true posterior @xmath12 . by doing one step gibbs updating as described in , ( with general kernel @xmath54 ) , we obtain a new draw @xmath55 with density @xmath56 . using the notations in theorem 1 ,",
    "if the kernel density function @xmath54 is fully supported on @xmath57 , then for any given @xmath58 , there exists a measurable set @xmath59 such that @xmath60 and , @xmath61 where @xmath62 is a positive value depending on @xmath63 but independent of @xmath64 .",
    "furthermore , if the kernel function satisfies the following condition , @xmath65 for given @xmath66 and @xmath67 , then the total variational distance follows @xmath68 for some @xmath69 , which is independent of @xmath64 .",
    "the proof of theorem [ thm2 ] is provided in the appendix .",
    "though the theorem is stated for univariate distributions , the result is applicable to multivariate distributions as well .",
    "the concrete refinement sampling algorithm is described below as algorithm [ alg : wrs ] .",
    "+ input @xmath43 ( or @xmath42 for univariate case ) for @xmath70 . @xmath71 ; + @xmath72 ;   +   +   +   + [ wrs:6 ] sample @xmath73 ; @xmath74 ; [ wrs:9 ] +   + @xmath75 ; @xmath76 ;    there are a number of advantages of this refinement sampler .",
    "first , the method addresses the dimensionality curse ( issue 1 in section 2 , which appears as the inefficiency of the gibbs sampler and when @xmath16 is small ) , with the large sample approximation only used as an initial rough starting point that is then refined .",
    "we find in our experiments that we have good performance even when the true posterior is high - dimensional , multimodal and very non - gaussian .",
    "second , as can be seen from , the subset posterior densities are multiplied by a common conditional component , which brings them close to each other and limits the problem with subset posterior disconnection ( issue 2 of section 2 ) . in addition , step [ wrs:6]-[wrs:9 ] can be fully parallelized , as each draw is an independent operation . there may be advantages of an iterative version of the algorithm , which runs more than one step of the gibbs update on each of the initial draws ( repeating algorithm [ alg : wrs ] several times ) .",
    "the choice of tuning parameters @xmath43 and the relationship with number of subsets @xmath2 is an important issue .",
    "the parameter @xmath43 on the one hand controls the approximation error for each subset posterior .",
    "apparently , @xmath77 has to be reduced as the number of subsets @xmath2 increases . on the other hand",
    ", @xmath43 also determines the efficiency of the gibbs sampler ( how fast can the gibbs sampler evolve the initial approximation towards the true posterior ) , thus , @xmath43 might need to be chosen adequately large for efficient refinement .",
    "such an argument leads to the conclusion of changing @xmath43 during the refinement sampling process if the refinement will be repeated multiple times .    according to fukunaga",
    "s @xcite approach in choosing the bandwidth , if we attempt to apply kernel density estimation directly to the posterior distribution obtained from the full data set , the optimal choice will be @xmath78 where @xmath11 is the number of parameters , @xmath79 is the total number of posterior samples and @xmath80 is the sample covariance of the posterior distribution ( to be approximated by the inverse of the hessian matrix at mode )",
    ". based on this result , the starting value for @xmath43 could be chosen as @xmath81 , of which the magnitude is comparable to the covariance of each subset posterior distribution , admitting efficient refinement in the beginning .",
    "as the refinement proceeds , the ending point of @xmath43 should be around @xmath82 indicating an accurate approximation .",
    "the tuning parameters in the middle of the process should be chosen in between .",
    "for example , for a 10-step refinement procedure , one could use @xmath81 for the first three steps , @xmath83 for the next five steps and @xmath82 for the last two steps .",
    "algorithm [ alg : wrs ] requires an initial approximation to the target distribution . to avoid this initialization",
    ", we propose an alternative self - adaptive algorithm , which is designated as _",
    "weierstrass rejection sampler_. because of the self - adapting feature , the algorithm suffers from certain drawbacks , which will be discussed along with possible solutions in the latter part of this section .",
    "we begin with a description of the algorithm .",
    "the formula shown in immediately evokes a rejection sampler for sampling from the joint distribution : assuming @xmath84 .",
    "since @xmath85 , we can accept a draw of @xmath86 with probability @xmath87 .",
    "such an approach makes use of the average of the draws from the subsets to generate further posterior draws , which is similar to the kernel smoothing method proposed by @xcite .",
    "however , with slight modification , the weierstrass rejection sampler can use the subset posterior draws as approximated samples directly without averaging , and thus avoid the mode misspecification issue .",
    "the result is stated below ( for univariate case ) .",
    "[ thm3 ] if the subset posterior densities @xmath88 satisfy all the conditions stated in theorem 1 with @xmath33 and posterior draws @xmath89 , and the second order kernel function @xmath90 ( which is a density function ) satisfies that @xmath91 for some positive constant @xmath92 , then for any given @xmath67 , the rejection sampler accepts @xmath93 with probability @xmath94 . referring to the density of the accepted draws by @xmath95 , the total variation distance of the approximation error follows @xmath96 where @xmath97 .",
    "the constants @xmath40 are defined in theorem 1 .",
    "the following corollary is the multivariate version .",
    "( multivariate case ) if the p - variate posterior densities @xmath98 satisfy the conditions in corollary [ corollary1 ] with @xmath33 and posterior draws @xmath89 , and the second order kernels @xmath99 are bounded by positive constant @xmath92 , then for any given @xmath100 , the rejection sampler accepts @xmath93 if @xmath101 where @xmath102 . referring to the density of the accepted draws by @xmath95 , the approximation error follows , @xmath96 where @xmath103 .",
    "the constants @xmath40 are defined in theorem 1 .    with the above theorem",
    ", it is easy to construct a rejection sampler as follows : for each iteration we randomly select one draw from the pool ( or according to some reasonable weights ) , and perform the rejection sampling according to theorem [ thm3 ] .",
    "we repeat the procedure for all iterations and then gather the accepted draws .",
    "the reason that the rejection operation is only conducted on one draw within the same iteration is to avoid incorporating extra undesirable correlation between the accepted draws .",
    "the effectiveness of a rejection sampler is determined by the acceptance rate . for a @xmath11-variate model and @xmath2 subsets ,",
    "the acceptance rate for the weierstrass rejection sampler can be calculated as ( assuming all @xmath104s are equal to @xmath105 ) , @xmath106 for adequately small @xmath105 .",
    "clearly , the acceptance rate suffers from the curse of dimensionality in both @xmath2 and @xmath11 , so the number of posterior samples has to increase exponentially with @xmath2 and @xmath11 .",
    "this is the same problem as with the kernel smoothing method discussed before . to ameliorate the dimensionality curse , we provide the following solution .",
    "the number of subsets @xmath2 is easier to tackle .",
    "it is straightforward to bring @xmath2 down to @xmath107 by using a * pairwise combining strategy * : we first combine the @xmath2 subsets into a pairwise manner to obtain posterior draws on @xmath108 subsets .",
    "this process is repeated to obtain @xmath109 subsets and so on until obtaining the complete data set .",
    "the whole procedure takes about @xmath110 steps , and thus brings the power from @xmath2 down to @xmath110 .",
    "the curse of dimensionality in the number of parameter @xmath11 is less straightforward to address .",
    "if the @xmath11-variate posterior distribution @xmath111 satisfies that the parameters are all independent , then @xmath112 and @xmath113 where @xmath114 and @xmath115 are the posterior marginal densities for the @xmath116 parameter on the full set and on the @xmath117 subset , respectively .",
    "the equation indicates that the posterior marginal distribution satisfies the independent product equation as well .",
    "therefore , one can obtain the joint posterior distribution from the marginal posterior distributions , which are obtained by combining the subset marginal posterior distributions via weierstrass rejection sampling .",
    "this approach thus avoids the dimensionality issue ( since in this case @xmath11 is always equal to 1 ) .    in general",
    ", the posterior marginal distribution does not satisfy the equation because , @xmath118 the first term in the above formula is exactly the independent product equation , while the second term is due to parameter dependence .",
    "however , inspired by this marginal combining procedure , we propose a ( parameter - wise ) sequential rejection sampling scheme that decomposes the whole sampling procedure into @xmath11 steps , where each step is a one - dimensional conditional combining .",
    "the intuition is as follows : we first sample @xmath55 from its subset marginal distribution @xmath119 , and combine the draws via the weierstrass rejection sampler to obtain @xmath120 .",
    "next , we plug in @xmath121 into each subset likelihood to sample @xmath122 from its subset conditional distribution @xmath123 and then combine them to obtain the @xmath124 , where @xmath125 is the normalizing constant ( notice that @xmath125 depends on the value of @xmath121 ) .",
    "we continue the procedure until @xmath126 , obtaining one posterior draw @xmath127 that follows @xmath128    the numerator of is exactly the target formula , while the denominator serves as the importance weights .",
    "the advantage of this new scheme is that it eliminates the dimensionality curse while keeping the number of required sequential steps low ( @xmath11 steps ) .",
    "moreover , the importance weights can be calculated easily and accurately as @xmath129 notice that the integrated functions are all one - dimensional .",
    "thus , an estimated ( kernel - based ) density @xmath130 , combined with the numerical integration technique , is adequate to provide an accurate evaluation for @xmath131 .",
    "an alternative approach for estimating @xmath131 is as follows , @xmath132 where @xmath133 can be obtained from kernel density estimation on the combined draws of @xmath134 ( which requires to sample more than one @xmath134 at each iteration ) .",
    "@xmath135 can be obtained similarly as before from the subset draws .",
    "the whole scheme is described in algorithm [ alg : psis ] .",
    "+ input @xmath136 .",
    "+   set @xmath137 for @xmath70 and @xmath138 . @xmath139 ; +   + sample @xmath140 @xmath141 ; obtain one @xmath142 by combining @xmath143 via weierstrass rejection sampling .",
    "[ alg : rep ] calculate @xmath144 as @xmath145 .",
    "+    algorithm [ alg : psis ] only produces one simulated posterior draw .",
    "therefore , in order to obtain a certain number of posterior draws , the algorithm needs to be executed in parallel on multiple machines .",
    "for example , if one aims to acquire @xmath79 posterior draws , then @xmath79 parallel machines can be used , with each machine able to run @xmath2 sub - threads to fulfill the whole procedure .",
    "it is worth noting that this new scheme is also applicable to the kernel smoothing method proposed by @xcite for overcoming the dimensionality curse : just substituting the step in algorithm [ alg : psis ] with the corresponding kernel method .",
    "the new algorithm still involves a sequential updating structure , but the number of steps is bounded by the number of parameters @xmath11 , which is different from the usual mcmc updating .",
    "a brief interpretation for the effectiveness of the new algorithm is that it changes how error is accumulated .",
    "the original @xmath11-dimension function with a bandwidth @xmath16 will accommodate the error in a manner as @xmath147 , while now with the decomposed p steps , the error is accumulated linearly as @xmath148 which reduces the dimensionality curse .",
    "in this section , we will illustrate the performance of weierstrass samplers in various setups and compare them to other partition based methods , such as subset averaging and kernel smoothing . more specifically",
    ", we will compare the performance of the following methods .    * single chain mcmc : running a single markov chain monte carlo on the total data set .",
    "* simple averaging : running independent mcmc on each subset , and directly averaging all subset posterior draws within the same iteration .",
    "* inverse variance weighted averaging : running mcmc on all subsets , and carrying out a weighted average for all subset posterior draws within the same iteration .",
    "the weight follows @xmath149 where @xmath150 is the posterior variance for the subset @xmath7 .",
    "* weierstrass sampler : the detailed algorithms are provided in previous section . for the weierstrass rejection sampler",
    ", we do not specify the value of the tuning parameter @xmath151 , but instead , we specify the acceptance rate .",
    "( i.e. , we first determine the acceptance rate and then calculate the corresponding @xmath151 ) . *",
    "non - parametric density estimation ( kernel smoothing ) : using kernel smoothing to approximate the subset posteriors and obtain the product thereafter .",
    "because this method is very sensitive to the choice of the bandwidth and the covariance of the kernel function when the dimension of the model is relatively high , in most cases , only the procedure of marginal subset densities combining will be considered in this section . * laplacian approximation : a gaussian distribution with the posterior mode as the mean and the inverse of the hessian matrix evaluated at the mode as the variance .    the models adopted in this section include logistic regression , binomial model and gaussian mixture model .",
    "the performance of the seven methods will be evaluated in terms of approximation accuracy , computational efficiency , and some other special measures that will be specified later .",
    "we made use of the r package `` bayeslogit '' @xcite to fit the logistic model and wrote our own jags code for the gaussian mixture model .",
    "we assess the performance of the refinement sampler in this section .",
    "the first part will be an evaluation of the refinement property as claimed in theorem [ thm2 ] and in the second part we will compare the performance for various methods within the logistic regression framework .",
    "we evaluate the refinement property under both a bi - modal posterior distribution and the real data . for the bi - modal posterior distribution , the two subset posterior densities",
    "are @xmath152 then according to , the posterior on full data set will be roughly ( omitting a tiny portion of probability ) , @xmath153    the initial approximation adopted for the refinement sampling is a normal approximation to @xmath154 which has the same mean and variance .",
    "we trace the change of the refined densities for different numbers of iterations and illustrated them in fig [ fig : ref ] ( left ) .",
    "because the initial approximation is very different from the true target , the tuning parameter @xmath16 will start at a large value and then decline at a certain rate , in particular , @xmath155 . as shown in fig [ fig : ref ] ( left ) , the approximation has evolved to a bi - modal shape after the first iteration and it appears that 10 steps are adequate to obtain a reasonably good result .",
    "fig [ fig : ref ] ( right ) shows part of the refinement results for the real data set ( which will be described in more detail in the real data section ) , in which a logit model was fitted on the 200,000 data set with a partition into 20 subsets .",
    "we set the initial approximation apart from the posterior mode and monitor the evolution of the refinement sampler ( the tuning parameters are chosen according to , and @xmath83 is chosen to be the inverse of the hessian matrix at the mode ) .",
    "the refinement sampler quickly moves from the initial approximation to the truth in just 10 steps .",
    "( no posterior distribution from a full mcmc was plotted , as the sample size is too large to run a full mcmc )      we adopt the logistic model for assessing the approximation performance .",
    "the logistic regression model is broadly adopted in many scientific fields for modeling categorical data and conducting classification , @xmath156 where @xmath157 is the corresponding link function and @xmath158 is the intercept . the predictors @xmath159 follow a multivariate normal distribution @xmath160 , and the covariance matrix @xmath161 follows @xmath162 where @xmath163 will be assigned two different values ( 0 and 0.3 ) to manipulate two different correlation levels ( independent to correlated ) .    in this study ,",
    "the model contains @xmath164 predictors and @xmath165 or @xmath166 observations , both of which will be partitioned into @xmath167 subsets .",
    "the coefficients @xmath168 follow @xmath169 where @xmath170 is a bernoulli random variable with @xmath171 and @xmath172 .",
    "the reason to specify the coefficients in this way is to demonstrate the performance of all methods in different situations ( both easy and challenging ) . for logistic regression , the closer the coefficient is to zero , the larger the effective sample size and the better the performance of a gaussian approximation .",
    "see fig [ fig:0 ] .    because kernel density estimation is very sensitive to the choice of the kernel covariance when the dimension of the model is moderately high ,",
    "we only demonstrate the performance of kernel smoothing for marginally combined posterior distributions in this section .",
    "the weierstrass rejection sampler is carried out in a similar way .",
    "these two methods will be compared more formally in the next section .",
    "50 synthetic data sets were simulated for each pair of @xmath163 and @xmath1 .",
    "for posterior inference , we drew 20,000 samples ( thinning to 2,000 ) after 50,000 burn - in for single mcmc chain and each subset mcmc .",
    "for weierstrass refinement sampling , the laplacian approximation is adopted as initial approximation , and the kernel variance is chosen according to .",
    "we conduct 10 steps refinement to obtain 2,000 refined draws ( within each refinement step , we run 100 mcmc iterations on each subset to obtain an updated draw ) .",
    "the results are summarized in fig [ fig : logit_ind_10000 ] , [ fig : logit_03_10000 ] , [ fig : logit_ind_30000 ] , [ fig : logit_03_30000 ] , and table [ tab:1 ] .",
    "the posterior distribution of two selected parameters ( including both zero and nonzero ) for different @xmath163 s and @xmath1 s are illustrated in the figures .",
    "the nonzero parameter was plotted in two different scales in order to incorporate multiple densities in one plot .",
    "the numerical comparisons include the difference of the marginal distribution of each parameter , the difference of the joint posterior and the estimation error of the parameters .",
    "we evaluate the difference of the marginal distribution by the average total variation difference ( upper bounded by 1 ) between the approximated marginal densities and the true posterior densities . the result will be separately demonstrated for nonzero and zero coefficients , and denoted by @xmath173 ( nonzero ) and @xmath174 ( zero ) respectively .",
    "evaluating the difference between joint distributions is difficult for multivariate distributions , as one needs to accurately estimate a distance between a true joint distribution and a set of samples from an approximation .",
    "therefore , we adopted the approximated kullback - leibler divergence between two densities ( approximating two densities by gaussian ) for reference , @xmath175 where @xmath176 and @xmath161 are the sample mean and sample variance of the true posterior @xmath177 , while @xmath178 and @xmath179 are those for @xmath180 .",
    "finally , the error of the parameter estimation will be demonstrated as the ratio between the estimation error of the approximating method and that of the true posterior mean , which is @xmath181 .",
    "the average results are shown in table.[tab:1 ] .",
    ".approximation accuracy for marginal and joint densities .",
    "and w. rej stand for weierstrass refinement sampling and rejection sampling ( without weight correction ) . [",
    "cols=\"<,<,<,^,^,^,^,^,^\",options=\"header \" , ]     [ tab : realdata ]    because the positive category ( annual income greater than 50,000 usd ) is rare in the training set , laplacian approximation is likely to overfit and the posterior might not be approximately gaussian , leading to low accuracy in predicting positiveness for most methods . on the contrary",
    ", it can be seen that all methods perform well on the test set 2 ( which mimics the ratio of the training set ) .",
    "in this article , we proposed a new , flexible and efficient weierstrass sampler for parallelizing mcmc .",
    "the weierstrass sampler contains two different algorithms , which are carefully designed for different situations .",
    "extensive numerical evidence shows that , compared to other methods in the same direction , weierstrass sampler enjoys better performance in terms of approximation accuracy , chain mixing rate and a potentially faster speed .",
    "faced with the same difficult issues , such as the dimensionality curse , weierstrass sampler attempts to seize the balance in trading off between the accuracy and computation efficiency . as illustrated in the numerical study",
    ", the rejection sampler can not only well approximate the original mcmc , but also improve its performance in the posterior modes exploration .",
    "in the simulation , the sampler correctly identifies all the mixture components , removing problems of the original gibbs sampler .",
    "future works of weierstrass samplers may lie in the following aspects .",
    "first , investigating the asymptotic justification for the marginal combining strategy , which could help eliminate the dimensionality concern for both kernel density estimation method and weierstrass rejection sampling .",
    "second , investigating the potential application in parallel tempering . in parallel",
    "tempering , there is a temperature parameter @xmath182 which controls both the approximation accuracy and the chain exploration ability .",
    "here , with the tuning parameter @xmath16 , one is able to achieve the same thing : small @xmath16 entails a high accuracy , while large @xmath16 ensures a better exploration ability .",
    "therefore , one could design a set of different values of @xmath16 for the sampling procedure , providing a ` parallel ' way of doing parallel tempering , which may potentially improve the performance of the original method .",
    "[ lemma:1 ] assume a real - valued function @xmath111 is h \" older @xmath29 differentiable with constant @xmath183 , i.e. , for @xmath184 , the @xmath185-th derivative of @xmath111 follows , @xmath186 for some positive constant @xmath183 .",
    "let @xmath90 be a @xmath187-th order kernel function satisfying that @xmath188 , @xmath189 for @xmath190 , and @xmath191 . defining the weierstrass transform as @xmath192 we have @xmath193 where @xmath194 , @xmath195 , @xmath196 and @xmath197 .",
    "the proof relies on the higher - order taylor expansion on the function .",
    "if @xmath198 , we have @xmath199 where @xmath184 and @xmath200 lies between @xmath23 and @xmath201 . because @xmath202 , we thus have @xmath203 and because @xmath204 , @xmath205 therefore , @xmath206 the case @xmath207 follows the same argument . just notice that the taylor expansion now becomes , @xmath208 which entails that @xmath209 where @xmath210 , and thus completes the proof .    in this article , we only focus on the case when @xmath54 is chosen to be a density function ( second order kernel function ) , and thus @xmath211 .",
    "the result stated in lemma [ lemma:1 ] can be naturally generalized to the multivariate case .",
    "[ lemma:2 ] let @xmath111 be a real - valued function on @xmath30 . define the multivariate weierstrass transform as @xmath212 if @xmath111 is hlder @xmath29 smooth with a constant @xmath183 , i.e. , for @xmath184 , @xmath111 is @xmath185-th differentiable and for all the @xmath185-th derivatives of @xmath111 , we have @xmath213 assuming @xmath214s are all @xmath187-th order kernels , the approximation error of the weierstrass transform follows @xmath215 where @xmath216 , @xmath217 and @xmath218 are defined in lemma [ lemma:1 ] .",
    "the proof of lemma [ lemma:2 ] is essentially an application of lemma [ lemma:1 ] and is omitted . with lemma 1",
    "we proceed to prove theorem 1 .",
    "the original theorem 1 is stated in terms of second - order differentiable function and second - order kernels . here",
    ", we provide a more general version of theorem 1 .    _",
    "* theorem 1 . *",
    "if the posterior densities and the kernel functions satisfy the condition in lemma 1 with @xmath29 and @xmath187 , then the distribution defined in is proper and there exists a positive constant @xmath35 such that when @xmath219 for @xmath194 , the total variation distance between the posterior distribution and the approximation follows @xmath220 where @xmath38 and @xmath39 are the normalizing constants , and @xmath40 are defined as @xmath221 _",
    "we merge @xmath222 into @xmath42 to simplify the notation . with this modification",
    "the result can be expressed as , @xmath223 the derivation is divided into two steps . in the first step , we obtain an estimate of the difference between the two products @xmath224 , and then apply it in the second step to bound the total variation distance .    a typical way to quantify the difference between two products is to decompose it into sums of relative differences , i.e. , @xmath225 define @xmath226 and @xmath227 , and notice that @xmath228 we can then bound each relative difference as @xmath229 summing over all @xmath230 , we have @xmath231 where @xmath232 . using the same trick to decompose @xmath233 entails that ( noticing that @xmath234 ) , @xmath235 and for @xmath236 , a direct computation shows that @xmath237 if all @xmath238 .",
    "defining @xmath239 , by mathematical induction , it is easy to verify that for @xmath240 and @xmath238 , @xmath241 and therefore we have @xmath242 an application of can help deriving the difference between the two normalizing constants , @xmath243 we then bound the @xmath244 distance . for @xmath238 and @xmath240",
    "we have @xmath245 which gives the error bound stated in the theorem .    * _ remarks : _ * theorem 1 is a special case of the above theorem with @xmath246 and @xmath34 . though the features of weierstrass sampler will not rely on asymptotics ( no requirement on the sample size ) , a rough asymptotic analysis on @xmath51 and @xmath222 can provide a general idea on their magnitudes and behavior with the change of sample size .",
    "if the likelihood function satisfies certain regularity conditions , local asymptotic normality will ensure the posterior density converges to a normal density both pointwise and in @xmath244 . replacing the posterior by its asymptotic distribution , i.e. , substituting @xmath13 with @xmath247 , where @xmath248 is the fisher s information matrix , and @xmath249s are locally consistent estimators which satisfy that @xmath250 asymptotically",
    ", we have that @xmath251,\\end{aligned}\\ ] ] where @xmath252 .",
    "since @xmath249 follows @xmath253 roughly , we can replace @xmath254 by @xmath11 for large values of @xmath187 and obtain a more concise approximation as @xmath255 the above approximation suggests that @xmath256 , where @xmath257 .",
    "therefore , the following approximations hold asymptotically @xmath258 to quantify @xmath222 requires more elaborate analysis , because the result depends on the choice of kernel functions . if the kernel function is a density function with @xmath259 ( sufficiently smooth likelihood ) , @xmath260 .    by definition",
    "we have , @xmath261 where @xmath262 . as a result",
    ", we have , @xmath263 the right hand side is essentially the same as the expression @xmath264 in theorem 1 , except for the @xmath117 term .",
    "therefore , following the same argument in theorem 1 , we can prove that @xmath265      to prove theorem 2 we first need three lemmas with proofs provided immediately after the statement of lemma .",
    "it is worth noting that the lemma [ lemma:2.2 ] is a common tool used in showing geometric ergodicity of mcmc algorithms @xcite .",
    "the original proof used the common coupling inequality trick , while here we will adopt a different approach to derive a slightly different conclusion that is more useful for this paper .      let @xmath269 .",
    "define the following functions @xmath270 and @xmath271 because @xmath272 are continuous , the above functions are all smooth and non - decreasing functions for @xmath273 , and satisfy that @xmath274 from the assumption that @xmath275 ( otherwise the result is trivial ) , we are guaranteed that @xmath276 is positive .",
    "the following proof is divided into two parts .",
    "_ case 1 : _ if @xmath277 .",
    "define @xmath278 .",
    "due to the continuity and monotonic property of @xmath279 and @xmath280 , we can always find @xmath281 and @xmath282 such that @xmath283 and @xmath284 such that @xmath285    now if @xmath286 , we can simply set @xmath287 .",
    "otherwise , without loss of generality , assuming @xmath288 , due to the choice of @xmath281 , we know that @xmath289 , which ensures @xmath290 again making use of the continuity and the property of limit , we are able to find @xmath291 such that @xmath292 and @xmath59 is then taken to be @xmath293    _ case 2 : _ if @xmath294 .",
    "this case is even simpler .",
    "define @xmath295 . since @xmath296",
    ", there exist @xmath281 and @xmath284 such that , @xmath297 which at same time guarantees that @xmath298 .",
    "therefore , by a similar argument to case 1 , we will be able to find @xmath282 such that @xmath299 then @xmath300 .",
    "it is easy to verify that the set @xmath59 defined in the proof satisfies the properties in the lemma .",
    "[ lemma:2.2 ] assume the gibbs sampler defines a transition kernel @xmath301 , @xmath302 let @xmath266 denote the equilibrium distribution and @xmath267 the approximation to @xmath266 .",
    "for any measurable set @xmath59 which satisfies that @xmath303 if there exists a probability density @xmath304 and a positive value @xmath305 such that @xmath306 for any @xmath66 and @xmath307 , then we have @xmath308 where @xmath309",
    ". if @xmath310 , the conclusion becomes , @xmath311 where @xmath312 denots the total variation distance .",
    "because @xmath266 is the equilibrium distribution of the gibbs sampler , we have @xmath313 therefore , @xmath314 for the second term we have @xmath315 where the last equality is due to fubini s theorem . for the first term ,",
    "notice that @xmath316 , where @xmath317 for @xmath318 .",
    "as a result , we have @xmath319 consequently , we have , @xmath320 which completes the proof .",
    "[ lemma:2.3 ] considering the following gibbs sampler , @xmath321 which defines a transition kernel on @xmath23 as @xmath322 .",
    "if the kernel @xmath54 is fully supported on @xmath57 , then for any bounded measurable set @xmath59 , there exists an @xmath323 and a probability density @xmath324 such that @xmath325 for any @xmath326 .",
    "furthermore , if the condition is satisfied , i.e. , @xmath327 for any @xmath328 and @xmath67 , then the set @xmath59 can be taken as @xmath57 .",
    "consider the full gibbs transition kernel @xmath329 , @xmath330 where @xmath331 is a probability density over @xmath332 , and define @xmath333 which is strictly greater than 0 for any given @xmath334 because @xmath59 is bounded and @xmath54 is strictly positive on @xmath57 .",
    "consequently , @xmath335 let @xmath336 and @xmath337 .",
    "because @xmath338 is strictly positive on @xmath57 , thus we have @xmath339 .",
    "apparently @xmath324 is a probability density satisfying that , @xmath340 for any @xmath341 .        with lemma [ lemma:2.1 ]",
    "we are guaranteed the existence of a set @xmath59 which is bounded and satisfies that @xmath343 now lemma [ lemma:2.1 ] ensures the existence of a density @xmath324 such that the transition kernel defined in and ( with general kernel @xmath54 ) @xmath344 satisfies that @xmath345 for any @xmath341 or for any @xmath346 if the condition is also satisfied .",
    "combining these facts with lemma 4 , we have the result listed in theorem [ thm2 ] .",
    "ahn , s. , korattikara , a. and welling , m. ( 2012 ) .",
    "bayesian posterior sampling via stochastic gradient fisher scoring .",
    "_ proceedings of the 29th international conference on machine learning _",
    "1591 - 1598 .",
    "neal , r. m. ( 1993 ) probabilistic inference using markov chain monte carlo methods . _ technical report_. university of toronto , toronto .",
    "( available from http://www.cs.toronto.edu/@xmath347radford/review .",
    "abstract.html . )",
    "weierstrass , k. ( 1885 ) .",
    "ber die analytische darstellbarkeit sogenannter willkrlicher functionen einer reellen vernderlichen .",
    "_ sitzungsberichte der kniglich preuischen akademie der wissenschaften zu berlin _ , ( ii ) .",
    "erste mitteilung ( part 1 ) pp .",
    "633639 , zweite mitteilung ( part 2 ) pp .",
    "789 - 805 .",
    "white , s.r .",
    ", kypraios , t. , and preston , s.p .",
    "piecewise approximate bayesian computation : fast inference for discretely observed markov models using a factorised posterior distribution . _",
    "statistics and computing _ , 1 - 13 ."
  ],
  "abstract_text": [
    "<S> with the rapidly growing scales of statistical problems , subset based communication - free parallel mcmc methods are a promising future for large scale bayesian analysis . in this article , we propose a new weierstrass sampler for parallel mcmc based on independent subsets . </S>",
    "<S> the new sampler approximates the full data posterior samples via combining the posterior draws from independent subset mcmc chains , and thus enjoys a higher computational efficiency . </S>",
    "<S> we show that the approximation error for the weierstrass sampler is bounded by some tuning parameters and provide suggestions for choice of the values . </S>",
    "<S> simulation study shows the weierstrass sampler is very competitive compared to other methods for combining mcmc chains generated for subsets , including averaging and kernel smoothing .    </S>",
    "<S> _ keywords _ : big data ; communication - free ; embarassingly parallel ; mcmc ; scalable bayes ; subset sampling ; weierstrass transform . </S>"
  ]
}