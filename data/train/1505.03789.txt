{
  "article_text": [
    "estimating the direction of arrival ( doa ) of multiple signals impinging on an array of sensors from observation of a finite number of array snapshots has been extensively studied in the literature @xcite .",
    "maximum likelihood estimators ( mle ) and cramr - rao bounds ( crb ) , derived under the assumption of additive white gaussian noise , and either for the so - called conditional or unconditional model @xcite , serve as references to which newly developed doa estimators have been systematically compared . in many instances however , additive noise is usually colored and , consequently , the problem of doa estimation in spatially correlated noise fields has been studied , see e.g. , @xcite .    when the spatial covariance matrix of this additive noise is known a priori , maximum likelihood estimators and cramr - rao bounds are changing in a straightforward way with whitening operations .",
    "the new statistical problem appears when the covariance matrix of the additive noise is not known a priori and information about this matrix is substituted by a number of independent and identically distributed ( i.i.d . )",
    "training samples , that form the so - called secondary training sample data set . in many cases",
    "one can assume that the statistical properties of the training noise data are the same as per noise data within the primary training set data : such conditions are usually referred to as the supervised training conditions .",
    "therefore , under these conditions , one has two sets of measurements , one primary set @xmath2 which contains signals of interest ( soi ) and noise , and a second set @xmath3 ( secondary training set ) which contains noise only . examples of this problem formulation are numerous in the area of passive location and direction finding . for instance , in the so - called over - sampled 2d hf antenna arrays , ionospherically propagated external noise is spatially non white @xcite , and some parts of hf spectrum ( distress signals for example ) with no signals may be used for external noise sampling @xcite . despite its relevance in many practical situations ,",
    "this problem has been relatively scarcely studied @xcite . for parametric description of the gaussian noise covariance matrix with @xmath4 the unknown parameter vector , in @xcite",
    ", the authors derive the cramr - rao bound for joint soi parameters ( doa ) @xmath5 and noise parameters @xmath4 estimation , assuming a conventional unconditional model , i.e. , @xmath6 and @xmath7 where @xmath8 stands for the complex gaussian distribution whose respective parameters are the mean , row covariance matrix and column covariance matrix .",
    "@xmath9 is the usual steering matrix with @xmath10 the vector of unknowns doa , @xmath11 denotes the waveforms covariance matrix and @xmath12 corresponds to the noise covariance matrix , which is parameterized by vector @xmath4 .    in many cases however",
    ", the gaussian assumption for the predominant part of the noise can not be advocated .",
    "typical example is the hf external noise , heavily dominated by powerful lighting strikes @xcite . evidence of deviations from the gaussian assumption has been demonstrated numerous times for different applications , with the relevance of the compound - gaussian ( cg ) models being justified @xcite .",
    "in essence , the individual @xmath13-variate snapshot of such a noise over the face of an antenna array may be treated as a gaussian random vector , whose power can randomly fluctuate from sample to sample .",
    "cg models belong to a larger class of distributions , namely multivariate elliptically contoured distributions ( ecd ) @xcite . for the sake of clarity , we briefly review the main definitions of ecd .",
    "a vector @xmath14 follows an ec distribution if it admits the following stochastic representation @xmath15 where @xmath16 means `` has the same distribution as '' . in",
    ", @xmath17 is a non - negative real random variable and is independent of the complex random vector @xmath18 which is uniformly distributed over the complex sphere @xmath19 .",
    "the matrix @xmath20 is such that @xmath21 where @xmath22 is the so - called scatter matrix , and we assume here that @xmath22 is non - singular . the probability density function ( p.d.f . ) of @xmath23 can then be written as @xmath24 where @xmath25 stands for proportional to .",
    "the function @xmath26 is called the density generator and satisfies finite moment condition @xmath27 .",
    "it is related to the p.d.f . of the modular variate @xmath17 by @xmath28 .",
    "going back to our scenario of two data sets @xmath29 and @xmath30 , we assume that they are independent , and that their columns are independent and distributed ( i.i.d . ) according to .",
    "in other words , one has @xmath31 and @xmath32 , where @xmath33 and @xmath34 are i.i.d .",
    "variables drawn from @xmath35 , and @xmath36 and @xmath37 are i.i.d .",
    "random vectors uniformly distributed on the unit sphere .",
    "it then follows that the joint distribution of @xmath38 is given by @xmath39 where @xmath40 and    [ p(xp , xs ) ] @xmath41    where @xmath42 .",
    "additionally , we assume that @xmath43 depends on a parameter vector @xmath5 while @xmath22 depends on @xmath4 .",
    "our objective is then to estimate @xmath44 from @xmath45 .",
    "let us emphasize an essential difference of the problem in with respect to the typical problem of target detection in cg clutter @xcite .",
    "there , within each range resolution cell the clutter is perfectly gaussian and therefore the optimum space - time processing is the same as per the standard gaussian problem formulation .",
    "it is the data dependent threshold and clutter covariance matrix ( in adaptive formulation ) that needs to be calculated from the secondary data , if not known a priori @xcite . in the problem",
    ", the soi doa estimation should be performed on a number @xmath46 of ecd i.i.d .",
    "primary training samples , and maximum likelihood doa estimation algorithm and crb should be expected to be very different from the gaussian case .",
    "the paper is organized in the following way . in section [ section : crb ] , we derive a general expression of the fim for elliptically distributed noise using two data sets .",
    "section [ section : doak ] focuses on the case of doa estimation in @xmath0-distributed noise . in section [ section : crbk ] , we derive conditions under which the fim is bounded / unbounded , and provide a sufficient condition for unboundedness of the fim with general elliptical distribution . the maximum likelihood estimate , as well as an approximation ,",
    "are derived in section [ section : mlek ] . in the same section ,",
    "we derive lower and upper bounds on the mean - square error of the mle for non - regular estimation conditions , i.e. , when the fisher information matrix is unbounded .",
    "numerical simulations serve to evaluate the performance of the estimators in section [ section : numerical ] and our conclusions are drawn in section [ section : conclu ] .",
    "in this section , we derive the crb for estimation of parameter vector @xmath47 from the distribution in .",
    "the fisher information matrix ( fim ) for the problem at hand can be written as @xcite @xmath48 where we used the fact that @xmath49 hence , the total fim is the sum of two matrices @xmath50 , with straightforward definition from . in order to derive each matrix",
    ", we will make use of the general expression of the fisher information matrix for ecd recently derived in @xcite .",
    "first , let us introduce @xmath51 where @xmath52 . then",
    ", we have from @xcite that the @xmath53-th element of the fisher information matrices is given by @xmath54 { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}_{j}}\\ } } { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}_{k}}\\ } } \\nonumber   \\\\ & + \\frac{\\alpha_{2 } { t_{p}}}{m(m+1 ) } { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}_{j}}{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}_{k}}\\}}\\end{aligned}\\ ] ] @xmath55 { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}_{j}}\\ } } { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}_{k}}\\ } } \\nonumber   \\\\ & + \\frac{\\alpha_{2 } { t_{s}}}{m(m+1 ) } { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}_{j}}{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}_{k}}\\}}\\end{aligned}\\ ] ] where @xmath56 .",
    "since @xmath22 depends only on @xmath4 , it follows that @xmath57 takes the following form @xmath58 with @xmath59 { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}^n_{j}}\\ } } { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}^n_{j}}\\ } } \\nonumber \\\\ & + \\frac{\\alpha_{2 } { t_{s}}}{m(m+1 ) } { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}^n_{j}}{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}^n_{k}}\\}}\\end{aligned}\\ ] ] where @xmath60 .",
    "let us now consider @xmath61 .",
    "using the fact that @xmath22 depends only on @xmath4 and @xmath62 depends only on @xmath5 , @xmath61 is block - diagonal , i.e. , @xmath63 with    @xmath64 { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}^n_{j}}\\ } } { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}^n_{j}}\\ } } \\nonumber \\\\ & + \\frac{\\alpha_{2 } { t_{p}}}{m(m+1 ) } { { \\mathrm{tr}}\\{{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}^n_{j}}{{{\\mathbf{r}}}^{-1}}{{{\\mathbf{r}}}^n_{k}}\\}}.\\end{aligned}\\ ] ]    the whole fim is thus given by @xmath65 the crb for estimation of @xmath5 is obtained as the upper - left block of the inverse of the fim and is thus simply @xmath66 .",
    "similarly to the gaussian case , the crb for estimation of @xmath5 in the conditional model is the same as if @xmath22 was known .",
    "as for the crb for estimation of @xmath4 , it is the same as if we had a set of @xmath67 noise only samples .",
    "we address the specific problem where the primary data can be written as @xmath68 where @xmath69 follows a gamma distribution with shape parameter @xmath70 and scale parameter @xmath71 , i.e. , its p.d.f",
    ". is given by @xmath72 which we denote as @xmath73 , and @xmath74 .",
    "the noise component is known to follow a @xmath0 distribution and @xmath75 in admits a ces representation similar to with @xmath76 . the p.d.f . of @xmath33 in this case is given by @xmath77 where @xmath78 is the modified bessel function .",
    "note that the @xmath79-th order moment of @xmath80 is @xmath81 where we used the fact @xcite that @xmath82 the density generator is thus here @xmath83 where , for the sake of notational convenience , we have dropped the subscript @xmath84 .",
    "the fim for @xmath0-distributed noise can be obtained from the fim for gaussian distributed noise and the calculation of the scalar @xmath85^{2}\\right\\}}\\ ] ] for @xmath86 .",
    "for the signal parameters part only , we indeed have @xmath87 where the subscript @xmath88 and @xmath89 stand for @xmath0-distributed and gaussian distributed noise . using the fact that @xmath90 , it follows that @xmath91 \\nonumber \\\\ & = - { q}^{\\frac{\\nu - m-1}{2 } } \\beta^{-1/2 } k_{m+1-\\nu}\\left(2 \\sqrt{{q}/ \\beta}\\right).\\end{aligned}\\ ] ] it",
    "then ensues that @xmath92 and thus @xmath93^{2 } \\right\\ } } \\nonumber \\\\ & =   \\frac{2 \\beta^{-\\frac{\\nu+m}{2}-1}}{\\gamma(\\nu ) \\gamma(m ) } \\int_{0}^{\\infty } { q}^{\\mu-2 + \\frac{\\nu+m}{2 } } \\frac{k^{2}_{m+1-\\nu}\\left(2 \\sqrt{{q}/ \\beta}\\right)}{k_{m-\\nu}\\left(2 \\sqrt{{q}/ \\beta}\\right ) } d { q}\\nonumber \\\\ & = \\frac{\\beta^{\\mu-2}}{2^{2\\mu+\\nu+m-4}\\gamma(\\nu ) \\gamma(m ) } \\int_{0}^{\\infty } z^{2\\mu+\\nu+m-3 } \\frac{k^{2}_{m+1-\\nu}\\left(z\\right)}{k_{m-\\nu}\\left(z\\right ) } d z.\\end{aligned}\\ ] ] a formula for the fim in case of @xmath0-distributed noise was derived in @xcite based on the compound gaussian representation .",
    "while it resembles our derivations based on the fim for ecd derived in @xcite , it does not match exactly our expression herein .",
    "moreover , we study herein the _ existence of the fim _ and derive _ a closed - form approximation of the fim_.    let us investigate the conditions under which the integral @xmath94 converges . towards this end , let us use the following inequality which holds for @xmath95 and @xmath96 @xcite @xmath97 it follows that @xmath98 the first integral converges for @xmath99 while the second converges for @xmath100 .",
    "hence , for @xmath101 , one has @xmath102 accordingly , one has , for @xmath96 @xmath103 which implies that @xmath104 the first integral converges for @xmath101 and the second converges for @xmath105 . in the former case , one has @xmath106 consequently , we conclude that _ the integral converges only for _ @xmath101 : for @xmath107 this implies that @xmath108 which is verified .",
    "in contrast , when @xmath109 , one must have @xmath110 . in other words , the term in _ the fim corresponding to the noise parameters",
    "is always bounded _ since it depends on @xmath111 only .",
    "the situation is different for signal parameters . in an unconditional model where @xmath22 would depend on signal parameters as well",
    ", the fim is bounded .",
    "in contrast , in the conditional model where signal parameters are embedded in the mean of the distribution , the fim corresponding to signal parameters _ is bounded only for @xmath110 _ : otherwise , it is unbounded .",
    "the latter case corresponds to the so - called non regular case corresponding to distributions with singularities , as studied e.g. , in @xcite .    before pursuing our study of the fim for",
    "the specific case of @xmath0-distributed noise , let us make an important observation . for the @xmath0 distribution",
    ", we have just proven that @xmath112 does not exist for @xmath113 . however , see , @xmath114 exists if and only if @xmath115 and @xmath116 . the latter condition implies that , when @xmath113 , @xmath117 does not exist .",
    "observe that convergence of the latter integral is problematic in a neighborhood of @xmath118 , since for @xmath119 , @xmath120 as @xmath121 is a density .",
    "therefore , at least for @xmath0-distributed noise , if @xmath122 does not exist , then @xmath123 is unbounded . at this stage ,",
    "one may wonder if this property extends to any other elliptical distribution .",
    "it turns out that this is indeed the case , as stated and proved in the next proposition .",
    "whatever the p.d.f . of the modular variate @xmath33 , if @xmath124 then @xmath125",
    ".    for the sake of notational convenience",
    ", we temporarily omit the subscript @xmath84 and use @xmath17 instead of @xmath33 .",
    "let us first observe that @xmath126 since @xmath127 , one can write @xmath128 which implies that @xmath129^{2}.\\end{aligned}\\ ] ] therefore @xmath130^{2 } p({q } ) d{q}\\nonumber \\\\ & = ( m-1)^{2 } \\int_{a}^{b } { q}^{-1 } p({q } ) d{q}- 2 ( m-1 ) \\int_{a}^{b }    p'({q } ) d{q}\\nonumber \\\\ & + \\int_{a}^{b }   { q}\\left [ \\frac{\\partial \\ln   p({q})}{\\partial { q } } \\right]^{2 } p({q } ) d{q}\\nonumber \\\\ & = ( m-1)^{2 } \\int_{a}^{b } { q}^{-1 } p({q } ) d{q}- 2   ( m-1 ) \\left [ p(b ) - p(a ) \\right ] \\nonumber \\\\ & + \\int_{a}^{b }   { q}\\left [ \\frac{\\partial \\ln   p({q})}{\\partial { q } } \\right]^{2 } p({q } ) d{q}. \\end{aligned}\\ ] ] the third term of the sum is always positive . in the second term , we have that @xmath131 .",
    "it follows that divergence of @xmath132 is a sufficient condition for divergence of @xmath133 . as said before @xmath134 exists , and",
    "therefore a sufficient condition for @xmath135 to be undounded is that @xmath136 is unbounded .",
    "let us now go back to the @xmath0-distributed case and investigate whether it is possible to derive a simple expression for @xmath137 and subsequently @xmath138 , assuming that @xmath101 . towards this end ,",
    "let us make use of @xmath139 to write that @xmath140 the last term is obviously not possible to obtain in closed - form so that we use a `` large @xmath141 '' approximation of the modified bessel function @xcite @xmath142 which results in @xmath143 therefore , @xmath144 we finally have @xmath145    if the large @xmath141 approximation is made from the start , then one has @xmath146 so that @xmath147 and hence @xmath148    figure [ fig : alphamu_approx ] compares the approximations in and , as well as a method which uses random number generation to approximate @xmath138 based on its initial definition in .",
    "more precisely , we generated a large number of random variables @xmath149 and replace the statistical expectation of by an average over the so - generated random variables .",
    "as can be observed from figure [ fig : alphamu_approx ] , the @xmath150 approximations provide very close values , which enable one to validate the closed - form expressions in and .     in and .",
    "@xmath151 and @xmath152.,width=283 ]      we now focus on maximum likelihood ( ml ) estimation of direction of arrival @xmath153 , signal waveforms @xmath154 and covariance matrix @xmath22 in the model @xmath155 where @xmath156 , and @xmath157 .",
    "the joint distribution of @xmath158 is given by @xmath159^{\\frac{\\nu - m}{2 } } k_{m-\\nu}\\left ( 2 \\sqrt{{{\\mathbf{y}}_{{t_{s}}}}^{h } { { { \\mathbf{r}}}^{-1}}{{\\mathbf{y}}_{{t_{s}}}}/ \\beta } \\right ) \\nonumber \\\\ & \\times \\prod_{{t_{p}}=1}^{{t_{p } } } \\left [ { { { \\mathbf{z}}}_{{t_{p}}}}^{h } { { { \\mathbf{r}}}^{-1}}{{{\\mathbf{z}}}_{{t_{p}}}}\\right]^{\\frac{\\nu - m}{2 } } k_{m-\\nu}\\left ( 2 \\sqrt { { { { \\mathbf{z}}}_{{t_{p}}}}^{h } { { { \\mathbf{r}}}^{-1}}{{{\\mathbf{z}}}_{{t_{p}}}}/ \\beta } \\right)\\end{aligned}\\ ] ] where @xmath160 .",
    "joint estimation of all parameters appears to be very complicated and hence we will proceed in two steps . at first , we assume that @xmath22 is known and derive the ml estimates of @xmath161 and @xmath154 . then , @xmath22 is substituted for some estimate obtained from observation of @xmath162 only .      assuming that @xmath22 is known , one needs to maximize with respect to @xmath161 and @xmath154 @xmath163^{h } { { { \\mathbf{r}}}^{-1}}\\left[{{\\mathbf{x}}_{{t_{p}}}}- { { \\mathbf{a}}}(\\phi ) s_{{t_{p}}}\\right ] \\right)\\ ] ] where @xmath164 is given by .",
    "since @xmath164 is monotonically decreasing , see , it follows that @xmath165 is maximized when the argument of @xmath164 is minimized .",
    "however , @xmath166^{h } { { { \\mathbf{r}}}^{-1}}\\left[{{\\mathbf{x}}_{{t_{p}}}}- { { \\mathbf{a}}}(\\phi ) s_{{t_{p}}}\\right ] \\nonumber \\\\ & = \\left[{{\\mathbf{a}}}^{h}(\\phi ) { { { \\mathbf{r}}}^{-1}}{{\\mathbf{a}}}(\\phi )   \\right ] \\left| s_{{t_{p } } } - \\frac{{{\\mathbf{a}}}^{h}(\\phi ) { { { \\mathbf{r}}}^{-1}}{{\\mathbf{x}}_{{t_{p}}}}}{{{\\mathbf{a}}}^{h}(\\phi ) { { { \\mathbf{r}}}^{-1}}{{\\mathbf{a}}}(\\phi ) } \\right|^{2 } \\nonumber \\\\ & + { { \\mathbf{x}}_{{t_{p}}}}^{h } { { { \\mathbf{r}}}^{-1}}{{\\mathbf{x}}_{{t_{p}}}}- \\frac{\\left| { { \\mathbf{a}}}^{h}(\\phi ) { { { \\mathbf{r}}}^{-1}}{{\\mathbf{x}}_{{t_{p}}}}\\right|^{2}}{{{\\mathbf{a}}}^{h}(\\phi ) { { { \\mathbf{r}}}^{-1}}{{\\mathbf{a}}}(\\phi)}.\\end{aligned}\\ ] ] therefore , for any @xmath161 , @xmath165 is maximized when @xmath167 it ensues that one needs now to maximize , with respect to @xmath161 @xmath168 with @xmath169 . in order to avoid calculation of a modified bessel function and thus in order to simplify estimation",
    ", we propose to make use of the `` large @xmath141 '' approximation of the modified bessel function given in to write @xmath170 this approximation results in an approximate maximum likelihood ( aml ) estimator of @xmath161 which consists in maximizing @xmath171^{\\nu - m}.\\ ] ] note that @xmath172\\ ] ] which should be compared to the concentrated log likelihood function in the gaussian case , as given by @xmath173.\\ ] ]    a few remarks are in order about these estimates , in particular about the behavior of the aml estimator in the case of unbounded fim , i.e. , when @xmath174 .",
    "first , note that all estimates will be a function of @xmath175 where @xmath176 is the projection onto the orthogonal complement of @xmath177 .",
    "compared to , the logarithm operation in will strongly emphasize those snapshots @xmath75 for which @xmath178 is small .",
    "let us thus investigate the properties of this statistic , when evaluated at the _ true _ value of signal doa @xmath153 .",
    "using the fact that @xmath179 , where @xmath180 and @xmath181 is a short - hand notation for @xmath182 , one has @xmath183 for small @xmath70 ( @xmath174 ) , it follows that , in the vicinity of @xmath153 , the snapshot with minimal @xmath178 is more or less the snapshot for which @xmath69 is minimum , hence the snapshot for which noise power is minimum , which makes sense . if we let @xmath184 , then its cumulative density function ( c.d.f . )",
    "is given by @xmath185 } & = 1 - \\left ( 1 - { { \\mathrm{pr}}\\left[\\tau_{{t_{p } } } \\leq \\eta\\right ] } \\right)^{{t_{p } } } \\nonumber \\\\ & = 1 - \\left [ 1 - \\gamma\\left(\\nu , \\eta \\beta^{-1}\\right ) \\right]^{{t_{p } } } \\end{aligned}\\ ] ] which is shown in figure [ fig : cdf_min_tau_t=4 ] .",
    "obviously , with small @xmath70 , the snapshot which corresponds to the minimum value of @xmath69 exhibits a very high signal to noise ratio and , due to the emphasizing effect of the @xmath186 operation in , the performance of the aml estimator is likely to be driven mainly by this particular snapshot .",
    "this is illustrated in figure [ fig : mse_rknown_order_tau_vs_t_vs_nu ] where we display the mean - square error ( mse ) of the aml estimate which uses all @xmath46 snapshots and the mse of an hypothetical aml estimator which would use only the snapshot @xmath187 corresponding to the minimum value of @xmath69 . the scenario of this simulation is described in the next section .",
    "this figure shows a marginal loss of the aml estimator using @xmath187 only , as compared to the full aml estimator , especially for small @xmath70 .    .",
    "@xmath188 and @xmath189.,width=283 ]    .",
    "@xmath22 known , @xmath188 and @xmath190db.,width=283 ]    let us thus analyze the behavior of the aml estimators .",
    "for the sake of notational convenience , let @xmath191 and @xmath192 denote the aml estimator using @xmath46 snapshots with @xmath0-distributed noise and the aml estimator using the snapshot @xmath187 corresponding to the minimal @xmath69 , respectively . observe that , when using a single snapshot @xmath187 , minimizing is equivalent to minimizing the gaussian likelihood function in with @xmath193 .",
    "since @xmath187 exhibits a high signal to noise ratio , @xmath192 is close to @xmath153 , one can make a taylor expansion and relate the error @xmath194 to the error @xmath195 as @xmath196 where @xmath197 is some vector that depends essentially on the derivatives of @xmath198 @xcite and whose expression is not needed here .",
    "one can simply notice that @xmath197 would be the same with gaussian noise and a single snapshot , since maximizing or is equivalent when one snapshot is used .",
    "this implies that @xmath199 observe that @xmath200 is the mean - square error ( mse ) that would obtained in gaussian noise and a single snapshot , which is about @xmath46 times the mse obtained in the gaussian case and using @xmath46 snapshots , and the latter is approximately the gaussian crb .",
    "the mse of @xmath192 depends on @xmath201 where @xmath202 is the minimum value of a set of @xmath46 independent and identically distributed ( actually gamma distributed ) variables .",
    "therefore , in order to obtain @xmath201 , one must consider statistics of extreme values , a field that has received considerable attention for a long time , see e.g. , @xcite .",
    "it turns out that only asymptotic ( as @xmath203 ) results are available and we build upon them to derive the rate of convergence of @xmath204 .",
    "first , note that @xmath205 } & =   { { \\mathrm{pr}}\\left[u_{{t_{p } } } \\geq { t_{p}}^{-1/ \\nu } x\\right ] } \\nonumber \\\\ & = \\left ( { { \\mathrm{pr}}\\left[\\tau_{{t_{p } } } \\geq { t_{p}}^{-1/ \\nu } x\\right ] } \\right)^{{t_{p } } } \\nonumber \\\\ & = \\left(1 - { { \\mathrm{pr}}\\left[\\tau_{{t_{p } } } \\leq { t_{p}}^{-1/ \\nu } x\\right ] } \\right)^{{t_{p } } } \\nonumber \\\\ & = \\left [ 1 - \\gamma\\left(\\nu,\\beta^{-1}{t_{p}}^{-1/ \\nu } x\\right ) \\right]^{{t_{p}}}.\\end{aligned}\\ ] ] now since @xmath70 is small and @xmath46 is large , @xmath206 is very small and we can approximate @xmath207^{-1 } y^{a}$ ] , which yields @xmath208 } & = \\left [ 1 - \\gamma\\left(\\nu,\\beta^{-1}{t_{p}}^{-1/ \\nu } x\\right ) \\right]^{{t_{p } } } \\nonumber \\\\ & \\simeq \\left [ 1 - \\frac{\\beta^{-\\nu } { t_{p}}^{-1 } x^{\\nu}}{\\nu \\gamma(\\nu ) } \\right]^{{t_{p } } } \\nonumber \\\\ & \\simeq { \\exp \\left\\{-\\frac{\\beta^{-\\nu } { t_{p}}^{-1 } x^{\\nu}}{\\nu \\gamma(\\nu)}\\right\\}}^{{t_{p } } } \\nonumber \\\\ & = { \\exp \\left\\{-\\frac{\\beta^{-\\nu } x^{\\nu}}{\\nu \\gamma(\\nu)}\\right\\}}.\\end{aligned}\\ ] ] it follows that asymptotically , @xmath209 converges to the distribution in , whose probability density function is @xmath210 using integration by parts , it follows that @xmath211^{\\infty}_{0 } + \\int_{0}^{\\infty } { \\exp \\left\\{-\\frac{\\beta^{-\\nu } x^{\\nu}}{\\nu \\gamma(\\nu)}\\right\\ } } dx \\nonumber \\\\ & = \\beta \\nu^{1/\\nu -1 } \\gamma(\\nu)^{1/\\nu } \\int_{0}^{\\infty } z^{1/\\nu -1 } { \\exp \\left\\{-z\\right\\ } } dz \\nonumber \\\\ & = \\beta \\nu^{1/\\nu -1 } \\gamma(\\nu)^{1/\\nu } \\gamma(\\nu^{-1 } ) \\triangleq c(\\nu,\\beta).\\end{aligned}\\ ] ] one can then conclude that , as @xmath46 goes to infinity , @xmath212 c(\\nu,\\beta ) { t_{p}}^{-1/\\nu}.\\ ] ] therefore , in the case of @xmath174 , the mse of @xmath192 decreases as @xmath213 , a rate of convergence much faster than the usual @xmath214 .",
    "note that this case corresponds to unbounded fim .",
    "such rates of convergence are also found with distributions possessing singularities @xcite .    as for the aml estimate obtained from @xmath46 snapshots , namely @xmath191 ,",
    "its mse is upper - bounded by that @xmath192 ( since it uses all snapshots , including @xmath187 ) , and is lower - bounded by the mse that would be obtained if @xmath215 for @xmath216 , and this mse is @xmath214 times the mse of @xmath192 . additionally ,",
    "as said before , we have @xmath217 where @xmath218 is the gaussian crb using @xmath46 snapshots .",
    "hence , one can bound the mse of @xmath191 as @xmath219 as will be illustrated in the next section , the upper bound is rather tight , while the lower bound is much lower than the actual mse .",
    "when @xmath22 is not known , then the secondary data @xmath162 can be used to estimate it .",
    "the maximum likelihood estimator is obtained ( for @xmath220 ) as the solution ( up to a scaling factor ) to the following implicit equation @xcite @xmath221 @xmath222 can be obtained through an iterative procedure , whose convergence is guaranteed under the assumptions made @xcite . in order to avoid evaluation of the modified bessel function , one can use the large @xmath141 approximation of @xmath223 in to define @xmath224 as the solution to the fixed - point solution @xmath225 note that @xmath224 is more or less the well - known tyler fixed - point estimator @xcite , which again can be obtained from an iterative procedure whose convergence is guaranteed @xcite .",
    "the drawbacks of the two above estimators are that 1)they are suited to a @xmath0 distribution for the noise and 2)@xmath226 is required to be larger than @xmath13 . in order to gain robustness against these problems ,",
    "a solution is to use normalized data @xmath227 whose distribution is independent of that of the noise , and to use regularization .",
    "more precisely , we suggest to resort to the following scheme @xcite    [ fp_r(eta ) ] @xmath228^{-1 } { { { \\mathbf{z}}}_{{t_{s } } } } }   + \\eta { { \\mathbf{i}}}_{m } \\\\ { \\hat{{{\\mathbf{r}}}}}_{k+1}(\\eta ) & = \\frac{m}{{{\\mathrm{tr}}\\{\\breve{{{\\mathbf{r}}}}_{k+1}(\\eta)\\}}}\\breve{{{\\mathbf{r}}}}_{k+1}(\\eta).\\end{aligned}\\ ] ]    and define @xmath229 since convergence of this iterative scheme has been proved @xcite .",
    "the very good performance of this scheme has been illustrated in various applications , see e.g. , @xcite , where discussions on how to select the regularization parameter @xmath230 can also be found .",
    "we assume a linear array of @xmath188 elements spaced a half - wavelength apart and we consider the simple scenario of a single source impinging from @xmath231 embedded in unit power @xmath0-distributed noise . the covariance matrix @xmath22 is given by @xmath232 with @xmath233 .",
    "the exact and approximate maximum likelihood estimators , which consists in maximizing @xmath234 in @xmath235 in were implemented using the matlab function ` fminbnd ` , and the maximum was searched in the interval @xmath236 $ ] where @xmath237 is the half - power beamwidth of the array .",
    "the signal waveform was generated from i.i.d .",
    "gaussian variables with power @xmath238 and the signal to noise ratio ( snr ) is defined as @xmath239 .",
    "the asymptotic gaussian crb , multiplied by the scalar @xmath240 was used as the bound for @xmath0-distributed noise . for the regularized covariance matrix estimator @xmath241 of ,",
    "the value of @xmath230 was set to @xmath242 .",
    "@xmath243 monte - carlo simulations were used to evaluate the mean - square error ( mse ) of the estimates .    in figures [ fig : mse_vs_t_ts=32_nu>1 ] and [ fig : mse_vs_t_ts=32_nu<1 ] we plot the crb ( for @xmath110 ) or the lower and upper bounds of when @xmath244 , as well as the mse of the ml and aml estimators , as a function of @xmath46 , and compare the case where @xmath22 is known to the case where it is estimated from with @xmath245 snapshots in the secondary data .",
    "the following observations can be made :    * there is almost no difference between the mle and the amle , and therefore the latter should be favored since it does not require evaluating modified bessel functions . *",
    "the mse in the case where @xmath22 is known is lower than that when @xmath22 is to be estimated , which is expected .",
    "however , the difference is smaller when @xmath244 : in other words , it seems that adaptive whitening is not so much penalizing with small @xmath70 while it seems more crucial for @xmath246 .",
    "indeed , for small @xmath70 , what matters most is the fact that some snapshots are nearly noiseless , and this is more influential than obtaining a very good whitening . *",
    "the decrease of the mse for @xmath110 is roughly of the order @xmath214 . when @xmath244 , this rate is significantly increased and the mse decreases very quickly as @xmath206 , as predicted by the analysis above .",
    "this rate of convergence is also observed in figure [ fig : mse_multi_rknown_vs_t_vs_nu ] where we consider a scenario with two sources at @xmath247 . * the upper bound in seems to provide quite a good approximation of the actual mse , at least for @xmath46 large enough .",
    "the influence of @xmath226 is investigated in figure [ fig : mse_vs_ts_t=16 ] , where one can observe that about @xmath248 is necessary for the performance with estimated @xmath22 to be very close to the performance for known @xmath22 .",
    "however , as indicated above , this is less pronounced when @xmath244 , where the difference becomes smaller with lower @xmath226 .",
    "finally , we investigate whether the rate of convergence of the mle or amle when @xmath70 varies is impacted by a small amount of gaussian noise .",
    "more precisely , we run simulations where the data is generated as @xmath249 where @xmath250 , i.e. , the noise is a mixture of @xmath0-distributed noise and gaussian distributed noise .",
    "the covariance matrix of the noise is now @xmath251 and we use the aml estimator assuming that the noise has a @xmath0 distribution with parameter @xmath70 and known covariance matrix @xmath252 . in figure",
    "[ fig : mse_mixture_vs_t ] , we display the mse of the aml estimator versus @xmath46 and versus @xmath70 for different values of @xmath253 .",
    "clearly , the rate of convergence of the estimator is affected by a small amount of gaussian noise , even when @xmath70 is small .",
    "this indicates that , if noise is not purely @xmath0-distributed with small @xmath70 , we recover the usual behavior of the mse versus @xmath46 .",
    "in this paper we addressed the doa estimation problem in @xmath0-distributed noise using two data sets .",
    "the main result of the paper was to show that , when the shape parameter @xmath70 of the texture gamma distribution is below @xmath254 , the fim is unbounded .",
    "on the other hand , for @xmath110 , the fim is bounded and we derived an accurate closed - form approximation of the crb .",
    "the maximum likelihood estimator was derived as well as an approximation , which induces non significant losses compared to the exact mle . in the non regular case where @xmath244 , we derived lower and upper bounds on the mean - square error of the ( a)ml estimates and we showed that the rate of convergence of these ( a)ml estimates is about @xmath213 where @xmath46 is the number of snapshots .                          c.  j. coleman .",
    "the directionality of atmospheric noise and its impact upon an hf receiving system . in _",
    "proceedings 8th international conference hf radio systems techniques _ , pages 363366 , guildford , uk , 2000 .",
    "abramovich , g.  san antonio , and g.  j. frazer . over - the horizon radar signal - to -external noise ratio improvement in over - sampled uniform 2d antenna arrays : theoretical analysis of superdirective snr gains . in _",
    "proceedings ieee radar conference _ , pages 15 , ottawa , canada , 29 april-3 may 2013 .",
    "abramovich and g.  san antonio . over - the horizon radar",
    "potential signal parameter estimation accuracy in harsh sensing environment . in _",
    "proceedings ieeee international conference acoustics speech signal processing _ , pages 801804 , florence , italy , 4 - 9 may 2014 .                                  f.  pascal , y.  chitour , j .-",
    "ovarlez , p.  forster , and p.  larzabal .",
    "covariance structure maximum - likelihood estimates in compound gaussian noise : existence and algorithm analysis .",
    ", 56(1):3448 , january 2008 .",
    "m.  n. el korso , a.  renaux , and p.  forster . under @xmath255-distributed observation with",
    "parameterized mean . in _",
    "proceedings 8th ieee sensor array and multichannel signal processing workshop _ , pages 461464 , a corua , spain , 22 - 25 june 2014 .",
    "y.  i. abramovich and n.  k. spencer .",
    "diagonally loaded normalised sample matrix inversion ( lnsmi ) for outlier - resistant adaptive filtering . in _",
    "proceedings icassp _ , pages 11051108 , honolulu , hi , april 2007 .          y.  i. abramovich and o.  besson .",
    "regularized covariance matrix estimation in complex elliptically symmetric distributions using the expected likelihood approach - part 1 : the oversampled case . , 61(23):58075818 ,",
    "december 2013 .",
    "o.  besson and y.  i. abramovich .",
    "regularized covariance matrix estimation in complex elliptically symmetric distributions using the expected likelihood approach - part 2 : the under - sampled case . , 61(23):58195829 , december 2013 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating the direction of arrival of a signal embedded in @xmath0-distributed noise , when secondary data which contains noise only are assumed to be available . based upon a recent formula of the fisher information matrix ( fim ) for complex elliptically distributed data </S>",
    "<S> , we provide a simple expression of the fim with the two data sets framework . in the specific case of @xmath0-distributed noise , we show that , under certain conditions , the fim for the deterministic part of the model can be unbounded , while the fim for the covariance part of the model is always bounded . in the general case of elliptical distributions , we provide a sufficient condition for unboundedness of the fim . </S>",
    "<S> accurate approximations of the fim for @xmath0-distributed noise are also derived when it is bounded . </S>",
    "<S> additionally , the maximum likelihood estimator of the signal doa and an approximated version are derived , assuming known covariance matrix : the latter is then estimated from secondary data using a conventional regularization technique . when the fim is unbounded , an analysis of the estimators reveals a rate of convergence much faster than the usual @xmath1 . </S>",
    "<S> simulations illustrate the different behaviors of the estimators , depending on the fim being bounded or not . </S>"
  ]
}