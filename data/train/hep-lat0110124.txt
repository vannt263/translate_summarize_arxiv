{
  "article_text": [
    "the debate over which computing platforms are suitable for the simulation of lattice gauge theories has continued at this year s lattice conference .",
    "it has been shown that recent off - the - shelf pc processors by intel or amd are capable of delivering impressive floating - point performance on the order of 1 gflops if the vector processing units are employed and cache management is optimized @xcite .",
    "unfortunately , this single - node performance can not be fully utilized in a pc cluster if the local lattice volume becomes small . in this case",
    ", the surface - to - volume ratio is large , and the communications latency inherent in standard network solutions such as ethernet or myrinet slows down the individual processors .",
    "therefore , to achieve reasonable efficiencies with a pc cluster , the local lattice volume must not be too small .",
    "this implies that for a given total lattice volume , the number of nodes working on a single problem is limited .",
    "however , to finish a dynamical fermion calculation on a moderately large lattice in a reasonable amount of time , it is essential to distribute the total volume onto as many nodes as possible , which implies very small local lattice volumes , perhaps as small as @xmath0 .",
    "this requires communications between neighboring nodes with extremely low latencies that can not be achieved using off - the - shelf networking components .",
    "massively parallel machines with custom - designed communications hardware ( with latencies 10100 times smaller than in the case of myrinet ) appear to be the only viable alternative .",
    "this is especially true in the field of lattice gauge theory with its very regular communications requirements . in this contribution , we describe the design of the qcdoc architecture which is capable of delivering computing power in the tens of tflops range at a price / performance ratio of 1  us-$ per sustained mflops .",
    "a first description of the qcdoc project has been given at last year s lattice conference @xcite .",
    "in the nomenclature of parallel computers , this is a multiple - instruction , multiple - data ( mimd ) machine with distributed memory .",
    "the name qcdoc stands for `` qcd on a chip '' .",
    "an essential element of this project is the collaboration with ibm research and the resulting ability to utilize ibm s system - on - a - chip technology .",
    "nowadays it is possible to integrate most of the components that make up a single processing node on a single chip , creating an application - specific integrated circuit , or asic .",
    "the qcdoc chip is such an asic , consisting of    * 500 mhz , 32-bit powerpc 440 processor core * 64-bit , 1 gflops floating - point unit * 4 mbytes on - chip memory ( embedded dram ) * controllers for embedded and external memory * nearest - neighbor serial communications unit with estimated latencies of 120ns ( 300 ns ) for send ( receive ) , overlapped between the 12 independent directions and an aggregate bandwidth of 12 gbit / s * other components such as ethernet controller , interrupt controller , etc .",
    "the chip will occupy a die size of about @xmath1 , and the power consumption is expected to be in the range of 12 w.    two such asics will be mounted on a daughterboard , together with two industry - standard double data rate ( ddr ) sdram modules ( one per asic ) whose capacity will be determined by the price of memory when the machine is assembled .",
    "( at the time of this writing , 256 mbyte registered dimms with ecc were available for $ 40 . )",
    "32 daughterboards will be mounted on a motherboard , and 8 motherboards in a crate with a single backplane .",
    "the final machine then consists of a certain number of such crates .",
    "there are two separate networks : the physics network and an ethernet - based auxiliary network .",
    "the physics network consists of high - speed serial links between nearest neighbors with a bandwidth of 2@xmath2500 mbits / s per link .",
    "the nodes are arranged in a 6-dimensional torus which allows for an efficient partitioning of the machine in software , as described in detail in ref .",
    "the serial - communications unit in the qcdoc asic provides direct memory access , single - bit error detection with automatic resend , and a low - latency store - and - forward mode for global operations . the auxiliary network is used for booting , diagnostics , and i / o over ethernet , with an ethernet controller integrated on the asic .",
    "the ethernet traffic to and from the asic will run at 100 mbit / s . hubs or switches on the motherboard will provide a bandwidth of 1.6 gbit / s off a motherboard to commercial switches and the host workstation .",
    "-5 mm -40 mm    a schematic picture of the qcdoc asic is shown in fig .  [",
    "fig : asic ] . in this section",
    "we describe the various components in more detail .",
    "many of the ingredients of the asic are provided by ibm , see secs .",
    "[ sec : ibmlib][sec : ejtag ] .",
    "the major custom - designed components are the serial communications unit ( scu ) and the prefetching edram controller ( pec ) , see secs .",
    "[ sec : scu][sec : pec ] .",
    "* there are three main busses : * * the processor local bus ( plb ) is a 128-bit wide , fully synchronous bus that runs at 1/3 of the cpu frequency .",
    "it has separate read and write busses , supports pipelining ( up to 4 deep ) , and has a large number of sophisticated features such as split transactions , burst transfers , etc . * * the on - chip peripheral bus ( opb ) is a 32-bit wide , synchronous bus running at 1/2 of the plb frequency .",
    "its basic purpose is to off - load slower devices from the plb bus .",
    "* * the device control register bus ( dcr ) is a slow and simple bus used to read and write control registers .",
    "the devices on this bus are connected in a ring - like fashion .",
    "the 440 is the only master on this bus . *",
    "the 440 powerpc processor core is a dual - issue , superscalar , 32-bit implementation of the ibm book - e architecture ( the e stands for embedded ) .",
    "the core has a 7 stage pipeline",
    ". there are instruction and data caches of size 32 kbytes each .",
    "these are 64-way associative , partitionable , and lockable .",
    "hardware memory protection is provided through a translation - lookaside buffer ( tlb ) .",
    "the 440 has three plb master interfaces , one each for instruction read , data read , and data write",
    ". the 440 will run at 500 mhz and is connected to a 64-bit , 1 gflops ieee floating - point unit ( for details of the fpu see ref .",
    ". an important feature of the 440 is that it contains a jtag interface .",
    "jtag ( joint test action group ) is an industry - standard protocol that allows an external device to take complete control of the processor .",
    "this functionality will be used for booting and debugging , see below . *",
    "the plb arbiter provides programmable arbitration for the up to eight allowed masters that can control plb transfers . in our design",
    "we have seven masters : the three plb master interfaces of the 440 ( two of which , namely data read and data write , will be channeled through the pec , see below ) , the mcmal , two master interfaces for the send and receive operations of the scu and one for the dma unit . *",
    "the 4 mbytes of embedded dram ( edram ) are a standard ibm library component that will be accessed with low latency and high bandwidth through a custom - designed controller , the pec , see sec .",
    "[ sec : pec ] below . *",
    "the universal interrupt controller ( uic ) processes the various interrupts generated on- and off - chip and provides critical and non - critical interrupt signals to the 440 . *",
    "the ddr controller is a slave on the plb , capable of transferring data to / from external ddr sdram at a peak bandwidth of 2.6 gbytes / s .",
    "it supports an address space of 2 gbytes and provides error detection , error correction , and refresh of the off - chip sdram . *",
    "the plb - opb bridge is used to transfer data between the two busses .",
    "it is the only master on the opb and a slave on the plb . *",
    "the ethernet media access controller ( emac ) provides a 100 mbit / s ethernet interface .",
    "the media - independent interface ( mii ) signals at the asic boundary are connected to a physical layer chip on the daughterboard .",
    "the emac is a slave on the opb and has sideband signals to the mcmal on the plb . * the dma - capable memory access layer ( mcmal ) loads",
    "/ unloads the emac through the sideband signals .",
    "it is a master on the plb . *",
    "the inter - integrated circuit ( i@xmath3c ) controller is a slave on the opb used to read the configuration of the ddr dimm .",
    "this information is then used to configure the ddr controller . *",
    "the general purpose i / o ( gpio ) unit is another slave on the opb whose 32-bit wide data bus will be taken out to the asic boundary .",
    "it can be used , e.g. , to drive leds . *",
    "the high - speed serial links ( hssl ) used by the scu each contain eight independent ports ( four each for send / receive ) through which bits are clocked into / out of the asic at 500 mbit / s per port .",
    "the bits are converted to bytes ( or vice versa ) in the hssl .",
    "the hssl input clocks are phase - aligned by another ibm macro , the phase - locked loop ( pll ) .",
    "as mentioned above , the 440 core has a jtag interface over which one can take complete control of the processor . in particular , this interface can be used to load boot code into the instruction cache and start execution .",
    "this completely eliminates the need for boot rom .",
    "the question is how the jtag instructions should be loaded into the 440 .",
    "( there are special tools that use the jtag interface , but it would be impractical to connect one tool per asic for booting . ) a solution to this question has already been developed at ibm research , implemented using a field - programmable gate array ( fpga ) , that converts special ethernet packets to jtag commands and vice versa .",
    "this logic is now part of the qcdoc asic and will be used not only for booting but also to access the cpu for diagnostics / debugging at run time .",
    "the unique mac address of each asic is provided to the ethernet - jtag component by location pins on the asic , and the ip address is then derived from the mac address .",
    "the task of the scu is to reliably manage the exchange of data between neighboring nodes with minimum latency and maximum bandwidth .",
    "the design takes into account the particular communication requirements of lattice qcd simulations . a schematic picture of the scu is shown in fig .",
    "[ fig : scu ] .",
    "the custom protocol governing the data transfers defines packets that contain a 64-bit data word and an 8-bit header containing control and parity bits .",
    "when the receive unit receives a packet , it first interprets the header , buffers the bytes from the hssl , and assembles the 64-bit word .",
    "it then transfers the word to the receive register or passes it on to the send unit .",
    "the receive buffer can store three 64-bit words so that the send unit ( in a neighboring node ) can send three words before an acknowledgment is received .",
    "the functionality of the send unit is essentially the inverse of that of the receive unit .",
    "send and receive operations can proceed simultaneously .",
    "each send or receive unit is controlled by a dma engine which then transfers the data between memory and a send / receive register .",
    "each dma engine is controlled by block - strided - move instructions stored in sram in the scu itself .",
    "a low - latency passthrough mode is provided for global operations . because of the latencies associated with the hssl , the most efficient method to perform global sums is `` shift - and - add '' , using a store - and - forward capability built into the scu .",
    "the expected latency of a send ( receive ) operation is 120 ns ( 300 ns ) .",
    "this is one to two orders of magnitude lower than the latencies associated with myrinet . since a write instruction from the 440 can initiate many independent transfers on any subset of the 24 send or receive channels , the latencies associated with multiple transfers can be overlapped to some degree .",
    "the total off - chip bandwidth using all 24 hssl ports is 12 gbit / s . in a 4-dimensional physics application only 16 of the 24 hssl ports",
    "will be used , resulting in a total bandwidth of 8 gbit / s .",
    "this provides a good match for the communications requirements of typical applications . as a worst - case example",
    ", we consider a @xmath0 local lattice and the application of a staggered fermion conjugate gradient . in this case",
    "we obtain    [ cols= \" < , < \" , ]     the 75% efficiency in the first line is a reasonable assumption based on the performance figures quoted in table  1 below .",
    "[ [ sec : pec ] ]    -10 mm    the pec ( shown schematically in fig .",
    "[ fig : pec ] ) is designed to provide the 440 with high - bandwidth access to the edram .",
    "it interfaces to the 440 data read and data write busses via a fast version of the plb that runs at the cpu frequency and that we call processor direct bus ( pdb ) .",
    "the pec also contains a plb slave interface to allow for read and write operations from / to any master on the plb as well as a dma engine to transfer data between edram and the external ddr memory .",
    "the maximum pdb bandwidth is 8 gbytes / s for read and 8 gbytes / s for write .",
    "the latency of the pdb itself is 1 - 2 cpu cycles .",
    "this very low latency eliminates the need for pipelining .",
    "the access to edram ( which is memory - mapped ) proceeds at 8 gbytes / s .",
    "ecc is built into the pec , with 1-bit error correct and 2-bit error detect functionality .",
    "the pec also refreshes the edram .",
    "the read data prefetch from edram occurs in two 1024-bit lines .",
    "three read ports ( pdb , plb slave , dma ) arbitrate for the common edram . the coherency between pdb , plb slave , and dma is maintained within the pec .",
    "each read port has four 1024-bit registers that are paired in two sets to allow for ping - ponging between different memory locations .",
    "there are also two 1024-bit write buffer registers each for the pdb / plb slave / dma write interfaces .",
    "in this section the various aspects of the software environment are described .",
    "efforts are being made to make this environment user - friendly and to allow a wide range of application codes to run on the qcdoc platform without the need for extensive porting .",
    "* boot kernel*. the boot kernel will be loaded from the host work station via ethernet using the reliable udp protocol .",
    "the ethernet - jtag interface in the asic converts the packets to jtag commands which in turn load the boot code into the instruction cache and start execution ( see also sec .",
    "[ sec : ejtag ] ) .",
    "the boot kernel then assigns an ip address to the asic ( based on the mac address provided by the location pins ) and loads the ethernet driver .    * run - time kernel*. the run - time kernel communicates with the host using the ethernet driver loaded by the boot kernel .",
    "it performs complete node diagnostics and provides read , write , and execute capability .",
    "it is based on the unix rpc protocol .",
    "* riscwatch debugger*. this is an ibm - provided powerpc development / debugging environment for a set of selected nodes .",
    "it provides single - stepping capability with an advanced , user - friendly graphical user interface showing a cycle - by - cycle display of the internal state of the powerpc processor as os or user code executes .    *",
    "extensive diagnostic programs*. these will be provided to permit extensive , real - time hardware monitoring during program execution as well as targeted diagnostic tests for hardware maintenance .",
    "* host operating system*. the host machine will be an smp workstation with a suitable number of gbit ethernet ports , using a threaded transport protocol to efficiently manage tens of thousands of nodes .",
    "the host os will manage both the boot and the run - time kernel operation .",
    "it will provide both a qcsh and a socket - based interface to the qcdoc operation , thus allowing for both csh - scripted and perl - based machine control .",
    "* disk software*. we are planning a parallel disk system supporting a general distribution of disks .",
    "a simplified unix - like hierarchical file system will be used .",
    "a utility will be provided to store parallel files as a single , flattened , position - indexed file for archiving and use on other systems .",
    "* compiler , linker , loader*. the compiler and linker are gnu - based tools supporting c and c++ as well as book - e compliant , powerpc assembly language .",
    "* lattice qcd application code*. a code structure is being developed that will both provide the cross - platform capabilities needed by the ukqcd collaboration and follow the qcd - api standard being developed by the u.s .  scidac - supported lattice qcd software initiative .",
    "the functional design of the qcdoc asic is nearly finished .",
    "single - node physics code is running on the vhdl simulator , and from these simulations we obtained the performance figures shown in table  1 .",
    "these all refer to double - precision calculations .",
    "[ tab : perf ]    table 1 .",
    "double - precision performance of single - node physics code running on the simulator .",
    "+    l@l + single node fpu , su(3 ) @xmath2 2-spinor & 84% of peak from l1 cache + & 78% of peak from edram + cache fill / flush to edram & 8.0 gb / s peak + & 3.2 gb / s sustained ( due to 440 latency ) + software visible bandwidth to edram & 2.5 gb / s read , 2.0 gb / s write + ddr fill / flush & 1.2 gb / s read , 1.7 gb / s write +    we have set up a test environment in which two nodes can successfully communicate in the vhdl simulator over their ethernet interfaces and/or the hssls .",
    "the operating system is being developed on a system with six powerpc 405gp boards , one powerpc 750 board , five ethernet - jtag boards , and a riscwatch probe .",
    "this hardware configuration is shown in figure  [ fig : ppc_devel ] .",
    "-10 mm    the current design has been successfully synthesized , and an analysis net list has been provided to ibm for floorplanning purposes .",
    "anticipating a few iterations to satisfy timing constraints the complete design should be transferred to ibm by the end of 2001 .",
    "the qcdoc architecture combines state - of - the - art system - on - a - chip technology with custom - designed logic specially optimized for lattice qcd calculations to provide computing power in the tens of tflops range at a price / performance ratio of 1  us-$ per sustained mflops .",
    "optimized qcd code is expected to run with an efficiency of roughly 50% .",
    "the communications latencies of the qcdoc design are one to two orders of magnitude smaller than those of commercial network solutions used in pc clusters , thus allowing for a large problem to be distributed onto many nodes .",
    "this fact , along with significantly lower power consumption , higher reliability , more compact packaging and a better price / performance ratio , motivates the design efforts that are required to construct such a supercomputer .",
    "the first qcdoc machines with a combined peak speed of about 7 tflops are projected to be finished in 2002 , and by the end of 2003 the combined peak speed of qcdoc machines world - wide is targeted at 40 tflops ( 10 tflops of which will be installed in edinburgh , 10 tflops are planned for the rbrc and 20 tflops for the u.s .",
    "lattice community ) .",
    "this research was supported in part by the u.s .",
    "department of energy , the institute of physical and chemical research ( riken ) of japan , and the u.k .",
    "particle physics and astronomy research council ."
  ],
  "abstract_text": [
    "<S> a status report is given of the qcdoc project , a massively parallel computer optimized for lattice qcd using system - on - a - chip technology . </S>",
    "<S> we describe several of the hardware and software features unique to the qcdoc architecture and present performance figures obtained from simulating the current vhdl design of the qcdoc chip with single - cycle accuracy . </S>"
  ]
}