{
  "article_text": [
    "consider a problem of estimation of two lines by perturbed observations of points that lie on the lines .",
    "let the true points @xmath0 lie on the union of two different lines @xmath1 and @xmath2 , that is , @xmath3 let these points be observed with perturbations @xmath4 , @xmath5 , that  is , the observed points are @xmath6 , @xmath5 , with @xmath7 the perturbations are assumed to be independent and identically normally distributed , @xmath8 where @xmath9 is the @xmath10 identity matrix .",
    "the parameters @xmath11 , @xmath12 , @xmath13 , @xmath14 , and @xmath15 are to be estimated .",
    "we consider both functional and structural models . in _",
    "functional _ model , the true points are assumed to be nonrandom . in _",
    "structural _ model , the true points are assumed to be independent and identically distributed ( i.i.d . ) .",
    "the errors @xmath4 are i.i.d . and independent of the true points .",
    "in the structural model , @xmath16 are i.i.d .",
    "random vectors , and thus , the observed points @xmath6 are i.i.d . in the functional model ,",
    "the observed points are independent , gaussian , with different means but with common covariance matrix .",
    "the true lines defined by eqs .   can not be parallel to the . in order to avoid overflows during evaluation of the estimators ( except of rban - moment estimator ) , another parameterization is used internally : @xmath17 , where @xmath18 is a unit vector orthogonal to the line , and @xmath19 is a point on the line .",
    "the computation of the rban - moment estimator ( see section  [ ss : rbanmome ] ) is implemented for explicit parameterization only .",
    "computational optimization of the rban - moment estimator is a matter of further work .",
    "the explicit parameterization has the advantage that the number of parameters is equal to the dimension of parameter space .",
    "( in @xcite , the second - order equation has six unknown coefficients , but the conic section can be parameterized with five parameters . the parameter space for the parameters of the conic section was the five - dimensional unit sphere in the six - dimensional euclidean space .",
    "mismatch between the number of parameters and the dimension of the parameter space made the asymptotic covariance matrix of the estimator _",
    "singular_. )    in simulations , the confidence intervals for the coordinates of the intersection point of the two lines are obtained based on the asymptotic covariance matrix for the intersection point . for the projections",
    "the als2 estimator , that asymptotic covariance matrix can be evaluated without use of explicit line parameterization .",
    "let the true points @xmath0 lie on the second - order algebraic curve @xmath20 hereafter , a second - order algebraic curve is called a `` conic section '' or a `` conic . ''",
    "the points are observed with gaussian perturbations , and the perturbed points are denoted as @xmath6 .",
    "we have the same equations @xmath21  in the two - line fitting model .",
    "the vector of coefficients is denoted by @xmath22 .",
    "the nonzero vector @xmath23 and the error variance @xmath15 are the parameters of interest .",
    "similarly to the two - line fitting model , the _ functional _ and the _ structural _ models are distinguished .",
    "a couple of lines is a degenerate case of a conic section .",
    "therefore , the conic section fitting model is an extension of the two - line fitting model .",
    "we consider the adjusted least squares ( als ) estimator for unknown @xmath15 .",
    "the estimator is constructed in @xcite . introduce the @xmath24 symmetric matrix @xmath25 asterisks are typed instead of some entries above the diagonal of a symmetric matrix .",
    "the entries of the matrix @xmath26 are generalized hermite polynomials in @xmath27 and @xmath28 .",
    "the matrix @xmath26 is constructed such that @xmath29 in the functional model and @xmath30 for the true points and true parameters .",
    "denote @xmath31    the estimator @xmath32 of the error variance @xmath15 is obtained from the equation @xmath33    equation   always has a unique nonnegative solution . if @xmath34 , then the solution to is positive almost surely .",
    "the matrix @xmath35 is singular .",
    "define the estimator @xmath36 of the vector @xmath23 as a nonzero solution to the equation @xmath37    the strong consistency of the als2 estimator is proved in @xcite and @xcite under somewhat different conditions .",
    "the asymptotic normality is proved in @xcite for the functional model and in @xcite for the structural model .",
    "two consistent estimators of the asymptotic covariance matrix are constructed in @xcite .",
    "denote @xmath38 under the conditions of proposition  [ prop : corec1 ] stated further , the latter limits exist almost surely .",
    "see @xcite for explicit expressions of the matrices @xmath39 , @xmath40 , @xmath41 , and @xmath42 .",
    "note that @xmath43 is a constant matrix .",
    "[ prop : corec1 ] in the functional model , for all integer @xmath44 and @xmath45 such that @xmath46 , let the following limits exist and be finite : @xmath47 whereas in the structural model , let @xmath48 and @xmath49 . in both models , let @xmath50 . then    1 .",
    "the estimator @xmath36 is strongly consistent in the following sense : @xmath51 2 .   @xmath52 .",
    "3 .   eventually , @xmath53 .",
    "`` eventually '' in the previous statement means that almost surely there exists @xmath54 such that @xmath55 for all @xmath56 . in other words ,",
    "almost surely , @xmath57 holds only for finitely many @xmath58",
    ".    denote the normalized version of the true parameter @xmath59    normalize the estimator of @xmath23 in such a way that @xmath60 and @xmath61 . therefore , denote @xmath62    -\\sqrt{\\tfrac { -n } { { \\hat{\\boldsymbol\\beta}}{}^\\top { \\boldsymbol\\varpsi}'_n(\\hat\\sigma^2 ) { \\hat{\\boldsymbol\\beta } } } } \\,{\\hat{\\boldsymbol\\beta } } &",
    "\\mbox{if $ { \\mbox{\\boldmath$\\beta$}}{}^\\top { \\hat{\\boldsymbol\\beta } } < 0$}. \\end{cases}\\ ] ]    [ prop : propals2 ] under the conditions of proposition  [ prop : corec1 ] , the estimator @xmath63 is a strongly consistent estimator of @xmath64 , that is , @xmath65",
    "a.s .    in the functional model , for all integer @xmath44 and @xmath45 such that @xmath66 , let the following limits exist and be finite : @xmath47 whereas in the structural model , let @xmath67 and @xmath68 . in both models , let @xmath50 .",
    "then the estimator @xmath69 is asymptotically normal in the following sense : @xmath70 where @xmath71    under the conditions of part of proposition  [ prop : propals2 ] , the following estimator of the asymptotic covariance matrix is consistent : @xmath72 that is , @xmath73 in probability .",
    "the methods of fitting an algebraic curve ( or surface ) to observed points can be classified as follows .",
    "[ [ algebraic - distance - methods ] ] algebraic distance methods , + + + + + + + + + + + + + + + + + + + + + + + + + + +    where the residuals in the equations for the algebraic curve are minimized .",
    "for example , the minimum point of the sum of squared residuals @xmath74 ( with some normalizing constraint in order to avoid @xmath75 ) in the conic fitting problem and @xmath76 in the two - line fitting problem is called the _ ordinary least squares _ ( ols ) estimator .",
    "the criterion function for the ols estimator is simple enough and can be adjusted so that the resulting estimator is consistent ( under some conditions ) .",
    "such an estimator is called the _ adjusted least squares _ ( als ) estimator . the ols and als",
    "estimators are method - of - moments estimators , meaning that the criterion functions for the estimators are polynomials whose coefficients are sample moments of coordinates of the observed points . hence , the ols and als estimators can be computed efficiently .    in order to obtain parameters of two lines ,",
    "the observed points are fitted with a conic section , and then the parameters of the conic section are used to obtain the parameters of two lines .",
    "there are some papers where this idea is used .",
    "the problem of estimating the fundamental matrix for two - camera view is considered in @xcite .",
    "the fundamental matrix is a singular matrix whose left and right null - vectors are the coordinates of each camera in the coordinate system of the other camera .",
    "initially , the als estimator of the fundamental matrix is evaluated .",
    "then it is projected so that the estimated fundamental matrix is singular .    in @xcite the problem of segmentation of a finite - dimensional vector space onto linear subspaces is considered , and the generalized principal component analysis method is introduced .",
    "the sample is fitted with an algebraic cone ( a  set of points that satisfy a homogeneous algebraic equation ) by the ols method .",
    "then subspaces are extracted from the algebraic cone with use of a  small learning sample .",
    "an application of segmentation of a vector space onto hyperplanes for searching planes on binocular image is given in @xcite .    in @xcite an ellipsoid fitting problem with a constraint such that a center of the ellipsoid lies on a given line is considered . the algebraic distance with embedded constraint",
    "is minimized .",
    "the analytical ( behavioral ) properties of the optimization problem are studied .",
    "we consider a conic section fitting problem but with different constraint  the conic is degenerated to a couple of straight lines .",
    "[ [ geometric - distance - methods ] ] geometric distance methods , + + + + + + + + + + + + + + + + + + + + + + + + + + +    where distances between the estimated curve and each point are minimized .",
    "the sum of squares of those distances is minimized , and the _ orthogonal regression _ ( or ) estimator is obtained .",
    "a numerical algorithm for evaluation of the orthogonal regression estimator is presented in monograph @xcite .",
    "the orthogonal regression is consistent in the single straight line fitting problem ( * ? ? ?",
    "* section 1.3.2(a ) ) .",
    "in nonlinear models , the estimator may be inconsistent .",
    "there is a one - step correction procedure in explicit and implicit models @xcite with application in the ellipsoid fitting model @xcite .",
    "however , in the two - line fitting model , the correction from @xcite is unstable .",
    "[ [ probabilistic - methods ] ] probabilistic methods + + + + + + + + + + + + + + + + + + + + +    they are used to obtain the _ maximum likelihood _ ( ml ) estimator and bayes estimators .      let @xmath77 be a sequence of random events .",
    "the random event @xmath78 is said to hold eventually if almost surely there exists @xmath54 such that @xmath78 occurs for all @xmath56 .",
    "in other words , the random event @xmath78 holds eventually if and only if it does not occur only for finitely many @xmath58 almost surely .    the estimator @xmath79 is called asymptotically normal if @xmath80 in distribution , were the asymptotic covariance matrix @xmath81 may be singular , and @xmath58 is the sample size .",
    "this definition differs from the conventional one adopted in asymptotic theory because here only @xmath82-asymptotic normality is considered .",
    "let @xmath83 be a bivariate random vector .",
    "then @xmath84 is called the 40% ellipsoid of the normal distribution because @xmath85 .",
    "this is the ellipsoid where the probability density function is at least @xmath86 of its maximum .      in section",
    "[ sec : estimators ] , we construct five estimators for parameters of the two line fitting model . in section  [ sec : equivariance ] , we propose two definitions of the equivariance of an estimator and state that all of the five estimators are equivariant .",
    "the estimators are compared numerically in section  [ sec : simulations ] .",
    "the proofs are given in appendix  [ appendix ] .",
    "the two - line fitting model is a restriction of the conic section fitting model .",
    "a  couple of lines defined by the equation @xmath87 is a degenerate conic section @xmath88 with coefficients @xmath89 with a constraint @xmath90 .",
    "the conic section als2 estimator provides estimation of the error variance  @xmath15 and the coefficients @xmath91 .",
    "denote by @xmath92 the indicator of a line which the true point @xmath93 belongs to . equation   can be rewritten as @xmath94 the indicator @xmath95 is nonrandom in the functional model , and it is a random variable in the structural model .",
    "[ prop:2lcals2f ] let , in the functional model,@xmath96 \\xi_i^ { } & \\xi_i^2   & \\xi_i^3 \\\\[2pt ] \\xi_i^2   & \\xi_i^3   & \\xi_i^4 \\end{pmatrix } \\right ) > 0 \\quad \\mbox{for $ j=1,\\,2$}.\\end{gathered}\\ ] ] then the asl2 estimators @xmath36 and @xmath32 are strongly consistent in the sense of   and  .",
    "there are two cases where the structural model is not identifiable . if the common distribution of the true points is concentrated on a straight line and on a single point ( presumably not on the line ) , that is , @xmath97 then there are many ways to fit the true points with two lines . if the common distribution of the true points is concentrated in four points , that is , @xmath98 then there are three ways to fit the true points with two lines ( unless three of the four points lie on a straight line , which is a particular case of ) .",
    "[ prop:2lcals2s ] in the structural model , assume that @xmath99 and that nonidentifiability conditions and do not hold .",
    "then the als2 estimator is strongly consistent in the sense of and .    in order to estimate the parameters @xmath11 , @xmath12 , @xmath13 , and @xmath14",
    ", we can solve eqs .  .",
    "with ignoring the last equation @xmath100 , the solution is @xmath101    substituting the elements of the als2 estimator @xmath102 into the right - hand side of  , we obtain an `` ignore-@xmath103 '' estimator : @xmath104    if the conic section estimated by the als2 estimator is a hyperbola , then the `` ignore-@xmath103 '' estimate of the two lines comprises the asymptotes of the hyperbola .",
    "choose the sign @xmath105 in such that @xmath106 .",
    "we need the notation @xmath107 for the function that expresses the line parameters @xmath11 , @xmath12 , @xmath13 , @xmath14 in elements of @xmath23 and is defined  . with this notation , we can write @xmath108    [ prop : ignf_func_cons ] in the functional model , assume the following : @xmath109 \\xi_i^ { } & \\xi_i^2   & \\xi_i^3 \\\\[2pt ] \\xi_i^2   & \\xi_i^3   & \\xi_i^4 \\end{pmatrix } \\right ) > 0 \\quad \\mbox{for $ j=1,\\,2$}.\\end{gathered}\\ ] ] then the `` ignore-@xmath103 '' estimator of the parameters of two lines is strongly consistent , that is , @xmath110 as @xmath111 almost surely .    [ prop : ignf_struct_cons ] if in the structural model , @xmath112 , @xmath99 , and neither condition nor condition holds , then the `` ignore-@xmath103 '' estimator is consistent .",
    "now , we state the asymptotic normality of the `` ignore-@xmath103 '' estimator .    [ prop : ignf_func_an ] in the functional model , assume the following :    * @xmath112 , * for @xmath113 and @xmath114 , the following limits exist and are finite : @xmath115 * for @xmath116 and @xmath117 , the matrices @xmath118 \\mu_1^{(j ) } & \\mu_2^{(j ) } & \\mu_3^{(j ) } \\\\[2pt ] \\mu_2^{(j ) } & \\mu_3^{(j ) } & \\mu_4^{(j ) } \\end{pmatrix}\\ ] ] are nonsingular .    then the `` ignore-@xmath119 '' estimator @xmath120 is asymptotically normal , namely @xmath121 where @xmath122 is the asymptotic covariance matrix of @xmath123 , and @xmath124 is the @xmath125 matrix of derivatives of the mapping @xmath126 defined in  at the true parameters @xmath127 , that is , @xmath128    the matrix @xmath129 is nonsingular .",
    "[ prop : ignf_struct_an ] if , in the structural model , @xmath112 , @xmath67 , and neither nor holds , then the `` ignore-@xmath119 '' estimator is asymptotically normal , that is , holds .",
    "the estimators @xmath130 , @xmath131 , @xmath132 , and @xmath133 obtained in  do not change if @xmath134 , @xmath135 ,  , @xmath136 are multiplied by a common factor .",
    "so it does not matter which normalization of @xmath23 is used .",
    "equation   represents a couple of intersecting straight lines if and only if @xmath137 denote @xmath138 where the function @xmath139 and its derivative @xmath140 are evaluated at the point @xmath141 .",
    "perform one - step update of the estimator @xmath63 to make it closer to the surface@xmath142 : @xmath143 then use expressions  to estimate @xmath144 : @xmath145    [ prop : can_1st ] under the conditions of proposition  [ prop : ignf_func_an ] in the functional model or under the conditions of proposition  [ prop : ignf_struct_an ] in the structural model , the estimator @xmath146 is consistent and asymptotically normal , and its asymptotic covariance matrix is equal to @xmath147    the normalization of the estimator @xmath36 affects its asymptotic covariance matrix , and hence has effect on the estimates @xmath148 .",
    "however , the normalization does not affect the asymptotic covariance matrix of @xmath149 .",
    "the sum of squared distances between each observed point and the closer of two lines is equal to @xmath150 the _ orthogonal regression _",
    "estimator is a borel - measurable function of observations such that @xmath151    in the functional model , the orthogonal regression estimator is the maximum likelihood estimator .",
    "however , because the dimension of parameter space grows as the sample size is increasing , the orthogonal regression estimator may be inconsistent .",
    "the estimator is constructed in the structural model , so it should be called the structural maximum likelihood estimator .",
    "if a gaussian distribution of a random point @xmath152 is concentrated on a  straight line @xmath153 , then it is a singular normal distribution : @xmath154 where @xmath155 and @xmath156 are the expectation and variance of the random variable @xmath157 .",
    "note that the covariance matrix @xmath158 is singular and positive semidefinite .",
    "if the distribution of a random point @xmath0 is concentrated on two straight lines @xmath1 and @xmath2 and the distribution on each line is gaussian , then , due to , the conditional distributions are @xmath159 \\sim n \\left ( \\begin{pmatrix } \\mu_{j\\xi } \\\\ k_j \\mu_{j\\xi } + h_j \\end{pmatrix } , \\ , \\begin{pmatrix } \\sigma_{j\\xi}^2    & k_j \\sigma_{j\\xi}^2 \\\\",
    "k_j \\sigma_{j\\xi}^2 & k^2_j \\sigma_{j\\xi}^2 \\end{pmatrix } \\right ) = n({\\mbox{\\boldmath$\\mu$}}_j , \\varsigma_{0j})\\ ] ] for @xmath160 .",
    "the matrices @xmath161 are positive semidefinite and singular , that is,@xmath162 , and the points @xmath163 are the centers of gaussian distribution of the points on each line .",
    "the distribution of @xmath93 is a mixture of two singular normal distributions @xmath164 where @xmath165 is the probability that the point @xmath93 lies of the first line .",
    "the distribution of the observed points is also a mixture of two gaussian distributions @xmath166 with @xmath167 , where @xmath15 is the error variance ; see .",
    "note that @xmath168 .",
    "the likelihood function for the sample of points with a mixture of two normal distributions is @xmath169 where @xmath170 is the density of a bivariate normal distribution",
    ".    one method of evaluating the maximum likelihood estimator is as follows :    1 .",
    "find the point of conditional minimum @xmath171 } l(p , { \\mbox{\\boldmath$\\mu$}}_1 , \\varsigma_1 , { \\mbox{\\boldmath$\\mu$}}_2 , \\varsigma_2 ) .\\ ] ] 2 .",
    "set @xmath172 here @xmath173 , @xmath174 , and @xmath175 are the entries of the matrix @xmath176 , and @xmath177 and @xmath178 are the elements of the vector @xmath179 : @xmath180 3 .",
    "find the estimates @xmath130 , @xmath131 , @xmath132 , @xmath133 from the equations @xmath181 that is , set @xmath182    the denominator @xmath183 may be equal to 0 with some positive probability .",
    "occurrence of this event means that the estimated figure is a straight line and a single point outside the line rather than two straight lines .    in order to make the statement of consistency easier ,",
    "assume that @xmath112 and choose the estimator such that @xmath184 .",
    "the _ regular best asymptotically normal _ ( rban ) estimators were developed by chiang @xcite .",
    "our rban moment estimator differs from the original rban so that not only the observed points @xmath6 , but also monomials @xmath185 , @xmath46 , are averaged .",
    "introduce the 14-dimensional vectors whose elements are the monomials of coordinates of observed points : @xmath186    evaluate the average and sample covariance matrix of the vectors @xmath187 : @xmath188    denote @xmath189 where @xmath157 , @xmath190 , and @xmath191 are independent random variables such that @xmath192 basically , the function @xmath193 is defined for all @xmath194 , @xmath195 , that comprise possible 4-tuples of moments of a random variable , that is , satisfy @xmath196 see @xcite .",
    "however , since the elements of the vector - function @xmath193 are polynomials of its arguments , it can be extended to @xmath197",
    ".    denote @xmath198    in the structural model , @xmath199 \\bigr)_{j=1,q=1}^{2\\mskip28mu 4 } \\bigr).\\ ] ]    consider the equation @xmath200 it is a system of 14 equations in 14 variables .",
    "has a solution , then the moment estimator can be defined as one of the solutions .",
    "may have no solution .    in the rest of section  [ ss : rbanmome ]",
    ", @xmath201 is a @xmath202 matrix .",
    "the estimator is defined as a point where @xmath203 attains its minimum : @xmath204 this minimization problem is similar to that in theorem 6 in @xcite .",
    "the minimum @xmath205 can be evaluated explicitly , and this allows us to reduce the dimension of minimization problem .",
    "the reduction of dimension of the optimization problem was used , for example , in @xcite .",
    "the routines evaluating the rban - moment estimator and the estimator for its covariance matrix are developed without rigid theoretical basis ; see section  [ ss : rban - momenteval ] .",
    "the similarity transformation of @xmath206 is @xmath207 where @xmath208 is an orthogonal matrix , @xmath209 is a scaling coefficient , and @xmath210 is an intercept .",
    "the transformation of a sample of points acting elementwise is also denoted @xmath211 : if @xmath212 , then @xmath213 .",
    "hereafter , we use vector notation : the observed points are denoted @xmath214 , and the true points are denoted @xmath215 .",
    "the underlying statistical structure is @xmath216 , where @xmath217 is the observed sample , @xmath212 , @xmath218 is the borel @xmath219-field , and @xmath220 is a parameter that uniquely identifies the distribution of the observed points ; @xmath221 in the functional model , and @xmath222 in the structural model .",
    "here @xmath223 are points located on two strait lines , and @xmath224 is a probability measure concentrated on two straight lines .",
    "the statistical structure is invariant with respect to transformation @xmath225 if the change of the probability measure induced by the transformation of the sample can be obtained by some transformation @xmath226 of parameters , that is , if there exists a bijection @xmath227 such that @xmath228 here @xmath229 is the induced probability measure ; it is sometimes denoted @xmath230 .",
    "the statistical structure is similarity invariant if it is invariant with respect to all similarity transformations of the form  .    in order to become similarity invariant",
    ", the underlying statistical structure needs some extension .",
    "we assume that the true points lie on two lines , which _ may _ be parallel to the @xmath28-axis .",
    "the following restrictions do not ruin the invariance :    * the true lines @xmath231 and @xmath232 intersect each other but do not coincide . *",
    "the true points @xmath233 in the functional model or the set @xmath234 where the true points are concentrated in the structural model can be covered with two lines uniquely . in the structural model",
    ", this means that the nonidentifiability conditions and do not hold .    with these restrictions ,",
    "the statistical structure is invariant with @xmath235 in the functional model and @xmath236 in the structural model .",
    "let @xmath237 and @xmath238 be functions that extract the parameters of interest . if , in the functional model , @xmath239 and points @xmath240 lie on the lines @xmath231 and @xmath232 or if , in the structural model , @xmath241 and the probability measure is concentrated on the union of two lines @xmath242 , then @xmath243 .",
    "if @xmath244 , then @xmath245 .",
    "we treat @xmath246 as an unordered couple , that is , @xmath247 .",
    "the transformation of the lines parameters and the transformation of @xmath15 do not interfere each other , and these transformations are not interfered by a  particular location or distribution of true points on the lines , that is , the parameter transformation @xmath226 is such that there exists transformations @xmath248 and @xmath249 such that @xmath250 these transformations are @xmath251    the estimator is called equivariant with respect to the transformation @xmath225 if , when the data are transformed , the estimator follows the inducing transformation of parameters .",
    "the estimator @xmath252 for two lines and the estimator @xmath253 for error variance are equivariant with respect to similarity transformation @xmath225 if @xmath254 the estimator is called similarity equivariant if it is equivariant with respect to any similarity transformation @xmath225 .    in a fitting problem , an estimator for a `` true figure ''",
    "is called _ fitting equivariant _ with respect to transformation @xmath255 , @xmath256 if , when the sample is transformed , the estimated `` true figure '' follows the same transformation @xmath225 .",
    "an estimator is called _ similarity fitting equivariant _ if it is _ fitting equivariant _ with respect to any similarity transformation .    in the two - line fitting problem ,",
    "denote by @xmath257 the union of a pair of two lines .",
    "an estimator @xmath252 is similarity fitting equivariant if and only if for any similarity transformation @xmath255 , @xmath258    the similarity fitting equivariant estimator depends on geometry of the plane and does not depend on the cartesian coordinate system used .    because of , in the two - line fitting model , the estimator for two lines @xmath252 is similarity equivariant if and only if it is similarity fitting equivariant .",
    "some troubles , which may arise during estimation , are not addressed yet .",
    "* the estimation may fail with small positive probability .",
    "for example , the conic section estimated with the als2 estimator is an ellipse with some positive probability , and if it is , then the `` ignore-@xmath119 '' estimator fails .",
    "( if the estimator is consistent , then the failure probability tends to 0 as @xmath259 ) .",
    "* the estimation may fail , for example , because the estimated line should be parallel to the @xmath28-axis , but the estimating procedure does not handle such case .",
    "* the optimization problem may have multiple extremal points .",
    "for the als2 estimator , it may occur that @xmath260 .    in order to define the equivariance of an unreliable estimator",
    ", we allow that the estimators fail simultaneously in both sides  , , or .",
    "also , we allow that for fixed similarity transformation @xmath255 ,  , , or does not hold with probability 0 .",
    "the equivariance of the als2 estimator in the conic section fitting problem is verified in ( * ? ? ?",
    "* section 5.5 ) ( see theorem  30 there for similarity fitting equivariance ) .",
    "that implies the equivariance of the `` ignore-@xmath119 '' estimator .    in order to make the _ updated before ignore-@xmath119 step _ estimator equivariant , we use normalization of the als2 estimator rather than @xmath261 .",
    "the _ orthogonal regression _ estimator and the parametric _ maximum likelihood _ estimator are maximum likelihood estimators , but in different models .",
    "thus , they are equivariant .",
    "the criterion function for the _ rban - moment _ estimator is similarity invariant .",
    "this means that the criterion function does not change when the data sample follows a similarity transformation and the parameters follow the inducing transformation .",
    "thus , the rban - moment estimator is equivariant .",
    "consider a further restriction of the mixture - of - two - normal - distributions model from section  [ ss : pml ] .",
    "assume that covariance matrices of @xmath262 and @xmath263 have the same diagonal entries but additive inverse off - diagonal entries : @xmath264    the statistical structure is invariant in scaling of the @xmath28-coordinate , @xmath265 , @xmath266 .",
    "this transformation maps the lines @xmath267 and @xmath268 onto the lines @xmath269 and @xmath270 , respectively .",
    "the maximum likelihood estimator in this model is equivariant .",
    "however , this equivariance is somewhat strange .",
    "the transformation of parameters that induces the scaling of the @xmath28-coordinate of the observed points does not induce the same transformation of the true points nor the same mapping of the true lines .",
    "the estimated lines follow the transformation of parameters rather than the transformation of observed points .",
    "this is illustrated in fig .",
    "[ fig : f1 ] .",
    "( the unmodified sample ) and one of points @xmath271 ( the shrunken - in-@xmath28 sample ) , are fitted with two lines ( the estimated lines are the solid lines on the figures ) .",
    "the estimated lines for the unmodified sample ( blue solid lines on the left figure ) when scaled with the same transformation as the observed points are scaled ( blue dashed line on the right figure ) do not coincide with the actually estimated lines for the shrunken - in-@xmath28 sample ( red solid lines on the right figure ) .",
    "the ellipsoids are the 40% ellipsoids of the estimated normal distributions ( the compound distributions of the estimated mixture ]    let @xmath272 be the true value of the parameter @xmath273 before the transformation . then after the transformation , the value of the parameter is @xmath274 with @xmath275 . if @xmath276 , @xmath277 , and @xmath278 , then @xmath279 .",
    "hence , the maximum likelihood estimator is not fitting equivariant with respect to scaling of the @xmath28-coordinate here .",
    "a sample of the true points @xmath0 , @xmath5 , is generated from a random distribution concentrated on ( a subset of ) two lines .",
    "three distributions of the true points are used ; see fig .",
    "[ fig:3distrs ] :    * a mixture of two singular normal distributions , * a discrete distribution , * a uniform distribution on two line segments .",
    "these three distributions of true points are concentrated on the same two lines @xmath280 which intersect one another at the point @xmath281 .",
    "for the same sample of true points @xmath282 , 100 samples of the measurement errors @xmath283 , @xmath284 , are simulated , and 100 samples of the observed points @xmath285 are obtained ; see   and  . for each sample of the observed points , the estimates of the parameters of the true lines",
    "were evaluated with the following five methods : two als2-based estimators ( the ignore-@xmath103 estimator and the estimator with one - step update of the als2 estimator before the ignore-@xmath103 step ) , the orthogonal regression estimator , the parametric maximum likelihood estimator , and the rban moment estimator .    for each estimated couple of lines ,",
    "the point of their intersection is found .",
    "the 100 estimates of intersection points are averaged , and their sample standard deviations are evaluated . for the als2-based estimators and the rban moment estimator ,",
    "the standard errors of the estimators are also evaluated .      for computation of the _ orthogonal regression _ estimator , the @xmath273-means method is used . initially , two lines were chosen randomly",
    ". then _ classification _ and _ mean _ steps are alternated . on the classification step ,",
    "the observed points are split into two clusters based on which line is closer to the point .",
    "( the first cluster contains all the observed points that are closer to the first line than to the second line , and the second cluster contains the other observed points . ) on the means step , each cluster is fitted with a straight line by the orthogonal regression method ( the two lines are updated ) .",
    "the algorithm is completed when the classification step does now change the clusters .",
    "the obtained parameters of the two lines deliver a local minimum to the criterion function @xmath286 . trying to obtain the global minimum ,",
    "the algorithm is restarted several times with different initial two lines .    for computation of the _ parametric maximum likelihood estimator _ , the expectation ",
    "maximization algorithm @xcite is used . the equation for optimization problem of finding a minimum of the likelihood function @xmath287 such that @xmath288 is @xmath289 where @xmath290 is a vector parameterization of @xmath291 such that @xmath288 .",
    "hence , the maximum likelihood estimator is a stationary point of the function @xmath292 with fixed @xmath293    the em algorithm is iterative .",
    "once the @xmath294th approximation @xmath295 is obtained , the weights are evaluated : @xmath296 then @xmath297th approximation @xmath298 is obtained by minimizing @xmath299 under the constraint @xmath300 . the point where the minimum is attained can be explicitly expressed in @xmath301 , @xmath302 , and @xmath303 , @xmath5 .      in case",
    "the criterion function @xmath304 has multiple minima , a consistent estimator  that is , the `` ignore-@xmath119 '' estimator ",
    "is used as the initial point , and the criterion function @xmath305 is searched for a  local minimum nearby . here @xmath306 is a vector meaning the parameters of interest",
    ".    the knowledge or misspecification of the parameter @xmath307 does not affect the estimator for the parameters of interest @xmath11 , ",
    ", @xmath15 .",
    "thus , for estimation of the asymptotic covariance matrix , assume @xmath308 to be known .",
    "the estimator of the asymptotic covariance matrix of @xmath309 is @xmath310 where @xmath311 the estimator of the asymptotic covariance matrix of @xmath220 is the principal submatrix of @xmath312 .",
    "average of estimated centers over 100 simulations , standard deviations over 100 simulations , and medians of estimated standard errors are presented in tables  [ table1-normal][table1-uniform ] .",
    "@l@d .. 4d .. 4 @d .. 4d",
    ".. 4 @d .. 4d .. 4@ & & & + true value & -0.08 & 0.31 & & & & +   + ignore-@xmath119 & -0.1098 & 0.2753 & 0.8611 & 0.3866 & 0.1918 & 0.1125 + update & -0.0820 & 0.2912 & 0.0706 & 0.0620 & 0.0437 & 0.0479 + or & 0.6533 & 3.4524 & 0.0877 & 0.6783 & & + ml & -0.0795 & 0.3077 & 0.0326 & 0.0269 + rban & 0.0647 & 0.3759 & 0.3563 & 0.2606 & 0.0350 & 0.0438 +   + ignore-@xmath119 & -0.0909 & 0.3052 & 0.0646 & 0.0308 & 0.0601 & 0.0303 + update & -0.0796 & 0.3080 & 0.0127 & 0.0156 & 0.0124 & 0.0155 + or & 0.5492 & 3.1488 & 0.0175 & 0.1701 & & + ml & -0.0776 & 0.3083 & 0.0103 & 0.0088 & & + rban & -0.0789 & 0.3100 & 0.0126 & 0.0154 & 0.0127 & 0.0154 +   + ignore-@xmath119 & -0.0799 & 0.3101 & 0.0211 & 0.0095 & 0.0188 & 0.0093 + update & -0.0801 & 0.3098 & 0.0037 & 0.0042 & 0.0039 & 0.0047 + or & 0.5606 & 3.2041 & 0.0063 & 0.0484 & & + ml & -0.0801 & 0.3101 & 0.0030 & 0.0025 & & + rban & -0.0801 & 0.3101 & 0.0038 & 0.0042 & 0.0039 & 0.0048 +   + ignore-@xmath119 & -0.0799 & 0.3099 & 0.0151 & 0.0075 & 0.0147 & 0.0072 + update & -0.0795 & 0.3097 & 0.0052 & 0.0051 & 0.0052 & 0.0049 + or & -0.0792 & 0.3092 & 0.0050 & 0.0044 & & + ml & -0.0794 & 0.3093 & 0.0048 & 0.0043 & & + rban & -0.0797 & 0.3098 & 0.0063 & 0.0057 & 0.0052 & 0.0049 +    @l@d .. 4d .. 4 @d .. 4d .. 4 @d .. 4d .. 4@ & & & + true value & -0.08 & 0.31 & & & & +   + ignore-@xmath119 & -0.0699 & 0.3077 & 0.0263 & 0.0290 & 0.0241 & 0.0263 + update & -0.0722 & 0.3116 & 0.0197 & 0.0186 & 0.0203 & 0.0175 + or & -0.0755 & 0.3188 & 0.0148 & 0.0144 & & + ml & -0.0958 & 0.3315 & 0.0131 & 0.0120 & & + rban & -0.0717 & 0.3105 & 0.0209 & 0.0175 & 0.0205 & 0.0178 +   + ignore-@xmath119 & -0.0783 & 0.3109 & 0.0092 & 0.0078 & 0.0080 & 0.0083 + update & -0.0785 & 0.3114 & 0.0071 & 0.0054 & 0.0065 & 0.0061 + or & -0.0721 & 0.3157 & 0.0048 & 0.0046 & & + ml & -0.0931 & 0.3278 & 0.0043 & 0.0035 & & + rban & -0.0786 & 0.3113 & 0.0071 & 0.0053 & 0.0065 & 0.0061 +   + ignore-@xmath119 & -0.0798 & 0.3098 & 0.0031 & 0.0024 & 0.0026 & 0.0027 + update & -0.0799 & 0.3099 & 0.0025 & 0.0016 & 0.0021 & 0.0019 + or & -0.0715 & 0.3151 & 0.0017 & 0.0013 & & + ml & -0.0932 & 0.3283 & 0.0013 & 0.0011 & & + rban & -0.0799 & 0.3099 & 0.0024 & 0.0017 & 0.0021 & 0.0019 +   + ignore-@xmath119 & -0.0796 & 0.3094 & 0.0033 & 0.0032 & 0.0036 & 0.0033 + update & -0.0798 & 0.3097 & 0.0030 & 0.0024 & 0.0033 & 0.0023 + or & -0.0782 & 0.3086 & 0.0021 & 0.0018 & & + ml & -0.0786 & 0.3087 & 0.0019 & 0.0018 & & + rban & -0.0796 & 0.3092 & 0.0030 & 0.0028 & 0.0033 & 0.0024 +    @l@d .. 4d .. 4 @d .. 4d .. 4 @d .. 4d .. 4@ & & & + true value & -0.08 & 0.31 & & & & +   + ignore-@xmath119 & -0.0785 & 0.3122 & 0.0363 & 0.0274 & 0.0318 & 0.0301 + update & -0.0794 & 0.3140 & 0.0216 & 0.0258 & 0.0205 & 0.0290 + or & -0.0616 & 0.3127 & 0.0185 & 0.0167 & & + ml & -0.0934 & 0.3118 & 0.0116 & 0.0111 & & + rban & -0.0807 & 0.3126 & 0.0219 & 0.0293 & 0.0193 & 0.0292 +   + ignore-@xmath119 & -0.0796 & 0.3107 & 0.0103 & 0.0103 & 0.0099 & 0.0095 + update & -0.0796 & 0.3110 & 0.0067 & 0.0103 & 0.0065 & 0.0094 + or & -0.0639 & 0.3087 & 0.0064 & 0.0049 & & + ml & -0.0904 & 0.3106 & 0.0042 & 0.0033 & & + rban & -0.0797 & 0.3107 & 0.0066 & 0.0104 & 0.0064 & 0.0095 +   + ignore-@xmath119 & -0.0798 & 0.3098 & 0.0035 & 0.0030 & 0.0032 & 0.0030 + update & -0.0798 & 0.3098 & 0.0021 & 0.0029 & 0.0020 & 0.0030 + or & -0.0625 & 0.3085 & 0.0015 & 0.0014 & & + ml & -0.0891 & 0.3107 & 0.0012 & 0.0011 & & + rban & -0.0796 & 0.3097 & 0.0023 & 0.0030 & 0.0020 & 0.0030 +   + ignore-@xmath119 & -0.0799 & 0.3100 & 0.0041 & 0.0032 & 0.0041 & 0.0035 + update & -0.0798 & 0.3101 & 0.0033 & 0.0032 & 0.0032 & 0.0034 + or & -0.0803 & 0.3103 & 0.0023 & 0.0021 & & + ml & -0.0805 & 0.3101 & 0.0022 & 0.0020 & & + rban & -0.0798 & 0.3100 & 0.0035 & 0.0033 & 0.0032 & 0.0034 +    using the estimator @xmath119 ( by _ one - step update _ before ignore-@xmath119 step ) , we improve the precision of estimation .",
    "the precision of the _ rban - moment _ estimator approximates the precision of the _ updated before ignore-@xmath119 step _ estimator .",
    "the parametric _ maximum likelihood _",
    "estimator is the best when the normality condition , which was assumed during construction of the estimator , is satisfied . otherwise , it is biased .",
    "the _ orthogonal regression _ and the _ maximum likelihood _ estimators are good for small error variance ( @xmath313 ) . for @xmath314 ,",
    "the _ orthogonal regression _",
    "estimator is broken down when the distribution of true points is a  mixture of two normal distributions and is biased for the two other distributions of true points.=1    mean - square deviance of the intersection of the estimated lines from the true intersection point is presented in table  [ table - fulldeviances ] .",
    ".. 4 d .. 4 d .. 4 d .. 4 d .. 4@ & & & & & & +   + 1000 & 0.1 & 0.9403 & 0.0954 & 3.2978 & 0.0421 & 0.6124 + 10000 & 0.1 & 0.0722 & 0.0201 & 2.9127 & 0.0138 &",
    "0.0199 + 100000 & 0.1 & 0.0230 & 0.0056 & 2.9645 & 0.0038 & 0.0056 + 1000 & 0.02 & 0.0168 & 0.0073 & 0.0067 & 0.0065 & 0.0084 +   + 1000 & 0.1 & 0.0403 & 0.0281 & 0.0228 & 0.0320 & 0.0284 + 10000 & 0.1 & 0.0121 & 0.0091 & 0.0118 & 0.0228 & 0.0090 + 100000 & 0.1 & 0.0039 & 0.0029 & 0.0101 & 0.0226 & 0.0029 + 1000 & 0.02 & 0.0046 & 0.0038 & 0.0036 & 0.0032 & 0.0042 +   + 1000 & 0.1 & 0.0453 & 0.0367 & 0.0310 & 0.0209 & 0.0365 + 10000 & 0.1 & 0.0145 & 0.0123 & 0.0181 & 0.0117 & 0.0123 + 100000 & 0.1 & 0.0046 & 0.0036 & 0.0177 & 0.0093 & 0.0037 + 1000 & 0.02 & 0.0052 & 0.0046 & 0.0031 & 0.0030 & 0.0048 +    for small errors , the _ rban - moment _ estimator is a bit less accurate than the _ updated before ignore-@xmath119 step _ estimator . for @xmath314 ,",
    "the difference is negligible .",
    "the parametric _ maximum likelihood _ estimator has the smallest deviation from the true value , except for the discrete distribution of true points and @xmath315 .    for small errors ( @xmath313 ) , the _",
    "orthogonal regression _ estimator outperforms the consistent estimators and has the deviation approximately as small as the parametric _ maximum likelihood _ estimator .",
    "normalization of the estimator of @xmath23 affects the als2-based estimator of two lines with _ one - step update before the ignore-@xmath119 step_. with normalization @xmath316 , the derived estimator of two lines is not equivariant , whereas with normalization @xmath317 , the derived estimator is equivariant .",
    "comparison of equivariant and nonequivariant versions of the estimator is displayed in table  [ table - updatevers ] .",
    "@rd .. 2c d .. 6d .. 6 d .. 6d .. 6 d .. 6d .. 6@ & & ver . & & & + & -0.08 & 0.31 & & & & +   + 1000 & 0.1 & ev & -0.082046 & 0.291175 & 0.070617 & 0.062003 & 0.043713 & 0.047939 + & & ne & -0.044038 & 0.247382 & 0.251372 & 0.514473 & 0.038853 & 0.050167 + 10000 & 0.1 & ev & -0.079623 & 0.308039 & 0.012652 & 0.015582 & 0.012403 & 0.015471 + & & ne & -0.085055 & 0.304177 & 0.015924 & 0.018695 & 0.012506 & 0.015550 + 100000 & 0.1 & ev & -0.080137 & 0.309780 & 0.003710 & 0.004173 & 0.003925 & 0.004749 + & & ne & -0.080991 & 0.309386 & 0.003880 & 0.004255 & 0.003926 & 0.004742 + 1000 & 0.02 & ev & -0.079548 & 0.309703 & 0.005156 & 0.005131 & 0.005174 & 0.004891 + & & ne & -0.079918 & 0.309508 & 0.005247 & 0.005149 & 0.005179 & 0.004918 +   + 1000 & 0.1 & ev & -0.072202 & 0.311648 & 0.019709 & 0.018553 & 0.020266 & 0.017500 + & & ne & -0.071460 & 0.312230 & 0.020049 & 0.018740 & 0.020213 & 0.017457 + 10000 & 0.1 & ev & -0.078482 & 0.311377 & 0.007087 & 0.005371 & 0.006518 & 0.006066 + & & ne & -0.078418 & 0.311436 & 0.007090 & 0.005387 & 0.006520 & 0.006054 + 100000 & 0.1 & ev & -0.079868 & 0.309929 & 0.002460 & 0.001647 & 0.002060 & 0.001900 + & & ne & -0.079863 & 0.309934 & 0.002461 & 0.001647 & 0.002060 & 0.001901 + 1000 & 0.02 & ev & -0.079772 & 0.309728 & 0.002967 & 0.002376 & 0.003320 & 0.002344 + & & ne & -0.079755 & 0.309732 & 0.002963 & 0.002375 & 0.003319 & 0.002339 +   + 1000 & 0.1 & ev & -0.079405 & 0.313977 & 0.021551 & 0.025759 & 0.020451 & 0.028992 + & & ne & -0.078507 & 0.315350 & 0.022219 & 0.026091 & 0.020512 & 0.029115 + 10000 & 0.1 & ev & -0.079604 & 0.311024 & 0.006673 & 0.010337 & 0.006467 & 0.009389 + & & ne & -0.079576 & 0.311176 & 0.006685 & 0.010349 & 0.006456 & 0.009372 + 100000 & 0.1 & ev & -0.079795 & 0.309802 & 0.002075 & 0.002919 & 0.001974 & 0.002994 + & & ne & -0.079794 & 0.309818 & 0.002076 & 0.002921 & 0.001972 & 0.002992 + 1000 & 0.02 & ev & -0.079833 & 0.310081 & 0.003252 & 0.003249 & 0.003172 & 0.003418 + & & ne & -0.079825 & 0.310097 & 0.003250 & 0.003249 & 0.003169 & 0.003411 +    @rlcccccc@ & & & + & & & & & + & & & & & & & + & & & & & & & +   + 1000 & 0.1 & 70.6 & 80.2 & 1449 .",
    "& 70.0 & 79.2 & 1562 .",
    "+ 10000 & 0.1 & 79.4 & 93.8 & 236.2 & 79.6 & 92.9 & 259.4 + 100000 & 0.1 & 80.7 & 94.9 & 15.38 & 80.6 & 94.9 & 15.17 + 1000 & 0.02 & 80.4 & 95.1 & 15.95 & 78.0 & 94.1 & 19.86 +   + 1000 & 0.1 & 78.1 & 93.9 & 81.80 & 77.4 & 93.4 & 78.94 + 10000 & 0.1 & 81.1 & 95.6 & 12.34 & 80.9 & 95.8 & 12.39 + 100000 & 0.1 & 80.1 & 94.6 & 1.205 & 79.9 & 94.7 & 1.204 + 1000 & 0.02 & 81.0 & 94.9 & 1.984 & 81.3 & 95.2 & 1.988 +   + 1000 & 0.1 & 82.1 & 94.3 & 152.9 & 81.5 & 94.3 & 138.4 + 10000 & 0.1 & 81.0 & 96.6 & 20.36 & 80.3 & 96.3 & 18.92 + 100000 & 0.1 & 78.4 & 95.0 & 1.823 & 78.6 & 95.0 & 1.842 + 1000 & 0.02 & 78.7 & 94.7 & 2.926 & 78.5 & 94.1 & 3.041 +    there is a tendency that the equivariant version of the estimator is more accurate for small samples than the nonequivariant version .",
    "the two versions of the estimator are consistent and asymptotically equivalent .",
    "when the estimation is precise , the difference between the versions is negligible .",
    "when the estimation is imprecise , it is impossible to make inference which version is more accurate .      in @xcite a conic section fitting model is considered , and two estimators ( @xmath318 and @xmath319 ) for the asymptotic covariance matrix of the als2 estimator are constructed .",
    "the software developed here can be used to make numerical comparison of the estimates of the asymptotic covariance matrices .",
    "the data are generated as described in section  [ ss : simulationsetup ] with 1000 simulations for each set of true points .",
    "thus , the true conic unnecessarily was chosen degenerate . for each simulation ,",
    "the parameters of the conic section were estimated ; its center is found , and two confidence ellipsoids for the center were constructed using two different estimators of the asymptotic covariance matrix .    the sample coverage probability and median ( over 1000 ellipsoids ) area of the confidence ellipsoids is presented in table  [ tab : table2acm ] .",
    "the ellipsoids were constructed for confidence levels 0.8 and 0.95 .",
    "the area of 95% confidence ellipsoids is displayed in table  [ tab : table2acm ] , and the area of 80% confidence ellipsoid is @xmath320 of the area of 95% confidence ellipsoids .",
    "note that standard errors for coverage probability are @xmath321 for 80% confidence ellipsoids and @xmath322 for 95% confidence ellipsoids .",
    "the simulations do not allow us to make an inference which estimator is better .",
    "thus , @xmath323-based estimator _ updated before ignore-@xmath119 step _ is compared with other estimators in simulations in section  [ ss : simresults ] because of simpler explicit expression for @xmath323 .",
    "the strong consistency of the estimator follows from ( * ? ? ? * theorem 17 ) . under the conditions of proposition  [ prop : corec1 ] , @xmath324 by lemma 5 in @xcite , @xmath325 , which , together with , implies @xmath326 ( see the proof of theorem 2 in @xcite ) . then @xmath327 eventually , the left - hand side of in negative .",
    "the strong consistency of @xmath63 follows from and .",
    "the proof of asymptotic normality and consistency of the estimator of the asymptotic covariance matrix can be obtained by modification of the proofs of theorem 2 in @xcite and theorem 3 in @xcite .",
    "proposition  [ prop:2lcals2s ] follows from proposition 25 in @xcite . the identifiability condition ( s5- ) in @xcite holds because the intersection of a  couple of lines and a conic section may be a finite set with not more than four points , a straight line , a straight line and a point outside the line , or the couple of lines and the conic section coincide ; in the last case , the coefficients of the equations for the lines and the conic section satisfy relations .",
    "consistence of the `` ignore-@xmath119 '' estimator follows from the consistency of the als2 estimator @xmath336 and from the continuity of the function @xmath337 at the point of the true value of the parameter @xmath23 .",
    "the asymptotic normality of the `` ignore-@xmath119 '' follows from the asymptotic normality of @xmath338 and the differentiability of @xmath337 at the point @xmath127 .",
    "the consistency and asymptotic normality of the @xmath338 estimator , the differentiability of the functional @xmath139 at point @xmath127 ( note that @xmath339 ) , and the convergence @xmath340 imply the convergence and asymptotic normality of the updated estimator  @xmath341 .",
    "thus , the consistency and asymptotic normality of @xmath148 can beproved similarly to those of the `` ignore-@xmath119 '' estimator .          : a gentle tutorial of the em algorithm and its application to parameter estimation for gaussian mixture and hidden markov models .",
    "technical report tr-97 - 021 , international computer science institute ( 1998 )    : a gentle tutorial of the em algorithm and its application to parameter estimation for gaussian mixture and hidden markov models .",
    "technical report tr-97 - 021 , international computer science institute ( 1998 )"
  ],
  "abstract_text": [
    "<S> we consider the two - line fitting problem . </S>",
    "<S> true points lie on two straight lines and are observed with gaussian perturbations . for each observed point , it is not known on which line the corresponding true point lies . </S>",
    "<S> the parameters of the lines are estimated .    </S>",
    "<S> this model is a restriction of the conic section fitting model because a couple of two lines is a degenerate conic section . </S>",
    "<S> the following estimators are constructed : two projections of the adjusted least squares estimator in the conic section fitting model , orthogonal regression estimator , parametric maximum likelihood estimator in the gaussian model , and regular best asymptotically normal moment estimator .    </S>",
    "<S> the conditions for the consistency and asymptotic normality of the projections of the adjusted least squares estimator are provided . </S>",
    "<S> all the estimators constructed in the paper are equivariant . </S>",
    "<S> the estimators are compared numerically .    </S>",
    "<S> ./style / arxiv - vmsta.cfg    conic section fitting , curve fitting , subspace clustering 62j05,62h12,62h30 </S>"
  ]
}