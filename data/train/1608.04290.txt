{
  "article_text": [
    "structured matrix factorization ( smf ) has been a popular tool in signal processing and machine learning . for decades",
    ", factorization models such as the singular value decomposition ( svd ) and eigen - decomposition have been applied for dimensionality reduction ( dr ) , subspace estimation , noise suppression , feature extraction , etc .",
    "motivated by the influential paper of lee and seung @xcite , new smf models such as _ nonnegative matrix factorization _ ( nmf ) have drawn much attention , since they are capable of not only reducing dimensionality of the collected data , but also retrieving loading factors that have physically meaningful interpretations .",
    "in addition to nmf , some related smf models have attracted considerable interest in recent years .",
    "the remote sensing community has spent much effort on a class of factorizations where the columns of one factor matrix are constrained to lie in the unit simplex @xcite .",
    "the same smf model has also been utilized for document clustering @xcite , and , most recently , multi - sensor array processing and blind separation of power spectra for dynamic spectrum access @xcite .",
    "the first key question concerning smf lies in identifiability  when does a factorization model or criterion admit unique solution in terms of its factors ?",
    "identifiability is important in applications such as parameter estimation , feature extraction , and signal separation . in recent years",
    ", identifiability conditions have been investigated for the nmf model @xcite .",
    "an undesirable property of nmf highlighted in @xcite is that identifiability hinges on both loading factors containing a certain number of zeros . in many applications ,",
    "however , there is at least one factor that is dense . in hyperspectral unmixing ( hu ) , for example , the basis factor ( i.e. , the spectral signature matrix ) is always dense .",
    "on the other hand , very recent work @xcite showed that the smf model with the coefficient matrix columns lying in the unit simplex admits much more relaxed identifiability conditions .",
    "specifically , fu _ et al .",
    "_ @xcite and lin _ et al . _",
    "@xcite proved that , under some realistic conditions , unique loading factors ( up to column permutations ) can be obtained by finding a minimum - volume enclosing simplex of the data vectors .",
    "notably , these identifiability conditions of the so - called _ volume minimization _ ( volmin ) criterion allow working with dense basis matrix factors ; in fact , the model does not impose any constraints on the basis matrix except for having full - column rank . since the nmf model can be recast as ( viewed as a special case of ) the above smf model @xcite",
    ", such results suggest that volmin is an attractive alternative to nmf for the wide range of applications of nmf and beyond .",
    "compared to nmf , volmin - based matrix factorization is computationally more challenging .",
    "the notable prior works in @xcite and @xcite formulated volmin as a constrained ( log-)determinant minimization problem , and applied successive convex optimization and alternating optimization to deal with it , respectively .",
    "the major drawback of these pioneering works is that the algorithms were developed under a noiseless setting , and thus only work well for high signal - to - noise ratio ( snr ) cases . also , these algorithms work in the dimension - reduced domain , but the dr process may be sensitive to outliers and modeling errors .",
    "the work @xcite took noise into consideration , but the algorithm is computationally prohibitive and has no guarantee of convergence . some other algorithms @xcite work in the original data domain , and deal with a volume - regularized data fitting problem .",
    "such a formulation can tolerate noise to a certain level , but is harder to tackle than those in @xcite  volume regularizers typically introduce extra difficulty to an already very hard bilinear fitting problem .",
    "the second major challenge of implementing volmin is that the volmin criterion is very sensitive to outliers : it has been noted in the literature that even a single outlier can make the volmin criterion fail @xcite .",
    "however , in real - world applications , outlying measurements are commonly seen : in hu , pixels that do not always obey the nominal model are frequently spotted because of the complicated physical environment @xcite ; and in document clustering , articles that are difficult to be classified to any known category may also act like outliers .",
    "the algorithm in @xcite is the state - of - the - art volmin algorithm that takes outliers into consideration .",
    "it imposes a ` soft penalty ' on outliers that lie outside the simplex that is sought , thereby allowing the existence of some outliers and achieving robustness .",
    "the algorithm works fairly well when the data are not severely corrupted , but it works in the reduced - dimension domain  and dr pre - processing can fail due to outliers .    * contributions * in this work , we explore both theoretical and practical aspects of volmin . on the theory side , we show that two existing sufficient conditions for volmin identifiability are in fact equivalent .",
    "the two identifiability results were developed in parallel , rely on different mathematical tools , and offer seemingly different characterizations of the sufficient conditions  so their equivalence is not obvious .",
    "our proof ` cross - validates ' the existing results , and thus leads to a deeper understanding of the volmin problem . on the algorithm side , we propose a new algorithmic framework for dealing with the volmin criterion .",
    "the proposed framework takes outliers into consideration , without requiring dr pre - processing .",
    "specifically , we impose an outlier - robust loss function onto the data fitting part , and propose a modified log - determinant loss function as the volume regularizer . by majorizing both functions ,",
    "the fitting and the volume - regularization terms can be taken care of in a refreshingly easy way , and a simple _ inexact _ alternating optimization algorithm is derived .",
    "a nesterov - type first - order optimization technique is further employed within this framework to accelerate convergence .",
    "the proposed algorithm is flexible  problem - specific prior information on the factors and different volume regularizers can be easily incorporated .",
    "convergence of the proposed algorithm to a stationary point is also shown .",
    "besides a judiciously designed set of simulations , we also validate the proposed algorithm using real - life datasets .",
    "specifically , we use remotely sensed hyperspectral image data and document data to showcase the effectiveness of the proposed algorithm in hyperspectral unmixing and document clustering applications , respectively .",
    "notice that volmin has never been used for document clustering before , to the best of our knowledge , and our work shows that volmin is indeed very effective in this context , outperforming the state - of - art in terms of clustering accuracy .    a conference version of part of this work appears in @xcite . beyond @xcite",
    ", this journal version includes the equivalence of the identifiability conditions , first - order optimization - based updates , consideration of different types of regularization and constraints , proof of convergence , extensive simulations , and experiments using real data .",
    "+ _ notation _ : we largely follow common notational conventions in signal processing . @xmath0 and @xmath1 denote a real - valued @xmath2-dimensional vector and a real - valued @xmath3 matrix , respectively ( resp . )",
    ". @xmath4 ( resp .",
    "@xmath5 ) means that @xmath6 ( resp .",
    "@xmath7 ) is element - wise non - negative .",
    "@xmath8 ( resp .",
    "@xmath9 ) also means that @xmath6 ( resp .",
    "@xmath7 ) is element - wise non - negative . @xmath10 and",
    "@xmath11 mean that @xmath7 is positive definite and positive semidefinite , resp .",
    "the superscripts `` @xmath12 '' and `` @xmath13 '' stand for the transpose and inverse operations , resp",
    ". the @xmath14 norm of a vector @xmath15 , @xmath16 , is denoted by @xmath17 . the @xmath14 quasi - norm , @xmath18 , is denoted by the same notation . the frobenious norm and the matrix 2-norm are denoted by @xmath19 and @xmath20 , respectively .",
    "the all - one vector is denoted by @xmath21 .    in this paper",
    ", we also make extensive use of convex analysis .",
    "let @xmath22 $ ] .",
    "the convex cone of @xmath23 is denoted by @xmath24 ; the convex hull of @xmath23 is denoted by @xmath25 ; when @xmath26 are linearly independent , @xmath27 is also called a simplex ; the set of extreme rays of @xmath28 is denoted by @xmath29 ; and the dual cone of a convex @xmath30 is denoted by @xmath31 ; @xmath32 denotes the set of the boundary points of the _ second order cone _ @xmath30 .",
    "we point the readers to @xcite for detailed illustration of the above concepts .",
    "in this section , we first give a brief introduction to the volmin criterion for smf and a concise review of the existing identifiability results .",
    "then , we prove that the two independently developed identifiability results ( using rather different mathematical tools ) are equivalent .",
    "consider the following signal model : @xmath33 = { \\bm a}{\\bm s}[\\ell]+{\\bm v}[\\ell],\\quad \\ell=1,\\ldots , l,\\ ] ] where @xmath34 \\in\\mathbb{r}^{m}$ ] is a measured data vector that is indexed by @xmath35 , @xmath36 is a basis which is assumed to have full column - rank , @xmath37\\in\\mathbb{r}^{k}$ ] is the coefficient vector representing @xmath38 $ ] in the low dimensional subspace @xmath39 , and @xmath40\\in\\mathbb{r}^m$ ] denotes noise .",
    "we assume that every @xmath37 $ ] satisfies @xmath41\\geq{\\bm 0}~\\text{and}~{\\bm 1}^t{\\bm s}[\\ell]=1.\\ ] ] the model can be compactly written as @xmath42 , where @xmath43,\\ldots,{\\bm x}[l]]$ ] , @xmath44,\\ldots,{\\bm s}[l]]$ ] and @xmath45,\\ldots,{\\bm v}[l]]$ ] .",
    "the task of smf is to factor @xmath7 into @xmath46 and @xmath47 .",
    "the simple model in - parsimoniously captures the essence of a large variety of applications . for _",
    "document clustering _ or _ topic mining _ @xcite , estimating @xmath46 and @xmath47 can help recognize the most popular topics / opinions in textual data ( e.g. , documents , web content , or social network posts ) , and cluster the data according to their weights on different topics / opinions . in _ hyperspectral _ _ remote sensing _",
    "@xcite , @xmath38 $ ] represents a remotely sensed pixel using sensors of high spectral resolution , @xmath48 denote @xmath49 different spectral signatures of materials that comprise the pixel @xmath38 $ ] , and @xmath50 $ ] denotes the proportion of material @xmath51 contained in pixel @xmath38 $ ] . estimating @xmath46 enables recognition of the underlying materials in a hyperspectral image .",
    "see fig .",
    "[ fig : motivation_rvolmin ] for an illustration of these motivating examples .",
    "very recently , the same model has been applied to _ power spectra separation _",
    "@xcite for dynamic spectrum access and _ fast blind speech separation _ @xcite .",
    "in addition , many applications of nmf can also be considered under the model in - , after suitable normalization @xcite .",
    "many algorithms have been developed for finding such a factorization , and we refer the readers to @xcite for a survey . among these algorithms , we are particularly interested in the so - called _ volume minimization _ ( volmin ) criterion , which is identifiable under certain reasonable conditions .",
    "volmin is motivated by the nice geometrical interpretation of the constraints in : under these constraints , all the data points live in a _ convex hull _ spanned by @xmath48 ( or , a _ simplex _ spanned by @xmath48 when the @xmath52 s are linearly independent ) ; see fig .",
    "[ fig : experiments ] .",
    "if the data points are sufficiently spread in @xmath53 , then the minimum - volume enclosing convex hull coincides with @xmath53 .",
    "formally , the volmin criterion can be formulated as    [ eq : volmin_form ] @xmath54\\})=&\\arg\\min_{{\\bm b},~\\{{\\bm c}[\\ell]\\}}~{\\rm",
    "vol}({\\bm b})\\\\                                   { \\rm s.t.}~&{\\bm x}[\\ell ] = { \\bm b}{\\bm c}[\\ell ] ,   \\label{eq : simp1}\\\\                                       & { \\bm 1}^t{\\bm c}[\\ell]=1,~{\\bm c}[\\ell]\\geq{\\bm 0},~\\forall \\ell,\\label{eq : simp2}\\end{aligned}\\ ] ]    where @xmath55 denotes a measure that is related or proportional to the volume of the simplex @xmath56 , and - mean that every @xmath38 $ ] is enclosed in @xmath56 ( i.e. , @xmath38 \\in{\\rm conv}\\{{\\bm b}_1,\\ldots,{\\bm b}_k\\}$ ] ) . in the literature , various functions for @xmath55 have been used @xcite .",
    "one representative choice of @xmath55 is @xmath57,\\ ] ] or its variants ; see @xcite .",
    "the reason of employing such a function is that @xmath58 is the volume of the simplex @xmath56 by definition @xcite .",
    "another popular choice of @xmath55 is @xmath59 see @xcite .",
    "note that @xmath60 is the volume of the simplex @xmath61 , which should scale similarly with the volume of @xmath56 .",
    "the upshot of is that has a simpler structure than .",
    "the most appealing aspect of volmin is its identifiability of @xmath46 and @xmath47 : under mild and realistic conditions , the optimal solution to problem   is essentially the true @xmath62 . to be precise ,",
    "let us make the following definition .",
    "( volmin identifiability ) consider the matrix factorization model in - , and let @xmath63 be any optimal solution to problem  .",
    "if every optimal @xmath63 satisfies @xmath64 and @xmath65 , where @xmath66 denotes a permutation matrix , then we say that volmin identifies the true matrix factors , or _",
    "volmin identifiability holds_.     fu _ et al .",
    "_ @xcite have shown that    [ thm : fumahuasid ] let @xmath67 be the function in .",
    "define a second order cone @xmath68 .",
    "then volmin identifiability holds if @xmath69 and    1 .   @xmath70 ; and 2 .",
    "@xmath71 where @xmath72 is any unitary matrix except the permutation matrices .    in plain words , a sufficient condition under which volmin identifiability holds is when @xmath73\\}_{\\ell=1}^l$ ]",
    "are sufficiently scattered over the unit simplex , such that the second order cone @xmath74 is a subset of @xmath75 . by comparing theorem  [ thm : fumahuasid ] to the identifiability conditions for nmf ( see @xcite ) , we see a remarkable advantage  volmin does not have any restriction on @xmath46 except being full - column rank .",
    "in fact , @xmath46 can be dense , partially negative , or even complex - valued .",
    "this result allows us to apply volmin to a wider variety of applications than nmf .    .",
    "the sufficient conditions in @xcite and @xcite both require that @xmath74 ( the inner circle ) is contained in @xmath75.,width=188 ]    in @xcite , another sufficient condition for volmin identifiability was proposed :    [ thm : linma ] let @xmath67 be the function in .",
    "assume @xmath69 .",
    "define @xmath76 and @xmath77 .",
    "then volmin identifiability holds if @xmath78 .",
    "theorem  [ thm : linma ] does not characterize its identifiability condition using convex cones like theorem  [ thm : fumahuasid ] did .",
    "instead , it defines a ` diameter ' @xmath79 of the convex hull spanned by the columns of @xmath47 , and then develops an identifiability condition based on it .",
    "the sufficient conditions presented in the two theorems seemingly have different flavors , but we notice that they are related in essence . to see the connections , we first note that theorem  [ thm : fumahuasid ] still holds after replacing @xmath75 and @xmath74 with convex hulls @xmath80,\\ldots,{\\bm s}[l]\\}$ ] and @xmath81 , respectively  since the @xmath37 $ ] s are all in @xmath82 .",
    "in fact , @xmath81 is exactly the set @xmath83 for @xmath84 .",
    "geometrically , we illustrate the conditions in fig .  [",
    "fig : suff ] using @xmath85 for visualization .",
    "we see that , if we look at the conditions in theorem  [ thm : fumahuasid ] at the 2-dimensional hyperplane that contains @xmath86 , the two conditions both mean that the inner shaded region is contained in @xmath80,\\ldots,{\\bm s}[l]\\}$ ] .",
    "motivated by this observation , in this paper , we rigorously show that    [ thm : equivalence ] the sufficient conditions for volmin identifiability in theorem  [ thm : fumahuasid ] and theorem  [ thm : linma ] are equivalent .",
    "the proof of theorem  [ thm : equivalence ] can be found in appendix  [ app : thm ] .",
    "although the geometrical connection may seem clear on hindsight , rigorous proof is highly nontrivial .",
    "we first show that the condition in theorem  [ thm : fumahuasid ] is equivalent to another condition , and then establish equivalence between the ` intermediate ' condition and the condition in theorem  [ thm : linma ] .",
    "the equivalence between the sufficient conditions in theorem  [ thm : fumahuasid ] and theorem  [ thm : linma ] is interesting and surprising  although the corresponding theoretical developments started from very different points of view , they converged to equivalent conditions .",
    "their equivalence brings us deeper understanding of the volmin criterion .",
    "the proof itself clarifies the role of regularity condition ii ) in theorem  [ thm : fumahuasid ] , which was originally difficult to describe geometrically  and now we understand that condition ii ) is there to ensure @xmath78 , i.e. , the existence of a convex cone that is ` sandwiched ' by @xmath75 and @xmath74 .",
    "in addition , the equivalence also suggests that the different cost functions in and ensure identifiability of @xmath46 and @xmath47 under the same sufficient conditions , and thus they are expected to perform similarly in practice . on the other hand , since the function in is easier to handle , using it in practice is more appealing . as a by - product , since we have proved that condition ii )",
    "is equivalent to a condition that was used for nmf identifiability in @xcite ( cf .",
    "lemma  [ lem : conditions ] ) , our result here also helps better understand the sufficient condition for nmf identifibility in @xcite in a more intuitively pleasing way .",
    "in this section , we turn our attention to designing algorithms for dealing with the volmin criterion .",
    "optimizing the volmin criterion is challenging .",
    "in early works such as @xcite , linear dr with @xmath87 is assumed such that the basis after dr is a square matrix .",
    "this subsequently enables one to write the dr - domain volmin problem as @xmath88=\\tilde{\\bm b}{\\bm c}[\\ell]\\\\             & { \\bm 1}^t{\\bm c}[\\ell]=1,~{\\bm c}[\\ell]\\geq { \\bm 0 } , \\end{aligned}\\ ] ] where @xmath89\\in\\mathbb{r}^k$ ] is the dimension - reduced data vector corresponding to @xmath90 $ ] , and @xmath91 is a dimension - reduced basis .",
    "note that minimizing @xmath92 is the same as minimizing @xmath93 .",
    "problem   can be efficiently tackled via either alternating optimization @xcite or successive convex optimization @xcite .",
    "the drawback with these existing algorithms is that noise was not taken into consideration . also , these approaches require dr to make the effective @xmath46 square  but dr may not be reliable in the presence of outliers or modeling errors .",
    "another major class of algorithms such as those in @xcite considers @xmath94 where @xmath95 is a parameter that balances data fidelity versus volume minimization .",
    "the formulation in avoids dr and takes noise into consideration . however , our experience is that volume regularizers , such as @xmath96 , are numerically harder to cope with , which will be explained in detail later",
    ". we should also compare problem   with the volmin formulation in .",
    "problem   enforces a hard constraint @xmath97 , and thus ensures that every feasible @xmath98 and @xmath99 have full rank in the noiseless case . on the other hand , problem   employs a fitting - based criterion , and an overly large @xmath100 could result in rank - deficient factors even in the noiseless case .",
    "hence , @xmath100 should be chosen with caution .",
    "another notable difficulty is that outliers are very damaging to the volmin criterion . in many cases ,",
    "a single outlier can make the minimum - volume enclosing convex hull very different from the desired one ; see fig .  [",
    "fig : minvol ] for an illustration .",
    "the state - of - the - art algorithm that considers outliers for the volmin - based factorization is _ simplex identification via split augmented lagrangian _ ( sisal ) @xcite , but it takes care of outliers in the dimension - reduced domain . as already mentioned , the dr process itself may be impaired by outliers , and thus dealing with outliers in the original data domain is more appealing .",
    "directly factoring @xmath7 in the original data domain also has the advantage of allowing us to incorporate any _ a priori _ information on @xmath46 and @xmath47 , such as nonnegativity , smoothness , and sparsity .",
    "$ ] s ; the shaded area is @xmath101 , the triangles with dashed lines are data - enclosing convex hulls , and the one with solid lines is the minimum - volume enclosing convex hull .",
    "left : the case where no outliers exist .",
    "right : the case where a single outlier exists.,width=226 ]      we are interested in the volmin - regularized matrix factorization , but we take the outlier problem into consideration . specifically , we propose to employ the following optimization surrogate of the volmin criterion : @xmath102-{\\bm b}{\\bm c}[\\ell ] \\right\\|_2 ^ 2+\\epsilon\\right)^{\\frac{p}{2 } } + \\frac{\\lambda}{2 }   \\log\\det({\\bm b}^t{\\bm b}+\\tau{\\bm i } ) \\nonumber\\\\                                    { \\rm s.t.}&~{\\bm 1}^t{\\bm c}[\\ell]=1,~{\\bm c}[\\ell]\\geq{\\bm 0},~\\forall \\ell,\\ ] ] where @xmath103 $ ] , @xmath95 , @xmath104 , and @xmath105 . here",
    ", @xmath106 is a small regularization parameter , which keeps the first term inside its smooth region for computational convenience when @xmath107 ; if @xmath108 $ ] , we can simply let @xmath109 .",
    "the parameter @xmath110 is also a small positive number , which is used to ensure that the cost function is bounded from below for any @xmath111 .",
    "the motivation of using @xmath112 instead of the commonly used volume regularizers such as @xmath113 is computational simplicity : although both functions are non - convex and conceptually equally hard to deal with , the former features a much simpler update rule because it admits a tight upper bound while the latter does not  this point will become clearer shortly .",
    "interestingly , @xmath112 has been used in the context of low - rank matrix recovery @xcite , but here we instead apply it for simplex - volume minimization .",
    "the @xmath114-(quasi- ) norm data fitting part is employed to downweight the impact of the outliers  when @xmath115 , such a fitting criterion is less sensitive to large fitting errors and thus is robust against outliers .",
    "other robust fitting criteria can also be considered ",
    "e.g. , the @xmath14 norm - based criterion @xmath116 for @xmath117 where @xmath118 is known to be robust to entry - level outliers @xcite .",
    "nevertheless , the type of outliers that matters in volmin is column outliers ( or gross outliers ) which represents a point lying outside the ground - truth convex hull , and the proposed criterion is natural for fending against such outliers .",
    "in addition , computationally , the @xmath114 mixed - norm criterion can be handled efficiently , as we will see .",
    "our primary objective is to handle problem   efficiently .",
    "nonetheless , we will also show that the proposed algorithmic framework can easily incorporate different volume - associated regularizers in the literature , such as the previously mentioned @xmath119 , and @xmath120 see @xcite .",
    "notice that is a coarse approximation of the volume of @xmath121 , which measures the volume by simply adding up the squared distances between the vertices .",
    "our idea is to update @xmath98 and @xmath99 alternately , i.e. , using block coordinate descent ( bcd ) .",
    "unlike classic bcd @xcite , we solve the partial optimization problems in an _ inexact _ fashion for efficiency .",
    "we first consider updating @xmath99 . the problem w.r.t .",
    "@xmath99 is separable w.r.t . @xmath35 and convex .",
    "therefore , after @xmath122 iterations with the current solution @xmath123 , we consider : @xmath124:= \\arg\\min_{{\\bm c}[\\ell]}&\\quad\\frac{1}{2}\\left\\|{\\bm x}[\\ell]-{\\bm b}^{t}{\\bm c}[\\ell]\\right\\|_2 ^ 2\\\\                                       { \\rm s.t.}&\\quad { \\bm 1}^t{\\bm c}[\\ell]=1,~{\\bm c}[\\ell]\\geq { \\bm 0 } ,                                 \\end{aligned}\\ ] ] for @xmath125 . since problem   is convex , one can update @xmath99 by solving problem   to optimality .",
    "an _ alternating direction method of multipliers _",
    "( admm)-based algorithm was provided in the conference version of this work for this purpose ; see the detailed implementation in @xcite .",
    "nevertheless , exactly solving problem   at each iteration is computationally costly , especially when the problem size is large .",
    "here , we propose to deal with problem   using local approximation .",
    "specifically , let @xmath126;{\\bm b}^t)=\\frac{1}{2}\\|{\\bm x}[\\ell]-{\\bm b}^{t}{\\bm c}[\\ell]\\|_2 ^ 2.}\\ ] ] then , @xmath127;{\\bm b}^t)$ ] can be locally approximated at @xmath128 $ ] by the following : @xmath129;{\\bm b}^t ) & = f({\\bm c}^t[\\ell];{\\bm b}^t)+\\left(\\nabla f({\\bm c}^t[\\ell];{\\bm b}^t)\\right)^t({\\bm c}[\\ell]-{\\bm c}^t[\\ell])\\\\                     & \\quad\\quad\\quad+ \\frac{l^t}{2}\\|{\\bm c}[\\ell]-{\\bm c}^t[\\ell]\\|_2 ^ 2,\\end{aligned}\\ ] ] where @xmath130 . on the right hand side ( rhs ) of the above ,",
    "the first two terms constitute a first - order approximation of @xmath127;{\\bm b}^t)$ ] at @xmath128 $ ] , and the second term restrains @xmath131 $ ] to be close to @xmath128 $ ] in terms of euclidean distance .",
    "it is well - known that when @xmath132 , @xmath133;{\\bm b}^t ) \\geq",
    "f({\\bm c}[\\ell];{\\bm b}^t),~\\forall { \\bm c}[\\ell]\\in\\mathbb{r}^k}\\ ] ] holds for all @xmath134 $ ] and the equality holds if and only if @xmath134={\\bm c}^t[\\ell]$ ] @xcite .",
    "in other words , when @xmath132 , @xmath135)$ ] is a ` majorizing ' function of @xmath127;{\\bm b}^t)$ ] . given this majorizing function , we update @xmath134 $ ] by the following simple rule : @xmath136 =   \\arg\\min_{{{\\bm 1}^t{\\bm c}[\\ell]=1,~{\\bm c}[\\ell]\\geq { \\bm 0}}}u({\\bm c}[\\ell];{\\bm b}^t).}\\ ] ] by re - arranging the terms and discarding constants , problem   is equivalent to the following @xmath137=1,~{\\bm c}[\\ell]\\geq { \\bm 0 } } } ~\\left\\|{\\bm c}[\\ell ] -\\left({\\bm",
    "c}^t[\\ell]- \\frac{1}{l^t } \\nabla f({\\bm c}^t[\\ell];{\\bm b}^t)\\right)\\right\\|_2 ^ 2 .    \\end{aligned}\\ ] ] the rhs of the above can be considered as a gradient projection step with step size @xmath138 .",
    "letting @xmath139)$ ] denote the optimal solution of the above , we simplify the notation of updating @xmath134 $ ] as @xmath140 =   p_{l^t}({\\bm c}^t[\\ell]).\\ ] ] problem   is a simple projection that can be solved with worst - case complexity of @xmath141 flops ; see @xcite for a detailed implementation",
    ".    the described update of @xmath99 has light per - iteration complexity , but it could result in slow convergence of the overall alternating optimization algorithm ; see fig .",
    "[ fig : converge ] in the simulations . to improve the convergence speed in practice , and inspired by the success of nesterov s optimal first - order algorithm and its related algorithms @xcite , we propose the following update of @xmath99 :    [ eq : c_fista ] @xmath142&=p_{l^t_c}({\\bm y}^t[\\ell])\\\\                                                       q^{t+1}&=\\frac{1+\\sqrt{1 + 4(q^t)^2}}{2}\\\\                     { \\bm y}^{t}[\\ell ] & = { \\bm c}^t[\\ell]+\\left(\\frac{q^{t}-1}{q^{t+1}}\\right)\\left({\\bm c}^t[\\ell]-{\\bm c}^{t-1}[\\ell]\\right ) , \\label{eq : proof}\\end{aligned}\\ ] ]    where @xmath143 is a sequence with @xmath144 .",
    "simply speaking , instead of locally approximating @xmath127;{\\bm b}^t)$ ] at @xmath128 $ ] , we approximate it at an ` extrapolated point ' @xmath145 $ ] . without the alternating optimization procedure ,",
    "using extrapolation is provably much faster than using the plain gradient - based methods @xcite .",
    "embedding extrapolation into alternating optimization was first considered in @xcite in the context of tensor factorization , where acceleration of convergence was observed . in our case",
    ", the extrapolation procedure also substantially reduces the number of iterations for achieving convergence , as will be shown in the simulations .",
    "the update of @xmath98 relies on the following two lemmas :    [ lem : conjugate ] @xcite assume @xmath146 , @xmath147 , and let @xmath148 .",
    "then , we have @xmath149 also , the minimizer is unique and given by @xmath150 .",
    "[ lem : logdet ] @xcite let @xmath151 be any matrix such that @xmath152 .",
    "consider the function @xmath153 then , @xmath154 , and the minimizer is uniquely given by @xmath155 .",
    "the lemmas provide two functions that majorize the data fitting part and the volume - regularization part in , respectively .",
    "specifically , at iteration @xmath122 and after updating @xmath99 , we have @xmath156\\}_{\\ell=1}^l)$ ] .",
    "then , the following holds : @xmath157 where @xmath158 and the equality holds when @xmath159 .",
    "similarly , we have @xmath160-{\\bm b}{\\bm c}^{t+1}[\\ell ] \\right\\|_2 ^ 2+\\epsilon\\right)^{\\frac{p}{2 } } \\\\      & \\leq   \\sum_{\\ell=1}^l \\frac{w^t_\\ell}{2}\\left\\|{\\bm x}[\\ell]-{\\bm b}{\\bm c}^{t+1}[\\ell ] \\right\\|_2 ^ 2+\\sum_{\\ell=1}^l\\phi_p(w^t_\\ell ) , \\end{aligned}\\ ] ] where @xmath161\\|_2 ^ 2+\\epsilon)^{\\frac{p-2}{2}}$ ] and the equality holds when @xmath159 . putting - together and dropping the irrelevant terms , we find @xmath162 by solving the following : @xmath163-{\\bm b}{\\bm c}^{t+1}[\\ell ] \\right\\|_2 ^ 2 \\\\                   & \\quad\\quad\\quad\\quad+\\frac{\\lambda}{2 } { \\rm tr}({\\bm f}^t({\\bm b}^t{\\bm b } ) ) .",
    "\\end{aligned}\\ ] ] problem   is a convex quadratic program that admits the following closed - form solution : @xmath164 where @xmath165 .",
    "the expression in reveals why the proposed criterion and algorithm can automatically downweight the effect brought by the outliers .",
    "suppose that @xmath166 is a `` good enough '' solution which is close to the ground truth .",
    "then , @xmath167 is small when @xmath38 $ ] is an outlier since the fitting error term @xmath168-{\\bm b}^t{\\bm c}^{t+1}[\\ell]\\|_2 ^ 2 $ ] is large .",
    "hence , for the next iteration , @xmath162 is estimated with the importance of the outlier @xmath38 $ ] downweighted .    in practice , adding constraints on @xmath98 by letting @xmath169 is sometimes instrumental , since a lot of applications do have prior information that can be used to enhance performance .",
    "for example , in image processing , a nonnegative @xmath98 is often sought , and thus one can set @xmath170 .",
    "when @xmath171 is convex , the problem in can usually be solved in an efficient manner ; e.g. , one can call general - purpose solvers such as interior - point methods . however , using general - purpose solvers here may lose efficiency since solving constrained least squares to a certain accuracy _ per se _ may require a lot of iterations . to simplify the update ,",
    "we update @xmath172 following the same spirit of updating @xmath99 : let @xmath173-{\\bm b}{\\bm c}^{t+1}[\\ell ] \\right\\|_2 ^ 2 + \\frac{\\lambda}{2 } { \\rm tr}({\\bm f}^t({\\bm b}^t{\\bm b}))+{\\rm const}$ ] , where @xmath174 .",
    "we solve a local approximation of @xmath175 : @xmath176 where @xmath177 and @xmath178 is the partial derivative of the cost function in w.r.t .",
    "@xmath98 at @xmath172 , and @xmath179 denotes the euclidean projection of @xmath180 on @xmath171 . for some @xmath171 s ,",
    "the projection is easy to compute ; e.g. , when @xmath181 , we have @xmath182 see other easily implementable projections in @xcite .",
    "notice that the update in can also easily incorporate extrapolation .",
    "the robust volume minimization ( rvolmin ) algorithm is summarized in algorithm  [ algo : rvolmin - bsum ] .",
    "its convergence properties are stated in proposition  [ prop : convergence ] , whose proof is relegated to appendix  [ app : prop ] .",
    "[ prop : convergence ] assume that @xmath183 and @xmath184 are chosen such that @xmath185 and @xmath186 , respectively .",
    "also , assume that @xmath171 is a convex closed set .",
    "then , if the initial objective value is finite , the whole solution sequence generated by algorithm  [ algo : rvolmin - bsum ] converges to the set @xmath187 that consists of all the stationary points of problem  , i.e. , @xmath188 where @xmath189 .",
    "@xmath190 ;    @xmath191 ; @xmath192 ; @xmath193 ; @xmath194 ;    as mentioned before , we may also use different volume regularizers . let us consider the volume regularizer in first .",
    "it was shown in @xcite that this regularizer can also be expressed as @xmath195 , where @xmath196 .",
    "therefore , by letting @xmath197 in algorithm  [ algo : rvolmin - bsum ] , the updates can be directly applied to handle the regularizer in .",
    "dealing with is more difficult .",
    "one possible way is to make use of since @xmath113 is differentiable .",
    "the difficulty is that a global upper bound of the subproblem w.r.t . @xmath98",
    "may not exist . under such circumstances ,",
    "_ sufficient decrease _ at each iteration needs to be guaranteed for establishing convergence to a stationary point @xcite . in practice ,",
    "the _ armijo rule _ is usually invoked to achieve this goal , which in general is computationally more costly compared to the cases where @xmath184 can be determined in closed form .",
    "problem   is a nonconvex optimization problem .",
    "hence , a good starting point of rvolmin can help reach meaningful solutions quickly . in practice ,",
    "different initializations can be considered :    @xmath198  existing volmin algorithms . many volmin algorithms , such as the ones working in the reduced - dimension domain ( e.g. , the algorithms in @xcite ) , exhibit good efficiency .",
    "the difficulty is that these algorithms are usually sensitive to the dr process in the presence of outliers .",
    "nevertheless , one can employ robust dr algorithms together with the algorithms in @xcite as an initialization approach .",
    "nuclear norm - based algorithms @xcite are viable options for robust dr , but are not suitable for large - scale problems because of the computational complexity . under such circumstances",
    ", one may adopt simple alternatives such as that proposed in @xcite .",
    "@xmath198  nonnegative matrix factorization .",
    "if @xmath46 is known to be nonnegative , any nmf algorithm can be employed as initialization . in practice , dealing with nmf is arguably simpler relative to volmin , and many efficient solvers for nmf exist ",
    "see @xcite for a survey .",
    "although nmf usually does not provide a satisfactory result on its own in cases where it can not guarantee the identifiability of its factors , using the nmf - estimated factors to initialize the algorithms that provide identifiability guarantees can sometimes enhance the performance of the latter .",
    "in this section , we provide simulations to showcase the effectiveness of the proposed algorithm .",
    "we generate the elements of @xmath36 from the uniform distribution between zero and one .",
    "we generate @xmath199 $ ] on the unit simplex and with @xmath200 \\leq \\gamma$ ] , where @xmath201 is given .",
    "we choose @xmath202 , which results in a so - called ` no - pure - pixel case ' in the context of remote sensing and is known to be challenging to handle ; see @xcite for details .",
    "zero - mean white gaussian noise is added to the generated data . to model outliers",
    ", we define the outlier at data point @xmath35 as @xmath203 $ ] and let @xmath204 be the index set of outliers .",
    "we assume that @xmath203={\\bm 0}$ ] if @xmath205 and @xmath38={\\bm o}[\\ell]$ ] otherwise .",
    "we denote @xmath206 as the total number of outliers .",
    "those active outliers are generated following the uniform distribution between zero and one , and are scaled to satisfy problem specifications . for the proposed algorithm , we fix @xmath207 , @xmath208 , and @xmath209 unless otherwise specified .",
    "we stop the proposed algorithm when the absolute change of the cost function is smaller than @xmath210 or the number of iterations reaches 1000 .",
    "we define the signal - to - noise ratio ( snr ) as @xmath211\\|_2 ^ 2\\}}{\\mathbb{e}\\{\\|{\\bm v}[\\ell]\\|_2 ^ 2\\}}\\right)$ ] .",
    "also , to quantify the corruption caused by the outliers , we define the signal - to - outlier ratio ( sor ) as @xmath212\\|_2 ^ 2\\}}{\\mathbb{e}\\{\\|{\\bm o}[\\ell]\\|_2 ^ 2\\}}\\right)$ ] .",
    "we use the mean - squared - error ( mse ) of @xmath46 as a measure of factorization performance , defined as @xmath213 where @xmath214 is the set of all permutations of @xmath215 ; and @xmath216 is the estimate of @xmath52 .",
    "in this section , we use the sisal algorithm proposed in @xcite as a baseline .",
    "sisal is a state - of - art robust volmin algorithm that takes outliers into account by solving @xmath217=\\tilde{\\bm b}{\\bm c}[\\ell]\\}}~\\log\\det(\\tilde{\\bm b})+\\eta\\|{\\bm c}\\|_h,\\ ] ] where @xmath218,0)$ ] is an element - wise hinge function",
    ". the intuition behind sisal is to penalize the outliers whose @xmath134 $ ] has negative elements , but still allowing them to exist , thereby having some robustness to outliers .",
    "the tuning parameter @xmath219 in sisal controls the amount of outliers that are `` allowed '' , and we test multiple @xmath220 s for sisal in the simulations .",
    "we run the original sisal that uses svd - based dimension reduction and the modified sisal which uses the robust dimension reduction ( rdr ) algorithm in @xcite .",
    "the latter is also used to initialize the proposed algorithm .",
    "we first use an illustrative example to show the effectiveness of the proposed algorithm in the presence of outliers . in this example , we set snr@xmath221db , sor@xmath222db , @xmath223 , @xmath224 , and @xmath225 .",
    "the results are projected onto the affine set that contains @xmath226 , i.e. , a two - dimensional hyperplane . in fig .",
    "[ fig : toy ] , we see that sisal with different @xmath220 s can not yield reasonable estimates of @xmath46 since the dr stage threw the data to a badly estimated subspace .",
    "using rdr , sisal performs better , but is still not satisfactory . in this case",
    ", the proposed algorithm yields the most accurate estimate of @xmath46 .",
    "s estimated by various algorithms .",
    "blue points are @xmath38$]s.,width=298 ]    in fig .",
    "[ fig : converge ] , we show the convergence curves of the algorithm under different update rules of @xmath99 , i.e. , admm in @xcite , the proposed local approximation , and local approximation with extrapolation .",
    "we show the results averaged from 10 trials , where snr@xmath221db and sor@xmath227db .",
    "we see that using admm , the objective value converges within 400 iterations .",
    "local approximation with extrapolation uses around 800 iterations to attain convergence of the objective value , but the objective value can not converge within 3000 iterations without extrapolation . in terms of runtimes ,",
    "the local approximation methods uses 0.003 second per iteration ( a complete update of both @xmath99 and @xmath98 ) , while admm costs 0.05 second per iteration .",
    "obviously , local approximation with extrapolation is the most favorable update scheme : its number of iterations for achieving convergence is around twice of that of admm , but it is 15 times faster relative to admm for completing an update of @xmath99 .",
    "specifically , in the case under test , the average time for the algorithm using admm to update @xmath99 to achieve the pointed objective value in fig .",
    "[ fig : converge ] is 20.5 seconds , while using local approximation with extrapolation costs 2.58 seconds to reach the same objective level . in the upcoming simulations ,",
    "all the results of the proposed algorithms are obtained with the extrapolation strategy .",
    ".,width=298 ]    in fig .",
    "[ fig : snrvary ] , we show the mse performance of the proposed algorithm versus snr .",
    "we fix sor@xmath227db , and let @xmath223 , @xmath228 , and @xmath225 . in fig .",
    "[ fig : snrvary ] , we see that the original sisal fails for different @xmath220 s . using rdr , sisal with @xmath229 yields reasonable results for all the tested snrs .",
    "the proposed algorithm with @xmath230 and @xmath231 gives the lowest mses .",
    "the mses given by rvolmin with @xmath230 are the lowest when snr@xmath232db , and rvolmin with @xmath231 exhibits the best mse performance when snr@xmath233db .",
    "the results are consistent with the intuition behind selecting @xmath100 : when the snr is low , a relatively large @xmath100 is needed to enhance the effect of the volume - minimization regularization .    to understand the effect of selecting @xmath100 , we plot the mses of the proposed algorithm versus @xmath100 in fig .  [ fig : lambda ] .",
    "we see that there exists an ( snr - dependent ) optimal choice of @xmath100 for achieving the lowest mse , but also note that any @xmath100 in the range considered yields satisfactory results in both cases .",
    "obtained by different algorithms vs. snr .",
    "@xmath228 ; @xmath223 ; sor@xmath227db.,width=226 ]    .",
    "@xmath228 ; @xmath223 ; @xmath225 ; sor@xmath227db.,width=226 ]    fig .",
    "[ fig : kvary ] shows the mse performance of the algorithms versus @xmath49 .",
    "we fix snr@xmath234db and the other settings are the same as in the previous simulation .",
    "the results of sisal and sisal with rdr are also used as baselines .",
    "we run several @xmath220 s for sisal and present the results of the one with the lowest mses .",
    "as expected , all the algorithms work better when the rank of the factorization model is lower  which is consistent with past experience on different matrix factorization algorithms , such as @xcite .",
    "sisal and sisal with rdr work reasonably when @xmath235 , but deteriorate when @xmath236 . on the other hand , even when @xmath237 , the proposed algorithm still works well , giving the lowest mse .",
    "[ fig : sorvary ] shows the mses of the algorithms versus sors .",
    "one can see that when some data are badly corrupted , i.e. , when sor@xmath238db , the proposed algorithm yields significantly lower mses than sisal and sisal with rdr .",
    "when sor@xmath239db , all three algorithms provide comparable performance .",
    "we also test the algorithms versus the number of outliers . in fig .",
    "[ fig : novary ] , one can see that the proposed algorithm is not very sensitive to the change of @xmath240 : the mse curve of the proposed algorithm is quite flat for different @xmath240 s in this simulation .",
    "sisal with rdr yields reasonable mses when @xmath241 , but its performance deteriorates when @xmath240 is larger .     vs. @xmath49 .",
    "@xmath242 ; @xmath223 ; @xmath225 ; sor@xmath227db.,width=226 ]     vs. sor .",
    "@xmath228 ; @xmath223 ; @xmath225 ; snr@xmath234db.,width=226 ]     vs. @xmath240 .",
    "@xmath228 ; @xmath225 ; sor@xmath227db ; snr@xmath234db.,width=226 ]     [ tab : ill ]     table  [ tab : ill ] presents the mses of the estimated @xmath243 under well- and ill - conditioned @xmath46 s , respectively . to generate an ill - conditioned @xmath46",
    ", we use a way that is similar to the method suggested in @xcite : in each trial , we first generate @xmath244 whose columns are uniformly distributed between zero and one , and such @xmath244 s are relatively well - conditioned .",
    "then , we apply singular value decomposition to obtain @xmath245 .",
    "finally , we replace @xmath246 by @xmath247)$ ] and obtain @xmath248 .",
    "this way , the condition number of the generated @xmath46 is @xmath249 .",
    "the other settings are the same as those in fig .",
    "[ fig : snrvary ] .",
    "one can see from table  [ tab : ill ] that using such ill - conditioned @xmath46 , all the algorithms perform worse compared to the scenario where @xmath46 has uniformly distributed columns ( cf .",
    "the first and second columns in table  [ tab : ill ] ) .",
    "nevertheless , the proposed algorithm still gives the lowest mses .",
    "in table  [ tab : vol ] , we present the mse performance of the proposed algorithm using different volume regularizers . we see that using @xmath250 has the shortest runtime since the subproblem w.r.t .",
    "@xmath98 is convex and can be solved in closed form .",
    "when @xmath119 , the algorithm requires much more time compared to that of the other two regularizers .",
    "this is because the armijo rule has to be implemented at each iteration . in terms of accuracy , using the @xmath251 regularizer gives the lowest mses when sor@xmath252db . using @xmath113 also exhibits good mse performance when sor@xmath239db . using",
    "@xmath253 performs slightly worse in terms of mse , since it is a coarse approximation to simplex volume .",
    "interestingly , although our proposed log - determinant regularizer is not an exact measure of simplex volume as the determinant regularizer , it yields lower mses relative to the latter .",
    "our understanding is that the performance gain results from the ease of computation .",
    "table  [ tab : nn ] presents the mse of the proposed algorithm with and without nonnegativity constraint on @xmath98 , respectively .",
    "we see that the mses are similar , with those of the nonnegativity - constrained algorithm being slightly lower .",
    "this result validates the soundness of our update rule for the constrained case , i.e. , . in terms of speed ,",
    "the unconstrained algorithm requires less time .",
    "we note that the nonnegativity constraint seems to only bring marginal performance gain in this simulation .",
    "this might be because the data are generated following the model in and , and under this model volmin identifiability does not depend on the nonnegativity of @xmath111 .",
    "however , when we are dealing with real data , adding nonnegativity constraints makes much sense , as will be shown in the next section .",
    "[ tab : vol ]    [ tab : nn ]    fig .",
    "[ fig : pvary ] shows the effect of changing @xmath254 .",
    "when sor@xmath255db , we see that using @xmath256 $ ] gives relatively low mses .",
    "this is because using a small @xmath254 is more effective in fending against outliers that largely deviate from the nominal model .",
    "it is interesting to note that using @xmath257 gives slightly worse result compared to using @xmath256 $ ] .",
    "our understanding is that using a very small @xmath254 may lead to numerical problems , since the weights @xmath258 can be scaled in a very unbalanced way in such cases , resulting in ill - conditioned optimization subproblems . for the cases where sor@xmath227db and @xmath259db",
    ", a similar effect can be seen .",
    "in addition , a larger range of @xmath254 , i.e. , @xmath260 $ ] , can result in good performance when sor@xmath261db .",
    "the results suggest a strategy of choosing @xmath254 : when the data is believed to be badly corrupt , using @xmath254 around @xmath262 is a good choice ; and when the data is only moderately corrupted , using @xmath263 $ ] is preferable , since such a @xmath254 gives good performance and can better avoid numerical problems .",
    "s under various sors .",
    "@xmath228 ; @xmath225 ; @xmath223 ; snr@xmath234db.,width=226 ]",
    "in this section , we validate the proposed algorithm using two real data sets , i.e. , a hyperspectral image dataset with known outliers and a document dataset .",
    "hyperspectral unmixing ( hu ) is the application where volmin - based factorization is most frequently applied ; see @xcite . as introduced before , hu aims at estimating @xmath46 , i.e. , the spectral signatures of the materials that are contained in a hyperspectral image , and also their proportions @xmath37 $ ] in each pixel .",
    "it is well - known that there are outliers in hyperspectral images , due to the complicated reflection environment , spectral band contamination , and many other reasons @xcite . in this experiment",
    ", we apply the proposed algorithm to a subimage of the real hyperspectral image that was captured over the moffett field in 1997 by the airborne visible / infrared imaging spectrometer ( aviris ) ; see fig .",
    "[ fig : moffet ] .",
    "we remove the water absorption bands from the original 224 spectral bands , resulting in @xmath264 bands for each pixel @xmath38 $ ] . in this subimage with @xmath265 pixels , there are three types of materials  water , soil , and vegetation . in the areas where different materials intersect , e.g. , the lake shore , there are many outliers as identified by domain study .",
    "our goal here is to test whether our algorithm can identify the three materials and the outliers simultaneously .",
    "we apply sisal , sisal with rdr , and the proposed algorithm to estimate @xmath46 .",
    "we set @xmath230 for our algorithm and tune @xmath220 for sisal carefully .",
    "notice that we let @xmath266 in this case , since the spectral signatures are known to be nonnegative .",
    "the estimated spectral signatures are shown in fig .",
    "[ fig : signs ] . as a benchmark",
    ", we also present the spectra of some manually selected pixels , which are considered purely contributed by only one material , and thus can approximately serve as the ground truth .",
    "we see that both sisal and sisal with rdr does not yield accurate estimates of @xmath46 .",
    "particularly , the spectrum of water is badly estimated by both of these algorithms  one can see that there are many negative values of the spectra of water given by sisal and sisal with rdr . on the other hand ,",
    "the proposed algorithm with nonnegativity constraint on @xmath98 gives spectra that are very similar to those of the pure pixels .",
    "[ fig : maps ] shows the spatial distributions of the materials ( i.e. , the abundance maps @xmath267\\}_{\\ell=1}^l$ ] for @xmath268 ) that are estimated by the proposed algorithm in the first three subimages .",
    "we see that the three materials are well separated , and their abundance maps are consistent with previous domain studies @xcite . in the last subimage of fig .",
    "[ fig : maps ] , we plot @xmath269 for @xmath125 .",
    "notice that the weight @xmath270 corresponds to the ` importance ' of the data @xmath38 $ ] .",
    "the algorithm is designed to automatically give small @xmath270 to outlying pixels .",
    "we see that there are a lot of pixels on the lake shore that have very small weights , indicating that they are outliers . physically , these outliers correspond to those areas where the solar light reflects several times between water and soil , resulting in nonlinearly mixed spectral signatures @xcite .",
    "the locations of the outlying pixels identified by our algorithm are also consistent with domain study @xcite",
    ".                  we also present experimental results using the reuters21578 document corpus .",
    "we use the subset of the full corpus provided by @xcite , which contains 8,213 single - labeled documents from 41 clusters . in our experiment",
    ", we test our algorithm under different @xmath49 ( number of clusters ) , from 3 to 10 . following standard pre - processing ,",
    "each document is represented as a term - frequency - inverse - document - frequency ( tf - idf ) vector , and _ normalized cut weighting _ is applied ; see @xcite for details .",
    "we apply our volmin algorithm to factor the document data @xmath7 to ` topics ' @xmath46 and a ` weight matrix ' @xmath47 ( cf .",
    "[ fig : motivation_rvolmin ] ) , and use @xmath47 to indicate the cluster labels of the documents . a regularized nmf - based approach , namely , _ locally consistent concept factorization _",
    "( lccf ) @xcite is employed as the baseline , which is considered a state - of - the - art algorithm for clustering the reuters21578 corpus . for each @xmath49 , we perform 100 monte - carlo trials by randomly selecting @xmath49 clusters out of the total 41 clusters and 100 documents from each cluster .",
    "we report the performance by comparing the results with the ground truth .",
    "performance is measured by a commonly used metric called _ clustering accuracy _ , whose detailed definition can be found in @xcite  the clustering accuracy ranges from 0 to 1 , and higher accuracies indicate better performances .    table  [ tab : doc ] presents the results averaged from the 100 trials . for the proposed algorithm , we set @xmath271 and @xmath272 ; we also present the result of @xmath273 in this experiment , which we also use to initialize the @xmath274 case .",
    "note that here we use a larger @xmath100 relative to what was used in the simulations .",
    "the reason is that the document corpus contains considerable modeling errors and the fitting residue is relatively large .",
    "the rule of thumb for selecting @xmath100 is to set it at a similar level as the fitting error part ; i.e. , we let @xmath275 , where @xmath276 and can be coarsely estimated using plain nmf . this way",
    ", @xmath100 can balance the fitting part and the volume regularizer . from table  [ tab : doc ] , we see that volmin with @xmath273 already yields comparable clustering accuracy with lccf .",
    "this indicates that , even without outlier - robustness , modeling the document clustering problem using volmin - based smf is effective .",
    "better accuracies can be seen by using @xmath272 , where we see that for most @xmath49 , the proposed rvolmin algorithm gives the best accuracy .",
    "in particular , for @xmath277 , more than @xmath278 accuracy improvement can be seen , which is considered significant in the context of document clustering .",
    "interestingly , further decreasing @xmath254 does not yield better performance .",
    "this implies that the modeling error is not very severe , but outliers do exist , since using @xmath274 gives better clustering result than using @xmath273 which is not robust to modeling errors .",
    "in this work , we looked into theoretical and practical aspects of the volume minimization criterion for matrix factorization . on the theory side",
    ", we showed that two independently developed sufficient conditions for volmin identifiability are in fact equivalent . on the practical side , we proposed an outlier - robust optimization surrogate of the volmin criterion , and devised an inexact bcd algorithm to deal with it .",
    "extensive simulations showed that the proposed algorithm outperforms a state - of - the - art robust volmin algorithm , i.e. , sisal .",
    "the proposed algorithm was also validated using real - world hyperspectral image data and document data , where interesting and favorable results were observed .",
    "the authors would like to thank prof .",
    "nicolas dobigeon for providing the subimage of the moffet data .",
    ".the clustering accuracy on reuters 21578 corpus . [ cols=\"^,^,^,^,^ \" , ]     [ tab : doc ]",
    "_ proof _ : we first show the `` @xmath287 '' part . given a unitary @xmath72 , suppose that @xmath288 by the basic properties of convex cones , we see that @xmath289 , and @xmath290 .",
    "combining , we see @xmath291 also , we have @xmath292 combining eq .   and",
    ", we have @xmath293 we also know that the extreme rays of @xmath294 lie in the boundary of @xmath295 , i.e. , @xmath296 ( * ? ? ?",
    "* lemma  1 ) .",
    "thus , we have @xmath297 since we assumed @xmath285 , we have @xmath298 therefore , @xmath72 can only be a permutation matrix .",
    "we now show the `` @xmath299 '' part .",
    "following , and knowing that @xmath75 is a subset of the convex cone of some permutation matrix , we see that @xmath300 now , suppose that there are a set of vectors @xmath301 that does not include any unit vectors such that @xmath302 then , we see that we can represent @xmath303 . by property  [ pro : interseccone ] , we see that @xmath304 since @xmath305 , leads to @xmath306 by property  [ pro : dualcone ] , we see that @xmath307 this is a contradiction to the assumption that @xmath301 does not include the unit vectors .",
    "@xmath308    now we are ready to prove theorem  [ thm : equivalence ] .",
    "we first notice that @xmath70 is equivalent to @xmath309 .",
    "in fact , we see that @xmath310 , where @xmath311 , and thus the claim holds . thus , our remaining work is to show that condition ( ii ) in theorem  [ thm : fumahuasid ] is equivalent to restricting @xmath312 such that @xmath78    step 1 ) : let us consider a conic representation of theorem  [ thm : linma ] . specifically , the corresponding convex cone of @xmath313 is @xmath314 , where @xmath315 and @xmath316 under this definition .",
    "it is also noticed that @xmath317 can be re - expressed as @xmath318 in words , the vectors whose angles between @xmath21 are less than or equal to @xmath319 comprises @xmath317 .",
    "therefore , by the definition of dual cone , i.e. , @xmath320 , @xmath321 contains all the vectors that have the angle with @xmath21 less than or equal to @xmath322 , which leads to @xmath323        where we have used @xmath326 , i.e. , property  [ pro : unitcone ] . according to property  [ pro : dualcone ]",
    ", we see that @xmath327 if we define @xmath328 , we see from and the definition of @xmath312 that @xmath329    step 3 ) : to show the equivalence between the sufficient conditions , let us begin from theorem  [ thm : fumahuasid ] .",
    "the condition @xmath70 means that @xmath330 by property  , and it further implies that @xmath331 @xcite .",
    "suppose the rest of @xmath332 s extreme rays are @xmath333 , and @xmath122 is defined as @xmath334 then , we have @xmath335 hence , by the definitions of @xmath122 and @xmath336 , we have @xmath337 .",
    "given the above analysis , we first show that theorem  [ thm : fumahuasid ] implies theorem  [ thm : linma ] .",
    "now , assume that condition ( ii ) in theorem  [ thm : fumahuasid ] is satisfied .",
    "we see that @xmath285 by lemma  [ lem : conditions ] .",
    "then , @xmath333 are in the interior of @xmath338 .",
    "therefore , we have @xmath339 , and subsequently @xmath340 and @xmath341 strictly .",
    "now we show the converse by contradiction .",
    "assume that @xmath342 holds ( the condition in theorem  [ thm : linma ] is satisfied ) .",
    "if at least one point in @xmath333 touches the boundary of @xmath295 ( condition ( ii ) in theorem  [ thm : fumahuasid ] is not satisfied ) , then @xmath343 , and we can not decrease it further while still contain @xmath332 in @xmath344 .",
    "this means that @xmath345 , or , equivalently , @xmath346 , which contradicts our first assumption that @xmath342 .      in the following ,",
    "we prove the proposition under the algorithmic structure without extrapolation .",
    "for the case where we update @xmath99 using , it is easy to see that @xmath347\\rightarrow{\\bm c}^t[\\ell]$ ] given @xmath348 by . hence , if the proposition holds for the algorithm without extrapolation , it also holds for the extrapolated version asymptotically .",
    "first , let us cast the proposed algorithm into the framework of _ block successive upper bound minimization _ ( bsum ) @xcite .",
    "unlike the classic block coordinate descent algorithm that solves every block subproblem exactly @xcite , bsum cyclically solves the upper - bound problems of every block subproblems .",
    "we consider the updates using and as an example .",
    "the proof of using other updates will follow .",
    "our update rule in and can be equivalently written as      where @xmath350;{\\bm b}^{t})+\\epsilon)^{p/2}+\\frac{\\lambda}{2}\\log\\det(({\\bm b}^t)^t{\\bm b}^t+\\tau{\\bm i})$ ] , @xmath135;{\\bm b}^{t})$ ] is defined as before , and @xmath351-{\\bm b}{\\bm c}^{t+1}[\\ell ] \\right\\|_2 ^ 2\\\\ & + \\frac{\\lambda}{2 } { \\rm tr}({\\bm f}^t({\\bm b}^t{\\bm b}))+{\\rm const},\\end{aligned}\\ ] ] in which @xmath174 .",
    "note that solving is equivalent to solving over different @xmath35 s since the problems w.r.t .",
    "@xmath125 are not coupled .",
    "also denote @xmath352 as the objective value of problem  . when @xmath185 , we have      where holds because under @xmath185 we have @xmath354;{\\bm b}^t)=\\frac{1}{2}\\|{\\bm x}[\\ell]-{\\bm b}^t{\\bm c}[\\ell]\\|_2 ^ 2\\leq u({\\bm c}[\\ell];{\\bm b}^t),~\\forall { \\bm c}[\\ell],\\ ] ] and thus @xmath355;{\\bm b}^t)+\\epsilon \\right)^{\\frac{p}{2}}\\\\                        & \\quad+\\frac{\\lambda}{2}\\log\\det(({\\bm b}^t)^t{\\bm b}^t+\\tau{\\bm i } )      \\\\                            & \\leq \\sum_{\\ell=1}^l \\frac{1}{2}(2u({\\bm",
    "c}[\\ell];{\\bm b}^{t})+\\epsilon)^{\\frac{p}{2}}\\\\                            & \\quad+\\frac{\\lambda}{2}\\log\\det(({\\bm b}^t)^t{\\bm b}^t+\\tau{\\bm i})\\\\                            & = u_c({\\bm c};{\\bm b}^{t});\\end{aligned}\\ ] ] eq .   holds because of lemmas  [ lem : conjugate ] and [ lem : logdet ] ; also note that the equalities hold when @xmath356 and @xmath159 , respectively .",
    "since all the functions above are continuously differentiable , we also have      note that if we update @xmath99 using admm , we have @xmath358 for all @xmath99  eqs .   and",
    "still hold . also , if we update @xmath98 by , the conditions in and are also satisfied when @xmath359 since we now we have @xmath360 and it can be shown that @xmath361 and @xmath362and the equalities hold simultaneously at @xmath363 .",
    "eqs  - satisfy the sufficient conditions for a generic bsum algorithm to converge ( cf .",
    "assumption 2 in @xcite ) .",
    "in addition , by ( * ? ? ? * theorem  2 ( b ) ) , if we can show that @xmath123 lives in a compact set for all @xmath122 , we can prove proposition  [ prop : convergence ] .",
    "next , we show that in every iteration , @xmath172 and @xmath364 are bounded .",
    "the boundness of @xmath364 is evident because we enforce feasibility at each iteration . to show that @xmath172 is bounded , we first note that @xmath365 where the inequality holds since the non - increasing property of the bsum framework @xcite . by the assumption that @xmath366 is bounded , i.e. , @xmath367 where @xmath368 , we have @xmath369-{\\bm b}{\\bm c}[\\ell ] \\right\\|_2",
    "^ 2+\\epsilon\\right)^{\\frac{p}{2 } } + \\frac{\\lambda}{2 }   \\log\\det({\\bm b}^t{\\bm b}+\\tau{\\bm i})\\leq v\\ ] ] holds for every @xmath370 that is generated by the algorithm .",
    "since the first term on the left hand side of the above inequality is nonnegative , we have      where @xmath372 denote the singular values of @xmath98 , and holds since @xmath373 for all @xmath374 . the right hand side of is bounded , which implies that every singular value of @xmath172 for @xmath375 is bounded .",
    "since @xmath171 is a closed convex set , we conclude that the sequence @xmath376 lies in a compact set .",
    "now , invoking ( * ? ? ?",
    "* theorem  2 ( b ) ) , the proof is completed ."
  ],
  "abstract_text": [
    "<S> this paper considers _ volume minimization _ ( volmin)-based structured matrix factorization ( smf ) . </S>",
    "<S> volmin is a factorization criterion that decomposes a given data matrix into a basis matrix times a structured coefficient matrix via finding the minimum - volume simplex that encloses all the columns of the data matrix . </S>",
    "<S> recent work showed that volmin guarantees the identifiability of the factor matrices under mild conditions that are realistic in a wide variety of applications . </S>",
    "<S> this paper focuses on both theoretical and practical aspects of volmin . on the theory side , exact equivalence of two independently developed sufficient conditions for volmin identifiability </S>",
    "<S> is proven here , thereby providing a more comprehensive understanding of this aspect of volmin . on the algorithm side , computational complexity and sensitivity to outliers </S>",
    "<S> are two key challenges associated with real - world applications of volmin . </S>",
    "<S> these are addressed here via a new volmin algorithm that handles volume regularization in a computationally simple way , and automatically detects and iteratively downweights outliers , simultaneously . </S>",
    "<S> simulations and real - data experiments using a remotely sensed hyperspectral image and the reuters document corpus are employed to showcase the effectiveness of the proposed algorithm . </S>"
  ]
}