{
  "article_text": [
    "kernel - based methods are well - established tools for supervised learning , allowing to perform various tasks , such as regression or binary classification , with linear and non - linear predictors . like most statistical models ,",
    "kernel - based methods can be considered within two frameworks : in the frequentist approach , estimators are obtained by minimizing a regularized empirical risk , leading e.g. to kernel ridge regression or the support vector machine @xcite ; in the bayesian approach , gaussian processes ( gps ) provide a bayesian interpretation to kernel - based methods  @xcite , with the potential to learn the kernel parameters from the data without having to use cross - validation .",
    "crucial to the predictive performance of kernel methods is the choice of the kernel function . in the bayesian",
    "setting , the kernel function ( often called covariance function ) determines the correlations between the predictions we make . assuming that the predictor s smoothness is fully specified by these correlations can be formalised by a gaussian process imposed over function space .",
    "techniques based on automatic relevance determination have been successful at learning the parameters of kernel functions such as the individual length scales of the squarred exponential kernel  @xcite . in the frequentist",
    "setting , a specific parameterization of kernel functions has led to a significant amount of work , namely positive linear combination of pre - defined kernel functions ( or kernel matrices ) , leading to the multiple kernel learning ( mkl ) framework  @xcite .",
    "the first contribution of this paper is to propose a gaussian process ( gp ) formulation of the multiple kernel learning framework , which we refer to as _ multiple gaussian process _ ( mgp ) models .",
    "its second contribution is to provide a framework to consider all @xmath0-norms at once and to determine _ from data _ whether we should use a sparsity - inducing prior or not .",
    "currently , there is no consensus in the frequentist community on how to choose the type of regularization . in practice",
    ", however , the choice of the regulariser leads to solutions of very different kinds .",
    "for example , when considering an @xmath1-norm a sparse solution will be obtained whether or not it is supported by the data .",
    "obviously , if all kernels are important for prediction , this will be detrimental  @xcite .",
    "let @xmath2 be the set of noisy targets and @xmath3 the set of features , which are assumed to be non - random column vectors .",
    "we consider a weighted linear model of the @xmath4 feature vectors with i.i.d .",
    "gaussian noise : @xmath5 where @xmath6 and @xmath7 is the identity matrix of dimension @xmath8 .",
    "the weights associated to the feature matrix @xmath9 are denoted by @xmath10 and the residual precision by @xmath11 .",
    "the case of interest is the one where the weight vectors are sparse , i.e. , many of their elements are ( nearly ) zero .",
    "however , we do not know a priori the degree of sparsity . from a bayesian perspective",
    ", the spike and slab prior is the golden standard for inducing sparsity . here ,",
    "follow a different approach and choose the prior @xmath12 to be a gaussian scale mixture  @xcite centred at zero . in effect",
    "we approximate the spike and slab prior by a continuous prior which favours sparse solutions .",
    "although zero probability mass is put on exact zero values , the use of heavy - tailed priors allows us to infer large , as well as quasi zero values for the kernel weights .",
    "formally , we impose the following product of zero - mean gaussian scale mixtures on the weights : @xmath13 where @xmath14 is the vector of unobserved scale variables on which independent generalised inverse gaussian densities ( see appendix  [ sect : inverse gaussian ] ) are imposed .",
    "the marginal @xmath15 is then a symmetric generalised hyperbolic density @xcite , which has fat tails compared to the gaussian .",
    "this family contains the student-@xmath16 , the laplace , the gamma - variance and jeffrey s as special cases .",
    "given this probabilistic model , one can integrate out @xmath17 , leading to a closed form expression for the marginal density of the observations : @xmath18 where @xmath19 is the kernel matrix associated with the @xmath20-th feature matrix . clearly , the marginal density is well - defined for any set of valid kernel matrices since @xmath21 for all @xmath20 .",
    "the linear additive model defined in ( [ eq : like lin ] ) corresponds to the _ weight - space _ view representation of the multiple gaussian process model ( mgp ) .",
    "from ( [ eq : marginal likelihood ] ) , however , we see that the marginal density only depends on the linear kernel matrices @xmath22 .",
    "hence , the model can be generalised to a non - linear additive model by replacing these linear kernel matrices by non - linear ones .",
    "this new representation corresponds to the multiple _ function - space _ view representation .",
    "let @xmath23 be a set of @xmath4 latent functions on which we impose scaled gaussian process priors : @xmath24 for all @xmath20 .",
    "the functions @xmath25 are covariance functions , which are also valid kernel functions @xcite .",
    "again we consider i.i.d .",
    "gaussian noise , but assume @xmath2 are noisy observations of a sum of @xmath4 latent _ function _ values @xmath26 .",
    "the likelihood function and the mgp prior are given by @xmath27 where @xmath28 , @xmath29 and @xmath30 .",
    "vector @xmath31 is the unit vector of dimension @xmath4 and the operator @xmath32 denotes the kronecker product .",
    "the prior on @xmath33 is still given by ( [ eq : gam prior ] ) and the marginal @xmath34 has the same form as in the weight - space view representation .",
    "the mgp model corresponds to imposing @xmath4 independent _ non - gaussian _ process priors over function space .",
    "if we condition on the corresponding scale variable , any finite subset of latent function values is distributed according to a multivariate gaussian marginal . for any of these marginals",
    "one can integrate out the scale variable , such that any finite set of latent function values is distributed according to a product of @xmath4 independent multivariate gaussian scale mixture densities : @xmath35 where @xmath36 is the modified bessel function of the second kind .",
    "hence , the prior measure imposed over function space is a heavy - tailed one , known as the generalised hyperbolic measure  @xcite .",
    "the gaussian process is recovered for @xmath37 and the symmetric multivariate zero - mean hyperbolic process is obtained for @xmath38 .",
    "other special cases include the multivariate gamma - variance process ( @xmath39 and @xmath40 ) , the multivariate laplace process ( @xmath38 and @xmath40 ) , the multivariate student-@xmath16 process ( @xmath41 and @xmath42 ) and the multivariate cauchy process ( @xmath43 and @xmath42 ) .",
    "the most straightforward approach for the estimation of @xmath33 is to use type ii maximum a postriori ( or type ii maximum likelihood in absence of prior on @xmath33 as adopted in @xcite ) .",
    "the optimisation can be performed using standard nonlinear optimisation tools , but the regulariser needs to be chosen in advance .",
    "instead , we turn our attention to the inference problem of these parameters from data .",
    "we view @xmath33 as a latent variable and the desired level of sparsity is learnt from the data by optimising the hyperparameters by type ii maximum likelihood ( ml ) .",
    "we follow a mean field approach @xcite . in order to find an analytically tractable solution ,",
    "the posterior over the latent function values @xmath44 and the scale vector @xmath33 is assumed to factorise given the data , that is @xmath45 .",
    "it can be shown that the variational posteriors maximising the negative variational free energy ( a lower bound to the log - marginal likelihood ) are given by @xmath46 and @xmath47 .",
    "the parameters are defined as @xmath48 where @xmath49 .",
    "the predictive mgp is obtained by assuming that @xmath50 is peaked around its mean such that @xmath51 .",
    "the true predictive density can then be approximated by the following analytically tractable integral : @xmath52 where @xmath53 with @xmath54 . from these expression",
    "we see that the posterior mean and variance have the same form as in standard gp regression ; the kernel is simply replaced by a convex combination of kernels .",
    "note , moreover , that the expression @xmath55 has the same form as the one we would obtain with a frequentist method such as kernel ridge regression .",
    "the ml ii updates for the hyperparameters are obtained by solving the following expressions ( which are simple root finding equations , with unique solutions , hence easily solved by binary search ) : @xmath56 where @xmath57 .",
    "these updates are obtained by direct maximising of the variational bound .",
    "the update for @xmath11 is obtained in the same manner .",
    "we restrict ourselves to binary classification and consider a scaled probit model in which the likelihood is derived from the gaussian cumulative density .",
    "a probit model is equivalent to a gaussian noise and a step function likelihood @xcite .",
    "let @xmath58 be the class labels , with @xmath59 for all @xmath60 .",
    "the likelihood ( [ eq : likelihood ] ) is replaced by @xmath61 where @xmath62 for @xmath63 and @xmath64 otherwise .    as in the case of regression",
    ", we consider a mean field approximation and assumes the posterior is of the form @xmath65 .",
    "we further assume the variational posterior @xmath66 is a product of truncated gaussians ( see appendix  [ sect : truncated gaussian ] ) : @xmath67 where @xmath68 and @xmath69 . the posterior mean and the posterior covariance of @xmath44 are unchanged , except that @xmath70 is replaced by @xmath71 .",
    "the elements of @xmath71 are defined in ( [ eq : trun mean ] ) .",
    "the posterior @xmath50 and the updates for the hyperparameters are identical to the ones in mgp regression .    in bayesian classification the label with highest probability @xmath72 is selected . since an exact computation is analytically intractable , we assume the posteriors @xmath66 and @xmath50 are highly peaked around their mean leading to the following classification rule : @xmath73 where @xmath55 and @xmath74 are as before with @xmath70 replaced by @xmath71 .",
    "deciding whether the label is @xmath75 or @xmath76 is equivalent to using the sign of @xmath55 as the decision rule .",
    "we compare frequentist and bayesian approaches to kernel combination .",
    "we demonstrate the flexibility and the performance of the mgp models on the following two detat sets :    * toy regression data . *",
    "we generate random functions from the hilbert spaces induced by 10 laplacian kernels and add gaussian i.i.d .",
    "noise ; we show results on three different settings : a sparse problem , where only one kernel is used to generate the response , a semi - sparse problem with 3 functions are used and a non - sparse problem where all ten functions are active .",
    "table  [ table : regression ] compares several mgp models and several cross - validated mkl models with fixed regularisation norms .",
    "[ fig : regression student ] in the appendix shows that the hierarchical bayesian approach is able to adapt to the sparsity of the data .",
    "lcccr number of active kernels & 1 out of 10 & 3 out of 10 & 10 out of 10 +   +   + multiple student-@xmath16 & .033 ( @xmath77 ) & .067 ( @xmath78 ) & .719 ( @xmath79 ) + multiple laplace & .034 ( @xmath80 ) & .076 ( @xmath81 ) & .704 ( @xmath82 ) + multple gamma - variance & .033 ( @xmath77 ) & .067 ( @xmath78 ) & .719 ( @xmath83 ) + ard & .033 ( @xmath77 ) & .066 ( @xmath84 ) & .746 ( @xmath83 ) + mkl @xmath1 & .037 ( @xmath78 ) & .066 ( @xmath85 ) & .720 ( @xmath86 ) + mkl @xmath87 & .830 ( @xmath88 ) & .831 ( @xmath89 ) & .762 ( @xmath90 ) + mkl @xmath91 & .097 ( @xmath92 ) & .233 ( @xmath93 ) & .719 ( @xmath94 ) +    * * flowers data set . * * due to a lack a space we do not describe the data and the features , but only mention it is a standard mkl benchmark for multi - class image classification . for each of the @xmath95 flower classes we learn a one - versus - all classifier .",
    "[ fig : roc student ] shows the roc curve for two classes when considering a student-@xmath16 process , for which we obtained an average @xmath96 .",
    "the average @xmath97 for gamma - variance process and ard are respectively given by @xmath98 and @xmath99 .",
    "all are better than state - of - the - art mkl results @xcite .",
    "the generalised inverse gaussian density is defined as follows @xcite : @xmath100 where @xmath101 and @xmath102 is the modified bessel function of the second kind with index @xmath103 . depending on the value taken by @xmath104",
    ", we have the following constraints on @xmath105 and @xmath106 : @xmath107 let us define @xmath108 .",
    "the following expectations are useful : @xmath109 when @xmath42 and @xmath41 , the generalised inverse gaussian density reduces to the gamma density .",
    "when @xmath40 and @xmath39 , it reduces to the inverse gamma density .",
    "the expectations simplify also .",
    "the ( positive / negative ) truncated gaussian density is defined as follows : @xmath110 where @xmath111 is the cumulative density of the unit gaussian .",
    "let @xmath112 . the mean and variance are given by @xmath113"
  ],
  "abstract_text": [
    "<S> we consider a gaussian process formulation of the multiple kernel learning problem . the goal is to select the convex combination of kernel matrices that best explains the data and by doing so improve the generalisation on unseen data . </S>",
    "<S> sparsity in the kernel weights is obtained by adopting a hierarchical bayesian approach : gaussian process priors are imposed over the latent functions and generalised inverse gaussians on their associated weights . </S>",
    "<S> this construction is equivalent to imposing a product of heavy - tailed process priors over function space . </S>",
    "<S> a variational inference algorithm is derived for regression and binary classification . </S>"
  ]
}