{
  "article_text": [
    "minimizing non - convex error functions over continuous and high - dimensional spaces has been a primary challenge .",
    "specifically , training modern deep neural networks presents severe difficulties , mainly because of the large number of critical points with respect to the number of dimensions , including various saddle points and local minima  @xcite .",
    "in addition , the landscapes of the error functions are theoretically and computationally impossible to characterize rigidly .",
    "recently , some researchers have attempted to investigate the landscapes of the objective functions for several types of neural networks . under some strong assumptions , previous works",
    "@xcite showed that there exists multiple , almost equivalent local minima for deep neural networks , using a wide variety of theoretical analysis and empirical observations . despite of the nearly equivalent local minima during training , obtaining good generalization performance",
    "is often more challenging with current stochastic gradient descent ( sgd ) or some of its variants .",
    "@xcite demonstrated that deep network structures are sensitive to initialization and learning rates.and even networks without nonlinear activation functions may have degenerate or hard to escape saddle points  @xcite .",
    "one important reason of the difficulty to achieve good generalization is , that sgd and some variants may tend to trap into a certain local minima or flat regions with poor generalization property  @xcite . in other words , most of existing optimization methods do not explore the landscapes of the error functions efficiently and effectively . to increase the possibility of sufficient exploration of the parameter space , @xcite proposed to train multiple deep networks in parallel and made individual networks explore by modulating their distance to the ensemble average .",
    "another kind of approaches attempt to tackle this issue through borrowing the idea of classical simulated annealing or tempering  @xcite .",
    "@xcite proposed to inject gaussian noise with annealed variance ( corresponding to the annealed temperature in simulated annealing ) into the standard sgd to make the original optimization dynamics more `` stochastic '' .",
    "in essence , this approach is the same as a scalable bayesian learning method , stochastic gradient langevin dynamics ( sgld , @xcite ) with decayed stepsizes .",
    "santa algorithm  @xcite incorporated a similar idea into a more sophisticated stochastic gradient markov chain monte carlo ( sg - mcmc ) framework .",
    "however , previous studies show that the efficiency and performance of these methods for training deep neural networks is very sensitive to the annealing schedule of the temperature in these methods .",
    "slow annealing will lead to significantly slow optimization process as observed in the literature of simulated annealing  @xcite , while fast annealing hinders the exploration dramatically , leading in the optimizer getting trapped in poor local minima too early .",
    "unfortunately , searching for a suitable annealing schedule for training deep neural network is hard and time - consuming according to empirical observations in these works .    to facilitate more efficient and effective exploration for training deep networks",
    ", we divide the whole training process into two phases : bayesian sampling for exploration and optimization for fine - tuning .",
    "the motivation of implementing a sampling phase is that sampling is theoretically capable of fully exploring the parameter space and can provide a good initialization for optimization phase .",
    "this strategy is motivated by the sharp minima theory  @xcite and its validity will be verified by our empirical experiments .",
    "crucially , in the sampling phase , we employ the idea of continuous tempering  @xcite in molecule dynamics  @xcite , and implement an extended stochastic gradient second - order langevin dynamics _ with smoothly varying temperatures_. importantly , the change of temperature is governed _ automatically _ by a specifically designed dynamics coupled with the original langevin dynamics .",
    "this is different from the idea of simulated annealing adopted by @xcite and @xcite , in which the temperature is only allowed to decrease according to a manually predefined schedule .",
    "our `` temperature dynamics '' is beneficial in the sense that it increases the capability of exploring the energy landscapes and hopping between different modes of the sampling distributions .",
    "thus , it may avoid the problem of early trapping into bad local minima that exists in other algorithms .",
    "we name our approach * ctld * , abbreviated for `` continuously tempered langevin dynamics '' . with support of extensive empirical evidence",
    ", we demonstrated the efficiency and effectiveness of our proposed algorithm for training various types of deep neural networks . to the best of our knowledge ,",
    "this is the first attempt that adopts continuous tempering into training modern deep networks and produces remarkable improvements over the state - of - the - art techniques .",
    "the rest of the paper is organized as follows .",
    "section  [ sec : pre ] provides the preliminary knowledge on langevin dynamics and stochastic gradient markov chain monte carlo framework .",
    "section  [ sec : twophases ] and [ sec : ctld ] present our proposed ctld for training deep neural networks .",
    "discussion of parameters settings for ctld is in section  [ sec : hyper ] .",
    "various experiments are conducted in section  [ sec : exp ] .",
    "section  [ sec : con ] concludes the paper .",
    "the goal of training deep neural network is to minimize the objective function @xmath0 corresponding to a non - convex model of interest , where @xmath1 are the model parameters . in a bayesian",
    "setting , the objective @xmath0 can be treated as the potential energy function , i.e. , the negative log posterior , @xmath2 where @xmath3 represents the @xmath4-th observed data point , @xmath5 is the prior distribution for the model parameters and @xmath6 is the likelihood term for each observation . in optimization scenario ,",
    "the counterpart of the complete negative log likelihood is the loss function and @xmath7 is typically referred to as a regularization term .      in the scenario of bayesian learning , obtaining the samples of a complex high - dimensional distribution",
    "is a necessary procedure for many related tasks .",
    "classic dynamics offers such a way to sample the distribution .",
    "the hamiltonian in classic dynamics is @xmath8 , the sum of the potential energy @xmath0 and kinetic energy @xmath9 , where @xmath10 is the momentum term in the kinetic term @xmath11 since we always assume it is an identity matrix . ] .",
    "standard ( second - order ) langevin dynamics with constant temperature @xmath12 can be described by following stochastic differential equations ( sdes ) , @xmath13 where @xmath14 is the gradient the potential energy w.r.t .",
    "the configuration states @xmath15 , @xmath16 denotes the friction coefficient , @xmath17 with boltzmann constant @xmath18 , and @xmath19 is the standard wiener process . in the context of this work for markov chain monte carlo ( mcmc ) and optimization theory , we always assume @xmath20 for simplicity .",
    "if we simulate the dynamics in eqs  ( [ eq : ldsde1])-([eq : ldsde2 ] ) , a well - known stationary distribution can be achieved  @xcite @xmath21 where @xmath22 is the normalization constant for the probability density . the desired probability distribution associated with the parameters @xmath15",
    "can be obtained by marginalizing the joint distribution , @xmath23 .",
    "the mcmc procedures using the analogy of dynamics described by sdes are often referred to as dynamics - based mcmc methods .",
    "however , in the `` big data '' settings with large @xmath24 , evaluating the full gradient term @xmath25 is computationally expensive .",
    "the usage of stochastic approximation reduces the computational burden dramatically , where a much smaller subset of the data , @xmath26 , is selected randomly to approximate the full one , @xmath27 and the resulting stochastic gradient @xmath28 is an unbiased estimation of the true gradient .",
    "then the stochastic gradient approximation can be used in the dynamics - based mcmc methods , often referred to as sg - mcmc , such as @xcite .",
    "simulated annealing ( sa , @xcite ,  @xcite ,  @xcite ) is a probabilistic technique for approximating the global optimum of a given function @xmath0 .",
    "@xcite proposed a brownian - type of diffusion algorithm for continuous optimization by discretizing the following sde , @xmath29 where @xmath30 decays according to the following rate to ensure theoretical convergence , @xmath31 with a sufficiently large constant @xmath32 .",
    "unfortunately , this logarithmic annealing schedule is extremely slow for optimization . in practice , the polynomial schedules are often adopted to accelerate the optimization processes though without any theoretical guarantee , such as @xmath33 where @xmath34 are hyperparameters .",
    "recently , @xcite and @xcite incorporated the simulated annealing techniques with this polynomial cooling schedule into the training of deep neural networks .",
    "the critical issue behind these methods is that the generalization performance and efficiency of the optimization are highly sensitive to the the cooling schedule .",
    "slow annealing will lead to significantly slow optimization process as observed in the literature of simulated annealing , thus reducing the possibility of obtaining good local minima . .",
    "unfortunately , searching for a suitable annealing schedule for training deep neural network is hard and time - consuming according to empirical observations in these works .",
    "these challenges motivate our work .",
    "we proposed to divide the whole optimization process into two phases : bayesian sampling based on stochastic gradient for parameter space exploration and standard sgd with momentum for parameters optimization .",
    "the key step in the first phase is that we employ a new tempering scheme to facilitate more effective exploration over the whole energy landscape .",
    "now , we will elaborate on the proposed approach .",
    "as mentioned in section  [ sec : intro ] , the objective functions of deep networks contain multiple , nearly equivalent local minima .",
    "the key difference between these local minima is whether they are `` _ _ flat _ _ '' or `` _ _ sharp _ _ '' , i.e. , lying in `` wide valleys '' or `` stiff valleys '' of the energy landscape .",
    "a recent study by @xcite showed that sharp minima often lead to poorer generalization performance .",
    "flat minimizers of the energy landscape tend to generalize better due to their robustness to data perturbations , noise in the activations as well as perturbations of the parameters .",
    "however , most of existing optimization methods lack the ability to efficiently explore the flat minima , often trapping into sharp minima too early .",
    "we consider this issue in a bayesian way : the flat minima corresponds to `` fat '' modes of the induced probability distribution over @xmath15 , @xmath35 .",
    "obviously , these fat modes own much more probability mass than `` thin '' ones since they are nearly as `` tall '' as each other .",
    "based on this simple observation , we propose to implement a bayesian sampling procedure before the optimization phase .",
    "bayesian learning is capable of exploring the energy landscape more thoroughly .",
    "due to the large probability mass , the sampler tends to capture the desired regions near the `` flat '' minima .",
    "this provides a good starting region for optimization phase to fine - tune the parameters learning .",
    "when sampling the distribution @xmath36 , the multi - modality issue demands the samplers to transit between isolated modes efficiently . to this end",
    ", we incorporate the continuous tempering and stochastic approximation techniques into the langevin dynamics to derive an efficient and effective sampling process for training deep neural networks .",
    "faced with high - dimensional and non - convex energy landscapes @xmath0 , such as the error functions in deep neural networks , the key challenge is how to efficiency and effectively explore the energy landscapes .",
    "inspired by the idea of continuous tempering  @xcite in molecule dynamics , we incorporate the `` temperature dynamics '' and stochastic approximation into the langevin dynamics in a principled way to allow a more effective exploration of the energy landscape .",
    "the temperature in ctld evolves _ automatically _ governed by the embedded `` temperature dynamics '' , which is different from the predefined annealing schedules used by @xcite and @xcite .",
    "the primary dynamics we use for bayesian sampling is as follows , @xmath37 where @xmath38 is the newly augmented variable to control the inverse temperature @xmath39 , @xmath40 is the corresponding friction coefficient . note that in eq .",
    "( [ eq : ctsde2 ] ) @xmath41 , depending on the augmented variable @xmath38 to dynamically adjust the temperature .",
    "the function @xmath42 plays the role as scaling the constant temperature @xmath12 .",
    "the dynamics of @xmath15 and @xmath38 are coupled through the function @xmath43 .",
    "both of the two functions will be described later .",
    "it can be shown that if we simulate the sdes described in eqs([eq : ctsde1])-([eq : ctsde4 ] ) , the following stationary distribution will be achieved  @xcite , @xmath44 with the extended hamiltonian and the coupling function @xmath45 as @xmath46 where @xmath47 is some confining potential to enforce additional properties of @xmath38 , discussed in section  [ sec : control_alpha ] .",
    "the proof of achievement of this stationary distribution @xmath48 is provided in the supplementary material for completeness .    in order to allow the system to overcome the issue of muli - modality efficiently ,",
    "the temperature scaling function @xmath42 can be any convenient form that satisfies : @xmath49 and being smooth .",
    "this will allow the system to experience different temperature configurations smoothly .",
    "a simple choice would be the following piecewise polynomial function , @xmath50 where @xmath51 .",
    "figure  [ fig : coupling_g ] presents this temperature scaling function with @xmath52 and @xmath53 . in this case ,",
    "@xmath54 $ ] .",
    "-0.4 in    experiencing high temperature configurations continuously allows the sampler to explore the parameter space more `` wildly '' , significantly alleviating the issue of trapping into local minima or flat regions .",
    "moreover , it can be easily seen that when @xmath55 , we can recover the desired distribution @xmath56 .      with large - scale datasets",
    ", we adopt the technique of stochastic approximation to estimate the full potential term @xmath57 and its gradient @xmath58 , as shown in eq .",
    "( [ eq : stochastic_app ] ) .",
    "one way to analyse the impact of the stochastic approximation is to make use of the central limit theorem , and therefore , @xmath59 the usage of stochastic approximation results in a noisy potential term and gradient .",
    "simply plugging in the the noisy estimation into the original dynamics will lead to a dynamics with additional noise terms . to dissipate the introduced noise",
    ", we assume the covariance matrices , @xmath60 and @xmath61 , are available , and satisfy the positive semi - definiteness , @xmath62 and @xmath63 with @xmath64 as the associated step size of numerical integration for the sdes . with @xmath64 small enough ,",
    "this is always true since the introduced stochastic noise scales down faster than the added noise .",
    "then , we propose ctld with stochastic approximation , @xmath65 where the coupling function @xmath66 then we have the following theorem to show the stationary distribution of the dynamics described in eq .",
    "( [ eq : sgctsde1])-([eq : sgctsde4 ] ) .",
    "@xmath67 is the stationary distribution of the dynamics sdes eq .",
    "( [ eq : sgctsde1])-([eq : sgctsde4 ] ) , when @xmath68 has the form as shown in eq.([eq : happ ] ) and the variance terms @xmath60 and @xmath61 are available .",
    "the proof for this theorem is provided in the supplementary materials . in practical implementation of simulating the eq.([eq : sgctsde2 ] ) and ( [ eq : sgctsde4 ] ) , we have @xmath69 where @xmath70 and @xmath71 are the estimation of the noise variance terms . in eq.([eq : sgrupdate ] ) and ( [ eq : sgr_alphaupdate ] ) , the noise introduced by the stochastic approximation is compensated by multiplying @xmath72 , implying that the discrepancy between these dynamics and those of eq.([eq : sgctsde1])-([eq : sgctsde4 ] ) approaches zero as @xmath73 goes to zero . as such , in this infinitesimal step size limit , since eq.([eq : sgctsde1])-([eq : sgctsde4 ] ) yield the correct invariant distribution , so do eq.([eq : sgrupdate ] ) and ( [ eq : sgr_alphaupdate ] ) .",
    "this avoids the need for a costly or potentially intractable mh correction .",
    "however , having to decrease @xmath73 to zero comes with the cost of increasingly small updates .",
    "we can also use a finite , small step size @xmath64 in practice , resulting in a biased ( but faster ) sampler .",
    "more importantly , to avoid the estimation of the variance terms , we often choose @xmath64 small enough and @xmath74 large enough to make the @xmath75 and @xmath76 numerically negligible , and thus ignored in practical use .",
    "it is expected that the distribution of experienced temperatures of the system should only depend on the form of the scaling function @xmath42 .",
    "this would help us achieve the desired temperature distribution , thus resulting in a more controllable system . to this end",
    ", two strategies are shown in this part .",
    "firstly , we confine the augmented variable @xmath38 to be in the interval @xmath77 $ ] . one simple choice to achieve this is to configure its gradient as a `` force well '' : @xmath78 where @xmath79 is some appropriate constant .",
    "intuitively , when the particle @xmath38 `` escapes '' from the interval @xmath80 $ ] , a force induced by @xmath81 will `` pull '' it back .",
    "secondly , we restrict the distribution of @xmath38 to be _ uniform _ over the specified range .",
    "together with the design of @xmath42 , this restriction can guarantee the required percent of running time for sampling with the original inverse temperature @xmath82 , and the remaining for high temperatures .",
    "for example , in case of @xmath42 in eq.([eq : g_alpha ] ) , the percent of simulation time for high temperatures is @xmath83 .    as suggested by @xcite , an adaptive biasing method metadynamics  @xcite",
    "can be used to achieve a flat density across a bounded range of @xmath38 .",
    "metadynamics incorporates a history - dependent potential term to gradually fill the minima of energy surface corresponding to @xmath38 s marginal density , resulting in a uniform distribution of @xmath38 . in its essence ,",
    "metadynamics biases the extended hamiltonian by an additional potential @xmath84 , @xmath85 the bias potential term is initialized @xmath86 , and then updated by iteratively adding gaussian kernel terms , @xmath87 where @xmath88 is the value of the @xmath89-th time step during simulation , the magnitude @xmath90 and variance term @xmath91 are hyperparameters . to update the bias potential over the range @xmath92 $ ]",
    ", we can discretize this interval into @xmath93 equal bins , @xmath94 and in each time step update @xmath38 in each bin .",
    "thus , the force induced by the bias potential can be approximated by the difference between adjacent bins divided by the length of each bin .",
    "the force @xmath95 over the particle @xmath38 will be biased due to the force induced by metadynamics , @xmath96 where @xmath97 denotes the bin index inside which @xmath98 is located .",
    "finally , we summarize ctld in alg .",
    "[ alg : ctld ] .",
    "@xmath99 , @xmath64 , number of steps for sampling @xmath100 , @xmath74 ; parameters of metadynamics : @xmath79 , @xmath90 , @xmath101 and @xmath93 .",
    "initialize @xmath102 , @xmath103 , @xmath104 , @xmath105 , and @xmath106 .",
    "randomly sample a minibatch of the dataset with size @xmath99 to obtain @xmath107 ;",
    "sample @xmath108 , @xmath109 ; @xmath110 @xmath111 @xmath112 . update @xmath84 according to eq .",
    "( [ eq : updatevb ] ) ; find the @xmath97 indexing which bin @xmath98 is located in ; evaluate @xmath68 : @xmath113 update @xmath114 : @xmath115 @xmath110 @xmath116      there is a direct relationship between the proposed method and sgd with momentum . in the optimization phase ,",
    "ctld essentially implements sgd with momentum : as shown in sghmc @xcite , the learning rate in the sgd with momentum corresponds to @xmath117 in our method , the momentum coefficient the sgd is equivalent to @xmath118 .",
    "the key difference appears in the bayesian learning phase , a dynamical diffusion term @xmath119 is added to the update of the momentum to empower the sampler / optimizer to explore the parameter space more thoroughly .",
    "this directly avoids the issue of being stuck into poor local minima too early .",
    "ctld introduces stochastic approximation and temperature dynamics into the langevin dynamics in a principled way .",
    "this distinguishes it from the deterministic annealing schedules adopted in santa  @xcite and sgld / annealsgd  @xcite .",
    "since we apply stochastic approximation into ctld , the convergence properties can be analyzed based on the sg - mcmc framework by  @xcite .",
    "let @xmath120 denote any local minima of @xmath0 and its corresponding objective @xmath121 , and @xmath122 be a sequence of samples from the algorithm .",
    "the sample average can be defined as @xmath123 .",
    "our analysis focuses on using the sample average @xmath124 as an approximation of @xmath121",
    ".    denote the difference @xmath125 and the operators @xmath126 , @xmath127 . under some necessary smoothness and boundedness assumptions ( see assumption 1 in the supplementary materials ) , we establish the following theorem to characterize the closeness between @xmath124 and @xmath121 in terms of bias and mean square error ( mse ) .",
    "this also indicates the stability performance of our method .    the bias and mse of @xmath124 from ctld with stochastic approximation w.r.t .",
    "@xmath121 are bounded with some positive constants @xmath128 and @xmath129 , @xmath130 - u^*\\right|   \\leq    \\frac{c_1 e^{-u^*}}{l }   \\sum_{t=1}^l \\int e^{-\\tilde{\\beta}(\\alpha^{(t)})\\up u(\\thetab ) } { { \\rm d}}\\thetab \\nonumber \\\\ & + c_2 \\left(\\frac{1}{l\\eta } + \\frac{\\sum_t \\ebb\\left [ \\| \\up g_t \\| + \\| \\up b_t \\|\\right ] } { l }    \\right ) + \\ocal ( \\eta ) \\nonumber \\\\ & \\ebb   ( \\hat{u } - u^*)^2    \\leq c_1 ^ 2 e^{-2u^ * } \\left ( \\frac{1}{l } \\sum_{t=1}^l \\int e^{-\\tilde{\\beta}(\\alpha^{(t)})\\up u(\\thetab ) } { { \\rm d}}\\thetab \\right)^2 \\nonumber \\\\ & + c_2 ^ 2 \\left(\\frac{1}{l\\eta } + \\frac{\\sum_t \\ebb\\left [ \\| \\up g_t \\|^2 +   \\| \\up b_t \\|^2\\right ] } { l^2 }    \\right ) + \\ocal ( \\eta^2 ) , \\nonumber\\end{aligned}\\ ] ] where @xmath131 is the norm operator .",
    "the proofs are provided in the supplementary materials .",
    "both of the two bounds involves two parts .",
    "the first one is the distance between the considered optima , @xmath132 and the unnormalized annealing distribution , @xmath133 , which is a bounded quantity related to @xmath134 .",
    "the second part characterizes the approximation error introduced by stochastic approximation and numerical integration of sdes .",
    "to facilitate the practical use of our method and reduce the number of hyperparameterss to be tuned , we always fix these parameters across all the experiments , @xmath135 and @xmath136 .",
    "the only parameters we need to tune are the learning rate and the momentum . in the following , we elaborate how other parameters are configured according to the learning rate .",
    "[ [ friction - coefficients ] ] friction coefficients + + + + + + + + + + + + + + + + + + + + +    fortunately , the connection with sgd - momentum provides us a direct guide for configuring the friction coefficients @xmath16 and @xmath40 similar as the momentum in sgd - momentum . across all the experiments , we suggest this setting , @xmath137 where @xmath138 $ ] denotes the momentum coefficient to be tuned . for @xmath40 , we set @xmath40 equal to @xmath139 corresponding to the momentum equal @xmath140 to enable fast sampling across parameter space .",
    "[ [ temperature - scaling - function ] ] temperature scaling function + + + + + + + + + + + + + + + + + + + + + + + + + + + +    according to the prior analysis of parameter settings , temperature scaling function ( tsf ) @xmath42 is expected to have low values corresponding to high temperatures when @xmath0 is large to explore the parameter space boldly and high values when @xmath0 is small to enable accurate sampling behavior like sghmc  @xcite . for simplicity",
    ", we would require tsf to be symmetric and differentiable with one plateau in the middle to represent the range of the normal temperature @xmath12 .",
    "the simplest form of tsf would be the piecewise polynomial function , as shown in eq .",
    "( [ eq : g_alpha ] ) .",
    "we use this form of @xmath42 for all the experiment to demonstrate its insensitivity to various types of deep learning models .    [",
    "[ confining - potential - function ] ] confining potential function + + + + + + + + + + + + + + + + + + + + + + + + + + + +    note that a large value of @xmath79 in eq .",
    "( [ eq : potentialwell ] ) could induce a rebouncing force on @xmath38 to overcome the inertia and hit the other boundary of the specified interval .",
    "then the dynamics of @xmath38 would degenerate into an equilibrium due to the repeated large force . to avoid this",
    ", we propose the configuration of @xmath79 as follows , @xmath141 indicating the augmented variable @xmath38 will be pulled to the origin once it touches the boundaries of the interval @xmath80 $ ] .",
    "this restricts the temperature to the desired range without loss of exploration abilities , while effectively avoiding the hamiltonian system to spend too much time on sampling with high temperatures .",
    "[ [ metadynamics ] ] metadynamics + + + + + + + + + + + +    the goal of metadynamics is to derive an asymptotically uniform distribution of the augmented variable @xmath38 to achieve the transition of between different modes of @xmath15 . the weight @xmath90 is to control the convergence speed .",
    "we propose the setting of @xmath90 as @xmath142 see the supplementary materials for a more detailed explanation for @xmath90 .",
    "empirical studies show that the performance of our approach is not sensitive to values of @xmath90 around this configuration .    through the proposed configuration of confining potential function @xmath47 and metadynamics",
    ", we successfully handle the boundary effects and convergence speed of metadynamics .",
    "these directly help to achieve a uniform distribution of @xmath38 .",
    "figure  [ fig : alpha_hist ] shows the histogram of @xmath38 in the experiment of training stacked denoising autoencoders in section  [ sec : sda ] .",
    "we can easily observe the achievement of an approximate uniform distribution of @xmath38 ; and there exists some bumps due to the force induced by the confining potential @xmath47 .",
    "+    thus , the proposed algorithm only needs the learning rate and the momentum to be adjusted that is almost as simple as sgd - momentum .",
    "this will be shown in the experiments section , especially imagenet classification task which has not been studied thoroughly for comparing optimization methods in previous researches .",
    "to evaluate the proposed method , we conduct experiments on different popular large - scale neural network models , including stacked denoising autoencoders , long - short - term - memory neural networks , inception - bn neural networks ( presented in supplementary materials ) , and residual neural networks . the same parameter initialization is used when comparing different algorithms , including sgd with momentum , rmsprop , adadelta  @xcite , adam  @xcite , annealsgd  @xcite and santa  @xcite .",
    "all the compared methods are trained with the same number of epochs .",
    "the hyperparameter settings for each compared method are implemented by grid search , provided in the supplementary materials .",
    "we will release the code to public later .",
    "autoencoders have been proven to be useful in pre - training neural networks for improved performance .",
    "the denoising autoencoder takes a noise - corrupted input @xmath143 instead of the original input vector @xmath144 to compute the mapping resulting in better performance .",
    "the basic denoising autoencoder is used as the building block of stacked denoising autoencoders ( sda )  @xcite .",
    "greedy layer - wise training procedure yields significantly better local minima than random initialization . in this setting , the representation outputted by the trained @xmath145-th layer is used as the input to train the @xmath146-th layer .",
    "dropout layers are appended to each layer with a rate of @xmath147 except for the first and last layer . for simplicity and without loss of generality , we focus on the greedy layer - wise training procedure of sdas .",
    "we use the training set of mnist data consisting of 60000 training images for this task .",
    "the network size for dimension reduction is 784 - 500 - 500 - 2000 - 10 .",
    "the mean square errors ( mse ) for each method are shown in table .",
    "annealsgd , santa and ctld show low mses , while our approach produces the best result ."
  ],
  "abstract_text": [
    "<S> minimizing non - convex and high - dimensional objective functions are challenging , especially when training modern deep neural networks . in this paper , a novel approach </S>",
    "<S> is proposed which divides the training process into two consecutive phases to obtain better generalization performance : bayesian sampling and stochastic optimization . </S>",
    "<S> the first phase is to explore the energy landscape and to capture the ` fat' modes ; and the second one is to fine - tune the parameter learned from the first phase . in the bayesian learning phase , </S>",
    "<S> we apply continuous tempering and stochastic approximation into the langevin dynamics to create an efficient and effective sampler , in which the temperature is adjusted automatically according to the designed `` temperature dynamics '' . </S>",
    "<S> these strategies can overcome the challenge of early trapping into bad local minima and have achieved remarkable improvements in various types of neural networks as shown in our theoretical analysis and empirical experiments . </S>"
  ]
}