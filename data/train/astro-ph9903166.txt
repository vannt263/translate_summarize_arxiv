{
  "article_text": [
    "the detection of the cosmic microwave background ( cmb ) in 1965 stands as one of the most important scientific discoveries of the century , the strongest evidence we have of the hot big bang model .",
    "we know from the cobe satellite that it is an almost perfect blackbody with temperature @xmath1 k , with expected tiny spectral distortions only very recently discovered .",
    "once the cmb was discovered , the search was on for the inevitable angular fluctuations in the temperature , which theorists knew would encode invaluable information about the state of the universe at the epoch when big bang photons decoupled from the matter .",
    "this occurred as the universe cooled sufficiently for the ionized plasma to combine into hydrogen and helium atoms .",
    "this epoch was a few hundred thousand years after the big bang , at a redshift @xmath2 when the universe was a factor of a thousand smaller than it is today .",
    "theorists led the experimenters on a merry chase , originally predicting the fractional temperature fluctuation level would be @xmath3 , then in the seventies @xmath4 , then @xmath5 , where it has been since the early eighties , when the effects of the dark matter which dominates the mass of the universe were folded into the predictions .",
    "fortunately the experimenters were persistent , and upper limits on the anisotropy dropped throughout the eighties , leaving in their wake many failed ideas about how structure may have formed in the universe .",
    "a major puzzle of the hot big bang model was how regions that would not have been in causal contact at redshift @xmath6 could have the same temperature to such a high precision .",
    "this led to the theory of inflation , accelerated expansion driven by the energy density of a scalar field , dubbed the inflaton , in which all of the universe we can see was in contact a mere @xmath7 seconds after the big bang .",
    "it explained the remarkable isotropy of the cmb and had a natural byproduct : quantum oscillations in the scalar field could have generated the density fluctuations that grew via gravitational instability to create the large scale structure we see in the universe around us .",
    "this theory , plus the hypothesis that the dark matter was made up of elementary particle remnants of the big bang , led to firm predictions of the anisotropy amplitude . in the eighties ,",
    "competing theories arose , one of which still survives : that topologically stable configurations ( defects , such as cosmic strings ) of exotic particle fields arising in phase transitions could have formed in the early universe and acted as seeds for the density fluctuations in ordinary matter .    immediately following the headline - generating detection of anisotropies by cobe [ @xcite ] in 1992 at the predicted @xmath8 level ,",
    "many ground and balloon experiment began seeing anisotropies over a broad range of angular scales .",
    "the emerging picture from this data has sharpened our theoretical focus to a small group of surviving theories , such as the inflation idea .",
    "the figures in this article tell the story of where we go from here .",
    "[ fig : allsky ] shows a realization of how the temperature fluctuations would look on the sky in an inflation - based model , at the @xmath9 resolution of the cobe satellite and what would be revealed at essentially full resolution .",
    "one sees not only the long wavelength ups and downs that cobe saw , but also the tremendous structure at smaller scales in the map .",
    "one measure of this is the _ power spectrum _ of the temperature fluctuations , denoted by @xmath10 , a function of angular wavenumber @xmath11 , or , more precisely , the multipole number in a spherical harmonic expansion . fig .",
    "[ fig : cltheory ] shows typical predictions of this for the inflation and defect theories , and contrasts it with the best estimate from all of the current data .",
    "the ups and downs in @xmath11-space are associated with sound waves at the epoch of photon decoupling .",
    "the damping evident at high @xmath11 is a natural consequence of the viscosity of the gas as the cmb photons are released from it .",
    "the flat part at low @xmath11 is associated with ripples in the past light cone arising from gravitational potential fluctuations that accompany mass concentrations .",
    "all of these effects are sensitive to cosmological parameters , _",
    "e.g. _ , the densities of baryons and dark matter , the value of the cosmological constant , the average curvature of the universe , and parameters characterizing the inflation - generated fluctuations .",
    "if the spectrum can be measured accurately enough experimentally , such cosmological parameters can also be determined with high accuracy . for a review of cmb science see [ @xcite ] .    once it became clear that there was something to measure , the race was on to design high - precision experiments that would cover large areas of the sky at the fine resolution needed to reveal all this structure and the wealth of information it encodes .",
    "these include ground - based interferometers and long duration balloon ( ldb ) experiments ( flying for 10 days vs. 10 hours for conventional balloon flights ) , as well as the use of large arrays of detectors .",
    "nasa will launch the microwave anisotropy probe ( map ) [ @xcite ] satellite in 2000 and esa will launch the planck surveyor [ @xcite ] around 2006 . they will each spend a year or two mapping the full sky .",
    "[ fig : clproj ] gives an idea of how well we think that the ldb and satellite experiments can do in determining @xmath10 if everything goes right .",
    "theorists have also estimated how well the cosmological parameters that define the functional dependence of @xmath10 in inflation models can in principle be determined with these experiments . in one exercise that allowed a mix of nine cosmological parameters to characterize the space of inflation - based theories , cobe was shown to determine one combination of them to better than 10% accuracy , ldbs and map could determine six , and planck seven .",
    "map would also get three combinations to 1% accuracy , and planck seven ! this is the promise of a high - precision cosmology as we move into the next millennium .",
    "cmb anisotropy experiments often involve a number of microwave and sub - millimeter detectors covering at least a few frequencies , located at the focal plane of a telescope .",
    "the raw data comes to us as noisy time - ordered recordings of the temperature for each frequency channel , which we shall refer to as timestreams , along with the pointing vector of each detector on the sky .",
    "the resolution of the experiment is usually fixed by the size of the telescope and the frequency of the radiation one looks at .",
    "we must learn from the data itself almost everything about the noise and the many signals expected , both wanted and unwanted , with only some guidance from other astrophysical observations .",
    "we shall see that to a large degree this appears to be a well - posed problem in bayesian statistical analysis .",
    "the major data products from the cobe anisotropy experiment were six maps , each with 6144 pixels , derived from six timestreams , one for each detector .",
    "the timestream noise was gaussian , which translated into correlated gaussian noise in the maps .",
    "much effort went into full statistical analyses of the underlying sky signals , most often under the hypothesis that the sky signal was a gaussian process as well .",
    "the amount of cobe data was at the edge of what could be done with 1992 workstations .",
    "the other experiments used in the estimate of the power spectrum in fig .",
    "[ fig : cltheory ] had less data , and full analysis was also feasible .",
    "we are now entering a new era : ldb experiments will have up to two orders of magnitude more data , map three and planck four .",
    "for the forecasts of impressively small @xmath10 errors to become reality , we must learn to deal with this huge volume of data . in this article",
    ", we discuss the computational challenges associated with current methods for going from the timestreams to multi - frequency sky maps , and for separating out from these maps of the different sky signals .",
    "finally , from the cmb map and its statistical properties , cosmological parameters can be derived . to illustrate the techniques , we use them to find estimates of @xmath10 .",
    "this represents an extreme form of data compression , but from which cosmological parameters and their errors can finally be derived .",
    "as we shall discuss at considerable length in this article , the analysis procedure we will describe is necessarily _ global _ ; that is , making the map requires operating on the entire time - ordered data , and estimating the power spectrum requires analyzing the entire map at once .",
    "this is due to the statistically - correlated nature of both the instrumental noise and the expected cmb sky signal which links up measurements made at one point with those made at all others .",
    "of the signals we know are present , there are of course the _ primary _ cmb fluctuations from the epoch of photon decoupling that we have already discussed , the primary goal of this huge worldwide effort .",
    "there are also _ secondary _ fluctuations of great interest to cosmologists arising from nonlinear processes at lower redshift : some come from the epoch of galaxy formation and some from scattering of cmb photons by hot gas in clusters of galaxies .",
    "extragalactic radio sources are another nontrivial signal . on top of this , there are various emissions from dust and gas in our milky way galaxy . while these are foreground nuisances to cosmologists , they are signals of passionate interest to interstellar medium astronomers .",
    "fortunately these signals have very different dependences on frequency ( fig .  [",
    "fig : frequencyforegrounds ] ) , and , as we now know , rather statistically distinct sky patterns ( fig .",
    "[ fig : spatialforegrounds ] ) .    we know how to calculate in exquisite detail the statistics of the primary signal for the various models of cosmic structure formation .",
    "the fluctuations are so small at the epoch of photon decoupling that linear perturbation theory is a superb approximation to the exact non - linear evolution equations .",
    "the simplest versions of the inflation theory predict that the fluctuations from the quantum noise form a gaussian random field .",
    "linearity implies that this translates into anisotropy patterns that are drawn from a gaussian random process and which can be characterized solely by their power spectrum .",
    "thus our emphasis is on confronting the theory with the data in the power spectrum space , as in fig .",
    "[ fig : cltheory ] .",
    "primary anisotropies in defect theories are more complicated to calculate , because non - gaussian patterns are created in the phase transitions which evolve in complex ways and for which large scale simulations are required , a computing challenge we shall not discuss in this article . in both theories , algorithmic advances have been very important for speeding - up the computations of @xmath10 .",
    "the _ secondary _ fluctuations involve nonlinear processes , and the full panoply of @xmath12-body and gas - dynamical cosmological simulation techniques discussed in this volume are being brought to bear on the calculation .",
    "non - gaussian aspects of the predicted patterns are fundamental , and much beyond @xmath10 is required to specify them .",
    "further , some secondary signals , such as radiation from dusty star - burst regions in galaxies , are too difficult to calculate from first principles , and statistical models of their distribution must be guided by observations . at least for most cmb experiments ,",
    "they can be treated as point sources , much smaller than the observational resolution .",
    "the _ foreground _ signals from the interstellar medium are also non - gaussian and not calculable . they must be modeled from the observations and have the added complication of being extended sources .    for each signal @xmath13 present , there is therefore a theoretical `` prior probability '' function specifying its statistical distribution , @xmath14 .",
    "a gaussian @xmath15 has the important property that it is completely specified by the two - point correlation function which is the expectation value of the product of the temperature in two directions @xmath16 and @xmath17 on the sky , @xmath18 . for non - gaussian processes an infinite number of higher order temperature correlation functions",
    "are needed in principle .",
    "the inflation - generated or defect - generated temperature anisotropies are also usually statistically _ isotropic _ , that is , the @xmath12-point correlation functions are invariant under a uniform rotation of the @xmath12 sky vectors @xmath19 .",
    "this implies @xmath18 is a function only of the angular separation .",
    "if the temperature field is expanded in spherical harmonics @xmath20 , then the two - point function of the coefficients @xmath21 is related to @xmath10 by a_m a_ m^ * = c___mm ,   t ( * * ) = _ m a_m y_m ( * * ) , [ eqn : sph - trn ] so the correlation function is related to the @xmath10 by @xmath22 where @xmath23 is a legendre polynominal . just as a fourier wavenumber @xmath24 corresponds to a scale @xmath25 , the spherical - harmonic coefficients correspond to an angular scale @xmath26 .",
    "figure  [ fig : cltheory ] shows @xmath10 for two different cosmologies given the same primordial theory ; we plot @xmath27 since at high @xmath11 it gives the power per logarithmic bin of @xmath11 .",
    "a nice way to think about gaussian fluctuations is that for a given power spectrum , they distribute this power with the smallest dispersion .",
    "temperature fluctuations are typically within @xmath28 and rarely exceed @xmath29 , where @xmath30 is _ rms _ amplitude .",
    "such is the map in fig .",
    "[ fig : allsky ] .",
    "since the term non - gaussian covers all other possibilities , it may seem impossible to characterize , but the way the greater dispersion often manifests itself is that the power is more concentrated , _",
    "e.g. _ in extended hot and/or cold spots for the galactic foregrounds , and point - like concentrations for the extragalactic sources , as is evident in fig .",
    "[ fig : spatialforegrounds ] .",
    "although we may marvel at how well the basic inflation prediction from the 1980 s is doing relative to the current data in fig .",
    "[ fig : cltheory ] , it will be astounding is if no anomalies are found in the passage from those large error bars to the much smaller ones of fig .",
    "[ fig : clproj ] and human musings about such exotic ultra - early universe processes are confirmed .",
    "the new cmb anisotropy data sets will come from a variety of platforms : large arrays of detectors on the ground or on balloons , long duration balloons ( ldbs ) , ground - based interferometers and satellites .",
    "most of these experiments measure the sky at anywhere between 3 to 10 photon frequencies , with several detectors at each frequency . with detector sampling rates of about 100 hz and durations of weeks to years , the raw data sets range in size from gigabytes to nearly terabytes .",
    "another measure of the size of a data set is the number of resolution elements , or beam - size pixels , in the maps that are derived from the raw data . over the next two years",
    ", ldbs and interferometers will measure between @xmath31 to @xmath32 resolution elements , which is an impressive improvement upon cobe / dmr s @xmath33 elements .",
    "nasa s map satellite will measure the whole sky with @xmath34 resolution in its highest frequency channel , resulting in cmb maps with @xmath35 resolution elements .",
    "the planck surveyor has @xmath36 resolution , that of the lower panel of fig .",
    "[ fig : allsky ] , and will create maps with @xmath37 resolution elements .    in fig .",
    "[ fig : clproj ] , forecasts of power spectra and their errors for tophat and boomerang ( two ldb missions ) and map and planck are given .",
    "these results ignore foregrounds and assume maps have homogeneous noise , and thus are highly idealized .",
    "extracting the angular power spectrum from such large maps presents a formidable computing challenge . except for the complication of being on a sphere , the difficulties are those shared with the more usual problem of power spectrum estimation in flat spaces ; in general , it is an @xmath38 process , where @xmath39 is the number of pixels in the map .",
    "what makes the process @xmath38 is either matrix inversion or determinant evaluation , depending on the particular implementation .",
    "( in special cases , the fast fourier transform is a particularly elegant matrix factorization , reducing the operations count from @xmath38 to @xmath40 , but it is not generally applicable . )",
    "in addition to the operations count , storage is also a challenge , since the operations are manipulations of @xmath41 matrices .",
    "for example , the noise correlation matrix for a megapixel map requires 2000 gbytes for single precision ( four byte ) storage !",
    "conceptually , the process of extracting cosmological information from a cmb anisotropy experiment is straightforward .",
    "first , maps of microwave emission at the observed wavelengths are extracted from the lengthy time - ordered data ; these are the maximum - likelihood estimates of the sky signal given a noise model . then , the various physical components are separated : solar - system contamination , galactic and extragalactic foregrounds , and the cmb itself .",
    "finally , given the cmb map , we can find the maximum - likelihood power spectrum , @xmath10 , from which the underlying cosmological parameters can be computed .",
    "this entire data analysis pipeline can be unified in a bayesian likelihood formalism .",
    "of course , this pipeline is complicated by the correlated nature of the instrumental noise , by unavoidable systematic effects and by the non - gaussian nature of the various sky signals .",
    "experiments measure the microwave emission from the sky convolved with their _ beam_. measurements of different parts of the sky are often combined using complicated difference schemes , called _ chopping patterns_. for example , while the planck surveyor will measure the temperature of a single point on the sky at any given time , map and cobe measure the temperature difference between two points .",
    "the purpose of these chops is to reduce the noise contamination between samples , which can be large and may have long - term drifts and other complications .",
    "observations are repeated many times over the experiment s lifetime in different orientations on the sky and in many detectors sensitive to a range of photon wavelengths .",
    "schematically , we can write the observation as @xmath42 here , @xmath43 is the vector of observations at frequency @xmath44 and time @xmath45 , @xmath46 is the noise contribution , @xmath47 is the microwave emission at that frequency and position @xmath48 on the sky , smeared by the experimental beam and averaged over the pixel .",
    "the pointing matrix , @xmath49 , is an operator which describes the location of the beam as a function of time and its chopping pattern . for a scanning experiment ,",
    "it is a sparse matrix with a 1 whenever position @xmath50 is observed at time @xmath45 ; for a chopping experiment it will have positive and negative weights describing the differences made at time @xmath45 .",
    "( note that we shall often drop the reference to the channel , @xmath44 , when referring to a single frequency ) .",
    "the first challenge is to separate the noise from the signal and create an estimate of the map , @xmath51 , and its noise properties .",
    "this alone is a daunting task : long - term correlations in the noise mean that the best estimate for the map is not simply a weighted sum of the observations at that pixel .",
    "rather , a full least - squares solution is required .",
    "this arises naturally as the maximum - likelihood estimate of the map if the noise is taken to be gaussian ( see eq .",
    "[ eqn : pofeta ] , below ) .",
    "this in turn requires complex matrix manipulations due to the long - term noise correlations .",
    "one of the most difficult forms of noise results from the random long term drifts in the instrument .",
    "these make it hard to measure the absolute value of temperature on a pixel , though temperature differences along the path of the beam can be measured quite well because the drifts are small on short time scales .",
    "however , by the time the instrument returns to scan a nearby area of the sky , the offset due to this drift can be quite large , resulting in an apparent _ striping _ of the sky along the directions of the scan pattern .",
    "the problem is even more complicated than a simple offset because the detector noise has a `` @xmath52 '' component at low frequencies accompanying the high frequency white noise .",
    "this striping can be reduced by using a better observing strategy .",
    "if the scan pattern is such that it often passes over one of a set of well sampled reference points , then the offset can be measured and removed from the timestreams .",
    "more complicated crossing patterns in which many pixels are quickly revisited along different scan directions provide a better sampling of the offset drift and allow it to be removed more effectively .",
    "the striping issue highlights the global nature of the problem of map - making .",
    "if the map did not need to be analyzed globally , then one could cut the map into @xmath12 pieces and speed up processing time by @xmath53 .",
    "however , including the reference points is essential and these can be far removed from the subset of pixels in which one is interested .",
    "more complicated crossing patterns which reduce these errors unfortunately increase the `` non - locality '' of the problem , making it difficult to use divide - and - conquer tactics successfully .",
    "solving for the map in the presence of this noise is , in general , an @xmath54 process , where @xmath55 is the number of elements in the time - ordered data .",
    "since @xmath55 may be anywhere from @xmath56 to upwards of @xmath57 , the general problem can not be solved in a reasonable time .",
    "fortunately , the problem becomes tractable if one can exploit the _",
    "stationarity _ , or time - translation invariance , of the noise .",
    "in addition to solving for the map , one also needs the statistical properties of the errors in the map .",
    "accurate calculation of the `` map noise matrix '' is critical , since the signal we are looking for is excess variance in the map , beyond that which is expected from the noise .",
    "it turns out that it is both easier to calculate and store the inverse of the map noise matrix , called the map weight matrix .",
    "the weight matrix is typically very sparse , whereas its inverse may be quite dense .",
    "it is therefore advantageous to have algorithms for power spectrum and parameter estimation which require the weight matrix , rather than its inverse .",
    "maps are made at a number of different wavelengths .",
    "each of these maps will be the sum of the cmb signal , @xmath58 , and contributions from astrophysical foregrounds : sources of microwave emission in the universe other than the cmb itself .",
    "this includes low - frequency galactic emission from the 20k dust that permeates the galaxy and from gas , emitting synchrotron and bremsstrahlung ( or free - free ) radiation .",
    "there are also extragalactic sources of emission : galaxies that emit in the infrared and the radio .",
    "these are treated as point sources , since their angular size is much smaller than the experimental resolution .",
    "in addition , clusters of galaxies and the filamentary structures connecting them will appear because their hot gas of electrons can compton scatter cmb photons to shorter wavelengths , a phenomenon known as the sunyaev - zeldovich ( sz ) effect .",
    "these clusters are typically a few arcminutes across , small enough to be resolved by planck but not map . in figure",
    "[ fig : spatialforegrounds ] , we schematically show the spatial patterns of some of these foregrounds , and in figure  [ fig : frequencyforegrounds ] , we show their frequency spectra .    the next challenge , then , is to separate these foregrounds from the cmb itself in the noisy maps .",
    "we write @xmath59 here , @xmath13 is the frequency - independent cmb temperature fluctuation , @xmath60 is the noise contribution whose statistics have been calculated in the map - making procedure , and @xmath61 is the contribution of the foreground or secondary anisotropy component @xmath62 .",
    "the shapes of the expected frequency dependences shown in figure  [ fig : frequencyforegrounds ] show some uncertainty .",
    "there is none for some secondary anisotropy sources , _",
    "e.g. _ , the sunyaev - zeldovich effect , so @xmath63 can be considered a product of the given function of frequency times a spatial function . in the past ,",
    "an approximation like this involving a single spatial template and one function of frequency has been used for all of the foregrounds , but it is essential to consider fluctuations about this for the accuracy that will be needed in the data sets to come .    a crude but reasonably effective method is to separate the signals using the multifrequency data on a pixel - by - pixel basis .",
    "however , it is clearly better to use our knowledge of the spatial patterns in the forms adopted for @xmath64 , _",
    "e.g. _ , the foreground power spectra shown in fig .",
    "[ fig : cltheory ] . even using a gaussian approximation for the foreground prior probabilities has been shown to be relatively effective at recovering the signals . in this case , the statistical distribution of the maps is again gaussian , with a mean given by the maximum likelihood , which turns out to involve _ wiener filtering _ of the data [ @xcite ] . in simulations for planck performed by bouchet and gispert",
    ", the layers making up the `` cosmic sandwich '' in figure  [ fig : spatialforegrounds ] have been convolved with the frequency - dependent beams , and realistic noise has been added .",
    "the recovered signals look remarkably like the input ones .",
    "there is some indication that the performance degrades if too large a patch of the sky is taken , possibly because the non - gaussian aspects become more important .",
    "of course , good estimates of the power spectra for each of the foregrounds are essential ingredients for @xmath65 , and these must be obtained from the cmb data in question by iterative techniques , or with other cmb data .",
    "radio astronomers have a long history of image construction using interferometry data .",
    "one of the most effective techniques is the `` maximum entropy method '' .",
    "although this is often a catch - all phrase for finding the maximum likelihood solution , the implementation of the method involves a specific assumption for the nature of @xmath66 , derived as a limit of a poisson distribution . for small fluctuations",
    "it looks like a gaussian , but has higher probability in the tails than the gaussian does .",
    "the poisson aspect makes it well - suited to find and reconstruct point sources . to apply it to the cmb , which has both positive and negative excursions , and to include signal correlation function information",
    ", some development of the approach was needed .",
    "this has been recently carried out and applied to the cosmic sandwich exercise [ @xcite ] .",
    "it did at least as well at recovery as the wiener method did , and was superior for the concentrated sunyaev - zeldovich cluster sources and more generally for point sources , as might be expected .",
    "errors on the maximum entropy maps are estimated from the second derivative matrix of the likelihood function .",
    "we regard these exercises as highly encouraging , but since the accuracy with which cosmological parameters can be determined is very dependent upon the accuracy with which separation can be done , it is clear that much work is in order for improving the separation algorithms .",
    "armed with a cmb map and its noise properties , we can try to extract its cosmological information .",
    "if we assume the cosmological signal is the result of a statistically isotropic gaussian random process , then all of the information is contained in the power spectrum , @xmath10 . with gaussian noise as well , we can write down the exact form of its likelihood function . unfortunately , because of incomplete sky coverage , and the presence of correlated , anisotropic noise , maximizing this likelihood function ( either directly or by some sort of an iterative procedure ) requires manipulation of @xmath67 matrices , typically needing @xmath38 operations and @xmath68 storage .",
    "this becomes computationally prohibitive on typical workstations when @xmath39 exceeds about @xmath31 ; for the @xmath69 satellite missions even supercomputers may be inadequate to the task .",
    "for example , on a single 1000  mhz processor , even one calculation of @xmath70 operations necessary for a ten - million - pixel map would take _ 30,000  years _ ! there is , as of yet , no general solution to this problem .",
    "however , in some cases , such as for the map satellite , a solution has been proposed which relies upon the statistical isotropy of the signal and a simple form for the noise .",
    "unfortunately , most experiments will produce maps with more complicated noise properties .",
    "the power spectrum is a highly compressed form of the data in the map , but it is not the end of the story .",
    "the real goal remains to determine the underlying _ cosmological parameters _ , such as the density of the different components in the universe .",
    "for the simple inflationary models usually considered , there are still at least ten different parameters which affect the cmb power spectrum , so we must find the best fit in a ten ( or more ) dimensional parameter space .",
    "just as the frequency channel maps were derived from the timestreams , the cmb map from the frequency maps , and the power spectrum from the cmb map , the cosmological parameters can be estimated from the power spectrum .",
    "although in doing so , one must be careful about the non - gaussian distribution of the uncertainty in the @xmath10 [ @xcite ] .",
    "we now take a more in - depth look at the problems of map - making and parameter estimation .",
    "the most general algorithms for solving these problems operate globally on the data set and are prohibitively expensive : both require matrix operations @xmath71 , where @xmath72 is either the number of points in the time series ( @xmath73 for upcoming satellites ) or the number of pixels on the sky ( @xmath69 ) .",
    "special properties , such as the approximate _",
    "stationarity _ of the instrumental noise , must be exploited in order to make the analysis of large data sets possible .",
    "to date most work has concentrated on efficient algorithms for the exact global problem , but for the new data sets it will be essential to develop approximate methods as well .",
    "we wish to find the _ most likely _ maps and power spectra .",
    "we can write down likelihood functions for both these quantities if we assume that both the noise and signal are gaussian . while the maximum - likelihood map has a closed - form solution",
    ", there is no such solution for the most likely power spectrum .",
    "thus , the problem of the cost of evaluating the likelihood function is compounded by having to search a very high - dimensional space for the global maximum .",
    "even these complex problems are an oversimplification because we know that foregrounds and secondary anisotropies have non - gaussian distributions .",
    "thus , although we expect to get valuable results using simplified approximations for @xmath74 , in particular the gaussian one we use in the discussion below , monte carlo approaches in which many @xmath61 maps are made will undoubtedly be necessary to accurately determine the uncertainty in the derived cosmological parameter .      as described in eq .",
    "[ eq : data ] , for each channel we model the timestream , @xmath75 , as due to signal , @xmath76 , and noise , @xmath77 , @xmath78 , where @xmath79 is the pointing matrix that describes the observing strategy as a function of time . in the ideal case ,",
    "the noise is gaussian - distributed , _",
    "i.e. _ , its probability distribution is [ eqn : pofeta ] p ( ) = ^-1/2 ( -^n^-1 /2 ) , where @xmath55 is the number of time - ordered data points and @xmath80 is the noise covariance matrix . here",
    "the @xmath81 denotes transpose and the brackets indicate an ensemble average ( integration over @xmath82 ) . substituting @xmath83 for @xmath77 in this expression",
    "gives the probability of the time - ordered data given a map , @xmath84 , which is also referred to as the likelihood of the map , @xmath85 .",
    "we are actually interested in the probability of a map given the data , @xmath86 . if we assign a uniform prior probability to the underlying map , _",
    "i.e. _ , @xmath87 is constant , then by bayes theorem @xmath88 is simply proportional to the likelihood function , @xmath85 .    the map that maximizes this likelihood function is [ eqn : mapsoln ] | = c_n p^n^-1 d where @xmath89 is the noise covariance matrix of the map , c_n(|- ) ( |-)^= ( p^n^-1 p)^-1 .",
    "this map is known as a _ sufficient statistic _ , in that @xmath90 and @xmath89 contain all of the sky information in the original data set , provided the pixels are small enough . as discussed above ,",
    "it is preferable to work with @xmath91 , the map weight matrix , which is often sparse or nearly so .",
    "for many purposes , the variance - weighted map , [ eqn : solvefordelta ] c_n^-1| = p^n^-1d may be more useful than the map itself , so that we can avoid the computationally intensive step of inverting the weight matrix .",
    "this is true for optimally combining maps , since variance - weighted maps and their weight matrices simply sum , and for finding the minimum - variance map in a different basis , such as fourier modes or spherical harmonics .",
    "an algorithm for finding the most likely power spectrum exploits this , as we will see below .",
    "if we do need to find @xmath92 , we can solve eq .",
    "[ eqn : solvefordelta ] iteratively by techniques like the conjugate gradient method . in general ,",
    "such methods require @xmath39 iterations and are effectively still @xmath38 methods .",
    "fortunately , we expect @xmath89 to be sufficiently diagonal - dominant that many fewer than @xmath39 iterations are required .",
    "this is aided by the use of _ pre - conditioners _ , which will be discussed further in the context of finding the maximum - likelihood power spectrum .",
    "whether we are interested in @xmath92 or @xmath93 , we still must convolve the inverse of @xmath12 with the data vector .",
    "the direct inversion of @xmath12 by brute force is impractical since it is an @xmath94 matrix where @xmath55 is often about @xmath57 .",
    "however , this is greatly simplified if the noise is stationary , which means its statistical properties are time translation invariant , so that @xmath95 .",
    "stationarity means that @xmath12 is diagonal in fourier space with eigenvalues @xmath96 , the noise _",
    "power spectrum_. @xmath97 is then just the inverse fourier transform of @xmath98 .",
    "knowing @xmath97 , it is easy to calculate the map weight matrix , @xmath99 .",
    "the convolution of @xmath97 with @xmath75 appears to be an @xmath100 operation .",
    "since there is much more timestream data @xmath101 , this is potentially the slowest step in the calculation of the map .",
    "fortunately , the convolution is actually much faster because @xmath102 generally goes nearly to zero for @xmath103 .",
    "the absence of weight at long time scales can be due to the `` @xmath52 '' nature of the instrument noise at low temporal frequencies .",
    "atmospheric fluctuations also have more power on long time scales than on short time scales , as do many noise sources .",
    "since these characteristic times do not scale with the mission duration , the convolution is actually @xmath104 .",
    "similarly , the multiplication of the pointing matrix is also @xmath104 because of its sparseness .",
    "thus , we can reduce the timestream data to an estimate of the map and its weight matrix in only @xmath68 operations , a substantial savings compared to the @xmath54 operations required for a direct calculation .",
    "these algorithms , or similar ones , have been implemented in practice , _",
    "e.g. _ , [ @xcite ] .",
    "above , we made two simplifying assumptions : that the statistical properties of the noise in the timestream were known and that the noise sources were all stationary .",
    "here we try to deal with the more general case .",
    "we would like to estimate the statistical properties of the noise by using a model of the instrument , but in practice , these models are never sufficient",
    ". one must always estimate the noise from the data set itself , and doing this from the timestream requires some assumptions .",
    "it is usually assumed that the noise is stationary over sufficiently long intervals of time and is gaussian",
    ". often the data set is dominated by noise and to a first approximation , is all noise .",
    "thus one has many pairs of points separated by @xmath105 to estimate @xmath106 .",
    "techniques are being developed [ @xcite ] to simultaneously determine the map and noise power spectrum and the covariance between the two .",
    "non - stationary noise can arise in a number of ways : possible sources include contamination by radiation from the ground , balloon or sun , some components of atmospheric fluctuations and cosmic ray hits .",
    "often they are synchronous with a periodic motion of the instrument .",
    "they can be taken into account by extending the model of the timestream given in eq .",
    "[ eq : data ] to include contaminants of amplitude @xmath107 with a known `` timestream shape '' , @xmath108 : @xmath109 the contaminant amplitudes are now on the same mathematical footing as the map pixels , @xmath110 , and both can be solved for simultaneously .",
    "a more conservative approach assigns infinite noise to modes of the time - ordered data which can be written as a linear combination of the @xmath108 .",
    "doing so removes all sensitivity of the map to the contaminant , irrespective of the assumption of gaussianity .",
    "operationally , we replace the timestream noise covariance matrix , @xmath111 with [ eq : matcon ] n_tt n_tt+_c ^2_c _ tc _ tc where the @xmath112 are taken to be very large , thereby setting the appropriate eigenvalues of @xmath97 to zero .",
    "this noise matrix has lost its time - translation invariance and so is no longer directly invertible by fourier transform methods . fortunately , there is a theorem called the _ woodbury formula _ [ @xcite ] which allows one to find the resulting correction to @xmath97 for additions to @xmath12 of the form in eq .",
    "[ eq : matcon ] while only having to invert matrices of dimension equal to the number of contaminants .",
    "we now turn to the determination of some set of cosmological parameters from the map .",
    "we will focus on the case where the parameters are the @xmath10 s because it is a model independent way of compressing the data .",
    "however , the discussion below can easily be generalized to any kind of parameterization , including the ten or more cosmological parameters that we would like to constrain .",
    "we wish to evaluate the likelihood of the parameters @xmath113 , which folds in the probability of the map given the data with all of the prior probability distributions , for the target signal @xmath13 and the foregrounds and secondary anisotropies @xmath114 , in a bayesian way : @xmath115 only in the gaussian or uniform prior cases is the integration over @xmath13 and @xmath114 analytically calculable .",
    "the usual procedure for `` maximum entropy '' priors is to estimate errors from the second derivative of the likelihood , _",
    "i.e. _ effectively use a gaussian approximation . exploring how to break away from the gaussian assumption",
    "is an important research topic .",
    "assuming all signals and the noise are gaussian - distributed , the likelihood function is @xmath116 \\over \\left [ \\left(2\\pi\\right)^{m_p } |c_n + c_s| \\right]^{1/2 } } , \\ ] ] where @xmath117 is the maximum - likelihood cmb map , with the foregrounds removed .",
    "@xmath89 is the noise matrix calculated above , modified to include variances determined for the foreground maps , and @xmath118 is the primary signal autocorrelation function which depends on @xmath10 ( as in eq .",
    "[ eqn : tt ] , but corrected for the effect of the beam pattern and finite pixel size ) .",
    "the likelihood function is a gaussian distribution _ in the data _ , but a complicated nonlinear function of the parameters , which enter into @xmath118 through the power spectrum . unlike the map - making problem ( eq .",
    "[ eqn : mapsoln ] ) , there is no closed - form solution for the most likely @xmath119 .",
    "thus we must use a search strategy and it should be a very efficient one , since brute force evaluation of the likelihood function requires determinant evaluation and matrix inversion which are both @xmath38 problems . compounding this , evaluating the likelihood is more difficult here because the signal and noise matrices have different symmetries , making it harder to find a basis in which @xmath120 has a simple form .",
    "a particularly efficient search technique for finding the maximum - likelihood parameters is a generalization of the _ newton - raphson _ method of root finding .",
    "the newton - raphson method finds the zero of a function of one parameter iteratively .",
    "one guesses a solution and corrects that guess based on the first derivative of the function at that point .",
    "if the function is linear , this correction is exact ; otherwise , more iterations are required until it converges .    in maximizing the likelihood",
    ", we are searching for regions where the first derivative of the likelihood with respect to the parameters goes through zero , so it can be solved analogously to the newton - raphson method .",
    "we actually maximize @xmath121 , which simplifies the calculation and also speeds its convergence since the derivative of the logarithm is generally much more linear in @xmath10 than the derivative of the likelihood itself .",
    "solving for the roots of @xmath122 using the newton - raphson method requires that we calculate @xmath123 , which is known as the curvature of the likelihood function .",
    "operationally , we often replace the curvature with its expectation value @xmath124 , the _ fisher matrix _ , because it is easier to calculate and still results in convergence to the same parameters .    the change in the parameter values at each iteration for this method is a quadratic form involving the map ; hence it is referred to as a _",
    "quadratic estimator_. using @xmath10 as our parameter , the new guess is modified by [ @xcite ] @xmath125\\ ] ] where the fisher matrix is given by @xmath126 we can recover the full shape of the likelihood for the @xmath10 s from this and one other set of numbers , calculated in approximately the same number of steps as the fisher matrix itself [ @xcite ] .",
    "the procedure is very similar to that of the levenberg - marquardt method [ @xcite ] for minimizing a @xmath127 with non - linear parameter dependence .",
    "there the curvature matrix ( second derivative of the @xmath127 ) is replaced by its expectation value and then scaled according to whether the @xmath127 is reduced or increased from the previous iteration .",
    "similar manipulations may possibly speed convergence of the likelihood maximization , although one would want to do this without direct evaluation of the likelihood function .",
    "this method has been used for the power spectrum estimates for cobe and other experiments , and for the compressed power spectrum bands estimated from current data shown in fig .",
    "[ fig : cltheory ] .",
    "this brute force approach is quite tractable for the current data and for idealized simulations of the satellite and ldb data , such as the power spectrum forecasts of fig .",
    "[ fig : clproj ] , in which the noise was assumed ( incorrectly ) to be homogeneous .",
    "we can calculate the time and memory required to do this quadratic estimation for a variety of realistic data sets and kinds of computing hardware . for this algorithm ,",
    "the @xmath38 operations must be performed for each parameter ( e.g. , each band of @xmath11 for @xmath10 ) .",
    "borrill [ @xcite ] has considered this issue under several different scenarios .",
    "for cobe , power spectrum calculation can easily be done on a modern workstation in less than one day . however , for the ldb data sets expected over the next several years ( with @xmath128 or so ) the required computing power becomes prohibitive , requiring 640  gb of memory and of order @xmath129 floating - point operations , which translates to _ 40  years _ of computer time at 400  mhz .",
    "this pushes the limits of available technology ; even spread over a cray t3e with @xmath130 900  mhz processors , this would take a week or more .",
    "this data set is in hand _ now _ , so we can not even wait for computers to speed up .",
    "when the satellite data arrives , with @xmath69 , a brute - force calculation will clearly be impossible even with projected advances in computing technology over the next decade .",
    "the ten million pixel planck data set would require 1600  tb of storage and @xmath131 floating - point operations or 25,000 years of serial cpu time at 400  mhz .",
    "even a hundredfold increase in computing over the next decade , predicted by moore s law , still renders this infeasible .      to solve these computing challenges ,",
    "shortcuts must be found .",
    "one area where there is great potential benefit is in deciding how the discretized map elements are to be distributed on the sky and stored . imposing enough symmetries at this early step can help greatly to speed up everything that follows .",
    "obviously it is important to keep the number of pixels as small as possible . for a given resolution , fixed for example by the beam size",
    ", the number of pixels is minimized by having them all roughly of the same area .",
    "if there are many pixels in a resolution element much smaller than the beam size , they will be highly correlated and little information is gained by treating them individually .",
    "the hierarchical nature of the pixelization used for the cobe maps was also a very useful property . in this pixelization , known as",
    "the quadrilateralized spherical cube , the sky was broken into six base pixels corresponding to faces of a cube .",
    "higher resolution pixels were created hierarchically , by dividing each pixel into four smaller pixels of approximately equal area .",
    "one advantage of this hierarchical structure is that the data is effectively stored via a branching structure , so that pixels that are physically close to each other are stored close to each other . among other things ,",
    "this allows one to coarsen a map very quickly , by adding the ordered pixels in groups of four .",
    "finally , it is very beneficial to have a pixelization which is azimuthal , where many pixels share a common latitude .",
    "this is incredibly useful in making spherical harmonic transforms between the pixel space , where the data and inverse noise matrix are simply defined , and multipole space , where the theories are simple to describe .",
    "specifically , one wishes to make transforms of the type described by eq .",
    "[ eqn : sph - trn ] , as well as the inverse transformation . when discretized , these transforms naively take @xmath132 operations , because @xmath39 spherical harmonic functions need to be evaluated at @xmath39 separate points on the sky .",
    "however , as has been recently emphasized , if one uses a pixelization with azimuthal symmetry , then the spherical transforms can be greatly sped up [ @xcite ] .",
    "this utilizes the fact that the azimuthal dependence of the spherical harmonic functions can be simply factored out , @xmath133 .",
    "if one further requires that the pixels have discrete azimuthal symmetry , then the azimuthal sum can be performed quickly with a fast fourier transform .",
    "effectively , this means that the @xmath39 functions need only be evaluated at @xmath134 different latitudes , so that the whole process requires only @xmath135 operations .",
    "efforts have been made to speed this up even further , by attempting to use fft s in the @xmath136 direction as well , which in principle could perform the transform in @xmath137 operations .",
    "such implementations are still being developed , and do not tend to pay off until @xmath39 is very large .",
    "pixelizations have been developed which have all of these symmetries .",
    "healpix , devised by kris gorski and collaborators [ @xcite ] , has a rhombic dodecahedron as its fundamental base , which can be divided hierarchally while remaining azimuthal .",
    "it was used for the rapid construction of the map in fig .",
    "[ fig : allsky ] .",
    "another class of pixelizations is based on a naturally azimuthal igloo structure which has been specially designed to be hierarchical [ @xcite ] . in this scheme ,",
    "pixel edges lie along lines of constant latitude and longitude , so it is easy to integrate over each pixel exactly .",
    "this allows any suppression effects due to averaging over the varying pixel shapes to be simply and accurately included when making the transforms .",
    "since many of the signals are most simply described in multipole space , it is natural to try to exploit this basis when implementing the parameter estimation method described above .",
    "we should also try recasting the calculation to take advantage of the simple form the weight matrix @xmath91 has in the pixel basis . finally , with iterative methods we can exploit approximate symmetries of these matrices which can speed up the algorithms tremendously .",
    "oh , spergel and hinshaw [ @xcite ] , hereafter osh , have recently applied these techniques to simulations of the operation of parameter estimation for the map satellite to great effect .",
    "the newton - raphson method does not require the full inverse correlation matrix , but rather @xmath138 , which can be expressed in terms of @xmath91 and various @xmath139 factors .",
    "the equation can be solved using a simple conjugate gradient technique , which iteratively solves the linear system @xmath140 by generating an improved guess and a new search direction ( orthogonal to previous search directions ) at each step . in general ,",
    "conjugate gradient is no faster than ordinary methods , requiring of order @xmath39 iterations with @xmath132 operations per iteration required for the matrix - vector multiplications .",
    "however , this can be sped up in two ways .",
    "first , one can make the matrix well conditioned by finding an appropriate preconditioner which allows the series to converge much faster , in only a few iterations .",
    "second , one can exploit whatever symmetries exist to do the multiplications in fewer operations .",
    "a preconditioner @xmath141 is a matrix which approximately solves the linear system and is used to transform it to @xmath142 , making the series converge much faster .",
    "there are two requirements of a good preconditioner : it should be close enough to the original matrix to be useful and it should be quickly invertible .",
    "one can rewrite the linear system we need to solve as ( i + c_s^1/2c_n^-1c_s^1/2 ) c_s^1/2z = c_s^1/2c_n^-1 |t .",
    "osh use a preconditioner @xmath143 , where @xmath144 is an approximation to the inverse noise matrix in multipole space : @xmath144 is taken to be azimuthally symmetric , so that it is proportional to @xmath145 in multipole space , which makes it block diagonal and possible to invert quickly . for the case they looked at , which includes only uncorrelated pixel noise and an azimuthally symmetric sky cut , this turned out to be a very good approximation which allows for quick convergence .",
    "because the matrices are simple in the bases chosen , the vector - matrix multiplications are much faster than @xmath132 . in multipole space , the theory correlation matrix is simply diagonal , @xmath146 , where @xmath147 denotes the beam pattern in @xmath11 space .",
    "similarly , in pixel space , operations using the inverse noise matrix are much faster .",
    "( osh simplified to a case where the noise matrix was exactly diagonal in pixel space . )",
    "a time - consuming aspect is the transformation between pixel and multipole space , which is @xmath148 .",
    "the whole process is actually dominated by the calculation of the trace in eq .",
    "[ eq : quadraticcl ] , which is performed by monte carlo iterations of the above method , exploiting the fact that @xmath149 $ ] .",
    "the osh method requires effectively @xmath132 operations , a dramatic improvement over traditional methods .",
    "the methods highlighted here have focused on solving one well - posed problem under a number of important simplifying assumptions .",
    "it is not obvious whether any of these assumptions are correct or indeed if the problem itself is as simple as we have described .",
    "in addition , there remain other problems , as or more complex , which remain to be addressed . here",
    ", we briefly touch on some of these issues .",
    "the improvements in speed discussed in the last section relied heavily on assuming the error matrix was close to being both diagonal and azimuthally symmetric .",
    "this may well be the case for the map satellite , because it measures the temperature difference between each point on the sky and very many other points at a fixed angular separation of @xmath150 at many different time scales .",
    "in doing so , the off - diagonal elements of the noise matrix are `` beaten down '' and may indeed be negligible . however , for almost all other cases ( and indeed possibly for map when the effects of foreground subtraction are taken into account , ) the @xmath10 estimation problem becomes much more complicated . in the presence of significant striping or inhomogeneous sky coverage , the block - diagonality of the noise matrix is no longer a good approximation . in this case , finding a basis where both the signal and noise matrices are simple may not be possible .",
    "people have found signal - to - noise eigenmodes of the matrix @xmath151 ( or @xmath152 as in sec .",
    "[ sec : osh ] ) to be useful for data compression and computation speedup , but finding them is another @xmath38 problem .",
    "one might try to solve this by splitting the data set up into smaller bits and analyzing them separately , recombining the results at the end .",
    "however , as emphasized above , this can be difficult to do because of the global nature of the the mapmaking process . ignoring correlations between different regions is often a poor approximation . due to the complicated noise correlation structure ,",
    "optimally splitting and recombining may itself require the @xmath38 operations we are trying to avoid .",
    "another feature of realistic experiments that has not been properly accounted for in the formalism we have outlined is that of asymmetric or time - varying beams .",
    "the model of the experimental procedure we have given here ( eq .  [ eq : data ] ) assumes that all observations of a given pixel see the same temperature .",
    "this implicitly assumes an underlying model of the sky that has been both beam - smoothed and pixelized .",
    "( pixelization effects were touched on in sec .",
    "[ sec : pixel ] . )",
    "if the beam is not symmetric , or if it is time - varying , then different sweeps through the same pixel will see different sky temperatures .",
    "this is very difficult to account for exactly and may be crucial for some upcoming experiments which can have significantly asymmetric beams .",
    "in addition , large uncertainties in the nature of the foregrounds may make their removal quite tricky .",
    "not only are they non - gaussian , but unlike the cmb , their frequency dependence is not well understood .",
    "above , we have cast the problem of foreground separation as essentially a separate step in the process , between the making of maps at various frequencies and the estimation of the cosmological power spectrum .",
    "however , we may need to study foregrounds contaminants in as much detail as the cmb fluctuations themselves in order to fully understand their impact on parameter determination .    throughout ,",
    "we have emphasized the assumption of gaussianity for both the instrumental noise and the cosmological model .",
    "if one or both of these assumptions are violated , the theoretical underpinning of the algorithms we have described becomes shaky .",
    "non - gaussianity issues arise even in intrinsically gaussian theories , due to foregrounds and non - linear effects .",
    "more worrisome are models with intrinsic non - gaussianity at larger angular scales .",
    "how do we even begin to characterize an arbitrary distribution of sky temperatures ? as it is sometimes put , describing non - gaussian distributions is like describing `` non - dog animals . ''",
    "however , techniques do exist for finding specific flavors of non - gaussianity ; for example , estimations have been made recently of the so - called connected @xmath60-point functions for @xmath153 which vanish for a gaussian theory .",
    "other methods have tried to find structures using wavelets , which localize phenomena in both position on the sky and scale ( wavenumber @xmath11 ) .",
    "still others have attempted to find topological measures of non - gaussianity , focusing on fixed temperature contours , like the isotherms of a weather map .",
    "for all of these cases , however , both the theoretical predictions and data analysis are considerably more difficult than the algorithms presented here ; in particular , none of them have been considered in the presence of complicated correlated noise .",
    "the computational challenges we have highlighted are associated specifically with parameter estimation from cmb data , but the problems are generic to other statistical measures that might be of interest .",
    "for example , goodness - of - fit tests ( like a simple @xmath127 or more complicated examples like those explored in [ @xcite ] ) require calculation of a quadratic form involving inversion of @xmath67 matrices , as in the parameter estimation examples above .",
    "one might hope that these problems may also be solvable given similar assumptions to those considered above , but this has yet to be addressed .",
    "finally , we have not even touched on the problem of analyzing measurements of the polarization of the cmb , which results from thomson scattering at the surface of last scattering .",
    "although the essential aspects of the analysis are the same , polarization data will be considerably more difficult to handle for several reasons .",
    "first , because polarization is defined with respect to spatially fixed axes , we must combine measurements from different experimental channels in order to make an appropriate sky map .",
    "second , the signal is expected to be about one tenth the amplitude of the already very small temperature anisotropies .",
    "third , the polarization of foreground contaminants is even less well - understood than their temperatures . with these greater experimental challenges , the resulting maps , and their construction algorithms ,",
    "will be more complicated .",
    "upcoming cmb data sets will contain within them many of the answers to questions that have interested cosmologist for decades : how much matter is there in the universe ?",
    "what does it consist of ?",
    "what did the universe look like at very early times ?",
    "our task will be to extract the answers and assess the errors from these large data sets . especially challenging are the necessities for a _",
    "global _ analysis of the data and for separating the various signals .",
    "although some of the issues we face are specific to the cmb problem , many are of common concern to all astronomers facing the huge onslaught of data from the ground , balloons and space that the next millennium is bringing ( see , _ e.g. _ the article on the sloan digital sky survey ) .",
    "we can not rely on raw computing power alone .",
    "computer scientists and statisticians are now collaborating with cosmologists in the quest for algorithmic advances .",
    "figure  [ fig : allsky ] was provided by kris gorski and both computation and visualization have been handled using the http://www.tac.dk/  healpix software package .",
    "figure  [ fig : spatialforegrounds ] was provided by francois bouchet and richard gispert .",
    "we also thank julian borrill and david spergel for discussion of computer timings and algorithmic issues .",
    "bennett , m.s .",
    "turner & m.  white , `` the cosmic rosetta stone '' , physics today , november , 1997 . 2 .",
    "bennett et al .",
    ", `` 4-year cobe dmr cosmic microwave background observations : maps and basic results '' , astrophys.j . 464 ( 1996 ) l1-l4 .                        \\13 .",
    "muciaccia , p.  natoli & n.  vittorio , `` fast spherical harmonic analysis : a quick algorithm for generating and/or inverting full sky , high resolution cmb anisotropy maps , '' astrophys .",
    "j , 488 , l63 ( 1998 ) .",
    "m.  tegmark , `` how to measure cmb power spectra without losing information '' , _ phys .",
    "rev . _ * d55 * , 5895 ( 1997 ) .",
    "wright , `` scanning and mapping strategies for cmb experiments '' , astro - ph/9612006 ( 1996 ) ."
  ],
  "abstract_text": [
    "<S> the cosmic microwave background ( cmb ) encodes information on the origin and evolution of the universe , buried in a fractional anisotropy of one part in @xmath0 on angular scales from arcminutes to tens of degrees . </S>",
    "<S> we await the coming onslaught of data from experiments measuring the microwave sky from the ground , from balloons and from space . </S>",
    "<S> however , we are faced with the harsh reality that current algorithms for extracting cosmological information can not handle data sets of the size and complexity expected even in the next few years . here </S>",
    "<S> we review the challenges involved in understanding this data : making maps from time - ordered data , removing the foreground contaminants , and finally estimating the power spectrum and cosmological parameters from the cmb map . </S>",
    "<S> if handled naively , the global nature of the analysis problem renders these tasks effectively impossible given the volume of the data . </S>",
    "<S> we discuss possible techniques for overcoming these issues and outline the many other challenges that wait to be addressed .    </S>",
    "<S> [ invited article for _ computing in science and engineering . _ ]    0.2 in </S>"
  ]
}