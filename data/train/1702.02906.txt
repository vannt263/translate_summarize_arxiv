{
  "article_text": [
    "( eeg ) headsets are the most commonly used sensing devices for brain - computer interface ( bci ) , which have been employed in many applications , such as healthcare and gaming @xcite , because of the general ease of setup for normal individuals .",
    "however , bci applications have not received widespread acceptance for real - world applications .",
    "one reason for this is the inability of bci technologies to adapt to the numerous potential sources of variation inherent in the underlying technologies .",
    "these can include human sources of variability , such as individual differences and intra individual variability .",
    "they can also include sources of variability in the technology , such as unintentional differences in recording locations for the eeg electrodes from session to session , or even differences between different eeg headsets .",
    "to date , this latter source remains largely unexplored .",
    "there are many existing eeg headsets , with new models and styles continually becoming available @xcite .",
    "ideally , eeg classification methods should be completely independent from any specific eeg hardware , such that classifiers trained using data from one eeg headset will be transferable to other headsets with little or no recalibration .",
    "this would help ensure that applications could reach a broad base of users and would not become obsolete through hardware upgrades .",
    "however , evidence comparing the performance of various classifiers when using different headsets has shown that often performance is not equal across systems ; that is , the headset does in fact matter @xcite . from a hardware standpoint",
    ", systems can vary along a number of dimensions , including ( but not limited to ) onboard filter characteristics , electrode types and contact methods , electrode locations , or online reference schemes .",
    "all of these inherently change the resulting signal characteristics , some of which are critical features on which the classifiers operate .",
    "thus , it is not surprising that currently switching to a new or different headset requires the subject to re - calibrate it , which can take anywhere from 5 - 20 minutes @xcite .",
    "when implemented into a bci system this calibration session would decrease the utility and appeal of the overall system , likely slowing the rate of acceptance .",
    "while it is not currently possible to switch between eeg headsets completely calibration - free , it is certainly possible to decrease the amount of time and data needed to calibrate an eeg data classifier for use with another eeg system .    in this paper , we specifically attempt to address the problem of developing classifiers that can account for variation due to different eeg headsets within a transfer learning ( tl ) @xcite framework . in tl , some data from a prior calibration or other user sessions is used to facilitate learning of the calibration in a new target context . according to a recent literature review @xcite",
    ", there are mainly three types of tl approaches for bci applications :    1 .",
    "_ feature representation transfer _ @xcite , which encodes the knowledge across different subjects or sessions as features .",
    "these features are generally better than extracting features directly from only the limited number of samples from a new subject or session .",
    "2 .   _ instance transfer _ @xcite , which uses certain parts of the data from other subjects or sessions to help the learning for the current subject or session .",
    "the underlying assumption is that data distributions for these subjects or sessions are similar .",
    "classifier transfer _ , which includes domain adaptation @xcite , i.e. , handling the different data distributions for different subjects or sessions , and ensemble learning @xcite , i.e. , combining multiple classifiers from multiple subjects or sessions , and their combinations @xcite .    in our case ,",
    "data acquired from one style of headset is used to facilitate classification of data currently being acquired from a different one , through domain adaptation and regularized optimization @xcite .",
    "we look at this problem within the context of offline single - trial event - related potential ( erp ) classification , with the eventual goal of moving to online single - trial classification within a bci system .    in some application domains",
    ", we have existing unlabeled data and the calibration session is focused on labeling this data , e.g. , bci applications focused on labeling images , using eeg data @xcite . in these applications ,",
    "the user can manually label a few images , and based on the eeg signals associated with these images a classifier can be trained to automatically label the rest . improved calibration performance can be achieved by selecting the most informative images for manual labeling . in other words , a desired level of calibration performance",
    "can be obtained with less labeling effort if the most informative images are selected for labeling .",
    "this is the idea of active learning ( al ) @xcite , which has also started to find application in bci @xcite .",
    "for example , in our recent work on eeg artifacts classification @xcite , we showed that classification accuracy equivalent to classifiers trained on full data annotation can be obtained while labeling less than 25% of the data by al . in another study @xcite , we applied al to a simulated bci system for target identification using data from a rapid serial visual presentation paradigm , and showed that it can produce similar overall classification accuracy with significantly less labeled data ( in some cases less than 20% ) when compared to alternative calibration approaches .",
    "tl and al are complementary to each other , and hence can be integrated to further reduce the number of labeled training samples in offline bci calibration .",
    "the idea of integrating tl and al was proposed recently @xcite and is beginning to be explored @xcite . however , most of this work is outside of the eeg analysis domain . in our previous work @xcite ,",
    "we investigated how tl and al can be integrated to reduce the amount of subject - specific calibration data in a visual - evoked potential ( vep ) task , by making use of data collected using the same headset but from other subjects ; in contrast , this paper considers the problem of reducing subject - specific calibration data when the same subject switches from one headset to another .",
    "this paper introduces weighted adaptation regularization ( war ) , a particular tl algorithm , and designs a novel al algorithm for it . using a single - trial erp experiment ,",
    "we demonstrate that war can achieve improved performance over the tl approach used in @xcite , and active weighted adaptation regularization ( awar ) , which integrates war and al , can further reduce the offline calibration effort when switching between different eeg headsets .",
    "it should be noted that , while the ultimate goal is an understanding of how well these approaches work when transferring both within and across subjects , here , in order to minimize sources of variability , our analyses are focused on within subjects tl .",
    "the rest of the paper is organized as follows : section  [ sect : war ] introduces the details of war .",
    "section  [ sect : awar ] introduces the details of awar .",
    "section  [ sect : experiments ] describes experimental results and a performance comparison of war and awar with other algorithms .",
    "section  [ sect : conclusions ] draws conclusions .",
    "this section introduces the details of the war algorithms .",
    "we consider two - class classification of eeg data , but the algorithms can also be generalizable to other calibration problems .      given a large amount of labeled eeg epochs from one headset , how can that data be used to customize a classifier for a different headset ?",
    "although eeg epochs from the two headsets are usually not completely consistent , previous data still contain useful information , due to the fact that they came from the same subject . as a result",
    ", the amount of calibration data may be reduced if these auxiliary eeg epochs are used properly .",
    "tl @xcite , particularly war , is a framework for addressing the aforementioned problem .",
    "some notations used in tl and war are introduced next .    *",
    "( domain ) * @xcite a domain @xmath0 is composed of a @xmath1-dimensional feature space @xmath2 and a marginal probability distribution @xmath3 , i.e. , @xmath4 , where @xmath5 .    if two domains @xmath6 and @xmath7 are different , then they may have different feature space , i.e. , @xmath8 , and/or different marginal probability distributions , i.e. , @xmath9 @xcite .    * ( task ) * @xcite given a domain @xmath0 , a task @xmath10 is composed of a label space @xmath11 and a prediction function @xmath12 , i.e. , @xmath13 .",
    "let @xmath14 , then @xmath15 can be interpreted as the conditional probability distribution .",
    "if two tasks @xmath16 and @xmath17 are different , then they may have different label spaces , i.e. , @xmath18 , and/or different conditional probability distributions , i.e. , @xmath19 @xcite .    *",
    "( domain adaptation * ) given a source domain @xmath20 , and a target domain @xmath21 with @xmath22 labeled samples @xmath23 and @xmath24 unlabeled samples @xmath25 , domain adaptation transfer learning aims to learn a target prediction function @xmath26 with low expected error on @xmath7 , under the assumptions @xmath27 , @xmath28 , @xmath9 , and @xmath19 .    in our application ,",
    "eeg epochs from the new headset are in the target domain , while eeg epochs from the previous headset are in the source domain .",
    "a single data sample would consist of the feature vector for a single eeg epoch from a headset , collected as a response to a specific stimulus . though the features in source and target domains are computed in the same way ,",
    "generally their marginal and conditional probability distributions are different , i.e. , @xmath9 and @xmath19 , because the two headsets may have different sensor locations , filters , and signal fidelity . as a result",
    ", the auxiliary data from the source domain can not represent the primary data in the target domain accurately and must be integrated with some labeled data in the target domain to induce the target predictive function .",
    "because @xmath29 to use the source domain data in the target domain , we need to make sure is also close to @xmath30 . however , in this paper we assume all subjects conduct similar vep tasks , so @xmath31 and @xmath30 are intrinsically close",
    ". our future research will consider the more general case that @xmath31 and @xmath30 are different . ]",
    "@xmath32 is close to @xmath33 , and @xmath34 is also close to @xmath35 .",
    "let the classifier be @xmath36 , where @xmath37 is the classifier parameters , and @xmath38 is the feature mapping function that projects the original feature vector to a hilbert space @xmath39 . the learning framework of war is formulated as : @xmath40 where @xmath41 is the loss function , @xmath42 is the overall weight of target domain samples , @xmath43 is the kernel function induced by @xmath44 such that @xmath45 , and @xmath46 , @xmath47 and @xmath48 are non - negative regularization parameters .",
    "@xmath42 is the overall weight for target domain samples , which should be larger than 1 so that more emphasis is given to target domain samples than source domain samples .",
    "@xmath49 is the weight for the @xmath50 sample in the source domain , and @xmath51 is the weight for the @xmath50 sample in the target domain , i.e. , @xmath52 in which @xmath53 is the set of samples in class @xmath54 of the source domain , and @xmath55 is the set of samples in class @xmath54 of the target domain , @xmath56 and @xmath57 .",
    "the goal of @xmath49 and @xmath51 is to balance the number of positive and negative samples in source and target domains , respectively .    briefly speaking , the meanings of the five terms in ( [ eq : f ] ) are :    1 .",
    "the 1st term minimizes the loss on fitting the labeled samples in the source domain .",
    "the 2nd term minimizes the loss on fitting the labeled samples in the target domain .",
    "the 3rd term minimizes the structural risk of the classifier .",
    "4 .   the 4th term minimizes the distance between the marginal probability distributions @xmath32 and @xmath33 .",
    "the 5th term minimizes the distance between the conditional probability distributions @xmath34 and @xmath35 .    by the representer theorem @xcite ,",
    "the solution of ( [ eq : f ] ) admits an expression : @xmath58 where @xmath59^t$ ] , and @xmath60^t$ ] are coefficients to be computed .",
    "note that our algorithm formulation and derivation closely resemble those in @xcite ; however , there are several major differences :    1 .",
    "we consider the scenario that there are a few labeled samples in the target domain , whereas @xcite assumes there are no labeled samples in the target domain .",
    "2 .   we explicitly consider the class imbalance problem in both domains by introducing the weights on samples from different classes .",
    "war is iterative and we further design an al algorithm for it , whereas in @xcite domain adaptation is performed only once and there is no al .",
    "@xcite also considers manifold regularization @xcite .",
    "we investigated it , but we were not able to achieve improved performance in our application , so we excluded it in this paper .    also note that one of the war algorithms ( war - rls ) described in this paper was introduced in our previous publication @xcite ; however , this paper includes a new war algorithm ( war - svm ) , and shows how al can be integrated with war - rls and war - svm .",
    "the application scenario is also different .",
    "two widely used loss functions are the squared loss for regularized least squares ( rls ) : @xmath61 and the hinge loss for support vector machines ( svms ) : @xmath62 both will be considered in this paper . in the following ,",
    "we denote the classifier obtained using squared loss as war - rls , and the one obtained using hinge loss as war - svm .",
    "let @xmath63^t \\label{eq : y}\\end{aligned}\\ ] ] where @xmath64 are known labels in the source domain , @xmath65 are known labels in the target domain , and @xmath66 are pseudo labels for the unlabeled target domain samples , i.e. labels estimated using another classifier and known samples in both source and target domains .",
    "define @xmath67 as a diagonal matrix with @xmath68    substituting ( [ eq : l2 ] ) into the first two terms in ( [ eq : f ] ) , it follows that @xmath69      using the hinge loss and @xmath70 defined in ( [ eq : e ] ) , the first two terms on the right - hand side of ( [ eq : f ] ) can be re - expressed as : @xmath71    often in svm formulations , an unregularized bias term @xmath72 is added to ( [ eq : f2 ] ) , i.e. , @xmath73 we also use this convention in this paper . then , by introducing non - negative slack variables @xmath74 ( @xmath75 ) , the minimization of ( [ eq : hingeloss ] ) is equivalent to : @xmath76\\ge 1-\\xi_i\\nonumber\\\\ & \\qquad \\xi_i\\ge 0,\\qquad i=1, ... ,n+m_l \\nonumber\\end{aligned}\\ ] ]      as in @xcite , we define the structural risk as the squared norm of @xmath77 in @xmath78 , i.e. , @xmath79      similar to @xcite , we compute @xmath80 using the projected maximum mean discrepancy ( mmd ) : @xmath81 ^ 2 \\nonumber \\\\ = & \\boldsymbol{\\alpha}^tkm_0k\\boldsymbol{\\alpha } \\label{eq : dfkp}\\end{aligned}\\ ] ] where @xmath82 is the mmd matrix : @xmath83      similar to the idea proposed in @xcite , we first need to compute pseudo labels for the unlabeled target domain samples and construct the label vector @xmath84 in ( [ eq : y ] ) .",
    "these pseudo labels can be borrowed directly from the estimates in the previous iteration if the algorithm is used iteratively , or estimated using another classifier , e.g. , a svm .",
    "we then compute the projected mmd w.r.t .",
    "each class . the distance between the conditional probability distributions in source and target domains is next computed as : @xmath85 ^ 2 \\label{eq : dfkq}\\end{aligned}\\ ] ] where",
    "@xmath86 , @xmath87 , @xmath88 and @xmath89 have been defined under ( [ eq : wt ] ) .    substituting ( [ eq : f2 ] ) into ( [ eq : dfkq ] )",
    ", it follows that @xmath90 ^ 2 \\nonumber\\\\ = & \\sum_{c=1}^2\\boldsymbol{\\alpha}^tkm_ck\\boldsymbol{\\alpha } = \\boldsymbol{\\alpha}^tkmk\\boldsymbol{\\alpha } \\label{eq : dfkq2}\\end{aligned}\\ ] ] where @xmath91 in which @xmath92 and @xmath93 are mmd matrices computed as : @xmath94      substituting ( [ eq : l3 ] ) , ( [ eq : fk ] ) , ( [ eq : dfkp ] ) , and ( [ eq : dfkq2 ] ) into ( [ eq : f ] ) , it follows that @xmath95 setting the derivative of the objective function above to 0 leads to @xmath96^{-1}e\\mathbf{y } \\label{eq : alpha}\\end{aligned}\\ ] ]      substituting ( [ eq : l4 ] ) , ( [ eq : fk ] ) , ( [ eq : dfkp ] ) , and ( [ eq : dfkq2 ] ) into ( [ eq : f ] ) , then @xmath97 in ( [ eq : f2 ] ) can be re - expressed as : @xmath98\\ge 1-\\xi_i\\nonumber\\\\ & \\quad \\xi_i\\ge 0,\\quad i=1, ... ,n+m_l \\nonumber\\end{aligned}\\ ] ] define @xmath99\\\\ \\mathbf{f}&=[\\mathbf{0}^{1\\times (",
    "n+m_l+m_u)}\\   w_{s,1}\\ \\cdots\\ w_{s , n } \\ w_tw_{t,1}\\ \\cdots\\ w_tw_{t , m_l } \\   0]\\\\ h&=\\left[\\begin{array}{cc }            \\sigma k+k(\\lambda_pm_0+\\lambda_qm)k & \\mathbf{0}^{(n+m_l+m_u)\\times(n+m_l+1 ) } \\\\             \\mathbf{0}^{(n+m_l+1)\\times(n+m ) } & \\mathbf{0}^{(n+m_l+1)\\times(n+m_l+1 ) }           \\end{array}\\right]\\\\ a&=-[a'\\quad \\mathbf{i}^{(n+m_l)\\times(n+m_l ) } \\quad \\mathbf{y}]\\\\ b&=diag([\\mathbf{0}^{1\\times(n+m_l+m_u)}\\quad \\mathbf{1}^{1\\times ( n+m_l)}\\quad 0])\\\\ \\mathbf{b}&=-\\mathbf{1}^{(n+m_l)\\times 1}\\end{aligned}\\ ] ] where @xmath100 and @xmath101 , @xmath102 is a vector of all zeros , @xmath103 is a vector of all ones , and @xmath104 is the identity matrix .",
    "then , solving for @xmath97 and @xmath72 in ( [ eq : fhinge ] ) is equivalent to solving for @xmath105 below : @xmath106 which can be easily done using quadratic programming .    in summary ,",
    "the pseudo code for war - rls and war - svm is shown in the first part of algorithm  1 .",
    "compute @xmath49 and @xmath51 by ( [ eq : ws ] ) and ( [ eq : wt ] ) compute the kernel matrix @xmath107 construct @xmath108 , pseudo labels for the @xmath24 unlabeled target domain samples , using the estimates from the previous iteration , or build another classifier ( e.g. , a basic svm ) to estimate the pseudo labels if this is the first iteration construct @xmath84 in ( [ eq : y ] ) , @xmath70 in ( [ eq : e ] ) , @xmath109 in ( [ eq : m0 ] ) , and @xmath110 in ( [ eq : m ] ) compute @xmath97 by ( [ eq : alpha ] ) for war - rls , or @xmath97 and @xmath72 by ( [ eq : war - svm ] ) for war - svm compute @xmath111 by ( [ eq : f2 ] ) for war - rls , or by ( [ eq : f3 ] ) for war - svm * return * @xmath112 , where @xmath113 construct @xmath114 sort @xmath115 in ascending order according to @xmath116 , @xmath117 construct @xmath118 sort @xmath119 in ascending order according to @xmath116 , @xmath120 concatenate @xmath115 and @xmath119 to form an ordered set @xmath121 * return * the first @xmath122 elements in @xmath123 .",
    "as mentioned in the introduction , war can be integrated with al @xcite for better performance .",
    "al tries to select the most informative samples to label so that a given learning performance can be achieved with less labeling effort .",
    "the key problem in using al is estimating which of the data samples are the most informative .",
    "there are many different heuristics for this purpose @xcite . in this paper",
    "we select the most volatile and uncertain ones as the most informative ones .",
    "more sophisticated approaches will be studied in our future research .",
    "our al for identifying the @xmath122 most informative samples is a two - step procedure : the first step identifies the most volatile unlabeled target domain samples , and the second step further selects the @xmath122 most uncertain ones from them .    recall that at the beginning of war we obtain @xmath108 , the pseudo labels for unlabeled target domain samples , from the previous iteration , and finally we output @xmath112 , the updated estimates of these labels .",
    "if @xmath124 is different from @xmath125 for a certain sample , then there is evidence that that sample is volatile , probably because it is close to the decision boundary .",
    "according to the volatility of the unlabeled target domain samples , we partition them into two groups : @xmath114 and @xmath118 .",
    "samples in @xmath115 are more volatile than those in @xmath119 , and hence they are better candidates for labeling .",
    "we further rank the uncertainties of the samples in @xmath115 by their closeness to the current decision boundary : a sample closer to the decision boundary means the classifier has more uncertainty about its class , and hence we should select it for labeling in the next iteration . to do this ,",
    "we first sort @xmath115 in ascending order according to @xmath116 .",
    "since a smaller @xmath116 means a closer distance to the decision boundary and hence higher uncertainty , we select the first @xmath122 samples in @xmath115 for labeling in the next iteration .",
    "if @xmath122 is larger than the number of samples in @xmath115 , then we also sort @xmath119 in ascending order according to @xmath116 and select the first @xmath126 samples from it .",
    "the complete awar algorithm is given in algorithm  [ alg : awar ] .",
    "we denote the one based on war - rls as awar - rls , and the one based on war - svm as awar - svm . in each algorithm",
    ", we first use war to classify the unlabeled target domain samples , and then al to identify @xmath122 such samples that are most volatile and uncertain .",
    "awar - rls and awar - svm can easily be embedded into an iterative procedure ( section  [ sect : process ] ) so that @xmath122 target domain samples are labeled in each iteration until the maximum number of iterations is reached , or the desired classification performance is achieved .      in algorithm  1",
    ", we assume the source and target domains have consistent features , i.e. , the old and new headsets have same channels so that the features extracted from them have the same dimensionality and meaning .",
    "this also works if the old headset has more channels , but it includes all channels in the new headset , in which case only the common channels are used in feature extraction .",
    "however , things become more complicated if the new headset has channels that are not included in the old headset .",
    "we can again use the common channels for feature extraction and then apply algorithm  1 , but there is information loss if the extra channels in the new headset are completely ignored .",
    "we next propose a solution for this problem .",
    "the extra channels are difficult to use in war , because the target domain does not contain them .",
    "however , it is possible to use them in al , as shown in algorithm  2 , which can be used to replace the al part in algorithm  1 .",
    "algorithm  2 still consists of two steps .",
    "the first step identifies the most volatile unlabeled target domain samples , which is the same as that in the original al algorithm .",
    "the second step ranks the uncertainties of the unlabeled samples by incorporating the uncertainty information from all channels ( common channels plus extra channels ) . for",
    "that we first build a separate classifier using features extracted from all channels and trained from only the @xmath22 labeled samples .",
    "for each unlabeled sample , we compute the sum of two signed distances : 1 ) the distance from the decision boundary determined by this additional classifier , and 2 ) the distance from the decision boundary determined by war .",
    "the smaller the sum , the larger the uncertainty .",
    "we then return the top @xmath122 unlabeled samples that are volatile and most uncertain .",
    "design another classifier , e.g. , a svm , to classify the @xmath24 unlabeled target domain samples using features from all channels ; denote the signed distances to its decision boundary as @xmath127 @xmath114 sort @xmath115 in ascending order according to @xmath128 , @xmath117 @xmath118 sort @xmath119 in ascending order according to @xmath128 , @xmath120 concatenate @xmath115 and @xmath119 to form the ordered set @xmath121 * return * the first @xmath122 elements in @xmath123 .",
    "experimental results are presented in this section to compare war - rls , war - svm , awar - rls , and awar - svm with several other algorithms .",
    "we used data from a vep oddball task @xcite . in this task ,",
    "image stimuli were presented to subjects at a rate of 0.5 hz ( one image every two seconds ) .",
    "the images presented were either an enemy combatant [ target ; an example is shown in fig .",
    "[ fig : t ] ] or a u.s . soldier [ non - target ; an example is shown in fig .  [",
    "fig : nt ] ] .",
    "the subjects were instructed to identify each image as being target or non - target with a unique button press as quickly , but as accurately , as possible .",
    "there were a total of 270 images presented to each subject , of which 34 were targets .",
    "the experiments were approved by the u.s .",
    "army research laboratory ( arl ) institutional review board ( protocol # 20098 - 10027 ) .",
    "the voluntary , fully informed consent of the persons used in this research was obtained as required by federal and army regulations @xcite .",
    "the investigator adhered to army policies for the protection of human subjects .",
    "eighteen subjects participated in the experiments , which lasted on average 15 minutes .",
    "data from four subjects were not used due to data corruption or poor responses .",
    "signals were recorded with three different eeg headsets , including a wired 64-channel activetwo system ( sample rate set to 512hz ) from biosemi , a wireless 9-channel 256hz b - alert x10 eeg headset system from advanced brain monitoring ( abm ) , and a wireless 14-channel 128hz epoc headset from emotiv .",
    "we considered switching between biosemi and emotiv headsets , and between biosemi and abm headsets , respectively .",
    "switching between emotiv and abm headsets was not considered because they have too few common channels .",
    "we used eeglab @xcite for eeg signal preprocessing and feature extraction .",
    "raw amplitude features were used in this study .",
    "the performances of awar - rls and awar - svm on other feature sets are studied later in this section .    for switching between biosemi and emotiv headsets",
    ", we used their 14 common channels ( af3 , af4 , f3 , f4 , f7 , f8 , fc5 , fc6 , o1 , o2 , p7 , p8 , t7 , t8 ) .",
    "for switching between biosemi and abm headsets , we used their nine common channels ( c3 , c4 , cz , f3 , f4 , fz , p3 , p4 , poz ) . for each headset ,",
    "we first band - passed the eeg signals to [ 1 , 50 ] hz , then downsampled them to 64 hz , performed average reference , and next epoched them to the @xmath129 $ ] second interval timelocked to stimulus onset .",
    "we removed mean baseline from each channel in each epoch and removed epochs with incorrect button press responses .",
    "the final numbers of epochs from the 14 subjects are shown in table  [ tab : epoch ] .",
    "observe that there is significant class imbalance for all headsets ; that s why we need to use @xmath49 and @xmath51 in ( [ eq : f ] ) to balance the two classes in both domains .    [",
    "cols=\"<,^,^,^,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]      in this subsection we study the robustness of awar - rls and awar - svm to three different factors : the number of linear pc features , the feature sets extracted using different methods , and the parameters @xmath46 and @xmath47 ( @xmath48 ) .",
    "to save space , we only show the bca results when switching from biosemi to abm .",
    "similar results were obtained from other switching scenarios .",
    "the average bcas of awar - rls and awar - svm for different number of linear pcs are shown in fig .",
    "[ fig : numpcs ] . observe that awar - rls and awar - svm are very robust to the number of pcs .",
    "20 pcs were used in this paper mainly for the computational cost consideration .",
    "two other feature sets were employed to study the robustness of awar - rls and awar - svm to different feature extraction methods : 1 ) 20 nonlinear pca features extracted from an auto - encoder @xcite ; and , 2 ) 18 power spectral density features [ theta band ( 4 - 7.5hz ) and alpha band ( 7.5 - 12hz ) ] from the 9 common channels using welch s method @xcite .",
    "the bca results are shown in fig .",
    "[ fig : features ] .",
    "observe that awar - rls and awar - svm still achieved the best overall bcas in both cases , and they had more obvious performance improvements over other methods than the linear pca case in fig .",
    "[ fig : b2aavg ] .",
    "the bcas of atl decreased on these two feature sets , suggesting that atl is not as robust as awar - rls and awar - svm to different features .",
    "the average bcas of awar - rls and awar - svm for different @xmath46 ( @xmath47 and @xmath48 were fixed at 10 ) are shown in fig .",
    "[ fig : sigma ] , and for different @xmath47 and and @xmath48 identical value because they are conceptually close . ] @xmath48 ( @xmath46 was fixed at 0.1 ) are shown in fig .",
    "[ fig : lambda ] . observe from fig .",
    "[ fig : parameters ] that awar - rls and awar - svm are robust to both @xmath46 and @xmath47 ( @xmath48 ) .",
    "extensive experimental results have demonstrated that awar - rls and awar - svm can indeed reduce the calibration effort when switching to a new eeg headset , and they are very robust .",
    "however , they still have some limitations , which will be considered in our future research :    1 .",
    "awar - rls and awar - svm assume that the old and new headsets have enough common channels .",
    "we will need to quantify the minimum number of common channels for them to work well , and develop approaches to perform transfer for headsets with none or very few common channels , e.g. , more sophisticated feature extraction methods that allow compensation from close - by electrodes .",
    "2 .   in the current study",
    "each subject performed the same task in three sessions on three different days , with the subject wearing a different headset each day .",
    "the headset difference was the most challenging problem in this transfer learning setting , but there could also be session transfer effects , e.g. , nonstationarity of the brain , mind wandering , distraction , human - system mutual adaptation , environment impacts , physical condition changes , electrode re - positioning , etc . in future research",
    "we will conduct additional experiments , in which each subject wears the same headset in multiple sessions . by comparing the transfer learning performance between sessions with the same headset and between sessions with different headsets",
    ", we can separately study the effects of headset transfer and session transfer .",
    "in this paper , we have introduced two active weighted adaptation regularization approaches , which integrate domain adaptation transfer learning and active learning , to expedite the calibration process when a subject switches to a new eeg headset .",
    "domain adaptation makes use of labeled data from the subject s previous headset , whereas active learning selects the most informative samples from the new headset to be labeled .",
    "experiments on single - trial classification of erps using three different eeg headsets showed that active weighted adaptation regularization can significantly improve the classification performance , given the same number of labeled samples from the new headset ; or , equivalently , it can effectively reduce the number of labeled samples from the new headset , given a desired classification accuracy .",
    "while the current examples are based on intra - subject transfer ( e.g. , same - subject , different headsets ) , our ultimate goal is the application of this approach to more sophisticated preprocessing and feature extraction techniques , such as active weighted adaptation regularization from multiple sources ( e.g. , use data from other subjects and multiple headsets in a new headset calibration ) , and the generalization of weighted adaptation regularization to online bci calibration .",
    "together , these will open the door for a host of applications facilitating bci technology across a wide range of domains .",
    "for example , cross - headset transfer learning , as shown here , will allow data acquired from one research group to be utilized by others , enabling a vast wealth of resources for generating calibration data .",
    "to date , this has not been a possible practice due to a wide variety of hardware used in research settings . however , the techniques discussed here not only suggest feasibility , but also lay the foundation for understanding the most critical features of data acquisition hardware which affect transfer and classifier performance .",
    "this information can , in turn , be used to further refine and propel the system design industry .",
    "the authors would like to thank scott kerick , jean vettel and anthony ries at the us army research laboratory ( arl ) for designing the experiment and collecting the data .",
    "a.  bamdadian , c.  guan , k.  k. ang , and j.  xu , `` improving session - to - session transfer performance of motor imagery - based bci using adaptive extreme learning machine , '' in _ proc .",
    "35th annual intl conf . of the ieee engineering in medicine and biology society ( embc ) _ , osaka , japan , july 2013 , pp",
    ". 21882191 .",
    "m.  belkin , p.  niyogi , and v.  sindhwani , `` manifold regularization : a geometric framework for learning from labeled and unlabeled examples , '' _ journal of machine learning research _ , vol .  7 , pp . 23992434 , 2006 .",
    "y.  benjamini and y.  hochberg , `` controlling the false discovery rate : a practical and powerful approach to multiple testing , '' _ journal of the royal statistical society , series b ( methodological ) _ , vol .",
    "57 , pp . 289300 , 1995 .",
    "chang and c .- j .",
    "lin , `` libsvm : a library for support vector machines , '' _ acm trans . on intelligent systems and technology",
    "_ , vol .  2 , no .  3 , pp .",
    "27:127:27 , 2011 , software available at http://www.csie.ntu.edu.tw/$\\sim$cjlin/libsvm .",
    "m.  chen and x.  tan , `` batch mode active learning algorithm combining with self - training for multiclass brain - computer interfaces , '' _ journal of information & computational science _ , vol .  12 , no .  6 , pp .",
    "23512359 , 2015 .",
    "a.  delorme and s.  makeig , `` eeglab : an open source toolbox for analysis of single - trial eeg dynamics including independent component analysis , '' _ journal of neuroscience methods _ , vol .",
    "134 , pp . 921 , 2004 .",
    "d.  devlaminck , b.  wyns , m.  grosse - wentrup , g.  otte , and p.  santens , `` multisubject learning for common spatial patterns in motor - imagery bci , '' _ computational intelligence and neuroscience _ , vol .",
    "20 , no .  8 , 2011 .        w.  d. hairston , k.  w. whitaker , a.  j. ries , j.  m. vettel , j.  c. bradford , s.  e. kerick , and k.  mcdowell , `` usability of four commercially - oriented eeg systems , '' _ journal of neural engineering _ ,",
    "11 , no .  4 , 2014 .",
    "b.  hamadicharef , `` brain - computer interface ( bci ) literature  a bibliometric study , '' in _ proc .",
    "10th intl . conf . on information sciences",
    "signal processing and their applications _",
    ", kuala lumpur , may 2010 , pp .",
    ".    s .- j .",
    "huang , r.  jin , and z .- h .",
    "zhou , `` active learning by querying informative and representative examples , '' _ ieee trans . on pattern analysis and machine",
    "36 , no .",
    "10 , pp . 19361949 , 2014 .",
    "v.  j. lawhern , d.  j. slayback , d.  wu , and b.  j. lance , `` efficient labeling of eeg signal artifacts using active learning , '' in _ proc .",
    "ieee intl .",
    "conf . on systems ,",
    "man and cybernetics _ , hong kong , october 2015 .",
    "y.  li , h.  kambara , y.  koike , and m.  sugiyama , `` application of covariate shift adaptation techniques in brain - computer interfaces , '' _ ieee trans . on biomedical engineering _ ,",
    "57 , no .  6 , pp .",
    "13181324 , 2010 .",
    "y.  li , y.  koike , and m.  sugiyama , `` a framework of adaptive brain computer interfaces , '' in _ proc .",
    "2nd ieee intl . conf . on biomedical engineering and informatics ( bmel )",
    "_ , tianjin , china , october 2009 .",
    "m.  long , j.  wang , g.  ding , s.  j. pan , and p.  s. yu , `` adaptation regularization : a general framework for transfer learning , '' _ ieee trans . on knowledge and data engineering _ , vol .",
    "26 , no .  5 , pp .",
    "10761089 , 2014 .",
    "f.  lotte and c.  guan , `` learning from other subjects helps reducing brain - computer interface calibration time , '' in _ proc .",
    "ieee intl .",
    "conf . on acoustics",
    "speech and signal processing ( icassp ) _ , dallas , tx , march 2010 .    a.  marathe , v.  lawhern , d.  wu , d.  slayback , and b.  lance , `` improved neural signal classification in a rapid serial visual presentation task using active learning , '' _ ieee trans . on neural systems and rehabilitation engineering _ ,",
    "24 , no .  3 , pp .",
    "333343 , 2016 .",
    "rai , a.  saha , h.  daum , iii , and s.  venkatasubramanian , `` domain adaptation meets active learning , '' in _ proc .",
    "naacl hlt 2010 workshop on active learning for natural language processing _ ,",
    "los angeles , ca , june 2010 , pp . 2732 .",
    "a.  j. ries , j.  touryan , j.  vettel , k.  mcdowell , and w.  d. hairston , `` a comparison of electroencephalography signals acquired from conventional and mobile systems , '' _ journal of neuroscience and neuroengineering _ , vol .  3 , no .  1 ,",
    "pp . 1020 , 2014 .",
    "a.  satti , c.  guan , d.  coyle , and g.  prasad , `` a covariate shift minimisation method to alleviate non - stationarity effects for an adaptive braincomputer interface , '' in _ proc .",
    "20th ieee intl . conf . on pattern recognition ( icpr ) _ ,",
    "istanbul , turkey , august 2010 , pp . 105108 .",
    "m.  spuler , w.  rosenstiel , and m.  bogdan , `` principal component based covariate shift adaption to reduce non - stationarity in a meg - based brain - computer interface , '' _ eurasip journal on advances in signal processing _ , vol .",
    "2012 , no .  1 , pp . 17 , 2012 .",
    "c.  vidaurre , m.  kawanabe , p.  v. bunau , b.  blankertz , and k.  muller , `` toward unsupervised adaptation of lda for brain - computer interfaces , '' _ ieee trans . on biomedical engineering _ , vol .",
    "58 , no .  3 , pp .",
    "587597 , 2011 .",
    "p.  wang , j.  lu , b.  zhang , and z.  tang , `` a review on transfer learning for brain - computer interface classification , '' in _ prof .",
    "5th intl . conf . on information science and technology ( ic1st )",
    "_ , changsha , china , april 2015 .",
    "p.  welch , `` the use of fast fourier transform for the estimation of power spectra : a method based on time averaging over short , modified periodograms , '' _ ieee trans . on audio electroacoustics _ ,",
    "15 , pp . 7073 , 1967 .",
    "d.  wu , c .- h .",
    "chuang , and c .- t .",
    "lin , `` online driver s drowsiness estimation using domain adaptation with model fusion , '' in _ proc .",
    "intl . conf . on affective computing and intelligent interaction _ , xian , china , september 2015 .",
    "d.  wu , b.  j. lance , and v.  j. lawhern , `` active transfer learning for reducing calibration data in single - trial classification of visually - evoked potentials , '' in _ proc .",
    "ieee intl .",
    "conf . on systems ,",
    "man , and cybernetics _ , san diego , ca , october 2014 .",
    "d.  wu , v.  j. lawhern , and b.  j. lance , `` reducing bci calibration effort in rsvp tasks using online weighted adaptation regularization with source domain selection , '' in _ proc .",
    "intl . conf . on affective computing and intelligent interaction _ , xian , china , september 2015 .",
    "d.  wu , v.  j. lawhern , and b.  j. lance , `` reducing offline bci calibration effort using weighted adaptation regularization with source domain selection , '' in _ proc .",
    "ieee intl .",
    "conf . on systems ,",
    "man and cybernetics _ , hong kong , october 2015 .",
    "d.  wu and t.  d. parsons , `` inductive transfer learning for handling individual differences in affective computing , '' in _ proc .",
    "4th intl conf . on affective computing and intelligent interaction _ ,",
    "vol .  2 ,",
    "memphis , tn , october 2011 , pp .",
    "142151 .",
    "y.  zhang , g.  zhou , j.  jin , m.  wang , x.  wang , and a.  cichocki , `` l1-regularized multiway canonical correlation analysis for ssvep - based bci , '' _ ieee trans . on neural systems and rehabilitation engineering _ ,",
    "21 , no .  6 , pp .",
    "887896 , 2013 ."
  ],
  "abstract_text": [
    "<S> electroencephalography ( eeg ) headsets are the most commonly used sensing devices for brain - computer interface . in real - world applications , there are advantages to extrapolating data from one user session to another . however , these advantages are limited if the data arise from different hardware systems , which often vary between application spaces . </S>",
    "<S> currently , this creates a need to recalibrate classifiers , which negatively affects people s interest in using such systems . in this paper </S>",
    "<S> , we employ active weighted adaptation regularization ( awar ) , which integrates weighted adaptation regularization ( war ) and active learning , to expedite the calibration process . </S>",
    "<S> war makes use of labeled data from the previous headset and handles class - imbalance , and active learning selects the most informative samples from the new headset to label . </S>",
    "<S> experiments on single - trial event - related potential classification show that awar can significantly increase the classification accuracy , given the same number of labeled samples from the new headset . in other words , awar can effectively reduce the number of labeled samples required from the new headset , given a desired classification accuracy , suggesting value in collating data for use in wide scale transfer - learning applications .    </S>",
    "<S> eeg ; event - related potential ; visual evoked potential ; single - trial classification ; transfer learning ; domain adaptation ; weighted adaptation regularization ; active learning ; active transfer learning ; active weighted adaptation regularization </S>"
  ]
}