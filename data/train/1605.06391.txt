{
  "article_text": [
    "the paradigm of multi - task learning is to learn multiple related tasks simultaneously so that knowledge obtained from each task can be re - used by the others .",
    "early work in this area focused on neural network models @xcite , while more recent methods have shifted focus to kernel methods , sparsity and low - dimensional task representations of linear models @xcite .",
    "nevertheless given the impressive practical efficacy of contemporary deep neural networks ( dnn)s in many important applications , we are motivated to revisit mtl from a deep learning perspective .",
    "while the machine learning community has focused on mtl for shallow linear models recently , applications have continued to exploit neural network mtl @xcite .",
    "the typical design pattern dates back at least 20 years @xcite : define a dnn with shared lower representation layers , which then forks into separate layers and losses for each task .",
    "the sharing structure is defined manually : full - sharing up to the fork , and full separation after the fork .",
    "however this complicates dnn architecture design because the user must specify the sharing structure : how many task specific layers ?",
    "how many task independent layers ? how to structure sharing if there are many tasks of varying relatedness ?    in this paper",
    "we present a method for end - to - end multi - task learning in dnns .",
    "this contribution can be seen as generalising shallow mtl methods @xcite to learning how to share _ at every layer _ of a deep network ; or as learning the sharing structure for deep mtl @xcite which currently must be defined manually on a problem - by - problem basis .    before proceeding it is worth explicitly distinguishing some different problem settings , which have all been loosely referred to as mtl in the literature .",
    "* homogeneous mtl : * each task corresponds to a _",
    "single _ output .",
    "for example , mnist digit recognition is commonly used to evaluate mtl algorithms by casting it as 10 binary classification tasks @xcite .",
    "* heterogeneous mtl : * each task corresponds to a unique set of output(s ) @xcite .",
    "for example , one may want simultaneously predict a person s age ( task one : multi - class classification or regression ) as well as identify their gender ( task two : binary classification ) from a face image .    in this paper",
    ", we propose a multi - task learning method that works on all these settings .",
    "the key idea is to use tensor factorisation to divide each set of model parameters ( i.e. , both fc weight matrices , and convolutional kernel tensors ) into _ shared _ and _ task - specific _ parts .",
    "it is a natural generalisation of shallow mtl methods that explicitly or implicitly are based on matrix factorisation @xcite . as linear methods ,",
    "these typically require pre - engineered features .",
    "in contrast , as a deep network , our generalisation can learn directly from raw image data , determining sharing structure in a layer - wise fashion .",
    "for the simplest nn architecture  no hidden layer , single output ",
    "our method reduces to matrix - based ones , therefore matrix - based methods including @xcite are special cases of ours .",
    "* multi - task learning * most contemporary mtl algorithms assume that the input and model are both @xmath0-dimensional vectors .",
    "the models of @xmath1 tasks can then be stacked into a @xmath2 sized matrix @xmath3 . despite different motivations and implementations , many matrix - based mtl methods work by placing constrains on @xmath3 .",
    "for example , posing an @xmath4 norm on @xmath3 to encourage low - rank @xmath3 @xcite .",
    "similarly , @xcite factorises @xmath3 as @xmath5 , i.e. , it assigns a lower rank as a hyper - parameter .",
    "an earlier work @xcite proposes that the linear model for each task @xmath6 can be written as @xmath7 .",
    "this is the factorisation @xmath8 $ ] and @xmath9 $ ] .",
    "in fact , such matrix factorisation encompasses many mtl methods .",
    "e.g. , @xcite assumes @xmath10 ( the @xmath11th column of @xmath12 ) is a unit vector generated by a dirichlet process and @xcite models @xmath3 using linear factor analysis with indian buffet process @xcite prior on @xmath12 .    * * tensor factorisation**in deep learning , tensor factorisation has been used to exploit factorised tensors fewer parameters than the original ( e.g. , 4-way convolutional kernel ) tensor , and thus compress and/or speed up the model , e.g. , @xcite . for shallow linear mtl , tensor factorisation has been used to address problems where tasks are described by multiple independent factors rather than merely indexed by a single factor @xcite . here the @xmath0-dimensional linear models for all unique tasks stack into a tensor @xmath13 , of e.g. @xmath14 in the case of two task factors .",
    "knowledge sharing is then achieved by imposing tensor norms on @xmath13 @xcite .",
    "our framework factors tensors for the different reason that for dnn models , parameters include convolutional kernels ( @xmath15-way tensors ) or @xmath16 fc layer weight matrices ( @xmath17-way tensors ) . stacking up these parameters for many tasks results in @xmath18 tensors within which we share knowledge through factorisation .",
    "* heterogeneous mtl and dnns * some studies consider heterogeneous mtl , where tasks may have different numbers of outputs @xcite .",
    "this differs from the previously discussed studies @xcite which implicitly assume that each task has a single output .",
    "heterogeneous mtl typically uses neural networks with multiple sets of outputs and losses .",
    "e.g. , @xcite proposes a shared - hidden - layer dnn model for multilingual speech processing , where each task corresponds to an individual language .",
    "@xcite uses a dnn to find facial landmarks ( regression ) as well as recognise facial attributes ( classification ) ; while @xcite proposes a dnn for query classification and information retrieval ( ranking for web search ) .",
    "a key commonality of these studies is that they all require a user - defined parameter sharing strategy .",
    "a typical design pattern is to use shared layers ( same parameters ) for lower layers of the dnn and then split ( independent parameters ) for the top layers .",
    "however , there is no systematic way to make such design choices , so researchers usually rely on trial - and - error , further complicating the already somewhat dark art of dnn design .",
    "in contrast , our method learns where and how much to share representation parameters across the tasks , hence significantly reducing the space of dnn design choices .    * * parametrised dnns**our mtl approach is a _ parameterised dnn _",
    "@xcite , in that dnn weights are dynamically generated given some side information  in the case of mtl , given the task identity . in a related example of speaker - adaptive speech recognition @xcite",
    "there may be several clusters in the data ( e.g. , gender , acoustic conditions ) , and each speaker s model could be a linear combination of these latent task / clusters models .",
    "they model each speaker @xmath11 s weight matrix @xmath19 as a sum of @xmath20 base models @xmath21 , i.e. , @xmath22 .",
    "the difference between speakers / tasks comes from @xmath23 and the base models are shared .",
    "an advantage of this is that , when new data come , one can choose to re - train @xmath23 parameters only , and keep @xmath21 fixed .",
    "this will significantly reduce the number of parameters to learn , and consequently the required training data . beyond this , @xcite show that it is possible to train another neural network to _ predict _ those @xmath23 values from some abstract metadata .",
    "thus a model for an _ unseen _ task can be generated on - the - fly with _ no _ training instances given an abstract description of the task .",
    "the techniques developed here are compatible with both these ideas of generating models with minimal or no effort .",
    "we first recap some tensor factorisation basics before explaining how to factorise dnn weight tensors for multi - task representation learning .",
    "an @xmath15-way tensor @xmath13 with shape @xmath24 is an @xmath15-dimensional array containing @xmath25 elements .",
    "scalars , vectors , and matrices can be seen as @xmath26 , @xmath27 , and @xmath17-way tensors respectively , although the term tensor is usually used for @xmath28-way or higher .",
    "a mode-@xmath29 fibre of @xmath13 is a @xmath30-dimensional vector obtained by fixing all but the @xmath29th index .",
    "the mode-@xmath29 flattening @xmath31 of @xmath13 is the matrix of size @xmath32 constructed by concatenating all of the @xmath33 mode-@xmath29 fibres along columns .",
    "the dot product of two tensors is a natural extension of matrix dot product , e.g. , if we have a tensor @xmath34 of size @xmath35 and a tensor @xmath36 of size @xmath37 , the tensor dot product @xmath38 will be a tensor of size @xmath39 by matrix dot product @xmath40 and reshaping . more generally , tensor dot product can be performed along specified axes , @xmath41 and reshaping . here the subscripts indicate the axes of @xmath34 and @xmath36 at which dot product is performed .",
    "e.g. , when @xmath34 is of size @xmath42 and @xmath36 is of size @xmath43 , then @xmath44 is a tensor of size @xmath45 .    * * matrix - based knowledge sharing**assume we have @xmath1 linear models ( tasks ) parametrised by @xmath0-dimensional weight vectors , so the collection of all models forms a size @xmath2 matrix @xmath3 .",
    "one commonly used mtl approach @xcite is to place a structure constraint on @xmath3 , e.g. , @xmath46 , where @xmath47 is a @xmath48 matrix and @xmath12 is a @xmath49 matrix .",
    "this factorisation recovers a _ shared _ factor @xmath47 and a _ task - specific _ factor @xmath12 .",
    "one can see the columns of @xmath47 as latent basis tasks , and the model @xmath50 for the @xmath11th task is the linear combination of those latent basis tasks with task - specific information @xmath51 .",
    "@xmath52 * from single to multiple outputs * consider extending this matrix factorisation approach to the case of multiple outputs .",
    "the model for each task is then a @xmath53 matrix , for @xmath54 input and @xmath55 output dimensions .",
    "the collection of all those matrices constructs a @xmath56 tensor . a straightforward extension of eq .",
    "[ eq : gomtl ] to this case is @xmath57 this is equivalent to imposing the same structural constraint on @xmath58 ( transposed mode-@xmath28 flattening of @xmath13 ) .",
    "it is important to note that this allows knowledge sharing across the tasks _",
    "only_. i.e. , knowledge sharing is only across - tasks not across dimensions within a task",
    ". however it may be that the knowledge learned in the mapping to one output dimension may be useful to the others within one task .",
    "e.g. , consider recognising photos of handwritten and print digits  it may be useful to share across handwritten - print ; as well as across different digits within each . in order to support general knowledge sharing across both tasks and outputs within tasks ,",
    "we propose to use more general tensor factorisation techniques .",
    "unlike for matrices , there are multiple definitions of tensor factorisation , and we use tucker @xcite and tensor train ( tt ) @xcite decompositions .      * * tucker decomposition**given an @xmath15-way tensor of size @xmath59 , tucker decomposition outputs a core tensor @xmath60 of size @xmath61 , and @xmath15 matrices @xmath62 of size @xmath63 , such that ,    @xmath64    tucker decomposition is usually implemented by an alternating least squares ( als ) method @xcite . however @xcite treat it as a higher - order singular value decomposition ( hosvd ) , which is more efficient to solve : @xmath62 is exactly the @xmath65 matrix from the svd of mode-@xmath29 flattening @xmath31 of @xmath13 , and the core tensor @xmath60 is obtained by ,    @xmath66    * * tensor train decomposition**tensor train ( tt ) decomposition outputs @xmath17 matrices @xmath67 and @xmath68 of size @xmath69 and @xmath70 respectively , and @xmath71 @xmath28-way tensors @xmath72 of size @xmath73 .",
    "the elements of @xmath13 can be computed by ,    @xmath74    where @xmath75 is a matrix of size @xmath76 sliced from @xmath72 with the second axis fixed at @xmath77 . the tt decomposition is typically realised with a recursive svd - based solution @xcite .    *",
    "* knowledge sharing**if the final axis of the input tensor above indexes tasks , i.e. if @xmath78 then the last factor @xmath68 in both decompositions encodes a matrix of task specific knowledge , and the other factors encode shared knowledge .      to realise deep multi - task representation learning ( dmtrl ) , we learn one dnn per - task each with the same architecture",
    ". however each corresponding layer s weights are generated with one of the knowledge sharing structures in eq .",
    "[ eq : laf_mtl ] , eq .",
    "[ eq : tucker2 ] or eq .",
    "[ eq : tt3 ] .",
    "it is important to note that we apply these ` right - to - left ' in order to generate weight tensors with the specified sharing structure , rather than actually applying tucker or tt to decompose an input tensor . in the forward pass ,",
    "we synthesise weight tensors @xmath13 and perform inference as usual , so the method can be thought of as tensor _ composition _ rather than decomposition .",
    "our weight generation ( construct tensors from smaller pieces ) does not introduce non - differentiable terms , so our deep multi - task representation learner is trainable via standard backpropagation . specifically , in the backward pass over fc layers , rather than directly learning the 3-way tensor @xmath13 , our methods learn either @xmath79 ( dmtrl - tucker , eq .",
    "[ eq : tucker2 ] ) , @xmath80 ( dmtrl - tt , eq .  [ eq : tt3 ] ) , or in the simplest case @xmath81 ( dmtrl - laf , eq .  [ eq : laf_mtl ] ) .",
    "besides fc layers , contemporary dnn designs often exploit convolutional layers . those layers usually contain kernel filter parameters that are @xmath28-way tensors of size @xmath82 , ( where @xmath83 is height",
    ", @xmath3 is width , and @xmath84 is the number of input channels ) or @xmath85-way tensors of size @xmath86 , where @xmath87 is the number of filters in this layer ( i.e. , the number of output channels ) . the proposed methods naturally extend to convolution layers as convolution just adds more axes on the left - hand side .",
    "e.g. , the collection of parameters from a given convolutional layer of @xmath1 neural networks forms a tensor of shape @xmath88 .",
    "these knowledge sharing strategies provide a way to _ softly _ share parameters across the corresponding layers of each task s dnn : where , what , and how much to share are learned from data .",
    "this is in contrast to the conventional deep - mtl approach of manually selecting a set of layers to undergo _ hard _ parameter sharing : by tying weights so each task uses exactly the same weight matrix / tensor for the corresponding layer @xcite ; and a set of layers to be completely separate : by using independent weight matrices / tensors .",
    "in contrast our approach benefits from : ( i ) automatically learning this sharing structure from data rather than requiring user trial and error , and ( ii ) smoothly interpolating between fully shared and fully segregated layers , rather than a hard switching between these states .",
    "an illustration of the proposed framework for different problem settings can be found in fig .",
    "[ fig : demo ] .",
    "* * implementation details**our method is implemented with tensorflow @xcite .",
    "the code is released on github . for dmtrl - tucker , dmtrl - tt , and dmtrl - laf",
    ", we need to assign the rank of each weight tensor",
    ". the dnn architecture itself may be complicated and so can benefit from different ranks at different layers , but grid - search is impractical .",
    "however , since both tucker and tt decomposition methods have svd - based solutions , and vanilla svd is directly applicable to dmtrl - laf , we can initialise the model and set the ranks as follows : first train the dnns independently in single task learning mode . then pack the layer - wise parameters as the input for tensor decomposition .",
    "when svd is applied , set a threshold for relative error so svd will pick the appropriate rank .",
    "thus our method needs only a _ single _",
    "hyper parameter of max reconstruction error ( we set to @xmath89 throughout ) that indirectly specifies the ranks of every layer . note that training from random initialisation also works , but the stl - based initialisation makes rank selection easy and transparent .",
    "nevertheless , like @xcite the framework is not sensitive to rank choice so long as they are big enough .",
    "if random initialisation is desired to eliminate the pre - training requirement , good practice is to initialise parameter tensors by a suitable random weight distribution first , then do decomposition , and use the decomposed values for initialising the factors ( the _ real _ learnable parameters in our framework ) . in this way ,",
    "the resulting re - composed tensors will have approximately the intended distribution .",
    "our sharing is applied to weight parameters only , bias terms are not shared .",
    "apart from initialisation , decomposition is not used anywhere .",
    "* dataset , settings and baselines * we use mnist handwritten digits .",
    "the task is to recognise digit images zero to nine .",
    "when this dataset is used for the evaluation of mtl methods , ten 1-vs - all binary classification problems usually define ten tasks @xcite .",
    "the dataset has a given train ( 60,000 images ) and test ( 10,000 images ) split .",
    "each instance is a monochrome image of size @xmath90 .",
    "we use a modified lenet @xcite as the cnn architecture .",
    "the first convolutional layer has @xmath91 filters of size @xmath92 , followed by @xmath93 max pooling .",
    "the second convolutional layer has @xmath94 filters of size @xmath95 , and again a @xmath96 max pooling .",
    "after these two convolutional layers , two fully connected layers with @xmath97 and @xmath27 output(s ) are placed sequentially .",
    "the convolutional and first fc layer use relu @xmath98 activation function .",
    "we use hinge loss , @xmath99 , where @xmath100 is the true label and @xmath101 is the output of each task s neural network .",
    "conventional matrix - based mtl methods @xcite are linear models taking vector input only , so they need a preprocessing that flattens the image into a vector , and typically reduce dimension by pca . as per our motivation for studying _ deep _ mtl , our methods decisively outperform such shallow linear baselines .",
    "thus to find a stronger mtl competitor , we instead search user defined architectures for deep - mtl parameter sharing ( cf @xcite ) . in all of the four parametrised layers",
    "( pooling has no parameters ) , we set the first @xmath15 ( @xmath102 ) to be _ hard _ shared .",
    "we then use cross - validation to select among the three user - defined mtl architectures and the best option is @xmath103 , i.e. , the first three layers are fully shared ( we denote this model ud - mtl ) . for our methods ,",
    "all four parametrised layers are softly shared with the different factorisation approaches . to evaluate different mtl methods and a baseline of single task learning ( stl )",
    ", we take ten different fractions of the given 60k training split , train the model , and test on the 10k testing split . for each fraction , we repeat the experiment @xmath104 times with randomly sampled training data .",
    "we report two performance metrics : ( 1 ) the mean error rate of the ten binary classification problems and ( 2 ) the error rate of recognising a digit by ranking each task s 1-vs - all output ( multi - class classification error ) .        * * results**as we can see in fig .",
    "[ fig : mnist ] , all mtl approaches outperform stl , and the advantage is more significant when the training data is small .",
    "the proposed methods , dmtrl - tt and dmtrl - tucker outperform the best user - defined mtl when the training data is very small , and their performance is comparable when the training data is large .    * * further discussion**for a slightly unfair comparison , in the case of binary classification with 1000 training data , shallow matrix - based mtl methods with pca feature @xcite reported @xmath105 / @xmath106 error rate . with the same amount of data , our methods have error rate below @xmath107 .",
    "this shows the importance of our deep end - to - end multi - task representation learning contribution versus conventional shallow mtl .",
    "since the error rates in @xcite were produced on a private subset of mnist dataset with pca representations only , to ensure a direct comparison , we implement several classic mtl methods and compare them in appendix  [ appx2 ] .    for readers interested in the connection to model capacity ( number of parameters ) , we present further analysis in appendix  [ appx1 ] .      * * dataset , settings and baselines**the adiencefaces @xcite is a large - scale face images dataset with the labels of each person s gender and age group .",
    "we use this dataset for the evaluation of heterogeneous mtl with two tasks : ( i ) gender classification ( two classes ) and ( ii ) age group classification ( eight classes ) .",
    "two independent cnn models for this benchmark are introduced in @xcite .",
    "the two cnns have the same architecture except for the last fully - connected layer , since the heterogeneous tasks have different number of outputs ( two / eight ) .",
    "we take these cnns from @xcite as the stl baseline .",
    "we again search for the best possible user - defined mtl architecture as a strong competitor : the proposed cnn has six layers  three convolutional and three fully - connected layers .",
    "the last fully - connected layer has non - shareable parameters because they are of different size . to search the mtl design - space ,",
    "we try setting the first @xmath15 ( @xmath108 ) layers to be hard shared between the tasks . running 5-fold cross - validation on the train set to evaluate the architectures",
    ", we find the best choice is @xmath109 ( i.e. , all layers fully shared before the final heterogeneous outputs ) . for our proposed methods , all the layers before the last heterogeneous dimensionality fc layers are softly shared .",
    "we select increasing fractions of the adiencefaces train split randomly , train the model , and evaluate on the same test set . for reference ,",
    "there are 12245 images with gender labelled for training , 4007 ones for testing , and 11823 images with age group labelled for training , and 4316 ones for testing .        * * results**fig .",
    "[ fig : ga ] shows the error rate for each task . for the gender recognition task ,",
    "we find that : ( i ) user - defined mtl is not consistently better than stl , but ( ii ) our methods , esp . , dmtrl - tucker , consistently outperform both stl and the best user - defined mtl . for the harder age group classification task , our methods generally improve on stl",
    ". however ud - mtl does not consistently improve on stl , and even reduces performance when the training set is bigger .",
    "this is the negative transfer phenomenon @xcite , where using a transfer learning algorithm is worse than not using it .",
    "this difference in outcomes is attributed to sufficient data eventually providing some effective task - specific representation .",
    "our methods can discover and exploit this , but ud - mtl s hard switch between sharing and not sharing can not represent or exploit such increasing task - specificity of representation .      * * dataset , settings and baselines**we next consider the task of learning to recognise handwritten letters _ in multiple languages _ using the omniglot @xcite dataset .",
    "omniglot contains handwritten characters in 50 different alphabets ( e.g. , cyrillic , korean , tengwar ) , each with its own number of unique characters ( @xmath110 ) . in total , there are 1623 unique characters , and each has exactly 20 instances .",
    "here each task corresponds to an alphabet , and the goal is to recognise its characters .",
    "mtl has a clear motivation here , as cross - alphabet knowledge sharing is likely to be useful as one is unlikely to have extensive training data for a wide variety of less common alphabets .",
    "the images are monochrome of size @xmath111 .",
    "we design a cnn with @xmath28 convolutional and @xmath17 fc layers .",
    "the first conv layer has @xmath112 filters of size @xmath92 ; the second conv layer has @xmath113 filters of size @xmath114 , and the third convolutional layer has @xmath115 filters of size @xmath114 .",
    "each convolutional layer is followed by a @xmath93 max - pooling .",
    "the first fc layer has @xmath94 neurons , and the second fc layer has size corresponding to the number of unique classes in the alphabet .",
    "the activation function is @xmath116 .",
    "we use a similar strategy to find the best user - defined mtl model : the cnn has @xmath104 parametrised layers , of which @xmath85 layers are potentially shareable .",
    "so we tried hard - sharing the first @xmath15 ( @xmath117 ) layers . evaluating these options by @xmath104-fold cross - validation ,",
    "the best option turned out to be @xmath103 , i.e. , the first _ three _ layers are hard shared . for our methods ,",
    "_ all four _ shareable layers are softly shared .",
    "since there is no standard train / test split for this dataset , we use the following setting : we repeatedly pick at random @xmath118 of images per class for training . note that @xmath119 is the minimum , corresponding to one - shot learning .",
    "the remaining data are used for evaluation .    * results * fig .  [",
    "fig : alphabet ] reports the average error rate across all @xmath120 tasks ( alphabets ) .",
    "our proposed mtl methods surpass the stl baseline in all cases .",
    "user - defined mtl does not work well when the training data is very small , but does help when training fraction is larger than @xmath121 .",
    "* measuring the learned sharing * compared to the conventional user - defined sharing architectures , our method learns how to share from data .",
    "we next try to quantify the amount of sharing estimated by our model on the omniglot data . returning to the key factorisation @xmath122",
    ", we can find that @xmath12-like matrix appears in all variants of proposed method .",
    "it is @xmath12 in dmtrl - laf , the transposed @xmath123 in dmtrl - tucker , and @xmath68 in dmtrl - tt ( @xmath15 is the last axis of @xmath13 ) .",
    "@xmath12 is a @xmath49 size matrix , where @xmath1 is the number of tasks , and @xmath20 is the number of latent tasks @xcite or the dimension of task coding @xcite .",
    "each column of @xmath12 is a set of coefficients that produce the final weight matrix / tensor by linear combination .",
    "if we put stl and user - defined mtl ( for a certain shared layer ) in this framework , we see that stl is to _ assign _ ( rather than _ learn _ ) @xmath12 to be an identity matrix @xmath124 .",
    "similarly , user - defined mtl ( for a certain shared layer ) is to assign @xmath12 to be a matrix with all zeros but one particular row is all ones , e.g. , @xmath125 $ ] . between these two extremes , our method learns the sharing structure in @xmath12 .",
    "we propose the following equation to measure the learned sharing strength : @xmath126     +     here @xmath127 is a similarity measure for two vectors @xmath128 and @xmath129 and we use cosine similarity .",
    "@xmath130 is the average on all combinations of column - wise similarity .",
    "so @xmath130 measures how much sharing is encoded by @xmath12 between @xmath131 for stl ( nothing to share ) and @xmath132 for user - defined mtl ( completely shared ) . since",
    "@xmath12 is a real - valued matrix in our scenario , we normalise it before applying eq .  [ eq : rho ] : first we take absolute values , because large either positive or negative value suggests a significant coefficient .",
    "second we normalise each column of @xmath12 by applying a softmax function , so the sum of every column is @xmath27 .",
    "the motivation behind the second step is to make a matched range of our @xmath12 with @xmath133 or @xmath125 $ ] , as for those two cases , the sum of each column is @xmath27 and the range is @xmath134 $ ] .    for the omniglot experiment , we plot the measured sharing amount for training fraction @xmath135 .",
    "[ fig : alphabet ] reveals that three proposed methods tend to share more for bottom layers ( ` conv1 ' , ` conv2 ' , and ` conv3 ' ) and share less for top layer ( ` fc1 ' ) .",
    "this is qualitatively similar to the best user - defined mtl , where the first three layers are fully shared ( @xmath132 ) and the @xmath85th layer is completely not shared ( @xmath131 ) .",
    "however , our methods : ( i ) learn this structure in a purely data - driven way and ( ii ) benefits from the ability to smoothly interpolate between high and low degrees of sharing as depth increases . as an illustration , fig .",
    "[ fig : alphabet ] also shows example text from the most and least similar language pairs as estimated at our multilingual character recogniser s fc1 layer ( the result can vary across layers ) .",
    "in this paper , we propose a novel framework for end - to - end multi - task representation learning in contemporary deep neural networks . the key idea is to generalise matrix factorisation - based multi - task ideas to tensor factorisation , in order to flexibly share knowledge in fully connected and convolutional dnn layers .",
    "our method provides consistently better performance than single task learning and comparable or better performance than the best results from exhaustive search of user - defined mtl architectures .",
    "it reduces the design choices and architectural search space that must be explored in the workflow of deep mtl architecture design @xcite , relieving researchers of the need to decide how to structure layer sharing / segregation .",
    "instead sharing structure is determined in a data - driven way on a layer - by - layer basis that moreover allows a smooth interpolation between sharing and not sharing in progressively deeper layers .    * * acknowledgements**this work was supported by epsrc ( ep / l023385/1 ) , and the european union s horizon 2020 research and innovation program under grant agreement no 640891 .",
    "32 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    martn abadi , ashish agarwal , paul barham , eugene brevdo , zhifeng chen , craig citro , greg  s. corrado , andy davis , jeffrey dean , matthieu devin , sanjay ghemawat , ian goodfellow , andrew harp , geoffrey irving , michael isard , yangqing jia , rafal jozefowicz , lukasz kaiser , manjunath kudlur , josh levenberg , dan man , rajat monga , sherry moore , derek murray , chris olah , mike schuster , jonathon shlens , benoit steiner , ilya sutskever , kunal talwar , paul tucker , vincent vanhoucke , vijay vasudevan , fernanda vigas , oriol vinyals , pete warden , martin wattenberg , martin wicke , yuan yu , and xiaoqiang zheng . : large - scale machine learning on heterogeneous systems , 2015 .",
    "url http://tensorflow.org/. software available from tensorflow.org .",
    "andreas argyriou , theodoros evgeniou , and massimiliano pontil .",
    "convex multi - task feature learning .",
    "_ machine learning _ , 2008 .",
    "edwin  v bonilla , kian  m chai , and christopher williams . multi - task gaussian process prediction . in _ neural information processing systems ( nips ) _ , 2007 .",
    "rich caruana .",
    "multitask learning .",
    "_ machine learning _ , 1997 .",
    "hal daum iii . frustratingly easy domain adaptation . in _",
    "acl _ , 2007 .",
    "eran eidinger , roee enbar , and tal hassner .",
    "age and gender estimation of unfiltered faces .",
    "_ ieee transactions on information forensics and security _ , 2014 .",
    "theodoros evgeniou and massimiliano pontil .",
    "regularized multi ",
    "task learning . in _",
    "knowledge discovery and data mining ( kdd ) _ , 2004 .",
    "thomas  l. griffiths and zoubin ghahramani .",
    "the indian buffet process : an introduction and review .",
    "_ journal of machine learning research ( jmlr ) _",
    ", 2011 .",
    "jui - ting huang , jinyu li , dong yu , li  deng , and yifan gong .",
    "cross - language knowledge transfer using multilingual deep neural network with shared hidden layers . in _",
    "international conference on acoustics , speech , and signal processing ( icassp ) _",
    ", 2013 .",
    "laurent jacob , jean - philippe vert , and francis  r bach .",
    "clustered multi - task learning : a convex formulation . in _ neural information processing systems ( nips ) _ , 2009 .",
    "zhuoliang kang , kristen grauman , and fei sha .",
    "learning with whom to share in multi - task feature learning . in _ international conference on machine learning ( icml ) _ , 2011 .",
    "tamara  g. kolda and brett  w. bader .",
    "tensor decompositions and applications .",
    "_ siam review _",
    ", 2009 .",
    "abhishek kumar and hal daum iii .",
    "learning task grouping and overlap in multi - task learning . in _ international conference on machine learning ( icml ) _ , 2012 .",
    "brenden  m. lake , ruslan salakhutdinov , and joshua  b. tenenbaum .",
    "human - level concept learning through probabilistic program induction .",
    "_ science _ ,",
    "lieven  de lathauwer , bart  de moor , and joos vandewalle . a multilinear singular value decomposition . _ siam journal on matrix analysis and applications _ , 2000 .",
    "vadim lebedev , yaroslav ganin , maksim rakhuba , ivan  v. oseledets , and victor  s. lempitsky .",
    "speeding - up convolutional neural networks using fine - tuned cp - decomposition . in _",
    "international conference on learning representations ( iclr ) _",
    ", 2015 .",
    "y.  lecun , l.  bottou , y.  bengio , and p.  haffner .",
    "gradient - based learning applied to document recognition .",
    "_ proceedings of the ieee _ , 1998 .",
    "g.  levi and t.  hassncer .",
    "age and gender classification using convolutional neural networks . in _ computer vision and pattern recognition workshops ( cvprw ) _ , 2015 .",
    "xiaodong liu , jianfeng gao , xiaodong he , li  deng , kevin duh , and ye - yi wang .",
    "representation learning using multi - task deep neural networks for semantic classification and information retrieval .",
    "_ naacl _ , 2015 .",
    "alexander novikov , dmitry podoprikhin , anton osokin , and dmitry vetrov .",
    "tensorizing neural networks . in _ neural information processing systems ( nips )",
    "_ , 2015 .",
    "i.  v. oseledets . tensor - train decomposition .",
    "_ siam journal on scientific computing _ , 2011 .",
    "alexandre passos , piyush rai , jacques wainer , and hal daum iii .",
    "flexible modeling of latent task structures in multitask learning . in _",
    "international conference on machine learning ( icml ) _",
    ", 2012 .",
    "bernardino romera - paredes , hane aung , nadia bianchi - berthouze , and massimiliano pontil .",
    "multilinear multitask learning . in _",
    "international conference on machine learning ( icml ) _ , 2013 .",
    "michael  t. rosenstein , zvika marx , leslie  pack kaelbling , and thomas  g. dietterich . to transfer or not to transfer .",
    "in _ in nips workshop , inductive transfer : 10 years later _",
    ", 2005 .",
    "olivier sigaud , clement masson , david filliat , and freek stulp .",
    "gated networks : an inventory .",
    "_ arxiv _ , 2015 .",
    "sigurd spieckermann , steffen udluft , and thomas runkler .",
    "data - effiicient temporal regression with multitask recurrent neural networks . in _ nips workshop on transfer and multi - task learning _ , 2014 .",
    "tian tan , yanmin qian , and kai yu .",
    "cluster adaptive training for deep neural network based acoustic model .",
    "_ ieee / acm trans .",
    "audio , speech & language processing _",
    ", 240 ( 3):0 459468 , 2016",
    ".    l.  r. tucker .",
    "some mathematical notes on three - mode factor analysis .",
    "_ psychometrika _ , 1966 .",
    "kishan wimalawarne , masashi sugiyama , and ryota tomioka .",
    "multitask learning meets tensor factorization : task imputation via convex optimization . in _ neural information processing systems ( nips ) _ , 2014 .",
    "ya  xue , xuejun liao , lawrence carin , and balaji krishnapuram .",
    "multi - task learning for classification with dirichlet process priors .",
    "_ journal of machine learning research ( jmlr ) _",
    ", 2007 .",
    "yongxin yang and timothy  m. hospedales . a unified perspective on multi - domain and multi - task learning . in _",
    "international conference on learning representations ( iclr ) _ , 2015 .",
    "zhanpeng zhang , ping luo , chen  change loy , and xiaoou tang .",
    "facial landmark detection by deep multi - task learning . in _",
    "european conference on computer vision ( eccv ) _ , 2014 .",
    "we provide a comparison with classic ( shallow , matrix - based ) mtl methods for the first experiment ( mnist , binary one - vs - rest classification , @xmath136 training data , mean of error rates for 10-fold cv ) . a subtlety in making this comparison is",
    "what feature should the classic methods use ? conventionally they use a pca feature ( obtained by flattening the image , then dimension reduction by pca ) .",
    "however for visual recognition tasks , performance is better with deep features  a key motivation for our focus on deep approaches to mtl .",
    "we therefore also compare the classic methods when using a feature extracted from the penultimate layer of the cnn network used in our experiment .       the conventional hard - sharing method",
    "( ud - mtl ) design is to share all layers except the top layer .",
    "its number of parameter is roughly @xmath135 of the single task learning method ( stl ) , as most parameters are shared across the 10 tasks corresponding to 10 digits .",
    "our soft - sharing methods also significantly reduce the number of parameters compared to stl , but are larger than ud - mtl s hard sharing .    to compare our method to ud - mtl ,",
    "while controlling for network capacity , we expanded ud - mdl by adding more hidden neurons so its number of parameter is close to our methods ( denoted ud - mtl - large )",
    ". however ud - mdl performance does not increase .",
    "this is evidence that our model s good performance is not simply due to greater capacity than ud - mtl ."
  ],
  "abstract_text": [
    "<S> most contemporary multi - task learning methods assume linear models . </S>",
    "<S> this setting is considered _ shallow _ in the era of deep learning . in this paper , we present a new deep multi - task representation learning framework that learns cross - task sharing structure _ at every layer in a deep network_. our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional mtl algorithms to tensor factorisation , to realise automatic learning of end - to - end knowledge sharing in deep networks . </S>",
    "<S> this is in contrast to existing deep learning approaches that need a user - defined multi - task sharing strategy . </S>",
    "<S> our approach applies to both homogeneous and heterogeneous mtl . </S>",
    "<S> experiments demonstrate the efficacy of our deep multi - task representation learning in terms of both higher accuracy and fewer design choices . </S>"
  ]
}