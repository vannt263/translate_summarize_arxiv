{
  "article_text": [
    "the inverse gaussian distribution ( igd ) @xcite is widely used in a variety of application areas including reliability and survival analysis @xcite .",
    "it is more generally used for modeling non - negative positively skewed data because of its connections to exponential families and generalized linear models @xcite .",
    "our aim in this article is to develop reliable software for this distribution for the r programming environment ( http://www.r-project.org ) .",
    "basic probability functions for the igd have been implemented previously in james lindsey s r package rmutil @xcite and in the cran packages suppdists @xcite and star @xcite .",
    "we have found however that none of these igd functions work for all parameter values or return results to full machine accuracy .",
    "bob wheeler remarks in the suppdists documentation that the igd `` is an extremely difficult distribution to treat numerically '' .",
    "the rmutil package was removed from cran in 1999 but is still available from lindsey s webpage ( http://www.commanster.eu/rcode.html ) .",
    "suppdists was orphaned in 2013 but is still available from cran . the suppdists code is mostly implemented in c while the other packages are pure r as far as the igd functions are concerned .",
    "the probability density of the igd has a simple closed form expression and so is easy to compute .",
    "care is still required though to handle infinite parameter values that correspond to valid limiting cases .",
    "the cumulative distribution function ( cdf ) is also available in closed form via an indirect relationship with the normal distribution @xcite .",
    "considerable care is nevertheless required to compute probabilities accurately on the log - scale , because the formula involves a sum of two normal probabilities on the un - logged scale .",
    "random variates from igds can be generated using a combination of chisquare and binomial random variables @xcite .",
    "most difficult is the inverse cdf or quantile function , which must be computed by some iterative numerical approximation .",
    "two strategies have been used to compute igd quantiles .",
    "one is to solve for the quantile using a general - purpose equation solver such as the ` uniroot ` function in r. this is the approach taken by the ` qinvgauss ` functions in the rmutil and star packages .",
    "this approach can usually be relied on to converge satisfactorily but is computationally slow and provides only limited precision .",
    "the other approach is to use newton s method to solve the equation after applying an initial approximation @xcite .",
    "this approach was taken by one of the current authors when developing inverse gaussian code for s - plus @xcite .",
    "it is also the approach taken by the ` qinvgauss ` function in the suppdists package .",
    "this approach is fast and accurate when it works but can fail unpredictably when the newton iteration diverges .",
    "newton s method can not in general be guaranteed to converge , even when the initial approximation is close to the required value , and the parameter values for which divergence occurs are hard to predict .",
    "we have resolved the above difficulties by developing a newton iteration for the igd quantiles that has guaranteed convergence . instead of attempting to find a starting value that is close to the required solution",
    ", we instead use the convexity properties of the cdf function to approach the required quantiles in a predictable fashion .",
    "we show that newton s method for finding the quantiles of an igd always converges when started from the mode of the distribution .",
    "furthermore the convergence is monotonic , so that backtracking is eliminated .",
    "newton s method is eventually quadratically convergent , meaning that the number of decimal places corrected determined tends to double with each iteration @xcite .",
    "although the starting value may be far from the required solution , the rapid convergence means the starting value is quickly left behind .",
    "convergence tends to be rapid even when the required quantile in the extreme tails of the distribution .",
    "the above methods have been implemented in the ` dinvgauss ` , ` pinvgauss ` , ` qinvgauss ` and ` rinvgauss ` functions of the statmod package @xcite .",
    "the functions give close to machine accuracy for all possible parameter values .",
    "they obey similar conventions to the probability functions provided in the stats package that is bundled with r. tests show that the functions are faster , more accurate and more reliable than existing functions for the igd .",
    "every effort has to made to ensure that the functions return results for the widest possible range of parameter values .",
    "the inverse gaussian distribution , denoted ig(@xmath0,@xmath1 ) , has probability density function ( pdf ) @xmath2 for @xmath3 , @xmath4 and @xmath5 .",
    "the mean of the distribution is @xmath0 and the variance is @xmath6 . in generalized linear model theory",
    "@xcite , @xmath1 is called the _ dispersion _ parameter .",
    "another popular parametrization of the igd uses @xmath7 , which we call the _ shape _ parameter . for best accuracy ,",
    "we compute @xmath8 on the log - scale and then exponentiate if an unlogged value is required .    note that the mean @xmath0 can be viewed as a scaling parameter : if @xmath9 is distributed as ig(@xmath0,@xmath1 ) , then @xmath10 is also inverse gaussian with mean @xmath11 and dispersion @xmath12 .",
    "the skewness of the distribution is therefore determined by @xmath12 , and in fact @xmath12 is the squared coefficient of variation of the distribution .     with @xmath13 .",
    "the right panel shows densities for different @xmath0 for @xmath14 .",
    "the densities are unimodal with mode between 0 and @xmath0 .",
    "as @xmath15 increases the distribution becomes more right skew and the mode decreases relative to the mean . note that @xmath7 . ]",
    "the igd is unimodal with mode at @xmath16 where @xmath17 @xcite .",
    "the second factor in the mode is strictly between 0 and 1 , showing that the mode is strictly between 0 and @xmath0 .",
    "figure  [ fig : pdf ] shows the pdf of the igd for various choices of @xmath0 and @xmath18 .",
    ".probability density function values for special cases of the parameter values .",
    "the pdf values for infinite parameters are theoretical limit values . [ cols=\"<,^,^,^,^\",options=\"header \" , ]     care needs to be taken with special cases when evaluating the pdf ( table  [ tab : specialcases ] ) .",
    "when @xmath12 is large , a taylor series expansion shows that the mode becomes dependent on @xmath1 only : @xmath19 under the same conditions , the peak value of the density can be seen to converge to @xmath20 @xmath21 .",
    "this shows that the distribution has a spike at 0 whenever @xmath1 is very large , regardless of @xmath0 .",
    "it is also known that @xmath22 @xcite . amongst other things",
    ", this implies that @xmath23 asymptotically for @xmath0 large . for infinite @xmath0 ,",
    "the density becomes @xmath24 the pdf is always ` na ` if @xmath25 is ` na ` .",
    "missing values for @xmath1 lead to ` na ` values for the pdf except when @xmath26 or @xmath27 . missing values for @xmath0 lead to ` na ` values for the pdf except when @xmath26 , @xmath27 or @xmath28 .",
    "next we give some code examples .",
    "we start by loading the packages that we will compare .",
    "note that statmod is loaded last and is therefore first in the search path .",
    "> library(rmutil ) > library(suppdists ) >",
    "library(star ) >",
    "library(statmod )    the statmod ` dinvgauss ` function checks for out - of - range or missing values :    > options(digits = 3 ) > dinvgauss(c(-1 , 0 , 1 , 2 , inf , na ) , mean = 1.5 , dispersion = 0.7 ) [ 1 ] 0.000 0.000 0.440 0.162 0.000 na    infinite mean corresponds to an inverse - chisquare case :    > dinvgauss(c(-1 , 0 , 1 , 2 , inf , na ) , mean = inf , dispersion = 0.7 ) [ 1 ] 0.000 0.000 0.233 0.118 0.000 na    infinite dispersion corresponds to a spike at 0 regardless of the mean :    > dinvgauss(c(-1 , 0 , 1 , 2 , inf , na ) , mean = na , dispersion = inf ) [ 1 ] 0 inf 0 0 0 na    extreme @xmath25 values have zero density regardless of the mean or dispersion :    > dinvgauss(c(-1 , 0 , 1 , inf ) , mean = na , dispersion = na ) [ 1 ] 0 na na 0    all the existing functions ` rmutil::dinvgauss ` , ` suppdist::dinvgauss ` and ` star::dinvgauss ` return errors for the above calls ; they do not tolerate ` na ` values , or infinite parameter values , or @xmath25 values outside the support of the distribution .",
    "let @xmath29 be the left tail cdf , and write @xmath30 for the right tail probability @xmath31 .",
    "the formula developed by @xcite for the cdf is @xmath32 where @xmath33 , @xmath34 , @xmath35 and @xmath36 is the cdf of the standard normal distribution .",
    "the right tail probability can be written similarly : @xmath37 where @xmath38 is the right tail of the standard normal .",
    "the fact that this formula is additive on the unlogged scale poses some numerical problems .",
    "the @xmath39 evaluations are subject to floating underflow , the @xmath40 evaluation is subject to overflow , and there is the danger of subtractive cancellation when computing the right tail probability .",
    "it is possible to derive an asymptotic expression for the right tail probability .",
    "if @xmath41 is very large then : @xmath42 see the appendix for the derivation of this approximation .",
    "this approximation is very accurate when @xmath43 , but only gives 23 significant figures correctly for more modest values such as @xmath44 .    to avoid or minimize the numerical problems described above , we convert the terms in the cdf to the log - scale and remove a common factor before combining the two term terms to get @xmath45 . given a quantile value @xmath41 , we compute the corresponding @xmath45 as follows : @xmath46 where @xmath47 is computed by ` pnorm ` with ` lower.tail=true ` and ` log.p = true ` .",
    "note also that ` log1p ( ) ` is an r function that computes the logarithm of one plus its argument avoiding subtractive cancellation for small arguments .",
    "the computation of the right tail probability is similar but with @xmath48 because of this careful computation , ` statmod::pinvgauss ` function is able to compute correct cdf values even in the far tails of the distribution :    > options(digits = 4 ) > pinvgauss(0.001 , mean = 1.5 , disp = 0.7 ) [ 1 ] 3.368e-312 > pinvgauss(110 , mean = 1.5 , disp = 0.7 , lower.tail = false ) [ 1 ] 2.197e-18    none of the existing functions can distinguish such small left tail probabilities from zero :    > rmutil::pinvgauss(0.001 , m = 1.5 , s = 0.7 ) [ 1 ] 0 > suppdists::pinvgauss(0.001 , nu = 1.5 , lambda = 1/0.7 ) [ 1 ] 0 > star::pinvgauss(0.001 , mu = 1.5 , sigma2 = 0.7 ) [ 1 ] 0    ` rmutil::pinvgauss ` does nt compute right tail probabilities . ` star::pinvgauss ` does but ca nt distinguish right tail probabilities less than ` 1e-17 ` from zero :    > star::pinvgauss(110 , mu = 1.5 , sigma2 = 0.7 , lower.tail = false ) [ 1 ] 0    ` suppdists::pinvgauss ` returns non - zero right tail probabilities , but these are too large by a factor of 10 :    > suppdists::pinvgauss(110 , nu = 1.5 , lambda = 1/0.7 , lower.tail = false ) [ 1 ] 2.935e-17    the use of log - scale computations means that ` statmod::pinvgauss ` can accurately compute log - probabilities that are too small to be represented on the unlogged scale :    > pinvgauss(0.0001 , mean = 1.5 , disp = 0.7 , log.p = true ) [ 1 ] -7146.914    none of the other packages can compute log - probabilities less than about @xmath49 .    `",
    "pinvgauss ` handles special cases similarly to ` dinvgauss ` ( table  [ tab : specialcases ] ) . again",
    ", none of the existing functions do this :    > pinvgauss(c(-1 , 0 , 1 , 2 , inf , na ) , mean = 1.5 , dispersion = 0.7 ) [ 1 ] 0.0000 0.0000 0.5009 0.7742 1.0000 na    infinite mean corresponds to an inverse - chisquare case :    > pinvgauss(c(-1 , 0 , 1 , 2 , inf , na ) , mean = inf , dispersion = 0.7 ) [ 1 ] 0.000 0.000 0.232 0.398 1.000 na    infinite dispersion corresponds to a spike at 0 regardless of the mean :    > pinvgauss(c(-1 , 0 , 1 , 2 , inf , na ) , mean = na , dispersion = inf ) [ 1 ] 0 1 1 1 1 na    extreme @xmath25 values have cdf equal to 0 or 1 regardless of the mean or dispersion :    > pinvgauss(c(-1 , 0 , 1 , inf ) , mean = na , dispersion = na ) [ 1 ] 0 na na 1    we can test the accuracy of the cdf functions by comparing to the cdf of the @xmath50 distribution . for any @xmath51 ,",
    "let @xmath52 be that value satisfying @xmath53 from equation  [ chisq ] , we can conclude that the upper tail probability for the @xmath50 distribution at @xmath54 should be the sum of the igd tail probabilities for @xmath55 and @xmath56 , i.e. , @xmath57 the following code implements this process for an illustrative example with @xmath58 , @xmath59 and @xmath60 .",
    "first we have to solve for @xmath56 :    > options(digits = 4 ) > mu < - 1.5 > phi < - 0.7 > q1 < - 0.1 > z < - ( q1 - mu)^2 / ( phi * mu^2 * q1 ) > polycoef",
    "< - c(mu^2 , -2 * mu - phi * mu^2 * z , 1 ) > q < - re(polyroot(polycoef ) ) > q [ 1 ] 0.1 22.5    the chisquare cdf value corresponding to the left hand size of equation  [ chisqcdf ] is :    > options(digits = 18 ) > pchisq(z , df = 1 , lower.tail = false ) [ 1 ] 0.00041923696954098788    now we compute the right hand size of equation  [ chisqcdf ] using each of the igd packages , starting with statmod :    > pinvgauss(q[1 ] , mean = mu , disp = phi ) + + pinvgauss(q[2 ] , mean = mu , disp = phi , lower.tail = false ) [ 1 ] 0.00041923696954098701 > rmutil::pinvgauss(q[1 ] , m = mu , s = phi ) + + 1 - rmutil::pinvgauss(q[2 ] , m = mu , s = phi ) [ 1 ] 0.00041923696954104805 > suppdists::pinvgauss(q[1 ] , nu = mu , lambda = 1/phi ) + + suppdists::pinvgauss(q[2 ] , nu = mu , lambda = 1/phi , lower.tail = false ) [ 1 ] 0.00041923696954101699 > star::pinvgauss(q[1 ] , mu = mu , sigma2 = phi ) + + star::pinvgauss(q[2 ] , mu = mu , sigma2 = phi , lower.tail = false ) [ 1 ] 0.00041923696954100208    it can be seen that the statmod function is the only one to agree with ` pchisq ` to 15 significant figures , corresponding to a relative error of about @xmath61 .",
    "the other three packages give 12 significant figures , corresponding to relative errors of slightly over @xmath62 .",
    "more extreme tail values give even more striking results .",
    "we repeat the above process now with @xmath63 :    > q1 < - 0.01 > z < - ( q1 - mu)^2 / ( phi * mu^2 * q1 ) > polycoef < - c(mu^2 , -2 * mu - phi * mu^2 * z , 1 ) > q < - re(polyroot(polycoef ) )    the reference chisquare cdf value is :    > pchisq(z , df = 1 , lower.tail = false ) [ 1 ] 1.6427313604456241e-32    this can be compared to the corresponding values from the igd packages :    > pinvgauss(q[1 ] , mean = mu , disp = phi ) + + pinvgauss(q[2 ] , mean = mu , disp = phi , lower.tail = false ) [ 1 ] 1.6427313604456183e-32 > rmutil::pinvgauss(q[1 ] , m = mu , s = phi ) + + 1 - rmutil::pinvgauss(q[2 ] , m = mu , s = phi ) [ 1 ] 0 > suppdists::pinvgauss(q[1 ] , nu = mu , lambda = 1/phi ) + + suppdists::pinvgauss(q[2 ] , nu = mu , lambda = 1/phi , lower.tail = false ) [ 1 ] 8.2136568022278466e-33 > star::pinvgauss(q[1 ] , mu = mu , sigma2 = phi ) + + star::pinvgauss(q[2 ] , mu = mu , sigma2 = phi , lower.tail = false ) [ 1 ] 1.6319986233795599e-32    it can be seen from the above that rmutil and suppdists do not agree with ` pchisq ` to any significant figures , meaning that the relative error is close to 100% , while star manages 3 significant figures .",
    "statmod on the other hand continues to agree with ` pchisq ` to 15 significant figures .",
    "now consider the problem of computing the quantile function @xmath64 .",
    "the quantile function computes @xmath41 satisfying @xmath65 .",
    "if @xmath66 is an initial approximation to @xmath41 , then newton s method is a natural choice for refining the estimate .",
    "newton s method gives the updated estimate as @xmath67 for right - tail probabilities , the newton step is almost the same : @xmath68 where now @xmath69 .",
    "newton s method is very attractive because it is quadratically convergent if started sufficiently close to the required value .",
    "it is hard however to characterize how close the starting value needs to be to achieve convergence and in general there is no guarantee that the newton iteration will not diverge or give impossible values such as @xmath70 or @xmath71 .",
    "our approach is to derive simple conditions on the starting values such that the newton iteration always converges and does so without any backtracking .",
    "we call this behavior _ monotonic convergence_.    recall that the igd is unimodal for all parameter values with mode @xmath72 given previously .",
    "it follows that the pdf @xmath73 is increasing for all @xmath74 and decreasing for all @xmath75 and the cdf @xmath76 is convex for @xmath74 and concave for @xmath75 . in other words ,",
    "the cdf has a point of inflexion at the mode of the distribution .",
    "suppose that the required @xmath41 satisfies @xmath77 and suppose that the working estimate satisfies @xmath78 .",
    "it can be seen that the cdf is concave in the interval @xmath79 $ ] , the newton step will be positive and the updated estimate @xmath80 will still satisfy @xmath81 ( figure  [ fig : newton ] ) .",
    "suppose instead that @xmath74 and suppose that the working estimate satisfies @xmath82 .",
    "in this case it can be seen that the cdf is convex in the interval @xmath79 $ ] , the newton step will be negative and the updated estimate @xmath66 will still satisfy @xmath83 ( figure  [ fig : newton ] ) .",
    "it follows that newton s method is always monotonically convergent provided that the starting value lies between the mode @xmath72 and the required value @xmath41 .",
    "in fact the mode @xmath72 itself can be used as the starting value .",
    "note that to compute the mode @xmath72 accurately without subtractive cancellation we use equation  [ eq : modetaylor ] when @xmath84 is large and use equation  [ eq : mode ] otherwise .",
    "we use @xmath85 as the starting value for the newton iteration unless the left or right tail probability is very small .",
    "when the left tail probability is less than @xmath86 , we use instead @xmath87 where @xmath88 is the corresponding quantile of the standard normal distribution .",
    "when the right tail probability is less than @xmath86 , we use @xmath89 where @xmath90 is the corresponding quantile of the gamma distribution with the same mean and variances as the igd .",
    "these starting values are closer to the required @xmath41 than is @xmath72 but still lie between @xmath72 and the required @xmath41 and so are in the domain of monotonic convergence .",
    "we use the alterative starting values only for extreme tail probabilities because in other cases the computational cost of computing the starting value is greater than the saving enjoyed by reducing the number of newton iterations that are needed .",
    "the term @xmath91 in the newton step could potentially suffer loss of floating point precision by subtractive cancellation when @xmath92 and @xmath93 are nearly equal or if @xmath92 is very close to 1 . to avoid this we work with @xmath92 on the log - scale and employ a taylor series expansion when @xmath92 and @xmath93 are relatively close .",
    "let @xmath94 .",
    "when @xmath95 , we approximate @xmath96 here @xmath97 is computed by ` pinvgauss ` with ` log.p = true ` and @xmath98 is computed using the ` log1p ` function .",
    "we find that the statmod ` qinvgauss ` package gives 16 significant figures whereas the other packages give no more than 68 figures of accuracy .",
    "precision can be demonstrated by comparing the probability vector @xmath92 with the values obtained by passing the probabilities through ` qinvgauss ` and ` pinvgauss ` . `",
    "qinvgauss ` and ` pinvgauss ` are inverse functions , so the final probabilities should be equal in principle to the original values .",
    "error is measured by comparing the original and processed probability vectors :    >",
    "p < - c(0.000001 , 0.00001 , 0.0001 , 0.001 , 0.01 , 0.1 , 0.5 , + 0.9 , 0.99 , 0.999 , 0.9999 , 0.99999 , 0.999999 ) > > p1 < - pinvgauss(qinvgauss(p , mean = 1 , disp = 1 ) , mean = 1 , disp = 1 ) > p2 < - rmutil::pinvgauss(rmutil::qinvgauss(p , m = 1 , s = 1 ) , m = 1 , s = 1 ) > p3 < - suppdists::pinvgauss(suppdists::qinvgauss(p , nu = 1 , la = 1 ) , nu = 1 , la = 1 ) > p4 < - star::pinvgauss(star::qinvgauss(p , mu = 1 , sigma2 = 1 ) , mu = 1 , sigma2 = 1 ) > > options(digits = 4 ) > summary ( abs(p - p1 ) ) min .",
    "median mean 3rd qu . max .",
    "0.00e+00 0.00e+00 0.00e+00 1.92e-17 2.20e-19 2.22e-16 > summary ( abs(p - p2 ) ) min .",
    "median mean 3rd qu . max . 0.00e+00",
    "5.10e-09 8.39e-08 3.28e-07 5.92e-07 1.18e-06 > summary ( abs(p - p3 ) ) min .",
    "median mean 3rd qu . max .",
    "1.00e-12 6.00e-12 2.77e-10 1.77e-09 2.58e-09 1.03e-08 > summary ( abs(p - p4 ) ) min .",
    "median mean 3rd qu . max . 0.00e+00 0.00e+00 1.20e-08 8.95e-07 2.17e-07 6.65e-06    it can be seen that the error for ` statmod::qinvgauss ` is never greater than ` 2e-16 ` .",
    "similar results are observed if relative error is assessed in terms of the quantile @xmath41 instead of the probability @xmath92 :    > q < - qinvgauss(p , mean = 1 , disp = 1 ) > q1 < - qinvgauss(pinvgauss(q , mean = 1 , disp = 1 ) , mean = 1 , disp = 1 ) > q2 < - rmutil::qinvgauss(rmutil::pinvgauss(q , m = 1 , s = 1 ) , m = 1 , s = 1 ) > q3 < - suppdists::qinvgauss(suppdists::pinvgauss(q , nu = 1 , la = 1 ) , nu = 1 , la = 1 ) > q4",
    "< - star::qinvgauss(star::pinvgauss(q , mu = 1 , sigma2 = 1 ) , mu = 1 , sigma2 = 1 ) > summary ( abs(q1-q)/q ) min .",
    "median mean 3rd qu . max .",
    "0.00e+00 0.00e+00 0.00e+00 5.57e-17 0.00e+00 4.93e-16 > summary ( abs(q2-q)/q ) min .",
    "median mean 3rd qu . max . 0.00e+00",
    "1.70e-06 3.30e-06 8.94e-05 8.80e-05 5.98e-04 > summary ( abs(q3-q)/q ) min .",
    "median mean 3rd qu . max .",
    "1.09e-08 3.94e-08 4.78e-08 4.67e-08 5.67e-08 8.93e-08 > summary ( abs(q4-q)/q ) min .",
    "median mean 3rd qu . max . 0.00e+00",
    "3.00e-07 1.40e-06 9.20e-05 9.42e-05 5.46e-04    the relative error for ` statmod::qinvgauss ` is never worse than ` 5e-16 ` .",
    "speed was determined by generating @xmath92 as a vector of a million random uniform deviates , and running the ` qinvgauss ` or ` qinvgauss ` functions on p with mean and dispersion both equal to one .",
    "> set.seed(20140526 ) > u < - runif(1000 ) >",
    "p < - runif(1e6 ) > system.time(q1 < - qinvgauss(p , mean = 1 , shape = 1 ) ) user system elapsed 4.29 0.41 4.69 > system.time(q2 < - rmutil::qinvgauss(p , m = 1 , s = 1 ) ) user system elapsed 157.39 0.03 157.90 > system.time(q3",
    "< - suppdists::qinvgauss(p , nu = 1 , lambda = 1 ) ) user system elapsed 13.59 0.00 13.68 > system.time(q4 < - star::qinvgauss(p , mu = 1 , sigma2 = 1 ) ) user system elapsed 266.41 0.06 267.25    timings shown here are for a windows laptop with a 2.7ghz intel i7 processor running 64-bit r - devel ( built 31 january 2016 ) .",
    "the statmod qinvgauss function is 40 times faster than the rmutil or star functions about 3 times faster than suppdists .",
    "reliability is perhaps even more crucial than precision or speed . `",
    "suppdists::qinvgauss ` fails for some parameter values because newton s method does not converge from the starting values provided :    > options(digits = 4 ) > suppdists::qinvgauss(0.00013 , nu=1 , lambda=3 ) error in suppdists::qinvgauss(0.00013 , nu = 1 , lambda = 3 ) : iteration limit exceeded in newtonroot ( )    by contrast , ` statmod::qinvgauss ` runs successfully for all parameter values because divergence of the algorithm is impossible :    > qinvgauss(0.00013 , mean = 1 , shape = 3 ) [ 1 ] 0.1504    ` qinvgauss ` returns right tail values accurately , for example :    >",
    "qinvgauss(1e-20 , mean = 1.5 , disp = 0.7 , lower.tail = false ) [ 1 ] 126.3    the same probability can be supplied as a left tail probability on the log - scale , with the same result :    > qinvgauss(-1e-20 , mean = 1.5 , disp = 0.7 , log.p = true ) [ 1 ] 126.3    note that ` qinvgauss ` returns the correct quantile in this case even though the left tail probability is not distinguishable from 1 in floating point arithmetic on the unlogged scale . by contrast , the rmutil and star functions do not compute right tail values and the suppdists function fails to converge for small right tail probabilities :    >",
    "suppdists::qinvgauss(1e-20 , nu = 1.5 , lambda = 1/0.7 , lower.tail = false ) error in suppdists::qinvgauss(1e-20 , nu = 1.5 , lambda = 1/0.7 , lower.tail = false ) : infinite value in newtonroot ( )    similarly for log - probabilities , the rmutil and star functions do not accept log - probabilities and the suppdists function gives an error :    > suppdists::qinvgauss(-1e-20 , nu = 1.5 , lambda = 1/0.7 , log.p = true ) error in suppdists::qinvgauss(-1e-20 , nu = 1.5 , lambda = 1/0.7 , log.p = true ) : infinite value in newtonroot ( )    all the statmod igd functions allow variability to be specified either by way of a dispersion ( @xmath1 ) or shape ( @xmath18 ) parameter :    > args(qinvgauss ) function ( p , mean = 1 , shape = null , dispersion = 1 , lower.tail = true , log.p = false , maxit = 200l , tol = 1e-14 , trace = false )    boundary or invalid ` p ` are detected :    > options(digits = 4 ) > qinvgauss(c(0 , 0.5 , 1 , 2 , na ) ) [ 1 ] 0.0000 0.6758 inf na",
    "na    as are invalid values for @xmath0 or @xmath1 :    > qinvgauss(0.5 , mean = c(0 , 1 , 2 ) ) [ 1 ] na 0.6758 1.0285    the statmod functions ` dinvgauss ` , ` pinvgauss ` and ` qinvgauss ` all preserve the attributes of the first input argument provided that none of the other arguments have longer length .",
    "for example , ` qinvgauss ` will return a matrix if ` p ` is a matrix :    > p < - matrix(c(0.1 , 0.6 , 0.7 , 0.9 ) , 2 , 2 ) > rownames(p ) < - c(``a '' , `` b '' ) > colnames(p ) < - c(``x1 '' , `` x2 '' ) > p x1 x2 a 0.6001 0.3435 b 0.4919 0.4987 >",
    "qinvgauss(p ) x1 x2 a 0.8486 0.4759 b 0.6637 0.6739    similarly the names of a vector are preserved on output :    > p < - c(0.1 , 0.6 , 0.7 , 0.9 ) > names(p ) < - letters[1:4 ] > qinvgauss(p ) a b c d 0.2376 0.8483 1.0851 2.1430",
    "the functions ` statmod::rinvgauss ` , ` suppdists::rinvgauss ` and ` star::rinvgauss ` all use the same algorithm to compute random deviates from the igd .",
    "the method is to generate chisquare random deviates corresponding to @xmath99 , and then choose between the two possible @xmath9 values leading to the same chisquare value with probabilities worked out by @xcite . the suppdists function is faster than the others because of the implementation in c. nevertheless , the pure r statmod and star functions are acceptably fast .",
    "the statmod function generates a million random deviates in about a quarter of a second of elapsed time on a standard business laptop computer while star takes about half a second .",
    "the ` rmutil::rinvgauss ` function generates random deviates by running ` qinvgauss ` on random uniform deviates .",
    "this is far slower and less accurate than the other functions .",
    "basic probability calculations for the igd have been available in various forms for some time but the functions described here are the first to work for all parameter values and to return close to full machine accuracy .",
    "the statmod functions achieve good accuracy by computing probabilities on the log - scale where possible .",
    "care is given to handle special limiting cases , including some cases that have not been previously described .",
    "the statmod functions trap invalid parameter values , provide all the standard arguments for probability functions in the r and preserve argument attributes on output .",
    "a new strategy has been described to invert the cdf using a monotonically convergent newton iteration .",
    "it may seem surprising that we recommend starting the iteration from the same value regardless of the quantile required .",
    "intuitively , a starting value that is closer to the required quantile might have been expected to be better .",
    "however using an initial approximation runs the risk of divergence , and convergence of newton s method from the mode is so rapid that the potential advantage of a closer initial approximation is minimized .",
    "the statmod ` qinvgauss ` function is 40 times faster than the quantile functions in the rmutil or star packages , despite returning 16 rather than 6 figures of accuracy .",
    "it is also 3 times faster than suppdists , even though ` suppdists::qinvgauss ` is written in c , uses the same basic newton strategy and has a less stringent stopping criterion .",
    "the starting values for newton s method used by ` suppdists::qinvgauss ` are actually closer to the final values than those used by ` statmod::qinvgauss ` , but the latter are more carefully chosen to achieve smooth convergence without backtracking . `",
    "suppdists::qinvgauss ` uses the log - normal approximation of @xcite to start the newton iteration and the ` star::qinvgauss ` uses the same approximation to setup the interval limits for ` uniroot ` .",
    "unfortunately the log - normal approximation has much heavier tails than the igd , meaning that the starting values are more extreme than the required quantiles and are therefore outside the domain of monotonic convergence .    as well as the efficiency gained by avoiding backtracking , monotonic convergence has the advantage that any change in sign of the newton step is a symptom that the limits of floating point accuracy have been reached . in the statmod",
    "` qinvgauss ` function , the newton iteration is stopped if this change of sign occurs before the convergence criterion is achieved .",
    "the current statmod functions could be made faster by reimplementing in c , but the pure r versions have benefits in terms of understandability and easy maintenance , and they are only slightly slower than comparable functions such as ` qchisq ` and ` qt ` .",
    "this strategy used here to compute the quantile could be used for any continuous unimodal distribution , or for continuous distribution that can be transformed to be unimodal .",
    "> sessioninfo ( ) r under development ( unstable ) ( 2016 - 01 - 31 r70055 ) platform : x86_64-w64-mingw32/x64 ( 64-bit ) running under : windows 7 x64 ( build 7601 ) service pack 1    locale : [ 1 ] lc_collate = english_australia.1252 lc_ctype = english_australia.1252 [ 3 ] lc_monetary = english_australia.1252 lc_numeric = c [ 5 ] lc_time = english_australia.1252    attached base packages : [ 1 ] stats graphics grdevices utils datasets methods base    other attached packages : [ 1 ] statmod_1.4.24 star_0.3 - 7 codetools_0.2 - 14 gss_2.1 - 5 [ 5 ] r2html_2.3.1 mgcv_1.8 - 11 nlme_3.1 - 124 survival_2.38 - 3 [ 9 ] suppdists_1.1 - 9.2 rmutil_1.0    loaded via a namespace ( and not attached ) : [ 1 ] matrix_1.2 - 3 splines_3.3.0 grid_3.3.0 lattice_0.20 - 33    10    balakrishna , n. and rahul , t. ( 2014 ) .",
    "inverse gaussian distribution for modeling conditional durations in finance .",
    "_ communications in statistics - simulation and computation _ 43 , 476486 .",
    "bardsley , w. ( 1980 ) .",
    "note on the use of the inverse gaussian distribution for wind energy applications .",
    "_ journal of applied meteorology _ 19 , 11261130 .",
    "blough , d.k . , madden , c.w . , and hornbrook , m.c .",
    "( 1999 ) . modeling risk using generalized linear models .",
    "_ journal of health economics _ 18 , 153171 .",
    "chhikara , r. and folks , j. ( 1977 ) . the inverse gaussian distribution as a lifetime model .",
    "_ technometrics _ 19 , 461468 .",
    "chhikara , r.s .",
    "( 1989 ) . _ the inverse gaussian distribution_. marcel dekker , new york .",
    "chhikara , r.s . and folks , j.l .",
    "estimation of the inverse gaussian distribution function",
    ". _ journal of the american statistical association _ 69 , 250254 .",
    "de  jong , p. and heller , g.z .",
    "_ generalized linear models for insurance data_. cambridge university press , cambridge .",
    "johnson , n.l . and kotz , s. ( 1970 ) .",
    "_ continuous univariate distributions , vol .",
    "1_. wiley - interscience , new york .",
    "kallioras , a.g . and koutrouvelis , i.a .",
    "percentile estimation in inverse gaussian distributions",
    ". _ communications in statistics - simulation and computation _ 43 , 269284 .",
    "lindsey , j. ( 2010 ) .",
    "_ rmutil : utilities for nonlinear regression and repeated measurements models_. r package version 1.0 , last changed 2010 - 02 - 15 .",
    "mccullagh , p. and nelder , j.a .",
    "_ generalized linear models_.",
    "chapman & hall / crc , boca raton , florida , 2nd edition .",
    "michael , j.r . ,",
    "schucany , w.r . , and haas , r.w .",
    "generating random variates using transformations with multiple roots . _",
    "the american statistician _ 30 , 8890 .",
    "pouzat , c. ( 2012 ) .",
    "_ star : spike train analysis with r_. r  package version  0.3 - 7 , dated 2012 - 10 - 08 .",
    "press , w.h . ,",
    "teukolsky , s.a . , vetterling , w.t . , and flannery , b.p .",
    "_ numerical recipes in fortran_. cambridge university press , cambridge .",
    "seshadri , v. ( 1993 ) . _",
    "the inverse gaussian distribution : a case study in exponential families_. clarendon press , oxford .",
    "shuster , j. ( 1968 ) . on the inverse gaussian distribution function .",
    "_ journal of the american statistical association _ 63 , 15141516 .",
    "smyth , g. and verbyla , a. ( 1999 ) .",
    "adjusted likelihood methods for modelling dispersion in generalized linear models .",
    "_ environmetrics _ 10 , 695709 .",
    "smyth , g.k .",
    "( 1998 ) . _",
    "invgauss : inverse gaussian distribution_. functions for s - plus .",
    "smyth , g.k .",
    "_ statmod : statistical modeling_. r  package version  1.4.23 .",
    "tweedie , m.c .",
    "statistical properties of inverse gaussian distributions i. _ the annals of mathematical statistics _ 28 , 362377 .",
    "wang , x. and xu , d. ( 2010 ) . an inverse gaussian process model for degradation data . _ technometrics _ 52 , 188197 .",
    "wheeler , b. ( 2009 ) . _",
    "suppdists : supplementary distributions_. r  package version  1.1 - 9.1 , dated 2009 - 12 - 09 .    whitmore , g , a. ( 1975 ) .",
    "the inverse gaussian distribution as a model of hospital stay .",
    "_ health services research _ 10 , 297 .",
    "whitmore , g. and yalovsky , m. ( 1978 ) . a normalizing logarithmic transformation for inverse gaussian random variables .",
    "_ technometrics _ 20 , 207208 .",
    "here we derive an asymptotic expression for the right tail probability , @xmath30 , when @xmath41 is large . without loss of generality",
    ", we will assume @xmath13 .",
    "first , we drop the @xmath100 term in the exponent of the pdf ( [ pdf ] ) , leading to : @xmath101 for @xmath25 large . integrating the pdf",
    "gives the right tail probability as : @xmath102 for @xmath41 large .",
    "transforming the variable of integration gives : @xmath103 finally , we approximate the integral using @xmath104 which gives @xmath105 and @xmath106 for @xmath41 large ."
  ],
  "abstract_text": [
    "<S> the inverse gaussian distribution ( igd ) is a well known and often used probability distribution for which fully reliable numerical algorithms have not been available . </S>",
    "<S> our aim in this article is to develop software for this distribution for the r programming environment . </S>",
    "<S> we develop fast , reliable basic probability functions ( ` dinvgauss ` , ` pinvgauss ` , ` qinvgauss ` and ` rinvgauss ` ) that work for all possible parameter values and which achieve close to full machine accuracy . </S>",
    "<S> the most challenging task is to compute quantiles for given cumulative probabilities and we develop a simple but elegant mathematical solution to this problem . </S>",
    "<S> we show that newton s method for finding the quantiles of a igd always converges monotonically when started from the mode of the distribution . </S>",
    "<S> simple taylor series expansions are used to improve accuracy on the log - scale . </S>",
    "<S> the igd probability functions provide the same options and obey the same conventions as do probability functions provided in the standard r stats package . </S>",
    "<S> the igd functions are part of the statmod package available from the cran repository . </S>"
  ]
}