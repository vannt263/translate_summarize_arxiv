{
  "article_text": [
    "cloud storage solutions have become increasingly popular among customers .",
    "they usually provide a limited amount of storage space that is accessed over the internet and used for synchronization of personal data between devices or for backup purposes , e.g. , by using _ rsync _",
    "@xcite to synchronize a user s important data to the cloud storage in frequent intervals .",
    "ideally , cheap creation of snapshots should be supported , i.e. , the user should be able to preserve lots of consistent copies of specific states of her backed up data without being charged for redundant or duplicate data .",
    "such snapshots / versioning features are provided by many cloud storage providers , e.g. , dropbox  @xcite .    unfortunately , _",
    "security guarantees _ of today s popular providers are insufficient : malicious providers could read and modify data unnoticed by their users .",
    "the above - described scenario , thus , requires application of cryptographic measures on the client side as to ensure confidentiality and authenticity of outsourced data .",
    "secure encryption using tools like gnupg  @xcite , however , hides any information about file contents  including differences across versions  from the provider , thus preventing any savings from its snapshots feature .",
    "only few systems try to combine security and storage efficiency ; neither is both secure and able to provide efficiency comparable to unencrypted cloud storage to the best of our knowledge .",
    "consequentially , users have to decide between cheap & comfortable and expensive & secure solutions today .",
    "as we consider both aspects equally important , our goal is to advance development of practical solutions ( e.g. , backup systems ) for cloud storage with strong security and better efficiency guarantees . to this end",
    ", we present a novel data structure for file contents on untrusted cloud storage , ` sec - cs ` , with the following contributions :    we design and integrate a novel chunking - based data deduplication concept , ml- * , that outperforms existing approaches w.r.t .",
    "storage efficiency when storing contents with high redundancy .",
    "we achieve strong confidentiality and authenticity guarantees of stored data with zero storage overhead .",
    "we further publish a ready - to - use implementation and evaluate its efficiency analytically and empirically , proving superiority to other approaches .",
    "this paper is structured as follows : we give background information and present related work in sec .",
    "[ related_work ] . ml- * is introduced in sec .  [ recursive_chunking ] and",
    "detailed as part of ` sec - cs ` , which is described in sec .",
    "[ seccs ] and proven secure in sec .  [ basic_correctness ] .",
    "our implementation is discussed in sec .",
    "[ implementation ] and an evaluation is presented in sec .",
    "[ evaluation ] .",
    "[ conclusion ] concludes the paper .",
    "as both _ data deduplication _ ( i.e. , elimination of redundancy across stored data ) , and security are essential goals of ` sec - cs ` , different kinds of existing work are related .",
    "existing deduplication systems apply some deterministic chunking scheme @xmath0 to a content to split it into non - overlapping chunks , and avoid storage of resulting chunks that have already been stored before , usually by maintaining an index of cryptographic hash values of chunks .",
    "an overview and a classification of common approaches and their efficiency is provided by meister and brinkmann  @xcite : _ whole - file chunking ( wfc ) _ yields a single chunk and is thus able to detect identical file contents . _",
    "static chunking ( sc ) _ splits a content into chunks of fixed size that are individually deduplicated , so partially overlapping contents can be deduplicated as well .",
    "it is used , e.g. , in venti .",
    "@xcite _ content - defined chunking ( cdc ) _ , in contrast , can even tolerate shifted contents .",
    "it determines chunk boundaries by moving a sliding window of some fixed size @xmath1 over the content and creating chunk boundaries when a window content meets a specific criterion , typically a hash value being in a specific range .",
    "this yields chunks of some _ expected _ length  the _ target chunk length_under the assumption that different positions have different window contents and hash values are uniformly distributed . to deal with non - uniformly distributed contents , a minimum and maximum chunk size can be set .",
    "the scheme was introduced by muthitacharoen et al .",
    "@xcite for the low - bandwidth network file system and is usually implemented using a rolling hash , e.g. , rabin fingerprints  @xcite .",
    "alternatives to the basic sliding window approach usually used for cdc that might be worth consideration for future enhancements of ` sec - cs ` are presented by eshghi and tang  @xcite . according to @xcite",
    ", sc yields better deduplication efficiency than wfc and cdc is more efficient than sc for real - life data .",
    "few systems like admad  @xcite are able to achieve even better efficiency by employing _ application - specific chunking ( asc)_. asc , however , requires additional knowledge about the respective file format of a content .    instead of simply avoiding multiple storage of identical chunks ,",
    "some systems employ _ delta encoding _ : when a highly similar chunk ",
    "a _ base chunk_is known , a new chunk is represented as a reference to the base chunk and a _ difference _ , i.e. , a sequence of actions that define how to create it from the base chunk . in combination with wfc ,",
    "this scheme is , e.g. , used in version control systems ( vcs ) like subversion ( svn )  @xcite .",
    "svn s _ fsfs _ backend stores the first revision of a file content in its entirety and all subsequent revisions as differences to previous revisions .",
    "@xcite    a comparison of advantages of the different schemes is shown in tab .",
    "[ tab : efficiency ] : delta - based approaches are clearly able to yield lowest storage costs for changed contents in principle , but they have important limitations in practice : first , their efficiency depends on a priori knowledge of relations between chunks  a problem tackled by , e.g. , the derd framework  @xcite .",
    "second , they substantially increase retrieval costs as reconstruction of delta - encoded chunks requires retrieval of corresponding base chunks of which only parts are actually required .",
    "chunking - based schemes achieve savings when storing changed contents ( data ) irrespective of knowledge about relations ( their best values are highlighted in green in the table ) .",
    "the most efficient strategies cdc / asc yield their savings depending on the distribution of contents / specific file formats .",
    "l|c|cccc||cc|c storage costs for ... & delta & wfc & sc & cdc & asc & ml - sc & ml - cdc + single file ( data ) & @xmath2 & @xmath2 & @xmath2 & @xmath2 & @xmath2 & @xmath2 & @xmath2 + single file ( metadata ) & @xmath3 & @xmath3 & @xmath2 & @xmath2 & @xmath2 & @xmath2 & @xmath2 + single file ( @xmath4 ) & @xmath2 & @xmath2 & @xmath2 & @xmath2 & @xmath2 & @xmath2 & @xmath2 + change w / o shift ( d ) & @xmath3 & @xmath2 & @xmath3 & @xmath3 & @xmath3 & @xmath3 & @xmath3 + change w / o shift ( m ) & @xmath3 & @xmath3 & @xmath2 & @xmath2 & @xmath2 & @xmath5 & @xmath5 + change w / o shift ( @xmath4 ) & @xmath3 & @xmath2 & @xmath2 & @xmath2 & @xmath2 & @xmath5 & @xmath5 + change w/ shift ( d ) & @xmath3 & @xmath2 & @xmath2 & @xmath3 & @xmath3 & @xmath2 & @xmath3 + change w/ shift ( m ) & @xmath3 & @xmath3 & @xmath2 & @xmath2 & @xmath2 & @xmath2 & @xmath5 + change w/ shift ( @xmath4 ) & @xmath3 & @xmath2 & @xmath2 & @xmath2 & @xmath2 & @xmath2 & @xmath5 +   + size & & & & & & & + uniform distribution & & & & & & & + precise format & & & & & & & + relation to others & & & & & & & +    in addition to the costs for the actual content _ data _ , every data deduplication mechanism incurs storage costs for _ metadata _ : in case of wfc , this is only a small constant per content corresponding to its cryptographic hash value .",
    "chunking - based schemes do not only incur this overhead for every chunk , but they also require additional storage space to store of which chunks a specific content consists  typically a list of chunk identifiers , e.g. , cryptographic hash values .",
    "although the respective constants are small for large chunk sizes , metadata storage costs for contents deduplicated via sc , cdc and asc are linear in their lengths no matter how high their redundancy is .",
    "these scheme s chunk size parameters , thus , considerably impact their storage efficiency , as also noticed by eshghi and tang  @xcite : if set too high , deduplication performance is decreased , as only completely identical chunks allow space savings .",
    "if set too small , storage of contents with high redundancy cause considerable overhead due to the sheer amount of chunk references that have to be stored .",
    "this problem is solved by our proposals ml - sc / ml - cdc : due to a specific , multi - level application of sc / cdc , we achieve logarithmic metadata costs , allowing high storage efficiency even for small chunk sizes , independent from content sizes . to the best of our knowledge ,",
    "our strategy is unique .",
    "teodosiu et al .",
    "@xcite apply cdc recursively to enable efficient replication of files over a network ( assuming the receiver has a similar file to the one being transferred ) , but they do not target storage systems and fail to achieve logarithmic costs due to a fixed recursion depth .",
    "further , their approach is different to ours : instead of breaking large chunks recursively into smaller ones , they generate the smallest chunks first and use cdc recursively to break lists of chunk references into smaller parts  requiring multiple passes .",
    "yasa and nagesh  @xcite employ hierarchical chunking starting with cdc at the highest level as we do , but they use only two levels ( sc at second level ) , so storage overhead is still linear .",
    "lots of works exist in the related fields of cloud security and cryptographic file systems , but only few focus on authenticity and storage efficiency .",
    "popular cloud storage security solutions typically deal with confidentiality only .",
    "boxcryptor  @xcite , e.g. , is based on and uses a similar concept to encfs  @xcite : it encrypts file contents symmetrically , but does not provide authenticity . while not preventing storage - efficient snapshots of unchanged files , costs for changed files are high due to entirely different ciphertexts .",
    "sirius  @xcite , plutus  @xcite and tahoe - lafs  @xcite are examples of file systems with authenticity guarantees : they apply sc to contents and compute a merkle tree  @xcite over the chunks to allow authenticity verification even for parts of contents , but they do not support data deduplication : tahoe - lafs creates entirely different ciphertexts for similar file contents , the other systems even for identical ones .    to allow efficient usage of cloud storage for , e.g. , backups , more specialized tools are required .",
    "common backup tools like duplicity  @xcite rely on _ incremental _ backups , i.e. , they store differences to previous backups .",
    "this can be used in combination with gnupg to preserve snapshots in a storage - efficient and secure manner , but causes communication overhead when specific versions are read .",
    "vcs could be used for delta - based backups to a limited extent , but they are typically inefficient w.r.t .",
    "large files and have limited security properties : git  @xcite only ensures integrity / authenticity of the version history by integrating signatures .",
    "svn does not , but an extension  @xcite adds storage - efficient file - level encryption .",
    "cumulus  @xcite is a backup system that supports large files and allows direct access to arbitrary snapshots , but it is less storage - efficient as it only deduplicates identical data between different versions of individual files .",
    "farsite  @xcite , in contrast , is a distributed file system targeting on chunking - based backups that deduplicates different identical files despite secure encryption .",
    "it can not save space for snapshots of different versions of a file , though , as it relies on wfc .",
    "storer et al .",
    "@xcite extend farsite s concept to cdc , but they do not provide any explicit authenticity guarantees .",
    "many more works exist in the field of cloud storage security .",
    "most of them , however , have a different focus and are orthogonal to our work .",
    "athos  @xcite , e.g. , is a solution for outsourcing file systems that achieves integrity in a way that file system operations are possible with logarithmic communication costs .",
    "the solution is orthogonal to our work in the sense that this requirement is w.r.t .",
    "the total number of files / directories in the file system and not w.r.t .  the size of single file contents .",
    "a similar goal is pursued by heitzmann et al .",
    "both works are based on authenticated skip lists , an authenticated data structure ( ads ) initially proposed by goodrich et al .",
    "@xcite , while our work is based on another ads  the merkle tree  @xcite",
    ".    ads , in general , is an umbrella term for data structures involving three parties ( a trusted _ source _ who publishes data , an untrusted _ responder _ that stores structured data , and a _ user _ that requests data ) that enable authenticated , efficient queries to the data .",
    "@xcite in this sense , ` sec - cs ` can be considered an ads with additional data deduplication and confidentiality properties : the cloud storage backend can be considered the _ responder _ and the user / client plays the roles of _ source _ and _ user_. an overview of existing ads and methods for constructing ads in general are provided by martel et al .",
    "@xcite and miller et al .",
    "our first contribution are the chunking strategies ml - sc and ml - cdc which improve on the state of the art in terms of storage efficiency in presence of high redundancy ( see tab .",
    "[ tab : efficiency ] ) .",
    "the basic idea is simple : as the linear overhead for storing contents with a traditional strategy @xmath0 is caused by the need of storing references to each constant - size part of each content , we want to also deduplicate these references .",
    "this can be achieved by representing the results of @xmath0 on a content @xmath6 as a _ chunk tree _",
    "@xmath7 , whose    _ leaf nodes _ represent the chunks output by @xmath0 , and    _ inner nodes _ aggregate chunks , representing the concatenation of chunks represented by their children .",
    "in addition to _ leaf chunks _ ( chunks represented by leaf nodes ) ,",
    "we thus create `` larger '' _ superchunks _ ( chunks represented by inner nodes ) , which we persist as well and which can be referenced directly when new contents are stored .",
    "consequentially , each content is represented by one persisted _ root chunk _",
    "( the chunk represented by the tree s root node , which might be a leaf or superchunk ) .",
    "persisting a superchunk requires storing references to its children . to ease notation",
    ", we refer to storage costs of a chunk representation as its _ size _ and to the length of its represented content as its _",
    "length_. for leaf chunks we assume size is equal to length . to enable high storage efficiency ,",
    "we require _ sublinear storage overhead _ for storing a content @xmath8 having large overlaps with an existing content @xmath6 . for this , we generate their chunk trees @xmath7 , @xmath9 so that the following _ requirements _ are met :    1 .",
    "the ( expected ) size of each chunk is constant,[rec_chunking_req_constant ] 2 .",
    "identical parts of @xmath6 and @xmath8 share not only leaf , but also superchunks ( i.e. , @xmath7 and @xmath9 share subtrees ) , and[rec_chunking_req_identical ] 3 .   the heights of @xmath7 and @xmath9 are chosen logarithmically in the lengths of @xmath6 and @xmath8 , respectively.[rec_chunking_req_height ]    thus , if @xmath8 differs from @xmath6 in only one byte , @xmath7 and @xmath9 shall be equal except for one chunk at each level , so that their difference consists of @xmath10 constant - size chunks .",
    "different chunking strategies allow to achieve this . in the simplest case",
    ", we could aggregate fixed numbers of consecutive chunks output by sc to superchunks , and continue aggregating fixed numbers of subsequent superchunks until only a single superchunk  the root chunk  remains .",
    "this approach , however , would eradicate advantages of chunking schemes that go beyond those of sc .",
    "while leaf chunks output by cdc , for example , are robust against shifting , this property would not be true for superchunks . to account for that ,",
    "we define our multi - level chunking scheme in a more general way that preserves the properties of its underlying chunking algorithm @xmath0 .",
    "the only requirements we state is that @xmath0 has to be deterministic and that it has a parameter @xmath11 that allows to specify the target ( or expected ) length of its generated chunks .",
    "now let @xmath12 be the size required for representing a single _ chunk reference _",
    ", i.e. , anything that allows retrieval of the corresponding chunk ( @xmath13 is constant as chunks will be referenced by hash values ) .",
    "we define ml-@xmath0 as follows :    on input a content @xmath6 with length @xmath14 , choose the height @xmath15 of the to - be - built chunk tree @xmath7 as @xmath16    where @xmath17 describes a single - node tree .",
    "create root node of @xmath7 that should represent @xmath6 .",
    "iterate over the nodes of the tree in a breadth - first search manner . for each node with height @xmath18 ( beginning with the root node having height @xmath19 ) ,",
    "determine content @xmath20 that the node represents ,    apply the chunking strategy @xmath0 on @xmath20 with target chunk length @xmath21 , and    add child node for each chunk @xmath22 output by @xmath0 .",
    "this way , a content smaller than or equal to the target chunk size @xmath11 results in a single leaf node , and for @xmath23 , all leaf chunks have target chunk size @xmath11 .",
    "as superchunks at height @xmath24 have expected length @xmath25 and are chunked with target chunk length @xmath21 , they are expected to have @xmath26 children . as child references have size",
    "@xmath13 , the expected size of superchunks is @xmath11 as well , fulfilling req .",
    "[ rec_chunking_req_constant ] .",
    "[ rec_chunking_req_height ] is met by the choice of @xmath15 and req .",
    "[ rec_chunking_req_identical ] is expected to be achieved due to straightforward application of @xmath0 at each level .",
    "the latter is concretized in sec .",
    "[ basic_interface ] and discussed and evaluated in detail in sec .  [ evaluation ] .",
    "for a detailed analysis , we embed ml- * in a generic data structure that is described in this section .",
    "` sec - cs ` acts like a normal hash table that assigns a deterministically computed hash value to each inserted content , but comes with a combination of properties different from prior work : it employs _ multi - level chunking _ to significantly reduce storage overhead for large overlapping contents and it guarantees _ authenticity _ and _",
    "confidentiality_.    note that ` sec - cs ` is limited to immutable contents , i.e. , it does not support deletion of contents .",
    "we present this variant as it is sufficient for the evaluation of ml- * , but we emphasize that it is easy to extend it to a mutable variant , either by allowing deletion of root nodes ( requiring a garbage collection for non - referenced chunks ) , or using reference counters for chunks ( at the cost of some slight storage overhead ) .",
    "in fact , our implementation ( see sec .  [ implementation ] ) supports the latter .",
    "` sec - cs ` requires a backend to persist data .",
    "low - level storage management is out of scope of this paper , though .",
    "instead , we assume the existence of a backend providing the following _ key - value store ( kvs ) _ interface :    @xmath27persist value @xmath28 under key @xmath29    @xmath30return @xmath28 or @xmath31 if key @xmath29 does not exist    note that the kvs interface can be easily mapped to any commonly - used storage backend : key - value databases and many cloud ( object ) storage providers can be accessed by this interface and a mapping to a file / directory structure in a file system could be done in a straightforward manner .",
    "the major requirement is that the backend can deal efficiently with many key / value pairs .",
    "the goal of ` sec - cs ` is to allow efficient and secure usage of existing cloud storage for storing file contents , especially in presence of many ( similar ) versions of contents and an untrusted cloud storage provider . towards this goal ,",
    "our model includes two parties , a user ( client ) and a backend ( server ) .",
    "the user is assumed to be completely trustworthy : she instantiates the data structure and locally executes its operations in order to change its state . any operation invocation done by the user is considered legitimate . the user is required to locally store and keep secret a fixed number of constant - size cryptographic keys .",
    "the backend does not need to be trustworthy at all .",
    "it might read any stored data and also write , overwrite or delete any data as to perform malicious modifications ( e.g. , changes to file contents ) that remain undetected by the user .",
    "the only restriction is that it is assumed not to be able to get access to the client s cryptographic keys .",
    "we aim for achieving authenticity in the sense that only contents actually inserted into ` sec - cs ` by the user can be successfully retrieved , and we aim for confidentiality in the sense that the backend can not obtain any part of any stored content .",
    "security guarantees are defined formally in conjunction with efficiency goals in sec .",
    "[ basic_content_store_definition ] due to their interdependence .",
    "a general overview is given beforehand .",
    "note that the model allows a backend to mount dos attacks ( e.g. , deleting data ) . as such attacks",
    "are detectable by a user , a backend has a financial incentive in avoiding it .",
    "also , it could be prevented easily via replication .",
    "due to its storage efficiency guarantees , ` sec - cs ` requires a tailored security concept .",
    "we discuss reasons and design decisions now and give a formal definition thereafter .",
    "using cryptographic hash values to reference nodes of a chunk tree yields a merkle - tree@xcite - like data structure that trivially guarantees integrity of a content given the identifier of the root node of its chunk tree . we can use a _ message authentication code ( mac ) _ with a secret , symmetric key instead of an unkeyed hash to also guarantee authenticity of contents stored in the data structure .",
    "integration of confidentiality is more complicated : for ideal guarantees , we would have to encrypt contents ( e.g. , using a symmetric block cipher ) before constructing their chunk trees as to authenticate their ciphertexts ( _ encrypt - then - authenticate _ ) . unfortunately",
    ", this would prevent data deduplication : with a randomized encryption scheme , deduplication would not be possible at all , and with a deterministic scheme , deduplication would only be possible at the granularity of complete contents . to allow for storage efficiency , we have to employ encryption at the granularity of the chunks that are to be deduplicated .    a straightforward application of the generally favorable _ encrypt - then - authenticate _ approach on chunk tree node representations utilizing a randomized encryption scheme , however , would still prevent deduplication as even identical chunk tree nodes yielded different mac tags ( thus different keys ) due to different ciphertexts .",
    "_ authenticate - then - encrypt _ can also be considered secure for specific instantiations  @xcite , but randomized encryption of mac tags would lead to the same problem .",
    "the third option would be _ encrypt - and - authenticate_. if applied to chunk tree nodes during insertion into the backend , deduplication would be possible even with randomized encryption , as each chunk tree node was associated a randomized ciphertext during its _ first _ insertion without affecting other parts of the data structure .",
    "application of encrypt - and - authenticate , however , is generically considered insecure even for practical mac instantiations , thus requiring a careful analysis of the security properties actually achieved by any specific instantiation .",
    "@xcite    to avoid any of these potential pitfalls , we achieve _ confidentiality and authenticity _ by using an _ authenticated , deterministic encryption scheme _ to encrypt and authenticate chunk tree nodes before their insertion into the backend .",
    "block cipher modes like eax  @xcite and ocb  @xcite would be suitable for this purpose .",
    "they provide confidentiality and authenticity and their ciphertexts are length - preserving ( except for the authentication tag ) , eliminating padding - induced storage overhead .",
    "these schemes , however , depend on a nonce that would have to be stored somehow to allow decryption of persisted chunks , which again caused overhead .",
    "the siv construction  @xcite solves this issue : by using a plaintext s mac tag as iv for an underlying , conventional iv - based encryption scheme ( e.g. , ctr mode ) , it achieves authentication and length - preserving encryption . while siv depends on a nonce , too , it is resistant to nonce reuse in the sense that no more information than whether two encrypted plaintexts are identical is leaked .",
    "@xcite as this is leaked _ intentionally _ in our system to allow deduplication , it is safe to use siv without a nonce , leading to storage costs identical to those of an authentication - only solution ( i.e. , overhead equals authentication tag size ) .",
    "the data structure is now described in detail , including its interface , formal goals and internal algorithms .",
    "the minimum operation set for a content data structure includes _ insertion _ and _ retrieval _ :    @xmath32 shall insert the content @xmath6 into ` sec - cs ` and make it accessible by the key @xmath29 .",
    "we state the following _ storage efficiency goals _ :",
    "the ( expected ) increase of the data structure s storage consumption caused by @xmath33 should be in @xmath34.[goal_put_storage_simple ] 2 .   if @xmath6 is _ highly redundant _ ,",
    "i.e. , another @xmath8 is already stored that is identical to @xmath6 except for a single sequence of @xmath35 bytes , the expected increase in storage consumption caused by @xmath33 shall be in @xmath36.[goal_put_storage_efficiency ]    note that [ goal_put_storage_efficiency ] is defined rather vaguely .",
    "its precise semantics depends on the choice of @xmath0 : for sc , the difference between contents @xmath6 and @xmath8 is defined as the smallest byte range that would have to be copied from @xmath6 to @xmath8 to turn @xmath8 into @xmath6 , or vice versa . since cdc supports shifting of contents , some differences between two contents can be represented more compactly , i.e. , by a sequence of bytes that is inserted at or removed from a specific byte offset of one content .",
    "the more efficient the underlying chunking scheme , the stronger is thus the goal .",
    "@xmath37 shall retrieve a content @xmath6 previously inserted into ` sec - cs ` via the key @xmath29 .",
    "we state the following _ authenticity goal _ :",
    "if any call @xmath38 with @xmath39 has been issued before , then it holds @xmath40.[goal_get_authenticity ]    goal  [ goal_put_storage_efficiency ] implies the more general case of @xmath6 / @xmath8 being different in @xmath35 bytes spread over @xmath41 different positions : imagine the sequence @xmath42 , @xmath43 , @xmath44 , @xmath45 , @xmath46 of _ intermediate _ contents with @xmath47 containing the first @xmath48 differences between @xmath8 and @xmath6 and let @xmath49 refer to the number of bytes changed between @xmath50 and @xmath47 .",
    "if each of these contents was inserted one after another , insertion of @xmath47 would cause an increase in storage consumption of @xmath51 , totaling @xmath52 for all contents .",
    "the sizes of @xmath53 are upper - bounded by @xmath54 , so total increase in storage consumption is in @xmath55 .",
    "this boundary sublinear in the length of @xmath6 would not be possible if only leaf chunks were deduplicated , so goal  [ goal_put_storage_efficiency ] implies req .",
    "[ rec_chunking_req_identical ] .    as any operation execution has to preserve confidentiality of all contents",
    "ever stored , we state the _ confidentiality goal _ independent from a specific operation :    for each content @xmath6 ever inserted into the data structure , the storage provider must not learn anything beyond    its length[conf_leak_length ] ,    chunk boundary positions leaked by @xmath0 for target chunk sizes @xmath56 , where @xmath15 is chosen as in eq .",
    "[ eq : chunking_levels ] for @xmath14[conf_leak_chunking ] , and    equality of chunks of @xmath6 according to the aforementioned chunk boundary positions ( w.r.t .",
    "all leaf chunks and superchunks ever stored)[conf_leak_equality ]    .[goal_confidentiality ]    note that constraints  [ conf_leak_chunking ] and  [ conf_leak_equality ] are unavoidable for achieving storage efficiency , as worked out in sec .",
    "[ authenticity_confidentiality ] .",
    "thus , strength of goal  [ goal_confidentiality ] is highly dependent on @xmath0 .",
    "the data structure s efficiency can be tuned by setting the following parameter during initialization :    @xmath11 is the target chunk size , i.e. , the expected size of leaf / superchunks generated by multi - level chunking .",
    "further , there is an implementation - specific parameter @xmath13 referring to the storage consumption of chunk references in superchunk representations .",
    "we require @xmath57 , which will allow us to meet goal  [ goal_put_storage_simple ] ( see sec .",
    "[ basic_correctness_content_insertion ] ) .      `",
    "sec - cs ` is based on some algorithms and assumptions :    let @xmath58 be a dae - secure ( see  @xcite ) , deterministic authenticated encryption scheme that generates length - preserving ciphertexts and _ message authentication codes ( macs ) _ of length @xmath59 . note that macs are used to reference chunks , so it holds @xmath60 .",
    "let @xmath0 be a deterministic , _ single - level _ chunking scheme that produces chunks of a ( configurable ) expected length @xmath61 as used in sec .",
    "[ recursive_chunking ] .",
    "we assume that the backend s storage costs for storing a key - value pair @xmath62 are in @xmath63 .",
    "now we are ready to define the behaviour .",
    "[ [ initialization ] ] initialization + + + + + + + + + + + + + +    when ` sec - cs ` is initialized , parameter @xmath11 is specified and @xmath64 is executed to determine a symmetric cryptographic key @xmath65 for authenticated encryption .    [ [ content - insertion - k - leftarrow - textscputcontentm ] ] content insertion : @xmath32 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    insertion of contents is performed according to the definition of ml-@xmath0 ( see sec .  [ recursive_chunking ] ) which is refined here .",
    "first , the height @xmath15 of the chunk tree @xmath7 for content @xmath6 is calculated according to eq .",
    "[ eq : chunking_levels ] .",
    "the tree is then built and its nodes are persisted by executing the recursive alg .",
    "[ alg : putchunk ] , which utilizes @xmath0 to perform the appropriate chunking of the content at each individual level and yields some key @xmath66 for the root node .",
    "we return @xmath67 as the content s key .",
    "is an auxiliary construction .",
    "it enables equal length and size for leaf chunks by not requiring storage of the node type . ]",
    "each node is persisted by the algorithm using @xmath68 .",
    "confidentiality is achieved by encrypting node representations ; deduplication+authentication are achieved by using macs as keys .",
    "as superchunks are represented as lists of their children s keys , this yields a merkle - tree - like structure of mac values , achieving authentication of contents .",
    "@xmath69 @xmath70 @xmath71 @xmath70 [ ] apply @xmath0 to @xmath8 with target chunk length @xmath21 children.append ( ) @xmath69 @xmath70    [ [ content - retrieval - m - leftarrow - textscgetcontentk ] ] content retrieval : @xmath37 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    retrieval of a content works similar to its insertion .",
    "first , the root chunk key @xmath66 and the tree s height @xmath24 are extracted from the content key @xmath29 .",
    "afterwards , the recursive alg .",
    "[ alg : getchunk ] is executed , which retrieves all nodes of the chunk tree and concatenates its leaf chunks , yielding the corresponding content @xmath6 .",
    "each node is decrypted and checked for authenticity on that way .",
    "the algorithm aborts if any node is missing or any node with an erroneous @xmath72 tag is retrieved from the backend .",
    "the operation yields @xmath31 then .",
    "@xmath73 @xmath70 @xmath74 @xmath70 failure    we also designed optimized , non - recursive variants of these operations , which are equivalent but more computationally efficient as they need only a single pass of chunking .",
    "they are omitted from the paper due to space restrictions , but included in our implementation ( see sec .",
    "[ implementation ] ) .",
    "we show that the operations from sec .  [ basic_operations ] achieve the goals from sec .",
    "[ basic_interface ] .",
    "note that the ability of @xmath0 to produce chunks of an expected length is crucial for the discussion .",
    "[ [ basic_correctness_content_insertion ] ] content insertion + + + + + + + + + + + + + + + + +    _ insertion _ builds a chunk tree whose nodes at each level each represent the whole inserted content .",
    "all nodes are persisted in the backend and made accessible by individual keys .",
    "as the root node s key transitively allows access to all nodes , the operation is consistent with the required interface .",
    "regarding storage efficiency , we already showed a constant expected per - chunk storage consumption in sec .",
    "[ recursive_chunking ] .",
    "thus , it is sufficient to consider the number of modified chunks to analyze the asymptotic storage costs incurred by the operation .",
    "goal  [ goal_put_storage_simple ] requiring linear storage costs for a content @xmath6 is achieved due to the following argument : as size and length are equal for leaf chunks and as there can not be more than @xmath54 leaf chunks in total , storage costs of all leaf chunks are in @xmath34 .",
    "further , as we have @xmath75 , every superchunk is expected to have at least two children , implying less expected superchunks than leaf chunks .",
    "this proves an expected total storage consumption of @xmath34 .",
    "goal  [ goal_put_storage_efficiency ] is analyzed in detail in sec .",
    "[ evaluation ] , so we only provide an informal argument at this point : as described in sec .  [ recursive_chunking ] , a content differing in one byte from an existing content has storage consumption @xmath10 .",
    "the main technical difference when @xmath35 consecutive bytes differ instead of @xmath76 byte is that those @xmath35 bytes might be spread over multiple chunks .",
    "concerning storage costs , this is similar to inserting those @xmath35 bytes as a separate content , so the storage overhead is limited to @xmath77 , resulting in a total storage consumption of @xmath36 .",
    "[ [ proof_content_retrieval ] ] content retrieval + + + + + + + + + + + + + + + + +    _ retrieval _ retrieves all nodes of a previously built chunk tree and concatenates its leaf chunks , trivially fulfilling the interface . to prove authenticity , we formalize goal  [ goal_get_authenticity ] with the _ authenticity - breaking game _ :    the data structure is initialized .",
    "an adversary @xmath78 is given oracle access to insertcontent and to the implementation of ` sec - cs ` .",
    "she may issue queries at choice to fill it and is granted full read / write access to the backend .    at some point , @xmath78 outputs an identifier @xmath29 .",
    "we say @xmath78 _ wins _ if a retrieve query for @xmath29 returns @xmath8 but an insert of a different @xmath79 was performed under identifier @xmath29 before",
    ". otherwise @xmath78 loses .    using this game ,",
    "the authenticity property can be shown :    [ claimb ] if macs produced by @xmath80 are unforgeable under a chosen message attack , no adversary can win the auth .- breaking game with non - negligible probability .",
    "assume @xmath78 wins the game with non - negligible probability .",
    "let @xmath29 be the identifier and let @xmath8 be the forged content returned by _",
    "retrieve_. as the operation only depends on @xmath81 calls , which in turn only depend on @xmath82 operations , at least one @xmath82 call during execution of retrieve must have returned a forged result .",
    "let @xmath83 be the first such call . by definition of alg .",
    "[ alg : getchunk ] , verification of @xmath66 being a correct mac for @xmath74 must have been true for retrieve to be successful .",
    "then , as @xmath78 knows the algorithms used by the data structure , @xmath78 is able to find two different values @xmath84 with the same mac @xmath29 ( the value she inserted initially and the forged value ) . as macs produced by @xmath80",
    "are assumed to be unforgeable , this happens only with negligible probability , contradicting our assumption and proving goal  [ goal_get_authenticity ] .",
    "[ [ confidentiality_proof ] ] content confidentiality + + + + + + + + + + + + + + + + + + + + + + +    goal  [ goal_confidentiality ] states that an adversary must not learn anything more about any content @xmath6 ever stored in the data structure than its length , its chunk boundaries according to the used chunking scheme , and equality relations across all stored chunks . to prove that no more information is leaked by any operation execution ,",
    "we show that the intentionally leaked information is sufficient for a consistent simulation of any operation .",
    "let @xmath85 be the set of contents for which @xmath86 is executed at any time , let @xmath87 be any fixed content and let @xmath78 be an adversary trying to obtain information about @xmath6 .",
    "acc .  to constraint  [ conf_leak_length ] , @xmath78 is allowed to know the content s length @xmath14 . since the data structure s parameters ( see sec .  [ basic_parameters ] ) are public , @xmath78 can , thus , determine the height @xmath15 of the chunk tree @xmath7 of @xmath6 acc .  to eq .",
    "[ eq : chunking_levels ] .",
    "constraint  [ conf_leak_chunking ] reveals the chunk boundaries of @xmath6 output by @xmath0 for chunk sizes @xmath56 .",
    "it is easy to see that these are exactly the chunk boundaries that are computed during a legitimate @xmath33 call , i.e. , in line 7 of every execution of alg .",
    "[ alg : putchunk ] . in combination with the length of @xmath6 , @xmath78",
    "can , thus , determine the byte ranges of all leaf chunks and superchunks of @xmath6 .",
    "this allows her to construct an abstract chunk tree @xmath88 that has the exact same structure as @xmath7 , but whose nodes contain _ abstract _ chunk representations that represent only the respective chunk s length instead of actual chunk representations .    since equality of any two chunks ever stored",
    "is leaked acc .  to constraint  [ conf_leak_equality ] ,",
    "@xmath78 can further assign a unique identifier to any ( abstract ) chunk representation so that the identifiers of two chunk representations are equal iff their represented contents are identical . without loss of generality , we assume that @xmath78 assigns identifiers of the form @xmath89 , where @xmath90 is a value of length @xmath13 chosen uniformly at random and @xmath91 is a value chosen uniformly at random whose length equals the represented chunk s length in case of a leaf chunk or @xmath92 in case of a superchunk with @xmath93 children ( @xmath78 can calculate these values based on @xmath88 ) .",
    "now we can show that @xmath78 can also be provided with the encrypted / auth .",
    "representations of all chunks ever stored without revealing further information about any @xmath6 .",
    "[ claimc ] if @xmath80 is dae - secure , the probability that an adversary learns anything beyond  [ goal_confidentiality ] about any content @xmath6 from the nodes stored in the data structure is negligible .",
    "assume @xmath78 is able to learn something from the encrypted and authenticated chunk representations beyond the aforementioned information with non - negligible probability .",
    "first , it is easy to see that the lengths of @xmath94 previously generated chunk identifiers are equal to the lengths of the actual chunk representations , so she can not learn anything from the lengths .",
    "being able to learn something from the chunk representations thus implies that she is able to distinguish whether she is given the actual encrypted and authenticated chunk representations or just random strings with the respective lengths .",
    "let @xmath95 be her algorithm that on input the information about all contents ever stored in ` sec - cs ` as stated in [ goal_confidentiality ] and a complete set of chunk representations of the respective lengths outputs @xmath76 if the chunk representations are actual chunk representations and @xmath96 otherwise .",
    "now construct an algorithm @xmath97 with access to an @xmath98 oracle ( with a randomly chosen key ) as follows :    initialize a new ` sec - cs ` data structure and insert all contents @xmath99 , using the oracle as encryption function , but remembering and reusing oracle outputs instead of asking for same input multiple times .",
    "pass all information about every content of @xmath85 as stated in [ goal_confidentiality ] as well as all ( encrypted and authenticated ) chunk representations to @xmath95 , yielding output @xmath41 .",
    "return @xmath41 .    if @xmath97 has access to an actual @xmath98 oracle , @xmath95 is given actual chunk representations as created by the data structure . if a random oracle @xmath100 with outputs of respective lengths is given to @xmath97 instead , @xmath95 gets only random data .",
    "if @xmath95 is able to distinguish both cases with non - negligible probability , @xmath97 is thus able to distinguish a random oracle from an encryption oracle with non - negligible probability .",
    "according to the definitions given in @xcite , @xmath97 would be an adversary with non - negligible dae - advantage , which contradicts the assumption that @xmath80 is dae - secure .    at this point",
    ", @xmath78 knows the complete chunk tree @xmath7 for every content @xmath6 ever stored in a ` sec - cs ` instance , including the ( encrypted and authenticated ) chunk representation of every chunk tree node .",
    "we have already seen that @xmath78 can not obtain more information about any content based on this data than stated in goal  [ goal_confidentiality ] .",
    "now we show that even metadata ( i.e. , access patterns from individual operation executions ) do not reveal anything more about any individual content .",
    "the idea of the proof is as follows : when a data structure operation is executed , @xmath78 can only see kvs operation calls made by ` sec - cs ` . if @xmath78 is able to simulate any data structure operation execution to the extent that all kvs operation calls are consistent to a real execution based on information she already has , she does not learn anything from a real operation execution .",
    "consider a @xmath33 call .",
    "its execution simply consists of a call of alg .",
    "[ alg : putchunk ] with an additional argument @xmath15 .",
    "knowledge of @xmath15 allows her to simulate that call , although she can not provide the content @xmath6 to alg .",
    "[ alg : putchunk ] . the algorithm can be executed consistently given @xmath7 , though : consider any execution of @xmath101 . if @xmath102 , the execution corresponds to a leaf chunk of @xmath7 that is encrypted , authenticated and inserted using @xmath68 .",
    "since @xmath78 already knows the representation of the corresponding chunk , she can simply issue the @xmath68 call of line 4 .",
    "otherwise , alg .",
    "[ alg : putchunk ] performs chunking on the respective chunk , issues recursive calls for the resulting chunks and inserts an encrypted , authenticated superchunk .",
    "@xmath78 can perform the recursive calls by extracting the children of the current chunk from @xmath7 ; she can determine the superchunk representation @xmath69 from line 10 as it is contained in @xmath7 , and she can issue the @xmath82 and @xmath68 calls from lines 1112 since they depend only on @xmath66 , @xmath73 and the children s identifiers .    the call of alg .",
    "[ alg : getchunk ] during @xmath103 is possible for @xmath78 due to the same reasons as before .",
    "each individual recursive @xmath104 call corresponding to a node of @xmath7 can be trivially simulated by @xmath78 : the only operation not directly executable by @xmath78 is the @xmath105 call in line 3 . for the simulation , though , it is sufficient to distinguish three cases .",
    "first , @xmath105 fails whenever @xmath66 is not a valid authentication tag corresponding to ciphertext @xmath73 .",
    "since @xmath78 knows the correct chunk representation @xmath106 for the respective chunk from @xmath7 , she can assume the call to be successful iff @xmath107 except with negligible probability .",
    "second , if @xmath18 , @xmath66 is the identifier of a superchunk , so @xmath74 is a list of its children s keys , which she can simply extract from @xmath7 . only if @xmath102 , @xmath78 fails to compute @xmath74 . in this case , however , all subsequent operations performed by a benign client are exclusively local ( without any feedback to the storage backend ) , so the simulation is consistent and sound from an adversary s perspective .",
    "thus , @xmath78 is able to perform consistent simulations of all operations , which proves goal  [ goal_confidentiality ] .",
    "note that choice of @xmath0 defines a trade - off between confidentiality and storage efficiency .",
    "if @xmath0 , e.g. , was wfc , strongest security guarantees could be achieved ( although this would fail to achieve storage efficiency ) : [ conf_leak_chunking ] would not leak any information at all and [ conf_leak_equality ] would only leak equality of complete contents . if sc was used , [ conf_leak_chunking ] would still not leak any information as its output depends only on a content s length which is covered by [ conf_leak_length ] , but equality of ( small ) chunks naturally provides an adversary with more information . for cdc schemes , after all , [ conf_leak_chunking ] becomes relevant as chunk boundaries are computed based on plaintext content parts .",
    "precise security implications depend on the specific scheme and can not be determined in general .",
    "an analysis for one scheme is given in @xcite .",
    "to ease adoption in practice and to allow for an empirical evaluation ( see sec .  [ evaluation ] ) , we have created an implementation .",
    "the data structure including our chunking scheme ml- * is wrapped into a flexible python module named ` seccs ` available for download in the python package index  @xcite or via ` pip install seccs ` .",
    "unit tests verifying the implementation s correctness w.r.t .  goals  [ goal_put_storage_simple ] , [ goal_put_storage_efficiency ] and [ goal_get_authenticity ] are bundled with the module .",
    "since we could not find a sufficiently efficient rolling hash python implementation , we also developed a rolling - hash - based chunking module ` fastchunking ` compatible to ` seccs ` and available for download in pypi as well .",
    "it is a wrapper for parts of the highly efficient _ ngramhashing _ c++ library  @xcite by daniel lemire and thus able to outperform pure - python implementations .",
    "we present an extensive evaluation of ` sec - cs ` s storage efficiency consisting of two parts : sec .",
    "[ evaluation_seccs ] confirms our stated efficiency goals both analytically and empirically and sec .",
    "[ evaluation_comparison ] compares the performance of ` sec - cs ` s novel chunking scheme ml- * to other approaches .      to provide concrete numbers",
    ", we make some assumptions about the implementation of ` sec - cs ` .",
    "we assume that a deterministic authenticated encryption scheme with length - preserving ciphertexts and @xmath108-bytes macs is used ( e.g.  aes - siv-256 ) , resulting in a constant storage requirement of @xmath109 bytes for chunk references .    by _ storage costs _",
    ", we refer to the storage consumption of the used kvs ( see sec .  [ prerequisites ] ) for some state . to be independent of any specific backend data structure , we ignore any overheads and roughly estimate storage costs as the sum of the sizes of the kvs s elements , where an element s size is the sum of the sizes of its key and value .      to complement the proofs from sec",
    "[ basic_correctness ] , we evaluate [ goal_put_storage_efficiency ] in detail .",
    "we do not continue an asymptotic discussion but work out concrete storage costs to judge suitability of ` sec - cs ` in practice .",
    "the analytical deduction is given below and its validity is confirmed empirically afterwards .",
    "let @xmath8 be a content consisting of random bytes that is already present in ` sec - cs ` .",
    "goal  [ goal_put_storage_efficiency ] states that insertion of @xmath6 differing only in a sequence of @xmath35 bytes should cause storage costs in @xmath36 . to work these costs out more precisely ,",
    "we analyze the border cases first .",
    "[ [ border - case - delta - m ] ] border case : @xmath110 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    if @xmath110 , chunk trees @xmath7 and @xmath9 of @xmath6 and @xmath8 are not expected to share any nodes , so storage of @xmath6 should cause costs in @xmath34 acc .  to the stated goal .",
    "( note that this case also covers goal  [ goal_put_storage_simple ] . ) since leaf chunks have average length @xmath11 , the expected number of leaf nodes of @xmath7 is @xmath111 .",
    "for the same reasons as in the proof in sec .",
    "[ basic_correctness_content_insertion ] ( @xmath112 implies superchunks are expected to have @xmath113 children ) , @xmath7 has more expected leaf than superchunk nodes , so its total expected number of nodes is : @xmath114    every chunk tree node has an expected size of @xmath11 according to sec .  [ recursive_chunking ] and",
    "is stored under a @xmath59-byte digest , so storage costs for this case are as follows : @xmath115    [ [ border - case - delta-1 ] ] border case : @xmath116 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    if @xmath116 , @xmath6 and @xmath8 differ only in 1 byte . here , storage costs depend on ml- * s underlying chunking strategy @xmath0 . in case of @xmath117 , @xmath7 and @xmath9",
    "differ in exactly one node at each level , each having size @xmath118 , which trivially results in the following storage costs : @xmath119    for @xmath120 , however , the situation is more complex .",
    "first , @xmath116 allows for shifting in case of cdc .",
    "second , @xmath7 and @xmath9 might differ in more than one node at each level .",
    "the reason is that the modification of a single byte might change up to @xmath1 chunk boundaries at each level of the chunk tree , probably causing extra chunks to be inserted as well . to determine the total average number of chunk tree nodes that are inserted in this case , we analyze how many new chunks are created at each chunking level .",
    "let us fix some height @xmath121 . at height",
    "@xmath19 we only have a single ( root ) chunk that represents the whole content @xmath6 . at height",
    "@xmath122 we deal with chunks of average length @xmath25 ( a position is a boundary with probability @xmath123 ) according to sec .  [ recursive_chunking ] . when considering a fixed @xmath1-byte window containing the changed byte , the probability that this window yields a chunk boundary in @xmath6 but did not yield a chunk boundary in @xmath8 is @xmath124 .",
    "the same probability holds for the case in which a chunk boundary present in @xmath8 is not present in @xmath6 anymore . as the resulting new chunks and the chunk that has to be inserted anyway at this level are not necessarily consecutive boundaries in an area of @xmath1 bytes after the change position , but only the chunk before the first boundary contains the change .",
    "if two consecutive boundaries in this area exist in @xmath8 and @xmath6 , the chunk in between is unchanged , but might be followed by changed chunks if further boundaries are changed . ] , creation ( omission ) of a chunk boundary in contrast to @xmath8 might cause up to 1 ( 2 ) additional chunks at height @xmath24 , respectively .",
    "since there are up to @xmath1 window contents containing the changed byte at each chunking level and each position yields 1 or 2 new chunks each with probability @xmath124 , we expect up to @xmath125 new chunks at height @xmath24 .",
    "as we have a single changed chunk at height @xmath15 , we get the following upper bound for the expected number of chunk tree nodes differing between @xmath7 and @xmath9 : @xmath126    while chunking is performed in a way that achieves an average size of @xmath11 for every chunk when applied to random content , we can not assume an average size of @xmath11 for _ additional _ chunks created for inserting @xmath6 in addition to @xmath8 .",
    "the rationale is that new chunks are created from existing chunks _ not _ chosen uniformly at random : a random position in a content is more likely to hit large chunks than smaller ones as more positions are covered by them .",
    "consider the chunks of @xmath8 at some fixed height @xmath24 . as",
    "each byte position is a height-@xmath24 chunk boundary with probability @xmath127 , the probability for a chunk having length @xmath128 is @xmath129 .",
    "note that @xmath130 since @xmath131 . as we expect an average number of @xmath132 chunks at height @xmath24 in total ,",
    "the expected number of length-@xmath128 chunks at height @xmath24 is @xmath133 .",
    "since each of those chunks covers @xmath128 bytes and since the total content length is @xmath134 , the fraction of the content that is covered by chunks of length @xmath128 is : @xmath135    thus , the expected _ length _ of a height-@xmath24 chunk at a position chosen uniformly at random is : @xmath136    as height-@xmath24 superchunks store @xmath13-byte references to height-@xmath137 chunks of avg .",
    "length @xmath21 and as size equals length for leaf chunks , the exp .",
    "_ size _ of a height-@xmath24 chunk at a random position ( and thus the exp .",
    "size of any chunk created when inserting @xmath6 ) is upper - bounded by : @xmath138    this results in the following storage requirement:@xmath139    [ [ remaining - case-1-delta - m ] ] remaining case : @xmath140 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in the remaining case , i.e. , @xmath6 differing from @xmath8 in a sequence of more than @xmath76 but less than @xmath54 bytes , both the first and the last byte of the change bytestring affect chunks as in the case @xmath116 . for ml - cdc , they are likely to be part of _ large _ chunks of expected length @xmath141 for the same reasons as discussed above .",
    "we conservatively estimate that these two bytes cause storage costs of @xmath142 ( with @xmath143 ) . the remaining bytes are either part of those chunks ( so their storage costs have already been accounted for ) , or they result in the same chunks that would be created if they were inserted into ` sec - cs ` as a separate content , causing storage costs up to @xmath144 .",
    "thus , we estimate the total storage requirement as follows : @xmath145    as @xmath15 is logarithmic in @xmath54 , all cases fulfill goal  [ goal_put_storage_efficiency ] .              as there are no conceptual differences between ml - sc / ml - cdc w.r.t .",
    "to the goal evaluated in this section , we focus on the more interesting ml - cdc scheme in the empirical evaluation .",
    "we perform experiments with our implementation ( see sec .  [ implementation ] ) meeting the parameters from sec .",
    "[ evaluation_assumptions ] , using a rabin - karp - based cdc scheme with window size @xmath146 bytes ( based on evaluation results of the cdc authors  @xcite ) and no min / max  chunk size set .",
    "we simulate the scenario from the analytical evaluation : we choose a content @xmath6 of size @xmath54 uniformly at random , insert it into an empty ` sec - cs ` instance and remember its state .",
    "we replace a randomly chosen @xmath35-bytes substring ( @xmath147 ) of @xmath6 by a different @xmath35-bytes substring chosen uniformly at random , insert the resulting content @xmath8 and compare ` sec - cs ` s size to the remembered one to measure increase in storage costs .",
    "we executed the experiment @xmath148 times for each combination of content size @xmath54 , chunk size @xmath11 and @xmath35 , including border cases @xmath149 .",
    "[ fig : g2_delta_1 ] compares empirical and analytical results for @xmath116 , showing the minimal expected storage overhead for insertion of highly redundant contents .",
    "solid lines show the calculated relation between content sizes and increase in storage costs for different chunk sizes . while sublinear growth is visible for either chunk size , smaller sizes result in even lower storage costs , unless the chunk size is chosen unreasonably small : for the smallest evaluated chunk size ( @xmath150  bytes )",
    ", costs incurred by additional superchunk levels outweigh the smaller per - chunk costs .",
    "threshold content sizes resulting in a respective number of chunking levels are indicated by the positions of the numbers in the chart .",
    "the reason for the leaps at threshold sizes is that our estimate is based on a constant chunk size , while root nodes are smaller in practice ( proportional to content size for a fixed tree height ) , resulting in smaller trees .",
    "empirical results confirm growth is smoother in practice : dotted lines show loess  @xcite curves fitted to the measured increase in storage costs ( smoothing parameter 0.75 , degree 2 ) , summarizing its relation to content length for the respective chunk sizes .",
    "results are in line with the calculated upper bounds , confirming sublinear growth in general and least overhead for @xmath151 .    to verify whether the most promising chunk size of @xmath152 bytes also yields small overheads for larger modifications , results for @xmath152 in the general case ( @xmath153 ) are shown in detail in fig .",
    "[ fig : g2_delta_all ] .",
    "solid lines represent analytical upper bounds for different modification lengths , yielding least costs for @xmath116 ( red ) .",
    "empirical data including outliers ( black points ) are illustrated as box plots , which are barely visible since they indicate rather small fluctuations in storage costs .",
    "the box plots confirm that the analytical bound is conservative , especially for @xmath154 : even outliers are below their corresponding analytical line .",
    "we compare performance of ml- * empirically to that of the other approaches . to allow a fair comparison , we evaluate all schemes with ` sec - cs ` .",
    "note that all schemes are in fact special cases of ml - sc / ml - cdc : fixing the height of generated chunk trees to 1 results in sc / cdc , respectively , where all metadata representing a content are collected in a single root node ; a fixed height of 0 maps each content to a leaf - only tree , corresponding to wfc .",
    "every scheme incurs storage overhead that is necessary to enable data deduplication . in case of wfc ,",
    "only a small digest ( i.e. , a hash value ) has to be stored for each content for this purpose , resulting in constant overhead . when smaller chunks are stored , overhead is incurred both due to the digests for individual chunks and for storage of a content s representation , i.e. , a list of digests of chunks needed to reconstruct it . this overhead is usually compensated by savings from storage of deduplicable contents .    before comparing the savings achieved by the different schemes ,",
    "we take a look at the overhead that is incurred when _ non - deduplicable _ contents are stored .",
    "we instantiate ` sec - cs ` for each chunking strategy and with different chunk sizes and perform the following experiment : we insert a fixed - size content chosen uniformly at random into the empty data structure and measure the _ storage expansion factor _ , i.e. , total storage costs divided by the actual content size .",
    "we repeat the experiment for different content sizes , 20 times for each combination of parameters .    the loess curves in fig .",
    "[ fig : content_relative_storage_consumption ] show the measured expansion factor for different content sizes , which is constant for larger content sizes .",
    "it shows that the expansion factor is nearly 1 ( in fact , storage overhead is constant : 32 bytes per content ) for wfc ( solid black line ) and slightly above 1 for any scheme with small chunk sizes . with higher chunk sizes ,",
    "the overhead grows significantly : sc / cdc ( dotted lines ) have an expansion factor of 1.25 / 1.5 / 2.0 for chunk sizes 64 / 128 / 256 ; the expansion factor for ml- * ( dashed lines ) is even higher due to additional storage of non - root superchunk nodes .",
    "interestingly , ml - cdc produces even more storage overhead than ml - sc .",
    "this is due to the concept of ml- * handling each chunk as individual content . in case of ml - sc ,",
    "the last nodes at each chunk tree height represent chunks smaller than @xmath11 , causing overproportional costs , while any other node is the root of a full , balanced tree .",
    "in ml - cdc , these costs are caused recursively by the last nodes of _ every _ subtree .              to measure possible savings from deduplication ,",
    "we modify the previous experiment : we insert a second content that differs from the first in only a single byte at an offset chosen uniformly at random .",
    "[ fig : storage_costs_overwrite ] shows the total increase in storage costs after the second content has been inserted : for wfc , increase corresponds to the inserted content s size ; for sc / cdc , storage costs are only a fraction of that thanks to deduplication , but still linear in the content size .",
    "costs of ml - sc / ml - cdc are orders of magnitude lower and sublinear in the content size .      to account for the strengths of cdc , we perform a slight modification of the previous experiment : instead of overwriting , we _",
    "insert _ a random byte at a random position , leading to a shift of the remaining content .",
    "results are shown in fig .",
    "[ fig : storage_costs_insert ] : as expected , performance of wfc , cdc and ml - cdc is comparable to the previous experiment since they are robust against shifting .",
    "sc and ml - sc , however , yield storage costs of about half of the content size for chunk sizes @xmath155 ( with slight variations in chunk sizes ) , which corresponds to an expected amount of @xmath156 of the content being _ before _ the shift position and thus deduplicable .",
    "observe that costs for sc with @xmath157 are similar to wfc , due to storage expansion factor 2 .",
    "[ dedup_wo_shifting ] and  [ dedup_w_shifting ] have shown that ml- * is significantly more efficient than today s common deduplication strategies when storing contents that differ only slightly from already stored ones , especially for small chunk sizes .",
    "however , sec .  [ storage_overhead ] has shown that this efficiency comes at the cost of a higher storage expansion factor , i.e. , storage of non - deduplicable contents is more expensive in presence of ml- * and small chunk sizes .",
    "this raises the question as to whether and when ml- * is preferable . intuitively , this is the case whenever storage of _ many _ versions of contents is involved , e.g. , in a backup scenario .",
    "we investigate this as follows : we start with a fresh ` sec - cs ` instance containing a single , random 1 ( 10 ) mib content",
    ". then we insert modifications of this content and measure storage costs after each inserted version .",
    "[ fig : storage_costs_many_contents ] ( fig .",
    "[ fig : storage_costs_many_contents10 ] ) shows loess curves displaying smoothed results over 20 runs for each combination of parameters : as expected , wfc yields storage costs of about 1 mib ( 10 mib ) for every stored content version . costs for cdc are only a fraction thanks to deduplication : @xmath158 ( @xmath159 ) yields lowest costs ; for other chunk sizes , cdc incurs significantly higher costs",
    ". if only few versions are stored , ml - cdc yields slightly lower costs for _ any _ chunk size between 256 and 8192 ( 32768 ) bytes .",
    "the more content versions are stored , the more significant are the savings by ml - cdc : when 125 ( 250 ) similar versions are stored , ml - cdc with @xmath160 ( @xmath161 ) requires only half of the storage space as the most - efficient cdc variant ; for 1000 versions , costs are orders of magnitude lower .",
    "note that we omitted results for sc / ml - sc for readability : due to shifting , they are close to wfc .          while the previous experiments have proven ml- * s superiority in hypothetical scenarios involving slight changes on random data , it remains to be analyzed how good it performs in real life .",
    "we leave an extensive evaluation ( e.g. , involving a backup system ) for future work , but perform the following experiment as a starting point : we insert content versions into ` sec - cs ` as before , but instead of random data , we insert _ all _ file contents of all revisions ( as of 2016 - 05 - 16 ) of the redis key - value database git repository  @xcite and measure ` sec - cs ` s storage costs .    results ( fig .  [",
    "fig : storage_costs_revisions ] ) are promising : with about @xmath162 mib for all @xmath163 revisions , wfc causes by far the highest costs .",
    "sc / ml - sc can only slightly reduce these costs as they are not robust against shifting .",
    "costs for cdc are lower and range from about @xmath150 mib for @xmath164 to @xmath165 mib for @xmath157",
    ". for @xmath166 , performance of ml - cdc is comparable to cdc as only a single chunking level is used for most files acc .  to eq .",
    "[ eq : chunking_levels ] . for smaller @xmath11 ,",
    "ml - cdc is significantly more efficient than the other schemes .",
    "ml - cdc with @xmath167 performed best , causing only about @xmath168 mib of total storage costs , which is rather close to the @xmath169 mib required by ( unencrypted ) git .",
    "we have introduced a data structure for encrypted and authenticated storage of file contents , ` sec - cs ` , that employs a novel multi - level chunking strategy , ml- * , to achieve storage efficiency .",
    "the data structure transparently deduplicates identical parts of file contents without relying on information about relations between them , and achieves storage costs for highly redundant contents logarithmic in their lengths .",
    "we have proven its security and evaluated efficiency extensively w.r.t .  other common deduplication concepts .",
    "a ready - to - use , open source python implementation has been published as part of our work as to allow integration in other software projects .    as next step ,",
    "we work on a backup system based on ` sec - cs ` and on an extension that supports partial read and write access to contents , making it suitable as backend for future file systems based on untrusted cloud storage .",
    "m.  t. goodrich , c.  papamanthou , r.  tamassia , and n.  triandopoulos .",
    "athos : efficient authentication of outsourced file systems . in _",
    "information security _ ,",
    "volume 5222 of _ lncs _ , pages 8096 .",
    "springer berlin heidelberg , 2008 .",
    "c.  liu , y.  lu , c.  shi , g.  lu , d.  du , and d .- s .",
    "admad : application - driven metadata aware de - duplication archival storage system . in _ proc .",
    "5th ieee int . workshop on storage network architecture and parallel i / os _ , pages 2935 , 2008 .",
    "d.  teodosiu , n.  bjorner , j.  porkka , m.  manasse , and y.  gurevich . optimizing file replication over limited - bandwidth networks using remote differential compression .",
    "technical report msr - tr-2006 - 157 , microsoft research , november 2006 ."
  ],
  "abstract_text": [
    "<S> we present ` sec - cs ` , a hash - table - like data structure for file contents on untrusted storage that is both secure and storage - efficient . </S>",
    "<S> we achieve authenticity and confidentiality with zero storage overhead using deterministic authenticated encryption . </S>",
    "<S> state - of - the - art data deduplication approaches prevent redundant storage of shared parts of different contents irrespective of whether relationships between contents are known a priori or not .    instead of just adapting existing approaches , we introduce novel ( multi - level ) chunking strategies , ml - sc and ml - cdc , which are significantly more storage - efficient than existing approaches in presence of high redundancy .    </S>",
    "<S> we prove ` sec - cs ` s security , publish a ready - to - use implementation , and present results of an extensive analytical and empirical evaluation that show its suitability for , e.g. , future backup systems that should preserve _ many _ versions of files on little available cloud storage . </S>"
  ]
}