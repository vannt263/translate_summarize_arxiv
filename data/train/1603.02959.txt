{
  "article_text": [
    "we are interested in estimating the expected payoff value @xmath0 in option pricing problems , with @xmath1 and @xmath2 is a given diffusion model defined on @xmath3 .",
    "+ in view of reducing the variance in the estimation , we envisage using the importance sampling technique . for the gaussian setting , the practical use of this idea with monte carlo ( mc ) methods was firstly studied by arouna @xcite , galasserman , heidelberger and shahabuddin @xcite . later on , ben alaya , hajji and kebaier @xcite studied the use of this procedure with the statistical romberg ( sr ) algorithm known for reducing the computation time in the mc method . the approach used in @xcite , consists on applying the girsanov theorem , to obtain @xmath4 and @xmath5 is solution to @xmath6 in this case , the optimal @xmath7 reducing the asymptotic variance of the sr method is given by @xmath8 where @xmath9 is is a given diffusion associated to the process @xmath10}$ ] defined on an extension @xmath11 of the initial space b ( see further on ) . here , for @xmath12 and @xmath13",
    ", @xmath14 denotes the gradient of the function @xmath15 with respect to the second variable at the point @xmath16 . from a practical point of view",
    ", it is worth introducing the discretized version by considering the optimal @xmath17 given by @xmath18 where @xmath19 ( resp .",
    "@xmath20 ) is the euler scheme , with time step @xmath21 , associated to @xmath22 ( resp .",
    "@xmath23 ) .",
    "the aim of this paper is to combine the multilevel monte carlo ( mlmc ) with the importance sampling technique with a properly chosen optimal @xmath24 based on stochastic algorithms .",
    "the mlmc method was introduced and popularized for financial applications by giles @xcite .",
    "this method can be seen as a generalized version of sr method introduced by kebaier in @xcite .",
    "it has been extensively applied to various fields of numerical probability ( brownian stochastic differential equations , lvy - driven stochastic differential equations and more general numerical analysis problems , see e.g. @xcite ) . for more references ,",
    "we refer to the web page https://people.maths.ox.ac.uk/gilesm/mlmc_community.html and the references therein .",
    "note that jourdain and lelong @xcite introduced another approach based on a deterministic minimization of the empirical estimation of the variance @xmath25 .",
    "this approach was recently used by kebaier and lelong @xcite to the mlmc methods combined with importance sampling technique . in these two last references",
    ", the authors used deterministic optimization algorithm instead of stochastic algorithms .    in the context of euler discretization scheme ,",
    "the idea of the mlmc method is to apply the classical mc method for several nested levels of time step sizes and to compute different numbers of paths on each level , from a few paths when the time step size is small to many paths when the step size is large .",
    "more precisely , the euler mlmc method uses information from a sequence of computations with decreasing step sizes and approximates the quantity @xmath26 by @xmath27 the time step sizes is defined by @xmath28 , @xmath29 where @xmath30 and @xmath31 is the refinement factor . here ,",
    "@xmath32 and @xmath33 denote the euler schemes of @xmath22 with time steps @xmath34 and @xmath35 .",
    "it turns out that the optimal @xmath36 in this case is solution to the problem : @xmath37 see section [ preliminaries ] for more details .",
    "a new idea in the current paper , is to approximate the optimal parameter @xmath36 through the minimization of @xmath38,\\ ] ] which is nothing but a proper approximation of @xmath39 , as @xmath40 tends to infinity , where @xmath41 , @xmath42 , denotes the euler scheme of @xmath43 with time step @xmath44 .",
    "the advantage of this new approach is that the variance is computed without any need to discretize the sophisticated process @xmath45 appearing naturally in the limit variance of the problem .",
    "this is useful for practitioners since we need less conditions on the regularity of the payoff @xmath46 and the diffusion coefficients ( in finance , the computation of @xmath47 could constitute a constraint ) .    in our previous work @xcite on the adaptive sr method",
    ", we used a constrained version of robbins monro algorithm @xcite and an unconstrained one @xcite to compute @xmath17 . from a theoretical point of view",
    ", the use of these two stochastic algorithms seems to be problematic in the proof of the central limit theorem of the adaptive euler mlmc . to overcome these technical difficulties",
    ", we use an other version of robbins - monro type algorithms , namely the stochastic algorithm with projection ( see @xcite ) .",
    "moreover , in @xcite the proof of the central limit theorem for the adaptive sr algorithm uses the law stable convergence theorem for the error euler scheme obtained in jacod and protter @xcite and which is given by @xmath48 this theorem can not be used in the mlmc context , since we should consider the euler error on two consecutive levels @xmath49 and @xmath50 . to cope with situation",
    ", we rather use theorem 3 in @xcite given by @xmath51    in the next section , we introduce the mlmc method and the discretized version of our problem . in section [ rm_mlmc ] , we give the results on the convergence of the discretized optimal @xmath36 . moreover",
    ", we establish the convergence results of the problem of estimating @xmath36 using the constrained version of robbins - monro algorithm .",
    "in section [ cltadp_mlmc ] , we first introduce the new adaptive algorithm obtained by combining together the importance sampling procedure and the euler mlmc method .",
    "then , we prove a lindeberg - feller central limit theorem for this new algorithm ( see theorem [ cltmlmc_adaptatif2 ] ) .",
    "further , we obtain a berry - essen type bound ( this non - asymptotic result was not proved in our previous work @xcite ) . in section [ hestonmodel_mlmc ] ,",
    "we give a complexity analysis of the adaptive euler mlmc algorithm and we proceed to numerical simulations to illustrate the efficiency of this new method for pricing an european call option under the heston model .",
    "let @xmath2 be the process with values in @xmath52 , solution to the diffusion @xmath53 where @xmath54 is a @xmath55-dimensional brownian motion on some given filtered probability space @xmath3 and @xmath56 is the standard brownian filtration .",
    "the functions @xmath57 and @xmath58 , @xmath59 , satisfy condition @xmath60 this ensures strong existence and uniqueness of solution of ( [ 1_mlmc ] ) . in practice",
    ", we consider the euler continuous approximation @xmath61 of the process @xmath62 , with time step @xmath63 given by @xmath64\\delta.\\ ] ] it is well known that under condition @xmath65 we have the almost sure convergence of @xmath61 towards @xmath62 together with the following property ( see e.g. bouleau and lpingle @xcite ) @xmath66 \\leq\\frac{k_p(t)}{n^{p/2}}},\\ ] ] where @xmath67 is a positive constant depending only on @xmath68 , @xmath69 , @xmath70 , @xmath71 and @xmath55 .",
    "the euler mlmc method uses information from a sequence of computations with decreasing step sizes and approximates the quantity @xmath26 by @xmath27 the time step sizes is defined by @xmath28 , @xmath29 where @xmath30 and @xmath31 is the refinement factor . for the first empirical mean , the random variables @xmath72 are independent copies of @xmath73 which denotes the euler scheme with time step t. for @xmath74 , the couples @xmath75 are independent copies of @xmath76 whose components denote the euler schemes with time steps @xmath34 and @xmath35 .",
    "however , for fixed @xmath40 , the simulation of @xmath32 and @xmath33 has to be based on the same brownian path .",
    "giles @xcite proved that the mlmc method reduces efficiently the computational complexity of the combination of monte carlo method and the euler discretization scheme .",
    "in fact , the complexity in the monte carlo method is equal to @xmath77 and is reduced to @xmath78 in the mlmc method where @xmath79 $ ] is the order of the rate of convergence of the weak error given by @xmath80 .",
    "so , it is worth introducing the following assumption @xmath81\\quad n^{\\alpha}\\varepsilon_n\\rightarrow   c_{\\psi } , \\quad c_{\\psi}\\in \\rr.\\ ] ] in their recent works , ben alaya and kebaier @xcite proved a central limit theorem for the euler mlmc method with a rate of convergence equal to @xmath82 , @xmath83 $ ] ( see theorem 4 of @xcite ) .",
    "they also obtained a central limit theorem for higher order schemes ( see @xcite ) . in @xcite , they first consider the sample sizes @xmath84 of the form @xmath85 where @xmath86 is a real sequence of positive terms satisfying @xmath87 then , under assumptions @xmath88 and @xmath89 which is given by @xmath90 they proved that this method is tamed by a central limit theorem .",
    "more precisely , the global error normalized by @xmath82 converges in law to a gaussian random variable with bias equal to @xmath91 and a limit variance equal to @xmath92 where @xmath45 is the weak limit process of the error @xmath93 defined on @xmath94 an extension of the initial space @xmath95 ( see theorem 3 in ben alaya and kebaier @xcite ) .",
    "the process @xmath45 is solution to @xmath96 where @xmath97 is a @xmath98-dimensional standard brownian motion , defined on the extension @xmath99 , independent of @xmath100 , and @xmath101 ( respectively @xmath102 ) is the jacobian matrix of @xmath68 ( respectively @xmath103 .",
    "+ our target now is to use the euler mlmc introduced above to approximate @xmath104 by @xmath105 where @xmath106 is the euler scheme associated to @xmath107 ( [ eq_xtheta_mlmc ] ) with time step @xmath21 and @xmath108 obtained by using girsanov theorem in view to use importance sampling . according to relation ,",
    "our limit variance is given by @xmath109 where @xmath9 is the weak limit process of the error @xmath110 where @xmath111 defined on the extension @xmath94 and solution to @xmath112 note that @xmath45 and @xmath113 are the same processes obtained for the asymptotic behavior of @xmath114 and @xmath115 .",
    "in fact , @xmath36 is given by @xmath116.\\ ] ] + from a practical point of view , it is natural to choose the optimal @xmath36 minimizing the associated variance .",
    "note that from a practical point of view the quantity @xmath117 is not explicit , we use the euler scheme to discretize @xmath62 and we introduce the following quantity @xmath118 with @xmath119 \\\\   = \\mathbb{e}\\left [ \\left ( \\sqrt{\\frac{m^\\ell}{(m-1)t } } ( \\psi(x_{t}^{m^\\ell})-\\psi(x_{t}^{m^{\\ell-1 } } ) ) \\right)^2 e^{-\\theta \\cdot w_{t}+\\frac{1}{2}|\\theta|^{2}t } \\right].\\end{gathered}\\ ] ] the last relation is obtained by using a change of probability .",
    "in fact , according to girsanov theorem , the process @xmath120 under @xmath121 has the same law as @xmath122 under @xmath123 .",
    "the new variance associated to the mlmc is only based on the discretization of the process @xmath62 . in fact , this is different from the problem addressed in our previous work @xcite , where we discretized the process @xmath45 in order to compute @xmath124 e^{-\\theta \\cdot w_{t}+\\frac{1}{2}|\\theta|^{2}t } \\right).\\ ] ]",
    "the existence and uniqueness of @xmath125 is ensured by the following result .",
    "[ proposition1_mlmc ] suppose @xmath69 and @xmath68 are in @xmath126 and satisfy condition @xmath65 .",
    "let @xmath46 satisfying @xmath127 and @xmath128 then , the function @xmath129 is @xmath130 and strictly convex with @xmath131 for all @xmath132 where @xmath133 moreover , there exists a unique @xmath134 such that @xmath135 .",
    "the function @xmath136 is infinitely continuously differentiable with a first derivative equal to @xmath137 .",
    "note that , for @xmath138 we have @xmath139 using hlder s inequality , we obtain that @xmath140 it is clear that @xmath141 is finite .",
    "moreover , using , it follows from property @xmath142 , @xmath143 hence , we conclude the boundedness of @xmath144 . according to lebesgue s theorem , we deduce that @xmath145 is @xmath146 in @xmath147 and @xmath148 . in the same way , we prove that @xmath145 is of class @xmath130 in @xmath149 .",
    "we have @xmath150 .   \\end{gathered}\\ ] ] since @xmath151 , we get for all @xmath152 @xmath153 > 0 .   \\end{gathered}\\ ] ] hence , @xmath145 is strictly convex .",
    "consequently , to prove that the unique minimum is attained for a finite value of @xmath24 , it will be sufficient to prove that @xmath154 .",
    "recall that @xmath155.$ ] using fatou s lemma , we get @xmath156 \\\\\\leq \\liminf_{|\\theta| \\rightarrow + \\infty } \\mathbb e \\left [ \\left(r_\\ell \\ ( \\psi(x_{t}^{m^\\ell } ) - \\psi(x_{t}^{m^{\\ell-1}}))\\right)^2 e^{- \\theta.w_{t } + \\frac{1}{2 } |\\theta|^{2}t } \\right].\\end{gathered}\\ ] ] this completes the proof .",
    "[ th : convergence_mlmc ] suppose @xmath69 and @xmath68 are in @xmath126 and satisfy condition @xmath65 .",
    "let @xmath46 satisfying @xmath127 and condition .",
    "then , @xmath157    first of all , we will prove that @xmath158 is a @xmath159-bounded sequence . by way of contradiction , let us suppose that there is a subsequence @xmath160 that diverges to infinity , @xmath161 .",
    "this implies that on the event @xmath162 , we have the convergence in probability of the quantity @xmath163 towards @xmath164 .",
    "+ then , we can extract a subsequence that converges almost surely towards @xmath164 .",
    "therefore , we apply fatou s lemma and we get @xmath165 while @xmath166 the boundedness of the last expression is obtained using and property @xmath142 .",
    "this leads to a contradiction and we deduce that there is some @xmath167 such that @xmath168 for all @xmath169 .",
    "now , it remains to prove that the set @xmath170 is reduced to the singleton set @xmath171 .",
    "let us consider a subsequence @xmath172 as @xmath173 tends to infinity . according to proposition [ proposition1_mlmc ] above ,",
    "we have @xmath174=0.\\ ] ] now , let @xmath175 , using hlder s inequality with the boundedness of @xmath176 established in the first part of the proof , we check easily that there exists @xmath138 depending on @xmath177 , @xmath70 and @xmath178 such that @xmath179\\leq \\\\",
    "c   \\left\\| \\left ( r_\\ell \\ ( \\psi(x_t^{m^{\\ell_k}})-\\psi(x_t^{m^{\\ell_{k}-1 } } ) \\right)^2 \\right\\|_{a}^{{\\tilde a}}\\end{gathered}\\ ] ] we get the uniform integrability thanks to . therefore , according to the stable convergence theorem ( see theorem 3 in ben alaya and kebaier @xcite ) , we have @xmath180 so , we obtain @xmath181= 0.\\ ] ] we complete the proof using the uniqueness of the minimum .",
    "now , we focus our interest on the effective approximation of the above quantities @xmath125 and @xmath36 .",
    "our aim is to construct a sequence @xmath182 ( resp . for fixed @xmath40 , @xmath183 ) such that @xmath184 ( resp .",
    "@xmath185 ) almost surely . let @xmath186 be a compact convex subset containing @xmath187 . for @xmath188 , we introduce the sequences @xmath189 and @xmath190 defined recursively by @xmath191 \\\\ \\theta_{i+1}^{m^\\ell } & = \\pi_k \\left [ \\theta_i^{m^\\ell",
    "} - \\gamma_{i+1 } h_\\ell(\\theta_i^{m^\\ell } , x_{t , i+1}^{m^\\ell } , x_{t , i+1}^{m^{\\ell-1 } } , w_{t , i+1 } ) \\right ] ,   \\end{array } \\right.\\ ] ] where @xmath192 is the euclidean projection onto the constraint set @xmath193 , @xmath194 and @xmath195 are given respectively by the following expressions @xmath196 and the gain sequence @xmath197 is a decreasing sequence of positive real numbers satisfying @xmath198    [ constrained_algo_mlmc ] suppose @xmath69 and @xmath68 are in @xmath126 and satisfy condition @xmath65 .",
    "assume that @xmath199 and for all @xmath200 , @xmath127 .",
    "moreover , let @xmath46 and its derivatives satisfying .",
    "then , the following assertions hold .    * if @xmath201 is unique s.t @xmath202 and @xmath203 then @xmath204 . * if @xmath205 is unique s.t @xmath206 and @xmath207 then @xmath208 .    concerning the first assertion , according to theorem a.1 . in laruelle , lehalle and pags @xcite on robbins monro",
    "algorithm with projection : to prove that @xmath209 , we need to check firstly that @xmath210 this is satisfied using @xmath211 and thanks to the convexity of @xmath212 .",
    "secondly , we have to check the non explosion condition given by @xmath213 < c ( 1+|\\theta^2|).\\ ] ] using hlder s inequality , we can check that @xmath214 \\leq e^ { |\\theta|^{2}t }   \\ee^{1/a } \\left[|\\nabla \\psi(x_t)|^{4a}\\right ] \\\\ \\ee^{\\frac{a-1}{2a } } \\left[|u_t|^{\\frac{8a}{a-1 } } \\right ] \\ee^{\\frac{a-1}{2a } } \\left [ \\left| e^{-\\theta \\cdot w_{t } } ( \\theta t - w_{t } ) \\right|^{\\frac{4a}{a-1}}\\right].\\end{gathered}\\ ] ] since @xmath215 $ ] is finite and @xmath216 $ ] is finite thanks to @xmath217 , there exists @xmath218 such that @xmath219 \\leq c e^{|\\theta|^{2}t } \\ee^{\\frac{a-1}{2a } } \\left [ \\left| e^{-\\theta \\cdot w_{t } } ( \\theta t - w_{t } ) \\right|^{\\frac{4a}{a-1}}\\right].\\ ] ] using @xmath220 , we conclude that @xmath221 < \\infty$ ] . concerning the second assertion , we aim to prove that for fixed @xmath132 , @xmath208 . using @xmath222 and",
    "thanks to the convexity of @xmath145 ensured by proposition [ proposition1_mlmc ] , we prove that @xmath223 for the non explosion condition , we use hlder s inequality @xmath224 \\leq e^ { |\\theta|^{2}t }   \\ee^{1/a } \\left[\\left|r_\\ell \\ ( \\psi(x_{t}^{m^\\ell } ) - \\psi(x_{t}^{m^{\\ell-1}})\\right|^{4a}\\right ] \\\\ \\ee^{\\frac{a-1}{a } } \\left [ \\left| e^{-\\theta \\cdot w_{t } } ( \\theta t - w_{t } ) \\right|^{\\frac{2a}{a-1}}\\right].\\end{gathered}\\ ] ] using , there exists @xmath225 such that @xmath226 \\leq c e^{|\\theta|^{2}t } \\ee^{\\frac{a-1}{a } } \\left [ \\left| e^{-\\theta \\cdot w_{t } } ( \\theta t - w_{t } ) \\right|^{\\frac{2a}{a-1}}\\right].\\ ] ] using @xmath220 , we conclude that @xmath227 < \\infty$ ] .",
    "if the optimal @xmath24 is not in a compact set , we always do better ( see @xcite ) .",
    "in fact , even if it does nt converge towards the minimal variance , it converges towards a smaller variance .",
    "let @xmath228 be the extension probability space introduced above endowed with the filtration @xmath229 given in the beginning of this chapter .",
    "in what follows , let @xmath230 , @xmath231 and @xmath232 be two families of sequences satisfying @xmath233 with deterministic limits @xmath36 and @xmath234 .",
    "+ in this section we prove a central limit theorem for the adaptive euler mlmc method which approximates our initial quantity of interest @xmath235 $ ] by @xmath236 where for all @xmath237 , @xmath238 , @xmath239 .",
    "the sequences @xmath240 are independent copies of @xmath241 satisfying @xmath242 .",
    "note that the @xmath243 empirical means are independent , the brownian path as well as @xmath244 is independent in each empirical mean in the euler mlmc .",
    "[ cltmlmc_adaptatif1 ] for the above setting , assume that @xmath68 and @xmath69 are @xmath146 functions satisfying @xmath65 and @xmath46 satisfying assumptions @xmath245 , @xmath246 and @xmath247 then , for the choice of @xmath248 given by ( [ sample_size_mlmc ] ) , the following convergence holds @xmath249 , \\quad \\lim\\limits_{n \\to \\infty } n^{2 \\alpha } \\ee \\left[\\left| q_n -\\mathbb{e}\\psi(x_{t } ) \\right|^2 \\right ] = \\tilde{\\sigma}^2 + c_\\psi^2,\\ ] ] where @xmath91 and @xmath250 are given by relation @xmath245 and @xmath251^{2 } e^{-\\theta^{*}\\cdot w_{t}+\\frac{1}{2}|\\theta^{*}|^{2}t }   \\right]$ ] .    at first , we rewrite the total error as follows @xmath252 where @xmath253 and @xmath254 are given by the following expressions @xmath255 @xmath256 \\right ) \\right].\\end{gathered}\\ ] ] since @xmath257 and @xmath258 are independent and centered , we write also @xmath259= \\ee\\left [ |q_{n}^{1}|^2 \\right ] + \\ee \\left [ |q_{n}^{2}|^2\\right ] + n^{2 \\alpha } ( \\mathbb{e } \\psi(x_{t}^{n } ) - \\mathbb{e } \\psi(x_{t}))^2.\\ ] ] using assumption @xmath245 , the last term of the previous expression converges towards the discretization constant @xmath260 as @xmath261 goes to infinity .",
    "+ @xmath262**step 1 . * * for the first term @xmath263 , noticing that @xmath264 is a martingale with respect to the filtration @xmath265 , then we write @xmath266 \\right ) \\nonumber \\\\   & = & \\frac{n^{2\\alpha}}{n_0 ^ 2 } \\sum_{i=1}^{n_0 } \\left ( \\ee \\left ( \\psi(x_{t}^{m^0})^2 e^{-\\theta_{i-1}^{m^0 } \\cdot w_{t}+ \\frac{1}{2}|\\theta_{i-1}^{m^0}|^{2}t}\\right ) - \\left [ \\mathbb{e}\\psi(x_{t}^{m^0 } ) \\right]^2 \\right ) \\nonumber.\\end{aligned}\\ ] ] since @xmath267 and @xmath268 is @xmath269-measurable and thanks to girsanov theorem , we obtain the last equality by introducing a new couple of random variables @xmath270 independent of @xmath271 . as @xmath272 , we write @xmath273 ^ 2 \\right).\\ ] ] since @xmath274",
    ", we have @xmath275 this upper bound is clearly integrable using property @xmath276 and assumption together with the hlder s inequality .",
    "therefore , by the dominated convergence theorem and under assumption @xmath277 , we obtain that @xmath278 thus , by applying cesaro s lemma , we obtain that @xmath279 ^ 2 \\right ) \\\\ = \\ee \\left ( \\psi(x_{t}^{m^0})^2 e^{- \\theta^{*}_{m^0 } \\cdot w_{t}+ \\frac{1}{2}|\\theta^{*}_{m^0}|^{2}t}\\right ) - \\ee^2(\\psi(x_t^{m^0})).\\end{gathered}\\ ] ] as @xmath280 and @xmath83 $ ] , we conclude thanks to relation that @xmath281 so it remains now to study the asymptotic behavior of @xmath282 .",
    "+ @xmath262**step 2 . *",
    "* we consider the second term @xmath283 and we write @xmath284,\\ ] ] where @xmath285\\right).\\end{gathered}\\ ] ] as @xmath286 are independent , we have @xmath287 by the same argument as in the first step , noticing that for each @xmath74 , @xmath288 is @xmath289 martingale , we write @xmath290 \\right)\\nonumber \\\\    & = & \\sum_{\\ell=1}^{l } \\frac{n^{2\\alpha}}{n_{\\ell } }   \\left [ \\frac{1}{n_\\ell } \\sum_{i=1}^{n_\\ell } \\left(\\ee\\left ( [ \\psi(x_{t}^{m^\\ell})-\\psi(x_{t}^{m^{\\ell-1}})]^2 e^{-\\theta_{i-1}^{m^\\ell } \\cdot w_{t}+\\frac{1}{2}|\\theta_{i-1}^{m^\\ell}|^{2}t}\\right ) \\right .",
    "\\right.\\nonumber \\\\    & & \\hspace{6cm}\\left .",
    "\\left.-\\left(\\ee\\psi(x_{t}^{m^\\ell})-\\ee\\psi(x_{t}^{m^{\\ell-1}})\\right)^2 \\right ) \\right].\\end{aligned}\\ ] ] since for each @xmath74 , @xmath291 and @xmath292 is @xmath269-measurable and thanks to girsanov theorem , we obtain the last equality by introducing a new couple of random variables @xmath293 independent of @xmath271 . as @xmath294",
    "( see relation ) , we write @xmath295 ^ 2 e^{-\\theta_{i-1}^{m^\\ell } \\cdot w_{t}+\\frac{1}{2}|\\theta_{i-1}^{m^\\ell}|^{2}t}\\right ) \\\\",
    "\\hspace{5 cm } - \\frac{1}{\\sum_{\\ell=1}^{l } a_\\ell }   \\sum_{\\ell=1}^{l }   a_\\ell \\left ( r_\\ell \\ \\left[\\ee\\psi(x_{t}^{m^\\ell})-\\ee\\psi(x_{t}^{m^{\\ell-1 } } ) \\right ] \\right)^2.\\end{gathered}\\ ] ] now , for the last term of relation ( [ crocheta122_mlmc ] ) , under assumption @xmath89 , we apply the taylor s expansion theorem twice and we get @xmath296 the function @xmath297 is given by the taylor - young expansion , so it satisfies @xmath298 and @xmath299 . by property @xmath300",
    ", we get the tightness of @xmath301 and @xmath302 and we deduce @xmath303 so , according to the stable convergence theorem , we conclude that @xmath304 using , it follows from property @xmath300 that @xmath305 hence , we deduce using relation ( [ cv_stable_mlmc ] ) that @xmath306 then , @xmath307 converges to @xmath308 as @xmath40 goes to infinity . consequently , by applying toeplitz lemma , we obtain that @xmath309 \\right)^2 = 0.\\ ] ] we focus now on the asymptotic behavior of the first term on the right hand side on relation ( [ crocheta122_mlmc ] ) .",
    "since @xmath310 , we have @xmath311 this upper bound is clearly integrable using property @xmath276 and assumption together with the hlder s inequality .",
    "therefore , by the dominated convergence and under assumption @xmath277 , we obtain that @xmath312 ^ 2 e^{-\\theta_{i-1}^{\\ell , m^\\ell } \\cdot w_{t}+\\frac{1}{2}|\\theta_{i-1}^{\\ell , m^\\ell}|^{2}t}\\right)= \\\\",
    "\\ee\\left ( [ \\psi(x_{t}^{m^\\ell})-\\psi(x_{t}^{m^{\\ell-1}})]^2 e^{-\\theta_{\\ell}^ { * } \\cdot w_{t}+\\frac{1}{2}|\\theta_{\\ell}^{*}|^{2}t}\\right ) .",
    "\\end{gathered}\\ ] ] hence , by using cesaro s lemma , we obtain that @xmath313 ^ 2 e^{-\\theta_{i-1}^{\\ell , m^\\ell } \\cdot w_{t}+\\frac{1}{2}|\\theta_{i-1}^{\\ell , m^\\ell}|^{2}t}\\right)=",
    "\\\\ \\ee\\left ( [ \\psi(x_{t}^{m^\\ell})-\\psi(x_{t}^{m^{\\ell-1}})]^2 e^{-\\theta_{\\ell}^ { * } \\cdot w_{t}+\\frac{1}{2}|\\theta_{\\ell}^{*}|^{2}t}\\right ) .",
    "\\end{gathered}\\ ] ] otherwise , thanks to , we have @xmath314 e^{-\\frac{1}{2}\\theta_{\\ell}^{*}\\cdot w_{t}+\\frac{1}{4}|\\theta_{\\ell}^{*}|^{2}t }   \\overset{\\it stably}{\\underset{\\ell \\rightarrow \\infty}{\\longrightarrow } } \\nabla \\psi(x_{t})\\cdot u_{t } e^{-\\frac{1}{2}\\theta^{*}\\cdot w_{t}+\\frac{1}{4 } | \\theta^{*}|^{2}t}.\\ ] ] moreover , for @xmath315 we have by cauchy - schwartz inequality @xmath316 e^{-\\frac{1}{2 } \\theta_{\\ell}^{*}\\cdot w_{t}+\\frac{1}{4}|\\theta_{\\ell}^{*}|^{2}t}\\right|^{2 a } \\\\   \\leq r_\\ell^{2a } \\left[\\mathbb{e }   \\left| \\psi(x_{t}^{m^\\ell})-\\psi(x_{t}^{m^{\\ell-1 } } ) \\right|^{4a }   \\right]^{\\frac{1}{2}}e^ { \\frac{a(2a+1)}{2 } |\\theta_{\\ell}^{*}|^{2 } t}.\\end{gathered}\\ ] ] since @xmath317 and thanks to together with property @xmath318 , we obtain thanks to @xmath319   e^{-\\frac{1}{2}\\theta_{\\ell}^ { * } \\cdot w_{t}+\\frac{1}{4}|\\theta_{\\ell}^{*}|^{2}t}\\right|^{2 a }   < \\infty.\\ ] ] then , by the stable convergence obtained in ( [ stablecv_mlmc ] ) and the uniform integrability property given by ( [ unia122_mlmc ] ) and under the assumption @xmath277 , we deduce @xmath320 ^ 2 e^{-\\theta_{\\ell}^ { * } \\cdot w_{t}+\\frac{1}{2}|\\theta_{\\ell}^{*}|^{2}t}\\right)= \\\\",
    "\\left(\\left [   \\nabla \\psi(x_{t})\\cdot u_{t } \\right]^{2 } e^{-\\theta^ * \\cdot w_{t}+\\frac{1}{2}|\\theta^*|^{2}t }   \\right).\\end{gathered}\\ ] ] now , considering once again the first term on the right hand side of relation and using toeplitz lemma , we obtain that @xmath321 ^ 2 e^{-\\theta_{\\ell}^ { * } \\cdot w_{t}+\\frac{1}{2}|\\theta_{\\ell}^{*}|^{2}t}\\right)= \\\\",
    "\\left(\\left [   \\nabla \\psi(x_{t})\\cdot u_{t } \\right]^{2 } e^{-\\theta^ * \\cdot w_{t}+\\frac{1}{2}|\\theta^*|^{2}t }   \\right).\\end{gathered}\\ ] ] hence , combining this last result together with relation , we conclude thanks to that @xmath322^{2 } e^{-\\theta^ * \\cdot w_{t}+\\frac{1}{2}|\\theta^*|^{2}t }   \\right ) ,     \\quad \\mbox{$ \\tilde\\pp$-$a.s.$}\\ ] ] this completes the proof .",
    "our aim now is to prove a central limit theorem for the adaptive euler mlmc method .",
    "[ cltmlmc_adaptatif2 ] under assumptions of theorem [ cltmlmc_adaptatif1 ] and for the choice of @xmath248 given by ( [ sample_size_mlmc ] ) , the following convergence holds @xmath323 where @xmath251^{2 } e^{-\\theta^{*}\\cdot w_{t}+\\frac{1}{2}|\\theta^{*}|^{2}t }   \\right]$ ] .",
    "we consider the same decomposition given by relation in the beginning of the proof of theorem [ cltmlmc_adaptatif1 ] . under assumption",
    "@xmath245 , the last term on the right hand side of this relation converges to @xmath91 as @xmath261 goes to @xmath324 . for the convergence of the term @xmath257",
    ", we use the result obtained in theorem [ cltmlmc_adaptatif1 ] and we deduce that @xmath325 . concerning the convergence of the term @xmath258 , we plan to use the lindeberg - feller central limit theorem ( see theorem [ clt lindeberg feller ] ) with the lyapunov condition .",
    "we introduce the independent random variables @xmath326 and we need to check the assertions a1 . and a3 . in theorem [ clt lindeberg feller ] .",
    "more precisely , we will prove    * @xmath327^{2 } e^{-\\theta^ * \\cdot w_{t}+\\frac{1}{2}|\\theta^*|^{2}t }   \\right).$ ] * for @xmath328 , @xmath329 .",
    "concerning the first assertion a1 . , we have that @xmath330 .",
    "therefore , using relation in the proof of theorem [ cltmlmc_adaptatif1 ] , we get @xmath331^{2 } e^{-\\theta^ * \\cdot w_{t}+\\frac{1}{2}|\\theta^*|^{2}t }   \\right ) ,     \\quad \\mbox{$ \\tilde\\pp$-$a.s.$}\\ ] ] now , it remains to verify the assertion a3 .",
    "we get by burkholder s inequality ( see theorem 2.10 in @xcite ) : for @xmath328 , there exists @xmath332 such that @xmath333^{p/2}.\\ ] ] moreover , by using jensen s inequality , we obtain for @xmath328 @xmath334.\\ ] ] using that @xmath335 ( see relation ) and by conditioning , we get @xmath336 \\right)\\ ] ] we write also @xmath337 where @xmath338 , \\\\   b_\\ell & = & \\left(\\ee \\left [ r_\\ell \\ ( \\psi(x_t^{m^\\ell } ) - \\psi(x_t^{m^{\\ell-1 } } ) ) \\right ] \\right)^p .",
    "\\end{aligned}\\ ] ] since for each @xmath74 , @xmath291 and @xmath292 is @xmath269-measurable and thanks to girsanov theorem , we obtain the last equality by introducing a new couple of random variables @xmath293 independent of @xmath271 . by using relation together with assumption @xmath339",
    ", we deduce that the last term on the right hand side in converges to @xmath187 as @xmath40 goes to infinity @xmath340 now , we focus on the convergence of the first term on the right hand side in .",
    "since @xmath310 , there exists @xmath138 such that @xmath341 this upper bound is clearly integrable using property @xmath276 and assumption together with the hlder s inequality .",
    "therefore , by the dominated convergence and under assumption @xmath277 , we obtain that @xmath342 \\\\   = \\ee \\left [ \\left| r_\\ell \\ ( \\psi(x_t^{m^\\ell } ) - \\psi(x_t^{m^{\\ell-1 } } ) ) \\right|^p e^{-(p-1 ) \\theta_{\\ell}^ { * } \\cdot w_t -(p/2- 3/2 ) |\\theta_{\\ell}^{*}|^2 t}\\right ] .",
    "\\end{gathered}\\ ] ] then , by applying cesaro s lemma , we have that @xmath343",
    "\\\\   = \\ee \\left [ \\left| r_\\ell \\ ( \\psi(x_t^{m^\\ell } ) - \\psi(x_t^{m^{\\ell-1 } } ) ) \\right|^p e^{-(p-1 ) \\theta_{\\ell}^ { * } \\cdot w_t -(p/2- 3/2 ) |\\theta_{\\ell}^{*}|^2 t}\\right ] .",
    "\\end{gathered}\\ ] ] moreover , we obtain under assumption @xmath277 and thanks to relations and that @xmath344",
    "\\\\ = \\lim\\limits_{\\ell \\rightarrow \\infty } \\ee \\left [ \\left| r_\\ell \\ ( \\psi(x_t^{m^\\ell } ) - \\psi(x_t^{m^{\\ell-1 } } ) ) \\right|^p e^{-(p-1 ) \\theta_{\\ell}^ { * } \\cdot w_t -(p/2- 3/2 ) |\\theta_{\\ell}^{*}|^2 t}\\right ] \\\\ \\-\\-",
    "= \\tilde { \\mathbb{e } } \\left(\\left [   \\nabla \\psi(x_{t})\\cdot u_{t } \\right]^{p } e^{-\\theta^ * \\cdot w_{t}+\\frac{1}{2}|\\theta^*|^{2}t }   \\right).\\end{gathered}\\ ] ] once again , by using assumption @xmath339 ( @xmath345 ) , we conclude that @xmath346 this completes the proof .",
    "now , we prove a berry - essen type bound on our central limit theorem .",
    "this improves the relevance of the above result .",
    "we set @xmath347 and @xmath348 , where @xmath257 and @xmath349 are respectively given by relations and .",
    "the berry - essen type bound is a new type of result that we obtained in this paper compared to our previous work @xcite .    under assumptions of theorem [ cltmlmc_adaptatif1 ] ,",
    "we define by @xmath350 the distribution function of @xmath351 .",
    "then , we have @xmath352 where @xmath353 is the distribution function of a standard gaussian random variable . moreover , if @xmath46 is continuous lipschitz function , there exists a constant @xmath218 such that @xmath354 for the optimal choice @xmath355 , the obtained berry - essen type bound is of order @xmath356 .",
    "using theorem 2 page 544 in @xcite and since @xmath357 , by taking @xmath358 in both inequalities and in the proof of theorem [ cltmlmc_adaptatif2 ] , we deduce a berry - essen bound on our central limit theorem .",
    "@xmath359 when @xmath46 is lipschitz , using property @xmath142 , there exists a positive constant @xmath360 depending on @xmath68,@xmath69 , @xmath70 and @xmath46 such that @xmath361 hence , the berry - essen type bound on our central limit theorem is given by @xmath362 and @xmath363 are asymptotically constant .",
    "according to theorem [ cltmlmc_adaptatif2 ] , we deduce that for a total error of order @xmath364 , @xmath365 $ ] , the minimal computational effort necessary to run the adaptive mlmc algorithm is obtained for a sequence of sample sizes specified by relation ( [ sample_size_mlmc ] ) this leads to a time complexity given by @xmath366 so the time complexity reaches its minimum for the choice of weights @xmath367 .",
    "the optimal complexity of the euler mlmc method is given by @xmath368 we conclude that the adaptive mlmc method is more efficient in terms of time complexity in comparison with the monte carlo method one @xmath369 .",
    "+ we consider the problem of the an option pricing under the heston model .",
    "we remind that the the heston model is a popular stochastic volatility model in finance introduced by heston in @xcite solution to @xmath370 where @xmath371 and @xmath372 are two independent brownian motions .",
    "parameters @xmath373 , @xmath69 , @xmath374 and @xmath375 are strictly positive constants and @xmath376 . in this model , @xmath373 is the rate at which @xmath377 reverts to @xmath374 , @xmath374 is the long run average price variance , @xmath69 is the volatility of the variance , @xmath375 is the interest rate and @xmath378 is a correlation term .",
    "firstly , our aim is to use the importance sampling method in order to reduce the variance when computing the price of an european option , with strike @xmath193 , under the heston model .",
    "the price is @xmath379 where @xmath380 is the payoff of the option . after a density transformation , given by girsanov theorem ,",
    "the price will be defined by : @xmath381 , \\quad \\quad \\theta \\in \\mathbb{r}^{2}.\\ ] ] the price of the european call option is approximated by @xmath382 = e^{-rt } \\mathbb{e } \\left[\\psi(s_{t}^{n,\\theta } )",
    "\\ \\ e^{-\\theta.w_{t}-\\frac{1}{2 } |\\theta|^{2}t}\\right ] , \\quad \\quad \\theta \\in \\mathbb{r}^{2}.\\ ] ] the optimal @xmath383 of the mlmc method is given by @xmath384,\\ ] ] where @xmath385 denotes the euler discretization scheme obtained when we replace coefficients @xmath68 and @xmath69 of relation by the corresponding parameters in the heston model . here",
    ", in order to compare the adaptive euler mlmc algorithm with the adaptive sr and adaptive monte carlo ones , we use the constrained algorithm to approximate @xmath383 which is given by this routine let @xmath386 denote an increasing sequence of compact sets satisfying @xmath387 and @xmath388 . for @xmath389 , @xmath390 and a gain sequence @xmath391 satisfying ( [ gain_sequence_mlmc ] ) , we define the sequence @xmath392 recursively by @xmath393 @xmath394    we choose the parameters in the heston model : @xmath395 , @xmath396 , @xmath397 , the free interest rate @xmath398 , @xmath399 , @xmath400 , @xmath401 , @xmath402 and maturity time @xmath403 .",
    "we run a number of iterations @xmath404 and we obtain the two - dimensional vector of the optimal theta : @xmath405 . our aim now , is to compare the importance sampling multilevel monte carlo method ( denoted mlmcis ) with the two methods already studied in our previous work @xcite , the importance sampling statistical romberg method ( sris ) and importance sampling monte carlo method ( mcis ) to compute the price of the european call option :    * mlmc+is method : european option price approximation with @xmath84 given by expression ( [ sample_size_mlmc ] ) @xmath406 * sr+is method : european option price approximation method with @xmath407 and @xmath408 @xmath409 * mc+is method : european option price approximation with @xmath410 @xmath411    we consider a set of values @xmath412 , we compute the different estimators of the european call option ( [ mlmc+is_mlmc ] ) , ( [ sr+is_mlmc ] ) and ( [ mc+is_mlmc ] ) , for different values of the discretization steps @xmath261 . for each value",
    "@xmath261 and for each method , we compute the cpu time and the mean squared - error which is given by @xmath413 hence , for each given @xmath261 and for each method , we provide a couple of points ( mse , cpu time ) which are plotted on figure [ fig : theta_etoile ] .",
    "[ h ]    now let us interpret figure [ fig : theta_etoile ] .",
    "the curves of the importance sampling statistical romberg ( sris ) and the importance sampling multilevel monte carlo ( mlmcis ) methods are displaced below the curve of the importance sampling monte carlo ( mcis ) method .",
    "therefore , for a given error , the number of values computed in one second by the two methods is larger than the values computed by the mcis procedure . for a mean square error ( mse ) lower than @xmath414 , we observe that the mlmcis procedure becomes more effective than both methods .",
    "[ clt lindeberg feller ] let @xmath415 be a sequence such that @xmath416 , as @xmath417 and for each @xmath418 we consider a sequence @xmath419 of independent centered and real square integrable random variables .",
    "we make the following two assumptions ."
  ],
  "abstract_text": [
    "<S> this paper focuses on the study of an original combination of the euler multilevel monte carlo introduced by giles @xcite and the popular importance sampling technique . to compute the optimal choice of the parameter involved in the importance sampling method , </S>",
    "<S> we rely on robbins - monro type stochastic algorithms . on the one hand , we extend our previous work @xcite to the multilevel monte carlo setting . on the other hand , we improve @xcite by providing a new adaptive algorithm avoiding the discretization of any additional process . furthermore , from a technical point of view </S>",
    "<S> , the use of the same stochastic algorithms as in @xcite appears to be problematic and we are reduced to employ a specific version of stochastic algorithms with projection ( see e.g. laruelle , lehalle and pags @xcite ) . </S>",
    "<S> we firstly prove the almost sure convergence of this stochastic algorithm towards the optimal parameter . </S>",
    "<S> then , we prove a central limit theorem of type lindeberg - feller for the new adaptive euler multilevel monte carlo algorithm together with non - asymptotic berry - essen bounds . finally , we illustrate the efficiency of our method through applications in option pricing for the heston model .    _ </S>",
    "<S> * msc 2010 : * _ 60e07 , 60g51 , 60f05 , 62l20 , 65c05 , 60h35 .    _ </S>",
    "<S> * keywords : * _ multilevel monte carlo , stochastic algorithm , robbins - monro , variance reduction , lindeberg - feller central limit theorem , euler scheme , finance , heston model .. </S>"
  ]
}