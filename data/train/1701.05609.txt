{
  "article_text": [
    "until the last decade or so , the connections between statistics and numerical analysis had not been explored much .",
    "research work that addressed such connections were few and far in between until recently .",
    "diaconis ( 1988 ) was one of the first expository papers that provided examples of connection between bayesian analysis and numerical methods ; to paraphrase persi diaconis from that paper , ` it may even sound crazy to think that there exists connections between numerical analysis and statistics ' . however , such connections _ do _ exist , as pointed out in diaconis ( 1988 ) , and further explored in papers by ohagan , ylvisaker , ( ohagan ( 1991 ) , ylvisaker ( 1987 ) ) and many others and recently by conrad and co - authors ( conrad _ et al _ , 2015 ) and chkrebtii and co - authors ( chkrebtii _ et al _ , 2016 ) . for a comprehensive list of papers related to this area , please refer to the references within the last two papers mentioned above .",
    "ohagan s and diaconis work mainly addressed relations of statistics , especially bayesian statistics , to quadrature problems .",
    "ylvisaker s work was related to optimal statistical design in a general context , but also had specific connections to quadrature problems .",
    "all the above work were related to connections between statistical analysis and numerical methods for quadrature or design points ( mainly for quadrature ) ; none of them established a connection between statistical analysis and numerical methods for differential equations .",
    "recent papers , by conrad and co - authors and chkrebtii and co - authors were the first two papers that addressed this connection , to the best of our knowledge .",
    "the work for the present paper was done independently of the work by conrad and chkrebtii and respective co - authors ( -in summer , 2016 and the authors were not aware of conrad _ et al _ and chkrebtii _ et al _ until the writing stage of this paper , in fall 2016 , when they were published ) . although our paper is similar in spirit to conrad _ et al _ and chkrebtii _ et al _ , the examples that we provide are completely different from those two papers .",
    "our statistical approach is somewhat simpler than theirs ; we hope that our work will be a good addendum to the above mentioned papers , and to this new emerging field of research in general .    for many finite difference methods that find an approximate solution @xmath0 at discrete points for a differential equation ,",
    "we may associate a linear regression @xmath1 here @xmath2 is the unknown ( column ) vector of original solution values at the respective discrete points , @xmath3 is a known ( column ) vector , @xmath4 is a known matrix , and @xmath5 is a vector of errors , with @xmath3 and @xmath4 possibly depending on @xmath0 and @xmath5 on the truncation error due to the approximation .",
    "the type of the linear regression may vary from one finite difference method to the other . typically using this equation to estimate",
    "@xmath2 has limited value in the sense that the estimated @xmath2 will not be not be much different from the approximation , @xmath0 .",
    "however , the regression equation could be utilized to obtain confidence intervals and this has a few important practical merits .",
    "the importance and practical utility of a confidence interval obtained via this approach are mainly due to the fact that the width of the interval depend mostly on the size of the truncation error .",
    "one obvious merit is that confidence intervals will give one an idea about the precision of the approximation . in many physical , engineering and finance industry applications , differential",
    "equations are solved for daily practical purposes , and hence knowing the confidence interval will guide in making the decisions .",
    "another important practical use is in improving the finite difference approximation itself .",
    "if there are segments of the solution where the confidence intervals are much wider compared to other parts of the solution , then increasing the number of design points ( that is , decreasing the mesh width ) just for that particular segment will lead to a better approximation . in a way , this is an adaptive approach for coming up with a non - uniform grid , with grid points clustered in regions where they are most needed .",
    "the goal of our paper is to explain these ideas more clearly using many different examples of finite difference methods .",
    "we start with a basic example . before we get to the examples",
    ", we describe the simple bayesian statistical approach that we utilize in all these examples .",
    "we use a bayesian approach to obtain the confidence intervals for the estimate of @xmath2 based on eq.([eq:1 ] ) .",
    "we follow the standard approach by first assuming that @xmath5 is distributed @xmath6 ( that is , multivariate normal with mean vector @xmath7 and variance - covariance matrix @xmath8 , where @xmath9 is the identity matrix ) .",
    "we assume a flat prior for @xmath2 and an inverted - gamma prior , @xmath10 for @xmath11 , so that the posterior distribution for @xmath2 is given by @xmath12 in each of the examples below , we draw @xmath13 samples from inverse - gamma prior for @xmath11 and then draw the corresponding @xmath2 from the posterior density .",
    "the parameters for the inverse - gamma prior were chosen as @xmath14 and @xmath15 , where @xmath16 is the number of design points .",
    "we throw away the first @xmath17 @xmath2 draws , and then calculate the @xmath18 and @xmath19 percentiles from the remaining @xmath20 draws to obtain the lower and upper confidence limits for @xmath2 .",
    "we note here that although we refer to them as confidence intervals , technically the standard terminology is credible intervals .",
    "we start with a simple finite difference method , slightly adapted from one of the first examples given in leveque ( 2007 ) .",
    "we consider the second order ordinary differential equation ( ode ) @xmath21 the advantage of working with this simple problem is that we know the exact solution : @xmath22 with a mesh width @xmath23 and design points @xmath24 , we denote by @xmath25 the approximation to the solution @xmath26 , to be obtained via the finite difference scheme .",
    "we set @xmath27 and @xmath28 to correspond with the boundary conditions . using a centered difference approximation for @xmath29 , we may write a finite difference equation @xmath30 which leads to a linear system of @xmath31 equations @xmath32 , where @xmath33 , \\;\\;\\ ; f = \\left [ \\begin{array}{c } \\sin(x_{1 } ) - \\frac{1}{h^2 } \\\\                                                                 \\sin ( x_{2 } ) \\\\ \\vdots",
    "\\\\ \\sin ( x_{m-1 } ) \\\\",
    "\\sin ( x_{m } ) - \\frac{(\\pi + 1)}{h^2 } \\end{array } \\right ] \\;\\;\\ ; \\mathrm{and}\\;\\;\\ ;      \\hat{u } = \\left [ \\begin{array}{c } \\hat{u}_{1 } \\\\   \\hat{u}_{2 } \\\\ \\vdots \\\\ \\hat{u}_{m-1 } \\\\ \\hat{u}_{m } \\end{array } \\right].\\ ] ] the local truncation error is defined by replacing @xmath25 with the solution @xmath34 in the finite difference formula eq.([eq:2 ] ) .",
    "so , if @xmath35^{t}$ ] denote the actual values of the solution , then @xmath36 , where @xmath37 is the vector of truncation errors . this equation may be re - written in the form of eq.([eq:1 ] ) with @xmath38 , @xmath39 , @xmath40 and @xmath41 .    with @xmath42 ( that is , @xmath43 )",
    ", we fit the regression parameter @xmath2 , and obtained the confidence intervals using the bayesian method outlined in the introduction .",
    "the results are shown in figure 1 . in the first panel ( figure 1a ) ,",
    "the black curve is the actual solution , @xmath44 , between @xmath7 and @xmath45 , the blue points are the finite difference approximation @xmath46 and the red points correspond to the regression parameter fit . for this example , the red points are almost smack on top of the blue points ( so that it is hard to distinguish between the two sets of points ! ) and they are perfectly in alignment with the actual solution curve .",
    "the red dashed lines correspond to the confidence interval obtained using the bayesian fit .",
    "figures 1b and 1c illustrate the main point that we want to make with this example . in figure 1b",
    ", we plot the width of the confidence interval for the design points chosen between @xmath7 and @xmath45 . in the figure we scale it with a factor @xmath47 , so that if the errors are normally distributed , then this scaled width correspond approximately to the standard error. the shape of this confidence - interval - width - curve is roughly that of a sine curve .",
    "this makes sense because analytically ( by applying taylor series expansion to the @xmath48 row in @xmath36 ) we have the expression for the truncation error as @xmath49 - \\sin ( x_{j } ) \\\\ & = & \\displaystyle \\frac{h^{2}}{12}u''''(x_{j } ) + o(h^{4 } ) \\\\ & = & \\displaystyle -\\frac{h^{2}}{12}\\sin(x_{j } ) + o(h^{4 } ) .",
    "\\end{array}\\ ] ]    since the error term @xmath50 in the regression equation used in this example was equal to @xmath51 , the leading term in the error is a scaled sine curve . the leading term in the truncation error ,",
    "@xmath52 is plotted in figure 1c .",
    "it is easy to see that if we invert the curve in figure 1c , we get the same shape as in figure 1b , although the scale is different .",
    "thus , the main point is rather simple : the width of the confidence interval mainly depends on the truncation error at that point .",
    "the points where the confidence interval is relatively wider are the points where the truncation error is relatively larger .",
    "this simple point can have important practical utility as illustrated in the next few examples .",
    "-axis in all sub - figures is the interval @xmath53 $ ] .",
    "1a ) black curve is the exact solution , blue points are the finite difference solution , red points and the red dashed curves are , respectively , the solution to the corresponding regression equation and the confidence intervals obtained via the bayesian method .",
    "1b ) width of the confidence interval/(@xmath54 ) , 1c ) leading term of the truncation error , width=672,height=288 ]",
    "next we consider a nonlinear boundary value problem ( bvp ) which describes the motion of a simple pendulum with a weight attached to a ( massless ) bar of length @xmath55 .",
    "ignoring the forces of friction and air resistance , the well - known differential equation for pendulum motion is @xmath56 where @xmath57 is the angle of the pendulum from the vertical at time @xmath58 and @xmath59 is the gravitational constant .",
    "taking @xmath60 for simplicity , we have @xmath61 an initial value problem with the initial position @xmath62 and the initial angular velocity @xmath63 is most natural for the pendulum problem , and gives a unique solution at all later time points .",
    "however , we use the pendulum problem to discuss and illustrate our methods to a bvp . in order to specify the bvp , we set the initial location as @xmath64 and the location at a later time point @xmath65 as @xmath66 .",
    "formally , the 2-point bvp is @xmath67 for our specific example , we set @xmath68 and @xmath69 . as pointed out in leveque ( 2007 ) similar bvps do arise in more practical situations , for example , trying to shoot a missile in such a way that it hits a desired target .",
    "again , following leveque ( 2007 ) , with @xmath70 , discretization can be done as follows : @xmath71 for @xmath72 , where @xmath73 , @xmath74 and @xmath75 this can be thought of as a nonlinear system of equations @xmath76 where @xmath77 is the function whose @xmath78 component is @xmath79 leveque ( 2007 ) gives a newton s iterative scheme for solving eq.([eq:3 ] ) , and for our present example , we assume that this scheme has been used to obtain the solution @xmath80 to eq.([eq:3 ] ) .",
    "( in this section , we use bold letters to indicate that the solution values or approximations we consider at the design points / grid is a vector . )",
    "assume @xmath81 is a solution to the original bvp",
    ". then we may write @xmath82 where @xmath83_{m \\times m } = \\left [ \\frac{\\partial}{\\partial \\theta_{j } } g_{i}(\\boldsymbol{\\hat{\\theta } } ) \\right];\\ ] ] that is , @xmath84 since @xmath80 is a solution to eq.([eq:3 ] ) , we get @xmath85 if we denote the truncation error by @xmath37 and the global error @xmath86 by @xmath87 , then @xmath88 so that the above equation becomes @xmath89 rearranging , @xmath90 where we have collected all the error terms into the term @xmath50 .",
    "this is a linear regression equation of the form @xmath91 , where @xmath92 , @xmath93 and @xmath94 ( the unknown solution to the bvp at the discrete timepoints ) .",
    "since the higher order terms for @xmath95 are negligible , the error term @xmath5 is dominated by the truncation error term .    one more fact",
    "to keep in mind regarding pendulum motion is that the corresponding differential equation can be approximated using the linearized equation @xmath96 when the amplitude ( i.e. @xmath97 ) is small .",
    "the linearized equation has a harmonic solution @xmath98 however , for larger values of the amplitude the harmonic solution is substantially different from the actual solution to the nonlinear differential equation ( see , for example , belendez _ et al _ , 2007 ) .",
    "there is an  explicit \" solution to the ivp in terms of the jacobi sine function , sn ( belendez _ et al _ , 2007 ) : @xmath99 here @xmath100 represent the time that the pendulum reaches the bottom , and @xmath101 represents the total energy , which is a conserved quantity for this dynamical system .",
    "the explicit form allows us to obtain a very accurate solution to our bvp by solving two equations @xmath102 and @xmath103 for the unknowns @xmath100 and @xmath104 .",
    "although this requires a numerical method , the solution will contain an error that is negligible when compared to the error from solving a differential equation , so that for all practical purposes , we may take this numerical solution as the ` exact ' solution .",
    "our result is @xmath105 and @xmath106 ; these values are used to plot the blue curve in figure 2 .",
    "we first chose equally spaced design points @xmath107 between @xmath7 and @xmath108 by setting @xmath109 ( @xmath110 ) . with starting values @xmath111 } = \\alpha \\cos(t_{i } ) + ( \\beta - 0.2 ) \\sin ( t_{i})$ ] we ran newton s iterative scheme for 10 iterations , so that it converged to within a tolerance limit of @xmath112 , to obtain the solution for the finite difference scheme ( which , keep in mind , is an approximation to the original unknown solution ) . with this approximation @xmath80 ,",
    "we fit the regression model mentioned in the introduction to obtain the confidence intervals .",
    "-axis in all sub - figures is the interval @xmath113 $ ] .",
    "2a ) black curve is the finite difference solution , green curve is the harmonic solution to the linear differential equation , and the blue curve is the exact solution based on jacobi s elliptic function .",
    "the regression fit is plotted as red circles and the corresponding confidence intervals as the red dashed lines .",
    "2b ) colors denote the same thing as for figure 2a .",
    "2c ) circles represent the error corresponding to the finite difference solution in figure 2a , and stars represent the error corresponding to the finite difference solution in figure 2b , width=672,height=288 ]    the results are shown in figure 2a .",
    "the finite difference solution obtained using newton s iterative scheme is plotted as the black solid curve , the harmonic solution to the linear differential equation is plotted as the green curve , and the explicit solution based on jacobi s sine function is plotted as the blue curve .",
    "the regression fit is plotted as red circles and the corresponding confidence intervals as the red dashed lines . the important thing",
    "to notice is that the width of the confidence interval varies across the domain , @xmath114 .",
    "this suggests that the truncation error is larger in certain sections of the domain , and the accuracy could be improved by increasing the number of design points just in those regions .",
    "the confidence interval is relatively wider , for example , in the intervals @xmath115 $ ] and @xmath116 $ ] .",
    "so , we increased the number of design points in those intervals by decreasing @xmath117 from @xmath118 to @xmath119 .",
    "since we would like to compare the new results to those presented in figure 2a , we have to ensure that the number of design points is the same in the overall interval @xmath113 $ ] .",
    "hence we decreased the number of design points in the intervals @xmath120 $ ] and @xmath121 $ ] by taking @xmath122 , so that @xmath31 remains the same ( @xmath110 ) as in figure 2a .",
    "the revised results are plotted in figure 2b .",
    "it is immediately seen that the width of the confidence interval is smaller than that in figure 2a , overall .",
    "but , more importantly , the accuracy of the finite difference approximation ( the black curve ) , which overlaps with the bayesian regression estimate ( red triangles ) , has also increased .",
    "this latter fact can be seen more clearly in figure 2c , where the error corresponding to the finite difference solution in figure 2a is plotted as circles and the corresponding error related to figure 2b is plotted as stars .",
    "we may observe that the error was reduced not just in the intervals where we increased the number of design points , but overall as well , which is reflective of the fact that the error term in the regression and hence confidence interval in this example depends on _ both _ the local truncation error @xmath37 and the global error @xmath87 .",
    "thus this example illustrates clearly the helpfulness of calculating and plotting the confidence interval for the finite difference approximation , when we consider adaptively distributing the design points unevenly to reduce error .    as a last comment related to this example",
    ", one may wonder that instead of the eq.([eq:5 ] ) , why not simply consider @xmath123 for some @xmath124 following a normal distribution ( -after all , this is the equation one gets if one divides eq.([eq:5 ] ) throughout by @xmath125 ) .",
    "the answer is that the error structure based on @xmath124 is not reflective of the truncation error and hence considering this new equation is not very helpful . to illustrate , we fit the confidence interval for the regression estimate with @xmath126 and @xmath127 , the identity matrix .",
    "the results are shown in figure a1 in the appendix . in this case , the confidence intervals are just related to sampling error / variation .",
    "in this example , we consider the nonlinear boundary value problem @xmath128 which is a singular perturbation problem since @xmath129 multiplies the higher order derivative . with @xmath130 , the reduced first order ode can enforce only one boundary condition . if @xmath131 is imposed , then the non - trivial solution to the reduced equation is @xmath132 and if @xmath133 is imposed , the corresponding solution is @xmath134 on the other hand , for @xmath135 , the full equation ( [ eq : section.5a ] ) has a solution that satisfies both the boundary conditions by following the line given in eq .",
    "( [ eq : section.5b ] ) near @xmath136 and the ( parallel ) line in eq .",
    "( [ eq : section.5c ] ) near @xmath137 .",
    "connecting these two smooth portions of the solution is a narrow zone ( the interior layer ) where @xmath138 is rapidly varying . in this layer , @xmath139 is very large and the @xmath140 term in eq .",
    "( [ eq : section.5a ] ) is not negligible .",
    "singular perturbation analysis can be done to determine the location and the width of the interior layer , and then combining this inner layer with the outer sections ( [ eq : section.5b ] ) and ( [ eq : section.5c ] ) of the solution one may obtain an approximate solution of the form @xmath141 where @xmath142 ( see again leveque ( 2007 ) for details of the derivation of this approximate solution . ) the green curve shown in some of the plots below ( and appendix ) related to this example , correspond to approximate solution based on the above formula . for the specific illustrative examples below we chose @xmath143 and @xmath144 .    in order to obtain a numerical solution at grid points @xmath145 , @xmath146 , where @xmath147 , we may discretize eq .",
    "( [ eq : section.5a ] ) to obtain the following set of finite difference equations : @xmath148 with @xmath149 and @xmath150 .",
    "this gives a nonlinear system of equations @xmath151 as in example 2 , which can be solved using newton s method .",
    "the values of the perturbation - analysis based approximate solution above at the discrete points @xmath152 can be used as starting values for newton s algorithm . again , as in example 2 , assuming that an approximate solution @xmath153 has been obtained using newton s iterative scheme , we may set up a linear regression @xmath154 where @xmath155 ( the unknown solution to eq .",
    "( [ eq : section.5a ] ) ) , and the error term @xmath5 depends mainly on truncation error . here , @xmath156 is an @xmath157 matrix , with @xmath158 element @xmath159    there are two issues that pop up when dealing with this example . the regression approach and",
    "the corresponding confidence intervals will be of practical assistance when dealing with both the issues .",
    "the first issue is the non - convergence , or convergence to the wrong solution , of newton s iterative scheme for very small @xmath129 .",
    "for example , for @xmath160 , newton s method converges in about 150 iterations if we use @xmath161 ( that is , 200 equally spaced design points ) .",
    "however , when @xmath162 , for the same equally spaced mesh width , newton s method does nt converge even after 500 iterations ( see the animation figure uploaded as an ancillary file ) .",
    "in fact , the iterations seem to recycle among the same approximations periodically . even with a much smaller mesh width @xmath163 ( that is , 1000 equally spaced design points ) , convergence does nt occur even in 10000 iterations ( plot available on request ) .",
    "note that newton s method _ does _ converge if we choose large number of ( e.g. 1000 ) _ unevenly _ spaced design points appropriately ( -more on this in the next paragraph- ) , but in this case the convergent solution is far removed from the approximate solution obtained via singular perturbation analysis ( see supplementary figure a2 ) .",
    "hence it makes us suspect that newton s iteration converged to the wrong solution for this non - uniform design .",
    "the second issue is related to the spacing ( uniform vs. non - uniform ) of the design points for this example .",
    "using perturbation analysis it could be shown that the width of the interior layer is of order @xmath129 ( i.e. very small ) .",
    "thus , in order to obtain a sufficiently accurate solution , especially with good accuracy for the tiny interior layer , one needs grid points which are sufficiently close enough ( hence very large number of grid points ) .",
    "however , if we use a uniformly spaced grid , this would lead to a waste of resources since a large chunk of the solution ( that is , the section other than the interior layer ) is very smooth and requires only very few grid points to obtain good accuracy .",
    "this suggests using a non - uniform grid with design points highly clustered for the interior layer and very sparse for sections other than the interior layer - this could be obtained analytically using singular perturbation analysis .",
    "however analytical approaches may not be available all the time .",
    "an alternate ( non - analytical ) approach is based on the confidence intervals , which we illustrate below .",
    "as we mentioned above , when @xmath162 and with 200 equally spaced grid points , newton s method for the finite difference scheme does nt converge ; so , we stopped at the @xmath164 iteration and used the approximation obtained at the last iteration as @xmath153 in the regression equation .",
    "the regression fit obtained and the confidence intervals are shown in figure 3a .",
    "it is easy to see that the confidence intervals are very wide ( and hence the truncation error very large ) near the interior layer , compared to other regions .",
    "so , we introduced an extra 200 equally spaced points in the subinterval @xmath165 $ ] of the domain with mesh width @xmath166 .",
    "the resulting solution ( shown in figure 3b ) gets much closer to the approximate solution ( green curve ) obtained analytically using the perturbation analysis , but still there is a lot of scope for improvement .",
    "the width of the confidence interval in figure 3a or figure 3b , if plotted over the domain @xmath167 $ ] , is a bell shaped curve centered at a point near the interior layer .",
    "this suggests generating design points from a distribution with a bell - shaped density ( e.g. normal distribution ) .",
    "we generated 200 points from a standard normal distribution and rescaled them to fit within the interval @xmath167 $ ] .",
    "we used these 200 points in addition to the equally spaced 200 points to run newton s method till the @xmath164 iteration and used the approximation at the last iteration as @xmath153 to fit the regression .",
    "the results ( shown in figure 3c ) are definitely a big improvement over those shown in figures 3a or 3b .",
    "we redid the analysis with 400 extra points , and then again with 5000 extra points from the normal distribution , and the results are shown in figures 3d and 3e , respectively .",
    "the solution in figure 3e is very close to the approximate solution obtained analytically , and the corresponding confidence intervals are very thin indicating only very small truncation error . in conclusion , this example clearly illustrates how the ( width of the ) confidence intervals can be used in designing the non - uniform grid for the finite difference method .",
    "we would like to note here that non - convergence that we mentioned above and shown in animation figure a2 in the appendix , is a bit artificial . at each iteration in newton s method",
    ", we have to invert the corresponding matrix @xmath168 .",
    "we used the statistical software r for all the numerical computations in this paper . in r",
    ", there are 2 ways to invert a matrix : either using the function _solve_ or using _chol2inv_ ; _ solve _ is a more generic function which can be used to invert any invertible matrix , while _",
    "chol2inv _ inverts a symmetric , positive definite square matrix from its choleski decomposition . in this particular example , both _",
    "solve _ and _ chol2inv _ can be used .",
    "the non - convergence that we mentioned above is when we use _ solve _ ; when we use _",
    "newton s method converges in about 3 iterations . nevertheless , since our main goal was to emphasize the utility of confidence intervals , hopefully this example , even though a bit artificial , is illustrative .",
    "-axis in all sub - figures is the interval @xmath169 $ ] .",
    "green curves in all figures is the approximate solution obtained analytically using singular perturbation method .",
    "the solid red curve is the regression fit , and the dashed red lines are the confidence intervals for the regression fit . the number and type ( uniform vs. non - uniform ) of grid points varied across the subfigures : 2a ) 200 evenly spaced grid points with mesh width 0.005 ; in addition to the grid points in figure 2a , the following sub - figures had extra grid points : 2b ) 200 evenly spaced points between 0.2 and 0.8 with width 0.003 , 2c ) 200 points from a normal distribution scaled to lie in the interval [ 0 , 1 ] , 2d ) 400 points from a normal distribution scaled to lie in the interval [ 0,1 ] and 2e ) 5000 points from a normal distribution scaled to lie in the interval [ 0,1 ] . in sub - figure 2f",
    "we put together all the regression fits from the previous figures 2a to 2e , going from right to left , so that comparisons could be made .",
    ", width=672,height=288 ]",
    "next we consider discretization of the famous black scholes equation for european call option in finance ( hull , 2008 ) . denoting by @xmath170 , the value of the option as a function of the underlying asset value ( e.g. stock price ) and time @xmath37 , the corresponding black scholes equation is @xmath171 here @xmath172 , the interest rate , and @xmath173 , the volatility rate",
    "are given numbers .",
    "let @xmath65 denote the expiry time of the option .",
    "applying a change of variables , @xmath174 , so that we have an initial condition , rather than a final condition , we re - write the above equation as @xmath175 and the initial condition as @xmath176 , where @xmath87 is the exercise price .",
    "we also have the boundary conditions @xmath177 and @xmath178 as @xmath179 we consider an implicit finite difference method for numerically solving the equation using the following discretization .",
    "we divided the time interval @xmath180 $ ] into @xmath181 equally sized subintervals of length @xmath182 .",
    "technically , @xmath183 can take values in the infinite interval @xmath184 . in practical situations , for discretization ,",
    "an upper limit @xmath185 is imposed and the finite interval @xmath186 $ ] is divided into @xmath187 subintervals of length @xmath188 .",
    "@xmath185 is typically taken as three or four times the exercise price .",
    "based on the above discretization , the rectangle @xmath186 \\times [ 0 , t ] = [ 0 , n\\delta s ] \\times [ 0 , m\\delta t]$ ] is approximated by a grid @xmath189 we denote by @xmath190 , the value of @xmath191 at the grid point @xmath192 .    in the fully implicit scheme ,",
    "the derivatives are approximated by @xmath193 @xmath194 @xmath195 putting it all together , the discretized version of eq .",
    "( [ eq : section.bs1 ] ) is @xmath196 @xmath197 multiplying throughout by @xmath182 and re - arranging terms , the above equation can be written in matrix form as @xmath198 where @xmath199 ,                          v^{m+1 } = \\left [ \\begin{array}{c } v_{1}^{m+1 } \\\\",
    "v_{2}^{m+1 } \\\\ \\vdots \\\\ v_{n-2}^{m+1 } \\\\ v_{n-1}^{m+1 } \\end{array } \\right ] ,                              b^{m } = \\left [ \\begin{array}{c } v_{1}^{m } \\\\",
    "v_{2}^{m } \\\\ \\vdots \\\\ v_{n-2}^{m } \\\\ v_{n-1}^{m } \\end{array } \\right ]                                    - \\left [ \\begin{array}{c } l_{0}v_{0}^{m+1 } \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ u_{n}v_{n}^{m+1 } \\end{array } \\right],\\ ] ] @xmath200 @xmath201 here @xmath202 is an @xmath203 matrix and @xmath204 , @xmath205 are vectors of length @xmath206 for all @xmath31 . at each time step",
    ", the implicit method provides a linear system of equations based on eq .",
    "( [ eq : section.bs2 ] ) to be solved .",
    "if we denote by @xmath207 the true value of the solution at the grid point @xmath192 , @xmath208 the corresponding vector , and the truncation error at the @xmath209 time step by @xmath210 , then we have @xmath211 which gives a regression equation at each time step that would help us find a confidence interval for @xmath204 ( as described in the introduction ) .",
    "we implemented the numerical scheme described above for the following values : @xmath212 and @xmath213 .",
    "note that the numerical solution that we obtained can be compared with the exact solution for the black scholes equation for european call option given by @xmath214 where the @xmath215 is the cumulative standard normal distribution function and @xmath216    in the figure 4 below , we plot the results for @xmath217 ( i.e. m = 10 ) .",
    "the results and the conclusions for all other time points are similar . in figure 4a , we plot the results based on the exact solution , the numerical solution and the regression estimate , all of which more or less overlap .",
    "the confidence intervals for the regression estimate is also plotted . in figure 4b ,",
    "the absolute error ( i.e. @xmath218exact solution - numerical estimate@xmath218 ) is plotted as the black curve .",
    "it is easy to note that the absolute error goes up near the exercise price , which is consistent with what is known in the literature .",
    "absolute error may not give the best picture , since the solutions are very near zero to the left of the exercise price , and it is comparatively very high for values far to the right of exercise price .",
    "a better error estimate is the relative error , defined as absolute error / exact solution , which is plotted as the blue curve in figure 4b .",
    "relative error is consistently large for prices to the left of the exercise price , and falls off dramatically to the right of the exercise price .",
    "we were able to calculate the errors plotted in figure 4b because we know the exact solution .",
    "however for many other examples in finance , the exact solution is not available - we have to come up with the error estimates from the numerical solution itself .",
    "our goal is to see whether the confidence intervals proposed in the paper will help us with this query .",
    "the width of the confidence intervals is plotted in figure 4c .",
    "as one may see , it does nt quite match with either of the curves plotted in figure 4b .",
    "the width of the confidence intervals steadily goes down to the right as does the relative error , but the shape of the former does not look like that of the latter . in order to obtain a measure that is somewhat similar to the relative error , we may divide the width of the confidence interval by the regression estimate ( plotted as the blue curve in figure 4d ) or by the sum of the confidence limits ( plotted as the black curve in figure 4d ) . in order to avoid division by zero ,",
    "we add @xmath219 to the original value in the denominator .",
    "now , it is clearly seen that the shape of the curves in figure 4d is very similar to that of relative error .",
    "thus , in scenarios like this example , where the value of the solution is very near zero in certain segments and very high for certain others , width of the confidence interval could be used to get an idea of the relative error .     for the interval",
    "@xmath186 $ ] ; n = 200 , n = 0 ,  ,",
    "n and @xmath220 .",
    "the exercise price @xmath221 corresponds to @xmath222 .",
    "fig 6a : the y - axis represents the option price .",
    "red curve is based on the exact formula ; green points based on the numerical solution ; blue points based on the regression estimate and the the two blue curves represent the confidence intervals . fig .",
    "6b : absolute error is plotted as black points and relative error as blue points .",
    "6c : width of the confidence intervals plotted in figure 6a .",
    "fig 6d : estimates of the relative error based on the width of the confidence intervals , using the formula @xmath223 , where @xmath224 sum of the confidence limits for the black curve and @xmath225 regression estimate for the blue curve.,width=672,height=288 ]",
    "next we illustrate the usefulness of confidence intervals for an example from population genetics , that uses the crank - nicholson implicit finite difference scheme . in population genetics , it is considered that the frequency , @xmath226 , of an allele at a specific genomic location changes over time ( that is , from generation to generation ) . if we assume that the population is sufficiently large and the change in @xmath226 per generation sufficiently small then the change in @xmath226 through time may be approximated by a continuous stochastic process .",
    "if there are two alleles , @xmath227 and @xmath202 , at a specific location with frequencies @xmath228 and @xmath229 respectively , in the current generation , then it is possible that at some time in the future ( say , @xmath230 generation ) the frequency of the allele @xmath202 is @xmath231 .",
    "when this happens ( that is , when frequency becomes @xmath231 ) we say that the allele @xmath202 is fixed .",
    "one quantity of interest for population geneticists is the probability of allele fixation at a time / generation @xmath58 , denoted by @xmath232 . here",
    "time ( considered synonymous with generation ) is a discrete variable @xmath233 , where @xmath234 corresponds to the first generation ; @xmath235 is the frequency of allele - of - interest in the first generation .",
    "kimura ( 1964 ) was one of the first papers that dealt with this probability of fixation ; for our illustrative example , we focus on results from wang and rannala ( 2004 ) , where @xmath232 is given as a solution of the following differential equation ( assuming @xmath58 as continuous for a moment ) : @xmath236 with boundary conditions @xmath237 here @xmath238 , the size of the population , and @xmath239 , a constant called the selection coefficient are given numbers .",
    "we prefer to use the notation @xmath240 instead of @xmath235 , so that the above equation in our preferred notation is @xmath241 we also have the initial condition @xmath242 where @xmath243 is the frequency of the allele in the first generation .",
    "@xmath240 lies in the interval @xmath169 $ ] and we discretize the interval @xmath167 $ ] with a mesh - width of @xmath244 @xmath58 ranges from @xmath7 to @xmath65 for some large @xmath65 . since @xmath58 is interpreted as the @xmath245 generation , a natural discretization is based on @xmath246 . for our example , we follow the above discretization so that the generic grid point is @xmath247 , where @xmath248 , @xmath249 ; @xmath250 , and the value of @xmath251 at the generic point is represented as @xmath252 . for the crank - nicholson scheme we have @xmath253 @xmath254 @xmath255 @xmath256 so that eq .",
    "( [ eq : section.gen1 ] ) may be approximated by @xmath257 @xmath258 substituting the given values for @xmath259 and @xmath182 and denoting @xmath260 , we may rearrange the terms in the above approximation to get @xmath261 @xmath262 @xmath263 which in matrix notation will be @xmath264 @xmath265,\\ ] ] @xmath266,\\ ] ] @xmath267 ,                              b^{m } = \\left [ \\begin{array}{c } 249.25\\alpha_{1}(u_{0}^{m } + u_{0}^{m+1 } ) \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 250.75\\alpha_{n-1}(u_{n}^{m } + u_{n}^{m+1 } ) \\end{array } \\right].\\ ] ] based on the boundary conditions , we have @xmath268 and @xmath269 , so that @xmath205 reduces to @xmath270^{t}.$ ] if we denote by @xmath271 the actual values of the solution at the grid points and @xmath272 the corresponding vector , then from eq .",
    "( [ eq : section.gen2 ] ) we obtain @xmath273 where @xmath274 denotes the truncation error at the @xmath209 time step . with @xmath275 and @xmath276 ,",
    "this is a regression equation of the form mentioned in the introduction , based on which confidence intervals for @xmath277 can be obtained . in the table",
    "below , we present the fixation probability estimates , and the corresponding confidence intervals at the @xmath278 generation when the initial the initial frequency is @xmath279 and @xmath280 , obtained using the above scheme .",
    "@xmath281 & fixation probability & @xmath282 ci & @xmath282 ci + & ( @xmath283 ) & lower limit & upper limit +    0.0005 & 0.000168985 & 0.00 & 1.00 + 0.001 & 0.000339403 & 0.00 & 0.6608832 + 0.1 & 0.058862310 & 0.00 & 0.3229033 +",
    "recently , there has been interest in exploring the connections between statistics and numerical analysis related to differential equations . in this paper , we show how some of the common finite difference schemes used to obtain numerical solutions for differential equations can be thought of as a regression problem . using a simple bayesian approach to solve the corresponding regression problem ,",
    "we may obtain confidence intervals for the finite difference solutions . applying this simple strategy to several basic examples ,",
    "we show how the confidence intervals are related to truncation error and thus illustrate the utility of the confidence intervals for the examples .",
    "there are limitations to the work presented in this paper , the most obvious being extra ( sometimes large amount of ) time required to calculate the confidence intervals in addition to the numerical solutions",
    ". this would be less of an issue as the speed of computers increase in the future .",
    "25    diaconis , p. ( 1988 ) bayesian numerical analysis . stat .",
    "decision theory relat .",
    "iv 1 , 163175 .",
    "a. ohagan .",
    "( 1991 ) bayes - hermite quadrature .",
    "jspi , 29:245260 .",
    "ylvisaker , d. ( 1987 ) prediction and design .",
    "conrad p , mag , s.sarkka , a.m.stuart , k.zygalkis .",
    "( may 2016 ) probability measures for numerical solutions of differential equations , to appear statistics and computing chkrebtii , o.a . ,",
    "campbell , d.a . ,",
    "calderhead , b. , girolami , m. ( 2016 ) bayesian solution uncertainty quantification for differential equations , bayesian analysis with discussion , 11(4 ) , 1239 - 1267 .",
    "leveque , rj .",
    "( 2007 ) finite difference methods for ordinary and partial differential equations .",
    "steady - state and time - dependent problems .",
    "a. belendez , c. pascual , d.i .",
    "mendez , t. belendez and c. ( 2007 ) neipprevista brasileira de ensino de fisica , v. 29 , n. 4 , p. 645 - 648 .",
    "hull , john c. ( 2008 ) .",
    "options , futures and other derivatives ( 7 ed . ) . prentice hall .",
    "kimura , m. ( 1964 ) diffusion models in population genetics .",
    "probab . 1 : 177232 .",
    "wang , y and rannala , b. ( 2004 ) a novel solution for the time - dependent probability of gene fixation or loss under natural selection .",
    "genetics 168 : 1081 - 1084 .",
    "-axis is the interval @xmath113 $ ] .",
    "black solid curve is the finite difference solution obtained using newton s iterative scheme , green curve is the harmonic solution to the linear differential equation and the exact solution based on jacobi s elliptic function is plotted as the blue curve . the regression fit is plotted as red circles and the corresponding confidence intervals as the red dashed lines . , width=321,height=288 ]     and grid sample size equals 1000 ( 200 equally spaced and 800 from a normal distribution ) .",
    "black curve is the finite difference solution at the 10000th iteration of newton s iterative scheme , green curve is the approximate solution from singular perturbation analysis , width=321,height=288 ]"
  ],
  "abstract_text": [
    "<S> although applications of bayesian analysis for numerical quadrature problems have been considered before , it s only very recently that statisticians have focused on the connections between statistics and numerical analysis of differential equations . in line with this very recent trend , we show how certain commonly used finite difference schemes for numerical solutions of ordinary and partial differential equations can be considered in a regression setting . focusing on this regression framework , we apply a simple bayesian strategy to obtain confidence intervals for the finite difference solutions . </S>",
    "<S> we apply this framework on several examples to show how the confidence intervals are related to truncation error and illustrate the utility of the confidence intervals for the examples considered </S>",
    "<S> .    numerical analysis , finite difference method , bayesian analysis </S>"
  ]
}