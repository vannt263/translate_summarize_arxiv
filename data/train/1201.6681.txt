{
  "article_text": [
    "classical entropy power inequality ( epi ) was first established by shannon @xcite . due to its importance and usefulness ,",
    "epi was proved by several different authors using distinct methods . in @xcite ,",
    "stam provided the first rigorous proof , and stam s proof was further simplified by blachman @xcite and dembo et al .",
    "@xcite , respectively .",
    "verd and guo proposed a new proof of the epi based on the i - mmse concept @xcite .",
    "most recently , rioul proved the epi based only on information theoretic quantities @xcite . before rioul s proof , most of the reported proofs were based on de bruijn - type identities and fisher information inequality , i.e. , the previous proofs were conducted mainly via an estimation - theoretic approach rather than an information - theoretic approach .",
    "due to the significance of the epi , numerous versions of epis such as costa s epi @xcite , the epi for dependent random variables @xcite , and the extremal entropy inequality ( eei ) @xcite have been proposed . among the epis ,",
    "the extremal entropy inequality is especially prominent since it can be adapted to several important applications investigated recently in the wireless communications area . in @xcite , liu and viswanath proposed the extremal entropy inequality , motivated by multi - terminal information theoretic problems such as the vector gaussian broadcast channel and the distributed source coding with a single quadratic distortion constraint , and suggested several applications for the extremal entropy inequality .",
    "the eei is an entropy power inequality which includes a covariance constraint . because of the covariance constraint",
    ", the eei could not be proved directly by using the classical epi .",
    "therefore , a powerful technique , referred to as the channel enhancement technique @xcite , was adopted in the proofs reported in @xcite .    the proofs proposed in @xcite",
    "proceed as follows .",
    "first , the extremal entropy inequality is cast as an optimization problem . using the channel enhancement technique , which relies mainly on karush - kuhn - tucker ( kkt ) conditions , an alternative optimization problem , whose maximum value is larger than the maximum value of the original problem , is proposed , and the alternative problem is solved using the epi .",
    "finally , the proof is completed by showing that the maximum value of the alternative problem is equal to the maximum value of the original problem .",
    "even though liu and viswanath proposed two kinds of proofs , a direct proof and a perturbation proof , both proofs are commonly based on the channel enhancement technique , and they are derived in a similar way except de bruijn s identity is adapted in the perturbation proof .    the main theme of this paper is to develop a novel mathematical framework to prove the extremal entropy inequality without using the channel enhancement technique . since the channel enhancement technique is adapted to prove not only the extremal entropy inequality but also the capacity of several different kinds of gaussian channels , e.g. , the capacity of the gaussian broadcast channel and the secrecy capacity of the gaussian wire - tap channel , by finding an alternative proof for the extremal entropy inequality , one can also find novel techniques to calculate the capacity of gaussian broadcast channel , the secrecy capacity of gaussian wire - tap channel , etc .",
    "more important is the fact that the mathematical framework and tools developed in the first part of this paper to achieve an alternative proof of the extremal entropy inequality without using the channel enhancement technique are exploited in the second part of this paper to achieve a second proof of eei ( a more simplified proof of the eei that does not use neither the epi nor the worst additive noise lemma ) , to obtain the optimal solution of a special class of broadcasting problems that assume a private message , and to characterize the minimum mean - square error ( mmse ) performance of linear bayesian estimators of a gaussian source in additive noise channels .",
    "the first proof of the eei , proposed in the first part of this paper , exploits mainly four techniques : the data processing inequality , the moment generating function , the worst additive noise lemma , and the classical epi . by using the data processing inequality , the worst additive noise lemma , and the classical epi ,",
    "an upper bound is calculated .",
    "then , by applying the equality condition of the data processing inequality , we prove that the upper bound can be achieved .",
    "the moment generating functions are implemented to prove the achievement of the equality condition in the data processing inequality .",
    "the second proposed proof of the eei relies partly on the techniques and tools proposed in the first proof of the eei , and it is further simplified in the sense that it does not rely neither on the epi nor on the additive worst noise lemma .",
    "the contributions of our proof can be summarized as follows . in the first part of this paper , a first alternative proof of the eei",
    "is proposed , and it is shown to be simpler and more direct compared with the proofs in @xcite .",
    "the proposed proof yields a more information - theoretic approach without using the kkt conditions .",
    "the proposed approach relies on the data processing inequality , and the moment generating function helps to circumvent the step of using the kkt conditions .",
    "moreover , by simply analyzing some properties of positive semi - definite matrices , one can bypass the step of proving the existence of the optimal solution which satisfies the kkt conditions , a step which is very complicated to accomplish .",
    "in addition , the structure of the covariance matrix of the optimal solution is mentioned in detail by using properties of positive semi - definite matrices .",
    "therefore , the proposed approach yields an explicit description of the optimal solution as opposed to the implicit solutions in @xcite .",
    "furthermore , the proposed proof presents a novel investigation method not only for the extremal inequality but also for applications such as the capacity of gaussian broadcast channel , the secrecy capacity of gaussian wire - tap channel , and so on . in the second part of this paper , the tools and mathematical approach used in the first part of the paper to prove the eei",
    "are further simplified to obtain a second alternative proof of the eei without using the epi or the worst additive noise lemma .",
    "two additional applications of the proposed results in finding the optimal signaling scheme for a broadcasting problem with a private message and characterizing the mmse performance of linear bayesian estimation schemes for gaussian sources in additive noise channels are described as well .",
    "these applications support the usefulness of the developed mathematical results and the versatility of the extremal entropy inequality .",
    "the rest of this paper is organized as follows . the extremal entropy inequality without a covariance constraint and its alternative proof are presented in section [ sec2 ] .",
    "the extremal entropy inequality and its first alternative proof , which are the main results of this paper , are described in section [ sec3 ] . in section [ sec4 ] ,",
    "several novel applications of the eei are introduced , including a second much simplified alternative proof of the eei , to illustrate the usefulness and relevance of the developed mathematical framework and results .",
    "finally , section [ sec5 ] concludes this paper .      throughout this paper , random vectors",
    "are denoted by capital letters such as @xmath0 and @xmath1 , matrices are represented by bold capital letters such as @xmath2 and @xmath3 , and @xmath4 and @xmath4-by-@xmath4 denote the dimension ( size ) of a random vector and a matrix , respectively .",
    "all information theoretic quantities are represented by conventional notations .",
    "for example , @xmath5 and @xmath6 stand for differential entropy of a random vector @xmath0 and mutual information between random vector @xmath0 and random vector @xmath1 , respectively .",
    "conditional entropy and conditional mutual information are denoted as @xmath7 and @xmath8 , respectively .",
    "the notation @xmath9 or @xmath10 stands for positive ( semi)definite partial ordering between matrices , i.e. , @xmath11 means @xmath12 is a positive semidefinite matrix @xcite . in this paper ,",
    "a positive definite matrix means a strictly positive definite matrix , and @xmath13 stands for the jacobian matrix with respect to @xmath2 .",
    "the matrix @xmath14 denotes an @xmath4-by-@xmath4 identity matrix , and the matrix @xmath15 stands for an @xmath4-by-@xmath4 zero matrix .",
    "notation @xmath16 $ ] denotes an expectation with respect to all random vectors inside @xmath17 $ ] , and @xmath18 and @xmath19 stand for the moment generating functions of random vector @xmath0 and random vector @xmath0 given @xmath1 , respectively . for simplicity",
    ", @xmath20 denotes the natural logarithm .",
    "since the extremal entropy inequality is similar to the classical entropy power inequality , we first investigate a relationship between the eei and the epi . without a covariance constraint , the eei is equivalent to the epi as shown in theorem [ thm1 ] .",
    "[ thm1 ] for an arbitrary random vector @xmath0 with a covariance matrix @xmath21 and a gaussian random vector @xmath22 with a covariance matrix @xmath23 , there exists a gaussian random vector @xmath24 which satisfies the following inequality : @xmath25 where the constant @xmath26 , all random vectors are independent of each other , @xmath23 is a positive definite matrix , and @xmath24 is a gaussian random vector which satisfies the following :    1 .   the covariance matrix of @xmath24 is represented by @xmath27 , and it is proportional to @xmath23 .",
    "the differential entropy of @xmath28 , @xmath29 , is equal to the differential entropy of @xmath0 , @xmath5 .",
    "in addition , the inequality ( [ thm1e1_1 ] ) is equivalent to the epi .    [ lem1 ] for independent random vectors @xmath30 and @xmath31 , @xmath32 where @xmath33 and @xmath34 are independent gaussian random vectors , @xmath35 and @xmath36 , and the covariance matrices of @xmath33 and @xmath34 are proportional .    using lemma [ lem1 ] ,",
    "the following relations are obtained : @xmath37 where @xmath27 is proportional to @xmath23 , i.e. , @xmath38 , and @xmath39 is an appropriate constant which satisfies @xmath40 .",
    "therefore , the inequality ( [ thm1e1_1 ] ) is derived from lemma 1 , the epi , and the proof of the inequality ( [ thm1e1_1 ] ) is completed .",
    "if the inequality ( [ thm1e1_1 ] ) holds , @xmath41 since @xmath40 , and @xmath27 is proportional to @xmath42 .",
    "this is exactly the same as the epi in lemma [ lem1 ] .",
    "therefore , the inequality ( [ thm1e1_1 ] ) is equivalent to the epi .",
    "while theorem [ thm1 ] shows a local upper bound , i.e. , the upper bound is dependent on a random vector @xmath0 , since @xmath39 depends on the random vector @xmath0 , we can also find a global upper bound as shown in theorem [ thm2 ] and the reference @xcite .",
    "[ thm2 ] for an arbitrary random vector @xmath0 with a covariance matrix @xmath21 and a gaussian random vector @xmath22 with a covariance matrix @xmath23 , there exists a gaussian random vector @xmath43 which satisfies the following inequalities : @xmath44 where the constant @xmath45 , all random vectors are independent of each other , @xmath23 is a positive definite matrix , @xmath24 stands for the gaussian random vector defined in theorem [ thm1 ] , and @xmath43 is a gaussian random vector whose covariance matrix @xmath46 is represented by @xmath47 .",
    "the proof , here , is a little different from the proof in @xcite . in our proof",
    ", we deal with both a local upper bound and a global upper bound while a global upper bound is directly calculated in @xcite .",
    "define the function @xmath48 as follows : @xmath49 where @xmath4 denotes the dimension of a random vector , and @xmath50 stands for the determinant of a matrix .",
    "since @xmath48 is unimodal , and @xmath51 @xmath48 is maximized when @xmath52",
    ".    therefore , from theorem [ thm1 ] , the following inequality is derived as @xmath53    the inequalities ( [ thm2e4_1 ] ) include inequalities ( [ thm2e1_1 ] ) and ( [ thm2e1_2 ] ) , and the validity of inequalities ( [ thm2e1_1 ] ) and ( [ thm2e1_2 ] ) is proved . the upper bound in ( [ thm2e4_1 ] ) is a global maximum while the upper bound derived in theorem [ thm1 ] is a local maximum .",
    "when @xmath54 , the inequalities ( [ thm2e1_1 ] ) and ( [ thm2e1_2 ] ) are also satisfied .",
    "however , we can not specify the covariance matrix of @xmath43 since @xmath55 is increasing with respect to @xmath46 and it can be infinitely large as @xmath46 is increased .",
    "therefore , we omit the case when @xmath54 in theorem [ thm2 ] .    as shown in theorems [ thm1 ] and [ thm2 ] , for @xmath26 , @xmath56 is maximized when random vector @xmath0 is gaussian . however , when a covariance constraint is added in the inequalities ( [ thm1e1_1 ] ) , ( [ thm2e1_1 ] ) and ( [ thm2e1_2 ] ) , we can not prove whether a gaussian random vector still maximizes @xmath56 or not , based on the same methods as described in the proofs of theorems [ thm1 ] and [ thm2 ] , since the covariance constraint may alter the proportionality relationship between the covariance matrices @xmath46 and @xmath42 .",
    "in @xcite , liu and viswanath proved that a gaussian random vector still maximizes @xmath56 even when a covariance constraint is considered .",
    "the inequality ( [ thm2e1_1 ] ) was formulated as an optimization problem with a covariance constraint as follows : @xmath57 where @xmath22 and @xmath58 are independent gaussian random vectors with positive definite covariance matrices @xmath42 and @xmath59 , respectively , all random vectors are independent of each other , and the maximization is done over the distribution of random vector @xmath0 .",
    "two proofs , a direct proof and a perturbation proof , are provided in @xcite .",
    "each proof approaches the problem in a different way but both proofs share an important common approach , namely the channel enhancement technique based on the kkt conditions , proposed originally in @xcite .    unlike the original proofs in @xcite",
    ", we will prove theorems [ thm3 ] and [ thm4 ] without using the channel enhancement technique .",
    "before we deal with the problem ( [ exine_e1_1 ] ) , we first consider a simpler case of it next .    [ thm3 ] for an arbitrary random vector @xmath0 with a covariance matrix @xmath21 , a gaussian random vector @xmath22 with a covariance matrix @xmath23 , and a positive semi - definite matrix @xmath3",
    ", there exists a gaussian random vector @xmath43 with a covariance matrix @xmath46 which satisfies the following inequality : @xmath60 where the constant @xmath26 , all random vectors are independent of each other , @xmath23 is a positive definite matrix , @xmath61 , @xmath62 .    when @xmath3 is a positive definite but singular matrix , i.e. , @xmath63 , the inequality ( [ thm3e1_1 ] ) and its covariance constraints",
    "are equivalently changed into @xmath64 where @xmath65 is such that @xmath66 , @xmath67 , and @xmath68 is a positive definite matrix , as mentioned in @xcite . when @xmath54 , the inequality ( [ thm3e1_1 ] ) is easily proved by the lemma [ lem2 ] , which will be presented later .",
    "therefore , without loss of generality , we assume that @xmath69 and @xmath3 is a positive definite matrix . then , the right - hand side ( rhs ) of the equation ( [ thm3e1_1 ] ) is upper - bounded by means of the following lemma .",
    "[ lem2 ] for random vectors @xmath0 ,",
    "@xmath70 , @xmath71 , and @xmath72 , @xmath73 where @xmath0 is an arbitrary random vector , @xmath70 is a gaussian random vector with the covariance matrix identical to that of @xmath0 , @xmath71 and @xmath72 are gaussian random vectors , and all random vectors are independent .",
    "based on lemma [ lem2 ] , the following inequalities hold : @xmath74 where @xmath75 denotes equivalence . notice that the gaussian random vector @xmath22 can be expressed as the sum of two independent gaussian random vectors @xmath71 and @xmath72 whose covariance matrices satisfy : @xmath76 where @xmath42 , @xmath77 , and @xmath78 are the covariance matrices of @xmath22 , @xmath71 , and @xmath72 , respectively .",
    "henceforth , the gaussian random vector @xmath22 is represented as @xmath79 .",
    "based on ( [ thm3e2_3 ] ) and ( [ thm3e3_0 ] ) , the left - hand side ( lhs ) of the equation ( [ thm3e1_1 ] ) is upper - bounded as follows : @xmath80 using theorem [ thm2 ] , if @xmath81 , the rhs of equation ( [ thm3e3_3 ] ) is upper - bounded as follows : @xmath82 where @xmath83 is a gaussian random vector whose covariance matrix @xmath46 is defined as @xmath84 . unlike theorem [ thm2 ] , we additionally have to prove that there exists a random vector @xmath83 whose covariance matrix @xmath46 satisfies @xmath85 due to the covariance constraint .",
    "since @xmath86 , we will prove there exists a random vector @xmath83 whose covariance matrix @xmath46 satisfies @xmath87 instead of proving ( [ thm3e5_2 ] ) .",
    "equation ( [ thm3e4_2 ] ) is further processed by making use of the following lemma .",
    "[ lem3 ] when three random vectors @xmath88 , @xmath89 , and @xmath90 represent a markov chain @xmath91 , the following inequality is satisfied : @xmath92 the equality holds if and only if random vectors @xmath88 , @xmath89 , and @xmath90 form the markov chain : @xmath93 .",
    "if the inequality ( [ thm3e6_2 ] ) is satisfied , then we can form the markov chain : @xmath94 where all random vectors are independent . since a gaussian random vector @xmath70 can be expressed as the summation of two independent gaussian random vectors @xmath95 and @xmath83 whose covariance matrices satisfy @xmath96 where @xmath21 , @xmath97 , and @xmath46 stand for covariance matrices of @xmath70 , @xmath95 , and @xmath83 , respectively , the gaussian random vector @xmath70 will be represented as @xmath98 .",
    "based on lemma [ lem3 ] , we obtain @xmath99 the equivalence in ( [ thm3e8_3 ] ) is due to @xmath98 .",
    "even though we need an upper bound of the rhs term in equation ( [ thm3e4_2 ] ) , the equation ( [ thm3e8_4 ] ) generates a lower bound for the equation ( [ thm3e4_2 ] ) as follows : @xmath100    however , if we can construct the following markov chain : @xmath101 and using lemma [ lem3 ] again , it turns out that @xmath102 and this inequality leads us to a tight upper bound . indeed , @xmath103 the equivalence in ( [ thm3e12_3 ] ) is due to @xmath98 .    now using ( [ thm3e12_4 ] ) , the equations ( [ thm3e4_1 ] ) and ( [ thm3e4_2 ] ) are upper - bounded as follows : @xmath104 and this is exactly the same as the equation ( [ thm3e9_3 ] ) . therefore , the following equality is satisfied : @xmath105 due to ( [ thm3e9_3 ] ) and ( [ thm3e13_4 ] ) .",
    "now , we will prove that we can actually construct the markov chain ( [ thm3e10_1 ] ) using the following lemmas .",
    "[ lem4 ] for independent random vectors @xmath88 and @xmath89 , the following equality between moment generating functions ( mgfs ) is satisfied : @xmath106[lem4e1_1 ] where @xmath107 $ ] , @xmath108 $ ] is an expectation , and superscript @xmath109 denotes the transpose of a vector . for jointly gaussian random vectors",
    "@xmath88 and @xmath89 , this equality is a necessary and sufficient condition for the independence between @xmath88 and @xmath89 .",
    "[ lem5 ] for independent random vectors @xmath88 and",
    "@xmath89 given a random vector @xmath90 , the following equality is satisfied : @xmath110    [ lem6 ] for a gaussian random vector @xmath0 with a mean @xmath111 and a covariance matrix @xmath21 , the mgf is expressed as @xmath112    in the markov chain ( [ thm3e10_1 ] ) , since all random vectors are gaussian ( without loss of generality , they are assumed to have zero means ) , using lemma [ lem6 ] , the following moment generating functions are presented in closed - form expression : @xmath113 where @xmath114 , @xmath115 , @xmath116 , and their covariance matrices are represented by @xmath117 , @xmath118 , and @xmath119 , respectively . since @xmath120 is a positive definite matrix , there exists the inverse of @xmath119 .    on the other hand ,",
    "the mgf of @xmath121 given @xmath90 is represented as @xmath122 if the term ( a ) in ( [ thm3e15_2 ] ) vanishes , @xmath88 and @xmath89 are independent given @xmath90 , and the markov chain ( [ thm3e10_1 ] ) is obtained .",
    "using lemma 11 , ( 1 ) in @xcite , we define the covariance matrix @xmath77 as @xmath123 where @xmath124 , and @xmath15 denotes an @xmath4-by-@xmath4 zero matrix .",
    "the positive semi - definite matrix @xmath125 must be chosen to satisfy @xmath126 where @xmath127 , @xmath128 , @xmath129 .",
    "lemma [ lem7 ] will prove that such a positive semi - definite matrix @xmath125 exists .",
    "[ lem7 ] there exists a positive semi - definite matrix @xmath125 which satisfies @xmath130 where @xmath131 , @xmath127 , @xmath128 , and @xmath132 and @xmath23 stand for a positive semi - definite matrix and a positive definite matrix , respectively .",
    "see appendix [ appa ] .    by directly using lemma [ lem7 ] in ( [ thm3e13_4_1 ] ) , one can prove theorem [ thm3 ] .",
    "however , we prefer to include explicitly in the proof the step which exploits the equality condition in the data processing inequality and the moment generating function .",
    "this is due to the following reasons .",
    "first , the included step shows how to come up with lemma [ lem7 ] , and helps to understand the intuition behind the proposed proof .",
    "second , the proposed step guarantees the fact that the optimal solutions must force the factor @xmath133 in ( [ thm3e15_2 ] ) to be zero . in other words ,",
    "the proposed step provides the necessary condition for the optimality .",
    "the equation ( [ thm3e16_1 ] ) can be re - written as @xmath134 since @xmath135 , by multiplying @xmath97 to both sides of the equation ( [ thm3e17_2 ] ) , @xmath136 and @xmath137 since random vectors @xmath88 , @xmath89 , and @xmath90 are defined as @xmath114 , @xmath115 , and @xmath116 , respectively , and they are independent of each other , their covariance matrices are represented as    @xmath138    from the equations ( [ thm3e18_2 ] ) and ( [ thm3e19_1 ] ) , it follows that @xmath139 and from the equations ( [ thm3e18_4 ] ) and ( [ thm3e19_1 ] ) , one can infer that @xmath140    the more general problem , originally proved in @xcite , is now considered in theorem [ thm4 ] .",
    "[ thm4 ] for an arbitrary random vector @xmath0 with a covariance matrix @xmath21 , two independent random vectors @xmath22 and @xmath58 with covariance matrices @xmath23 and @xmath59 , respectively , and a positive semi - definite matrix @xmath3 , there exists a gaussian random vector @xmath83 with a covariance matrix @xmath46 which satisfies the following inequality : @xmath141 where the constant @xmath26 , all random vectors are independent of each other , @xmath23 is a positive definite matrix , @xmath61 , @xmath62 .    due to the same reason mentioned in the proof of theorem [ thm3 ] , without loss of generality , we assume @xmath69 and @xmath3 is a positive definite matrix .",
    "the proof is generally similar to the proof of theorem [ thm3 ] .",
    "using lemma [ lem3 ] , the inequality ( [ thm4e1_1 ] ) can be expressed as @xmath142 where @xmath71 is chosen to be a gaussian random vector whose covariance matrix , @xmath77 , satisfies @xmath143 where @xmath144 is the covariance matrix of the gaussian random vector @xmath145 , @xmath146 is a gaussian random vector with covariance matrix @xmath147 , @xmath148 , and @xmath71 , @xmath145 , and @xmath146 are independent of one another .",
    "the inequality in ( [ thm4e2_1 ] ) is due to lemma [ lem3 ] , the inequality ( [ thm4e2_2 ] ) is due to theorem [ thm3 ] , and the equality ( [ thm4e2_3 ] ) will be proved using the equality condition in lemma [ lem3 ] .",
    "we will also prove that there exists a gaussian random vector @xmath71 which satisfies the equations ( [ thm4e3_1 ] ) and ( [ thm4e3_2 ] ) by proving later lemma [ lem8 ] .    to satisfy the equality in the equation ( [ thm4e2_3 ] ) ,",
    "the equality condition in lemma [ lem3 ] must be satisfied , and the following two markov chains are formed :    1 .",
    "@xmath149 2 .",
    "@xmath150    where all random vectors are normally distributed , @xmath71 and @xmath72 are independent of each other , @xmath79 , and @xmath43 is independent of other random vectors .    the markov chain ( [ thm4e4_1 ] ) is naturally formed since @xmath43 , @xmath71 , and @xmath72 are independent gaussian random vectors .",
    "the validity of the markov chain ( [ thm4e4_2 ] ) is proved using the concept of moment generating function . in the markov chain ( [ thm4e4_2 ] ) , since all random vectors are gaussian ( without loss of generality , they are assumed to have zero means ) , using lemma [ lem6 ] , the following moment generating functions are expressed in closed - form : @xmath151 where @xmath152 , @xmath153 , @xmath154 , and their covariance matrices are represented by @xmath117 , @xmath118 , and @xmath119 , respectively . since @xmath42 is a positive definite matrix , there always exists the inverse of @xmath119 .    on the other hand ,",
    "the mgf of @xmath121 given @xmath90 is represented as @xmath155 if the factor @xmath156 in ( [ thm4e6_2 ] ) vanishes , @xmath88 and @xmath89 are independent given @xmath90 , and the markov chain ( [ thm4e4_2 ] ) is obtained .",
    "using lemma 11 , ( 1 ) in @xcite , we define a covariance matrix @xmath77 as follows : @xmath157 where @xmath158 , @xmath159 , and @xmath15 denotes an @xmath4-by-@xmath4 zero matrix .",
    "then , there exists a positive semi - definite matrix @xmath160 which satisfies @xmath161 where @xmath162 , and @xmath144 is a positive semi - definite matrix , which satisfies the following condition : @xmath163 .",
    "the existence of matrix @xmath160 is proved by the following lemma .",
    "[ lem8 ] there always exists a positive semi - definite matrix @xmath160 which satisfies @xmath164 where @xmath162 , and @xmath165 .",
    "since @xmath77 is defined as @xmath166 in ( [ thm4e7_1 ] ) , @xmath77 satisfies @xmath167 based on lemma 11 , ( 1 ) in @xcite .    since @xmath168 , multiplying with @xmath46 both sides of the equation ( [ thm4e9_1 ] ) , it follows that : @xmath169 and @xmath170    random vectors @xmath88 , @xmath89 , and @xmath90 are defined as @xmath152 , @xmath153 , and @xmath154 , respectively , and @xmath43 , @xmath71 , and @xmath72 are independent of each other . therefore , their covariance matrices are represented as @xmath171 from ( [ thm4e10_2 ] ) and ( [ thm4e11_3 ] ) , one can infer that @xmath172 and from ( [ thm4e10_4 ] ) and ( [ thm4e11_3 ] ) , it follows similarly that @xmath173 since the inverse matrix of @xmath77 exists , @xmath174 also exists .",
    "therefore , ( b ) in the equation ( [ thm4e6_2 ] ) is zero , and @xmath175 .",
    "it means @xmath88 and @xmath90 are independent given @xmath89 , i.e. , @xmath43 and @xmath176 are independent given @xmath177 , and the markov chain ( [ thm4e4_2 ] ) is valid",
    ". the equality in the equation ( [ thm4e2_3 ] ) is achieved by the above procedure , and the proof is completed .",
    "the versatility of the extremal entropy inequality was already illustrated by means of several applications in @xcite .",
    "however , the original proofs of the extremal entropy inequality in @xcite were based on the channel enhancement technique while one of those applications , the capacity of the vector gaussian broadcast channel , had been already proved by the channel enhancement technique in @xcite . even though the eei was adapted to prove the capacity of the vector gaussian broadcast channel in @xcite , it failed to show a novel perspective since the proof of the eei relied on the channel enhancement technique @xcite . on the other hand , based on our proof ,",
    "the extremal inequality shows not only its usefulness but also a novel perspective to prove the capacity of the vector gaussian broadcast channel .    to illustrate the usefulness of the proposed mathematical framework for proving the eei ,",
    "this section proposes three additional applications for the mathematical results presented in section [ sec3 ] .",
    "first , an alternative much simplified approach for proving the eei is provided .",
    "second , finding the optimal solution of a broadcasting channel with a private message and characterizing the mmse performance of a linear bayesian estimator for a gaussian source embedded in additive noise are presented as additional applications of the proposed results .",
    "based on the proof presented in previous section , one can come up with another more simplified proof for the eei .",
    "this method relies partly on calculus of variations techniques and partly on the results established in the previous section .",
    "however , this novel framework is very general and can be further adapted to proving many other information theoretic inequalities . in this regard ,",
    "a companion paper was submitted for publication @xcite .",
    "the proposed simplified proof of the eei runs as follows .",
    "first , select a gaussian random vector @xmath178 whose covariance matrix @xmath179 satisfies @xmath180 and @xmath181 . since the gaussian random vectors @xmath182 and",
    "@xmath183 can be represented as the sum of two independent random vectors @xmath184 and @xmath185 , and as the sum of two independent random vectors @xmath178 and @xmath186 , respectively , the lhs of the equation ( [ thm4e1_1 ] ) is expressed as follows : @xmath187    since the equation will be maximized over @xmath188 , the last two terms in ( [ ei_eq51_2 ] ) are ignored , and by defining the new random vectors @xmath189 and @xmath190 as @xmath191 and @xmath192 , respectively , the inequality in ( [ thm4e1_1 ] ) is equivalently expressed as the following variational problem : @xmath193 where @xmath194 and @xmath195 are vectors , @xmath196 , @xmath197 , @xmath198 , @xmath199 , @xmath200 , @xmath201 , and @xmath202 is the covariance matrix of the optimal solution @xmath203 .",
    "using euler s equations , we can solve this variational problem , and the problem in ( [ ei_eq52_1 ] ) is maximized when both @xmath204 and @xmath1 are gaussian random vectors ( as shown in appendix [ appd ] ) .",
    "the important thing to remark here is that solving this variational problem requires only the calculus of variations , i.e. , the proposed method does not require neither the classical epi nor the worst additive noise lemma .",
    "next the following inequality is obtained : @xmath205 based on lemma [ lem8 ] , the rhs of the equation ( [ ei_eq53_1 ] ) is expressed as @xmath206 and therefore , from the equations in ( [ ei_eq51_1 ] ) , ( [ ei_eq53_1 ] ) , and ( [ ei_eq54_1 ] ) , we obtain the following eei : @xmath207 and the proof is completed .",
    "consider the practical communication set - up depicted in figure [ fig1 ] , where a broadcasting channel with a private message is considered from the perspective of the mean square - error ( mse ) performance metric .",
    "the input - output relationship of this broadcast channel are governed by these equations : @xmath208 where @xmath209 and @xmath210 are additive gaussian noise vectors with zero means and covariance matrices @xmath211 and @xmath212 , respectively .",
    "the covariance matrices : @xmath211 and @xmath212 are assumed to be positive definite .",
    "matrix @xmath21 denotes the covariance matrix of @xmath0 , and @xmath3 stands for a positive semi - definite matrix .",
    "random vectors @xmath0 , @xmath209 , and @xmath210 are assumed independent of one another .",
    "random vectors @xmath88 and @xmath89 denote the received signals at the receiver @xmath213 and the receiver @xmath214 , respectively .",
    "assume that the message @xmath0 is expected to be decoded only at the receiver @xmath213 , but the message @xmath0 can be decoded at both the receivers @xmath213 and @xmath214 if they receive the message @xmath0 and the mses are below a certain threshold @xmath215 , respectively .",
    "therefore , the question here is whether or not we can find a random vector @xmath0 which guarantees the mse at the receiver @xmath213 is below the threshold @xmath215 , while the mse at the receiver @xmath214 is above the threshold @xmath215 , i.e. , the receiver @xmath213 can decode the message @xmath0 , but the receiver @xmath214 can not decode the message @xmath0 .",
    "the notation @xmath216 denotes the trace of a matrix .",
    "we are also interested in which distribution of @xmath0 is the most power efficient to maintain such a mse performance at the two receivers .     +    to compare the mse performance of the two receivers , we assume that both receivers use minimum mean - square error ( mmse ) estimators .",
    "since the minimum mse estimator is optimal in the sense that it achieves the lowest mse , this assumption is rational .    in summary ,",
    "the goal of this problem is to find the optimal distribution @xmath217 which satisfies the following problem : @xmath218 where @xmath219\\right)\\left(x-\\mathbb{e}\\left[x|y_1\\right]\\right)^t\\right]$ ] and @xmath220\\right)\\left(x-\\mathbb{e}\\left[x|y_2\\right]\\right)^t\\right]$ ] .",
    "the solution of the problem in ( [ new_app_eq1_3 ] ) can be obtained by the following procedure ; first , define a new gaussian random vector @xmath221 , which satisfies @xmath222 and @xmath223 , where @xmath224 stands for the covariance matrix of @xmath221 .",
    "second , find @xmath21 which satisfies @xmath225 . then , @xmath226 since @xmath227 , where @xmath228 , based on the data processing inequality for the covariance matrix @xcite .",
    "third , we will prove that there is a gaussian @xmath43 with a covariance matrix @xmath229 , which satisfies @xmath230 and @xmath231 , where @xmath232 .",
    "finally , based on lemma [ lem8 ] , we will show @xmath233 , where @xmath234 and @xmath235 . since @xmath236 is less than or equal to an arbitrary covariance matrix @xmath21 , which satisfies @xmath225",
    ", the gaussian random vector @xmath43 is the optimal solution ( the details of the proof are deferred to appendix [ appc ] ) .",
    "therefore , by choosing the message @xmath0 as a gaussian random vector in ( [ new_app_eq1_1 ] ) , we can securely transmit a private message , which is designed to arrive at the receiver @xmath213 , in the most power efficient way .",
    "this scenario can be interpreted as the secure transmission under a vector gaussian wire - tap channel . in this case , the receiver @xmath213 is the legitimate receiver , and the receiver @xmath214 is the eavesdropper .",
    "as shown in figure [ fig2 ] , the following additive noise channel is considered : @xmath237 where @xmath70 is a gaussian random vector with zero mean and covariance matrix @xmath132 , @xmath238 denotes an arbitrary random vector ( noise ) with zero mean and covariance matrix @xmath239 , and @xmath70 and @xmath238 are assumed independent of each other .",
    "we also assume that the covariance matrix of additive noise @xmath238 is upper - bounded , i.e. , @xmath240 , where @xmath3 is a given positive semi - definite matrix .     +    using the channel model in ( [ anc_eq1_1 ] )",
    ", we will next analyze the link between the channel input - output mutual information and the mmse performance of a linear bayesian estimator .",
    "first , consider the following optimization problem : @xmath241    the objective function in ( [ anc_eq2_1 ] ) can be expressed as @xmath242 using the extremal inequality in theorem [ thm3 ] , it follows that the optimal solution of the optimization problem in ( [ anc_eq2_1 ] ) is a multi - variate gaussian density function , and the objective criterion can be expressed as @xmath243 where @xmath244 is the covariance matrix of @xmath1 , @xmath245 , and @xmath246 is a gaussian random vector with the covariance matrix @xmath3 .",
    "the right - hand side of ( [ anc_eq4_1 ] ) can be further expressed in terms of the mse matrix of the linear minimum mse ( lmmse ) estimator under the worst case scenario , i.e. , the covariance matrix @xmath247 , as follows . given the channel ( [ anc_eq1_1 ] ) , the lmmse estimator for @xmath0 takes the form : @xmath248 + \\boldsymbol{\\sigma}_x \\left(\\boldsymbol{\\sigma}_x + \\mathbf{r}\\right)^{-1 } \\left(y - \\mathbb{e}\\left[x\\right]\\right)\\nonumber\\\\ \\label{anc_eq5_1 } & = & \\boldsymbol{\\sigma}_x \\left(\\boldsymbol{\\sigma}_x + \\mathbf{r}\\right)^{-1 } y,\\end{aligned}\\ ] ] where @xmath16 $ ] denotes the expectation , and @xmath204 stands for the lmmse estimator of @xmath0 .",
    "the equality in ( [ anc_eq5_1 ] ) is due to zero mean of @xmath0 .",
    "therefore , its mse is expressed as @xmath249    using ( [ anc_eq6_1 ] ) , the equation in ( [ anc_eq4_1 ] ) is expressed as @xmath250 and it follows that @xmath251    based on the equations in ( [ anc_eq8_1 ] ) , we can conclude the following facts .",
    "first , when the additive noise is gaussian , minimizing lmmse is equivalent to maximizing the mutual information between the input and the output .",
    "second , the worst case mutual information is expressed in terms of the lmmse , while the mutual information in general is lower bounded by a function of the lmmse .",
    "finally , we observe that the lmmse estimator is , in general , sub - optimal since the mutual information between the input and the output is larger than the function of the lmmse as shown in ( [ anc_eq8_1 ] ) .",
    "the main contributions of this paper are summarized as follows . in the first part of this paper , an alternative proof of the extremal entropy inequality",
    "is described in detail .",
    "the alternative proof is simpler , more direct , more explicit and more information - theoretic than the original proofs . the alternative proof is mainly based on the data processing inequality which enables to by - pass the kkt conditions .",
    "moreover , using properties of positive semi - definite matrices , one can skip the step of proving the existence of the optimal solution which satisfies the kkt conditions , a step which is quite complicated to justify .",
    "this novel technique is based on a data processing inequality , and it is very unique and creative in respect that it presents a novel paradigm for lots of applications such as the capacity of the vector gaussian broadcast channel and the secrecy capacity of the gaussian wire - tap channel , which were proved commonly based on the channel enhancement technique @xcite , @xcite , @xcite , and @xcite .",
    "additional relevant applications in this regard include @xcite-@xcite . in the second part of this paper ,",
    "several additional important applications for the extremal entropy inequality are presented . in this regard , a second and even more simplified approach for establishing the extremal entropy inequality without using epi or the worst data processing lemma is presented by exploiting the mathematical tools developed in the first part of this paper .",
    "two additional applications of the proposed mathematical results are presented for the problem of determining the optimal solution for the broadcasting channel problem with a private message and in establishing a mutual information - based performance bound for the mean square - error of a linear bayesian estimator for a gaussian source in an additive noise channel .",
    "one can observe that the last application presented can be sightly extended to non - gaussian sources , a fact that suggests that the extremal entropy inequality ( [ thm1e1_1 ] ) might hold true even for non - gaussian @xmath22 .",
    "however , establishing an extension of the eei in this direction or other directions such as proving the eei under a more general constraint ( such as an upper and/or lower - bound constraint on the power spectral density of the random vector @xmath0 instead of the covariance matrix constraint ) represent interesting open problems .",
    "finally , we would like to thank the reviewers for their constructive comments and bringing to our attention the reference @xcite which presents a completely different approach for proving eei .",
    "this approach relies on showing the optimality of gaussian distribution by exploiting the factorization of concave envelopes . at the time of submitting our paper in august 2011 , we were not aware of the parallel submission @xcite",
    "proving @xmath252 is equivalent to proving the following : @xmath253 since there always exists a non - singular matrix which simultaneously diagonalizes two positive semi - definite matrices @xcite , there exists a non - singular matrix @xmath254 which simultaneously diagonalizes both @xmath132 and @xmath42 as follows : @xmath255 where @xmath14 is an identity matrix , and @xmath256 is a diagonal matrix .",
    "since @xmath254 is a non - singular matrix , the inverse of @xmath254 always exists , and @xmath132 and @xmath42 are expressed as @xmath257 if we define @xmath258 as a diagonal matrix whose @xmath259 diagonal element is represented as @xmath260 , and which it is defined as @xmath261 where @xmath262 denotes the @xmath259 diagonal element of @xmath263 , and define @xmath125 as @xmath264 the equation ( [ appendeq1_4 ] ) is equivalent to @xmath265 the equation ( [ appendeq6_3 ] ) always holds since @xmath258 is defined as in ( [ appendeq4_1 ] ) and ( [ appendeq5_1 ] ) to satisfy ( [ appendeq6_3 ] ) .",
    "therefore , the inequality ( [ appendeq1_1 ] ) is also satisfied .",
    "we know that @xmath97 is @xmath266 .",
    "since @xmath267 , @xmath97 is expressed as @xmath268 , and @xmath269 and the equation ( [ appendeq7_1 ] ) is re - written as @xmath270 the equality ( [ appendeq8_2 ] ) is due to the equations ( [ appendeq3_1 ] ) , ( [ appendeq3_2 ] ) , and ( [ appendeq5_1 ] ) , and the equality ( [ appendeq8_4 ] ) is due to ( [ appendeq4_1 ] ) .",
    "similarly , @xmath271    therefore , by defining @xmath272 , we can make @xmath77 satisfy @xmath273 and the proof is completed .",
    "since the optimization problem in @xcite is generally nonconvex , the existence of optimal solution must be proved @xcite , @xcite , and this step is very complicated . however , in our proof , lemmas [ lem7 ] and [ lem8 ] serve as a substitute for this step since we by - pass kkt - condition related parts using the data processing inequality .",
    "this makes the proposed proof much simpler .",
    "proving @xmath274 is equivalent to proving the following : @xmath275 since there always exists a non - singular matrix which simultaneously diagonalizes two positive semi - definite matrices @xcite , there exists a non - singular matrix @xmath254 which simultaneously diagonalizes both @xmath42 and @xmath144 as follows : @xmath276 where @xmath14 is an identity matrix , and @xmath256 is a diagonal matrix . since @xmath254 is a non - singular matrix , the inverse of @xmath254 always exists , and @xmath42 and @xmath144 are expressed as @xmath277    if we define @xmath278 as a diagonal matrix whose @xmath259 diagonal element is represented as @xmath279 , and which it is defined as @xmath280 where @xmath262 denotes the @xmath259 diagonal element of @xmath263 , and define @xmath160 as @xmath281 then the equation ( [ apendeq1_2 ] ) is equivalent to @xmath282 the equation ( [ apendeq5_3 ] ) always holds since @xmath278 is defined in ( [ apendeq3_3 ] ) .",
    "therefore , the inequality ( [ apendeq1_1 ] ) is also satisfied .",
    "we know that @xmath46 is @xmath283 .",
    "therefore , @xmath284 and the equation ( [ apendeq6_1 ] ) is re - written as @xmath285 the equality ( [ apendeq7_2 ] ) is due to the equations ( [ apendeq3_1 ] ) , ( [ apendeq3_2 ] ) , and ( [ apendeq4_1 ] ) , and the equality ( [ apendeq7_5 ] ) is due to ( [ apendeq3_3 ] ) . similarly , @xmath286    therefore , by defining @xmath287 , we can make @xmath77 satisfy @xmath288 and the proof is completed .    in lemmas [ lem7 ] and [ lem8 ] ,",
    "we specify the structure of positive semi - definite matrices @xmath125 and @xmath160 , and this yields additional details about the structure of the covariance matrix of the optimal solution .",
    "the problem in ( [ ei_eq52_1 ] ) is more appropriately re - formulated as follows : @xmath289 where the arbitrary deterministic non - zero vector @xmath290 is defined as @xmath291^t$ ] , @xmath292 denotes the @xmath293 row and @xmath294 column element of @xmath295 , @xmath296 , and @xmath297 .    using lagrange multipliers , the functional problem and its constraints in ( [ ei_eq21_1 ] )",
    "are expressed as @xmath298 where @xmath299 where @xmath300 , @xmath301 , @xmath302 , @xmath303 , @xmath304 , and @xmath305 stand for the lagrange multipliers .",
    "the first - order variation condition is checked as follows : @xmath306 @xmath307 and @xmath308 are the first - order partial derivatives with respect to @xmath309 and @xmath310 , respectively .    since the equalities in ( [ ei_eq24_1 ] ) and ( [ ei_eq24_2 ] ) must be satisfied for any @xmath194 and @xmath195",
    ", one can easily obtain the following gaussian density functions @xmath311 and @xmath312 as solutions : @xmath313 since all the lagrange multipliers exist in this problem , the necessary optimal solutions @xmath311 and @xmath312 exist even though the original problem is non - convex in general .    to make the second variation positive ,",
    "the negative - definiteness of the following matrix is required : @xmath314,\\end{aligned}\\ ] ] where @xmath315 and @xmath316 stand for the second - order partial derivatives with respect to @xmath317 and @xmath318 , respectively , and @xmath319 denotes the second - order partial derivative with respect to @xmath311 and @xmath312 .",
    "thus , the following condition is required to hold : @xmath320 \\left[\\begin{array}{cc } k''_{f _ { \\hat{x}^ * } f _ { \\hat{x}^ * } } &   k''_{f _ { \\hat{x}^ * } f _ { y^*}}\\\\ k''_{f _ { y^ * } f _ { \\hat{x}^ * } } &   k''_{f _ { y^ * } f _ { y^ * } } \\end{array}\\right ] \\left[\\begin{array}{c } h _",
    "{ \\hat{x } } \\\\ h _ { y}\\end{array } \\right ] \\nonumber\\\\ & = & k''_{f _ { \\hat{x}^ * } f _ { \\hat{x}^ * } } h _ { \\hat{x}}^2 + k''_{f _ { y^ * } f _ { y^ * } } h _ { y}^2 +   ( k''_{f _ { \\hat{x}^ * } f _ { y^*}}+k''_{f _ { y^ * } f _ { \\hat{x}^ * } } ) h _ { y } h _ { \\hat{x}}\\nonumber\\\\ & \\leq & 0,\\end{aligned}\\ ] ] where @xmath321 and @xmath322 are arbitrary admissible functions .    since @xmath315 , @xmath319 , @xmath323 , and @xmath316",
    "are defined as @xmath324 the equation in ( [ ei_eq35_1 ] ) requires @xmath325 where @xmath326 .",
    "therefore , the optimal solutions @xmath317 and @xmath312 maximize the functional problem in ( [ ei_eq52_1 ] ) , and the proof is completed .",
    "using lemma [ lem8 ] , we can define a covariance matrix @xmath224 which satisfies @xmath222 and @xmath327 as follows : @xmath328 where @xmath160 is a positive semi - definite matrix , defined similarly to the one in lemma [ lem8 ] .    since @xmath329 where @xmath330 denotes the fisher information matrix of the random vector @xmath331 @xcite , by changing the covariance matrix of @xmath0 , we can always find @xmath0 , whose posterior covariance matrix @xmath332 satisfies @xmath225 .      using cramr - rao inequality @xcite",
    ", we can choose a gaussian random vector @xmath43 , whose covariance matrix @xmath236 satisfies the following : @xmath335 and @xmath336 therefore , for any random vector @xmath0 , whose covariance matrix @xmath132 satisfies @xmath225 , we can find a gaussian random vector @xmath43 , whose covariance matrix satisfies the relationship in ( [ appc_eq1_1 ] ) . also , due to the equations in ( [ appc_eq2_1 ] ) and ( [ appc_eq0_1 ] ) , @xmath337 where @xmath338 .",
    "now , based on lemma [ lem8 ] , we will show @xmath339 as follows .",
    "since @xmath340 , we can construct a markov chain as @xmath341 where @xmath342 is a gaussian random vector with the covariance matrix @xmath343 , and it satisfies @xmath344 .",
    "the markov chain in ( [ appc_eq6_1 ] ) is the same as the one in ( [ thm4e4_1 ] ) , and therefore , based on lemma [ lem8 ] , we can obtain the markov chain : @xmath345 and this markov chain is the same as the one in ( [ thm4e4_2 ] ) . in this case ,",
    "@xmath346 , and @xmath236 is defined as @xmath347 , where @xmath348 and @xmath349 are gaussian random vectors with covariance matrices @xmath350 and @xmath351 , respectively , @xmath352 , @xmath353 , and all random vectors are independent of one another .",
    "the positive semi - definite matrix @xmath160 is defined as the one in lemma [ lem8 ] .",
    "the constant @xmath39 must be chosen to satisfy the equation in ( [ appc_eq0_1 ] ) . by defining the matrix @xmath350 as follows : @xmath354 where matrix @xmath125 is similarly defined as the one in lemma [ lem7 ] ,",
    "the existence of such @xmath43 is guaranteed .",
    "therefore , by choosing a gaussian random vector @xmath43 as mentioned previously , @xmath355 and the covariance matrix @xmath236 is the minimum value with respect to the positive semi - definite partial ordering , and the proof is completed .      c.  e.  shannon , `` a mathematical theory of communication , '' _ bell system tech .",
    "j. _ , vol .",
    "623 - 656 , oct . 1948 .",
    "a.  j.  stam , `` some inequalities satisfied by the quantities of information of fisher and shannon , '' _ inf . & cont .",
    "101 - 112 , jun .",
    "1959 . n.  m.  blachman , `` the convolution inequality for entropy powers , '' _ ieee trans .",
    "inf . theory _ ,",
    "267 - 271 , apr . 1965 .",
    "a.  dembo , t.  m.  cover , and j.  a.  thomas , `` information theoretic inequalities , '' _ ieee trans .",
    "inf . theory _ ,",
    "37 , no . 6 , pp . 1501 - 1518 , nov .",
    "s.  verd and d.  guo , `` a simple proof of the entropy - power inequality , '' _ ieee trans .",
    "inf . theory _ ,",
    "2165 - 2166 , may 2006 .",
    "o.  rioul , `` information theoretic proofs of entropy power inequalities , '' _ ieee trans .",
    "inf . theory _ ,",
    "33 - 55 , jan 2011 .",
    "m.  h.  m.  costa , `` a new entropy power inequality , '' _ ieee trans .",
    "inf . theory _ ,",
    "31 , no . 6 , pp .",
    "751 - 760 , nov . 1985 .",
    "o.  johnson,``a conditional entropy power inequality for dependent variables , '' _ ieee trans .",
    "inf . theory _ ,",
    "50 , no . 8 ,",
    "pp . 1581 - 1583 , aug .",
    "d.  guo , s.  shamai ( shitz ) , and s.  verd , `` mutual information and minimum mean - square error in gaussian channels '' , _ ieee trans .",
    "inform . theory _",
    "1261 - 1282 , apr . 2005 .",
    "s.  park , e.  serpedin , and k.  qaraqe , `` on the equivalence between stein and de bruijn identities , '' _ ieee transactions on information theory",
    "12 , dec . 2012 .",
    "t.  liu and p. viswanath ,  an extremal inequality motivated by multiterminal information - theoretic problems , \" _ ieee trans .",
    "inf . theory _ ,",
    "1839 - 1851 , may 2007 .",
    "h.  weingarten , y.  steinberg , and s.  shamai ( shitz ) , `` the capacity region of the gaussian mutiple - input multiple - output broadcast channel , '' _ ieee trans .",
    "inf . theory _ ,",
    "3936 - 3964 , sep 2006 . s.  n.  diggavi and t.  m.  cover , `` the worst additive noise under a covariance constraint , '' _ ieee trans .",
    "inf . theory _ ,",
    "47 , no . 7 , pp . 3072 - 3081 , nov . 2001 .",
    "t.  liu and s.  shamai ( shitz ) , `` a note on the secrecy capacity of the multiple - antenna wiretap channel , '' _ ieee trans .",
    "inf . theory _",
    "55 , no . 6 ,",
    "2547 - 2553 , jun 2009 .",
    "e.  ekrem and s.  ulukus , `` the secrecy capacity region of the gaussian mimo multi - receiver wiretap channel , '' _ ieee trans .",
    "inf . theory _",
    "2083 - 2114 , mar 2011 .",
    "a.  khisti and g.  w.  wornell ,",
    "`` secure transmission with multiple antennas : the mimome channel , '' _ ieee trans .",
    "inf . theory _",
    "5515 - 5532 , nov .",
    "f.  oggier and b.  hassibi , `` the secrecy capacity of the mimo wiretap channel , '' _ in proc .",
    "information theory _ ,",
    "toronto , on , canada , jul .",
    "2008 , pp .",
    "524 - 528 .",
    "a.  khisti , g.  w.  wornell , a.  wiesel , and y.  eldar , `` on the gaussian mimo wiretap channel , '' _ in proc .",
    "information theory _ , nice , france , jun .",
    "2007 , pp .",
    "2471 - 2475 .",
    "a.  d.  wyner , `` the wire - tap channel , '' _ bell syst .",
    "j. _ , vol .",
    "54 , no . 8 , pp . 1355 - 1387 , oct .",
    "x.  he , and a.  yener , `` the gaussian many - to - one interference channel with confidential messages , '' _ ieee trans .",
    "inf . theory _ ,",
    "5 , pp . 2730 - 2745 , may 2011 .",
    "x.  he , and a.  yener , `` cooperation with an untrusted relay : a secrecy perspective , '' _ ieee trans .",
    "inf . theory _",
    "8 , pp . 3807 - 3827 , aug .",
    "e. ekrem , and s. ulukus , `` an outer bound for the gaussian mimo broadcast channel with common and private messages , '' _ ieee trans .",
    "inf . theory _ , vol .",
    "11 , pp . 6766 - 6772 ,",
    "nov . 2012 .",
    "e. ekrem , and s. ulukus , `` degraded compound multi - receiver wiretap channels , '' _ ieee trans .",
    "inf . theory _ ,",
    "5681 - 5698 , sep . 2012 .",
    "e. ekrem , and s. ulukus , `` capacity region of gaussian mimo broadcast channels with common and confidential messages , '' _ ieee trans .",
    "inf . theory _ , vol .",
    "9 , pp . 5669 - 5680 , sep . 2012 .",
    "t.  m.  cover and j.  a.  thomas , _ elements of information theory _ ,",
    "new york : wiley - interscience , 1991 .",
    "r.  a.  horn and c.  r.  johnson , _ matrix analysis _ , cambridge university press , 1985 .",
    "j.  r.  magnus and h.  neudecker , _ matrix differential calculus with applications in statistics and econometrics _ , john wiley&sons , 1999 . s. park , e. serpedin and k. qaraqe ,  a unifying veriational perspective on some fundamental information theoretic inequalities , \" _ ieee trans .",
    "inf . theory _ , ( submitted for publication ) sept .",
    "y. geng and c. nair ,  the capacity region of the two - receiver vector gaussian broadcast channel with private and common messages , \" _ ieee trans .",
    "inf . theory _ , arxiv.org/pdf/1202.0097 , february 2012 ."
  ],
  "abstract_text": [
    "<S> this paper first focuses on deriving an alternative approach for proving an extremal entropy inequality ( eei ) , originally presented in @xcite . </S>",
    "<S> the proposed approach does not rely on the channel enhancement technique , and has the advantage that it yields an explicit description of the optimal solution as opposed to the implicit approach of @xcite . compared with the proofs in @xcite , </S>",
    "<S> the proposed alternative proof is also simpler , more direct , more information - theoretic , and has the additional advantage that it offers a new perspective for establishing novel as well as known challenging results such the capacity of the vector gaussian broadcast channel , the lower bound of the achievable rate for distributed source coding with a single quadratic distortion constraint , and the secrecy capacity of the gaussian wire - tap channel . </S>",
    "<S> the second part of this paper is devoted to some novel applications of the proposed mathematical results . </S>",
    "<S> the proposed mathematical techniques are further exploited to obtain a more simplified proof of the eei without using the entropy power inequality ( epi ) , to build the optimal solution for a special class of broadcasting channels with private messages and to obtain a mutual information - based performance bound for the mean square - error of a linear bayesian estimator of a gaussian source embedded in an additive noise channel .    </S>",
    "<S> entropy power inequality ( epi ) , extremal entropy inequality ( eei ) , data processing inequality , channel enhancement , broadcast channel , wire - tap channel , cramer - rao bound , bayesian estimation </S>"
  ]
}