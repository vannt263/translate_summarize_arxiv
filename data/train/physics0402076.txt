{
  "article_text": [
    "the concept of small - world networks and the variant of scale free networks has become very popular recently @xcite , after the discovery that such networks are realized in diverse areas as the organization of human society ( milgram s experiment ) @xcite , in the www @xcite , in the internet @xcite , in the distribution of electrical power ( western us ) @xcite , and in the metabolic network of the bacterium _ escherichia coli _ @xcite .    according to watts and strogatz ,",
    "a small - world network is characterized by a clustering coefficient @xmath0 and a path length @xmath1 @xcite .",
    "the clustering coefficient measures the probability that given node @xmath2 is connected to nodes @xmath3 and @xmath4 then also nodes @xmath3 and @xmath4 are connected .",
    "the shortest path length from node @xmath2 to node @xmath3 is the minimal number of connections needed to get from @xmath2 to @xmath3 .",
    "a small - world network is defined by two properties .",
    "first , the average clustering coefficient @xmath0 is larger than for a corresponding random network with the same number of connections and nodes .",
    "the clustering coefficient expected for a regular rectangular grid network is zero while for a random network the probability @xmath5 of connection of two nodes is the same for neighboring nodes as for distant nodes .",
    "second , the average path length @xmath1 scales like @xmath6 , where @xmath7 is the number of nodes . for regular ( grid ) networks @xmath1 scales as @xmath8 where @xmath9 is the dimension of the space and for random networks @xmath1 scales as @xmath6 . as result , @xmath0 is large and @xmath1 is small in a small - world network .    in studies of small - world neural networks",
    "it has turned out that this architecture has potentially many advantages . in a hodgkin - huxley network",
    "the small - world architecture has been found to give a fast and synchronized response to stimuli in the brain @xcite . in associative memory models",
    "it was observed that the small - world architecture yields the same memory retrieval performance as randomly connected networks , using only a fraction of total connection length @xcite .",
    "likewise , in the hopfield associative memory model the small - world architecture turned out to be optimal in terms of memory storage abilities @xcite . with a integrate - and - fire type of neuron",
    "it has been shown @xcite that short - cuts permit self - sustained activity .",
    "a model of neurons connected in small - world topology has been used to explain short bursts and seizure - like electrical activity in epilepsy @xcite . in biological context",
    "some experimental works seem to have found scale - free and small - world topology in living animals like in the macaque visual cortex@xcite , in the cat cortex @xcite and even in networks of correlated human brain activity measured via functional magnetic resonance imaging @xcite .",
    "the network of cortical neurons in the brain has sparse long ranged connectivity , which may offer some advantages of small world connectivity @xcite .",
    "in the present work we study supervised learning with back - propagation in a multi - layered feed - forward network . around 1960 ,",
    "the perceptron model @xcite has been widely investigated .",
    "the information ( representing action potentials propagating in axons and dendrites of biological neurons ) feeds in forward direction .",
    "the perceptron model has been extended to include several layers .",
    "e.g. , a convolutional network , consisting of seven layers plus one input layer has been used for reading bank cheques @xcite .",
    "the task of learning consists of finding optimal weights @xmath10 between neurons ( representing synaptic connections ) such that for a given set of input and output patterns ( training patterns ) the network generates output patterns as close as possible to target patterns .",
    "we used the algorithm of back propagation @xcite to determine those weights .",
    "there are alternative , potentially faster methods to determine those weights like , e.g. simulated annealing . here",
    "we aim to compare different network architectures with respect to learning , using as reference a standard algorithm to determine those weights .",
    ".network parameters .",
    "@xmath11 and @xmath12 correspond to position where both @xmath13 and @xmath14 are small .",
    "scaling of @xmath15 ( eq.([eq : scalingr ] ) ) and of @xmath14 ( eq.([eq : scalingdglobal ] ) ) . [ cols=\"^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     we change the architecture of neural connections from regular to random architecture , while keeping the number @xmath16 of connections fixed .",
    "initially , neurons are connected feed - forward , i.e. each neuron of a given layer connects to all neurons of the subsequent layer .",
    "we make a random draw of two nodes which are connected to each other .",
    "we cut that `` old '' link . in order to create a `` new '' link",
    ", we make another random draw of two nodes .",
    "if those nodes are already connected to each other , we make further draws until we find two unconnected nodes .",
    "then we create a `` new '' link between those nodes .",
    "thus we conserve the total number @xmath16 of links .",
    "the number of links in a regular network is given by @xmath17 , where @xmath18 denotes the number of neurons per layer and @xmath19 denotes the number layers . in this way",
    "we create some connections between nodes in distant layers , i.e. short - cuts and the topology changes gradually ( see fig.[[fig : connectivity ] ] ) .",
    "in particular , while the initial connectivity is regular , after many rewiring steps the topology becomes random . somewhere in between lies the small - world architecture @xcite .",
    "we consider networks consisting of one input layer , one output layer and one or more hidden layers .",
    "while the standard perceptron model has only two layers , here we explore the use of more hidden layers , because only then the path length @xmath1 can become small and the network `` small - world '' .",
    "instead of measuring the network architecture by the functions @xmath0 and @xmath1 , we have used the functions of local and global connectivity length @xmath13 and @xmath14 , respectively , introduced by marchiori et al . @xcite .",
    "those functions are more suitable because they allow to take into account the connectivity matrix and the weight matrix in networks and also to treat networks with some unconnected neurons ( often nodes are unconnected due to rewiring ) . @xmath13 and @xmath14 are defined via the concept of global and local efficiency @xcite . for a graph @xmath20 its efficiency is defined by @xmath21 where @xmath22 denotes the shortest path length between vertices @xmath23 and @xmath24 and @xmath7 is the total number of vertices .",
    "moreover , one defines the local efficiency as average efficiency of subgraphs @xmath25 ( the subgraph @xmath25 is usually formed by the neurons directly connected to neuron @xmath23 .",
    "we have included also all neurons occuring in the same layer as neuron @xmath23 ) .",
    "@xmath26 the connectivity length is defined by the inverse of efficiency , @xmath27 it has been shown that @xmath14 is similar to @xmath1 , and @xmath13 is similar to @xmath28 @xcite , although this is not an exact relation . thus ,",
    "if a network is small - world , both , @xmath13 and @xmath14 should become small .",
    "fig.[[fig : dlocglob ] ] shows @xmath13 and @xmath14 as a function of the number of rewiring steps for a network of 5 neurons per layer and 5 layers .",
    "one observes that both , @xmath13 and @xmath14 , become small at about @xmath29 .",
    "the position of minima for other networks are shown in tab.[[tab : scaling ] ] .",
    "small world networks are characterized by the scaling law @xmath30 . from the similarity of @xmath14 and @xmath1",
    ", we expect @xmath31 in tab.[[tab : scaling ] ] we display @xmath32 , where @xmath12 denotes the value of @xmath14 at the position where both @xmath13 and @xmath14 are small .",
    "the onset of scaling is observed when networks become larger . from our data we observed another scaling law for the variable @xmath33 .",
    "our data are consistent with @xmath34 where @xmath35 and @xmath36 are constants . in tab.[[tab : scaling ] ] we display @xmath36 .",
    "this law says that the number of rewirings associated with the minimum of @xmath13 and @xmath14 , i.e. small world connectivity , measured in terms of the number of connections of the regular network , increases with the number of connections in a logarithmic way .",
    "we trained the networks with random binary input and output patterns .",
    "our neurons are sigmoid real - valued units .",
    "the learning time , defined as the number of iterations it takes until the error of training becomes smaller than a given error tolerance , depends on the error tolerance . in the following",
    "we will present the learning error instead of learning time .",
    "the effect on learning due to a variation of the number of layers and the effect of rewiring , i.e. introducing short - cuts , is shown in fig.[[fig : function_layer ] ] .",
    "it shows the absolute error , which is an average of the absolute error over the neurons , patterns and statistical tests . we have considered a network of 5 neurons per layer and varied the number of layers from 5 to 8",
    "we compare the regular network ( @xmath37 ) with the network of @xmath38 .",
    "we find that for up to a few thousand iterations the network with @xmath38 gives a smaller error compared to the regular network . in the regime of many iterations",
    ", one observes that the 5-layer regular network learns ( i.e. the error decreases with iterations ) , while the regular 8-layer network does not learn at all .",
    "in contrast , after 40 rewirings all networks do learn .",
    "the improvement over the regular network is small for 5 layers , but major for 8 layers .",
    "this indicates that the learning improvement due to small - world architecture becomes more efficient in the presence of more layers . in the case of 8 layers ,",
    "the error curve for @xmath38 is in the regime where @xmath13 and @xmath14 are both small ( see tab.[[tab : scaling ] ] ) , i.e. close to the small - world architecture .",
    "the effect on learning when changing the number of learning patterns and the rewiring of the network is shown in fig.[[fig : function_pattern ] ] for a network of 5 neurons per layer and 5 layers .",
    "when learning 5 patterns ( fig.[[fig : function_pattern]a ] ) in the domain of few iterations ( 1000 - 5000 ) , rewiring brings about a substantial reduction in the error compared to the regular network ( @xmath37 ) and also to the random network ( @xmath39 ) . for very many iterations ( about 500000 )",
    "there is little improvement with rewirings compared to the regular architecture . for learning of 20 patterns ( fig.[[fig : function_pattern]b ] )",
    "the behavior is similar , but the reduction of error in the presence of 20 rewirings is more substantial for many iterations . when learning 80 patterns fig.[[fig : function_pattern]c ] we see that the error is large , we are near or beyond the storage capacity . in this case",
    "the regular architecture is optimal and rewiring brings no advantage .",
    "the influence of the number of neurons per layer on learning is depicted in fig.[[fig : function_neurons ] ] .",
    "we compare ( a ) a network of 5 neurons per layer and 8 layers with a network ( b ) of 15 neurons per layer and 8 layers .",
    "we trained the network with 40 patterns in both cases . in case ( a ) ( not shown ) we found that the error as function of rewiring has a minimum at @xmath40 which is in coincidence with the minimum of @xmath13 and @xmath14 given in tab.[[tab : scaling ] ] , i.e in the small - world regime .",
    "this gives a clear improvement over the regular architecture ( @xmath37 ) and the random architecture ( @xmath39 ) . in case",
    "( b ) the learning error is shown in fig.[[fig : function_neurons ] ] .",
    "one observes that the learning error has a minimum at @xmath41 , which is close to the minimum of @xmath13 and @xmath14 at @xmath42 ( see tab.[[tab : scaling ] ] ) , also near the small - world regime .",
    "so far we studied learning when the training patterns were given all together in the beginning .",
    "however , sometimes one is interested in changing strategies or parameters during training ( enforced learning ) . in order to see",
    "if the small world architecture gives an advantage in such scenarios , we studied learning by adding patterns sequentially in groups of 2 patterns . for a network of 5 neurons per layer and 5 layers , fig.[[fig : addpattseq ] ] shows the learning error versus the number of rewirings .",
    "also here one observes a distinctive gain by introducing some rewirings ( optimum at @xmath43 ) over the regular network ( @xmath37 ) and the random network ( @xmath39 ) .    in the last experiment we studied generalization in a network of 5 neurons per layer and 8 layers .",
    "we trained the network to put patterns into classes .",
    "we considered 5 classes : a pattern belongs to class 1 , if neuron 1 in the input layer has the highest value of all neurons in this layer .",
    "class 2 corresponds to neuron 2 having the highest value .",
    "the classification of 200 patterns achieved by the network as function of connectivity is shown in fig.[[fig : gen5x8 ] ] .",
    "it turns out the network with some rewirings gives improvement over the regular and also random network architecture .",
    "we observe that the generalization error has a minimum at @xmath43 , which is in the regime where @xmath13 and @xmath14 are small ( see tab.[[tab : scaling ] ] ) , i.e. in the regime of small - world architecture .    in summary",
    ", we observed that the network with some rewirings .",
    ", i.e. short - cuts , gives reduced learning errors . the minimum of the error as function of rewiring lies in the neighborhood of the minimum of @xmath13 and @xmath13 which indicates small - world architecture . however , while @xmath13 and @xmath14 depend only on the connectivity of the network , learning error and learning time depend on other parameters like the number of learning patterns , or the mode of learning , e.g. standard learning or enforced learning .",
    "we believe that our results have important implications on artificial intelligence and artificial neural networks .",
    "neural networks are being widely used as computational devices for optimization problems based on learning in applications like pattern recognition , image processing , speech recognition , error detection , and quality control in industrial production , like of car parts or sales of air - line tickets .",
    "our study reveals a clear advantage and suggests to use small - world networks in such applications .",
    "our study may have potential applications in the domain of data - mining .",
    "labiouse , c.l , salah , a.a . & starikova , i . ,",
    "`` the impact of connectivity on the memory capacity and the retrieval dynamics of hopfield - type networks , '' in proceedings of the santa fe complex systems summer school , pp .",
    "77 - 84 , budapest , nm : sfi , 2002 ."
  ],
  "abstract_text": [
    "<S> we investigate supervised learning in neural networks . </S>",
    "<S> we consider a multi - layered feed - forward network with back propagation . </S>",
    "<S> we find that the network of small - world connectivity reduces the learning error and learning time when compared to the networks of regular or random connectivity . </S>",
    "<S> our study has potential applications in the domain of data - mining , image processing , speech recognition , and pattern recognition . </S>"
  ]
}