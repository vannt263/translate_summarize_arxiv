{
  "article_text": [
    "machine learning and neuroscience speak different languages today . brain science has discovered a dazzling array of brain areas , cell types , molecules , cellular states , and mechanisms for computation and information storage .",
    "machine learning , in contrast , has largely focused on instantiations of a single principle : function optimization .",
    "it has found that simple optimization objectives , like minimizing classification error , can lead to the formation of rich internal representations and powerful algorithmic capabilities in multilayer and recurrent networks  @xcite .",
    "here we seek to connect these perspectives .",
    "the artificial neural networks now prominent in machine learning were , of course , originally inspired by neuroscience  @xcite .",
    "while neuroscience has continued to play a role  @xcite , many of the major developments were guided by insights into the mathematics of efficient optimization , rather than neuroscientific findings  @xcite .",
    "the field has advanced from simple linear systems  @xcite , to nonlinear networks  @xcite , to deep and recurrent networks  @xcite .",
    "backpropagation of error  @xcite enabled neural networks to be trained efficiently , by providing an efficient means to compute the gradient with respect to the weights of a multi - layer network .",
    "methods of training have improved to include momentum terms , better weight initializations , conjugate gradients and so forth , evolving to the current breed of networks optimized using batch - wise stochastic gradient descent .",
    "these developments have little obvious connection to neuroscience .",
    "we will argue here , however , that neuroscience and machine learning are , once again , ripe for convergence .",
    "three aspects of machine learning are particularly important in the context of this paper .",
    "first , machine learning has focused on the optimization of cost functions ( * figure [ fig1]a * ) .",
    "second , recent work in machine learning has started to introduce complex cost functions , those that are not uniform across layers and time , and those that arise from interactions between different parts of a network .",
    "for example , introducing the objective of temporal coherence for lower layers ( non - uniform cost function over space ) improves feature learning  @xcite , cost function schedules ( non - uniform cost function over time ) improve generalization  @xcite and adversarial networks  an example of a cost function arising from internal interactions",
    " allow gradient - based training of generative models  @xcite .",
    "networks that are easier to train are being used to provide `` hints '' to help bootstrap the training of more powerful networks  @xcite .",
    "third , machine learning has also begun to diversify the architectures that are subject to optimization .",
    "it has introduced simple memory cells with multiple persistent states  @xcite , more complex elementary units such as `` capsules '' and other structures  @xcite , content addressable  @xcite and location addressable memories  @xcite , as well as pointers  @xcite and hard - coded arithmetic operations  @xcite .",
    "these three ideas have , so far , not received much attention in neuroscience .",
    "we thus formulate these ideas as three hypotheses about the brain , examine evidence for them , and sketch how experiments could test them . but",
    "first , let us state the hypotheses more precisely .",
    "the central hypothesis for linking the two fields is that biological systems , like many machine - learning systems , are able to optimize cost functions .",
    "the idea of cost functions means that neurons in a brain area can somehow change their properties , e.g. , the properties of their synapses , so that they get better at doing whatever the cost function defines as their role .",
    "human behavior sometimes approaches optimality in a domain , e.g. , during movement  @xcite , which suggests that the brain may have learned optimal strategies .",
    "subjects minimize energy consumption of their movement system  @xcite , and minimize risk and damage to their body , while maximizing financial and movement gains .",
    "computationally , we now know that optimization of trajectories gives rise to elegant solutions for very complex motor tasks  @xcite .",
    "we suggest that cost function optimization occurs much more generally in shaping the internal representations and processes used by the brain .",
    "we also suggest that this requires the brain to have mechanisms for efficient credit assignment in multilayer and recurrent networks .",
    "a second realization is that cost functions need not be global",
    ". neurons in different brain areas may optimize different things , e.g. , the mean squared error of movements , surprise in a visual stimulus , or the allocation of attention .",
    "importantly , such a cost function could be locally generated .",
    "for example , neurons could locally evaluate the quality of their statistical model of their inputs ( * figure [ fig1]b * ) .",
    "alternatively , cost functions for one area could be generated by another area .",
    "moreover , cost functions may change over time , e.g. , guiding young humans to understanding simple visual contrasts early on , and faces a bit later .",
    "this could allow the developing brain to bootstrap more complex knowledge based on simpler knowledge .",
    "cost functions in the brain are likely to be complex and to be arranged to vary across areas and over development .",
    "a third realization is that structure matters .",
    "the patterns of information flow seem fundamentally different across brain areas , suggesting that they solve distinct computational problems .",
    "some brain areas are highly recurrent , perhaps making them predestined for short - term memory storage  @xcite .",
    "some areas contain cell types that can switch between qualitatively different states of activation , such as a persistent firing mode versus a transient firing mode , in response to particular neurotransmitters  @xcite .",
    "other areas , like the thalamus appear to have the information from other areas flowing through them , perhaps allowing them to determine information routing  @xcite .",
    "areas like the basal ganglia are involved in reinforcement learning and gating of discrete decisions  @xcite . as every programmer knows ,",
    "specialized algorithms matter for efficient solutions to computational problems , and the brain is likely to make good use of such specialization ( * figure [ fig1]c * ) .",
    "these ideas are inspired by recent advances in machine learning , but we also propose that the brain has major differences from any of today s machine learning techniques .",
    "in particular , the world gives us a relatively limited amount of information that we could use for supervised learning  @xcite .",
    "there is a huge amount of information available for unsupervised learning , but there is no reason to assume that a _ generic _ unsupervised algorithm , no matter how powerful , would learn the precise things that humans need to know , in the order that they need to know it . the evolutionary challenge of making unsupervised learning solve the `` right '' problems is , therefore , to find a sequence of cost functions that will deterministically build circuits and behaviors according to prescribed developmental stages , so that in the end a relatively small amount of information suffices to produce the right behavior .",
    "for example , a developing duck imprints  @xcite a template of its parent , and then uses that template to generate goal - targets that help it develop other skills like foraging .",
    "generalizing from this and from other studies  @xcite , we propose that many of the brain s cost functions arise from such an internal bootstrapping process .",
    "indeed , we propose that biological development and reinforcement learning can , in effect , program the emergence of a sequence of cost functions that precisely anticipates the future needs faced by the brain s internal subsystems , as well as by the organism as a whole .",
    "this type of developmentally programmed bootstrapping generates an internal infrastructure of cost functions which is diverse and complex , while simplifying the learning problems faced by the brain s internal processes . beyond simple tasks like familial imprinting",
    ", this type of bootstrapping could extend to higher cognition , e.g. , internally generated cost functions could train a developing brain to properly access its memory or to organize its actions in ways that will prove to be useful later on .",
    "the potential bootstrapping mechanisms that we will consider operate in the context of unsupervised and reinforcement learning , and go well beyond the types of curriculum learning ideas used in today s machine learning  @xcite .    in the rest of this paper",
    ", we will elaborate on these hypotheses .",
    "first , we will argue that both local and multi - layer optimization is , perhaps surprisingly , compatible with what we know about the brain .",
    "second , we will argue that cost functions differ across brain areas and change over time and describe how cost functions interacting in an orchestrated way could allow bootstrapping of complex function .",
    "third , we will list a broad set of specialized problems that need to be solved by neural computation , and the brain areas that have structure that seems to be matched to a particular computational problem .",
    "we then discuss some implications of the above hypotheses for research approaches in neuroscience and machine learning , and sketch a set of experiments to test these hypotheses .",
    "finally , we discuss this architecture from the perspective of evolution .        2",
    "much of machine learning is based on efficiently optimizing functions , and , as we will detail below , the ability to use backpropagation of error  @xcite to calculate gradients of arbitrary parametrized functions has been a key breakthrough . in * hypothesis 1 *",
    ", we claim that the brain is also , at least in part , an optimization machine .",
    "but what exactly does it mean to say that the brain can optimize cost functions ?",
    "after all , many processes can be viewed as optimizations .",
    "for example , the laws of physics are often viewed as minimizing an action functional , while evolution optimizes the fitness of replicators over a long timescale . to be clear ,",
    "our main claims are : that * a ) * the brain has powerful mechanisms for credit assignment during learning that allow it to optimize global functions in multi - layer networks by adjusting the properties of each neuron to contribute to the global outcome , and that * b ) * the brain has mechanisms to specify exactly which cost functions it subjects its networks to , i.e. , that the cost functions are highly tunable , shaped by evolution and matched to the animal s ethological needs .",
    "thus , the brain uses cost functions as a key driving force of its development , much as modern machine learning systems do .    to understand the basis of these claims",
    ", we must now delve into the details of how the brain might efficiently perform credit assignment throughout large , multi - layered networks , in order to optimize complex functions .",
    "we argue that the brain uses several different types of optimization to solve distinct problems . in some structures",
    ", it may use genetic pre - specification of circuits for problems that require only limited learning based on data , or it may exploit local optimization to avoid the need to assign credit through many layers of neurons . it may also use a host of proposed circuit structures that would allow it to actually perform , in effect , backpropagation of errors through a multi - layer network , using biologically realistic mechanisms  a feat that had once been widely believed to be biologically implausible  @xcite .",
    "potential such mechanisms include circuits that literally backpropagate error derivatives in the manner of conventional backpropagation , as well as circuits that provide other efficient means of approximating the effects of backpropagation , i.e. , of rapidly computing the approximate gradient of a cost function relative to any given connection weight in the network .",
    "lastly , the brain may use algorithms that exploit specific aspects of neurophysiology  such as spike timing dependent plasticity , dendritic computation , local excitatory - inhibitory networks , or other properties  as well as the integrated nature of higher - level brain systems .",
    "such mechanisms promise to allow learning capabilities that go even beyond those of current backpropagation networks .",
    "not all learning requires a general - purpose optimization mechanism like gradient descent .",
    "many theories of cortex  @xcite emphasize potential self - organizing and unsupervised learning properties that may obviate the need for multi - layer backpropagation as such .",
    "hebbian plasticity , which adjusts weights according to correlations in pre - synaptic and post - synaptic activity , is well established .",
    "various versions of hebbian plasticity  @xcite can give rise to different forms of correlation and competition between neurons , leading to the self - organized formation of ocular dominance columns , self - organizing maps and orientation columns  @xcite .",
    "often these types of local self - organization can also be viewed as optimizing a cost function : for example , certain forms of hebbian plasticity can be viewed as extracting the principal components of the input , which minimizes a reconstruction error .    to generate complex temporal patterns , the brain may also implement other forms of learning that do not require any equivalent of full backpropagation through a multilayer network . for example ,",
    "`` liquid- ''  @xcite or `` echo - state machines ''  @xcite are randomly connected recurrent networks that form a basis set of random filters , which can be harnessed for learning with tunable readout weights .",
    "variants exhibiting chaotic , spontaneous dynamics can even be trained by feeding back readouts into the network and suppressing the chaotic activity  @xcite .",
    "learning only the readout layer makes the optimization problem much simpler ( indeed , equivalent to regression for supervised learning ) .",
    "additionally , echo state networks can be trained by reinforcement learning as well as supervised learning  @xcite .",
    "we argue that the above mechanisms of local self - organization are insufficient to account for the brain s powerful learning performance . to elaborate on the need for an efficient means of gradient computation in the brain , we will first place backpropagation into it s computational context  @xcite",
    ". then we will explain how the brain could plausibly implement approximations of gradient descent .",
    "the simplest mechanism to perform cost function optimization is sometimes known as the `` twiddle '' algorithm or , more technically , as `` serial perturbation '' .",
    "this mechanism works by perturbing ( i.e. , `` twiddling '' ) , with a small increment , a single weight in the network , and verifying improvement by measuring whether the cost function has decreased compared to the network s performance with the weight unperturbed .",
    "if improvement is noticeable , the perturbation is used as a direction of change to the weight ; otherwise , the weight is changed in the opposite direction ( or not changed at all ) .",
    "serial perturbation is therefore a method of `` coordinate descent '' on the cost , but it is slow and requires global coordination : each synapse in turn is perturbed while others remain fixed .",
    "weight perturbation ( or parallel perturbation ) perturbs all of the weights in the network at once .",
    "it is able to optimize small networks to perform tasks but generally suffers from high variance .",
    "that is , the measurement of the gradient direction is noisy and changes drastically from perturbation to perturbation because a weight s influence on the cost is masked by the changes of all other weights , and there is only one scalar feedback signal indicating the change in the cost .",
    "weight perturbation is dramatically inefficient for large networks .",
    "in fact , parallel and serial perturbation learn at approximately the same rate if the time measure counts the number of times the network propagates information from input to output  @xcite .",
    "some efficiency gain can be achieved by perturbing neural activities instead of synaptic weights , acknowledging the fact that any long - range effect of a synapse is mediated through a neuron . like weight perturbation and unlike serial perturbation , minimal global coordination is needed : each neuron only needs to receive a feedback signal indicating the global cost .",
    "the variance of node perturbation s gradient estimate is far smaller than that of weight perturbation under the assumptions that either all neurons or all weights , respectively , are perturbed and that they are perturbed at the same frequency . in this case ,",
    "node perturbation s variance is proportional to the number of cells in the network , not the number of synapses .",
    "all of these approaches are slow either due to the time needed for serial iteration over all weights or the time needed for averaging over low signal - to - noise ratio gradient estimates . to their credit however , none of these approaches requires more than knowledge of local activities and the single global cost signal .",
    "real neural circuits in the brain have mechanisms ( e.g. , diffusible neuromodulators ) that appear to code the signals relevant to implementing those algorithms . in many cases , for example in reinforcement learning , the cost function , which is computed based on interaction with an unknown environment , can not be differentiated directly , and an agent has no choice but to deploy clever twiddling to explore at some level of the system  @xcite .",
    "backpropagation , in contrast , works by computing the sensitivity of the cost function to each weight based on the layered structure of the system . the derivatives of the cost function with respect to the last layer can be used to compute the derivatives of the cost function with respect to the penultimate layer , and so on , all the way down to the earliest layers .",
    "backpropagation can be computed rapidly , and for a single input - output pattern , it exhibits no variance in its gradient estimate .",
    "the backpropagated gradient has no more noise for a large system than for a small system , so deep and wide architectures with great computational power can be trained efficiently .      to permit biological learning with efficiency approaching that of machine learning methods , some provision for more sophisticated gradient propagation may be suspected .",
    "contrary to what was once a common assumption , there are now many proposed `` biologically plausible '' mechanisms by which a neural circuit could implement optimization algorithms that , like backpropagation , can efficiently make use of the gradient .",
    "these include generalized recirculation  @xcite , contrastive hebbian learning  @xcite , random feedback weights together with synaptic homeostasis  @xcite , spike timing dependent plasticity ( stdp ) with iterative inference and target propagation  @xcite , complex neurons with backpropagating action - potentials  @xcite , and others  @xcite . while these mechanisms differ in detail , they all invoke feedback connections that carry error phasically .",
    "learning occurs by comparing a prediction with a target , and the prediction error is used to drive top - down changes in bottom - up activity .",
    "as an example , consider oreilly s temporally extended contrastive attractor learning ( xcal ) algorithm  @xcite .",
    "suppose we have a multilayer neural network with an input layer , an output layer , and a set of hidden layers in between .",
    "oreilly showed that the same functionality as backpropagation can be implemented by a bidirectional network with the same weights but symmetric connections .",
    "after computing the outputs using the forward connections only , we set the output neurons to the values they should have .",
    "the dynamics of the network then cause the hidden layers activities to evolve toward a stable attractor state linking input to output .",
    "the xcal algorithm performs a type of local modified hebbian learning at each synapse in the network during this process  @xcite .",
    "the xcal hebbian learning rule compares the local synaptic activity ( pre x post ) during the early phase of this settling ( before the attractor state is reached ) to the final phase ( once the attractor state has been reached ) , and adjusts the weights in a way that should make the early phase reflect the later phase more closely .",
    "these contrastive hebbian learning methods even work when the connection weights are not precisely symmetric  @xcite .",
    "xcal has been implemented in biologically plausible conductance - based neurons and basically implements the backpropagation of error approach .",
    "approximations to backpropagation could also be enabled by the millisecond - scale timing of of neural activities  @xcite .",
    "spike timing dependent plasticity ( stdp )  @xcite , for example , is a feature of some neurons in which the sign of the synaptic weight change depends on the precise millisecond - scale relative timing of pre - synaptic and post - synaptic spikes .",
    "this is conventionally interpreted as hebbian plasticity that measures the potential for a causal relationship between the pre - synaptic and post - synaptic spikes : a pre - synaptic spike could have contributed to causing a post - synaptic spike only if it occurs shortly beforehand . to enable a backpropagation mechanism ,",
    "hinton has suggested an alternative interpretation : that neurons could encode the types of error derivatives needed for backpropagation in the temporal derivatives of their firing rates  @xcite .",
    "stdp then corresponds to a learning rule that is sensitive to these error derivatives  @xcite . in other words , in an appropriate network context , stdp learning could give rise to a biological implementation of backpropagation .    another possible mechanism , by which biological neural networks could approximate backpropagation ,",
    "is `` feedback alignment ''  @xcite .",
    "there , the feedback pathway in backpropagation , by which error derivatives at a layer are computed from error derivatives at the subsequent layer , is replaced by a set of random feedback connections , with no dependence on the forward weights .",
    "subject to the existence of a synaptic normalization mechanism and approximate sign - concordance between the feedforward and feedback connections  @xcite , this mechanism of computing error derivatives works nearly as well as backpropagation on a variety of tasks . in effect , the forward weights are able to adapt to bring the network into a regime in which the random backwards weights actually carry the information that is useful for approximating the gradient .",
    "this is a remarkable and surprising finding , and is indicative of the fact that our understanding of gradient descent optimization , and specifically of the mechanisms by which backpropagation itself functions , are still incomplete . in neuroscience , meanwhile , we find feedback connections almost wherever we find feed - forward connections , and their role is the subject of diverse theories  @xcite .",
    "it should be noted that feedback alignment as such does not specify exactly how neurons represent and make use of the error signals ; it only relaxes a constraint on the transport of the error signals .",
    "thus , feedback alignment is more a primitive that can be used in fully biological ( approximate ) implementations of backpropagation , than a fully biological implementation in its own right . as such",
    ", it may be possible to incorporate it into several of the other schemes discussed here .",
    "the above `` biological '' implementations of backpropagation still lack some key aspects of biological realism .",
    "for example , in the brain , neurons tend to be either excitatory or inhibitory but not both , whereas in artificial neural networks a single neuron may send both excitatory and inhibitory signals to its downstream neurons . fortunately , this constraint is unlikely to limit the functions that can be learned  @xcite .",
    "other biological considerations , however , need to be looked at in more detail : the highly recurrent nature of biological neural networks , which show rich dynamics in time , and the fact that most neurons in mammalian brains communicate via spikes .",
    "we now consider these two issues in turn .",
    "[ [ temporal - credit - assignment ] ] temporal credit assignment : + + + + + + + + + + + + + + + + + + + + + + + + + + +    the biological implementations of backpropagation proposed above , while applicable to feedforward networks , do not give a natural implementation of `` backpropagation through time ''",
    "( bptt )  @xcite for recurrent networks , which is widely used in machine learning for training recurrent networks on sequential processing tasks .",
    "bptt `` unfolds '' a recurrent network across multiple discrete time steps and then runs backpropagation on the unfolded network to assign credit to particular units at particular time steps .",
    "while the network unfolding procedure of bptt itself does not seem biologically plausible , to our intuition , it is unclear to what extent temporal credit assignment is truly needed  @xcite for learning particular temporally extended tasks .",
    "if the system is given access to appropriate memory stores and representations  @xcite of temporal context , this could potentially mitigate the need for temporal credit assignment as such  in effect , memory systems could `` spatialize '' the problem of temporal credit assignment .",
    "for example , memory networks  @xcite store everything by default up to a certain buffer size , eliminating the need to perform credit assignment over the write - to - memory events , such that the network only needs to perform credit assignment over the read - from - memory events . in another example , certain network architectures that are superficially very deep , but which possess particular types of `` skip connections '' , can actually be seen as ensembles of comparatively shallow networks  @xcite ; applied in the time domain , this could limit the need to propagate errors far backwards in time . other , similar specializations or higher - levels of structure could , potentially , further ease the burden on credit assignment .",
    "can generic recurrent networks perform temporal credit assignment in in a way that is more biologically plausible than bptt ? indeed , new discoveries are being made about the capacity for supervised learning in continuous - time recurrent networks with more realistic synapses and neural integration properties . in internal force learning  @xcite , internally generated random fluctuations inside a chaotic recurrent network are adjusted to provide feedback signals that drive weight changes internal to the network while the outputs are clamped to desired patterns .",
    "this is made possible by a learning procedure that rapidly adjusts the network output to a state where it is close to the clamped values , and exerts continuous control to keep this difference small throughout the learning process .",
    "this procedure is able to control and exploit the chaotic dynamical patterns that are spontaneously generated by the network .",
    "werbos has proposed in his `` error critic '' that an online approximation to bptt can be achieved by learning to predict the backward - through - time gradient signal ( costate ) in a manner analogous to the prediction of value functions in reinforcement learning  @xcite .",
    "broadly , we are only beginning to understand how neural activity can itself represent the time variable  @xcite , and how recurrent networks can learn to generate trajectories of population activity over time  @xcite .",
    "moreover , as we discuss below , a number of cortical models also propose means , other than bptt , by which networks could be trained on sequential prediction tasks , even in an online fashion  @xcite .",
    "a broad range of ideas can be used to approximate bptt in more realistic ways .    [ [ spiking - networks ] ] spiking networks : + + + + + + + + + + + + + + + + +    it has been difficult to apply gradient descent learning directly to spiking neural networks .",
    "a number of optimization procedures have been used to generate , indirectly , spiking networks which can perform complex tasks , by performing optimization on a continuous representation of the network dynamics and embedding variables into high - dimensional spaces with many spiking neurons representing each variable  @xcite .",
    "the use of recurrent connections with multiple timescales can remove the need for backpropagation in the direct training of spiking recurrent networks  @xcite .",
    "fast connections maintain the network in a state where slow connections have local access to a global error signal .",
    "while the biological realism of these methods is still unknown , they all allow connection weights to be learned in spiking networks .",
    "these and other novel learning procedures illustrate the fact that we are only beginning to understand the connections between the temporal dynamics of biologically realistic networks , and mechanisms of temporal and spatial credit assignment . nevertheless , we argue here that existing evidence suggests that biologically plausible neural networks can solve these problems  in other words , it is possible to efficiently optimize complex functions of temporal history in the context of spiking networks of biologically realistic neurons . in any case , there is little doubt that spiking recurrent networks using realistic population coding schemes can , with an appropriate choice of connection weights , compute complicated , cognitively relevant functions .",
    "the question is how the developing brain efficiently learns such complex functions .",
    "the brain has mechanisms and structures that could support learning mechanisms different from typical gradient - based optimization algorithms .",
    "the complex physiology of individual biological neurons may not only help explain how some form of efficient gradient descent could be implemented within the brain , but also could provide mechanisms for learning that go beyond backpropagation .",
    "this suggests that the brain may have discovered mechanisms of credit assignment quite different from those dreamt up by machine learning .",
    "one such biological primitive is dendritic computation , which could impact prospects for learning algorithms in several ways .",
    "first , real neurons are highly nonlinear , with the dendrites of each _ single _ neuron implementing something computationally similar to a three - layer neural network  @xcite .",
    "second , when a neuron spikes , its action potential propagates back from the soma into the dendritic tree .",
    "however , it propagates more strongly into the branches of the dendritic tree that have been active  @xcite , potentially simplifying the problem of credit assignment  @xcite .",
    "third , neurons can have multiple somewhat independent dendritic compartments , as well as a somewhat independent somatic compartment , which means that the neuron should be thought of as storing more than one variable .",
    "thus , there is the possibility for a neuron to store both its activation itself , and the error derivative of a cost function with respect to its activation , as required in backpropagation , and biological implementations of backpropagation based on this principle have been proposed  @xcite .",
    "overall , the implications of dendritic computation for credit assignment in deep networks are only beginning to be considered .    beyond dendritic computation , diverse mechanisms  @xcite like retrograde ( post - synaptic to pre - synaptic ) signals using cannabinoids  @xcite , or rapidly - diffusing gases such as nitric oxide  @xcite , are among many that could enable learning rules that go beyond backpropagation .",
    "harris has suggested  @xcite how slow , retroaxonal ( i.e. , from the outgoing synapses back to the parent cell body ) transport of molecules like neurotrophins could allow neural networks to implement an analog of an exchangeable currency in economics , allowing networks to self - organize to efficiently provide information to downstream `` consumer '' neurons that are trained via faster and more direct error signals .",
    "the existence of these diverse mechanisms may call into question traditional , intuitive notions of `` biological plausibility '' for learning algorithms .",
    "another biological primitive is neuromodulation . the same neuron or circuit can exhibit different input - output responses and plasticity depending on a global circuit state , as reflected by the concentrations of various _ neuromodulators _ like dopamine , serotonin , norepinephrine , acetylcholine , and hundreds of different neuropeptides such as opiods  @xcite .",
    "these modulators interact in complex and cell - type - specific ways to influence circuit function .",
    "interactions with glial cells also play a role in neural signaling and neuromodulation , leading to the concept of `` tripartite '' synapses that include a glial contribution  @xcite .",
    "modulation could have many implications for learning .",
    "first , modulators can be used to gate synaptic plasticity on and off selectively in different areas and at different times , allowing precise , rapidly updated orchestration of where and when cost functions are applied .",
    "furthermore , it has been argued that a single neural circuit can be thought of as multiple overlapping circuits with modulation switching between them  @xcite . in a learning context , this could potentially allow sharing of synaptic weight information between overlapping circuits .",
    "@xcite discusses further computational aspects of neuromodulation .",
    "overall , neuromodulation seems to expand the range of possible algorithms that could be used for optimization .",
    "a number of models attempt to explain cortical learning on the basis of specific architectural features of the 6-layered cortical sheet .",
    "these models generally agree that a primary function of the cortex is some form of unsupervised learning via prediction  @xcite .",
    "some cortical learning models are explicit attempts to map cortical structure onto the framework of message - passing algorithms for bayesian inference  @xcite , while others start with particular aspects of cortical neurophysiology and seek to explain those in terms of a learning function .",
    "for example , the nonlinear and dynamical properties of cortical pyramidal neurons  the principal excitatory neuron type in cortex  are of particular interest here , especially because these neurons have multiple dendritic zones that are targeted by different kinds of projections , which may allow the pyramidal neuron to make comparisons of top - down and bottom - up inputs .    other aspects of the laminar cortical architecture could be crucial to how the brain implements learning .",
    "local inhibitory neurons targeting particular dendritic compartments of the l5 pyramidal could be used to exert precise control over when and how the relevant feedback signals and associative mechanisms are utilized .",
    "notably , local inhibitory networks could also give rise to competition  @xcite between different representations in the cortex , perhaps allowing one cortical column to suppress others nearby , or perhaps even to send more sophisticated messages to gate the state transitions of its neighbors  @xcite .",
    "moreover , recurrent connectivity with the thalamus , structured bursts of spiking , and cortical oscillations ( not to mention other mechanisms like neuromodulation ) could control the storage of information over time , to facilitate learning based on temporal prediction .",
    "these concepts begin to suggest preliminary , exploratory models for how the detailed anatomy and physiology of the cortex could be interpreted within a machine - learning framework that goes beyond backpropagation .",
    "but these are early days : we still lack detailed structural / molecular and functional maps of even a single local cortical microcircuit .",
    "human learning is often one - shot : it can take just a single exposure to a stimulus to never forget it , as well as to generalize from it to new examples .",
    "one way of allowing networks to have such properties is what is described by i - theory , in the context of learning invariant representations for object recognition  @xcite . instead of training via gradient descent ,",
    "image templates are stored in the weights of simple - complex cell networks while objects undergo transformations , similar to the use of stored templates in hmax  @xcite .",
    "the theories then aim to show that you can invariantly and discriminatively represent objects using a single sample , even of a new class  @xcite .",
    "additionally , the nervous system may have a way of replaying reality over and over , allowing to move an item from episodic memory into a long - term memory in the neural network  @xcite .",
    "this solution effectively uses many iterations of weight updating to fully learn a single item , even if one has only been exposed to it once .",
    "finally , higher - level systems in the brain may be able to implement bayesian learning of sequential programs , which is a powerful means of one - shot learning  @xcite .",
    "this type of cognition likely relies on an interaction between multiple brain areas such as the prefrontal cortex and basal ganglia .",
    "computer models , and neural network based models in particular  @xcite , have not yet reached fully human - like performance in this area , despite significant recent advances  @xcite .",
    "these potential substrates of one - shot learning rely on mechanisms other than simple gradient descent .",
    "it should be noted , though , that recent architectural advances , including specialized spatial attention and feedback mechanisms  @xcite , as well as specialized memory mechanism  @xcite , do allow some types of one - shot generalization to be driven by backpropagation - based learning .",
    "human learning is often active and deliberate .",
    "it seems likely that , in human learning , actions are chosen so as to generate interesting training examples , and sometimes also to test specific hypotheses .",
    "such ideas of active learning and `` child as scientist '' go back to piaget and have been elaborated more recently  @xcite .",
    "we want our learning to be based on maximally informative samples , and active querying of the environment ( or of internal subsystems ) provides a way route to this .    at some level of organization , of course , it would seem useful for a learning system to develop explicit representations of its uncertainty , since this can be used to guide the system to actively seek the information that would reduce its uncertainty most quickly . moreover , there are population coding mechanisms that could support explicit probabilistic computations  @xcite .",
    "yet it is unclear to what extent and at what levels the brain uses an explicitly probabilistic framework , or to what extent probabilistic computations are emergent from other learning processes  @xcite .",
    "standard gradient descent does not incorporate any such adaptive sampling mechanism , e.g. , it does not deliberately sample data so as to maximally reduce its uncertainty .",
    "interestingly , however , stochastic gradient descent can be used to generate a system that samples adaptively  @xcite . in other words ,",
    "a system can learn , by gradient descent , how to choose its own input data samples in order to learn most quickly from them by gradient descent .",
    "ideally , the learner learns to choose actions that will lead to the largest improvements in its prediction or data compression performance  @xcite . in  @xcite , this is done in the framework of reinforcement learning , and incorporates a mechanisms for the system to measure its own rate of learning . in other words , it is possible to reinforcement - learn a policy for selecting the most interesting inputs to drive learning .",
    "adaptive sampling methods are also known in reinforcement learning that can achieve optimal bayesian exploration of markov decision process environments  @xcite .",
    "these approaches achieve optimality in an arbitrary , abstract environment .",
    "but of course , evolution may also encode its implicit knowledge of the organism s natural environment , the behavioral goals of the organism , and the developmental stages and processes which occur inside the organism , as priors or heuristics which would further constrain the types of adaptive sampling that are optimal in practice .",
    "for example , simple heuristics like seeking certain perceptual signatures of novelty , or more complex heuristics like monitoring situations that other people seem to find interesting , might be good ways to bias sampling of the environment so as to learn more quickly .",
    "other such heuristics might be used to give internal brain systems the types of training data that will be most useful to those particular systems at any given developmental stage .",
    "we are only beginning to understand how active learning might be implemented in the brain .",
    "we speculate that multiple mechanisms , specialized to different brain systems and spatio - temporal scales , could be involved .",
    "the above examples suggest that at least some such mechanisms could be understood from the perspective of optimizing cost functions .",
    "we have described how the brain could implement learning mechanisms of comparable power to backpropagation .",
    "but in many cases , the system may be more limited by the available training signals than by the optimization process itself . in machine learning ,",
    "one distinguishes supervised learning , reinforcement learning and unsupervised learning , and the training data limitation manifests differently in each case .    both supervised and reinforcement learning require some form of teaching signal , but",
    "the nature of the teaching signal in supervised learning is different from that in reinforcement learning .",
    "in supervised learning , the trainer provides the entire vector of errors for the output layer and these are back - propagated to compute the gradient : a locally optimal direction in which to update all of the weights of a potentially multi - layer and/or recurrent network . in reinforcement learning , however , the trainer provides a scalar evaluation signal , but this is not sufficient to derive a low - variance gradient .",
    "hence , some form of trial and error twiddling must be used to discover how to increase the evaluation signal . consequently , reinforcement learning is generally much less efficient than supervised learning .",
    "reinforcement learning in shallow networks is simple to implement biologically . for reinforcement learning of a deep network to be biologically plausible , however , we need a more powerful learning mechanism , since we are learning based on a more limited evaluation signal than in the supervised case : we do not have the full target pattern to train towards .",
    "nevertheless , approximations of gradient descent can be achieved in this case , and there are cases in which the scalar evaluation signal of reinforcement learning can be used to efficiently update a multi - layer network by gradient descent .",
    "the `` attention - gated reinforcement learning '' ( agrel ) networks of  @xcite , and variants like kickback  @xcite , give a way to compute an approximation to the full gradient in a reinforcement learning context using a feedback - based attention mechanism for credit assignment within the multi - layer network . the feedback pathway , together with a diffusible reward signal , together gate plasticity . for networks with more than three layers ,",
    "this gives rise to a model based on columns containing parallel feedforward and feedback pathways .",
    "the process is still not as efficient as backpropagation , but it seems that this form of feedback can make reinforcement learning in multi - layer networks more efficient than a naive node perturbation or weight perturbation approach .    the machine - learning field has recently been tackling the question of credit assignment in deep reinforcement learning .",
    "deep q - learning  @xcite demonstrates reinforcement learning in a deep network , wherein most of the network is trained via backpropagation . in regular q learning",
    ", we define a function q , which estimates the best possible sum of future rewards ( the return ) if we are in a given state and take a given action . in deep q learning , this function is approximated by a neural network that , in effect , estimates action - dependent returns in a given state .",
    "the network is trained using backpropagation of local errors in q estimation , using the fact that the return decomposes into the current reward plus the discounted estimate of future return at the next moment . during training , as the agent acts in the environment , a series of loss functions is generated at each step , defining target patterns that can be used as the supervision signal for backpropagation . as q is a highly nonlinear function of the state ,",
    "tricks are sometimes needed to make deep q learning efficient and stable , including experience replay and a particular type of mini - batch training .",
    "it is also necessary to store the outputs from the previous iteration ( or clone the entire network ) in evaluating the loss function for the subsequent iteration .",
    "this process for generating learning targets provides a kind of bridge between reinforcement learning and efficient backpropagation - based gradient descent learning .",
    "importantly , only temporally local information is needed making the approach relatively compatible with what we know about the nervous system .",
    "even given these advances , a key remaining issue in reinforcement learning is the problem of long timescales , e.g. , learning the many small steps needed to navigate from london to chicago .",
    "many of the formal guarantees of reinforcement learning  @xcite , for example , suggest that the difference between an optimal policy and the learned policy becomes increasingly loose as the discount factor shifts to take into account reward at longer timescales .",
    "although the degree of optimality of human behavior is unknown , people routinely engage in adaptive behaviors that can take hours or longer to carry out , by using specialized processes like _ prospective memory _ to `` remember to remember '' relevant variables at the right times , permitting extremely long timescales of coherent action .",
    "machine learning has not yet developed methods to deal with such a wide range of timescales and scopes of hierarchical action .",
    "below we discuss ideas of hierarchical reinforcement learning that may make use of callable procedures and sub - routines , rather than operating explicitly in a time domain .",
    "as we will discuss below , some form of deep reinforcement learning may be used by the brain for purposes beyond optimizing global rewards , including the training of local networks based on diverse internally generated cost functions .",
    "scalar reinforcement - like signals are easy to compute , and easy to deliver to other areas , making them attractive mechanistically .",
    "if the brain does employ internally computed scalar reward - like signals as a basis for cost functions , it seems likely that it will have found an efficient means of reinforcement - based training of deep networks , but it is an open question whether an analog of deep q networks , agrel , or some other mechanism entirely , is used in the brain for this purpose .",
    "moreover , as we will discuss further below , it is possible that reinforcement - type learning is made more efficient in the context of specialized brain systems like short term memories , replay mechanisms , and hierarchically organized control systems .",
    "these specialized systems could reduce reliance on a need for powerful credit assignment mechanisms for reinforcement learning .",
    "finally , if the brain uses a diversity of scalar reward - like signals to implement different cost functions , then it may need to mediate delivery of those signals via a comparable diversity of molecular substrates .",
    "the great diversity of neuromodulatory signals , e.g. , neuropeptides , in the brain  @xcite makes such diversity quite plausible , and moreover , the brain may have found other , as yet unknown , mechanisms of diversifying reward - like signaling pathways and enabling them to act independently of one another .",
    "in the last section , we argued that the brain can optimize functions .",
    "this raises the question of what functions it optimizes . of course",
    ", in the brain , a cost function will itself be created ( explicitly or implicitly ) by a neural network shaped by the genome .",
    "thus , the cost function used to train a given sub - network in the brain is a key innate property that can be built into the system by evolution",
    ". it may be much cheaper in biological terms to specify a cost function that allows the rapid learning of the solution to a problem than to specify the solution itself .    in * hypothesis 2 *",
    ", we proposed that the brain optimizes not a single `` end - to - end '' cost function , but rather a diversity of internally generated cost functions specific to particular functions . to understand how and why the brain may use a diversity of cost functions , it is important to distinguish the differing types of cost functions that would be needed for supervised , unsupervised and reinforcement learning .",
    "we can also seek to identify types of cost functions that the brain may need to generate from a functional perspective , and how each may be implemented as supervised , unsupervised , reinforcement - based or hybrid systems .",
    "what additional circuitry is required to actually impose a cost function on an optimizing network ? in the most familiar case , supervised learning may rely on computing a vector of errors at the output of a network , which will rely on some comparator circuitry to compute the difference between the network outputs and the target values .",
    "this difference could then be backpropagated to earlier layers .",
    "an alternative way to impose a cost function is to `` clamp '' the output of the network , forcing it to occupy a desired target state .",
    "such clamping is actually assumed in some of the putative biological implementations of backpropagation described above , such as xcal and target propagation .",
    "alternatively , as described above , scalar reinforcement signals are attractive as internally - computed cost functions , but using them in deep networks requires special mechanisms for credit assignment .    in unsupervised learning , cost functions may not take the form of externally supplied training or error signals , but rather can be built into the dynamics inherent to the network itself , i.e. , there may be no need for a _ separate _ circuit to compute and impose a cost function on the network . indeed , beginning with hopfield s definition of an energy function for learning in certain classes of symmetric network  @xcite , researchers have discovered networks with inherent learning dynamics that implicitly optimizes certain objectives , such as statistical reconstruction of the input ( e.g. , via stochastic relaxation in boltzmann machines  @xcite ) , or the achievement of certain properties like temporally stable or sparse representations .",
    "alternatively , explicit cost functions could be computed , delivered to a network , and used for unsupervised learning , following a variety of principles being discovered in machine learning ( e.g. ,  @xcite ) , which typically find a way to encode the cost function into the error derivatives which are backpropagated .",
    "for example , prediction errors naturally give rise to error signals for unsupervised learning , as do reconstruction errors in autoencoders , and these error signals can also be augmented with additional penalty or regularization terms that enforce objectives like sparsity or continuity , as described below . in the next sections , we elaborate on these and other means of specifying and delivering cost functions in different learning contexts .",
    "there are many objectives that can be optimized in an unsupervised context , to accomplish different kinds of functions or guide a network to form particular kinds of representations .",
    "in one common form of unsupervised learning , higher brain areas attempt to produce samples that are statistically similar to those actually seen in lower layers .",
    "for example , the wake - sleep algorithm  @xcite requires the sleep mode to sample potential data points whose distribution should then match the observed distribution .",
    "unsupervised pre - training of deep networks is an instance of this  @xcite , typically making use of a stacked auto - encoder framework .",
    "similarly , in target propagation  @xcite , a top - down circuit , together with lateral information , has to produce data that directs the local learning of a bottom - up circuit and vice - versa .",
    "ladder autoencoders make use of lateral connections and local noise injection to introduce an unsupervised cost function , based on internal reconstructions , that can be readily combined with supervised cost functions defined on the network s top layer outputs  @xcite .",
    "compositional generative models generate a scene from discrete combinations of template parts and their transformations  @xcite , in effect performing a rendering of a scene based on its structural description .",
    "hinton and colleagues have also proposed cortical `` capsules ''  @xcite for compositional inverse rendering .",
    "the network can thus implement a statistical goal that embodies some understanding of the way that the world produces samples .",
    "learning rules for generative models have historically involved local message passing of a form quite different from backpropagation , e.g. , in a multi - stage process that first learns one layer at a time and then fine - tunes via the wake - sleep algorithm  @xcite .",
    "message - passing implementations of probabilistic inference have also been proposed as an explanation and generalization of deep convolutional networks  @xcite .",
    "various mappings of such processes onto neural circuitry have been attempted  @xcite .",
    "feedback connections tend to terminate in distinct layers of cortex relative to the feedforward ones  @xcite making the idea of separate but interacting networks for recognition and generation potentially attractive .",
    "interestingly , such sub - networks might even be part of the same neuron and map onto `` apical '' versus `` basal '' parts of the dendritic tree  @xcite .",
    "generative models can also be trained via backpropagation .",
    "recent advances have shown how to perform variational approximations to bayesian inference inside backpropagation - based neural networks  @xcite , and how to exploit this to create generative models  @xcite .",
    "through either explicitly statistical or gradient descent based learning , the brain can thus obtain a probabilistic model that simulates features of the world .",
    "a perceiving system should exploit statistical regularities in the world that are not present in an arbitrary dataset or input distribution .",
    "for example , objects are sparse : there are far fewer objects than there are potential places in the world , and of all possible objects there is only a small subset visible at any given time .",
    "as such , we know that the output of an object recognition system must have sparse activations . building the assumption of sparseness into simulated systems replicates a number of representational properties of the early visual system  @xcite , and indeed the original paper on sparse coding obtained sparsity by gradient descent optimization of a cost function  @xcite .",
    "a range of unsupervised machine learning techniques , such as the sparse autoencoders  @xcite used to discover cats in youtube videos , build sparseness into neural networks . building in such spatio - temporal sparseness priors should serve as an `` inductive bias ''  @xcite that can accelerate learning .",
    "but we know much more about the regularities of objects . as young babies , we already know  @xcite that objects tend to persist over time . the emergence or disappearance of an object from a region of space is a rare event .",
    "moreover , object locations and configurations tend to be coherent in time .",
    "we can formulate this prior knowledge as a cost function , for example by penalizing representations which are not temporally continuous .",
    "this idea of continuity is used in a great number of artificial neural networks and related models  @xcite .",
    "imposing continuity within certain models gives rise to aspects of the visual system including complex cells  @xcite , specific properties of visual invariance  @xcite , and even other representational properties such as the existence of place cells  @xcite .",
    "unsupervised learning mechanisms that maximize temporal coherence or slowness are increasingly used in machine learning .",
    "we also know that objects tend to undergo predictable sequences of transformations , and it is possible to build this assumption into unsupervised neural learning systems  @xcite .",
    "the minimization of prediction error explains a number of properties of the nervous system  @xcite , and biologically plausible theories are available for how cortex could learn using prediction errors by exploiting temporal differences  @xcite or top - down feedback  @xcite . in one implementation ,",
    "a system can simply predict the next input delivered to the system and can then use the difference between the actual next input and the predicted next input as a full vectorial error signal for supervised gradient descent .",
    "thus , rather than optimization of prediction error being implicitly implemented by the network dynamics , the prediction error is used as an explicit cost function in the manner of supervised learning , leading to error derivatives which can be back - propagated .",
    "then , no special learning rules beyond simple backpropagation are needed .",
    "this approach has recently been advanced within machine learning  @xcite .",
    "recently , combining such prediction - based learning with a specific gating mechanism has been shown to lead to unsupervised learning of disentangled representations  @xcite .",
    "neural networks can also be designed to learn to invert spatial transformations  @xcite . statistically describing transformations or sequences",
    "is thus an unsupervised way of learning representations .    furthermore , there are multiple modalities of input to the brain .",
    "each sensory modality is primarily connected to one part of the brain .",
    "but higher levels of cortex in each modality are heavily connected to the other modalities .",
    "this can enable forms of self - supervised learning : with a developing visual understanding of the world we can predict its sounds , and then test those predictions with the auditory input , and vice versa .",
    "the same is true about multiple parts of the same modality : if we understand the left half of the visual field , it tells us an awful lot about the right . maximizing mutual information is a natural way of improving learning  @xcite , and there are many other ways in which multiple modalities or processing streams could mutually train one another .",
    "relatedly , we can use observations of one part of a visual scene to predict the contents of other parts  @xcite , and optimize a cost function that reflects the discrepancy . this way , each modality effectively produces training signals for the others .      in what cases",
    "might the brain use supervised learning , given that it requires the system to `` already know '' the exact target pattern to train towards ? one possibility is that the brain can store records of states that led to good outcomes . for example , if a baby reaches for a target and misses , and then tries again and successfully hits the target , then the difference in the neural representations of these two tries reflects the direction in which the system should change .",
    "the brain could potentially use a comparator circuit  a non - trivial task since neural activations are always positive , although different neuron types can be excitatory vs. inhibitory  to directly compute this vectorial difference in the neural population codes and then apply this difference vector as an error signal .",
    "another possibility is that the brain uses supervised learning to implement a form of `` chunking '' , i.e. , a consolidation of something the brain already knows how to do : routines that are initially learned as multi - step , deliberative procedures could be compiled down to more rapid and automatic functions by using supervised learning to train a network to mimic the overall input - output behavior of the original multi - step process .",
    "such a process is assumed to occur in cognitive models like act - r  @xcite , and methods for compressing the knowledge in neural networks into smaller networks are also being developed  @xcite .",
    "thus supervised learning can be used to train a network to do in `` one step '' what would otherwise require long - range routing and sequential recruitment of multiple systems .",
    "certain generalized forms of reinforcement learning may be ubiquitous throughout the brain .",
    "such reinforcement signals may be repurposed to optimize diverse internal cost functions .",
    "these internal cost functions could be specified at least in part by genetics .",
    "some brain systems such as in the striatum appear to learn via some form of temporal difference reinforcement learning  @xcite .",
    "this is reinforcement learning based on a global value function  @xcite that predicts total future reward or utility for the agent .",
    "reward - driven signaling is not restricted to the striatum , and is present even in primary visual cortex  @xcite . remarkably , the reward signaling in primary visual cortex is mediated in part by glial cells  @xcite , rather than neurons , and involves the neurotransmitter acetylcholine  @xcite . on the other hand ,",
    "some studies have suggested that visual cortex learns the basics of invariant object recognition in the absence of reward  @xcite , perhaps using reinforcement only for more refined perceptual learning  @xcite .    but beyond these well - known global reward signals",
    ", we argue that the basic mechanisms of reinforcement learning may be widely re - purposed to train local networks using a variety of internally generated error signals .",
    "these internally generated signals may allow a learning system to go beyond what can be learned via standard unsupervised methods , effectively guiding or steering the system to learn specific features or computations  @xcite .",
    "special , internally - generated signals are needed specifically for learning problems where standard unsupervised methods  based purely on matching the statistics of the world , or on optimizing simple mathematical objectives like temporal continuity or sparsity  will fail to discover properties of the world which are statistically weak in an objective sense but nevertheless have special significance to the organism  @xcite .",
    "indigo bunting birds , for example , learn a template for the constellations of the night sky long before ever leaving the nest to engage in navigation - dependent tasks  @xcite . this memory template is directly used to determine the direction of flight during migratory periods , a process that is modulated hormonally so that winter and summer flights are reversed .",
    "learning is therefore a multi - phase process in which navigational cues are memorized prior to the acquisition of motor control .    in humans , we suspect that similar multi - stage bootstrapping processes are arranged to occur .",
    "humans have innate specializations for social learning .",
    "we need to be able to read their expressions as indicated with hands and faces .",
    "hands are important because they allow us to learn about the set of actions that can be produced by agents  @xcite .",
    "faces are important because they give us insight into what others are thinking .",
    "people have intentions and personalities that differ from one another , and their feelings are important .",
    "how could we hack together cost functions , built on simple genetically specifiable mechanisms , to make it easier for a learning system to discover such behaviorally relevant variables ?    some preliminary studies are beginning to suggest specific mechanisms and heuristics that humans may be using to bootstrap more sophisticated knowledge . in a groundbreaking study , @xcite asked how could we explain hands , to a system that does not already know about them , in a cheap way , without the need for labeled training examples ?",
    "hands are common in our visual space and have special roles in the scene : they move objects , collect objects , and caress babies .",
    "building these biases into an area specialized to detect hands could guide the right kind of learning , by providing a downstream learning system with many likely positive examples of hands on the basis of innately - stored , heuristic signatures about how hands tend to look or behave  @xcite .",
    "indeed , an internally supervised learning algorithm containing specialized , hard - coded biases to detect hands , on the basis of their typical motion properties , can be used to bootstrap the training of an image recognition module that learns to recognize hands based on their appearance .",
    "thus , a simple , hard - coded module bootstraps the training of a much more complex algorithm for visual recognition of hands .",
    "@xcite then further exploits a combination of hand and face detection to bootstrap a predictor for gaze direction , based on the heuristic that faces tend to be looking towards hands .",
    "of course , given a hand detector , it also becomes much easier to train a system for reaching , crawling , and so forth .",
    "efforts are underway in psychology to determine whether the heuristics discovered to be useful computationally are , in fact , being used by human children during learning  @xcite .",
    "ullman refers to such primitive , inbuilt detectors as innate `` proto - concepts ''  @xcite .",
    "their broader claim is that such pre - specification of mutual supervision signals can make learning the relevant features of the world far easier , by giving an otherwise unsupervised learner the right kinds of hints or heuristic biases at the right times . here",
    "we call these approximate , heuristic cost functions `` bootstrap cost functions '' .",
    "the purpose of the bootstrap cost functions is to reduce the amount of data required to learn a specific feature or task , but at the same time to avoid a need for fully unsupervised learning .",
    "could the neural circuitry for such a bootstrap hand - detector be pre - specified genetically ?",
    "the precedent from other organisms is strong : for example , it is famously known that the frog retina contains circuitry sufficient to implement a kind of `` bug detector ''  @xcite .",
    "ullman s hand detector , in fact , operates via a simple local optical flow calculation to detect `` mover '' events .",
    "this type of simple , local calculation could potentially be implemented in genetically - specified and/or spontaneously self - organized neural circuitry in the retina or early dorsal visual areas  @xcite , perhaps similarly to the frog s `` bug detector '' .",
    "how could we explain faces without any training data ?",
    "faces tend to have two dark dots in their upper half , a line in the lower half and tend to be symmetric about a vertical axis .",
    "indeed , we know that babies are very much attracted to things with these generic features of upright faces starting from birth , and that they will acquire face - specific cortical areas in their first few years of life if not earlier  @xcite .",
    "it is easy to define a local rule that produces a kind of crude face detector ( e.g. , detecting two dots on top of a horizontal line ) , and indeed some evidence suggests that the brain can rapidly detect faces without even a single feed - forward pass through the ventral visual stream  @xcite .",
    "the crude detection of human faces used together with statistical learning should be analogous to semi - supervised learning  @xcite and could allow identifying faces with high certainty .",
    "humans have areas devoted to emotional processing , and the brain seems to embody prior knowledge about the structure of emotions : emotions should have specific types of strong couplings to various other higher - level variables , should be expressed through the face , and so on .",
    "this prior knowledge , encoded into brain structure via evolution , could allow learning signals to come from the right places and to appear developmentally at the right times .",
    "what about agency ?",
    "it makes sense to describe , when dealing with high - level thinking , other beings as optimizers of their own goal functions .",
    "it appears that heuristically specified notions of goals and agency are infused into human psychological development from early infancy and that notions of agency are used to bootstrap heuristics for ethical evaluation  @xcite .",
    "algorithms for establishing more complex , innately - important social relationships such as joint attention are under study  @xcite , building upon more primitive proto - concepts like face detectors and ullman s hand detectors  @xcite",
    ". the brain can thus use innate detectors to create cost functions and training procedures to train the next stages of learning .",
    "it is intuitive to ask whether this type of bootstrapping poses a kind of `` chicken and egg '' problem : if the brain already has an inbuilt heuristic hand detector , how can it be used to train a detector that performs any better than those heuristics ?",
    "after all , is nt a trained system only as good as its training data ?",
    "the work of @xcite illustrates why this is not the case .",
    "first , the `` innate detector '' can be used to train a downstream detector that operates based on different cues : for example , based on the spatial and body context of the hand , rather than its motion .",
    "second , once multiple such pathways of detection come into existence , they can be used to improve each other . in @xcite , appearance , body context , and mover motion are all used to bootstrap off of one another , creating a detector that is better than any of it training heuristics . in effect , the innate detectors are used not as supervision signals per se , but rather to guide or steer the learning process , enabling it to discover features that would otherwise be difficult . if such affordances can be found in other domains , it seems likely that the brain would make extensive use of them to ensure that developing animals learn the precise patterns of perception and behavior needed to ensure their later survival and reproduction .",
    "thus , generalizing previous ideas @xcite , we suggest that the brain uses optimization with respect to internally generated heuristic detection signals to bootstrap learning of biologically relevant features which would otherwise be missed by an unsupervised learner .",
    "in one possible implementation , such bootstrapping may occur via reinforcement learning , using the outputs of the innate detectors as local reinforcement signals , and perhaps using mechanisms similar to  @xcite to perform reinforcement learning through a multi - layer network .",
    "it is also possible that the brain could use such internally generated heuristic detectors in other ways , for example to bias the inputs delivered to an unsupervised learning network towards entities of interest to humans ( joscha bach , personal communication ) , or to directly train simple classifiers  @xcite .",
    "it has been widely noticed in cognitive science and ai that the generation and understanding of stories are crucial to human cognition .",
    "researchers such as winston have framed story understanding as the key to human - like intelligence  @xcite .",
    "stories consist of a linear sequence of episodes , in which one episode refers to another through cause and effect relationships , with these relationships often involving the implicit goals of agents .",
    "many other cognitive faculties , such as conceptual grounding of language , could conceivably emerge from an underlying internal representation in terms of stories .",
    "perhaps the ultimate series of bootstrap cost functions would be those which would direct the brain to utilize its learning networks and specialized systems so as to construct representations that are specifically useful as components of stories , to spontaneously chain these representations together , and to update them through experience and communication .",
    "how could such cost functions arise ?",
    "one possibility is that they are bootstrapped through imitation and communication , where a child learns to mimic the story - telling behavior of others .",
    "another possibility is that useful representations and primitives for stories emerge spontaneously from mechanisms for learning state and action chunking in hierarchical reinforcement learning and planning .",
    "yet another is that stories emerge from learned patterns of saliency - directed memory storage and recall ( e.g. ,  @xcite ) .",
    "in addition , priors that direct the developing child s brain to learn about and attend to social agency seem to be important for stories .",
    "these systems will be discussed in more detail below .",
    "optimization of initially unstructured `` blank slate '' networks is not sufficient to generate complex cognition in the brain , we argue , even given a diversity of powerful genetically - specified cost functions and local learning rules , as we have posited above . instead , in * hypothesis 3 *",
    ", we suggest that specialized , pre - structured architectures are needed for at least two purposes .",
    "first , pre - structured architectures are needed to allow the brain to find efficient solutions to certain types of problems .",
    "when we write computer code , there are a broad range of algorithms and data structures employed for different purposes : we may use dynamic programming to solve planning problems , trees to efficiently implement nearest neighbor search , or stacks to implement recursion .",
    "having the right kind of algorithm and data structure in place to solve a problem allows it to be solved efficiently , robustly and with a minimum amount of learning or optimization needed .",
    "this observation is concordant with the increasing use of pre - specialized architectures and specialized computational components in machine learning  @xcite . in particular , to enable the learning of efficient computational solutions , the brain may need pre - specialized systems for planning and executing sequential multi - step processes , for accessing memories , and for forming and manipulating compositional and recursive structures .",
    "second , the training of optimization modules may need to be coordinated in a complex and dynamic fashion , including delivering the right training signals and activating the right learning rules in the right places and at the right times . to allow this",
    ", the brain may need specialized systems for storing and routing data , and for flexibly routing training signals such as target patterns , training data , reinforcement signals , attention signals , and modulatory signals .",
    "these mechanisms may need to be at least partially in place in advance of learning .",
    "looking at the brain , we indeed seem to find highly conserved structures , e.g. , cortex , where it is theorized that a similar type of learning and/or computation is happening in multiple places  @xcite .",
    "but we also see a large number of specialized structures , including thalamus , hippocampus , basal ganglia and cerebellum  @xcite .",
    "some of these structures evolutionarily pre - date  @xcite the cortex , and hence the cortex may have evolved to work in the context of such specialized mechanisms .",
    "for example , the cortex may have evolved as a trainable module for which the training is orchestrated by these older structures .    even within the cortex itself , microcircuitry within different areas",
    "may be specialized : tinkered variations on a common ancestral microcircuit scaffold could potentially allow different cortical areas , such as sensory areas vs. prefrontal areas , to be configured to adopt a number of qualitatively distinct computational and learning configurations  @xcite , even while sharing a common gross physical layout and communication interface .",
    "within cortex , over forty distinct cell types  differing in such aspects as dendritic organization , distribution throughout the six cortical layers , connectivity pattern , gene expression , and electrophysiological properties  have already been found  @xcite .",
    "central pattern generator circuits provide an example of the kinds of architectures that can be pre - wired into neural microcircuitry , and may have evolutionary relationships with cortical circuits  @xcite .",
    "thus , while the precise degree of architectural specificity of particular cortical regions is still under debate  @xcite , various mechanism could offer pre - specified heterogeneity .    in this section",
    ", we explore the kinds of computational problems for which specialized structures may be useful , and attempt to map these to putative elements within the brain .",
    "our preliminary sketch of a functional decomposition can be viewed as a summary of suggestions for specialized functions that have been made throughout the computational neuroscience literature , and is influenced strongly by the models of oreilly , eliasmith , grossberg , marcus , hayworth and others  @xcite .",
    "the correspondence between these models and actual neural circuitry is , of course , still the subject of extensive debate .",
    "many of the computational and neural concepts sketched here are preliminary and will need to be made more rigorous through future study .",
    "our knowledge of the functions of particular brain areas , and thus our proposed mappings of certain computations onto neuroanatomy , also remains tentative .",
    "finally , it is still far from established which processes in the brain emerge from optimization of cost functions , which emerge from other forms of self - organization , which are pre - structured through genetics and development , and which rely on an interplay of all these mechanisms .",
    "our discussion here should therefore be viewed as a sketch of potential directions for further study .",
    "one of the central elements of computation is memory .",
    "importantly , multiple different kinds of memory are needed  @xcite . for example",
    ", we need memory that is stored for a long period of time and that can be retrieved in a number of ways , such as in situations similar to the time when the memory was first stored ( content addressable memory ) .",
    "we also need memory that we can keep for a short period of time and that we can rapidly rewrite ( working memory ) .",
    "lastly , we need the kind of implicit memory that we can not explicitly recall , similar to the kind of memory that is classically learned using gradient descent on errors , i.e. , sculpted into the weight matrix of a neural network .",
    "content addressable memories are classic models in neuroscience  @xcite .",
    "most simply , they allow us to recognize a situation similar to one that we have seen before , and to `` fill in '' stored patterns based on partial or noisy information , but they may also be put to use as sub - components of many other functions .",
    "recent research has shown that including such memories allows deep networks to learn to solve problems that previously were out of reach , even of lstm networks that already have a simpler form of local memory and are already capable of learning long - term dependencies  @xcite .",
    "hippocampal area ca3 may act as an auto - associative memory capable of content - addressable pattern completion , with pattern separation occurring in the dentate gyrus  @xcite .",
    "such systems could permit the retrieval of complete memories from partial cues , enabling networks to perform operations similar to database retrieval or to instantiate lookup tables of historical stimulus - response mappings , among numerous other possibilities .",
    "cognitive science has long characterized properties of the working memory .",
    "it is somewhat limited , with the old idea being that it can represent `` seven plus or minus two '' elements  @xcite .",
    "there are many models of working memory  @xcite , some of which attribute it to persistent , self - reinforcing patterns of neural activation  @xcite in the recurrent networks of the prefrontal cortex .",
    "prefrontal working memory appears to be made up of multiple functionally distinct subsystems  @xcite .",
    "neural models of working memory can store not only scalar variables  @xcite , but also high - dimensional vectors  @xcite or sequences of vectors  @xcite . working",
    "memory buffers seem crucial for human - like cognition , e.g. , reasoning , as they allow short - term storage while also  in conjunction with other mechanisms  enabling generalization of operations across anything that can fill the buffer .",
    "saliency , or interestingness , measures can be used to tag the importance of a memory  @xcite .",
    "this can allow removal of the boring data from the training set , allowing a mechanism that is more like optimal experimentation . moreover",
    ", saliency can guide memory replay or sampling from generative models , to generate more training data drawn from a distribution useful for learning  @xcite .",
    "conceivably , hippocampal replay could allow a batch - like training process , similar to how most machine learning systems are trained , rather than requiring all training to occur in an online fashion .",
    "plasticity mechanisms in memory systems which are gated by saliency are starting to be uncovered in neuroscience  @xcite .",
    "importantly , the notions of `` saliency '' computed by the brain could be quite intricate and multi - faceted , potentially leading to complex schemes by which specific kinds of memories would be tagged for later context - dependent retrieval . as a hypothetical example",
    ", representations of both timing and importance associated with memories could perhaps allow retrieval only of important memories that happened within a certain window of time  @xcite .",
    "storing and retrieving information selectively based on specific properties of the information itself , or of `` tags '' appended to that information , is a powerful computational primitive that could enable learning of more complex tasks .",
    "to use its information flexibly , the brain needs structured systems for routing data .",
    "such systems need to address multiple temporal and spatial scales , and multiple modalities of control .",
    "thus , there are several different kinds of information routing systems in the brain which operate by different mechanisms and under different constraints .",
    "if we can focus on one thing at a time , we may be able to allocate more computational resources to processing it , make better use of scarce data to learn about it , and more easily store and retrieve it from memory .",
    "notably in this context , attention allows improvements in learning : if we can focus on just a single object , instead of an entire scene , we can learn about it more easily using limited data .",
    "formal accounts in a bayesian framework talk about attention reducing the sample complexity of learning  @xcite .",
    "likewise , in models , the processes of applying attention , and of effectively making use of incoming attentional signals to appropriately modulate local circuit activity , can themselves be learned by optimizing cost functions  @xcite .",
    "the right kinds of attention make processing and learning more efficient , and also allow for a kind of programmatic control over multi - step perceptual tasks .",
    "how does the brain determine where to allocate attention , and how is the attentional signal physically mediated ? answering this question is still an active area of neuroscience .",
    "higher - level cortical areas may be specialized in allocating attention .",
    "the problem is made complex by the fact that there seem to be many different types of attention  such as object - based , feature - based and spatial attention in vision  that may be mediated by interactions between different brain areas .",
    "the frontal eye fields ( area fef ) , for example , are important in visual attention , specifically for controlling saccades of the eyes to attended locations .",
    "area fef contains `` retinotopic '' spatial maps whose activation determines the saccade targets in the visual field .",
    "other prefrontral areas such as the dorsolateral prefrontal cortex and inferior frontal junction are also involved in maintaining representations that specify the targets of certain types of attention .",
    "certain forms of attention may require a complex interaction between brain areas , e.g. , to determine targets of attention based on higher - level properties that are represented across multiple areas , like the identity and spatial location of a specific face  @xcite .",
    "there are many proposed neural mechanisms of attention , including the idea that synchrony plays a role  @xcite , perhaps by creating resonances that facilitate the transfer of information between synchronously oscillating neural populations in different areas .",
    "other proposed mechanisms include specific circuits for attention - dependent signal routing  @xcite .",
    "various forms of attention also have specific neurophysiological signatures , such as enhancements in synchrony among neural spikes and with the ambient local field potential , changes in the sharpness of neural tuning curves , and other properties .",
    "these diverse effects and signatures of attention may be consequences of underlying pathways that wire up to particular elements of cortical microcircuits to mediate different attentional effects .",
    "one possibility is that the brain uses distinct groups of neurons , which we can call `` buffers '' , to store distinct variables , such as the subject or object in a sentence  @xcite .",
    "having memory buffers allows the abstraction of a variable .",
    "as is ubiquitously used in computer science , this comes with the ability to generalize operations across any variable that could meaningfully fill the buffer and makes computation flexible .",
    "once we establish that the brain has a number of memory buffers , we need ways for those buffers to interact .",
    "we need to be able to take a buffer , do a computation on its contents and store the output into another buffer .",
    "but if the representations in each of two groups of neurons are learned , and hence are coded differently , how can the brain `` copy and paste '' information between these groups of neurons ?",
    "malsburg argued that such a system of separate buffers is impossible because the neural pattern for `` chair '' in buffer 1 has nothing in common with the neural pattern for `` chair '' in buffer 2  any learning that occurs for the contents of buffer 1 would not automatically be transferable to buffer 2 .",
    "various mechanisms have been proposed to allow such transferability , which focus on ways in which all buffers could be trained jointly and then later separated so that they can work independently when they need to .",
    "dense connectivity is only achieved locally , but it would be desirable to have a way for any two cortical units to talk to one another , if needed , regardless of their distance from one another , and without introducing crosstalk .",
    "it is therefore critical to be able to dynamically turn on and off the transfer of information between different source and destination regions , in much the manner of a switchboard . together with attention",
    ", such dedicated routing systems can make sure that a brain area receives exactly the information it needs . such a discrete routing system is , of course , central to cognitive architectures like act - r  @xcite .",
    "the key feature of act - r is the ability to evaluate the if clauses of tens of thousands of symbolic rules ( called `` productions '' ) , in parallel , approximately every 50 milliseconds .",
    "each rule requires equality comparisons between the contents of many constant and variable memory buffers , and the execution of a rule leads to the conditional routing of information from one buffer to another .    what controls which long - range routing operations occur when , i.e. , where is the switchboad and what controls it ?",
    "several models , including act - r , have attributed such parallel rule - based control of routing to the action selection circuitry  @xcite of the basal ganglia ( bg )  @xcite , and its interaction with working memory buffers in the prefrontal cortex . in conventional models of thalamo - cortico - striatal loops ,",
    "competing actions of the direct and indirect pathways through the basal ganglia can inhibit or disinhibit an area of motor cortex , thereby gating a motor action .",
    "models like  @xcite propose further that the basal ganglia can gate not just the transfer of information from motor cortex to downstream actuators , but also the transfer of information between cortical areas .",
    "to do so , the basal ganglia would dis - inhibit a thalamic relay  @xcite linking two cortical areas .",
    "dopamine - related activity is thought to lead to temporal difference reinforcement learning of such gating policies in the basal ganglia  @xcite . beyond the basal ganglia , there are also other , separate pathways involved in action selection , e.g. , in the prefrontal cortex  @xcite . thus , multiple systems including basal ganglia and cortex could control the gating of long - range information transfer between cortical areas , with the thalamus perhaps largely constituting the switchboard itself .    how is such routing put to use in a learning context ?",
    "one possibility is that the basal ganglia acts to orchestrate the training of the cortex .",
    "the basal ganglia may exert tight control over the cortex , helping to determine when and how it is trained .",
    "indeed , because the basal ganglia pre - dates the cortex evolutionarily , it is possible that the cortex evolved as a flexible , trainable resource that could be harnessed by existing basal ganglia circuitry .",
    "all of the main regions and circuits of the basal ganglia are conserved from our common ancestor with the lamprey more than five hundred million years ago .",
    "the major part of the basal ganglia even seems to be conserved from our common ancestor with insects  @xcite .",
    "thus , in addition to its real - time action selection and routing functions , the basal ganglia may sculpt how the cortex learns .",
    "certain algorithmic problems benefit greatly from particular types of representation and transformation , such as a grid - like representation of space . in some cases , rather than just waiting for them to emerge via gradient descent optimization of appropriate cost functions , the brain may be pre - structured to facilitate their creation .",
    "we often have to plan and execute complicated sequences of actions on the fly , in response to a new situation . at the lowest level , that of motor control , our body and our immediate environment change all the time .",
    "as such , it is important for us to maintain knowledge about this environment in a continuous way . the deviations between our planned movements and those movements that we actually execute continuously provide information about the properties of the environment .",
    "therefore it seems important to have a specialized system that takes all our motor errors and uses them to update a dynamical model of our body and our immediate environment that can predict the delayed sensory results of our motor actions  @xcite .",
    "it appears that the cerebellum is such a structure , and lesions to it abolish our way of dealing successfully with a changing body .",
    "incidentally , the cerebellum has more connections than the rest of the brain taken together , apparently in a largely feedforward architecture , and the tiny cerebellar granule cells , which may form a randomized high - dimensional input representation  @xcite , outnumber all other neurons .",
    "the brain clearly needs a way of continuously correcting movements to minimize errors .",
    "newer research shows that the cerebellum is involved in a broad range of cognitive problems  @xcite as well , potentially because they share computational problems with motor control .",
    "for example , when subjects estimate time intervals , which are naturally important for movement , it appears that the brain uses the cerebellum even if no movements are involved  @xcite . even individual cerebellar purkinjie cells may learn to generate precise timings of their outputs  @xcite .",
    "the brain also appears to use inverse models to rapidly predict motor activity that would give rise to a given sensory target  @xcite .",
    "such mechanisms could be put to use far beyond motor control , in bootstrapping the training of a larger architecture by exploiting continuously changing error signals to update a real - time model of the system state .",
    "importantly , many of the control problems we appear to be solving are hierarchical .",
    "we have a spinal cord , which deals with the fast signals coming from our muscles and proprioception . within neuroscience",
    ", it is generally assumed that this system deals with fast feedback loops and that this behavior is learned to optimize its own cost function .",
    "the nature of cost functions in motor control is still under debate . in particular ,",
    "the timescale over which cost functions operate remains unclear : motor optimization may occur via real - time responses to a cost function that is computed and optimized online , or via policy choices that change over time more slowly in response to the cost function  @xcite .",
    "nevertheless , the effect is that central processing in the brain has an effectively simplified physical system to control , e.g. , one that is far more linear .",
    "so the spinal cord itself already suggests the existence of two levels of a hierarchy , each trained using different cost functions .",
    "however , within the computational motor control literature ( see e.g. , @xcite ) , this idea can be pushed far further , e.g. , with a hierarchy including spinal cord , m1 , pmd , frontal , prefrontal areas .",
    "a low level may deal with muscles , the next level may deal with getting our limbs to places or moving objects , a next layer may deal with solving simple local problems ( e.g. , navigating across a room ) while the highest levels may deal with us planning our path through life .",
    "this factorization of the problem comes with multiple aspects : first , each level can be solved with its own cost functions , and second , every layer has a characteristic timescale .",
    "some levels , e.g. , the spinal cord , must run at a high speed .",
    "other levels , e.g. , high - level planning , only need to be touched much more rarely . converting the computationally hard optimal control problem into a hierarchical approximation promises to make it dramatically easier .",
    "does the brain solve control problems hierarchically ?",
    "there is evidence that the brain uses such a strategy  @xcite , beside neural network demonstrations  @xcite .",
    "the brain may use specialized structures at each hierarchical level to ensure that each operates efficiently given the nature of its problem space and available training signals . at higher levels",
    ", these systems may use an abstract syntax for combining sequences of actions in pursuit of goals  @xcite .",
    "subroutines in such processes could be derived by a process of chunking sequences of actions into single actions  @xcite .",
    "some brain areas like broca s area , known for its involvement in language , also appear to be specifically involved in processing the hierarchical structure of behavior , as such , as opposed to its detailed temporal structure  @xcite .    at the highest level of the decision making and control hierarchy ,",
    "human reward systems reflect changing goals and subgoals , and we are only beginning to understand how goals are actually coded in the brain , how we switch between goals , and how the cost functions used in learning depend on goal state  @xcite .",
    "goal hierarchies are beginning to be incorporated into deep learning  @xcite .    given this hierarchical structure , the optimization algorithms can be fine - tuned . for the low levels , there is sheer unlimited training data . for the high levels ,",
    "a simulation of the world may be simple , with a tractable number of high - level actions to choose from .",
    "finally , each area needs to give reinforcement to other areas , e.g. , high levels need to punish lower levels for making planning complicated .",
    "thus this type of architecture can simplify the learning of control problems .",
    "progress is being made in both neuroscience and machine learning on finding potential mechanisms for this type of hierarchical planning and goal - seeking .",
    "this is beginning to reveal mechanisms for chunking goals and actions and for searching and pruning decision trees  @xcite .",
    "the study of model - based hierarchical reinforcement learning and prospective optimization  @xcite , which concerns the planning and evaluation of nested sequences of actions , implicates a network coupling the dorsolateral prefontral and orbitofrontal cortex , and the ventral and dorsolateral striatum  @xcite .",
    "hierarchical rl relies on a hierarchical representation of state and action spaces , and it has been suggested that error - driven learning of an optimal such representation in the hippocampus gives rise to place and grid cell properties  @xcite , with goal representations themselves emerging in the amygdala , prefrontal cortex and other areas  @xcite .    the question of how control problems can be successfully divided into component problems remains one of the central questions in neuroscience and machine learning , and the cost functions involved in learning to create such decompositions are still unknown .",
    "these considerations may begin to make plausible , however , how the brain could not only achieve its remarkable feats of motor learning  such as generating complex `` innate '' motor programs , like walking in the newborn gazelle almost immediately after birth  but also the kind of planning that allows a human to prepare a meal or travel from london to chicago .",
    "spatial planning requires solving shortest - path problems subject to constraints .",
    "if we want to get from one location to another , there are an arbitrarily large number of simple paths that could be taken .",
    "most naive implementations of such shortest paths problems are grossly inefficient .",
    "it appears that , in animals , the hippocampus aids  at least in part through place cell and grid cell systems  in efficient learning about new environments and in targeted navigation in such environments  @xcite . in some simple models , targeted navigation in the hippocampus",
    "is achieved via the dynamics of `` bump attractors '' or propagating waves in a place cell network with hebbian plasticity and adaptation  @xcite , which allows the network to effectively chart out a path in the space of place cell representations .",
    "higher - level cognitive tasks such as prospective planning appear to share computational sub - problems with path - finding  @xcite .",
    "interaction between hippocampus and prefrontal cortex could perhaps support a more abstract notion of `` navigation '' in a space of goals and sub - goals . having specialized structures for",
    "path - finding simplifies these problems .",
    "language and reasoning appear to present a problem for neural networks  @xcite : we seem to be able to apply common grammatical rules to sentences regardless of the content of those sentences , and regardless of whether we have ever seen even remotely similar sentences in the training data . while this is achieved automatically in a computer with fixed registers , location addressable memories , and hard - coded operations , how it could be achieved in a biological brain , or emerge from an optimization algorithm , has been under debate for decades .    as the putative key capability underlying such operations , variable",
    "binding has been defined as `` the transitory or permanent tying together of two bits of information : a variable ( such as an x or y in algebra , or a placeholder like subject or verb in a sentence ) and an arbitrary instantiation of that variable ( say , a single number , symbol , vector , or word ) ''  @xcite .",
    "a number of potential biologically plausible binding mechanisms  @xcite are reviewed in  @xcite .",
    "some , such as vector symbolic architectures , which were proposed in cognitive science  @xcite , are also being considered in the context of efficiently - trainable artificial neural networks  @xcite  in effect , these systems learn how to use variable binding .",
    "variable binding could potentially emerge from simpler memory systems .",
    "for example , the scrub - jay can remember the place and time of last visit for hundreds of different locations , e.g. , to determine whether high - quality food is currently buried at any given location  @xcite .",
    "it is conceivable that such spatially - grounded memory systems enabled a more general binding mechanism to emerge during evolution , perhaps through integration with routing systems or other content - addressable or working memory systems .",
    "fixed , static hierarchies ( e.g. , the hierarchical organization of cortical areas  @xcite ) only take us so far : to deal with long chains of arbitrary nested references , we need _ dynamic _ hierarchies that can implement recursion on the fly .",
    "human language syntax has a hierarchical structure , which berwick et al described as `` composition of smaller forms like words and phrases into larger ones ''  @xcite .",
    "specific fronto - temporal networks may be involved in representing and generating such hierarchies  @xcite .",
    "little is known about the underlying circuit mechanisms for such dynamic hierarchies , but it is clear that specific affordances for representing such hierarchies in an efficient way would be beneficial .",
    "this may be closely connected with the issue of variable binding , and it is possible that operations similar to pointers could be useful in this context , in both the brain and artificial neural networks  @xcite . augmenting neural networks with a differentiable analog of a push - down stack is another such affordance being pursued in machine learning  @xcite .",
    "humans excel at stitching together sub - actions to form larger actions  @xcite .",
    "structured , serial , hierarchical probabilistic programs have recently been shown to model aspects of human conceptual representation and compositional learning  @xcite .",
    "in particular , sequential programs were found to enable one - shot learning of new geometric / visual concepts  @xcite , a key capability that deep learning networks for object recognition seem to fundamentally lack .",
    "generative programs have also been proposed in the context of scene understanding  @xcite .",
    "the ability to deal with problems in terms of sub - problems is central both in human thought and in many successful algorithms .",
    "one possibility is that the hippocampus supports the construction and learning of sequential programs .",
    "the hippocampus appears to explore , in simulation , possible future trajectories to a goal , even those involving previously unvisited locations  @xcite .",
    "hippocampal - prefrontal interaction has been suggested to allow rapid , subconscious evaluation of potential action sequences during decision - making , with the hippocampus in effect simulating the expected outcomes of potential actions that are generated and evaluated in the prefrontal  @xcite .",
    "the role of the hippocampus in imagination , concept generation  @xcite , scene construction  @xcite , mental exploration and goal - directed path planning  @xcite suggests that it could help to create generative models to underpin more complex inference such as program induction  @xcite or common - sense world simulation  @xcite .",
    "another related possibility is that the cortex itself intrinsically supports the construction and learning of sequential programs  @xcite .",
    "recurrent neural networks have been used for image generation through a sequential , attention - based process  @xcite , although their correspondence with the brain is unclear .",
    "importantly , there are many other specialized structures known in neuroscience , which arguably receive less attention than they deserve , even for those interested in higher cognition . in the above , in addition to the hippocampus , basal ganglia and cortex",
    ", we emphasized the key roles of the thalamus in routing , of the cerebellum as a rapidly trainable control and modeling system , of the amygdala and other areas as a potential source of utility functions , of the retina or early visual areas as a means to generate detectors for motion and other features to bootstrap more complex visual learning , and of the frontal eye fields and other areas as a possible source of attention control .",
    "we ignored other structures entirely , whose functions are only beginning to be uncovered , such as the claustrum  @xcite , which has been speculated to be important for rapidly binding together information from many modalities .",
    "our overall understanding of the functional decomposition of brain circuitry still seems very preliminary .",
    "a recent analysis  @xcite suggested directions by which to modify and enhance existing neural - net - based machine learning towards more powerful and human - like cognitive capabilities , particularly by introducing new structures and systems which go beyond data - driven optimization .",
    "this analysis emphasized that systems should construct generative models of the world that incorporate compositionality ( discrete construction from re - usable parts ) , inductive biases reflecting causality , intuitive physics and intuitive psychology , and the capacity for probabilistic inference over discrete structured models ( e.g. , structured as graphs , trees , or programs )  @xcite to harness abstractions and enable transfer learning .",
    "we view these ideas as consistent with and complementary to the framework of cost functions , optimization and specialized systems discussed here .",
    "one might seek to understand how optimization and specialized systems could be used to implement some of the mechanisms proposed in  @xcite inside neural networks . @xcite",
    "emphasize how incorporating additional structure into trainable neural networks can potentially give rise to systems that use compositional , causal and intuitive inductive biases and that `` learn to learn '' using structured models and shared data structures .",
    "for example , sub - dividing networks into units that can be modularly and dynamically combined , where representations can be copied and routed , may present a path towards improved compositionality and transfer learning  @xcite .",
    "the control flow for recombining pre - existing modules and representations could be learned via reinforcement learning  @xcite . how to implement the broad set of mechanisms discussed in  @xcite is a key computational problem , and it remains open at which levels ( e.g. , cost functions and training procedures vs. specialized computational structures vs. underlying neural primitives ) architectural innovations will need to be introduced to capture these phenomena .",
    "primitives that are more complex than those used in conventional neural networks  for instance , primitives that act as state machines with complex message passing  @xcite or networks that intrinsically implement bayesian inference  @xcite  could potentially be useful , and it is plausible that some of these may be found in the brain .",
    "recent findings on the power of generic optimization also do not rule out the idea that the brain may explicitly generate and use particular types of structured representations to constrain its inferences ; indeed , the specialized brain systems discussed here might provide a means to enforce such constraints .",
    "it might be possible to further map the concepts of  @xcite onto neuroscience via an infrastructure of interacting cost functions and specialized brain systems under rich genetic control , coupled to a powerful and generic neurally implemented capacity for optimization .",
    "for example , it was recently shown that complex probabilistic population coding and inference can arise automatically from backpropagation - based training of simple neural networks  @xcite , without needing to be built in by hand .",
    "the nature of the underlying primitives in the brain , on top of which learning can operate , is a key question for neuroscience .",
    "hypotheses are primarily useful if they lead to concrete , experimentally testable predictions . as such , we now want to go through the hypotheses and see to which level they can be directly tested , as well as refined , through neuroscience .",
    "there are multiple general strategies for addressing whether and how the brain optimizes cost functions .",
    "a first strategy is based on observing the endpoint of learning .",
    "if the brain uses a cost function , and we can guess its identity , then the final state of the brain should be close to optimal for the cost function .",
    "if we know the statistics of natural environments , and know the cost function , we can compare receptive fields that are optimized in a simulation with the measured ones .",
    "this strategy is only beginning to be used at the moment because it has been difficult to measure the receptive fields or other representational properties across a large population of neurons , but this situation is beginning to improve technologically with the emergence of large - scale recording methods .    a second strategy could directly quantify how well a cost function describes learning . if the dynamics of learning minimize a cost function then the underlying vector field should have a strong gradient descent type component and a weak rotational component .",
    "if we could somehow continuously monitor the synaptic strengths , while externally manipulating them , then we could , in principle , measure the vector field in the space of synaptic weights , and calculate its divergence as well as its rotation . for at least the subset of synapses that are being trained via some approximation to gradient descent , the divergence component should be strong relative to the rotational component .",
    "this strategy has not been developed yet due to experimental difficulties with monitoring large numbers of synaptic weights .",
    "a third strategy is based on perturbations : cost function based learning should undo the effects of perturbations which disrupt optimality , i.e. , the system should return to local minima after a perturbation , and indeed perhaps to the same local minimum after a sufficiently small perturbation .",
    "if we change synaptic connections , e.g. , in the context of a brain machine interface , we should be able to produce a reorganization that can be predicted based on a guess of the relevant cost function .",
    "this strategy is starting to be feasible in motor areas .",
    "lastly , if we knew structurally which cell types and connections mediated the delivery of error signals vs. input data or other types of connections , then we could stimulate specific connections so as to impose a user - defined cost function . in effect",
    ", we would use the brain s own networks as a trainable deep learning substrate , and then study how the network responds to training .",
    "brain machine interfaces can be used to set up specific local learning problems , in which the brain is asked to create certain user - specified representations , and the dynamics of this process can be monitored  @xcite . in order to do this properly",
    ", we must first understand more about the system is wired to deliver cost signals .",
    "much of the structure that would be found in connectomic circuit maps , for example , would not just be relevant for short - timescale computing , but also for creating the infrastructure that supports cost functions and their optimization .",
    "many of the learning mechanisms that we have discussed in this paper make specific predictions about connectivity or dynamics . for example , the `` feedback alignment '' approach to biological backpropagation suggests that cortical feedback connections should , at some level of neuronal grouping , be largely sign - concordant with the corresponding feedforward connections , although not necessarily of concordant weight  @xcite , and feedback alignment also makes predictions for synaptic normalization mechanisms  @xcite .",
    "the kickback model for biologically plausible backpropagation has a specific role for nmda receptors  @xcite .",
    "some models that incorporate dendritic coincidence detection for learning temporal sequences predict that a given axon should make only a small number of synapses on a given dendritic segment  @xcite .",
    "models that involve stdp learning will make predictions about the dynamics of changing firing rates  @xcite , as well as about the particular network structures , such as those based on autoencoders or recirculation , in which stdp can give rise to a form of backpropagation .",
    "it is critical to establish the unit of optimization .",
    "we want to know the scale of the modules that are trainable by some approximation of gradient descent optimization .",
    "how large are the networks which share a given error signal or cost function ?",
    "on what scales can appropriate training signals be delivered ? it could be that the whole brain is optimized end - to - end , in principle . in this case",
    "we would expect to find connections that carry training signals from each layer to the preceding ones . on successively smaller scales",
    ", optimization could be within a brain area , a microcircuit , or an individual neuron @xcite .",
    "importantly , optimization may co - exist across these scales . there may be some slow optimization end - to - end , with stronger optimization within a local area and very efficient algorithms within each cell .",
    "careful experiments should be able to identify the scale of optimization , e.g. , by quantifying the extent of learning induced by a local perturbation .",
    "the tightness of the structure - function relationship is the hallmark of molecular and to some extent cellular biology , but in large connectionist learning systems , this relationship can become difficult to extract : the same initial network can be driven to compute many different functions by subjecting it to different training",
    ". it can be hard to understand the way a neural network solves its problems .",
    "how could one tell the difference , then , between a gradient - descent trained network vs. untrained or random networks vs. a network that has been trained against a different kind of task ?",
    "one possibility would be to train artificial neural networks against various candidate cost functions , study the resulting neural tuning properties  @xcite , and compare them with those found in the circuit of interest  @xcite .",
    "this has already been done to aid the interpretation of the neural dynamics underlying decision making in the pfc  @xcite , working memory in the posterior parietal cortex  @xcite and object representation in the visual system  @xcite .",
    "some have gone on to suggest a direct correspondence between cortical circuits and optimized , appropriately regularized  @xcite , recurrent neural networks  @xcite . in any case , effective analytical methods to reverse engineer complex machine learning systems  @xcite , and methods to reverse engineer biological brains , may have some commonalities .",
    "does this emphasis on function optimization and trainable substrates mean that we should give up on reverse engineering the brain based on detailed measurements and models of its specific connectivity and dynamics",
    "? on the contrary : we should use large - scale brain maps to try to better understand a ) how the brain implements optimization , b ) where the training signals come from and what cost functions they embody , and c ) what structures exist , at different levels of organization , to constrain this optimization to efficiently find solutions to specific kinds of problems .",
    "the answers may be influenced by diverse local properties of neurons and networks , such as homeostatic rules of neural structure , gene expression and function  @xcite , the diversity of synapse types , cell - type - specific connectivity  @xcite , patterns of inter - laminar projection , distributions of inhibitory neuron types , dendritic targeting and local dendritic physiology and plasticity  @xcite or local glial networks  @xcite .",
    "they may also be influenced by the integrated nature of higher - level brain systems , including mechanisms for developmental bootstrapping  @xcite , information routing  @xcite , attention  @xcite and hierarchical decision making  @xcite .",
    "mapping these systems in detail is of paramount importance to understanding how the brain works , down to the nanoscale dendritic organization of ion channels and up to the real - time global coordination of cortex , striatum and hippocampus , all of which are computationally relevant in the framework we have explicated here .",
    "we thus expect that large - scale , multi - resolution brain maps would be useful in testing these framework - level ideas , in inspiring their refinements , and in using them to guide more detailed analysis .      clearly , we can map differences in structure , dynamics and representation across brain areas . when we find such differences , the question remains as to whether we can interpret these as resulting from differences in the internally - generated cost functions , as opposed to differences in the input data , or from differences that reflect other constraints unrelated to cost functions .",
    "if we can directly measure aspects of the cost function in different areas , then we can also compare them across areas .",
    "for example , methods from inverse reinforcement learning might allow backing out the cost function from observed plasticity  @xcite .",
    "moreover , as we begin to understand the `` neural correlates '' of particular cost functions  perhaps encoded in particular synaptic or neuromodulatory learning rules , genetically - guided local wiring patterns , or patterns of interaction between brain areas  we can also begin to understand when differences in observed neural circuit architecture reflect differences in cost functions .",
    "we expect that , for each distinct learning rule or cost function , there may be specific molecularly identifiable types of cells and/or synapses .",
    "moreover , for each specialized system there may be specific molecularly identifiable developmental programs that tune it or otherwise set its parameters .",
    "this would make sense if evolution has needed to tune the parameters of one cost function without impacting others .",
    "how many different types of internal training signals does the brain generate ? when thinking about error signals , we are not just talking about dopamine and serotonin , or other classical reward - related pathways .",
    "the error signals that may be used to train specific sub - networks in the brain , via some approximation of gradient descent or otherwise , are not necessarily equivalent to reward signals .",
    "it is important to distinguish between cost functions that may be used to drive optimization of specific sub - circuits in the brain , and what are referred to as `` value functions '' or `` utility functions '' , i.e. , functions that predict the agent s aggregate future reward . in both cases ,",
    "similar reinforcement learning mechanisms may be used , but the interpretation of the cost functions is different . we have not emphasized global utility functions for the animal here , since they are extensively studied elsewhere ( e.g. , @xcite ) , and since we argue that , though important , they are only a part of the picture , i.e. , that the brain is not solely an end - to - end reinforcement trained system .",
    "progress in brain mapping could soon allow us to classify the types of reward signals in the brain , follow the detailed anatomy and connectivity of reward pathways throughout the brain , and map in detail how reward pathways are integrated into striatal , cortical , hippocampal and cerebellar microcircuits .",
    "this program is beginning to be carried out in the fly brain , in which twenty specific types of dopamine neuron project to distinct anatomical compartments of the mushroom body to train distinct odor classifiers operating on a set of high - dimensional odor representations  @xcite .",
    "it is known that , even within the same system , such as the fly olfactory pathway , some neuronal wiring is highly specific and molecularly programmed  @xcite , while other wiring is effectively random  @xcite , and yet other wiring is learned  @xcite .",
    "the interplay between such design principles could give rise to many forms of `` division of labor '' between genetics and learning .",
    "likewise , it is believed that birdsong learning is driven by reinforcement learning using a specialized cost function that relies on comparison with a memorized version of a tutor s song  @xcite , and also that it involves specialized structures for controlling song variability during learning  @xcite .",
    "these detailed pathways underlying the construction of cost functions for vocal learning are beginning to be mapped  @xcite . starting with simple systems",
    ", it should become possible to map the reward pathways and how they evolved and diversified , which would be a step on the way to understanding how the system learns .",
    "if different brain structures are performing distinct types of computations with a shared goal , then optimization of a joint cost function will take place with different dynamics in each area .",
    "if we focus on a higher level task , e.g. , maximizing the probability of correctly detecting something , then we should find that basic feature detection circuits should learn when the features were insufficient for detection , that attentional routing structures should learn when a different allocation of attention would have improved detection and that memory structures should learn when items that matter for detection were not remembered .",
    "if we assume that multiple structures are participating in a joint computation , which optimizes an overall cost function ( but see * hypothesis 2 * ) , then an understanding of the computational function of each area leads to a prediction of the measurable plasticity rules .",
    "machine learning may be equally transformed by neuroscience . within the brain ,",
    "a myriad of subsystems and layers work together to produce an agent that exhibits general intelligence .",
    "the brain is able to show intelligent behavior across a broad range of problems using only relatively small amounts of data . as such , progress at understanding the brain promises to improve machine learning . in this section ,",
    "we review our three hypotheses about the brain and discuss how their elaboration might contribute to more powerful machine learning systems .",
    "a good practitioner of machine learning should have a broad range of optimization methods at their disposal as different problems ask for different approaches .",
    "the brain , we have argued , is an implicit machine learning mechanism which has been evolved over millions of years .",
    "consequently , we should expect the brain to be able to optimize cost functions efficiently , across many domains and kinds of data .",
    "indeed , across different animal phyla , we even see _ convergent _ evolution of certain brain structures  @xcite , e.g. , the bird brain has no cortex yet has developed homologous structures which  as the linguistic feats of the african grey parrot demonstrate  can give rise to quite complex intelligence .",
    "it seems reasonable to hope to learn how to do truly general - purpose optimization by looking at the brain .",
    "indeed , there are multiple kinds of optimization that we may expect to discover by looking at the brain . at the hardware level",
    ", the brain clearly manages to optimize functions efficiently despite having slow hardware subject to molecular fluctuations , suggesting directions for improving the hardware of machine learning to be more energy efficient . at the level of learning rules , the brain solves an optimization problem in a highly nonlinear , non - differentiable , temporally stochastic , spiking system with massive numbers of feedback connections , a problem that we arguably still do not know how to efficiently solve for neural networks . at the architectural level ,",
    "the brain can optimize certain kinds of functions based on very few stimulus presentations , operates over diverse timescales , and clearly uses advanced forms of active learning to infer causal structure in the world .",
    "while we have discussed a range of theories  @xcite for how the brain can carry out optimization , these theories are still preliminary .",
    "thus , the first step is to understand whether the brain indeed performs multi - layer credit assignment in a manner that approximates full gradient descent , and if so , how it does this . either way , we can expect that answer to impact machine learning .",
    "if the brain does not do some form of backpropagation , this suggests that machine learning may benefit from understanding the tricks that the brain uses to avoid having to do so .",
    "if , on the other hand , the brain does do backpropagation , then the underlying mechanisms clearly can support a very wide range of efficient optimization processes across many domains , including learning from rich temporal data - streams and via unsupervised mechanisms , and the architectures behind this will likely be of long - term value to machine learning .",
    "moreover , the search for biologically plausible forms of backpropagation has already led to interesting insights , such as the possibility of using random feedback weights ( feedback alignment ) in backpropagation  @xcite , or the unexpected power of internal force learning in chaotic , spontaneously active recurrent networks  @xcite .",
    "this and other findings discussed here suggest that there are still fundamental things we do nt understand about backpropagation  which could potentially lead not only to more biologically plausible ways to train recurrent neural networks , but also to fundamentally simpler and more powerful ones .",
    "a good practitioner of machine learning has access to a broad range of learning techniques and thus implicitly is able to use many different cost functions .",
    "some problems ask for clustering , others for extracting sparse variables , and yet others for prediction quality to be maximized .",
    "the brain also needs to be able to deal with many different kinds of datasets . as such",
    ", it makes sense for the brain to use a broad range of cost functions appropriate for the diverse set of tasks it has to solve to thrive in this world .",
    "many of the most notable successes of deep learning , from language modeling  @xcite , to vision  @xcite , to motor control  @xcite , have been driven by end - to - end optimization of single task objectives .",
    "we have highlighted cases where machine learning has opened the door to multiplicities of cost functions that shape network modules into specialized roles .",
    "we expect that machine learning will increasingly adopt these practices in the future .    in computer vision ,",
    "we have begun to see researchers re - appropriate neural networks trained for one task ( e.g. , imagenet classification ) and then deploy them on new tasks other than the ones they were trained for or for which more limited training data is available  @xcite .",
    "we imagine this procedure will be generalized , whereby , in series and in parallel , diverse training problems , each with an associated cost function , are used to shape visual representations . for example",
    ", visual data streams can be segmented into elements like foreground vs. background , objects that can move of their own accord vs. those that can not , all using diverse unsupervised criteria  @xcite .",
    "networks so trained can then be shared , augmented , and retrained on new tasks .",
    "they can be introduced as front - ends for systems that perform more complex objectives or even serve to produce cost functions for training other circuits  @xcite . as a simple example , a network that can discriminate between images of different kinds of architectural structures ( pyramid , staircase , etc . ) could act as a critic for a building - construction network .",
    "scientifically , determining the order in which cost functions are engaged in the biological brain will inform machine learning about how to construct systems with intricate and hierarchical behaviors via divide - and - conquer approaches to learning problems , active learning , and more .",
    "a good practitioner of machine learning should have a broad range of algorithms at their disposal .",
    "some problems are efficiently solved through dynamic programming , others through hashing , and yet others through multi - layer backpropagation .",
    "the brain needs to be able to solve a broad range of learning problems without the luxury of being reprogrammed . as such",
    ", it makes sense for the brain to have specialized structures that allow it to rapidly learn to approximate a broad range of algorithms .",
    "the first neural networks were simple single - layer systems , either linear or with limited non - linearities  @xcite .",
    "the explosion of neural network research in the 1980s  @xcite saw the advent of multilayer networks , followed by networks with layer - wise specializations as in convolutional networks  @xcite . in the last two decades , architectures with specializations for holding variables stable in memory like",
    "the lstm  @xcite , the control of content - addressable memory  @xcite , and game playing by reinforcement learning  @xcite have been developed .",
    "these networks , though formerly exotic , are now becoming mainstream algorithms in the toolbox of any deep learning practitioner .",
    "there is no sign that progress in developing new varieties of structured architectures is halting , and the heterogeneity and modularity of the brain s circuitry suggests that diverse , specialized architectures are needed to solve the diverse challenges that confront a behaving animal .",
    "the brain combines a jumble of specialized structures in a way that works . solving this problem _ de novo _ in machine learning promises to be very difficult , making it attractive to be inspired by observations about how the brain does it .",
    "an understanding of the breadth of specialized structures , as well as the architecture that combines them , should be quite useful .",
    "deep learning methods have taken the field of machine learning by storm . driving",
    "the success is the separation of the problem of learning into two pieces : * ( 1 ) * an algorithm , backpropagation , that allows efficient distributed optimization , and * ( 2 ) * approaches to turn any given problem into an optimization problem , by designing a cost function and training procedure which will result in the desired computation .",
    "if we want to apply deep learning to a new domain , e.g. , playing jeopardy , we do not need to change the optimization algorithm  we just need to cleverly set up the right cost function .",
    "a lot of work in deep learning , perhaps the majority , is now focused on setting up the right cost functions .",
    "we hypothesize that the brain also acquired such a separation between optimization mechanisms and cost functions .",
    "if neural circuits , such as in cortex , implement a general - purpose optimization algorithm , then any improvement to that algorithm will improve function across the cortex . at the same time , different cortical areas solve different problems , so tinkering with each area s cost function is likely to improve its performance . as such , functionally and evolutionarily separating the problems of optimization and cost function generation could allow evolution to produce better computations , faster .",
    "for example , common unsupervised mechanisms could be combined with area - specific reinforcement - based or supervised mechanisms and error signals , much as recent advances in machine learning have found natural ways to combine supervised and unsupervised objectives in a single system  @xcite .",
    "this suggests interesting questions neurons are locally optimized to perform disentangling of the manifolds corresponding to their local views of the transformations of an object , allowing these manifolds to be linearly separated by readout areas . yet",
    ",  @xcite also emphasizes the possibility that certain computations such as normalization are pre - initialized in the circuitry prior to learning - based optimization . ] : when did the division between cost functions and optimization algorithms occur",
    "? how is this separation implemented ?",
    "how did innovations in cost functions and optimization algorithms evolve ? and how do our own cost functions and learning algorithms differ from those of other animals ?",
    "there are many possibilities for how such a separation might be achieved in the brain .",
    "perhaps the six - layered cortex represents a common optimization algorithm , which in different cortical areas is supplied with different cost functions .",
    "this claim is different from the claim that all cortical areas use a single unsupervised learning algorithm and achieve functional specificity by tuning the inputs to that algorithm . in that case , both the optimization mechanism and the implicit unsupervised cost function would be the same across areas ( e.g. , minimization of prediction error ) , with only the training data differing between areas , whereas in our suggestion , the optimization mechanism would be the same across areas but the cost function , _ as well as _ the training data , would differ .",
    "thus the cost function itself would be like an ancillary input to a cortical area , in addition to its input and output data . some cortical microcircuits",
    "could then , perhaps , compute the cost functions that are to be delivered to other cortical microcircuits .",
    "another possibility is that , within the same circuitry , certain aspects of the wiring and learning rules specify an optimization mechanism and are relatively fixed across areas , while others specify the cost function and are more variable",
    ". this latter possibility would be similar to the notion of cortical microcircuits as molecularly and structurally configurable elements , akin to the cells in a field - programmable gate array ( fpga )  @xcite , rather than a homogenous substrate .",
    "the biological nature of such a separation , if any exists , remains an open question .",
    "for example , individual parts of a neuron may separately deal with optimization and with the specification of the cost function , or different parts of a microcircuit may specialize in this way , or there may be specialized types cells , some of which deal with signal processing and others with cost functions .",
    "due to the complexity and variability of the brain , pure `` bottom up '' analysis of neural data faces potential challenges of interpretation  @xcite .",
    "theoretical frameworks can potentially be used to constrain the space of hypotheses being evaluated , allowing researchers to first address higher - level principles and structures in the system , and then `` zoom in '' to address the details . proposed `` top down '' frameworks for understanding neural computation include entropy maximization , efficient encoding , faithful approximation of bayesian inference , minimization of prediction error , attractor dynamics , modularity , the ability to subserve symbolic operations , and many others  @xcite .",
    "interestingly , many of the `` top down '' frameworks boil down to assuming that the brain simply optimizes a single , given cost function for a single computational architecture .",
    "we generalize these proposals assuming both a heterogeneous combination of cost functions unfolding over development , and a diversity of specialized sub - systems .",
    "much of neuroscience has focused on the search for `` the neural code '' , i.e. , it has asked which stimuli are good at driving activity in individual neurons , regions , or brain areas .",
    "but , if the brain is capable of generic optimization of cost functions , then we need to be aware that rather simple cost functions can give rise to complicated stimulus responses .",
    "this potentially leads to a different set of questions .",
    "are differing cost functions indeed a useful way to think about the differing functions of brain areas ? how does the optimization of cost functions in the brain actually occur , and how is this different from the implementations of gradient descent in artificial neural networks ?",
    "what additional constraints are present in the circuitry that remain fixed while optimization occurs ?",
    "how does optimization interact with a structured architecture , and is this architecture similar to what we have sketched ?",
    "which computations are wired into the architecture , which emerge through optimization , and which arise from a mixture of those two extremes ? to what extent are cost functions explicitly computed in the brain , versus implicit in its local learning rules ? did the brain evolve to separate the mechanisms involved in cost function generation from those involved in the optimization of cost functions , and if so how ? what kinds of meta - level learning might the brain apply , to learn when and how to invoke different cost functions or specialized systems , among the diverse options available , to solve a given task ?",
    "what crucial mechanisms are left out of this framework ?",
    "a more in - depth dialog between neuroscience and machine learning could help elucidate some of these questions .",
    "much of machine learning has focused on finding ever faster ways of doing end - to - end gradient descent in neural networks .",
    "neuroscience may inform machine learning at multiple levels .",
    "the optimization algorithms in the brain have undergone a couple of hundred million years of evolution .",
    "moreover , the brain may have found ways of using heterogeneous cost functions that interact over development so as to simplify learning problems by guiding and shaping the outcomes of unsupervised learning .",
    "lastly , the specialized structures evolved in the brain may inform us about ways of making learning efficient in a world that requires a broad range of computational problems to be solved over multiple timescales . looking at the insights from neuroscience may help machine learning move towards general intelligence in a structured heterogeneous world with access to only small amounts of supervised data .    in some ways",
    "our proposal is opposite to many popular theories of neural computation .",
    "there is not one mechanism of optimization but ( potentially ) many , not one cost function but a host of them , not one kind of a representation but a representation of whatever is useful , and not one homogeneous structure but a large number of them .",
    "all these elements are held together by the optimization of internally generated cost functions , which allows these systems to make good use of one another .",
    "rejecting simple unifying theories is in line with a broad range of previous approaches in ai .",
    "for example , minsky and papert s work on the society of mind  @xcite  and more broadly on ideas of genetically staged and internally bootstrapped development in connectionist systems  @xcite  emphasizes the need for a system of internal monitors and critics , specialized communication and storage mechanisms , and a hierarchical organization of simple control systems .    at the time these early works were written",
    ", it was not yet clear that gradient - based optimization could give rise to powerful feature representations and behavioral policies .",
    "one can view our proposal as a renewed argument against simple end - to - end training and in favor of a heterogeneous approach .",
    "in other words , this framework could be viewed as proposing a kind of `` society '' of cost functions and trainable networks , permitting internal bootstrapping processes reminiscent of the society of mind  @xcite . in this view",
    ", intelligence is enabled by many computationally specialized structures , each trained with its own developmentally regulated cost function , where both the structures and the cost functions are themselves optimized by evolution like the hyperparameters in neural networks ."
  ],
  "abstract_text": [
    "<S> neuroscience has focused on the detailed implementation of computation , studying neural codes , dynamics and circuits . in machine learning , </S>",
    "<S> however , artificial neural networks tend to eschew precisely designed codes , dynamics or circuits in favor of brute force optimization of a cost function , often using simple and relatively uniform initial architectures . </S>",
    "<S> two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives . </S>",
    "<S> first , structured architectures are used , including dedicated systems for attention , recursion and various forms of short- and long - term memory storage . </S>",
    "<S> second , cost functions and training procedures have become more complex and are varied across layers and over time . </S>",
    "<S> here we think about the brain in terms of these ideas . </S>",
    "<S> we hypothesize that ( 1 ) the brain optimizes cost functions , ( 2 ) these cost functions are diverse and differ across brain locations and over development , and ( 3 ) optimization operates within a pre - structured architecture matched to the computational problems posed by behavior . </S>",
    "<S> such a heterogeneously optimized system , enabled by a series of interacting cost functions , serves to make learning data - efficient and precisely targeted to the needs of the organism . </S>",
    "<S> we suggest directions by which neuroscience could seek to refine and test these hypotheses .    </S>",
    "<S> cost functions , neural networks , neuroscience , cognitive architecture    2 </S>"
  ]
}