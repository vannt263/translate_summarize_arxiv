{
  "article_text": [
    "the statics and the dynamics of large attractor and feed - forward neural networks , inspired in features of biological networks , have been studied over some time .",
    "some of these features are the asymmetry and the finite dilution of the synaptic connections as well as the low activity of the patterns and the neurons .",
    "it has been suggested that a recurrent attractor neural network model of multi - state neurons with these characteristics may describe the short term memorization performance of the @xmath0 region of the hyppocampus in both the human brain and in the brain of primates , and results of numerical simulations on the retrieval behavior are now available @xcite .",
    "full results on the dynamic evolution of the retrieval overlap and on the phase diagrams for the stationary states of low - activity networks of multi - state neurons with asymmetric interactions and finite dilution are still missing",
    ". it would be desirable to have such results in order to describe other implementations of neural network models , among them as devices to account for long - term memory in the brain @xcite .",
    "the retrieval behavior and thermodynamic properties of symmetrically diluted q - ising recurrent neural networks with finite dilution and low activity patterns have been studied and full phase diagrams have been obtained for @xmath1 and @xmath2 states as well as for a network with continuous response neurons @xcite . in the case of a discrete number of states ,",
    "phase boundaries between states of low and high performance are found to disappear beyond a finite dilution , allowing for the biologically appealing feature of a continuous improvement of the behavior of the network without the need of a precise threshold adjustment .",
    "the full study of the dynamics of these networks is rather involved ( see ref .",
    "@xcite ) and for practical , either hardware or biological implementations of neural networks , it would be interesting to have a simple dynamical system .",
    "a suitably tractable model to study these issues is a feed - forward layered network with no feedback loops , which has an exactly solvable non - trivial dynamics , in which the only non - zero synaptic connections are the asymmetric interactions that pass information from one layer to the next . despite the fact that the connectivity of the layered network is much lower than that of a recurrent network , the presence of non - trivial correlations makes it an ideal model to test the qualitative behavior of feature dependence in recurrent networks .",
    "the study of the model in itself is also of interest in view of the practical applications of feed - forward layered networks .",
    "the purpose of the present work is to present new results on the retrieval performance , the information content and the dynamic evolution for this network with three - state ising neurons and finite dilution .",
    "results for the diluted layered network with binary units and for the three - state network with no dilution can be found in the literature @xcite .",
    "the outline of the paper is the following . in section 2",
    "we introduce the model and the relevant macroscopic variables . in section 3",
    "we derive the recursion relations for these variables that establish the dynamics for the model .",
    "we discuss the results for the phase diagrams of the stationary states and the dynamical evolution of the retrieval overlaps in section 4 , and end with concluding remarks in section 5 .",
    "the network model consists of @xmath3 layers with @xmath4 neurons on each layer , that can take values @xmath5 ,",
    "@xmath6 from the set @xmath7 , where @xmath8 denote the active states . a macroscopic number of @xmath9 ternary patterns , where @xmath10 is the probability that two neurons are connected , is taken from a set of independent identically distributed random variables @xmath11 , @xmath12 , with the probability distribution @xmath13 which is assumed to be the same for every layer and where @xmath8 are the active patterns .",
    "the mean of each pattern ( over - lined ) is zero and @xmath14 , denotes their activity .",
    "a new random set of @xmath15 patterns is generated on each layer and the whole set on two consecutive layers is embedded in the diluted network according to the generalized hebbian learning rule @xmath16 where @xmath17 is a set of independent identically distributed random variables that account for the dilution of the synapses , particularly when the patterns are active , and such that @xmath18 with probability @xmath10 and zero with probability @xmath19 , for all @xmath20 .",
    "thus , @xmath21 is the mean number of neurons connected to each neuron . for the fully connected layered network , @xmath22 , and",
    "the case of extreme dilution corresponds to the limit @xmath23 , after taking the macroscopic @xmath24 limit .",
    "the dilution introduces an additional randomness into the dynamics of the fully connected network in the form of a static noise @xmath25 with mean zero and variance @xmath26 @xcite .",
    "there is no contribution to the learning rule from patterns in the same layer .",
    "the states of the network change as follows . given a configuration on the first layer , @xmath27 , the state @xmath28 of unit @xmath29 on layer @xmath30 is determined exclusively by the configuration @xmath31 of the units in the previous layer according to the stochastic law @xmath32 } { \\sum_{s\\in{s}}\\exp[-\\beta\\epsilon_i(s|\\bsigma_n^{l } ) ] } \\,\\,\\ , , \\label{3}\\ ] ] in terms of the single - site energy function on that unit @xmath33 where @xmath34 is the acting local field and @xmath35 is a local externally adjustable threshold parameter on layer @xmath30 . since the only changes in the network , in both the synaptic connections and the states of the units , are between units in consecutive layers one may associate the layer index with a discrete time step @xmath36 and we do this in the following .",
    "thus , the evolution of the network proceeds according to a parallel dynamics in which the states of all neurons are updated at each time step .",
    "next , we consider the relevant quantities that describe the performance of the network .",
    "first , the retrieval overlap between the state of the network and the pattern @xmath37 at time ( layer ) @xmath36 , given by @xmath38 which depends on the site @xmath29 .",
    "the dynamical activity and the activity overlap are defined as @xmath39 respectively .",
    "the latter will only be needed for the condensed pattern , that is the stored pattern to be retrieved .    for the mutual information , we need the conditional probability distribution @xmath40 that a neuron @xmath29 is in the state @xmath41 on layer @xmath36 given that the @xmath29-th bit of the condensed pattern is @xmath42 .",
    "as a consequence of the independence of the states of the units on a given layer , it is sufficient to consider the distribution for a single typical neuron , and we omit here the index @xmath29 .",
    "we also omit here , for clarity , the time index and use @xcite @xmath43 where @xmath44 the mutual information between patterns and neurons , regarding the patterns as the inputs and the neuron states as the output of the network channel on each layer , is an architecture independent property given by @xcite @xmath45 where @xmath46 + is the entropy and @xmath47 is the equivocation term with @xmath48 here , @xmath49 and @xmath50 is the parameter in the conditional probability @xmath51 .",
    "the mutual information can then be used to obtain the information content of the network , @xmath52 , where @xmath53 is the storage ratio .",
    "we consider in this work the retrieval of a single ( condensed ) pattern , say @xmath54 , in the dynamics of the network with a finite overlap @xmath55 and the remaining @xmath56 overlaps @xmath57 .",
    "the interest is in the mean overlap , @xmath58 $ ] , where @xmath59 $ ] denotes the average over the @xmath60 , @xmath61 denotes the thermal average with eq.(3 ) and the bar means the average over the patterns .",
    "we also need the activity overlap with pattern @xmath54 , and take @xmath62 $ ] .",
    "the recurrence relations that describe the dynamics of the network for large @xmath4 follow from the local field , eq.(5 ) , written as @xmath63 where the average condensed overlap @xmath64 depends on the local field at time @xmath65 and @xmath66 is a gaussian random variable with zero mean and unit variance that comes from the action of the macroscopic number of random overlaps of the diluted network with the uncondensed patterns and the use of the central limit theorem .",
    "the layer - dependent variance of the local field becomes site independent and is given by @xmath67 }   \\,\\,\\ , .",
    "\\label{14}\\ ] ] a direct calculation in the large-@xmath4 limit yields @xmath68 where @xmath53 , @xmath69 is the dynamical activity and @xmath70 is now the variance of the gaussian noise for the connected layered network in terms of the overlap with the uncondensed patterns @xmath71 thus , the noise in the local field is a superposition of two gaussian noises , one due to the dilution of the synaptic connections and the other to the macroscopic number of uncondensed patterns , in extension of an earlier result for the binary network @xcite .",
    "since our main interest in this work is in the effects of synaptic dilution in a low - activity network , we take a uniform and time - independent threshold @xmath72 . the averages @xmath73 and @xmath74 are then given , respectively , by @xmath75 which , in the zero temperature limit , @xmath76 , become @xmath77 where @xmath78 is the usual step function . the performance with a self - adjusting time - dependent threshold",
    "has been considered in a fully connected layered network @xcite and in other architectures [ 10 - 14 ] .",
    "exact dynamic equations are then obtained in the large @xmath4 limit in the form of recursion relations for @xmath79 , @xmath80 and @xmath81 , where the latter two are needed for the information content of the network .",
    "similarly , an exact recursion relation for the second term in the width of the stochastic noise , eq.(15 ) , is obtained in the form @xmath82 where @xmath83 is the susceptibility in which @xmath84 .",
    "thus we obtain @xmath85 where , as usual , @xmath86 .",
    "we also have @xmath87    the dynamics , including transients , and the stationary states of the diluted layered network follows then from the solutions of these equations together with eq.(20 ) .",
    "the stationary states are reached when @xmath88 , @xmath89 , @xmath90 , @xmath91 and @xmath92 , and we call the first three , respectively , @xmath93 , @xmath94 and @xmath95 .",
    "the phase diagrams for the stationary states and the time evolution of the order parameters are discussed in the next section .",
    "the stable stationary states yield one or more retrieval phases @xmath96 and a spin - glass phase @xmath97 , all as sustained activity solutions with @xmath98 .",
    "since the model is a dynamical one , the stability criterium is that the change in the order parameters should become smaller in every one of the final steps of the iteration procedure of the flow towards an attractor fixed point .",
    "thus , for the retrieval overlap one has @xmath99 and similar relationships for the other parameters .",
    "we are interested in this paper in the characteristic features of finite dilution of the phase diagrams and in the specific performance of the network .",
    "different kinds of phase diagrams are obtained depending on the pattern activity @xmath100 and on @xmath10 . in the case of full connectivity ( @xmath22 ) and low @xmath100",
    ", we find the @xmath101 phase diagram shown in fig .",
    "1 for @xmath102 and either @xmath103 ( full lines ) or @xmath104 ( dashed lines ) . the lines represent discontinuous phase boundaries where the locally stable retrieval states appear with decreasing load @xmath105 below a critical @xmath106 and the dotted line indicates the locus of optimal performance which yields the largest load capacity that still sustains retrieval behavior .",
    "there is a weak retrieval phase ( i ) and a strong retrieval phase ( ii ) separated by a discontinuous phase boundary .",
    "there is also a smaller retrieval phase in the lower triangular region . for larger activity and full connectivity , as in the case of uniform patterns ( @xmath107 )",
    ", there is mostly a single retrieval phase with a strong @xmath105 dependent optimal performance and this feature remains even for finite dilution , say for @xmath108 .    )",
    "phase diagram for the fully connected @xmath1 ising layered feed - forward network , with activity @xmath102 , @xmath103 ( full lines ) and @xmath104 ( dashed lines ) .",
    "the phases are described in the text and the dotted line is the locus of optimal performance.,height=377 ]    a different situation appears for both finite dilution , with @xmath10 smaller than a critical activity - dependent @xmath109 , and for low activity where part of the phase boundary between regions i and ii disappears . for @xmath102",
    "this is the case when @xmath110 at @xmath111 and @xmath103 . as shown in fig .",
    "2 , for typical @xmath112 and @xmath102 , there is now a gradual transition from region i to region ii and , as @xmath10 decreases , the continuous transition region increases until a stage is reached where the maxima in both regions merge into a single maximum with a lower optimal threshold @xmath113 . the phase diagram in fig .",
    "2 is reminiscent of the diagram for the finite diluted recurrent @xmath1 state network with uniform patterns @xcite .    )",
    "phase diagram for the diluted network , with @xmath112 , @xmath102 , for @xmath103 ( full lines ) and @xmath104 ( dashed lines ) .",
    "the phases are described in the text.,height=377 ]    in order to study the dynamics and the stability of the phases , we consider the time evolution of the retrieval overlap shown in fig . 3 for @xmath112 , @xmath102 , @xmath114 , @xmath103 and @xmath113 between @xmath115 and @xmath116 .",
    "for small @xmath113 the retrieval state is not stable as shown by the typical lower curve ( @xmath115 ) , and there is a whole part of region i where this is the case . since it is not clear that these instabilities have any significance , we do not explore this issue further in this work . on the other hand , as @xmath113 increases , the retrieval state becomes stable and a maximum overlap is reached after a relatively short transient .     for the diluted network with @xmath112 , @xmath102 , @xmath103 and @xmath114 , for @xmath117 , @xmath118 , @xmath119 , @xmath120 and @xmath116 , from bottom to top .",
    "note the instability for large @xmath36 in the first case , and the transients for the other cases.,height=377 ]    a further property of the network is the increase in the maximum information content with dilution as shown near to the optimal performance with @xmath121 for @xmath102 and various values of @xmath10 in fig .",
    "4 . note that the result of a non - zero information content for increasing @xmath105 with dilution is not surprising since we defined @xmath53 , but the increase of the maximum information content with dilution is a novel feature .     for @xmath102 , @xmath121 and @xmath22 ( fully connected layered network ) , @xmath112 , @xmath122 and @xmath123 , from bottom to top.,height=377 ]",
    "there is also a considerable increase in the maximum information content with an increasing threshold in the good performance region ii , as shown in fig . 5 for @xmath112 , @xmath102 and various values of @xmath113 .",
    "also , the decrease from the maximum is smoother with dilution than in the case of the fully connected layered network .",
    "finally , one may consider the robustness of the network to synaptic noise and in fig .",
    "6 we show the maximum information content , @xmath124 for @xmath102 , @xmath121 and various values of @xmath10 . clearly , there is a more gradual decrease in performance with @xmath125 for lower connectivity .     for @xmath102 , @xmath112 and threshold @xmath126 , @xmath120 , @xmath127 and @xmath128 , from bottom to top .",
    "the dotted parts of the lines indicate unstable states.,height=377 ]     as a function of the synaptic noise @xmath125 for @xmath102 , @xmath121 and @xmath22 ( fully connected layered network ) , @xmath129 , @xmath130 and @xmath131 ( extremely diluted network , from bottom to top.,height=377 ]",
    "we derived the exact recursion relations in the large-@xmath4 limit that describe the time evolution and the phase diagrams for the stationary states of the macroscopic variables in a three - state layered feed - forward network with finite synaptic dilution .",
    "synaptic dilution appears as a static stochastic noise for a macroscopic number of stored patterns @xcite .",
    "this is a model with asymmetric interactions between units in consecutive layers and we allow for variable pattern activity in training the network with ternary patterns . instead of discontinuous phase boundaries between retrieval and non - retrieval states , or between qualitatively different retrieval states for full synaptic connectivity , we find that there can be a continuous change with no boundary at all from weak to optimal retrieval states with a varying threshold in the local field , for low pattern activity .",
    "phase boundaries of continuous transitions are known to appear in extremely diluted symmetric or asymmetric networks , but we emphasize that the continuous changeover in the present model is due to the joint action of _ finite _ dilution and low activity patterns .    in view of a similar recent result for a three - state recurrent diluted network with symmetric synaptic connections @xcite , one may conclude that this is a feature of finite dilution which is independent of both the network architecture and of the interaction symmetry .",
    "thus , provided there is an above minimum threshold such that a network attains the ability to retrieve a nominated pattern after eliminating undesirable transient states , the good retrieval performance does not depend in an essential way on a precise threshold adjustment and this may explain why biological networks , in which there are no precise thresholds , can have a good performance despite a fraction of missing synaptic connections .",
    "this is an activity dependent property and it might help in the study of plasticity in neural networks",
    ". it may also be useful for artificial neural networks",
    ".    it may be worthwhile to note the asymmetric dual role of the threshold dependence discussed in this work . whereas there is a continuous improvement in network performance for low to moderate threshold",
    ", there is an abrupt end to the performance for large threshold , as one would expect , since in the latter case mostly the inactive states of the network become dominant .",
    "we also showed that there is an improvement with finite dilution in both the size of the information content transmitted by the network and in the continuous changes of the information with a varying threshold .",
    "that is , as long as there is a convergence to stable stationary states , here again the network performance seems to be less sensitive to threshold adjustment .    the network behavior discussed in this work is also expected to appear in a diluted layered @xmath2 state network with low activity patterns , based on recent results on a recurrent network which exhibits a continuous improvement in network performance with varying low - to - moderate threshold for low activity patterns @xcite .",
    "the work of one of the authors ( wkt ) was financially supported , in part , by cnpq ( conselho nacional de desenvolvimento cientfico e tecnolgico ) , brazil .",
    "rolls , a. treves , d. foster and c. perez - vicente , neural networks * 10 * , 1559 ( 1995 ) .",
    "arbib , editor ( m.i.t . press , 1995 ) .",
    "w. k. theumann and r. erichsen jr .",
    "e * 64 * , 061902 ( 2001 ) .",
    "a. c. coolen , _ handbook of biological physics iv _",
    ", 597 , ( f. moss and s. gielen ( eds . ) , elsevier science , amsterdam 2001 ) e. domany , w. kinzel and r. meir , j. phys .",
    "a * 22 * , 2081(1989 ) .",
    "d. boll , g. m. shim and b. vinck , j. stat .",
    "phys . * 74 * , 583 ( 1994 ) .",
    "d. boll and d. dominguez carreta , physica a * 286 * , 401(2000 ) . c. e. shannon , bell syst .",
    "j. * 27 * , 379 ( 1948 ) .",
    "r. e. blahut , _ principles and practice of information theory_(addison - wesley , reading , ma 1990 ) , chapter 5 .",
    "d. boll and g. massolo , j. phys .",
    "a * 33 * , 2597 ( 2000 ) .",
    "d. r. c. dominguez and d. boll , phys .",
    "80 * , 2961 ( 1998 ) .",
    "k. kitano and t. aoyagi , j. phys .",
    "a * 31 * , l613 ( 1998 ) .",
    "s. grosskinsky , j. phys .",
    "a * 32 * , 22983 ( 1999 ) .",
    "d. boll , d. r. c. dominguez and s. amari , neural networks * 13 * , 455 ( 2000 ) .",
    "h. sompolinsky , phys . rev . * 34 * , 2571 ( 1986 ) ."
  ],
  "abstract_text": [
    "<S> the dynamics and the stationary states of an exactly solvable three - state layered feed - forward neural network model with asymmetric synaptic connections , finite dilution and low pattern activity are studied in extension of a recent work on a recurrent network . </S>",
    "<S> detailed phase diagrams are obtained for the stationary states and for the time evolution of the retrieval overlap with a single pattern . </S>",
    "<S> it is shown that the network develops instabilities for low thresholds and that there is a gradual improvement in network performance with increasing threshold up to an optimal stage . the robustness to synaptic noise </S>",
    "<S> is checked and the effects of dilution and of variable threshold on the information content of the network are also established .    </S>",
    "<S> pacs numbers : 87.10.+e , 64.60.cn , 07.05.mh    * key words : * layered neural networks ; dynamics ; mutual information .    and </S>"
  ]
}