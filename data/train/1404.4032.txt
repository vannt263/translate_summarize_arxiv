{
  "article_text": [
    "nowadays our data is often high - dimensional , massive and full of gross errors ( e.g. , corruptions , outliers and missing measurements ) . in the presence of gross errors , the classical principal component analysis ( pca ) method , which is probably the most",
    "widely used tool for data analysis and dimensionality reduction , becomes brittle  a single gross error could render the estimate produced by pca arbitrarily far from the desired estimate . as a consequence , it is crucial to develop new statistical tools for robustifying pca .",
    "a variety of methods have been proposed and explored in the literature over several decades , e.g. ,  @xcite .",
    "one of the most exciting methods is probably the so - called rpca ( robust principal component analysis ) method by  @xcite , built upon the exploration of the following low - rank matrix recovery problem :    [ pb : lmr ] suppose we have a data matrix @xmath0 and we know it can be decomposed as @xmath1 where @xmath2 is a low - rank matrix in which each column is a data point drawn from some low - dimensional subspace , and @xmath3 is a sparse matrix supported on @xmath4 . except these mild restrictions ,",
    "both components are arbitrary .",
    "the rank of @xmath5 is unknown , the support set @xmath6 ( i.e. , the locations of the nonzero entries of @xmath7 ) and its cardinality ( i.e. , the amount of the nonzero entries of @xmath7 ) are unknown either",
    ". in particular , the magnitudes of the nonzero entries in @xmath7 may be arbitrarily large .",
    "given @xmath8 , can we recover both @xmath5 and @xmath7 , in a scalable and exact fashion ?",
    "the theory of rpca tells us that , very generally , when the low - rank matrix @xmath5 satisfies some _ incoherent conditions _",
    "( i.e. , the coherence parameters of @xmath5 are small ) , both the low - rank and the sparse matrices can be exactly recovered by using the following convex , potentially scalable program : @xmath9 where @xmath10 is the nuclear norm  @xcite of a matrix , @xmath11 denotes the @xmath12 norm of a matrix seen as a long vector , and @xmath13 is a parameter . besides of its elegance in theory , rpca also has good empirical performance in many practical areas , e.g. , image processing  @xcite , computer vision  @xcite , radar imaging  @xcite , magnetic resonance imaging  @xcite , etc .    while complete in theory and powerful in reality",
    ", rpca can not be an ultimate solution to the low - rank matrix recovery problem  [ pb : lmr ] .",
    "indeed , the method might not produce perfect recovery even when the latent matrix @xmath5 is strictly low - rank .",
    "this is because , seen from the aspect of mathematics , rpca requires @xmath5 to satisfy some incoherent conditions , which , however , might not hold in reality . in a physical sense",
    ", the reason is that rpca captures only the low - rankness property , which should not be the only property of our data , but essentially ignores the _ extra structures _ ( beyond low - rankness ) widely existed in data : given the situation that @xmath5 is low - rank , i.e. , the column vectors of @xmath5 locate on a low - dimensional subspace , it is quite normal that @xmath5 may have some extra structures , which specify in more detail _ how _ the data points ( i.e. , the column vectors of @xmath5 ) locate on the subspace .",
    "figure  [ fig : cluster ] demonstrates a typical example of extra structures ; that is , the clustering structure which is ubiquitous in modern applications  @xcite . whenever the data is exhibiting some clustering structure",
    ", the coherence parameters might be large and therefore rpca might be unsatisfactory . more precisely ,",
    "as will be shown in this paper , while the rank of @xmath5 is fixed and the underlying cluster number goes large , the coherence of @xmath5 keeps heightening and thus , arguably , the performance of rpca drops .        to well handle _ _ coherent data _ _ , a straightforward approach is to _ avoid _ the coherence parameters of @xmath5 . nevertheless ,",
    "as explained in  @xcite , the coherence parameters are indeed _ necessary _ for matrix recovery ( if there is no additional condition available ) . even more , this paper shall further indicate that the coherence parameters are related in nature to some extra structures intrinsically existed in @xmath5 and therefore _ can not _ be discarded simply .",
    "interestingly , we show that it is possible to _ avoid _ the coherence parameters by imposing some _ additional conditions _ , which are easy to obey in supervised environments and can also be approximately satisfied in unsupervised environments .",
    "our study is based on the following convex program termed low - rank representation ( lrr )  @xcite : @xmath14 where @xmath15 is a size-@xmath16 dictionary matrix constructed in advance .",
    "suppose @xmath17 is the optimal solution with respect to @xmath18",
    ". then lrr uses @xmath19 to restore @xmath5 .",
    "it is easy to see that lrr falls back to rpca when @xmath20 ( identity matrix ) , and it can actually be further proved that the recovery produced by lrr is the same as rpca whenever the dictionary @xmath21 is orthogonal . ] , and @xmath13 is a parameter . in order for lrr to avoid the coherence parameters which have potential to be large in the presence of extra structures , we prove that it is sufficient to construct in advance a dictionary matrix @xmath21 which is low - rank by itself .",
    "this additional condition ( i.e. , the dictionary @xmath21 is low - rank ) gives a generic prescription to defend the possible infections raised by coherent data , providing an elementary criterion for learning the dictionary matrix @xmath21 .",
    "subsequently , we propose a simple and effective algorithm that utilizes the output of rpca to construct the dictionary in lrr .",
    "our extensive experiments demonstrated on randomly generated matrices and motion data show promising results . in summary ,",
    "the contributions of this paper include :    * for the first time , this paper studies the problem of recovering low - rank , but coherent matrices from their corrupted versions .",
    "we investigate the physical regime where coherent data arises  the widely existed clustering structure is a typical example that leads to coherent data .",
    "we prove some basic theories for resolving the problem of recovering coherent data , and also establish a practical algorithm that works better than rpca in our experiments . * the studies of this paper help to understand the _ physical _ meaning of coherence , which is now standard and widely used in various literatures , e.g. ,  @xcite .",
    "we show that the coherence parameters are not `` assumptions '' for accomplishing a proof , but rather some excellent quantities that relate in nature to the _ extra structures _ ( beyond low - rankness ) intrinsically existed in @xmath5 .",
    "* this paper provides insights regarding the lrr model proposed by  @xcite .",
    "while the special case of @xmath22 has been extensively studied , the lrr model with general dictionaries was not fully understood .",
    "we show that lrr equipped with proper dictionaries could well handle coherent data .",
    "* the idea of replacing @xmath23 with @xmath24 is essentially related to the spirit of matrix factorization which has been explored for long , e.g. ,  @xcite . in that sense , the explorations of this paper help to understand why factorization techniques are useful .",
    "the remainder of this paper is organized as follows .",
    "section  [ sec : notation ] summarizes mathematical notations used throughout this paper . in section  [ sec : principle ] , we explore the problem of recovering coherent data from corrupted observations , providing some theories and an algorithm for resolving the problem .",
    "section  [ sec : proof ] presents the complete proof procedure of our main result .",
    "section  [ sec : exp ] demonstrates experimental results and section [ sec : con ] concludes this paper .",
    "capital letters such as @xmath25 are used to represent matrices , and accordingly , @xmath26_{ij}$ ] denotes its @xmath27th entry .",
    "letters @xmath28 , @xmath29 , @xmath6 and their variants ( complements , subscripts , etc . )",
    "are reserved for left singular vectors , right singular vectors and support set , respectively .",
    "we slightly abuse the notation @xmath28 ( resp .",
    "@xmath29 ) to denote the linear space spanned by the columns of @xmath28 ( resp .",
    "@xmath29 ) , i.e. , the column space ( resp .",
    "row space ) .",
    "the projection onto the column space @xmath28 , is denoted by @xmath30 and given by @xmath31 , and similarly for the row space @xmath32 .",
    "we also abuse the notation @xmath6 to denote the linear space of matrices supported on @xmath6 .",
    "then @xmath33 and @xmath34 respectively denote the projections onto @xmath6 and @xmath35 such that @xmath36 , where @xmath37 is the identity operator .",
    "the symbol @xmath38 denotes the moore - penrose pseudoinverse of a matrix : @xmath39 for a matrix @xmath25 with singular value decomposition ( svd ) matrix @xmath40 , its svd is of the form @xmath41 , with @xmath42 and @xmath43 . ]",
    "@xmath41 .",
    "six different matrix norms are used in this paper .",
    "the first three norms are functions of the singular values : 1 ) the operator norm ( i.e. , the largest singular value ) denoted by @xmath44 , 2 ) the frobenius norm ( i.e. , square root of the sum of squared singular values ) denoted by @xmath45 , and 3 ) the nuclear norm ( i.e. , the sum of singular values ) denoted by @xmath46 .",
    "the other three are the @xmath12 , @xmath47 ( i.e. , sup - norm ) and @xmath48 norms of a matrix : @xmath49_{ij}|$ ] , @xmath50_{ij}|\\}$ ] and @xmath51_{ij}^2}\\}$ ] .",
    "the greek letter @xmath52 and its variants ( e.g. , subscripts and superscripts ) are reserved to denote the coherence parameters of a matrix .",
    "we shall also reserve two lower case letters , @xmath53 and @xmath54 , to respectively denote the data dimension and the number of data points , and we use the following two symbols throughout this paper : @xmath55 a complete list of notations can be found in appendix  [ sec : app : notations ] for convenience of readers .",
    "in this section , we shall firstly investigate the physical regime that raises coherent data , and then discuss the problem of recovering coherent data from corrupted observations , providing some basic principles and an algorithm for resolving the problem .",
    "notice that the rank function can not fully capture all characteristics of @xmath5 , and thus it is indeed necessary to define some quantities for measuring the effects of various extra structures ( beyond low - rankness ) such as the clustering structure demonstrated in figure  [ fig : cluster ] .",
    "the _ coherence _ parameters defined in  @xcite are excellent exemplars of such quantities .      for an @xmath58 matrix @xmath5 with rank @xmath59 and svd @xmath60 ,",
    "some of its important properties can be characterized by two coherence parameters , denoted as @xmath56 and @xmath57 .",
    "the first coherence parameter , @xmath61 , which characterizes the column space identified by @xmath62 , is defined as @xmath63 where @xmath64 denotes the @xmath65th standard basis .",
    "the second coherence parameter , @xmath66 , which characterizes the row space identified by @xmath67 , is defined as @xmath68 in  @xcite , another coherence parameter , called as the third coherence parameter and denoted as @xmath69 , is also introduced : @xmath70 notice that @xmath71 is not indispensable , as it is actually a `` derivative '' of @xmath56 and @xmath57 : simple calculations give that @xmath72 .",
    "the analysis of work does not need to access @xmath71 .",
    "we include it just for the sake of consistence with  @xcite .",
    "the analysis in  @xcite merges the above three parameters into a single one : @xmath73 . as will be seen later , the behaviors of those three coherence parameters are different from each other , and thus it is indeed more adequate to consider them individually .",
    "@xcite have proven that the success condition ( regarding @xmath5 ) of rpca is @xmath74 where @xmath73 and @xmath75 is some numerical constant .",
    "so , rpca will be less successful when the coherence parameters are considerably larger : the success condition is narrowed when @xmath76 goes large . as an extreme example , consider the case where the latent matrix @xmath5 is one in only one column and zero everywhere else .",
    "such a matrix produces @xmath77 , and thus the success condition is invalid . in this subsection , we shall further show that the widely existed clustering structure can enlarge the coherence parameters and , accordingly , degrades the performance of rpca .",
    "are fixed to be @xmath78 and 100 , respectively .",
    "the underlying cluster number is varying from 1 to 50 .",
    "@xmath7 is fixed as a sparse matrix with 13% nonzero entries .",
    "( a ) the first coherence parameter @xmath79 vs cluster number .",
    "( b ) @xmath80 vs cluster number .",
    "( c ) @xmath81 vs cluster number .",
    "( d ) recover error ( produced by rpca ) vs cluster number .",
    "the numbers shown in above figures are averaged from 100 random trials .",
    "the recover error is computed as @xmath82 , where @xmath83 denotes an estimate of @xmath5.,scaledwidth=95.0% ]    given the situation that @xmath5 is low - rank , i.e. , @xmath84 , the data points ( i.e. , column vectors of @xmath5 ) should be sampled from a @xmath59-dimensional subspace .",
    "yet the sampling is unnecessary to be _",
    "uniform_. indeed , a more realistic interpretation is to consider the data points as samples from the union of @xmath85 number of subspaces ( i.e. , clusters ) , and the sum of those multiple subspaces together has a dimension @xmath59 .",
    "that is to say , there are multiple `` small '' subspaces inside one @xmath59-dimensional `` large '' subspace , as exemplified in figure  [ fig : cluster ] .",
    "it is arguable that such a structure of multiple subspaces exists widely in various domains , e.g. , face , texture and motion  @xcite .",
    "whenever the low - rank matrix @xmath5 is exhibiting such clustering behaviors , the second coherence parameter @xmath80 will increase with the cluster number underlying @xmath5 , as shown in figure  [ fig : ic ] .",
    "when the coherence is heightening , suggests that the performance of rpca will drop , as verified in figure  [ fig : ic](d ) . for the ease of citation , we call the phenomena shown in figure  [ fig : ic](b)@xmath86(d ) as the `` @xmath57-phenomenon '' .    to see why the second coherence parameter increases with the cluster number underlying @xmath5",
    ", please refer to appendix  [ app : why ] .",
    "as can be seen from figure  [ fig : ic](a ) , the first coherence parameter @xmath56 is _ invariant _ to the variation of the clustering number .",
    "this is because the behaviors of the data points ( i.e. , column vectors ) can only affect the row space , while @xmath56 is defined on the column space .",
    "nevertheless , if the row vectors of @xmath5 also own some clustering structure , @xmath56 could be large as well .",
    "this kind of data exists widely in text documents and we leave it as future work .      to accurately recover coherent matrices from their corrupted versions , one may establish some parametric models to _ capture _ the extra structures which produce high coherence . however , it is usually hard , if not impossible , to know in advance what kind of extra structures there are and which models are appropriate to use .",
    "even if the modalities of the extra structure are known , e.g. , the mixture of multiple subspaces shown in figure  [ fig : cluster ] , such a strategy still needs to face some difficult problems , e.g. , the estimate of the cluster number . in sharp contrast , it is much simpler to devise an approach that can _ avoid _ the second coherence parameter @xmath57 . unfortunately , as explained in  @xcite , the coherence parameters are _ necessary _ for identifying accurately the success conditions of matrix recovery .",
    "even more , the @xmath57-phenomenon actually implies that @xmath57 is related in nature to some intrinsic structures of @xmath5 and thus can not be eschewed freely .",
    "interestingly , we shall show that lrr can avoid @xmath57 by using some _ additional conditions _ , which are possible to obey in both supervised and unsupervised environments .",
    "+ * main result : * we shall show that , when the dictionary matrix @xmath21 itself is low - rank , the recovery performance of lrr does not depend on @xmath57 . our main result is presented in the following theorem ( the detailed proof procedure is deferred until section  [ sec : proof ] ) .",
    "[ thm : noiseless ] let @xmath15 with svd @xmath87 be a column - wisely unit - normed ( i.e. , @xmath88 ) dictionary matrix which satisfies @xmath89 ( i.e. , @xmath62 is a subspace of @xmath90 ) . for any @xmath91 and some numerical constant @xmath92 , if @xmath93 then with probability at least @xmath94 , the optimal solution to the lrr problem with @xmath95 is unique and exact , in a sense that @xmath96 where @xmath97 is the optimal solution to .    by @xmath98 , the column space of @xmath21",
    "should approximately have the same properties as @xmath5 , and thus , roughly , @xmath99 .",
    "so , as aforementioned , this paper needs to assume that the first coherence parameter of @xmath5 is small and only addresses the cases where the second coherence parameter might be large .",
    "it is worth noting that the restriction @xmath100 is looser than that of prca is probably the `` finest '' bound one could accomplish in theory . ] , which requires @xmath101 .",
    "the requirement of column - wisely unit - normed dictionary ( i.e. , @xmath88 ) is purely for complying the parameter estimate of @xmath95 , which is consistent with rpca .",
    "the condition @xmath89 , i.e. , @xmath62 is a subspace of @xmath90 , is indispensable if we ask for exact recovery , because @xmath89 is implied by the equality @xmath102 .",
    "this necessary condition , together with the condition that @xmath21 is low - rank , indeed provides an elementary criterion for learning the dictionary matrix @xmath21 in lrr .",
    "figure  [ fig : demo ] presents an example , which further confirms our main result : lrr is able to avoid @xmath57 as long as @xmath98 and @xmath21 is low - rank .",
    "note that it is unnecessary for the dictionary @xmath21 to strictly satisfy @xmath103 , and lrr is actually tolerant to the `` errors '' possibly existing in the dictionary .    .",
    "in this experiment , @xmath5 is a @xmath104 rank-1 matrix with one column being @xmath105 ( i.e. , a vector of all ones ) and everything else being zero .",
    "thus , @xmath106 and @xmath107 .",
    "the sparse matrix @xmath7 is with bernoulli @xmath108 values , and its nonzero fraction is set as 5% .",
    "the dictionary is set as @xmath109 $ ] ( @xmath21 is further normalized ) , where @xmath110 is a @xmath111 random gaussian matrix ( @xmath112 is varying ) . as long as @xmath113 , lrr with @xmath114 can exactly recover @xmath5 from a grossly corrupted observation matrix @xmath8.,scaledwidth=95.0% ]    the lrr program is designed for the cases where the uncorrupted observations are noiseless . in reality this is often not true and all entries of @xmath8 can be contaminated by a small amount of noises , i.e. , @xmath115 , where @xmath116 is a matrix of dense gaussian noises . in this case",
    ", the formula of lrr need be modified to @xmath117 where @xmath118 is a parameter that measures the noise level of data .",
    "in the experiments of this paper , we consistently use @xmath119 . in the presence of dense noises , the latent matrices , @xmath5 and @xmath7 , can not be exactly restored .",
    "yet we have the following theorem to guarantee the near recovery property of the solution produced by ( please refer to appendix  [ app : proof : noisy ] for the proof ) :    [ thm : noisy ] suppose @xmath120 .",
    "let @xmath15 with svd @xmath87 be a column - wisely unit - normed dictionary matrix which satisfies @xmath89 .",
    "for any @xmath121 and some numerical constant @xmath92 , if @xmath122 then with probability at least @xmath94 , any solution @xmath97 to the lrr program with @xmath95 gives a near recovery to @xmath123 , in a sense that @xmath124 and @xmath125 .      to well handle coherent data , theorem  [ thm : noiseless ] suggests that , ideally , the dictionary matrix @xmath21 should be low - rank and satisfy @xmath98 . in certain supervised environment",
    ", this would be easy as one could use clear , well - processed training data to construct the dictionary .",
    "in unsupervised environments , however , it is challenging to purse a low - rank dictionary that can also satisfy @xmath98 , since @xmath98 is essentially some kind of `` weak '' supervision information : as long as the dictionary matrix @xmath21 is low - rank , @xmath98 forms a prior that @xmath5 is known to be contained by a low - rank subspace identified by @xmath90 .",
    "interestingly , as will be shown later , it is possible to approximate the desired dictionary even when no prior about @xmath5 is given .",
    "we shall introduce a heuristic algorithm that works distinctly better than rpca in our experiments . as can be seen from ,",
    "rpca is actually not brittle with respect to coherent data : except for the extreme case where the coherence parameters reach the upper bound @xmath54 ( or @xmath53 ) , rpca could own a valid condition ( although the condition is narrowed ) to be _ exactly _ successful even when the coherence parameters are considerably large .",
    "based on this , we propose a pretty simple algorithm , as summarized in algorithm [ alg : mr ] , to achieve a _ solid _ improvement over rpca .",
    "our idea is straightforward : we firstly obtain an estimate of @xmath5 by using rpca and then utilize the estimate to construct the dictionary matrix @xmath21 .",
    "the post - processing steps ( step 2 and step 3 ) that slightly modify the solution of rpca are designed to encourage well - conditioned dictionary , which is the favorite circumstance indicated by theorem  [ thm : noiseless ] .",
    "* input : * observed data matrix @xmath126 . * adjustable parameter : * @xmath127 .",
    "solve for @xmath83 by optimizing the rpca problem with @xmath95 .",
    "* estimate the rank of @xmath83 by @xmath128 where @xmath129 are the singular values of @xmath83 .",
    "* form @xmath130 by using the rank-@xmath131 approximation of @xmath83 .",
    "that is , @xmath132 which is solved by svd . *",
    "* construct a dictionary @xmath133 from @xmath130 by normalizing the column vectors of @xmath130 : @xmath134_{:,i}=\\frac{[\\tilde{l}_0]_{:,i}}{\\|[\\tilde{l}_0]_{:,i}\\|_2 } , i=1,\\cdots , n,\\end{aligned}\\ ] ] where @xmath135_{:,i}$ ] denotes the @xmath65th column of a matrix .",
    "solve for @xmath17 by optimizing the lrr problem with @xmath136 and @xmath95 . *",
    "output : * @xmath137 .",
    "whenever the recovery produced by rpca is already exact , the claim in theorem  [ thm : noiseless ] gives that the recovery produced by our algorithm [ alg : mr ] is exact as well .",
    "when rpca fails to exactly recover @xmath5 , the produced dictionary is still possible to satisfy the success conditions required by theorem  [ thm : noiseless ] , namely @xmath21 is low - rank and @xmath98 .",
    "this is because those conditions are weaker than @xmath138 .",
    "thus , in terms of exactly recovering @xmath5 from a given @xmath8 , the success probability of our algorithm [ alg : mr ] is greater than or equal to that of rpca .",
    "also , in a computational sense , algorithm [ alg : mr ] does not double rpca , although there are two convex programs in our algorithm .",
    "in fact , according to our simulations , usually the computational time of algorithm [ alg : mr ] is just 1.2 times as much as rpca .",
    "the reason is that , as has been explored by  @xcite , the complexity of solving the lrr problem is @xmath139 ( assume @xmath140 ) , which is much lower than that of rpca ( which requires @xmath141 ) provided that the obtained dictionary matrix @xmath21 is fairly low - rank ( i.e. , @xmath142 is small ) .",
    "one may have noticed that the procedure of algorithm [ alg : mr ] could be made iterative , i.e. , one can consider @xmath137 as a new estimate of @xmath5 and use it to further update the dictionary matrix @xmath21 , and so on .",
    "nevertheless , we empirically find that such an iterative procedure often converges within two iterations .",
    "hence , for the sake of simplicity , we do not consider the iterative strategies in this paper .",
    "the same as in rpca  @xcite , we assume that the locations of the corrupted entries are selected _ uniformly at random_. in more details , we work with the bernoulli model @xmath143 , where @xmath144 s are i.i.d .",
    "variables taking value one with probability @xmath145 and zero with probability @xmath146 , so that the expected cardinality of @xmath6 is @xmath147 . for the ease of presentation ,",
    "we assume that the signs of the nonzero entries of @xmath7 are symmetric bernoulli @xmath148 values : @xmath149_{ij}=\\left\\{\\begin{array}{ll } 1 , & \\textrm{with probability } \\frac{\\rho_0}{2},\\\\ 0 , & \\textrm{with probability } 1-\\rho_0,\\\\ -1,&\\textrm{with probability } \\frac{\\rho_0}{2}. \\end{array}\\right.\\end{aligned}\\ ] ] for general sign matrices , the same as in rpca  @xcite , our theorem  [ thm : noiseless ] can still be proved by globally placing an elimination theorem and a derandomization scheme . yet",
    "the success conditions in theorem  [ thm : noisy ] have not been proven when @xmath150 has an arbitrary distribution , because the elimination theorem does not hold in the noisy case .",
    "the following two lemmas are well - known and will be used multiple times in the proof .",
    "[ app : lem : basic:1]for any matrix @xmath25 , the following holds :    * let the svd of @xmath25 be @xmath41",
    ". then we have @xmath151 .",
    "* let the support set of @xmath25 be @xmath152 .",
    "then we have @xmath153 .",
    "[ lem : basic:2 ] for any matrices @xmath25 and @xmath116 of consistent sizes , @xmath154      first of all , we would like to prove that the sparse matrix @xmath7 does not locate in the column space of the dictionary @xmath21 , i.e. , @xmath155 or @xmath156 as equal . provided that @xmath15 is fairly low - rank , the analysis in  @xcite gives that @xmath157 holds with high probability for any @xmath158 , where @xmath159 denotes the linear space given by @xmath160 .",
    "since @xmath161 and @xmath162 , it is natural to anticipate that @xmath163 is smaller than 1 with high probability .",
    "the difference is that we only need the first coherence parameter @xmath56 to finish the proof .",
    "following the techniques in  @xcite , we have the following lemma to bound the operator norm of @xmath164 .",
    "[ lem : papo ] suppose @xmath165 with @xmath166 . then for any @xmath158 , @xmath167 holds with probability at least @xmath94 , provided that @xmath168    for any matrix @xmath25 , we have @xmath169 and so @xmath170 which gives @xmath171 note that the frobenius norm of a matrix is equivalent to the vector @xmath172 norm , while considering the matrix as a long vector . in that sense",
    ", we have @xmath173 the definition of @xmath174 gives @xmath175 then by using the results in  @xcite and following the proof procedure of  @xcite , we have that @xmath176 holds with probability at least @xmath177 for some numerical constants @xmath178 and @xmath179 . for any @xmath158 , setting @xmath180 and @xmath181 gives that @xmath182 holds with probability at least @xmath94 , provided that @xmath183 .    by @xmath184 and the triangle inequality , @xmath185 finally , the fact @xmath186 completes the proof .",
    "while the above lemma implies that @xmath187 , we often need to bound the sup - norm of @xmath188 .",
    "the next lemma will show that , when the signs of the matrix entries are independent symmetric bernoulli variables , the sup - norm could be arbitrarily small .",
    "[ lem : inf : pas0 ] suppose @xmath189 is a symmetric linear projection with @xmath190 , and @xmath191 is a random sign matrix with i.i.d .",
    "entries distributed as @xmath192_{ij}=\\left\\{\\begin{array}{ll } 1 , & \\textrm{with probability } \\frac{1}{2},\\\\ -1,&\\textrm{with probability } \\frac{1}{2}. \\end{array}\\right.\\end{aligned}\\ ] ] for any @xmath158 , @xmath193 holds with high probability as long as @xmath168    let @xmath194_{ij}$ ] and @xmath195 then it can be seen that each entry of @xmath196 is a sum of independent random variables : @xmath197_{i_1j_1 } & = & \\sum_{i , j}y_{ij } \\textrm { with } \\\\",
    "y_{ij } & = & \\delta_{ij}\\xi_{ij}\\langle\\mathcal{p}_{u_a}\\mathcal{p}\\mathcal{p}_{u_a}(e_ie_j^t),e_{i_1}e_{j_1}^t\\rangle.\\end{aligned}\\ ] ] note here that the variables @xmath144 s are fixed and the randomness comes from @xmath198 s .",
    "it is easy to see that @xmath199 .",
    "we have @xmath200 we also have @xmath201 then the proof is finished by using bernstein s inequality , which states that for a collection of uniformly bounded independent random variables @xmath202 with @xmath203 , @xmath204 thus we have @xmath205_{i_1j_1}|>\\epsilon)&\\leq&\\exp\\left(-\\frac{0.5\\epsilon^2}{4\\frac{\\mu_1(a)r_a}{m}+2\\epsilon\\frac{\\mu_1(a)r_a}{3m}}\\right)\\\\ & \\leq & \\exp\\left(-\\frac{1.5\\epsilon^2m}{(12 + 2\\epsilon)\\mu_1(a)r_a}\\right).\\end{aligned}\\ ] ] by union bound , @xmath206 provided that @xmath183 with @xmath207 .",
    "it remains to prove theorem  [ thm : noiseless ] by two steps :    * * dual conditions : * identify the sufficient conditions for @xmath208 to be the unique optimal solution to the lrr problem .",
    "* * dual certificates : * show that the dual conditions can be satisfied , that is to say , construct the dual certificates .",
    "the dual conditions are presented in the following lemma .",
    "[ app : lem : dual ] let the svd of @xmath209 be @xmath210 .",
    "suppose @xmath89 and @xmath155 .",
    "then @xmath211 is the unique optimal solution to if there exists a matrix @xmath212 that obeys @xmath213    by standard convexity arguments  @xcite , @xmath211 is an optimal solution to if @xmath214 note that @xmath215 .",
    "furthermore , ( b ) and ( c ) imply that @xmath216 .",
    "thus , the conditions ( a ) , ( b ) and ( c ) are sufficient to conclude that @xmath211 is an optimal ( but may not be unique ) solution to .",
    "next , we shall consider a feasible perturbation @xmath217 and show that the objective strictly increases whenever @xmath218 .",
    "by @xmath219 , @xmath220 let @xmath221 .",
    "then by lemma  [ app : lem : basic:1 ] , @xmath222 is a subgradient of @xmath223 . by the convexity of the nuclear norm and @xmath224 norm , @xmath225 by @xmath226 , @xmath227 and the assumption @xmath228",
    ", we have @xmath229 .",
    "thus , we have @xmath230 strictly holds unless @xmath231 . as long as @xmath232 ,",
    "theorem 4.1 of  @xcite gives that @xmath233 strictly holds unless @xmath234 .",
    "hence , @xmath235 is the unique optimal solution to the lrr problem .      to construct a matrix @xmath212 which satisfies the dual conditions listed in lemma  [ app : lem : dual ]",
    ", we need the inverse of @xmath236 .",
    "the following lemma shows that @xmath237 is well defined and has a small operator norm .",
    "[ app : lem : inverse ] if @xmath238 , then the operator @xmath239 is an injection from @xmath240 to @xmath240 , and its inverse operator is given by @xmath241    let @xmath242 .",
    "by @xmath243 , we have that @xmath244 is well defined and has an operator norm not larger than @xmath245 .",
    "note that @xmath246 thus for any @xmath247 the following holds : @xmath248    the next lemma completes the construction of the dual certificates .",
    "[ app : lem : f ] let @xmath249 where @xmath28 and @xmath29 are the left and right singular vectors of @xmath250 , respectively .",
    "if the conditions stated in are obeyed , then the above @xmath212 using @xmath95 satisfies ( with high probability ) the dual conditions ( a ) , ( b ) and ( c ) in lemma [ app : lem : dual ] .",
    "* ( a ) : * we have @xmath251 where the last equality follows from theorem 4.3 of  @xcite . + * ( b ) : * it is easy to verify that @xmath252 . + * ( c ) : * let @xmath253 and @xmath254 , where @xmath255 in the following , we shall bound the sup - norm of each term individually .",
    "the proof for @xmath256 needs to access the distribution of @xmath150 .",
    "when the signs of the nonzero entries of @xmath7 are bernoulli @xmath148 values , i.e. , @xmath257 with @xmath258 being a random sign matrix as in lemma [ lem : inf : pas0 ] , we have indeed proven @xmath259 provided that @xmath260 , which follows from the condition of @xmath261 .",
    "so it remains to prove that @xmath262 this seems easy because we could set @xmath263 . nevertheless , to prove our main result , theorem  [ thm : noiseless ] , with @xmath95 ( which is a good choice in general ) , one essentially needs to establish an accurate bound for @xmath264 .",
    "even more , the golfing scheme widely adopted by previous literatures is indeed not easy to work with in our setting .",
    "fortunately , we can make use of the particular structure of @xmath265 and devise a simple approach to accomplish the proof .",
    "our idea is based on the following observation : for any matrix @xmath196 , the @xmath266th entry of the matrix @xmath267 is @xmath268_{i_1j_1}=\\sum_{i , j}\\delta_{ij}[q]_{ij}\\langle{}e_ie_j^t,\\mathcal{p}_{u_a}(e_{i_1}e_{j_1}^t)\\rangle=\\sum_{i}\\delta_{ij_1}[q]_{ij_1}[u_au_a^t]_{ii_1},\\end{aligned}\\ ] ] which reveals the fact that the absolute value of @xmath269_{i_1j_1}$ ] closely relates to the length of the @xmath270th column of @xmath271 .",
    "so it may not lose much accuracy to use the relaxation of @xmath272 .",
    "for the sake of consistency , we use the @xmath48 norm to define as follows the third coherence parameter of @xmath5 , associating with a dictionary matrix @xmath21 :    for @xmath2 of rank @xmath59 , its third coherence parameter , associating with a non - orthonormal , column - wisely unit - normed dictionary matrix @xmath21 which also satisfies @xmath89 , is defined as @xmath273 where @xmath28 and @xmath29 are the left and right singular vectors of @xmath250 , respectively , and @xmath274 is the condition number of the matrix @xmath21 .    .",
    "( a ) @xmath275 and @xmath276 are fixed , while @xmath54 is varying .",
    "( b ) @xmath277 and @xmath276 are fixed , while @xmath53 is varying .",
    "( c ) @xmath276 is fixed , while @xmath53 and @xmath54 are varying ( @xmath140 ) .",
    "( d ) @xmath278 are fixed , while @xmath59 is varying . in these experiments ,",
    "the dictionary @xmath21 is @xmath279 with normalized columns , where @xmath280 is an @xmath58 random gaussian matrix .",
    "the numbers shown in above figures are averaged from 10 random trials.,scaledwidth=98.0% ]    figure  [ fig : u3a ] demonstrates some properties about this particular coherence parameter , @xmath281 .",
    "it can be seen that @xmath281 is approximately a numerical constant equaling to 1 , as long as the rank is not too high such that the dictionary matrix @xmath21 is well - conditioned .    by lemma  [ lem : papo ] , @xmath282 . by , @xmath283 thus we have @xmath284 by @xmath285 and setting @xmath286 , @xmath287 since @xmath288 ( provided that @xmath21 is well - conditioned ) , we claim @xmath95 for the sake of simplicity may not be the `` best '' choice . ] .    now the dual condition @xmath227 is proved by @xmath289 we claim @xmath261 instead of @xmath290 because lemma  [ lem : inf : pas0 ] requires @xmath190 , which follows from @xmath261 .",
    "our main result , theorem  [ thm : noiseless ] , is useful in both supervised and unsupervised environments . for the fair of comparison , in the experiments of this paper",
    "we shall focus on demonstrating the superiorities of our unsupervised algorithm  [ alg : mr ] over rpca .",
    "we first verify the effectiveness of our algorithm [ alg : mr ] on randomly generated matrices .",
    "we generate a collection of @xmath291 data matrices according to the model of @xmath292 : @xmath6 is a support set chosen at random ; @xmath5 is created by sampling 200 data points from each of 5 randomly generated subspaces , and its values are normalized such that @xmath293 ; @xmath7 is consisting of random values from bernoulli @xmath148 .",
    "the dimension of each subspace varies from 1 to 20 with step size 1 , and thus the rank of @xmath5 varies from 5 to 100 with step size 5 .",
    "the fraction @xmath294 varies from 2.5% to 50% with step size 2.5% . for each pair of rank and support size @xmath295",
    ", we run 10 trials , resulting in a total of 4000 ( @xmath296 ) trials .",
    "vs rpca on recovering randomly generated matrices , both using @xmath95 .",
    "a curve shown in the third subfigure is the boundary for a method to be successful  the recovery is successful for any pair @xmath297 that locates below the curve . here , the success is in a sense that @xmath298 , where @xmath83 denotes an estimate of @xmath5.,scaledwidth=95.0% ]    figure  [ fig : recover ] compares our algorithm  [ alg : mr ] to rpca , both using @xmath95 .",
    "it can be seen that the learnt dictionary matrix works distinctly better than the identity dictionary adopted by rpca .",
    "namely , the success area ( i.e. , the area of the white region ) of our algorithm is 46% wider than that of rpca !",
    "one may have noticed that rpca owns a region to be exactly successful .",
    "this is because in these experiments the coherence parameters are not too large , namely @xmath299 and @xmath300 .",
    "whenever @xmath57 reaches the upper bound @xmath54 , e.g. , the example shown in figure  [ fig : demo ] , the success region of rpca will vanish .",
    "we now experiment with 11 additional sequences attached to the hopkins155  @xcite database . in those sequences , about 10% of the entries in the data matrix of trajectories are unobserved ( i.e. , missed ) due to visual occlusion .",
    "we replace each missing entry with a number from bernoulli @xmath148 , resulting in a collection of corrupted trajectory matrices for evaluating the effectiveness of matrix recovery algorithms .",
    "we perform subspace clustering on both the corrupted trajectory matrices and the recovered versions , and use the clustering error rates produced by existing subspace clustering methods as the evaluation metrics .",
    "we consider three state - of - the - art subspace clustering methods : shape interaction matrix ( sim )  @xcite , low - rank representation with @xmath22  @xcite ( which is referred to as `` lrrx '' ) and sparse subspace clustering ( ssc )  @xcite .    .clustering error rates ( % ) on 11 corrupted motion sequences . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]",
    "when the data points are sampled from a low - rank subspace _ uniformly at random _ , it has been proven by  @xcite that the first and second coherence parameters are bounded .",
    "namely , @xmath301 and @xmath302 for some numerical constant @xmath303 independent of the characteristics of @xmath5 . although correct , such a property is not enough to interpret the phenomenon that the coherence parameters increase with the cluster number underlying @xmath5 .",
    "hence , it is necessary to establish a more accurate rule to characterize the coherence parameters . through extensive experiments ,",
    "we find that the first and second coherence parameters actually follow the well - known zipf s law . more precisely ,",
    "if the data points ( which form the column vectors of @xmath2 ) are uniformly sampled from a @xmath59-dimensional subspace , then , roughly , the logarithm of coherence is inversely proportional to the logarithm of @xmath304 .",
    "that is , @xmath305 where @xmath306 and @xmath307 are two constants .",
    "the results in figure  [ fig : zipf ] verify the above zipf s law .",
    "note that the zipf s law can also induce the boundedness property proved by  @xcite .",
    "namely , approximately gives that @xmath308 and @xmath309 .",
    "the above zipf s law suggests that the coherence must be inversely proportional to the rank of data .",
    "this is intuitively interpretable .",
    "let @xmath310_{ij}$ ] and @xmath311 .",
    "then it can be seen that @xmath312 is the squared euclidean length of the first @xmath59 components of a unit vector distributed on the @xmath53-dimensional unit sphere . with these notations",
    ", it can be seen that @xmath56 is the largest order statistic of @xmath312 divided by the expectation of @xmath312 : @xmath313 now it is unfolded that the first ( and second ) coherence parameter of a matrix with rank @xmath59 is actually some kind of uncertainty of the first @xmath59 components of a unit - normed , @xmath53-dimensional random vector .",
    "thus if @xmath314 ( i.e. , @xmath5 is full rank ) , then the uncertainty vanishes and @xmath106 .",
    "similarly if @xmath315 , the uncertainty measured by @xmath316 is as high as that of a single random number .",
    "the zipf s law is useful , because it provides us a trackable approach to estimate the coherence parameters when the data points are _ not _ uniformly sampled , as will be shown in the next section .     and width @xmath54 of @xmath5 are random integers from the range 100 to 1000",
    ". the rank of @xmath5 is set as @xmath317 , where @xmath318 is a random number from the interval ( 0.1,0.9 ) . for the clarity of viewing ,",
    "we randomly select 10,000 out of one million simulation results to show .",
    "for all one million simulations , we have calculated that @xmath319 and @xmath320.,scaledwidth=88.0% ]      ideally , if the values in @xmath62 and @xmath67 are perfectly spreading out , namely @xmath321_{ij}=[u_0]_{i_1j_1}$ ] and @xmath322_{ij}=[v_0]_{i_1j_1 } , \\forall{}i , j , i_1,j_1 $ ] , then @xmath323 . however",
    ", this is unlikely for @xmath80 to happen , as it is provable that the row projector @xmath324 , which is also known as shape interaction matrix ( sim ) in subspace clustering , measures the subspace membership of the data points  @xcite .",
    "more precisely , if the data points in @xmath5 are sampled from @xmath85 number of _ independent _ subspaces , saying @xmath325 $ ] , where @xmath326 with svd @xmath327 is a matrix of data points from the @xmath65th subspace , then @xmath67 is equivalent to a block - diagonal matrix that has nonzero entries only on @xmath85 number of blocks : @xmath328.\\end{aligned}\\ ] ] in this case , it is demonstrable that the second coherence parameter @xmath80 depends on the cluster number @xmath85 . for the convenience of analysis , we assume that the dimensions of all subspaces are equal , i.e. , @xmath329 , and the sampling in each subspace is _",
    "uniform_. then the zipf s law gives @xmath330 where @xmath85 is the cluster number .",
    "hence , approximately , the second coherence parameter @xmath80 will increase with the cluster number underlying @xmath5 .",
    "let @xmath97 be an optimal solution to .",
    "denote @xmath331 , @xmath332 and @xmath333 .",
    "then we have @xmath334 provided that @xmath335 , the proof process of lemma  [ app : lem : f ] shows that @xmath336 by the optimality of @xmath97 , @xmath337 which leads to @xmath338 hence , @xmath339 by @xmath340 , @xmath341 where @xmath253 .",
    "by @xmath190 , @xmath342",
    "data matrix @xmath8 , dictionary matrix @xmath21 , parameter @xmath127 . @xmath343 .",
    "alternating minimization : * 1.1 . * fix the others and update @xmath344 by @xmath345 * 1.2 .",
    "* fix the others and update @xmath18 by @xmath346 * 1.3 .",
    "* fix the others and update @xmath347 by @xmath348 * 2 .",
    "* update the lagrange multipliers and the parameter @xmath349",
    "@xmath350    in this work , we use the exact alm method to solve the optimization problem .",
    "we first convert to the following equivalent problem : @xmath351 this problem can be solved by the alm method , which minimizes the following augmented lagrange function : @xmath352 with respect to @xmath344 , @xmath18 and @xmath347 , respectively , by fixing the other variables , and then updating the lagrange multipliers @xmath353 and @xmath110 .",
    "algorithm [ alg : alm : lrr ] summarizes the whole procedure of the optimization procedure .",
    "martin fischler and robert bolles .",
    "random sample consensus : a paradigm for model fitting with applications to image analysis and automated cartography .",
    "_ communications of the acm _ , 240 ( 6):0 381395 , 1981 .        qifa ke and takeo kanade .",
    "robust l@xmath354 norm factorization in the presence of outliers and missing data by alternative convex programming . in _ ieee conference on computer vision and pattern recognition _ , pages 739746 , 2005 .",
    "guangcan liu , zhouchen lin , xiaoou tang , and yong yu .",
    "unsupervised object segmentation with a hybrid graph model ( hgm ) .",
    "_ ieee transactions on pattern analysis and machine intelligence _ , 320 ( 5):0 910924 , 2010 .",
    "issn 0162 - 8828 .",
    "guangcan liu , zhouchen lin , shuicheng yan , ju  sun , yong yu , and yi  ma .",
    "robust recovery of subspace structures by low - rank representation .",
    "_ ieee transactions on pattern analysis and machine intelligence _ , 350 ( 1):0 171184 , 2013 .",
    "yigang peng , arvind ganesh , john wright , wenli xu , and yi  ma .",
    "rasl : robust alignment by sparse and low - rank decomposition for linearly correlated images . _ ieee transactions on pattern analysis and machine intelligence _ , 340 ( 11):0 22332246 , 2012 ."
  ],
  "abstract_text": [
    "<S> the recently established rpca  @xcite method provides us a convenient way to restore low - rank matrices from grossly corrupted observations . while elegant in theory and powerful in reality </S>",
    "<S> , rpca may be not an ultimate solution to the low - rank matrix recovery problem . </S>",
    "<S> indeed , its performance may not be perfect even when the data is strictly low - rank . </S>",
    "<S> this is because rpca prefers incoherent data , which , however , could be inconsistent with some natural structures of data . as a typical example , consider the clustering structure which is ubiquitous in modern applications . </S>",
    "<S> as the number of cluster grows , the coherence parameters of data keep increasing , and accordingly , the recovery performance of rpca degrades . </S>",
    "<S> we show that it is possible for low - rank representation ( lrr )  @xcite to overcome the challenges raised by coherent data , as long as the dictionary in lrr is configured appropriately . </S>",
    "<S> namely , we mathematically prove that if the dictionary itself is low - rank then lrr can avoid the coherence parameters which have potential to be large . </S>",
    "<S> this provides an elementary principle for dealing with coherent data and naturally leads to a practical algorithm for obtaining proper dictionaries in unsupervised environments . </S>",
    "<S> our extensive experiments on randomly generated matrices and real motion sequences show promising results . </S>"
  ]
}