{
  "article_text": [
    "estimating covariance matrices is of fundamental importance for an abundance of statistical methodologies . nowadays",
    ", the advance of new technologies has brought massive high - dimensional data into various research fields , such as fmri imaging , web mining , bioinformatics , climate studies and risk management , and so on .",
    "the usual sample covariance matrix is optimal in the classical setting with large samples and fixed low dimensions @xcite , but it performs very poorly in the high - dimensional setting @xcite . in the recent literature ,",
    "regularization techniques have been used to improve the sample covariance matrix estimator , including banding @xcite , tapering @xcite and thresholding @xcite . banding or tapering is very useful when the variables have a natural ordering and off - diagonal entries of the target covariance matrix decays to zero as they move away from the diagonal . on the other hand ,",
    "thresholding is proposed for estimating permutation - invariant covariance matrices .",
    "thresholding can be used to produce consistent covariance matrix estimators when the true covariance matrix is bandable @xcite . in this sense",
    ", thresholding is more robust than banding / tapering for real applications .",
    "let @xmath1 be the sample covariance matrix .",
    "@xcite defined the general thresholding covariance matrix estimator as @xmath2 where @xmath3 is the generalized thresholding function .",
    "the generalized thresholding function covers a number of commonly used shrinkage procedures , e.g. the hard thresholding @xmath4 , the soft thresholding @xmath5 , the smoothly clipped absolute deviation thresholding @xcite and the adaptive lasso thresholding @xcite .",
    "consistency results and explicit rates of convergence have been obtained for these regularized estimators in the literature , e.g. @xcite , @xcite , @xcite , @xcite . the recent work by @xcite has established the minimax rate of convergence under the @xmath0 matrix norm over a fairly wide range of classes of large covariance matrices , where the thresholding estimator is shown to be minimax rate optimal .",
    "the existing theoretical and empirical results show no clear favoritism to a particular thresholding rule . in this paper",
    "we focus on the soft - thresholding because it can be formulated as the solution of a convex optimization problem .",
    "let @xmath6 be the frobenius norm and @xmath7 be the element - wise @xmath0-norm of all non - diagonal elements .",
    "then the soft - thresholding covariance estimator is equal to [ soft - thresholding ] = _  2-_n_f^2+||_1 .    however , there is no guarantee that the thresholding estimator is always positive definite .",
    "although the positive definite property is guaranteed in the asymptotic setting with high probability , the actual estimator can be an indefinite matrix , especially in real data analysis . to illustrate this issue",
    ", we consider the michigan lung cancer gene - expression data @xcite which have @xmath8 tumor samples from patients with lung adenocarcinomas and @xmath9 gene expression values for each sample .",
    "more details about this dataset are referred to @xcite and @xcite .",
    "we randomly choose @xmath10 genes ( @xmath11 ) , and obtain the soft - thresholding sample correlation matrix for these genes .",
    "we repeat the process ten times for @xmath12 and @xmath13 respectively , and each time the thresholding parameter @xmath14 is selected via the 5-fold cross validation .",
    "we found that none of the soft - thresholding estimators would become positive definite for both @xmath12 and @xmath13 . on average , there exist @xmath15 and @xmath16 negative eigenvalues for the soft - thresholding estimator for @xmath12 and @xmath17 , respectively .",
    "figure [ plot : michigan_data ] displays the @xmath18 smallest eigenvalues for @xmath12 and the @xmath19 smallest eigenvalues for @xmath17 .",
    "[ plot : michigan_data ]    to deal with the indefiniteness , one possible solution is to utilize the eigen - decomposition of @xmath20 , and project @xmath20 into the convex cone @xmath21 .",
    "assume that @xmath20 has the eigen - decomposition @xmath22 , and then a positive semidefinite estimator @xmath23 can be obtained by setting @xmath24 .",
    "however , this strategy does not work well for sparse covariance matrix estimation , because the projection destroys the sparsity pattern of @xmath20 .",
    "consider the michigan data again .",
    "after semidefinite projection , the soft - thresholding estimator has no zero entry .    in order to simultaneously achieve sparsity and positive semidefiniteness ,",
    "a natural solution is to add the positive semidefinite constraint to ( [ soft - thresholding ] ) .",
    "consider the following constrained @xmath0 penalization problem [ soft-thresholding.plus0 ] ^+=_0  -_n_f^2/2+||_1 .",
    "note that the solution to ( [ soft-thresholding.plus0 ] ) could be positive semidefinite . to obtain a positive definite covariance estimator",
    ", we can consider the positive definite constraint @xmath25 for some arbitrarily small @xmath26 .",
    "then the modified @xmath27 is always positive definite . in this work ,",
    "we focus on solving the positive definite @xmath27 as follows [ soft-thresholding.plus ] ^+= _  -_n_f^2/2+||_1 .    despite its natural motivation , ( [ soft-thresholding.plus ] )",
    "is actually a very challenging optimization problem due to the positive semidefinite constraint . to our best knowledge ,",
    "the first attempt for solving ( [ soft-thresholding.plus ] ) was recently proposed by @xcite who added the log - determinant barrier function to ( [ soft-thresholding.plus ] ) : [ rothman ] ^+=_0  -_n_f^2/2-()+||_1 , where the barrier parameter @xmath28 is a small positive constant , say @xmath29 . from the optimization viewpoint , ( [ rothman ] ) is similar to the graphical lasso criterion @xcite which also has a log - determinant part and the element - wise @xmath0-penalty .",
    "@xcite derived an iterative procedure to solve ( [ rothman ] ) .",
    "@xcite s proposal is based on heuristic arguments and its convergence property is unknown .",
    "in this paper we present an alternating direction algorithm for solving ( [ soft-thresholding.plus ] ) directly .",
    "numerical examples show that our algorithm is much faster than the log - barrier method .",
    "we further prove the convergence properties of our algorithm and discuss the statistical properties of the positive - definite constrained @xmath0 penalized covariance estimator .",
    "we use an alternating direction method to solve ( [ soft-thresholding.plus ] ) directly .",
    "the alternating direction method is closely related to the operator - splitting method that has a long history back to 1950s for solving numerical partial differential equations , see e.g. , @xcite .",
    "recently , the alternating direction method has been revisited and successfully applied to solving large scale problems arising from different applications .",
    "for example , @xcite introduced the alternating linearization methods to efficiently solve the graphical lasso optimization problem .",
    "we refer to @xcite for more details on operator - splitting and alternating direction methods .    in the sequel , we propose an alternating direction method to solve the @xmath0 penalized covariance matrix estimation problem ( [ soft-thresholding.plus ] ) under the positive - semidefinite constraint .",
    "we first introduce a new variable @xmath30 and an equality constraint as follows [ soft-thresholding.alm ] ( ^+,^+)= _ ,  \\{-_n_f^2/2+||_1 :  = ,  } . the solution to ( [ soft-thresholding.alm ] )",
    "gives the solution to ( [ soft-thresholding.plus ] ) . to deal with the equality constraint in ( [ soft-thresholding.alm ] )",
    ", we shall minimize its augmented lagrangian function for some given penalty parameter @xmath31 , i.e. [ auglagfunction ] l ( , ; ) = -_n_f^2/2+||_1 - , -+-_f^2/(2 ) , where @xmath32 is the lagrange multiplier .",
    "we iteratively solve [ subproblem1 ] ( ^i+1,^i+1)=l(,;^i)and then update the lagrangian multiplier @xmath33 by @xmath34 for ( [ subproblem1 ] ) we do it by alternatingly minimizing @xmath35 with respect to @xmath30 and @xmath36 .    to sum up , the entire algorithm proceeds as follows :    : :    for @xmath37 , solve the following three    sub - problems sequentially till convergence   ^i+1 = _",
    "l(,^i;^i )    [ alg : adm-1 ]   ^i+1 = _",
    "l(^i+1,;^i ) [ alg : adm-2 ]   ^i+1=^i-(^i+1-^i+1)/.    [ alg : adm-3 ]    to further simplify the alternating direction algorithm , we derive the closed - form solutions for ( [ alg : adm-1])([alg : adm-2 ] ) . consider the @xmath30 step .",
    "define @xmath38 as the projection of a matrix @xmath39 onto the convex cone @xmath40 .",
    "assume that @xmath39 has the eigen - decomposition @xmath41 , and then @xmath38 can be obtained as @xmath42 .",
    "then the @xmath30 step can be analytically solved as follows ^i+1 & = & _ l(,^i;^i ) + & = & _ -^i,+-^i_f^2/(2 ) + & = & ( ^i+^i)_+ .",
    "next , define an entry - wise soft - thresholding rule for all the non - diagonal elements of a matrix @xmath39 as @xmath43 with @xmath44 then the @xmath36 step has a closed - form solution given below ^i+1 & = & _ l(^i+1,;^i ) + & = & _ -_n_f^2/2+||_1 + ^i,+-^i+1_f^2/(2 ) + & = & \\{((_n-^i)+^i+1 , ) } /(1 + ) .",
    "algorithm 1 shows the complete details of our alternating direction method for ( [ soft-thresholding.plus ] ) . in section 4",
    "we provide the convergence analysis of algorithm 1 and prove that algorithm 1 always converges to the optimal solution of from any starting point .",
    "[ alg : adm-1-formal ]    1 .",
    "input : @xmath31 , @xmath45 and @xmath46 .",
    "2 .   iterative alternating direction augmented lagrangian step : for the @xmath47-th iteration 1 .   solve @xmath48 ; 2 .",
    "solve @xmath49 ; 3 .",
    "update @xmath50 .",
    "3 .   repeat the above cycle till convergence .",
    "in our implementation we use the soft - thresholding estimator as the initial value for both @xmath51 and @xmath45 , and we set @xmath46 as a zero matrix .",
    "the value for @xmath31 is 2 . before invoking algorithm 1",
    ", we always check whether the soft - thresholding estimator is positive definite .",
    "if yes , then the soft - threhsolding estimator is the final solution to ( [ soft-thresholding.plus ] ) .",
    "before delving into theoretical analysis of the algorithm and the resulting estimator , we first use simulation to show the competitive performance of our proposal . in all examples",
    "we standardize the variables to have zero mean and unit variance . in each simulation model",
    ", we generated @xmath52 independent datasets , each with @xmath53 independent @xmath10-variate random vectors from the multivariate normal distribution with mean @xmath54 and covariance matrix @xmath55 for @xmath56 .",
    "we considered two covariance models with different sparsity patterns :    model 1 : : :    @xmath57 .",
    "model 2 : : :    partition the indices @xmath58 into    @xmath59 non - overlapping subsets of equal size , and let    @xmath60 denote the maximum index in @xmath61 .",
    "@xmath62    model 1 has been used in @xcite and @xcite , and model 2 is similar to the overlapping block diagonal design used in @xcite .",
    "first , we compare the run times of our estimator @xmath63 with the log - barrier estimator @xmath64 by @xcite . as shown in table [",
    "sim : timing ] , our method is much faster than the log - barrier method .    in what follows ,",
    "we compare the performance of @xmath63 , @xmath64 and the soft - thresholding estimator @xmath20 .",
    "for all three regularized estimators , the thresholding parameter was chosen by @xmath65-fold cross - validation @xcite .",
    "the estimation performance is measured by the average losses under both the frobenius norm and the spectral norm .",
    "the selection performance is examined by the false positive rate @xmath66 and the true positive rate @xmath67 moreover , we compare the average number of negative eigenvalues and the percentage of positive - definiteness to check the positive - definiteness .",
    "table [ stcor : model1 ] and table [ stcor : model2 ] show the average metrics over 100 replications .",
    "the soft - thresholding estimator @xmath20 is positive definite in 19 or fewer out of 100 simulation runs , while @xmath63 and @xmath64 can always guarantee a positive - definite estimator . the larger the dimension , the less likely for the soft - thresholding estimator to be positive definite . in terms of estimation ,",
    "both @xmath63 and @xmath64 are more accurate than @xmath20 . as for the selection performance , @xmath63 and @xmath64 achieve a slightly better true positive rate than @xmath20 .",
    "overall , @xmath63 is the best among all three regularized estimators .      to demonstrate our proposal",
    "we further consider two gene expression datasets : one from a small round blue - cell tumors microarray experiment @xcite and the other one from a cardiovascular microarray study @xcite .",
    "the first dataset has 64 training tissue samples with four types of tumors ( 23 ews , 8 bl - nhl , 12 nb , and 21 rms ) , and 6567 gene expression values for each sample .",
    "we applied the pre - filtering step used in khan et al .",
    "( 2001 ) and then picked the top 40 and bottom 160 genes based on the f - statistic as done in @xcite .",
    "the second dataset has 63 subjects with 44 healthy controls and 19 cardiovascular patients , and 20426 genes measured for each subject .",
    "we used the f - statistic to pick the top 50 and bottom 150 genes . by doing so",
    ", it is expected that there is weak dependence between the top and the bottom genes .",
    "we considered the soft - thresholding estimator @xcite , the log - barrier estimator @xcite and our estimator .",
    "for all three estimators , the thresholding parameter was chosen by 5-fold cross validation .",
    "eigenvalues of all three regularized estimators for the small round blue - cell data ( a ) and the cardiovascular data ( b ) : @xmath20 ( solid ) , @xmath63 ( dashed ) and @xmath64 ( dotted).,title=\"fig : \" ] ( a )     eigenvalues of all three regularized estimators for the small round blue - cell data ( a ) and the cardiovascular data ( b ) : @xmath20 ( solid ) , @xmath63 ( dashed ) and @xmath64 ( dotted).,title=\"fig : \" ] ( b )    as evidenced in plot [ real : eigen ] , the soft - thresholding estimator yields an indefinite matrix for both real examples whereas the other two regularized estimators guarantee the positive - definiteness .",
    "the soft - thresholding estimator contains @xmath68 negative eigenvalues in the small round blue - cell data , and @xmath69 negative eigenvalues in the cardiovascular data . regularized correlation matrix estimation has a natural application in clustering when the dissimilarity measure is constructed using the correlation among features .",
    "for both datasets we did hierarchical clustering using the three regularized estimators .",
    "the heat maps are shown in figure [ real : heatmap ] in which the estimated sparsity pattern well matches the expected sparsity pattern .",
    "( a1 , b1 ) , @xmath63 ( a2 , b2 ) and @xmath64 ( a3 , b3 ) .",
    "the genes are ordered by hierarchical clustering using the estimated correlations.,title=\"fig : \" ] ( a1 )     ( a1 , b1 ) , @xmath63 ( a2 , b2 ) and @xmath64 ( a3 , b3 ) .",
    "the genes are ordered by hierarchical clustering using the estimated correlations.,title=\"fig : \" ] ( a2 )     ( a1 , b1 ) , @xmath63 ( a2 , b2 ) and @xmath64 ( a3 , b3 ) .",
    "the genes are ordered by hierarchical clustering using the estimated correlations.,title=\"fig : \" ] ( a3 )     ( a1 , b1 ) , @xmath63 ( a2 , b2 ) and @xmath64 ( a3 , b3 ) .",
    "the genes are ordered by hierarchical clustering using the estimated correlations.,title=\"fig : \" ] ( b1 )     ( a1 , b1 ) , @xmath63 ( a2 , b2 ) and @xmath64 ( a3 , b3 ) .",
    "the genes are ordered by hierarchical clustering using the estimated correlations.,title=\"fig : \" ] ( b2 )     ( a1 , b1 ) , @xmath63 ( a2 , b2 ) and @xmath64 ( a3 , b3 ) .",
    "the genes are ordered by hierarchical clustering using the estimated correlations.,title=\"fig : \" ] ( b3 )    finally , we compared the average run times over @xmath65 cross validations for both @xmath63 and @xmath64 , as shown in table [ real : timing ] .",
    "it is obvious that our proposal is much more efficient .",
    "in this section , we prove that the sequence @xmath70 produced by the alternating direction method ( algorithm 1 ) converges to @xmath71 , where @xmath72 is an optimal solution of and @xmath73 is the optimal dual variable .",
    "this automatically implies that algorithm 1 gives an optimal solution of .",
    "we define some necessary notation for ease of presentation .",
    "let @xmath74 be a @xmath75 by @xmath75 matrix defined as @xmath76 define the norm @xmath77 as @xmath78 and the corresponding inner product @xmath79 as @xmath80 . before we give the main theorem about the global convergence of algorithm 1 , we need the following lemma .    [",
    "lem : fejer - monotone ] assume that @xmath72 is an optimal solution of and @xmath73 is the corresponding optimal dual variable associated with the equality constraint @xmath81",
    ". then the sequence @xmath82 produced by algorithm 1 satisfies [ lem : conclusion - eq ] u^i - u^*_g^2 - u^i+1-u^*_g^2 u^i - u^i+1_g^2 , where @xmath83 and @xmath84 .",
    "now we are ready to give the main convergence result of algorithm 1 .",
    "[ the : main - convergence ] the sequence @xmath82 produced by algorithm 1 from any starting point converges to an optimal solution of .",
    "define @xmath45 as the true covariance matrix for the observations @xmath85 , and define the active set of @xmath86 as @xmath87 with the cardinality @xmath88 .",
    "denote by @xmath89 the hadamard product @xmath90 .",
    "define @xmath91 as the maximal true variance in @xmath45 .",
    "[ theorem : stat - convergence ] assume that the true covariance matrix @xmath45 is positive definite .",
    "* under the exponential - tail condition that for all @xmath92 and @xmath93 @xmath94 we also assume that @xmath95 . for any @xmath96",
    ", we pick the thresholding parameter as @xmath97 where @xmath98 and @xmath99 with probability at least @xmath100 , we have @xmath101 * under the polynomial - tail condition that for all @xmath102 , @xmath103and @xmath93 @xmath104 we also assume that @xmath105 for some @xmath106 . for any @xmath96 ,",
    "we pick the thresholding parameter as @xmath107 with probability at least @xmath108 , we have @xmath101    define @xmath109 and assume that @xmath110 is bounded by a fixed constant , then we can pick @xmath111 to achieve the minimax optimal rate of convergence under the frobenius norm as in theorem 4 of @xcite that p^+-^0_f^2=o_p((1+p)n)= o_p(dn ) .",
    "however , to attain the same rate in the presence of the log - determinant barrier term , @xcite instead would require that @xmath112 , the minimal eigenvalue of the true covariance matrix , should be bounded away from zero by some positive constant , and also that the barrier parameter should be bounded by some positive quantity .",
    "we would like to point out that if @xmath112 is bounded away from zero , then the soft - thresholding estimator @xmath113 will be positive - definite with an overwhelming probability tending to @xmath114 , @xcite .",
    "therefore the theory requiring a lower bound on @xmath112 is not very appealing .",
    "the soft - thresholding estimator has been shown to enjoy good asymptotic properties for estimating large sparse covariance matrices . but its positive definiteness property can be easily violated , which means the soft - thresholding estimator could be in principle an inadmissible estimator for covariance matrices . in this paper",
    "we have put the soft - thresholding estimator in a convex optimization framework and considered a natural modification by imposing the positive definiteness constraint .",
    "we have developed a fast alternating direction method to solve the constrained optimization problem and the resulting estimator retains the sparsity and positive definiteness properties simultaneously .",
    "the algorithm and the new estimator are supported by numerical and theoretical results .",
    "we thank adam rothman for sharing his code .",
    "shiqian ma s research is supported by the national science foundation postdoctoral fellowship through institute for mathematics and its applications at university of minnesota .",
    "hui zou s research is supported in part by grants from the national science foundation and the office of naval research .",
    "since @xmath71 is optimal to , it follows from the kkt conditions that the followings hold .",
    "[ kkt-1 ] ( -^+-^++_n)_j /|_j^+|,j=1,  ,p , = 1,  ,p j , [ kkt-1.5 ] ( ^+-_n)_jj + ^+_jj=0 , j=1,  ,p , [ kkt-2]^+=^+,[kkt-2.5 ] ^+ , and [ kkt-3]^+,-^+ 0 , .",
    "note that the optimality conditions for the first subproblem in algorithm 1 , i.e. the subproblem with respect to @xmath30 in , are given by [ opt - cond - theta]^i-(^i+1-^i)/,-^i+10,.using the updating formula for @xmath115 in algorithm 1 , i.e. , [ update - lambda]^i+1=^i-(^i+1-^i+1)/ , can be rewritten as [ opt - cond - theta-1 ] ^i+1-(^i+1-^i)/,-^i+10,.now by letting @xmath116 in and @xmath117 in , we can get that [ proof - lemma - theta - inequa-1]^+,^i+1-^+0 , and [ proof - lemma - theta - inequa-2]^i+1-(^i+1-^i)/,^+-^i+10 . summing and yields [ proof - lemma - theta - inequa-3]^i+1-^+ , ( ^i+1-^+)+(^i-^i+1)/0 .",
    "the optimality conditions for the second subproblem in algorithm 1 , i.e. , the subproblem with respect to @xmath36 in are given by [ opt - cond - sigma - tmp1]0(^i+1-_n)_j+|^i+1_j| + ^i_j + ( ^i+1-^i+1)_j/,j=1,  ,p,=1,  ,p , j , and [ opt - cond - sigma - tmp2 ] ( ^i+1-_n)_jj+^i_jj+(^i+1-^i+1)_jj/=0,j=1,  ,p.note that by using , and can be respectively rewritten as : [ opt - cond - sigma ] ( -^i+1-^i+1+_n)_j/|^i+1_j| , j=1,  ,p,=1,  ,p , j , and [ opt - cond - sigma-1.5 ] ( ^i+1-_n)_jj+^i+1_jj=0,j=1,  ,p.using the fact that @xmath118 is a monotone function , , , and imply [ proof - lemma - sigma - inequa-1 ] ^i+1-^+ , ( ^+-^i+1)+(^+-^i+1)0 . the summation of and gives [ proof - lemma - combine - inequa-1 ]                  using the notation of @xmath123 and @xmath124 , can be rewritten as [ proof - lemma - final - inequa-3 ] u^i - u^*,u^i - u^i+1 _ g u^i - u^i+1_g^2+^i+1-^+ _ f^2-^i-^i+1,^i-^i+1 . combining with the following identity @xmath125 we get [ proof - lemma - final - inequa-4 ]      now , using and for @xmath47 instead of @xmath126 , we get , [ opt - cond - sigma - k ] ( -^i-^i+_n)_j/|^i_j| , j=1,  ,p,=1,  ,p , j , and [ opt - cond - sigma - k-1.5 ] ( ^i-_n)_jj+^i_jj=0,j=1,  ,p . combining , , , and using the fact that @xmath118 is a monotone function , we obtain , @xmath127 which immediately implies , [ proof - lemma - final - inequa-5 ] ^i-^i+1,^i+1-^i^i+1-^i_f^2 0 . by substituting into , we get the desired result .",
    "it follows from ( i ) that @xmath131 and @xmath132 .",
    "then implies that @xmath133 and @xmath134 . from ( ii ) we obtain that , @xmath123 has a subsequence @xmath135 that converges to @xmath136 , i.e. , @xmath137 and @xmath138 . from @xmath134",
    "we also get that @xmath139 .",
    "therefore , @xmath140 is a limit point of @xmath82 .        without loss of generality",
    ", we may always assume that @xmath141 for all @xmath142 . by the condition that @xmath45 is positive definite",
    ", we can always choose some very small @xmath26 such that @xmath143 is smaller than the minimal eigenvalue of @xmath45 .",
    "we introduce @xmath144 , and then we can write in terms of @xmath145 as follows , = _ : = ^,+^0  2+^0-_n_f^2+|+^0|_1(f ( ) ) .",
    "note that it is easy to see that @xmath146 .",
    "now we consider @xmath147 . under the probability event @xmath148",
    ", we have f()-f ( ) & = & 2+^0-_n_f^2 - 2 ^ 0-_n_f^2 + |+^0|_1-|^0|_1 + & = & 2_f^2+<,^0-_n > + |_a_0^c|_1 + ( |_a_0+^0_a_0|_1-|^0_a_0|_1 ) + & & 2_f^2 - + |_a_0^c|_1 -|_a_0|_1 + & & 2_f^2 - 2 + & & 2_f^2 - 2 ^ 1/2_f + & & 2 ^ 2 ( s+p ) + & > & 0    note that @xmath149 is also the optimal solution to the following convex optimization problem = _ : = ^,+^0  f()-f()(g ( ) ) . under the same probability event ,",
    "@xmath150 would always hold . otherwise , the fact that @xmath151 for @xmath152 should contradict with the convexity of @xmath153 and @xmath154 .",
    "therefore , we can obtain the following probability bound @xmath155    now we shall prove the probability bound under the exponential - tail condition .",
    "first it is easy to verify two simple inequalities that @xmath156 and @xmath157 .",
    "the first inequality can be proved by using the taylor expansion , and the second one can be easily derived using the obvious facts that @xmath158 and @xmath159 .",
    "let @xmath160 , @xmath161 and @xmath162 .",
    "for any @xmath96 , we can apply the markov inequality to obtain that ( _ ix_ij > n_0 ) & & ( -t_0n_0)_i=1^ne[(t_0x_ij ) ] + & & ( -t_0n_0)_i=1^n \\{1 + 2e[x_ij^2(t_0|x_ij| ) ] } + & & p^-c_0 ^ 1/2(2_i=1^ne[x_ij^2(t_0|x_ij| ) ] ) + & & p^-c_0 ^ 1/2(2_i=1^ne[(t_0 ^ 2x_ij^2 + 1 ) ] ) + & & p^-c_0 ^ 1/2(2ek_1p)(=p^-m-1 ) , where we apply @xmath163 in the second inequality and @xmath164 in the third inequality , and then use @xmath157 in the fourth inequality .",
    "moreover , the simple facts that @xmath165=0 $ ] ( @xmath166 ) and @xmath167 are also used .",
    "let @xmath168 and @xmath169 .",
    "define @xmath170 .",
    "for any @xmath96 , we first apply the cauchy inequality to obtain that e[x_ij^2x_ik^2(2|x_ijx_ik| ) ] & & e[x_ij^2x_ik^2(4(x^2_ij+x^2_ik ) ) ] + & & ( e[x_ij^4(x^2_ij/2)])^1/2(e[x_ik^4(x^2_ik/2)])^1/2 + & & 4 ^ -2(e[(x^2_ij)])^1/2 ( e[(x^2_ik)])^1/2 + & & 4k_1 ^ -2 , where we use the simple inequality @xmath159 in the third inequality .",
    "then , combining this result with the cauchy inequality again yields that & & e[(x_ijx_ik-^0_jk)^2(t_1|x_ijx_ik-^0_jk| ) ] + & & 2e[x_ij^2x_ik^2(2|x_ijx_ik-^0_jk|)]+ 2(^0_jk)^2e[(2|x_ijx_ik-^0_jk| ) ] + & & 8k_1 ^ -2(2 ^ 0_jk ) + 2(^0_jk)^2(2 ^ 0_jk)e[(4(x_ij^2+x_ik^2 ) ) ] + & & 8k_1 ^ -2(2_)+2_^2(2 _ ) ( e[(2x_ij^2)])^1/2(e[(2x_ik^2)])^1/2 + & & 2k_1(4 ^ -2+_^2)(2 _ ) where we use the fact that @xmath171 in the first inequality , and then use @xmath172 in the third inequality .",
    "now , we can apply the markov inequality to obtain the following probability bound & & ( _ i\\{x_ijx_ik-^0_jk } > n_1 ) + & & ( -t_1n_1)_i=1^ne + & & p^-2c_1_i=1^n \\{1 + 2t_1 ^ 2e } + & & p^-2c_1 ( 2t_1 ^ 2_i=1^ne ) + & & p^-2c_1(k_1(1 + 4 ^ 2_^2)(2_)p)(=p^-m-2 ) , where we apply @xmath163 and @xmath173=\\sigma^0_{jk}$ ] for @xmath174 in the second inequality , and we use @xmath164 in the third inequality .    recall that @xmath175 and @xmath176 therefore , we can complete the probability bound under the exponential - tail condition as follows ( _ j , k|^n_jk-^0_jk| > ) & & p^2(_ix_ijx_ik >",
    "n(^0_jk+_1))+2p(_ix_ij > n_0 ) + & & 3p^-m .    in the sequel",
    "we shall prove the probability bound under the polymonial - tail condition .",
    "first , we define @xmath177 and @xmath178 .",
    "define @xmath179 , @xmath180 and @xmath181 .",
    "then we have @xmath182 and @xmath165=e[y_{ij}]+e[z_{ij}]$ ] . by construction , @xmath183",
    "are bounded random variables , and @xmath184 $ ] are bounded by @xmath185 due to the fact that @xmath186\\le k_2\\delta_n^{-3}=o(\\varepsilon_2 ) .",
    "$ ] now we can apply the bernstein s inequality @xcite to obtain that ( _ i\\{y_ij - e[y_ij ] ) } > 2n_2 ) & & ( ) + & & ( ) + & = & o(p^-m-1 ) , where the fact that @xmath187\\le e[x^2_{ij}i_{\\{|x_{ij}|\\ge 1\\}}]+e[x^2_{ij}i_{\\{|x_{ij}|\\le 1\\}}]\\le k_2 + 1 $ ] is used in the second inequality .",
    "besides , we can apply the markov inequality to obtain that @xmath188 \\le k_2(\\log n)^{2(1+\\gamma+\\varepsilon)}n^{-1-\\gamma-\\varepsilon}.\\ ] ] then , we can derive the following probability bound ( _",
    "n_2 ) & = & ( _ i\\{y_ij+z_ij - e[y_ij+z_ij ] } > n_2 ) + & & ( _ i\\{y_ij - e[y_ij ] } > 2n_2)+ ( _ i\\{z_ij - e[z_ij ] } > 2n_2 ) + & & o(p^-m-1)+(_i\\{z_ij - o(_2 ) } > 2n_2 ) + & & o(p^-m-1)+_i(|x_ij|>_n ) + & & o(p^-m-1)+k_2(n)^2(1++)n^-- .    let @xmath189 and @xmath190 . recall that @xmath191 , and define @xmath192 .",
    "then we have @xmath193 and @xmath194=e[y_{ij}y_{ik}]+e[r_{ijk}]$ ] . by construction , @xmath195",
    "are bounded random variables , and @xmath196 $ ] is bounded by @xmath197 due to the fact that |e[r_ijk]| & &    & & _ n^-2 - 4e[x^4(1+)_iji_\\{|x_ij|>_n}]e[x^2_ik]+_n^-2 - 4e[x^4(1+)_iki_\\{|x_ik|>_n}]e[x^2_ij ] + & & 2 k_2_n^-2 - 4 ( = o(_3 ) ) .",
    "again , we can apply the bernstein s inequality to obtain that ( _ i\\{y_ijy_ik - e[y_ijy_ik ] } > 2n_3 ) & & ( ) + & & ( ) + & = & o(p^-m-2 ) , where the fact that @xmath198\\le ( e[x^4_{ij}]e[x^4_{ik}])^{1/2}\\le k_2 + 1 $ ] is used .",
    "& & ( _ j , k|_i(x_ijx_ik-^0_jk)| > n_3 ) + & & ( _ j , k|_i\\{y_ijy_ik - e[y_ijy_ik]}| > 2n_3)+(_j , k|_i\\{r_ijk - e[r_ijk]}| > 2n_3 ) + & & 2_j , k(_i\\{y_ijy_ik - e[y_ijy_ik ] } >",
    "2n_3 ) + ( _ j , k|_i\\{r_ijk - o(_3)}| >",
    "2n_3 ) + & & o(p^-m)+_i , j(|x_ij|>_n ) + & & o(p^-m)+k_2p(n)^2(1++)n^--    recall that @xmath199 . therefore , we can prove the desired probability bound under the polynomial - tail condition as follows & & ( _ j , k|^n_jk-^0_jk| > ) + & & ( _ j , k|_i\\{x_ijx_ik-^0_jk}| > n_3 ) + ( _ j|_ix_ij| > n_2 ) + & & o(p^-m)+3k_2p(n)^2(1++)n^-- .          beer , d. , kardia , s. , huang , c. , giordano , t. , levin , a. , misek , d. , lin , l. , chen , g. , gharib , t. , thomas , d. , et  al . ( 2002 ) , `` gene - expression profiles predict survival of patients with lung adenocarcinoma , '' _ nature med .",
    "_ , 8 , 816824 .",
    "khan , j. , wei , j. , ringnr , m. , saal , l. , ladanyi , m. , westermann , f. , berthold , f. , schwab , m. , antonescu , c. , peterson , c. , et  al .",
    "( 2001 ) , `` classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks , '' _ nature med .",
    "_ , 7 , 673679 .",
    "subramaniana , a. , tamayoa , p. , moothaa , v. , mukherjeed , s. , eberta , b. , gillettea , m. , paulovichg , a. , pomeroyh , s. , goluba , t. , landera , e. , et  al .",
    "( 2005 ) , `` gene set enrichment analysis : a knowledge - based approach for interpreting genome - wide expression profiles , '' _ proc .",
    "_ , 102 , 1554515550 ."
  ],
  "abstract_text": [
    "<S> the thresholding covariance estimator has nice asymptotic properties for estimating sparse large covariance matrices , but it often has negative eigenvalues when used in real data analysis . to simultaneously achieve sparsity and positive definiteness , we develop a positive definite @xmath0-penalized covariance estimator for estimating sparse large covariance matrices . an efficient alternating direction method is derived to solve the challenging optimization problem and its convergence properties are established . under weak regularity conditions , </S>",
    "<S> non - asymptotic statistical theory is also established for the proposed estimator . </S>",
    "<S> the competitive finite - sample performance of our proposal is demonstrated by both simulation and real applications .    </S>",
    "<S> = 1    * keywords * : alternating direction methods ; large covariance matrices ; matrix norm ; positive - definite estimation ; sparsity ; soft - thresholding . </S>"
  ]
}