{
  "article_text": [
    "a single - layered feed - forward network of neurons , referred to as a perceptron , is an elementary building block of complex neural networks .",
    "it is also one of the basic structures for learning and memory  @xcite . in a perceptron , @xmath8 input neurons ( units )",
    "are connected to a single output unit by synapses of continuous or discrete - valued synaptic weights .",
    "the learning task is to set the weight values for these @xmath8 synapses such that an extensive number @xmath9 of input patterns are correctly classified ( see fig .  [ perc]a ) .",
    "the parameter @xmath10 is called the constraint density .",
    "an assignment of these weights is referred to as a solution if the perceptron correctly classifies all the input patterns with this weight assignment .",
    "compared with perceptrons with real - valued synaptic weights , ising perceptrons , whose synaptic weights are binary , are much simpler for large - scale electronic implementations and more robust against noise .",
    "an ising perceptron is also relevant in real neural systems , as the synaptic weight between two neurons actually takes bounded values and has a limited number of synaptic states@xcite . on the other hand ,",
    "training a real - valued perceptron is easy ( e.g. , the minover algorithm @xcite and the adatron algorithm @xcite ) but training an ising perceptron is known to be an np - complete problem @xcite . given @xmath11 input patterns , the computation time needed to find a solution may grow exponentially with the number of weights @xmath8 in the worst case .",
    "a complete enumeration of all possible weight states is only feasible for small systems up to @xmath12  @xcite . in recent years",
    "researches on efficient heuristic algorithms were rather active @xcite .",
    "if the number @xmath13 of input patterns is too large , a perceptron will be unable to correctly classify all of them , no matter how the synaptic weights are modified .",
    "this is a phase transition phenomenon of the solution space of the perceptron . in the case",
    "that the @xmath13 input binary patterns are sampled uniformly and randomly from the set of all binary patterns , the maximal value @xmath14 of the constraint density @xmath7 , the storage capacity at which a solution still exists , has been calculated by statistical physics methods . for the continuous perceptron subject to the spherical constraint , gardner and derrida found that @xmath15 @xcite . at the thermodynamic limit of @xmath16 ,",
    "the continuous perceptron is impossible to correctly classify more than @xmath17 random input patterns .",
    "when the synaptic weight is restricted to binary values , @xmath14 was predicted to be @xmath18 by krauth and mzard using the first - step replica - symmetry broken spin - glass theory @xcite .",
    "this prediction was confirmed by numerical simulations of small size systems ( plus an extrapolation to large @xmath8 ) @xcite .",
    "the theoretically predicted storage capacity @xmath14 represents the upper limit of achievable constraint density @xmath7 by any learning strategies .",
    "as the constraint density @xmath7 increases , it is expected that the solution space of the ising perceptron breaks into a huge number of disjoint ergodic components @xcite .",
    "solutions from different components are significantly different .",
    "one can define a connected component of the weight space as a cluster of solutions in which any two solutions are connected by a path of successive single - weight flips @xcite .",
    "these solution clusters are separated by weight configurations that only correctly classify a subset of the input patterns .",
    "these partial solutions act as dynamical traps for local search algorithms and make the learning task hard .",
    "an adaptive genetic algorithm was suggested by khler in 1990 , which could reach @xmath19 for systems of @xmath20 @xcite .",
    "simulated annealing techniques were used by horner @xcite but critical slowing down of the search process was observed , due to the very rugged energy landscape of the problem .",
    "the simulated annealing was also used to study the statistical structure of the energy landscape for the ising perceptron .",
    "the analysis of the distribution of distances between global minima obtained by simulated annealing for small @xmath7 indicated that the distance distribution becomes a delta function in the thermodynamic limit @xcite . making use of the advantage that efficient algorithms exist for the real - valued perceptron , an alternative approach was to clip the trained real - valued weights of the continuous perceptron into binary values  @xcite .",
    "not all synaptic weights can be correctly specified by clipping , however , and for those uncertain weights , complete enumeration was then adopted .",
    "a message - passing algorithm was developed by braunstein and zecchina for the ising perceptron @xcite , which was able to reach @xmath19 for @xmath21 .",
    "the efficiency of this belief - propagation algorithm was later conjectured to be due to the existence of a sub - exponential number of large solution clusters in the weight space @xcite .",
    "an on - line learning algorithm inspired from this belief - propagation algorithm was also studied @xcite , in which hidden discrete internal states are added to the synaptic weights .    in real neural systems ,",
    "the microscopic mechanism of perceptronal learning is the hebbian rule of synaptic modification ( spiking - time - dependent synaptic plasticity may be exploited , see , e.g. , refs .",
    "the learning processes in biological perceptronal systems are expected to be much simpler than the various sophisticated learning processes of artificial perceptrons .",
    "two other important aspects of biological perceptron systems are ( i ) the patterns to be classified are usually read into the system in a sequential order , so they are being learned one by one , and ( ii ) when a new pattern is being learned , there are biological mechanisms which reactivate old learned patterns ; such recalling processes help to prevent old patterns from being forgot as new patterns are learned ( see , e.g. , the experimental investigation of ref .",
    "@xcite ) . motivated by these biological considerations , we investigate in this paper a simple sequential learning mechanism , namely synaptic - weight space random walking . in this random walking mechanism ,",
    "the @xmath11 patterns are introduced into the system in a randomly permuted sequential order , and random walk of single- or double - weight flips is performed until each newly added pattern is correctly classified ( learned ) .",
    "the previously learned patterns are _ not _ allowed to be misclassified in later stages of the learning process .",
    "we perform extensive numerical simulations on several variants of this simple sequential local learning rule and find that this mechanism has good performance on systems of @xmath22 neurons or less .",
    "the paper is organized as follows .",
    "the ising perceptron learning is defined in more detail in sec .",
    "[ sec_rcp ] .",
    "several strategies of learning by random walks are presented in sec .",
    "[ sec_lrw ] . in sec .",
    "[ sec_result ] , experimental study of learning algorithms is carried out .",
    "the overlap distribution of solutions as well as performances of different local search algorithms is reported .",
    "summary and discussion are given in sec .",
    "[ sec_sum ] .",
    "sequential random walk search algorithms were recently investigated in various combinatorial satisfaction problems ( see , e.g. , refs .",
    "the present work adds evidence that the solution space random walking mechanism , although very simple and easy to implement , is able to solve many nontrivial problem instances of a given complex learning or constraint satisfaction problem .",
    "input units ( open circles ) feed directly to a single output unit ( solid circle ) .",
    "a binary input pattern @xmath23 of length @xmath8 is mapped through a sign function to a binary output @xmath24 , i.e. , @xmath25 .",
    "the set of @xmath8 binary synaptic weights @xmath26 is regarded as a solution of the perceptron problem if the output @xmath27 for each of the @xmath28 input patterns @xmath29 $ ] , where @xmath30 is a preset binary value .",
    "( b ) a solution space random walking path ( indicated by arrows ) .",
    "an open circle represents a configuration that satisfies the first @xmath31 input patterns , while a black circle and a gray circle represents , respectively , a configuration that satisfies the first @xmath32 and the first @xmath33 input patterns .",
    "an edge between two configurations means that these two configurations are related by a single - weight flip .",
    ", title=\"fig:\",width=283 ] .5 cm   input units ( open circles ) feed directly to a single output unit ( solid circle ) .",
    "a binary input pattern @xmath23 of length @xmath8 is mapped through a sign function to a binary output @xmath24 , i.e. , @xmath25 . the set of @xmath8 binary synaptic weights @xmath26",
    "is regarded as a solution of the perceptron problem if the output @xmath27 for each of the @xmath28 input patterns @xmath29 $ ] , where @xmath30 is a preset binary value .",
    "( b ) a solution space random walking path ( indicated by arrows ) .",
    "an open circle represents a configuration that satisfies the first @xmath31 input patterns , while a black circle and a gray circle represents , respectively , a configuration that satisfies the first @xmath32 and the first @xmath33 input patterns .",
    "an edge between two configurations means that these two configurations are related by a single - weight flip .",
    ", title=\"fig:\",width=283].2 cm",
    "for the ising perceptron depicted schematically in fig .",
    "[ perc]a , @xmath8 input units are connected to a single output unit by @xmath8 synapses of weight @xmath34 @xmath35 .",
    "the perceptron tries to learn @xmath36 associations @xmath37 @xmath38 , where @xmath39 is an input pattern with @xmath40 , and @xmath41 is the desired classification of the input pattern @xmath42 .",
    "given the input pattern @xmath43 , the actual output @xmath44 of the perceptron is @xmath45 the perceptron can modify its synaptic weight configuration @xmath46 to achieve complete classification , i.e. , @xmath47 for each of the @xmath13 input pattern .",
    "the solution space of the ising perceptron is composed of all the weight configurations @xmath48 that satisfy @xmath49 for @xmath50 .    for the random ising perceptron problem studied in this paper , each of the @xmath13 input binary patterns @xmath43",
    "is sampled uniformly and randomly from the set of all @xmath51 binary patterns of length @xmath8 , and the classification @xmath30 is equal to @xmath52 with equal probability . for @xmath8",
    "sufficiently large , the solution space of such a model system is non - empty as long as @xmath53 @xcite . to construct such a solution configuration @xmath48 , however , is quite a non - trivial task .",
    "a more stringent learning problem is to find a weight configuration @xmath48 such that , for each input pattern @xmath43 , @xmath54 where @xmath55 is a preset parameter @xcite .",
    "the most efficient way of solving this constraint satisfaction problem appears to be the message - passing algorithm of refs .",
    "@xcite .",
    "one can perform a gauge transform of @xmath56 to each input pattern . under this gauge transform ,",
    "each desired output is transformed to @xmath57 . without loss of generality , in the remaining part of this paper we will assume @xmath57 for any input pattern @xmath42 .",
    "consider the case of @xmath8 being odd , we define the stability field of a pattern @xmath42 as @xmath58 to ensure the local stability of input pattern @xmath42 under changes of weight configuration @xmath48 , in analogy to eq .",
    "( [ eq : kappa ] ) , we introduce a stability parameter @xmath59 and require that @xmath60 for each @xmath42 .",
    "input patterns with @xmath61 are stable against a single - weight flip . for the single - weight flipping processes of the next section ,",
    "the input patterns with @xmath62 are referred to as barely learned patterns , as these patterns may become misclassified after the weight configuration makes a single flip .",
    "similarly , for the double - weight flipping process of the next section , the input patterns with @xmath62 or @xmath63 are referred to as barely learned patterns .",
    "random walk processes were used in a series of works @xcite to find solutions for constraint satisfaction problems .",
    "they were also used as tools to study the solution space structure of these constraint satisfaction problems @xcite .",
    "various local search strategies have been developed to improve the performance of random walk stochastic searching  @xcite .",
    "the random walk learning strategies of this work follow the seqsat algorithm of ref .",
    "an initial weight configuration @xmath64 is randomly generated at time @xmath65 .",
    "the first pattern @xmath66 is applied to the ising perceptron .",
    "if this pattern is correctly classified under the initial weight configuration ( i.e. , @xmath67 ) , then the second pattern @xmath68 is applied ; otherwise the weight configuration is adjusted by a sequence of elementary local changes until @xmath66 is correctly classified .",
    "the algorithm then proceeds with the second pattern @xmath68 , the third pattern @xmath69 , etc .",
    ", in a sequential order",
    ". an elementary local change of weight configuration is achieved either by a single - weight flip ( swf ) or by a double - weight flip ( dwf ) .",
    "suppose at time @xmath70 the weight configuration is @xmath71 , and suppose this configuration correctly classifies the first @xmath32 input patterns @xmath43 ( @xmath72 but not the @xmath73-th pattern @xmath74 . the configuration @xmath48 will keep wandering in the solution space of the first @xmath32 patterns until a configuration that correctly classifies @xmath74 is reached ( see fig .  [",
    "perc]b ) . in the swf protocol ,",
    "a set @xmath75 of allowed single - weight flips is constructed based on the current configuration @xmath76 and the @xmath32 learned patterns .",
    "@xmath75 includes all integer positions @xmath77 $ ] with the property that the single - weight flip of @xmath78 does not render any barely learned patterns @xmath79 $ ] ( whose @xmath62 ) being misclassified . at time @xmath80",
    "an integer position @xmath81 is chosen uniformly and randomly from set @xmath75 and the weight configuration is changed to @xmath82 such that @xmath83 if @xmath84 and @xmath85 .",
    "it is obvious that the new configuration @xmath82 also satisfies all the first @xmath32 patterns .",
    "the dwf protocol is very similar to the swf protocol , with the only difference that the allowed set @xmath75 at time @xmath70 contains ordered pairs of integer positions @xmath86 with @xmath87 .",
    "this set of ordered pairs can also be easily constructed .",
    "if , with respect to configuration @xmath76 , there are no barely learned patterns ( whose stability field @xmath62 or @xmath88 ) among the first @xmath32 learned patterns , then @xmath75 contains all the @xmath89 ordered pairs of integers @xmath86 with @xmath90 . otherwise , randomly choose a barely learned pattern , say @xmath91 $ ] , and for each integer @xmath92 $ ] with the property that @xmath93 , do the following : ( 1 ) if @xmath94 for all the other barely learned patterns , then add all the ordered pairs @xmath86 with @xmath95 $ ] into the set @xmath75 ; ( 2 ) otherwise , add all the ordered pairs @xmath86 into the set @xmath75 , with the property that the integer @xmath96 $ ] satisfies @xmath97 for all those barely learned patterns @xmath98 $ ] with @xmath99 .    the waiting time @xmath100 of satisfying the @xmath73-th pattern",
    "is defined as the total elapsed time from first satisfying the @xmath32-th pattern to first satisfying the @xmath73-th pattern . and",
    "the total time @xmath101 of satisfying the first @xmath73 patterns is simply @xmath102 .",
    "one time unit corresponds to @xmath8 elementary local changes of the weight configuration .",
    "the random walk searching process stops if all the @xmath13 input patterns have been correctly classified , or if the last visited weight configuration becomes an isolated point ( i.e. , the set @xmath75 becomes empty after a new pattern is included into the set of learned patterns ) , or if the last waiting time @xmath100 exceeds a preset maximal time value @xmath103 , which is equal to @xmath104 in the present work .",
    "the swf and dwf random walks processes as mentioned above are very simple to implement and they do not overcome any barriers in the energy landscape of the perceptron learning problem . however , as we demonstrate in the next section , their performances are quite remarkable for problem instances with pattern length @xmath105 .    the swf process , as a local search algorithm , will get stuck in one of the enormous metastable states when all the weights become frozen ( here we identify a synaptic weight as being frozen if flipping its value causes at least one of the learned patterns to be misclassified ) , at a constraint density value much smaller than the theoretical threshold value of @xmath18",
    ". the dwf process will also get jammed if the weight configuration becomes frozen with respect to any double - weight flips . to further improve the achievable storage capacity for the swf and dwf learning processes , a simple relearning strategy",
    "is added to the random walk searching .",
    "the basic idea of the relearning strategy is : if some learned patterns are hindering the learning of new patterns very much , we first ignore them and proceed to learn a number of new patterns ; after that , we learn the ignored patterns again and hope they can all be correctly classified .    in the present work ,",
    "we implement the relearning strategy in the following way .",
    "suppose that as the @xmath32-th input pattern is presented to the ising perceptron , the swf or the dwf process is unable to learn it in a waiting time @xmath106 .",
    "we then remove all the @xmath107 barely learned patterns @xmath108 $ ] with @xmath109 from the list of learned patterns , and proceed to learn the patterns @xmath110 $ ] in a sequential manner ( stage 1 ) .",
    "if the swf process or the dwf process succeeds in learning these @xmath107 patterns , we then return to learn the @xmath107 previously removed patterns again in a sequential manner ( stage 2 ) .",
    "if this relearning succeeds , we proceed with the patterns with index @xmath111 .",
    "if this attempt fails either at stage @xmath112 or at stage @xmath113 , we stop the whole random walk learning process or start with another trial by removing all the learned patterns . in practice , we find that the relearning process has a high probability to succeed in both stage @xmath112 and stage @xmath113 if @xmath7 is not too large and pattern length is of order @xmath114 or less .",
    "figure  [ capn ] demonstrates the simulation results for several random walk learning strategies . for each learning strategy ,",
    "@xmath115 set of random input patterns @xmath116 are generated .",
    "each input pattern @xmath43 has length @xmath8 .",
    "the random walk learning strategy is then applied to each set of patterns until it stops , at which point we record the number of correctly classified patterns @xmath32 and calculate the achieved storage capacity @xmath117 .",
    "the mean values of @xmath7 are reported in fig .",
    "it appears that the storage capacity of all the four learning strategies decreases with @xmath8 roughly as a power law @xmath118 . at each value of @xmath8",
    ", the swf strategy has the worst performance , while the dwf strategy with relearning has the best performance .     as averaged over many independent runs ( @xmath119 for the smallest @xmath8 and @xmath120 for the largest @xmath8 )",
    "are shown as a function of the pattern length @xmath8 .",
    "the solid lines are power - law fittings of the form @xmath121 , with @xmath122 for swf , swf with relearning , dwf and dwf with relearning , respectively . , scaledwidth=70.0% ]    the swf strategy is able to reach a storage capacity of @xmath123 for systems of @xmath124 and @xmath125 for systems of @xmath6 .",
    "these values are much less than the theoretical storage capacity of @xmath126 .",
    "however , the dwf strategy performs much better , with a capacity of @xmath127 for @xmath124 and @xmath2 for @xmath6 . in real neural systems , perceptronal learning of elementary patterns",
    "probably does not involve too many neuronal cells and a value of @xmath128 might be common . for perceptronal systems with @xmath129 , the swf and dwf strategies can be regarded as efficient .    if relearning is introduced into the random walk learning strategies , the performance can be further improved . for the dwf strategy with relearning , we find that the storage capacity is @xmath4 for @xmath124 and @xmath5 for @xmath6 .",
    "relearning is indeed a biologically relevant strategy in perceptronal learning of real neural systems @xcite . as a comparison , for problem instances of pattern length @xmath6 ,",
    "the belief - propagation inspired learning strategy of baldassi and coauthors @xcite achieves @xmath130 when the number @xmath131 of internal states of their algorithm is set to @xmath132 .",
    "this storage capacity @xmath7 decreases to @xmath133 at @xmath134 and to @xmath135 at @xmath136 .",
    "for the same set of input patterns @xmath137 , different runs of the swf strategy or the dwf strategy lead to different solution configurations .",
    "the similarity between solutions can be measured by an overlap value @xmath138 as defined by @xmath139 where @xmath140 and @xmath141 are two solutions .",
    "the reduced hamming distance @xmath142 between two solutions is related to the overlap @xmath138 by @xmath143 .",
    "the typical value of the overlap value at constraint density @xmath144 is predicted to be @xmath145 according to the replica - symmetric calculation @xcite , suggesting that solutions are still far away from each other ( with a reduced hamming distance @xmath146 ) as @xmath7 approaches the theoretical storage capacity @xmath14 .",
    "input patterns of length @xmath8 .",
    "@xmath119 solutions are constructed for each of the five instances with @xmath147 , @xmath148 , @xmath149 , @xmath150 , @xmath151 , respectively .",
    "the solid lines are gaussian fitting results to the histograms .",
    ", scaledwidth=80.0% ]    figure  [ histo ] shows the histogram @xmath152 of reduced hamming distances @xmath142 between different solutions found by the dwf strategy for a single problem instance with constraint density @xmath7 and pattern length @xmath8 .",
    "different pattern lengths of @xmath153 are used , and @xmath119 different solutions are constructed by repeated running of the dwf process .",
    "other problem instances show similar properties .",
    "we notice from fig .",
    "[ histo ] that , at the same value of @xmath7 , the histograms @xmath152 for different @xmath8 are peaked at almost the same @xmath142 value , but the width of @xmath152 decreases as @xmath8 is enlarged .",
    "such a behavior was observed earlier in ref .",
    "@xcite on a slightly modified ising perceptron problem .",
    "the solutions obtained by the dwf strategy therefore have a typical level of similarity .",
    "figure  [ histo ] also demonstrates that , as the constraint density increases , the histograms @xmath152 shift to smaller @xmath142 values , suggesting that the level of similarity between the dwf - constructed solutions increases with @xmath7 . at @xmath154",
    "the typical reduced hamming distance is @xmath155 , compatible with the mean - field predictions @xcite .",
    "similar results are obtained for solutions found by the swf strategy . in all our simulations , we do not observe double or multiple peaks for the histogram @xmath152 .",
    "the results of these and our other numerical simulations ( not shown ) are consistent with proposal that , for a given problem instance , the solutions obtained by the random walking strategies are members of the same ( large ) solution cluster of the solution space  @xcite . unlike the random @xmath131-satisfiability problem , the random @xmath156-coloring problem , or some locked constraint satisfaction problems @xcite , the solution space organization of the ising perceptron problem is still not very clear . kabashima and co - authors @xcite suggested that for @xmath157 the solution space of the ising perceptron problem is equally dominated by exponentially many clusters of vanishing entropy and a sub - exponential number of large clusters .",
    "our simulation results are compatible with this proposal , but more work needs to be done to clarify the solution space structure of the random ising perceptron problem .",
    "the total time @xmath158 used by the dwf strategy to correctly classify the first @xmath11 patterns for a problem instance with @xmath6 is shown in fig .",
    "[ fracfroz ] as a function of @xmath7 .",
    "the learning time grows almost linearly with @xmath7 for @xmath159 . as the constraint density @xmath7 becomes large ,",
    "different solution communities are expected to form in the solution space @xcite .",
    "then as @xmath7 further increases to certain larger value , the time needed for the random walk process to escape from a solution community may exceed the preset maximal waiting time of @xmath160 and the dwf process will then stop .",
    "the achieved storage capacity @xmath7 can be increased to some extent if we make @xmath103 larger , but the search process will become more and more viscous as the solution space of the problem becomes more and more heterogeneous and complex @xcite .",
    "we do not attempt to calculate the jamming point of the random walk searching processes .     as a function of @xmath7 for for three problem instances of @xmath6 .",
    ", scaledwidth=70.0% ]",
    "we proposed several stochastic learning strategies for the ising perceptron problem based on the idea of solution space random walking @xcite .",
    "our simulation results in fig .  [ capn ] demonstrated that , the dwf strategy is able to correctly classify @xmath161 random input patterns of length @xmath8 for @xmath162 .",
    "if a simple relearning strategy is added to the dwf strategy , the learning performance is further improved .",
    "the learning time of the dwf strategy grows roughly linearly with the number of input patterns .",
    "this work suggested that learning by local and random changes of synaptic weights is efficient for perceptronal systems with @xmath163 neurons .",
    "these local sequential learning strategies may be exploited in some biological perceptronal systems . in real neuronal systems ,",
    "the number @xmath8 of involved neurons in an elementary pattern classification task may be of the order of @xmath164 .",
    "the solutions obtained by the dwf strategy for a given perceptronal learning task are separated by a typical hamming distance , which reduces as the number of input patterns increases ( fig .",
    "[ histo ] ) .",
    "however , solutions are still far away from each other even near to the critical capacity . we suspected that for the problem instances studied in this paper , either the solution space of the problems is ergodic as a whole , or the solutions reached by the dwf strategies all belong to the same solution cluster of the solution space . in our random walking setting ,",
    "once all weights are frozen , particularly for swf , the current pattern with negative stability field will be no longer learned since the current weight configuration is isolated in the weight space ( this weight configuration is denoted as the completely frozen solution ) ; fortunately , dwf is able to go on even if all weights are frozen , since flipping certain pairs of weights is still permitted from the configuration where each single weight is not allowed to be flipped .",
    "if these flippable pairs of weights do not exist , dwf will get trapped , and the configuration is isolated once again . actually ,",
    "as the constraint density @xmath7 increases , many such isolated solutions will show up , and swf or dwf working by single- or double - weight flips , is not capable of crossing energy barriers separating the isolated solutions from those connected ones , which can be bypassed to some extent using the relearning strategy which helps to escape from these small clusters and makes swf or dwf keep on exploring the large cluster composed of exponentially many solutions . for small @xmath7 , replica symmetric ansatz",
    "is believed to give a good description of the solution space of ising percetpron  @xcite .",
    "up to @xmath165 , point - like clusters will form and searching for the compatible weights becomes more difficult@xcite .",
    "it is desirable to have a theoretical understanding on the structural evolution of the solution space of the random ising perceptron problem .",
    "how the dynamics of stochastic local search algorithms is influenced by the solution space structure of the random ising perceptron is an important open issue .",
    "another interesting problem is the generalization problem where the inputs - output associations are no longer uncorrelated but the desired outputs are given by a teacher perceptron @xcite .",
    "the student perceptron tries to learn the rule provided by the teacher .",
    "after an enough amount of examples are presented to the student perceptron , the student s weights should match those of the teacher , then the network undergoes a first - order transition from poor to perfect generalization @xcite .",
    "it is worthwhile to extend the current random walk strategies to analyze the generalization problem in ising perceptrons .",
    "this work was partially supported by the national science foundation of china ( grant numbers 10774150 and 10834014 ) and the china 973-program ( grant number 2007cb935903 ) ."
  ],
  "abstract_text": [
    "<S> several variants of a stochastic local search process for constructing the synaptic weights of an ising perceptron are studied . in this process , </S>",
    "<S> binary patterns are sequentially presented to the ising perceptron and are then learned as the synaptic weight configuration is modified through a chain of single- or double - weight flips within the compatible weight configuration space of the earlier learned patterns . </S>",
    "<S> this process is able to reach a storage capacity of @xmath0 for pattern length @xmath1 and @xmath2 for @xmath3 . </S>",
    "<S> if in addition a relearning process is exploited , the learning performance is further improved to a storage capacity of @xmath4 for @xmath1 and @xmath5 for @xmath6 . </S>",
    "<S> we found that , for a given learning task , the solutions constructed by the random walk learning process are separated by a typical hamming distance , which decreases with the constraint density @xmath7 of the learning task ; at a fixed value of @xmath7 , the width of the hamming distance distributions decreases with @xmath8 . </S>"
  ]
}