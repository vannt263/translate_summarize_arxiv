{
  "article_text": [
    "network coding was introduced in @xcite as a means to improve the rate of transmission in networks , and often achieve capacity in the case of single source networks .",
    "linear network coding was introduced in @xcite .",
    "an algebraic formulation of network coding was discussed in @xcite for both instantaneous networks and networks with delays .",
    "network error correction , which involved a trade - off between the rate of transmission and the number of correctable network - edge errors , was introduced in @xcite as an extension of classical error correction to a general network setting .",
    "along with subsequent works @xcite and @xcite , this generalized the classical notions of the hamming weight , hamming distance , minimum distance and various classical error control coding bounds to their network counterparts . in all of these works , it is assumed that the sinks and the source know the network topology and the network code , which is referred to as _ coherent network coding_. network error correcting codes were also developed for _ non - coherent ( channel oblivious ) network coding _ in @xcite,@xcite and @xcite .",
    "network error correction under probabilistic error settings has been studied in @xcite .",
    "most recently , multishot subspace codes were introduced in @xcite for the subspace channel @xcite based on block - coded modulation .    a set of code symbols generated at the source at any particular time instant",
    "is called a _ generation _ of code symbols .",
    "so far , network error correcting schemes have been studied only for acyclic _ instantaneous _ ( delay - free ) networks in which each node could take a linear combination of symbols of only the same generation .",
    "convolutional network codes were discussed in @xcite and a connection between network coding and convolutional coding was analyzed in @xcite .",
    "convolutional network error correcting codes ( which we shall henceforth refer to as cneccs ) have been employed for network error correction in instantaneous networks in @xcite .",
    "a _ network use _ @xcite is a single usage of all the edges of the network to multicast utmost min - cut number of symbols to each of the sinks .",
    "an _ error pattern _ is a subset of the set of edges of the network which are in error .",
    "it was shown in @xcite that any network error which has its error pattern amongst a given set of error patterns can be corrected by a proper choice of a convolutional code at the source , as long as consecutive network errors are separated by a certain number of network uses .",
    "bounds were derived on the field size for the construction of such cneccs , and on the minimum separation in network uses required between any two network errors for them to be correctable .",
    "_ unit - delay networks _",
    "@xcite are those in which every link between two nodes has a single unit of delay associated with it . in this work ,",
    "we generalize the approach of @xcite to the case of network error correction for acyclic , unit - delay , memory - free networks .",
    "we consider single source acyclic , unit - delay , memory - free networks where coherent network coding ( for the purpose of multicasting information to a set of sinks ) has been implemented and thereby address the following problem .",
    "_ given an acyclic , unit - delay , single source , memory - free network with a linear multicast network code , and a set of error patterns @xmath0 , how to design a convolutional code at the source which will correct network errors corresponding to the error patterns in @xmath0 , as long as consecutive errors are separated by a certain number of network uses ? _    the main contributions of this paper are as follows .",
    "* network error correcting codes for unit - delay , memory - free networks are discussed for the first time . * a convolutional code construction for the given acyclic , unit - delay , memory - free network that corrects a given pattern of network errors ( provided that the occurrence of consecutive errors is separated by certain number of network uses ) is given .",
    "for the same network , if the network code is changed , then the convolutional code obtained through our construction algorithm may also change .",
    "several results of this paper can be treated as a generalization of those in @xcite .",
    "* we derive a bound on the minimum field size required for the construction of cneccs for unit - delay networks with the required minimum distance , following a similar approach as in @xcite . *",
    "we also derive a bound on the minimum number of network uses that two error events must be separated by in order that they get corrected . * we also introduce _ processing functions _ at the sinks in order to address the realizability issues that arise in the decoding of cneccs for unit - delay networks .",
    "* we show that the unit - delay network demands a cnecc whose free distance should be at least as much as that of any cnecc for the corresponding instantaneous network to correct the same number of network errors .",
    "* using a probabilistic error model on a modified butterfly unit - delay memory - free network , we use simulations to study the performance of different cneccs .",
    "* towards achieving convolutional network error correction , we address the issue of network coding for an acyclic , unit - delay , memory - free network . as a by - product",
    ", we prove that an @xmath1-dimensional linear network code ( a set of local kernels at the nodes ) for an acyclic , instantaneous network continues to be an @xmath1-dimensional linear network code ( i.e the dimension does not reduce ) for the same acyclic network , however being of unit - delay and memory - free nature .",
    "the rest of the paper is organized as follows . in section [ sec3 ]",
    ", we discuss the general network coding set - up and network errors . in section [ sec4 ] , we give a construction for an input convolutional code for the given acyclic , unit - delay , memory - free network which shall correct errors corresponding to a given set of error patterns and also derive some bounds on the field size and minimum separation in network uses between two correctable network errors . in section [ sec5 ] , we give some examples for this construction . in section [ sec6 ]",
    "we provide a comparison between cneccs for instantaneous networks @xcite and those for unit - delay , memory - free networks of this paper . in section [ sec7 ]",
    ", we discuss the results of simulations of different cneccs run on a modified butterfly network assuming a probabilistic model on edge errors in the network .",
    "we conclude this paper in section [ sec8 ] with some remarks and some directions for further research .",
    "we consider acyclic networks with delays in this paper , the model for which is as in @xcite , @xcite .",
    "an acyclic network can be represented as an acyclic directed multi - graph ( a graph that can have parallel edges between nodes ) @xmath2 = ( @xmath3 ) where @xmath4 is the set of all vertices and @xmath5 is the set of all edges in the network .",
    "we assume that every edge in the directed multi - graph representing the network has unit _ capacity _ ( can carry utmost one symbol from @xmath6 ) . network links with capacities greater than unit are modeled as parallel edges .",
    "the network has delays , i.e , every edge in the directed graph representing the input has a unit delay associated with it , represented by the parameter  @xmath7 .",
    "such networks are known as _ unit - delay networks_. those network links with delays greater than unit are modeled as serially concatenated edges in the directed multi - graph .",
    "the nodes of the network may receive information of different generations on their incoming edges at every time instant .",
    "we assume that the internal nodes are memory - free and merely transmit a linear combination of the incoming symbols on their outgoing edges .",
    "let @xmath8 be the source node and @xmath9 be the set of all receivers .",
    "let @xmath10 be the unicast capacity for a sink node @xmath11 i.e the maximum number of edge - disjoint paths from @xmath12 to @xmath13 .",
    "then @xmath14 is the max - flow min - cut capacity of the multicast connection .",
    "we follow @xcite in describing the network code . for each node @xmath15 , let the set of all incoming edges be denoted by @xmath16 .",
    "then @xmath17 is the in - degree of @xmath18 .",
    "similarly the set of all outgoing edges is defined by @xmath19 , and the out - degree of the node @xmath18 is given by @xmath20 . for",
    "any @xmath21 and @xmath22 , let @xmath23 , if @xmath18 is such that @xmath24 .",
    "similarly , let @xmath25 , if @xmath18 is such that @xmath26 .",
    "we will assume an ancestral ordering on @xmath27 of the acyclic graph @xmath2 .",
    "the network code can be defined by the local kernel matrices of size @xmath28 for each node @xmath15 with entries from @xmath6 .",
    "the global encoding kernels for each edge can be recursively calculated from these local kernels .",
    "the network transfer matrix , which governs the input - output relationship in the network , is defined as given in @xcite for an @xmath1 dimensional network code . towards this end ,",
    "the matrices @xmath29,@xmath30,and @xmath31(for every sink @xmath32 are defined as follows .",
    "the entries of the @xmath33 matrix @xmath29 are defined as @xmath34 where @xmath35 is the local encoding kernel coefficient at the source coupling input @xmath36 with edge @xmath37 .",
    "the entries of the @xmath38 matrix @xmath30 are defined as @xmath39 where the set of @xmath40 is the local encoding kernel coefficient between @xmath41 and @xmath42 , at the node @xmath43 .    for every sink @xmath44 ,",
    "the entries of the @xmath45 matrix @xmath31 are defined as @xmath46 where all @xmath47 .    for unit - delay , memory - free networks , we have @xmath48 where @xmath49 is the @xmath38 identity matrix .",
    "now we have the following definition .    [ nettransmatrix ]",
    "_ the network transfer matrix _ , @xmath50 , corresponding to a sink node @xmath51 is a full rank ( over @xmath52 ) @xmath53 matrix defined as @xmath54    with an @xmath1-dimensional network code , the input and the output of the network are @xmath1-tuples of elements from @xmath55.$ ] definition [ nettransmatrix ] implies that if @xmath56 $ ] is the input to the unit - delay , memory - free network , then at any particular sink @xmath57 , we have the output , @xmath58 $ ] , to be @xmath59      a primer on the basics of convolutional codes can be found in appendix [ app1 ] .",
    "assuming that an @xmath1-dimensional linear network code multicast has been implemented in the given single source unit - delay , memory - free network , we extend the definitions of the input and output convolutional codes of cneccs for instantaneous networks from @xcite to the unit - delay , memory - free case .    an _ input convolutional code _",
    ", @xmath60 , corresponding to an acyclic , unit - delay , memory - free network is a convolutional code of rate @xmath61 with a _ input generator matrix _",
    "@xmath62 implemented at the source of the network .    the _ output convolutional code _",
    "@xmath63 , corresponding to a sink node @xmath51 in the acyclic , unit - delay , memory - free network is the @xmath61 convolutional code generated by the _ output generator matrix _ @xmath64 which is given by @xmath65 with @xmath66 being the full rank network transfer matrix corresponding to an @xmath1-dimensional network code .     network with one source and one sink , width=336 ]    consider the single source , single sink network as shown in fig.[fig : simplenetwork ] .",
    "let the field under consideration be @xmath67 the local kernels at the intermediate node are unity .",
    "therefore the network transfer matrix at the sink is ( assuming the given ancestral ordering ) @xmath68\\ ] ] suppose we choose the input convolutional code @xmath60 to be generated by the matrix @xmath69.\\ ] ] then the output convolutional code @xmath63 is generated by @xmath70.\\ ] ]      observing a ` snap - shot ' of the network at any particular time instant , we define the following terms .",
    "an _ error pattern _ @xmath71 as stated previously , is a subset of @xmath27 which indicates the edges of the network in error .",
    "an _ error vector _",
    "@xmath72 is a @xmath73 vector which indicates the error occurred at each edge .",
    "an error vector is said to match an error pattern @xmath74 if all non - zero components of @xmath72 occur only on the edges in @xmath75 . an _ error pattern set _",
    "@xmath0 is a collection of subsets of @xmath27 , each of which is an error pattern .",
    "let @xmath56 $ ] be the input to the network , and @xmath76 be the error vector corresponding to the network errors that occurred at any time instant @xmath36 ( @xmath77 , referenced from the first input time instant ) .",
    "then , the output , @xmath58 $ ] at any particular sink @xmath57 can be expressed as @xmath78 in case there are a number of errors at a number of time instants , we have the formulation as @xmath79 wherein every monomial of @xmath80 $ ] of the form @xmath81 incorporates the error vector @xmath82 occurring at the time instant @xmath83",
    "in section [ construction ] , we give a construction of a cnecc for a given acyclic , unit - delay , memory - free network . towards that end , we first address the problem of constructing network codes for acyclic , unit - delay , memory - free networks . although network code constructions have been given for acyclic instantaneous networks @xcite , the problem of constructing network codes for acyclic , unit - delay , memory - free networks is not directly addressed .",
    "the following lemma shows that solving an @xmath1-dimensional network code design problem for an acyclic , unit - delay , memory - free network is equivalent to solving that of the corresponding acyclic instantaneous network with the same number of dimensions .",
    "let @xmath84 be a single source acyclic , unit - delay , memory - free network , and @xmath85 be the corresponding instantaneous network ( i.e with the same graph as that of @xmath2 , but no delay associated with the edges ) .",
    "let @xmath86 be the set of all @xmath87 matrices @xmath88 @xmath89 , i.e , the set of local encoding kernel matrices at each node , describing an @xmath90-dimensional network code ( over @xmath6 ) for @xmath85 ( @xmath91 min - cut of the source - sink connections in @xmath85 )",
    ". then the network code described by @xmath92 continues to be an @xmath90-dimensional network code ( over @xmath52 ) for the unit - delay , memory - free network @xmath93    let @xmath94 be the @xmath95 network transfer matrix of any particular sink node @xmath57 in @xmath85 , and @xmath66 be the @xmath95 network transfer matrix of the same sink @xmath13 in @xmath96 we first note that the matrix @xmath94 can be obtained from @xmath66 by substituting @xmath97 , i.e , @xmath98 given that @xmath94 is full rank over @xmath6 , we will prove that @xmath66 is full rank over @xmath52 by contradiction .",
    "suppose that @xmath66 was not full rank over @xmath52 , then we will have @xmath99 where @xmath100 is the @xmath101 row of @xmath66 and @xmath102 $ ] @xmath88 @xmath103 are such that @xmath104 for at least one @xmath36 , and @xmath105    we have the following two cases    _ case 1 _ : @xmath106 @xmath107    substituting @xmath108 in ( [ eqn1 ] ) , we have @xmath109 where @xmath110 and @xmath111 is the @xmath101 row of @xmath112    clearly @xmath113 since @xmath94 is full rank , and hence the left hand side of ( [ eqn3 ] ) ca nt be zero",
    ". therefore some non - zero linear combination of the first @xmath114 rows of @xmath94 is equal to its @xmath115 row , which contradicts the given statement that @xmath94 is full rank over @xmath116 therefore @xmath66 must be full rank over @xmath117    _ case 2 _ : @xmath118 for at least one @xmath83    let @xmath119 such that @xmath120 for some positive integer @xmath121 let @xmath122 be an integer such that @xmath123 now , from ( [ eqn1 ] ) we haven @xmath124 let @xmath125 such that @xmath126 @xmath88 @xmath127 then we must have that @xmath128 @xmath88 @xmath129 since @xmath130 also , let @xmath131 $ ] @xmath88 @xmath127 hence we have @xmath132 where @xmath133 , since @xmath134 substituting @xmath135 in ( [ eqn2 ] ) , we have @xmath136 i.e , a non - zero linear combination of the rows of @xmath94 is equal to zero , which contradicts the full - rankness of @xmath94 , thus proving that @xmath66 has to be full rank over @xmath117      this subsection presents the main contribution of this work .",
    "we assume an @xmath1 dimensional network code ( @xmath1 being the min - cut ) on this network has implemented on the given network which is used to multicast information to a set of sinks .",
    "we describe a construction of an input convolutional code for the given acyclic , unit - delay , memory - free network which can correct network errors with patterns in a given error pattern set , as long as they are separated by certain number of network uses .",
    "let @xmath137 be the @xmath138 network transfer matrix from the source to any particular sink @xmath32 .",
    "let @xmath0 be the error pattern set given .",
    "we then define the _ processing matrix at sink t _",
    ", @xmath139 , to be a polynomial matrix as @xmath140 where @xmath141 $ ] is some _ processing function _ chosen such that @xmath139 is a polynomial matrix .",
    "now , we have the construction of a cnecc for the given network as follows .    1 .",
    "we first compute the set of all error vectors having their error pattern in @xmath0 that is defined as follows @xmath142 2 .",
    "let @xmath143 be computed for each sink @xmath13 .",
    "this is the set of @xmath1-tuples ( with elements from @xmath144 $ ] ) at the sink @xmath13 due to errors in the given error patterns @xmath145 .",
    "3 .   let the set @xmath146 $ ] @xmath147 be computed .",
    "4 .   let @xmath148 where @xmath149 indicates the hamming weight over @xmath116 5 .",
    "choose an input convolutional code @xmath60 with free distance at least @xmath150 as the cnecc for the given network .      before we discuss the decoding of cneccs designed",
    "according to subsection [ construction ] , we state some of the results from @xcite related to the bounded distance decoding of convolutional codes in this section .",
    "let @xmath151 be a rate @xmath152 convolutional code with a generator matrix @xmath153 then , corresponding to the information sequence @xmath154 and the codeword sequence @xmath155 , we can associate an encoder state sequence @xmath156 , where @xmath157 indicates the content of the delay elements in the encoder at a time @xmath158 we define the set of @xmath159 output symbols as @xmath160\\ ] ] the parameter @xmath161 @xcite is defined as follows . @xmath162where",
    "@xmath163 @xcite is defined as the set of all possible truncated codeword sequences @xmath164 of weight less than @xmath165 that start in the zero state is defined as follows @xmath166 where @xmath149 indicates the hamming weight over @xmath116 then , we have the following proposition .",
    "[ minweighttime ] the minimum hamming weight trellis decoding algorithm can correct all error sequences which have the property that the hamming weight of the error sequence in any consecutive @xmath161 segments ( a segment being the set of @xmath167 code symbols generated for every @xmath167 information symbols ) is utmost @xmath168 .",
    "now , we discuss the decoding of cneccs for unit - delay memory - free networks .",
    "let @xmath169 be the @xmath170 generator matrix of the input convolutional code , @xmath60 , obtained from the given construction .",
    "let @xmath171 be the generator matrix of the output convolutional code , @xmath172 , at sink @xmath44 , with @xmath66 being its network transfer matrix .    for each sink @xmath57 ,",
    "let @xmath173 let @xmath174 be the largest integer such that @xmath175 clearly , @xmath176 each sink can choose decoding on the trellis of the input or its output convolutional code based on the characteristics of the output convolutional code as follows    _ case - a : _ this is applicable in the event of all of the following conditions being satisfied .    1 .",
    "@xmath177 2 .",
    "@xmath178 3 .",
    "the output convolutional code generator matrix @xmath179 is non - catastrophic .",
    "@xmath180    in this case , the sink @xmath13 performs minimum distance decoding directly on the trellis of the output convolutional code , @xmath172 .",
    "_ case - b : _ this is applicable if at least one of the @xmath181 conditions of case - a is not satisfied , i.e , if either of the following conditions hold    1 .",
    "@xmath182 2 .",
    "@xmath183 3 .",
    "the output convolutional code generator matrix @xmath179 is catastrophic .      in @xcite ,",
    "the approach to the construction of a cnecc for an instantaneous network was the same as in here .",
    "however , the set @xmath193 was defined in @xcite as @xmath194 where the network transfer matrix @xmath94 and @xmath195 correspond to a sink @xmath13 in the instantaneous network .    in this paper ,",
    "the definition for @xmath193 is as in ( [ eqn5 ] ) and involves the processing matrix @xmath139 instead of the inverse of the network transfer matrix .",
    "the processing function @xmath196 for a sink @xmath13 is introduced because of the fact that the matrix @xmath197 might not be realizable and also for easily obtaining the hamming weight of the _ error vector reflections _ @xmath198 by removing rational functions in @xmath199    the degree of the processing function @xmath196 directly influences the memory requirements at the sinks and therefore should be kept as minimal as possible .",
    "therefore , with @xmath200 where the @xmath53 matrix @xmath201 is the adjoint of @xmath66 , ideally we may choose @xmath196 as follows .",
    "@xmath202 where @xmath203 , @xmath204 being the @xmath205 element of @xmath206      in this subsection we prove a main result of the paper given by theorem [ maintheorem ] which characterizes the error correcting capability of the code obtained via the construction of subsection [ construction ] .",
    "we recall the following observation that in every network use , @xmath1 encoded symbols which is equal to the number of symbols corresponding to one segment of the trellis , are to be multicast to the sinks .",
    "[ maintheorem ] the code @xmath60 resulting from the construction of subsection [ construction ] can correct all network errors that have their pattern as some @xmath145 as long as any two consecutive network errors are separated by @xmath207 network uses .",
    "we first prove the theorem in the event of case - a of the decoding .",
    "suppose the network errors are such that consecutive network errors are separated by @xmath207 network uses .",
    "then the vector of error sequences at sink @xmath13 , @xmath208 , is such that in every @xmath207 segments , the error sequence has utmost @xmath209 hamming weight ( over @xmath6 ) .",
    "therefore in @xmath210 segments , the hamming weight of the error sequence would be utmost @xmath211    then the given condition ( [ decodacond2 ] ) would imply that in every @xmath212 segments of the output trellis , the error sequences have hamming weight utmost @xmath211 condition ( [ decodacond1 ] ) together with ( [ eqn11 ] ) and proposition [ minweighttime ] implies that these error sequences are correctable .",
    "this proves the given claim that errors with their error pattern in @xmath0 will be corrected as long as no two consecutive error events occur within @xmath207 network uses .",
    "in fact , condition ( [ decodacond1 ] ) and ( [ eqn11 ] ) implies that network errors with pattern in @xmath0 will be corrected at sink @xmath13 , as long as consecutive error events are separated by @xmath213 .",
    "now we consider case b of the decoding .",
    "suppose that the set of error sequences in the formulation given , @xmath214 , is due to network errors that have their pattern as some @xmath145 , such that any two consecutive such network errors are separated by at least @xmath207 network uses .",
    "therefore , along with step @xmath215 of the construction , we have that the maximum hamming weight of the error sequence @xmath214 in any consecutive @xmath207 segments ( network uses ) would be utmost @xmath216 .",
    "because of the free distance of the code chosen and along with proposition [ minweighttime ] , we have that such errors will get corrected when decoding on the trellis of the input convolutional code .        towards obtaining a bound on the sufficient field size for the construction of a cnecc meeting our free distance requirement , we first prove the following lemmas .",
    "[ tdelay ] given an acyclic , unit - delay , memory - free network @xmath84 with a given error pattern set @xmath0 , let @xmath217 be the maximum degree of any polynomial in the @xmath218 matrix .",
    "let @xmath149 indicate the hamming weight over @xmath116 if @xmath219 is the maximum number of non - zero coefficients of the polynomials @xmath196 corresponding to all sinks in @xmath9 , i.e @xmath220 then @xmath221.\\ ] ] where @xmath193 is as in ( [ eqn5 ] ) in subsection [ construction ] .",
    "any element @xmath222 indicates the @xmath1 length sequences that would result in an output vector @xmath208 at some sink @xmath13 as a result of an error vector @xmath72 in the network at time @xmath223 , i.e @xmath224    because of the fact that any polynomial in @xmath218 has degree utmost @xmath225 , any error vector @xmath72 at time @xmath223 can result in non - zero symbols ( over @xmath226 ) in @xmath208 at any sink @xmath13 from the @xmath227 time instant only upto utmost @xmath225 time instants .",
    "@xmath228 where @xmath229    the numerator polynomial of any element @xmath230 of the matrix @xmath197 has degree utmost @xmath231 .",
    "therefore , considering the polynomial processing matrix @xmath232 , we note that any element from @xmath139 has utmost @xmath233 $ ] non - zero components ( over @xmath6 ) , the worst case being @xmath219 non - overlapping ` blocks ' of @xmath234 non - zero components each .",
    "therefore the first non - zero symbol of @xmath208 ( over @xmath226 ) at some time instant can result in utmost @xmath233 $ ] non - zero symbols in @xmath235 ( over @xmath226 ) .",
    "henceforth , every consecutive non - zero symbol ( over @xmath226 ) of @xmath208 will result in utmost additional @xmath219 @xmath226 symbols in @xmath236 therefore any @xmath237 is of the form @xmath238}\\boldsymbol{w}_{s , i}z^i \\right)\\ ] ] where @xmath239 therefore the hamming weight ( over @xmath6 ) of any @xmath237 is utmost @xmath240 $ ] , thus proving the lemma .",
    "our bound on the field size requirement of cneccs for unit - delay networks is based on the bound on field size for the construction of maximum distance separable ( mds ) convolutional codes @xcite , a primer on which can be found in appendix [ app2 ] .",
    "[ deltabound ] a @xmath241 mds convolutional code @xmath151 ( over some field @xmath6 ) with degree @xmath242 can correct any error sequence which has the property that the hamming weight(over @xmath6 ) of the error sequence in any consecutive @xmath161 segments is utmost @xmath158    because the generalized singleton bound is satisfied with equality by the mds convolutional code , we have @xmath243 substituting @xmath244 for @xmath245 , we have @xmath246 thus the free distance of the code @xmath151 is at least @xmath247 , and therefore by proposition [ minweighttime ] , such a code can correct all error sequences which have the property that in any consecutive @xmath161 segments ,",
    "the hamming weight ( over @xmath6 ) of the error sequence is utmost @xmath158    for an mds convolutional code being chosen as the input convolutional code ( cnecc ) , we therefore have the following corollary    [ deltaboundnec ] let @xmath84 be an acyclic , unit - delay , memory - free network with a network code over a sufficiently large field @xmath6 and @xmath0 be an error pattern set , the errors corresponding to which are to be corrected .",
    "an @xmath241 input mds convolutional code @xmath60 over @xmath6 with degree @xmath248 $ ] can be used to correct all network - errors with their error pattern in @xmath0 provided that consecutive network - errors are separated by at least @xmath207 network uses , where @xmath219 and @xmath249 are as in lemma [ tdelay ] .    from lemma [ tdelay ] , we have that in the construction of subsection [ construction ] , the maximum hamming weight @xmath216 of any element in the set @xmath193 is utmost @xmath240.$ ] for an input mds convolutional code @xmath60 to be capable of correcting such errors with hamming weight utmost @xmath240 $ ] , according to lemma [ deltabound ] , a degree @xmath250 $ ] would suffice .",
    "the following theorem gives a sufficient field size for the required network error correcting @xmath241 input convolutional code @xmath60 to be constructed with the required free distance condition ( @xmath251 ) .",
    "[ fieldsizebound ] the code @xmath60 can be constructed and used to multicast @xmath188 symbols to the set of sinks @xmath252 along with the required error correction in the given acyclic , unit - delay , memory - free network with min - cut @xmath1 ( @xmath253 ) , if the field size @xmath254 is such that @xmath255}{n - k}+2\\right\\}.\\end{aligned}\\ ] ]    from the sufficient condition for the existence of a linear multicast network code for a single source network with a set of sinks @xmath9 , we have @xmath256 now we prove the other conditions . from the construction in @xcite , we know that a @xmath257 mds convolutional code can be constructed over @xmath6 if @xmath258    thus , with @xmath250 $ ] as in corollary [ deltaboundnec ] , an input mds convolutional code @xmath60 can be constructed over @xmath6 if @xmath259}{n - k}+2.\\ ] ] such an mds convolutional code the requirements in the construction @xmath260 + 1 \\geq 2t_s+1\\right)$ ] , and hence the theorem is proved .      towards obtaining a bound on @xmath207 ,",
    "we first restate the following bound proved in @xcite .",
    "let @xmath151 be a @xmath261 convolutional code",
    ". then @xmath262    thus , for a network error correcting mds convolutional code @xmath60 for the unit - delay network , we have the following bound on @xmath207 .",
    "let us consider the modified butterfly network as shown in fig .",
    "[ fig : butterflydelay ] , with one of the edges at the bottleneck node ( of the original unmodified butterfly network ) having twice the delay as any other edge , thus forcing an inter - generation linear combination at the bottleneck node .",
    "the local kernels at the node defining the network code are the same as in that of the instantaneous butterfly case .",
    "we assume the network code to be over @xmath268 and we design a convolutional code over @xmath268 that will correct all single edge errors in the network , i.e , all network error vectors of hamming weight utmost @xmath269        for this network , the matrix @xmath29 is a @xmath270 matrix having a @xmath271 identity submatrix at the columns corresponding to edges @xmath272 and @xmath273 , and having zeros everywhere else .",
    "we assume @xmath274 and @xmath275 are @xmath276 matrices such that they have a @xmath271 identity submatrix at rows @xmath277 and @xmath278 respectively . with the given network code",
    ", we thus have the network transfer matrices at sink @xmath279 and @xmath280 as follows @xmath281=af_{t_1}(z)\\ ] ] where @xmath282^t\\ ] ] and @xmath283=af_{t_2}(z)\\ ] ] where @xmath284^t.\\ ] ]    for single edge errors , we have the error pattern set to be @xmath285 and thus the set @xmath286 is the set of all vectors @xmath268 that have hamming weight utmost @xmath269 the sets @xmath287 and @xmath288 as in ( [ eqn13 ] ) and ( [ eqn14 ] ) at the top of the next page .",
    "@xmath289    ' '' ''    now @xmath290\\ ] ] and @xmath291.\\ ] ] to obtain the processing matrices @xmath292 and @xmath293 , let us choose the processing functions @xmath294 and @xmath295 then we have @xmath296\\ ] ] and @xmath297.\\ ] ] therefore , @xmath193 can be computed to be as in ( [ eqn15 ] ) at the top of the next page .",
    "@xmath298    ' '' ''    thus we have @xmath299 , which means that we need a convolutional code with free distance at least @xmath300 let the chosen input convolutional code @xmath60 be generated by the generator matrix @xmath301.\\ ] ] this code has a free distance @xmath302 and @xmath303 therefore this code can be used to correct single edge errors in the butterfly network as long as consecutive errors are separated by @xmath304 network uses . with this code ,",
    "the output convolutional code @xmath305 at sink @xmath279 is generated by the matrix @xmath306\\ ] ] now @xmath305 has @xmath307 and @xmath308 . as condition ( [ decodacond2 ] ) is not satisfied , case - b applies and hence the sink @xmath279 has to use the processing matrix @xmath292 , and then decode on the trellis of the input convolutional code . upon performing a similar analysis for sink @xmath280 , we have table [ tab1 ] as shown at the top of the next page .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]     [ tab2 ]    similarly the sets @xmath309 and @xmath310 are computed .",
    "it is seen that for this network , @xmath311 and @xmath312 therefore we need a convolutional code with free distance @xmath313 to correct such errors .",
    "let this input convolutional code @xmath60 over @xmath314 be chosen as the code generated by @xmath315.\\ ] ] this code is found to have @xmath316 with @xmath317 thus it can correct all double edge network errors as long as consecutive network errors are separated by @xmath318 network uses .",
    "the output convolutional codes @xmath319 , their free distance and @xmath319 are computed and tabulated in table [ tab2 ] at the top of the next page . for this example , all the sinks satisfy the conditions ( [ decodacond1 ] ) and ( [ decodacond2 ] ) for case - a of the decoding and therefore decode on the trellises of the corresponding output convolutional codes .",
    "in the following discussion , we compare the cneccs for a given instantaneous network constructed in @xcite and the cneccs of subsection [ construction ] for the corresponding unit - delay , memory - free network .    with the given acyclic graph @xmath84",
    ", we will compare the maximum hamming weight @xmath216 of any @xmath1-tuple , over @xmath144 $ ] ( @xmath237 , where @xmath193 is as in ( [ eqn5 ] ) ) in the case of the unit - delay , memory - free network with the graph @xmath2 and over @xmath6 ( @xmath320 where @xmath193 is as in ( [ eqn12 ] ) ) in the case of instantaneous network with the graph @xmath2 .",
    "consider some @xmath237 such that @xmath321\\end{aligned}\\ ] ] where @xmath196 and @xmath139 indicate the processing function and matrix chosen according to ( [ eqn6 ] ) for some sink @xmath44 , and @xmath322.$ ] we have @xmath323 and also @xmath324 , the network transfer matrix and the @xmath195 matrix of the sink @xmath13 in the instantaneous network . now , by ( [ eqn7 ] ) , we have the @xmath1-length vector @xmath325 corresponding to the error vector @xmath72 as @xmath326 where @xmath327 by ( [ eqn6 ] ) . now",
    "@xmath328 since @xmath94 is full rank .",
    "also , @xmath329 for the same reason .",
    "therefore , @xmath330 thus we have @xmath331 therefore a cnecc for an instantaneous network may require a lesser free distance to correct networks errors matching one of the given set of patterns @xmath0 , while the cnecc for the corresponding unit - delay , memory - free network may require a larger free distance to provide the same error correction according to the construction of subsection [ construction ] .",
    "an example of this case is the code construction for double edge error correction for the @xmath332 combination instantaneous network in @xcite and for the @xmath332 unit - delay network in this paper in subsection [ subsec5b ] .",
    "it can be seen that while for the instantaneous network , the maximum hamming weight of any @xmath320 is @xmath333 , the maximum hamming weight of any @xmath237 in the unit - delay network is @xmath334 thus a code with free distance @xmath335 suffices for the instantaneous network , while the code for the unit - delay network has to have a free distance @xmath313 to ensure the required error correction as per the construction in subsection [ construction ] .",
    "it is in general not easy to obtain the general conditions under which equality will hold in ( [ eqn8 ] ) , as both the topology and the network code of the network influence the hamming weight of any element in @xmath336 for specific examples however , this can be checked .",
    "an example of this case is given in between the single edge - error correcting code construction for the butterfly network ( over @xmath268 ) for the instantaneous case in @xcite ( the additional intermediate node , @xmath337 , does not matter for the instantaneous case ) , and for the unit - delay case in this paper in subsection [ subsec5a ] . in both the cases , we have @xmath299 , which means that an input convolutional code with free distance @xmath335 is sufficient to correct all single edge network errors .",
    "however , as we see in subsection [ subsec5a ] , processing matrices with memory elements need to be used at the sinks for the unit - delay case , while the processing matrix in the instantaneous case is just the @xmath338 matrix which does not require any memory elements to implement .",
    "we define a probabilistic error model for a unit delay network @xmath84 by defining the probabilities of any set of @xmath339 edges of the network being in error at any given time instant as follows . across time instants ,",
    "we assume that the network errors are i.i.d . according to this distribution .",
    "@xmath340 where @xmath341 and @xmath342 are real numbers indicating the probability of any single edge error in the network and the probability of no edges in error respectively , such that @xmath343      with the probability model as in ( [ eq:1 ] ) and ( [ eq:2 ] ) with @xmath344 for the modified butterfly network as in fig .",
    "[ fig : butterflydelay ] , we simulate the performance of @xmath181 input convolutional codes implemented on this network with the sinks performing hard decision decoding on the trellis of the input convolutional code . in the following discussion",
    "we refer to sinks @xmath279 and @xmath280 of fig .",
    "[ fig : butterflydelay ] as sink 1 and sink 2 . the @xmath181 input convolutional codes and the rationality behind choosing them",
    "are given as follows .",
    "* code @xmath345 is generated by the generator matrix @xmath346,\\ ] ] with @xmath347 and @xmath348 this code is chosen only to illustrate the error correcting capability of codes with low values of @xmath165 and @xmath349 * code @xmath350 is generated by the generator matrix @xmath351,\\ ] ] with @xmath352 and @xmath353 this code corrects all double edge errors in the instantaneous version ( with all edge delays being zero ) of fig .",
    "[ fig : butterflydelay ] as long as they are separated by @xmath304 network uses .",
    "* code @xmath354 is generated by the generator matrix @xmath355,\\ ] ] with @xmath356 and @xmath357 this code corrects all double edge errors in the unit - delay network given in fig . [",
    "fig : butterflydelay ] as long as they are separated by @xmath358 network uses .",
    "we note here that values of @xmath161 of the @xmath181 codes are directly proportional to their free distances , i.e , the code with greater free distance has higher @xmath161 .",
    "also we note that with each of these @xmath181 codes as the input convolutional codes , the output convolutional codes violate at least one of the conditions of ` case - a ' of decoding , i.e , ( [ decodacond1]),([decodacond2 ] ) , or ( [ decodacond3 ] ) . therefore , hard decision viterbi decoding is performed on the trellis of the input convolutional code .",
    "[ fig : bersink1 ] and fig .",
    "[ fig : bersink2 ] illustrate the bers for different values for the parameter @xmath122 ( the probability of a single edge error ) of ( [ eq:1 ] ) .",
    "clearly the ber values fall with decreasing @xmath359        ' '' ''        ' '' ''    it may be observed that between any two of the @xmath181 codes , say @xmath360 and @xmath361 ( @xmath362 ) there exist a particular value of @xmath363 where the ber performance corresponding to the two codes gets reversed , i.e , if code @xmath360 has better ber performance than @xmath361 for any @xmath364 , then @xmath361 performs better than @xmath360 for any @xmath365 although such a cross - over value of @xmath122 exists for each pair of codes , we see that all @xmath181 codes have approximately the same crossover @xmath122 value in fig .",
    "[ fig : bersink1 ] ( @xmath366 ) and similarly in fig .",
    "[ fig : bersink2 ] ( @xmath367 ) .    with respect to such crossover points between the two codes @xmath360 and @xmath361",
    ", we can divide the performance curve into two regions which we call as ` @xmath368 dominated region ' ( @xmath122 values being greater than the crossover @xmath122 value ) and ` @xmath369 dominated region ' ( @xmath122 values being lesser than the crossover @xmath122 value ) , indicating the parameter which controls the performance of the codes in each of those regions respectively . again , because of the @xmath181 crossover points being approximately equal to one another in each of fig .",
    "[ fig : bersink1 ] and fig .",
    "[ fig : bersink2 ] , we divide the entire performance graph of all the @xmath181 codes into two regions .",
    "the following discussion gives an intuition into why the parameters @xmath161 and @xmath165 control the performance in the corresponding regions .",
    "* @xmath369 _ dominated region _ : in the @xmath369 dominated region , codes with higher free distance perform better than those with less free distance .",
    "we recall from proposition [ minweighttime ] that both the hamming weight of error events and the separation between any two consecutive error events are important to correct them .",
    "because of the fact @xmath122 is low in the @xmath369 dominated region , the hamming weight of the modified error sequences of ( [ eq:3 ] ) is less , and the error events that occur are also separated by sufficient number of network uses . therefore the condition on the separation of error events according to proposition [ minweighttime ] is automatically satisfied even for large @xmath161 codes . therefore codes which have more free distance ( though having more @xmath161 ) correct more errors than codes with low free distance ( though having less @xmath161 ) .",
    "it is noted that in this region the code @xmath354 ( which was designed for correcting double edge errors on the unit - delay network ) performs better than @xmath350 ( which was designed for correcting double edge errors on the instantaneous version of the network ) .",
    "* @xmath368 _ dominated region _ : in the @xmath368 dominated region , codes with lower @xmath161 perform better than codes with higher @xmath161 , even though their free distances might actually indicate otherwise .",
    "this is because of the fact that the error events related to the modified error sequences of ( [ eq:3 ] ) occur more frequently with lesser separation of network uses ( as @xmath122 is higher ) .",
    "therefore the codes with lower @xmath161 are able to correct more errors ( even though the errors themselves must accumulate less hamming weight to be corrected ) than the codes with higher @xmath161 which demand more separation in network uses between error events for them to be corrected ( despite having a greater flexibility in the hamming weight accumulated by the correctable error events ) .    the difference in the performance of code @xmath345 between sink 1 and sink 2 is probably due to the unequal error protection to the two code symbols .",
    "when the code is ` reversed ' , i.e. with @xmath370 $ ] , it is observed that the performance at the sinks are also interchanged for unchanged error characteristics .",
    "in this work , we have extended the approach of @xcite to introduce network error correction for acyclic , unit - delay , memory - free networks . a construction of cneccs for acyclic , unit - delay , memory - free networks has been given , which corrects errors corresponding to a given set of patterns as long as consecutive errors are separated by a certain number of network uses .",
    "bounds are derived on the field size required for the construction of a cnecc with the required error correction capability and also on the minimum separation in network uses between any two consecutive network errors .",
    "simulations assuming a probabilistic error model on a modified butterfly network indicate the implementability and performance tractability of such cneccs .",
    "the following problems remain to be investigated .",
    "* investigation of error correction bounds for network error correction in unit - delay , memory - free networks .",
    "* joint design of the cnecc and network code .",
    "* investigation of distance bounds for cneccs .",
    "* design of appropriate processing matrices at the sinks to minimize the maximum hamming weight of the error sequences . * construction of cneccs which are optimal in some sense . * further analytical studies on the performance of cneccs on unit - delay networks .",
    "this work was supported partly by the drdo - iisc program on advanced research in mathematical engineering to b.  s.  rajan .",
    "160 r. ahlswede , n. cai , r. li and r. yeung , `` network information flow '' , ieee transactions on information theory , vol.46 , no.4 , july 2000 , pp .",
    "1204 - 1216 .",
    "n. cai , r. li and r. yeung , `` linear network coding '' , ieee transactions on information theory , vol .",
    "2003 , pp .",
    "371 - 381 .",
    "r. koetter and m. medard , `` an algebraic approach to network coding '' , ieee / acm transactions on networking , vol .",
    "2003 , pp .",
    "782 - 795 .",
    "raymond w. yeung and ning cai , `` network error correction , part 1 and part 2 '' , comm . in inform . and systems , vol .",
    "6 , 2006 , pp .",
    "19 - 36 .",
    "zhen zhang , `` linear network - error correction codes in packet networks '' , ieee transactions on information theory , vol .",
    "2008 , pp .",
    "209 - 218 .",
    "shenghao yang and yeung , r.w .",
    ", `` refined coding bounds for network error correction '' , itw on information theory for wireless networks , july 16 , 2007 , bergen , norway , pp",
    ". 1 - 5 .",
    "r. koetter and f.r .",
    "kschischang , `` coding for errors and erasures in random network coding '' , ieee transactions on information theory , vol .",
    "54 , no . 8 , aug .",
    "2008 , pp.3579 - 3591 .",
    "d. silva , f .",
    "r kschischang , and r. koetter , `` a rank - metric approach to error control in random network coding '' , ieee transactions on information theory , vol .",
    "2008 , pp . 3951 - 3967 .",
    "t. etzion and n. silberstein , `` error - correcting codes in projective spaces via rank - metric codes and ferrers diagrams '' , arxiv:0807.4846v3[cs.it ] , july 2008 , available at : http://arxiv.org/abs/0807.4846 .",
    "d. silva , f .",
    "r kschischang , and r. koetter , `` capacity of random network coding under a probabilistic error model '' , 24th biennial symposium on communications , june 24 - 26 , 2008 , kingston , usa , pp .",
    "9 - 12 .",
    "roberto w. nbrega and bartolomeu f. ucha - filho , `` multishot codes for network coding : bounds and a multilevel construction '' , arxiv:0901.1655v1 [ cs.it ] , jan .",
    "2009 , available at : http://arxiv.org/abs/0901.1655 .",
    "e. erez and m. feder , `` convolutional network codes '' , isit , june 27-july 2 , 2004 , chicago , illinois , usa , pp . 146 .",
    "n. cai , r. li , r. yeung , z. zhang , `` network coding theory '' , foundations and trends in communications and information theory , vol .",
    "2 , no.4 - 5 , 2006 .    s. r. li and r. yeung , `` on convolutional network coding '' , isit , july 9 - 14 , 2006 , seattle , washington , usa , pp . 1743 - 1747 .    c. fragouli , and e. soljanin , `` a connection between network coding and convolutional codes '' , icc , june 20 - 24 , 2004 , paris , france , 2004 , vol .",
    "2 , pp . 661 - 666 .    k. prasad and b. sundar rajan , `` convolutional codes for network - error correction '' , arxiv:0902.4177v3 [ cs.it ] , august 2009 , available at : http://arxiv.org/abs/0902.4177 .",
    "a shortened version of this paper is to appear in the proceedings of globecom 2009 , nov .",
    "4 , honolulu , hawaii , usa .",
    "s. jaggi , p. sanders , p. a. chou , m. effros , s. egner , k. jain , and l. m. g. m. tolhuizen , `` polynomial time algorithms for multicast network code construction '' , ieee transactions on information theory , vol .",
    "51 , no . 6 , pp .",
    "1973 - 1982 , june 2005 .",
    "j. rosenthal and r. smaradanche , `` maximum distance separable convolutional codes '' , appl.algebra engrg .",
    "1 , june 1999 , pp .",
    "15 - 32 .",
    "j. rosenthal , h. gluesing - luerssen , and r.smaradanche , `` construction of mds convolutional codes '' , appl.algebra engrg .",
    "5 , july 2001 , pp . 2045 - 2049 .",
    "r. johannesson and k.s zigangirov , fundamentals of convolutional coding , john wiley , 1999 .",
    "g. d. forney , `` bases of rational vector spaces with applications to multivariable linear systems '' , siam j. contr .",
    "13 , no . 3 , 1975 , pp .",
    "493 - 520 .",
    "we review the basic concepts related to convolutional codes , used extensively throughout the rest of the paper . for @xmath371 power of a prime ,",
    "let @xmath6 denote the finite field with @xmath254 elements , @xmath144 $ ] denote _ the ring of univariate polynomials _ in @xmath7 with coefficients from @xmath372 @xmath52 denote _ the field of rational functions _ with variable @xmath7 and coefficients from @xmath6 and @xmath55 $ ] denote _ the ring of formal power series _ with coefficients from @xmath6 .",
    "every element of @xmath55 $ ] of the form @xmath373 .",
    "thus , @xmath144 \\subset \\mathbb{f}_q[[z]]$ ] .",
    "we denote the set of @xmath1-tuples over @xmath55 $ ] as @xmath374 $ ] .",
    "also , a rational function @xmath375 with @xmath376 is said to be _",
    "realizable_. a matrix populated entirely with realizable functions",
    "is called a realizable matrix .    for a convolutional code , the _ information sequence _",
    "@xmath377(\\boldsymbol{u}_i\\in\\mathbb{f}_q^b)$ ] and the _ codeword sequence _ ( output sequence ) @xmath378\\left(\\boldsymbol{v}_i\\in\\mathbb{f}_q^c\\right)$ ] can be represented in terms of the delay parameter @xmath7 as @xmath379    a _ convolutional code _ , @xmath380 of rate @xmath381 is defined as @xmath382~|~ \\boldsymbol{v}(z)=\\boldsymbol{u}(z)g(z ) \\}\\ ] ] where @xmath383 is a @xmath384 _ generator matrix _ with entries from @xmath52 and rank @xmath385 over @xmath52 , and @xmath386 being the codeword sequence arising from the information sequence , @xmath387 $ ] .",
    "two generator matrices are said to be _ equivalent _ if they encode the same convolutional code .",
    "_ polynomial generator matrix__@xcite for a convolutional code @xmath151 is a generator matrix for @xmath151 with all its entries from @xmath144 $ ] .",
    "it is known that every convolutional code has a polynomial generator matrix @xcite .",
    "also , a generator matrix for a convolutional code is _ _",
    "catastrophic__@xcite if there exists an information sequence with infinitely many non - zero components , that results in a codeword with only finitely many non - zero components .    for a polynomial generator matrix @xmath383 ,",
    "let @xmath388 be the element of @xmath383 in the @xmath101 row and the @xmath389 column , and @xmath390 be the @xmath101 _ row degree _ of @xmath383 .",
    "let @xmath391 be the _ degree _ of @xmath153    a polynomial generator matrix is called _ basic _ if it has a polynomial right inverse .",
    "it is called _ minimal _ if its degree @xmath245 is minimum among all generator matrices of @xmath151 .",
    "forney in @xcite showed that the ordered set @xmath392 of row degrees ( indices ) is the same for all minimal basic generator matrices of @xmath151 ( which are all equivalent to one another ) .",
    "therefore the ordered row degrees and the degree @xmath245 can be defined for a convolutional code @xmath393 a rate @xmath152 convolutional code with degree @xmath245 will henceforth be referred to as a @xmath261 code .",
    "also , any minimal basic generator matrix for a convolutional code is non - catastrophic .",
    "a _ convolutional encoder _ is a physical realization of a generator matrix by a linear sequential circuit .",
    "two encoders are said to be _ equivalent encoders _ if they encode the same code .",
    "minimal encoder _ is an encoder with the minimal number of delay elements among all equivalent encoders .",
    "the weight of a vector @xmath394 $ ] is the sum of the hamming weights ( over @xmath6 ) of all its @xmath395-coefficients .",
    "then we have the following definitions .",
    "the _ free distance _ of a convolutional code @xmath151 is given as @xmath396",
    "we discuss some results on the existence and construction of maximum distance separable ( mds ) convolutional codes . the following bound on the free distance , and the existence of codes meeting",
    "the bound , called mds convolutional codes , was proved in @xcite .        for any positive integers @xmath400 , @xmath245 and for any prime @xmath122",
    "there exists a field @xmath6 of characteristic @xmath122 , and a rate @xmath398 convolutional code @xmath151 of degree @xmath245 over @xmath6 , whose free distance meets the generalized singleton bound .    a method of constructing mds convolutional codes based on the connection between quasi - cyclic codes and convolutional codes was given in @xcite .",
    "the ordered forney indices for such codes are of the form @xmath401 where @xmath402 and @xmath403    it is known @xcite that the field size @xmath254 required for a @xmath257 convolutional code @xmath380 with @xmath165 meeting the generalized singleton bound in the construction in @xcite needs to be a prime power such that @xmath404"
  ],
  "abstract_text": [
    "<S> a single source network is said to be _ memory - free _ </S>",
    "<S> if all of the internal nodes ( those except the source and the sinks ) do not employ memory but merely send linear combinations of the symbols received at their incoming edges on their outgoing edges . in this work , we introduce network - error correction for single source , acyclic , unit - delay , memory - free networks with coherent network coding for multicast . </S>",
    "<S> a convolutional code is designed at the source based on the network code in order to correct network - errors that correspond to any of a given set of error patterns , as long as consecutive errors are separated by a certain interval which depends on the convolutional code selected . </S>",
    "<S> bounds on this interval and the field size required for constructing the convolutional code with the required free distance are also obtained . </S>",
    "<S> we illustrate the performance of convolutional network error correcting codes ( cneccs ) designed for the unit - delay networks using simulations of cneccs on an example network under a probabilistic error model . </S>"
  ]
}