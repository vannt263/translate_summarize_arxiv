{
  "article_text": [
    "in this article , we consider a sequence of stochastic differential equations with jumps @xmath0 here @xmath1 is a standard wiener process , @xmath2 is a compensated poisson random measure , and @xmath3 is nonrandom ( see section  [ sec : prelim ] for precise assumptions ) . assuming that @xmath4 , @xmath5 , @xmath6 , and @xmath7 as @xmath8 in an  appropriate sense , we are interested in convergence of hitting times @xmath9 , @xmath8",
    ", where@xmath10 is the first time when the process @xmath11 hits the set @xmath12 .",
    "the study is motivated by the following observation .",
    "jump - diffusion processes are commonly used to model prices of financial assets .",
    "when the parameters of a jump - diffusion process are estimated with the help of statistical methods , there is an estimation error .",
    "thus , it is natural to investigate whether the optimal exercise strategies are close for two jump - diffusion processes with close parameters .",
    "moreover , we should study particular hitting times since , in the markovian setting , the optimal stopping time is the hitting time of the optimal stopping set .",
    "there is a lot of literature devoted to jump - diffusion processes and their applications in finance .",
    "the book @xcite gives an extensive list of references on the subject .",
    "the convergence of stopping times for diffusion and jump - diffusion processes was studied in @xcite .",
    "all these papers are devoted to the one - dimensional case , and the techniques are different from ours . here",
    "we generalize these results to the multidimensional case and also relax the assumptions on the convergence of coefficients . as an auxiliary result of independent interest",
    ", we prove the convergence of solutions under very mild assumptions on the convergence of coefficients .",
    "let @xmath13 be a standard stochastic basis with filtration @xmath14 satisfying the usual assumptions .",
    "let @xmath15 be a standard wiener process in @xmath16 , and @xmath17 be a poisson random measure on @xmath18 .",
    "we assume that @xmath1 and @xmath19 are compatible with the filtration  @xmath20 , that  is , for any @xmath21 and any @xmath22 and @xmath23)$ ] , the increment @xmath24 and the value @xmath25 are @xmath26-measurable and independent of  @xmath27 .",
    "assume in addition that @xmath17 is homogeneous , that is , for all @xmath22 and @xmath28 , @xmath29 = \\mu(a)\\lambda(b)$ ] , where @xmath30 is the lebesgue measure , @xmath31 is a @xmath32-finite measure on @xmath33 having no atom at zero .",
    "denote by @xmath2 the corresponding compensated measure , that is ,  @xmath34 for all @xmath35 .    for each integer @xmath36 ,",
    "consider a stochastic differential equation in @xmath37 @xmath38 in this equation , the initial condition @xmath39 is nonrandom , and the coefficients @xmath40 , @xmath41 , @xmath42 , @xmath43 , are nonrandom and measurable .",
    "in what follows , we abbreviate eq .   as @xmath44    for the rest of the article",
    ", we adhere to the following notation . by @xmath45",
    "we denote the absolute value of a number , the norm of a vector , or the operator norm of a matrix , and by @xmath46 the scalar product of vectors @xmath47 and @xmath48 ; @xmath49 .",
    "the symbol @xmath50 means a generic constant whose value is not important and may change from line to line ; a constant dependent on parameters @xmath51 will be denoted by @xmath52 .",
    "the following assumptions guarantee that eq .",
    "has a unique strong solution .",
    "* for all @xmath36 , @xmath53 , @xmath54 $ ] , @xmath55 , @xmath56 * for all @xmath36 , @xmath57 , @xmath54 $ ] , @xmath58 , and @xmath59 @xmath60    moreover , under these assumptions , for any @xmath57 , we have the following estimate : @xmath61 } \\bigl{\\vert}x^n(t ) \\bigr{\\vert}^2 \\bigr ] \\le c_{t } \\bigl(1 + \\bigl{\\vert}x^n(0 ) \\bigr{\\vert}^2 \\bigr)\\ ] ] ( see , e.g. ,  ( * ? ? ?",
    "* section  3.1 ) ) . from this estimate",
    "it is easy to see from eq .",
    "that for all @xmath62 $ ] , @xmath63\\le c_{t } \\bigl(1 + \\bigl{\\vert}x^n(0 ) \\bigr{\\vert}^2 \\bigr){\\vert}t - s{\\vert}.\\ ] ]    now we state the assumptions on the convergence of coefficients of .",
    "* for all @xmath64 and @xmath55 , @xmath65 * @xmath7 , @xmath8 .",
    "first , we establish a result on convergence of solutions to stochastic differential equations .",
    "[ ucp - thm ] let the coefficients of eq .",
    "( [ sde ] ) satisfy assumptions ( a1 ) , ( a2 ) , ( c1 ) , and  ( c2 ) .",
    "then , for any @xmath66 , we have the convergence in probability @xmath67 } \\bigl{\\vert}x^n(t)-x^0(t ) \\bigr{\\vert}\\overset { { \\mathsf{p } } } { \\longrightarrow}0 , \\quad n\\to\\infty.\\ ] ] if additionally the constant in assumption is independent of @xmath68 , then for any @xmath66 , @xmath69 } \\bigl{\\vert}x^n(t)-x^0(t ) \\bigr{\\vert}^2 \\bigr ] \\to0,\\quad n\\to\\infty.\\ ] ]    denote @xmath70 } { \\vert}x^n(t)-x^0(t){\\vert}$ ] , @xmath71 , @xmath72 , @xmath73 , @xmath74 it is easy to see that @xmath75 and @xmath76 are martingales .    write @xmath77 } \\bigl{\\vert}i_a^{n}(s)-i_a^{0}(s ) \\bigr{\\vert}^2 \\\\ & \\quad+ \\sup_{s\\in[0,t ] } \\bigl{\\vert}i_b^{n}(s)-i_b^{0}(s ) \\bigr{\\vert}^2 + \\sup_{s\\in[0,t ] } \\bigl{\\vert}i_c^{n}(s)-i_c^{0}(s ) \\bigr { \\vert}^2 \\bigr).\\end{aligned}\\ ] ] for @xmath78 , define @xmath79 and denote @xmath80",
    ". then @xmath81&\\le\\mathsf{e } \\bigl[\\vardelta^n \\bigl(t\\wedge\\sigma^n_n \\bigr)^2 \\bigr ] \\\\ & \\le c \\biggl(\\bigl{\\vert}x^n(0)-x^0(0 ) \\bigr{\\vert}^2 + \\sum_{x\\in\\{a , b , c\\}}\\mathsf{e } \\bigl[\\sup_{s\\in[0,t\\wedge\\sigma ^n_n ] } \\bigl{\\vert}i_x^{n}(s)-i_x^{0}(s)\\bigr{\\vert}^2 \\bigr ] \\biggr).\\end{aligned}\\ ] ] we estimate @xmath82 } \\bigl { \\vert}i_a^{n}(s)-i_a^{0}(s ) \\bigr{\\vert}^2 \\bigr]\\le\\mathsf{e } \\biggl[\\sup_{s\\in[0,t ] } \\biggl(\\int_0^s \\bigl{\\vert}a^{n , n}_u - a^{0,0}_u \\bigr { \\vert}1_u du \\biggr)^2 \\biggr]\\notag \\\\ & \\quad\\le\\mathsf{e } \\biggl [ \\biggl(\\int_0^t \\bigl{\\vert}a^{n , n}_u- a^{0,0}_u \\bigr{\\vert}1_u du \\biggr)^2 \\biggr ] \\le t \\int _ 0^t \\mathsf{e } \\bigl [ \\bigl{\\vert}a^{n , n}_u-",
    "a^{0,0}_u \\bigr { \\vert}^2 1_u \\bigr]du\\notag \\\\ & \\quad\\le c_t \\int_0^t \\bigl ( \\mathsf{e } \\bigl [ \\bigl{\\vert}a^{n , n}_u - a^{n,0}_u \\bigr{\\vert}^2 1_u \\bigr ] + \\mathsf{e } \\bigl [ \\bigl { \\vert}a^{n,0}_u - a^{0,0}_u \\bigr { \\vert}^2 1_u \\bigr ] \\bigr)du.\\end{aligned}\\ ] ] in turn , @xmath83du = \\int_0^t \\ ! \\mathsf{e } \\bigl [ \\bigl{\\vert}a^{n } \\bigl(u , x^n(u ) \\bigr)-a^{n } \\bigl(u , x^0(u ) \\bigr ) \\bigr { \\vert}^2 1_u \\bigr]du \\\\ & \\quad\\le c_{n , t } \\int_0^t \\mathsf{e } \\bigl [ \\bigl{\\vert}x^n(u)-x^n(0 ) \\bigr { \\vert}^2 1_u \\bigr]du \\le c_{n , t } \\int _",
    "0^t \\mathsf{e } \\bigl[\\vardelta^n(u)^2 1_u \\bigr]du.\\end{aligned}\\ ] ] by the doob inequality and it isometry we obtain @xmath84 } \\bigl{\\vert}i_b^{n}(s)-i_b^{0}(s ) \\bigr{\\vert}^2 \\bigr ] & \\le c \\mathsf{e } \\bigl [ \\bigl{\\vert}i_b^{n } \\bigl(t\\wedge\\sigma^n_n \\bigr)-i_b^{0 } \\bigl(t\\wedge\\sigma^n_n \\bigr ) \\bigr{\\vert}^2 \\bigr ] \\\\ &",
    "= c \\int_0^t \\mathsf{e } \\bigl [ \\bigl{\\vert}b^{n , n}_s-",
    "b^{0,0}_s \\bigr { \\vert}^21_s \\bigr ] ds.\\end{aligned}\\ ] ] estimating as in , we arrive at @xmath85 ds\\\\ & \\quad \\le c_{n , t } \\int_0^t \\mathsf{e } \\bigl[\\vardelta^n(s)^2 1_s \\bigr]ds+ c \\int_0^t \\mathsf{e } \\bigl [ \\bigl{\\vert}b^{n,0}_s - b^{0,0}_s \\bigr{\\vert}^2 1_s \\bigr ] ds.\\end{aligned}\\ ] ] finally , the doob inequality yields @xmath86 } \\bigl{\\vert}i_c^{n}(s)-i_c^{0}(s ) \\bigr{\\vert}^2 \\bigr ] \\le c \\mathsf{e }",
    "\\bigl [ \\bigl{\\vert}i_c^{n } \\bigl(t\\wedge\\sigma^n_n \\bigr)-i_c^{0 } \\bigl(t\\wedge\\sigma^n_n \\bigr ) \\bigr{\\vert}^2 \\bigr ] \\\\ &",
    "\\quad= c \\int_0^t \\int_{{\\mathbb{r}}^m } \\mathsf{e } \\bigl [ \\bigl{\\vert}c^{n , n}_s(\\theta ) - c^{0,0}_s ( \\theta ) \\bigr{\\vert}^2",
    "1_s \\bigr]\\mu(d\\theta ) ds \\\\",
    "& \\quad\\le c\\int_0^t \\int_{{\\mathbb{r}}^m } \\bigl(\\mathsf{e } \\bigl [ \\bigl{\\vert}c^{n , n}_s(\\theta ) - c^{n,0}_s(\\theta ) \\bigr{\\vert}^2",
    "1_s \\bigr]+ \\mathsf{e } \\bigl [ \\bigl{\\vert}c^{n,0}_s(\\theta ) - c^{0,0}_s(\\theta ) \\bigr{\\vert}^2 1_s \\bigr ] \\bigr)\\mu(d\\theta ) ds.\\end{aligned}\\ ] ] by ( a2 ) we have @xmath87\\mu ( d\\theta ) ds \\\\ &",
    "\\quad\\le c_{n , t}\\int_0^t \\mathsf{e } \\bigl [ \\bigl{\\vert}x^{n}(s ) - x^{0}(s ) \\bigr { \\vert}^2 1_s \\bigr ] ds \\le c_{n , t}\\int _",
    "0^t \\mathsf{e } \\bigl[\\vardelta^{n}(s)^2 1_s \\bigr ] ds.\\end{aligned}\\ ] ] collecting all estimates , we arrive at the estimate @xmath88&\\le c \\bigl{\\vert}x^n(0)-x^0(0 ) \\bigr { \\vert}^2 + c_{n , t } \\int_0^t \\mathsf{e } \\bigl[\\vardelta^n(s)1_s \\bigr]ds \\\\ & \\quad+ c_t \\int_0^t \\mathsf{e } \\bigl [ \\bigl{\\vert}\\tilde a^{n,0}_s-\\tilde a^{0,0}_s \\bigr{\\vert}^2 1_s \\bigr ] ds + c \\int_0^t \\mathsf{e } \\bigl [ \\bigl{\\vert}b^{n,0}_s - b^{0,0}_s \\bigr { \\vert}^2 1_s \\bigr ] ds \\\\ & \\quad+ c\\int_0^t \\int_{{\\mathbb{r}}^m } \\mathsf{e } \\bigl [ \\bigl{\\vert}c^{n,0}_s(\\theta ) - c^{0,0}_s ( \\theta ) \\bigr{\\vert}^2",
    "1_s \\bigr]\\mu(d\\theta ) ds,\\end{aligned}\\ ] ] where we can assume without loss of generality that the constants are nondecreasing in @xmath89 .",
    "the application of the gronwall lemma leads to @xmath90 \\\\ & \\quad\\le c_{n , t } \\biggl ( \\bigl{\\vert}x^n(0)-x^0(0 ) \\bigr{\\vert}^2 + \\int_0^t \\mathsf{e } \\bigl [ \\bigl{\\vert}\\tilde a^{n,0}_s-\\tilde a^{0,0}_s \\bigr{\\vert}^2 1_s \\bigr ] ds \\\\ & \\qquad+ \\int_0^t\\ ! \\mathsf{e } \\bigl [ \\bigl { \\vert}b^{n,0}_s - b^{0,0}_s \\bigr { \\vert}^2 1_s \\bigr ] ds + \\ !",
    "\\int_0^t \\int_{{\\mathbb{r}}^m}\\ !",
    "\\mathsf{e } \\bigl [ \\bigl{\\vert}c^{n,0}_s ( \\theta ) - c^{0,0}_s(\\theta ) \\bigr{\\vert}^2 1_s \\bigr]\\mu(d \\theta ) ds \\biggr).\\end{aligned}\\ ] ] we claim that the right - hand side of the latter inequality vanishes as @xmath8 .",
    "indeed , the integrands are bounded by @xmath91 due to ( a1 ) and vanish pointwise due to ( c1 ) .",
    "hence , the convergence of integrals follows from the dominated convergence theorem .",
    "the first term vanishes due to ( c2 ) ; thus , @xmath92\\to0,\\quad n\\to \\infty.\\ ] ]    now to prove the first statement , for any @xmath93 , write @xmath94 + { \\mathsf{p}}\\bigl ( \\sigma_n^n < t \\bigr ) \\\\ & \\le\\frac{1}{{\\varepsilon}^2}\\mathsf{e } \\bigl[\\vardelta^n(t)^2 1_t \\bigr]+ { \\mathsf{p}}\\bigl(\\sup_{t\\in[0,t ] } \\bigl{\\vert}x^n(0 ) \\bigr{\\vert}\\ge n \\bigr ) \\\\ & \\quad+ { \\mathsf{p}}\\bigl(\\sup_{t\\in[0,t ] } \\bigl{\\vert}x^0(0 ) \\bigr{\\vert}\\ge n \\bigr).\\end{aligned}\\ ] ] this implies @xmath95 } \\bigl{\\vert}x^n(0 ) \\bigr{\\vert}\\ge n \\bigr).\\end{gathered}\\ ] ] by the chebyshev inequality we have @xmath96 } \\bigl{\\vert}x^n(0 ) \\bigr { \\vert}^2 \\bigr].\\end{gathered}\\ ] ] therefore , using and letting @xmath97 , we get @xmath98 as desired .    in order to prove the second statement , we repeat the previous arguments with @xmath99 , getting the estimate @xmath100&\\le c_{t } \\biggl ( \\bigl{\\vert}x^n(0)-x^0(0 ) \\bigr { \\vert}^2 + \\int_0^t \\mathsf{e } \\bigl [ \\bigl{\\vert}\\tilde a^{n,0}_s-\\tilde a^{0,0}_s \\bigr{\\vert}^2 \\bigr ] ds \\\\ & \\quad+ \\int_0^t \\mathsf{e } \\bigl [ \\bigl { \\vert}b^{n,0}_s - b^{0,0}_s \\bigr { \\vert}^2 \\bigr ] ds \\\\ & \\quad+ \\int_0^t \\int_{{\\mathbb{r}}^m } \\mathsf{e } \\bigl [ \\bigl{\\vert}c^{n,0}_s(\\theta ) - c^{0,0}_s ( \\theta ) \\bigr{\\vert}^2",
    "\\bigr]\\mu(d \\theta ) ds \\biggr).\\end{aligned}\\ ] ] hence , we get the required convergence as before , using the dominated convergence theorem .",
    "for each @xmath36 , define the stopping time @xmath101 with the convention @xmath102 ; @xmath103 is a function satisfying certain assumptions to be specified later . in this section ,",
    "we study the convergence @xmath104 as @xmath8 .",
    "the motivation to study stopping times of the form comes from the financial modeling . specifically , let a financial market model be driven by the process @xmath11 solving eq .  , and @xmath105 be a constant discount factor .",
    "consider the problem of optimal exercise of an american - type contingent claim with payoff function @xmath106 and maturity @xmath107 , that is ,  the maximization problem @xmath108\\to \\max,\\ ] ] where @xmath109 is a stopping time taking values in @xmath110 $ ] .",
    "define the value function @xmath111}\\mathsf{e } \\bigl[e^{-q(\\tau - t ) } f \\bigl(x^n(\\tau ) \\bigr ) \\mid x^n(t)=x \\bigr]\\ ] ] as the maximal expected discounted payoff provided that the price process @xmath11 starts from @xmath47 at the moment @xmath89 ; the supremum is taken over all stopping times with values in @xmath112 $ ] .",
    "then it is well known that the minimal optimal stopping time is given as @xmath113 that is ,  it is the first time when the process @xmath11 hits the so - called optimal stopping set @xmath114\\times{\\mathbb{r}}^d : v^n(t , x ) = f(x ) \\bigr\\}.\\ ] ] note that @xmath115 since @xmath116 . since , obviously , @xmath117 , we may represent @xmath118 in the form with @xmath119 .",
    "let @xmath66 be a fixed number playing the role of finite maturity of an american contingent claim .",
    "let also the stopping times @xmath120 , @xmath121 , be given by with @xmath122\\times{\\mathbb{r}}^d\\to{\\mathbb{r}}$ ] satisfying the following assumptions .",
    "* @xmath123 , and the derivative @xmath124 is locally lipschitz continuous in @xmath47 , that is ,  for all @xmath125 , @xmath58 , @xmath126 $ ] , and @xmath59 , @xmath127 * for all @xmath36 and @xmath55 , @xmath128 .",
    "* for all @xmath125 and @xmath55 , @xmath129    here by @xmath130 we denote the vector in @xmath16 with @xmath131th coordinate equal to @xmath132    [ rem : g1g2 ] assumption means that the diffusion is acting strongly enough toward the border of the set @xmath133 . in which situations does this assumption hold , will be studied elsewhere .",
    "here we just want to remark that it is more delicate than it might seem .",
    "for example , consider the optimal stopping problem described in the beginning of this section with @xmath134 in . then , under suitable assumptions ( see , e.g. ,  @xcite ) , we have the smooth fit principle : @xmath135 on the boundary of the optimal stopping set .",
    "this means that we can not set @xmath136 in order for to hold , contrary to what was proposed in the beginning of the section",
    ".    we will also assume the locally uniform convergence @xmath137 .",
    "* for all @xmath125 and @xmath58 , @xmath138\\times b_d(r ) } \\bigl{\\vert}\\varphi^n(s , x)- \\varphi ^0(s , x ) \\bigr{\\vert}\\to0 , \\quad n\\to\\infty.\\ ] ]    the convergence of value functions in optimal stopping problems usually holds under fairly mild assumptions on the convergence of coefficients and payoffs . however , as we explained in remark  [ rem : g1g2 ] , we can not use the value function for @xmath103 .",
    "this means that we should find a function @xmath103 defining @xmath139 _ different _ from @xmath140 , but it still should satisfy the convergence assumption  ( g4 ) .",
    "the question in which cases such functions exist and the convergence assumption ( g4 ) takes places will be a subject of our future research .    in the case",
    "where @xmath19 has infinite activity , that is ,  @xmath141 , we will also need some additional assumptions on the components of eq .  .",
    "* for each @xmath142 , @xmath143 .",
    "* for all @xmath64 , @xmath55 , and @xmath144 , @xmath145 where the functions @xmath146 are locally bounded , @xmath147 , and @xmath148 , @xmath149 .",
    "[ a34rem ] assumption ( a3 ) means that only small jumps of @xmath31 can accumulate on a finite interval ; assumption ( a4 ) means that small jumps of @xmath31 are translated by eq .",
    "to small jumps of @xmath11 .",
    "an important and natural example of a situation where these assumptions are satisfied is an equation @xmath150 driven by a lvy process @xmath151 .",
    "now we are in a position to state the main result of this section .",
    "[ thm - convmoments ] assume ( a1)(a4 ) , ( c1 ) , ( c2 ) , ( g1)(g4 ) .",
    "then we have the following convergence in probability : @xmath152    let @xmath153 be small positive numbers .",
    "we are to show that for all @xmath154 large enough , @xmath155 using estimate and the chebyshev inequality , we obtain that for some @xmath58 , @xmath156 } \\bigl{\\vert}x^0(t ) \\bigr { \\vert}\\ge r \\bigr ) < \\frac{\\delta}4.\\ ] ]    denote @xmath157\\times b_d(r+2)$ ] , @xmath158 where , with some abuse of notation , @xmath159 is the constant from ( a2 ) corresponding to @xmath107 and @xmath160 , @xmath161 is the sum of constants from ( a1 ) and , and @xmath162 is the constant from ( g1 ) corresponding to @xmath163 and @xmath160 .",
    "let @xmath164 $ ] be a number , which we will specify later .",
    "now we claim that there exists a function @xmath165 such that @xmath166 and , moreover , @xmath167\\\\x\\in b_d(r+1 ) } } \\bigl ( \\bigl{\\vert}\\partial_t \\varphi(t , x ) \\bigr{\\vert}+ \\bigl{\\vert}d_x \\varphi(t , x ) \\bigr{\\vert}+ \\bigl{\\vert}d^2_{xx}\\varphi(t , x ) \\bigr{\\vert}+ \\bigl { \\vert}b^0(t , x)^\\top d_x \\varphi(t , x ) \\bigr { \\vert}^{-1 } \\bigr ) \\\\ & \\quad\\le c_{t-{\\varepsilon}/2,r+2 } + \\sup_{(t , x)\\in\\mathcal k } \\bigl ( \\bigl{\\vert}\\partial_t \\varphi^0(t , x ) \\bigr{\\vert}+ \\bigl{\\vert}d_x\\varphi^0(t , x ) \\bigr{\\vert}\\\\ & \\qquad+ \\bigl{\\vert}b^0(t , x)^\\top d_x \\varphi^0(t , x ) \\bigr{\\vert}^{-1 } \\bigr)\\\\ & \\quad \\le m.\\end{aligned}\\ ] ] indeed , we can take the convolution @xmath168 with a delta - like smooth function @xmath169 , supported on a ball of radius less than @xmath170 .",
    "further , by ( g4 ) there exists @xmath171 such that for all @xmath172 , @xmath173 on the other hand , by theorem  [ ucp - thm ] there exists @xmath174 such that for all @xmath175 , @xmath176 } \\bigl{\\vert}x^n(t)-x^0(t ) \\bigr{\\vert}\\ge\\frac{\\varkappa } { m } \\biggr)<\\frac{\\delta}4.\\ ] ] in what follows , we consider @xmath177 .",
    "define the stopping time @xmath178 write @xmath179 }",
    "\\bigl{\\vert}x^0(t ) \\bigr{\\vert}\\ge r \\bigr ) + { \\mathsf{p}}\\biggl(\\sup_{t\\in[0,t ] } \\bigl{\\vert}x^n(t)-x^0(t ) \\bigr{\\vert}\\ge\\frac{\\varkappa}{m } \\biggr ) \\notag \\\\ & < { \\mathsf{p}}\\bigl ( \\bigl{\\vert}\\tau^{n}-\\tau^{0 } \\bigr{\\vert } > { \\varepsilon } , \\sigma^n > t-{\\varepsilon}/2",
    "\\bigr ) + \\frac{\\delta}2.\\end{aligned}\\ ] ] for any @xmath180 , @xmath181 and hence , @xmath182 now take some @xmath183 $ ] whose exact value will be specified later and write the obvious inequality @xmath184 assume that @xmath185 , @xmath186 , @xmath187 .",
    "then , for all @xmath188= : \\mathcal i_\\eta$ ] , @xmath189 therefore , in view of the inequality @xmath190 , we obtain @xmath191    further , we will work with the expression @xmath192 for @xmath193 . for convenience",
    ", we will abbreviate @xmath194 ; for example , @xmath195 .    let @xmath142 be a positive number , which we will specify later , and assume that @xmath19 does not have jumps on @xmath196 greater than @xmath197 , that is ,  @xmath198 .",
    "write , using the it formula , @xmath199 where @xmath200    start with estimating @xmath201 .",
    "since @xmath202 for any @xmath203 , by the definition of @xmath204 and @xmath205 we have @xmath206 further , by ( a4 ) , for @xmath203 and @xmath207 , @xmath208 , where @xmath209 , { \\vert}x{\\vert}\\le r } h(t , x)$ ] and @xmath210 . since",
    ", @xmath211 , we can assume that @xmath197 is such that @xmath212 .",
    "then , for @xmath203 , by the taylor formula @xmath213\\times b_d(r+1 ) } \\bigl{\\vert}d^2_{xx}\\varphi(u , x ) \\bigr{\\vert}\\int_{b_m(r ) } \\bigl{\\vert}c \\bigl(t , x^0(t-),\\theta \\bigr ) \\bigr { \\vert}^2 \\mu(d\\theta ) \\\\ & \\quad\\le\\frac{1}2 m^2 \\bigl(1 + \\bigl{\\vert}x(t ) \\bigr { \\vert}^2 \\bigr)\\le\\frac{1}2 m^2 \\bigl(1+r^2 \\bigr)\\le m^4.\\end{aligned}\\ ] ] summing up the estimates , we get @xmath214    now proceed to @xmath215 . by the doob inequality , for any @xmath216 ,",
    "@xmath217 } \\bigl{\\vert}i_3(t ) \\bigr{\\vert}\\ge a \\bigr ) \\\\",
    "& \\quad\\le c a^{-2}\\mathsf{e } \\biggl [ \\biggl(\\int_{0}^{t } \\int_{b_m(r ) } \\vardelta_s(\\theta ) 1_{[\\tau_0,(\\tau_0+\\eta)\\wedge \\sigma_n]}(s ) { \\widetilde{\\nu}}(d\\theta , ds ) \\biggr)^2 \\biggr ] \\\\ & \\quad= c a^{-2}\\int_{0}^{t } \\int _ { b_m(r ) } \\mathsf{e } \\bigl[\\vardelta_s ( \\theta)^2 1_{[\\tau_0,(\\tau _ 0+\\eta)\\wedge\\sigma_n]}(s ) \\bigr ] \\mu(d\\theta)ds \\\\ & \\quad\\le c a^{-2 } m^2 \\int_{0}^{t } \\int_{b_m(r ) } \\mathsf{e } \\bigl [ \\bigl{\\vert}c \\bigl(s , x^0(s- ) , \\theta \\bigr ) \\bigr{\\vert}^2 1_{[\\tau_0,(\\tau_0+\\eta ) \\wedge \\sigma_n]}(s ) \\bigr ] \\mu(d\\theta)ds \\\\ & \\quad\\le c a^{-2 } m^2 \\int_{0}^{t } \\int_{b_m(r ) } \\mathsf{e } \\bigl[k_1 ^ 2m_r^2 1_{[\\tau_0,(\\tau_0+\\eta ) \\wedge\\sigma_n]}(s ) \\bigr ] \\mu(d\\theta)ds \\le k_2 a^{-2 } m_r^2 \\eta\\end{aligned}\\ ] ] with some constant @xmath218 .",
    "further , we fix @xmath219 and some @xmath142 such that @xmath220 and @xmath212 .",
    "then @xmath221    hence , in view of  , we obtain @xmath222 assume further that @xmath223 ( not yet fixing its exact value ) . setting @xmath224 ,",
    "we get @xmath225    write @xmath226 , where @xmath227    taking into account that @xmath228 for @xmath229 , we estimate with the help of doob s inequality @xmath230&\\le\\mathsf{e } \\bigl[\\sup_{t\\in[\\tau^{0},(\\tau^{0}+\\eta ) \\wedge\\sigma^n]}j_1(t)^2 \\bigr ] \\\\ & \\le c\\mathsf{e } \\biggl [ \\biggl(\\int_{\\tau^{0}}^{(\\tau^{0}+\\eta ) \\wedge\\sigma ^n } \\bigl(d_x \\varphi_s - d_x\\varphi_{\\tau^{0 } } , b^0_s\\ , dw(s ) \\bigr ) \\biggr)^2 \\biggr ] \\\\ & \\le c \\mathsf{e } \\biggl[\\int_{\\tau^{0}}^{(\\tau^{0}+\\eta)\\wedge \\sigma ^n}{\\vert}d_x \\varphi_s - d_x \\varphi_{\\tau^{0 } } { \\vert}^2 \\bigl{\\vert}b^0_s \\bigr { \\vert}^2ds \\biggr ] \\\\ & \\le c m^3 \\mathsf{e } \\biggl[\\int_{\\tau^{0}}^{\\tau^{0}+\\eta } \\bigl{\\vert}x^0(s ) - x^0 \\bigl(\\tau^{0 } \\bigr ) \\bigr{\\vert}^2ds \\biggr ] \\\\ & \\le c m^4 \\bigl(1 + \\bigl{\\vert}x^0(0 ) \\bigr { \\vert}^2 \\bigr)\\eta^2\\le c",
    "m^4 \\bigl(1+r^2 \\bigr)\\eta^2\\le cm^6 \\eta^2.\\end{aligned}\\ ] ] similarly , using ( a2 ) , we get @xmath231\\le c m^6 \\eta^2.\\ ] ] the chebyshev inequality yields @xmath232 with certain constant @xmath233 .",
    "assume further that @xmath234 in which case the right - hand side of the last inequality does not exceed @xmath235 , and that @xmath236 so that @xmath237 . hence , in view of , we obtain @xmath238 further , due to the strong markov property of @xmath1 , @xmath239 \\\\ & \\quad\\ !",
    "= \\mathsf{e } \\bigl[\\mathbf{1}_{\\mathcal k } \\bigl(\\tau ^{0},x^0 \\bigl(\\tau^{0 } \\bigr ) \\bigr ) \\\\ & \\qquad\\",
    "! \\times{\\mathsf{p}}\\bigl(\\inf_{z\\in[0,\\eta ] } \\bigl(u(s , x),w(s\\ , { + } \\,z)\\,{-}\\,w(s ) \\bigr)\\ge{-}\\ , 2\\eta^{2/3}\\,{-}\\,\\delta^2 \\eta^{1/2 } \\bigr)\\,|_{(s , x)= ( \\tau^{0},x^0(\\tau^{0 } ) ) } \\bigr],\\end{aligned}\\ ] ] where @xmath240 .",
    "observe now that @xmath241 is a standard wiener process multiplied by @xmath242 .",
    "therefore , @xmath243 } \\bigl(u(s , x),w(s+z)-w(s ) \\bigr ) \\ge-2 \\eta^{2/3 } -\\delta^2\\eta^{1/2 } \\bigr ) \\\\ & \\quad= 1-",
    "2{\\mathsf{p}}\\bigl ( \\bigl(u(s , x),w(s+\\eta)-w(s ) \\bigr ) < -2\\eta ^{2/3 } -\\delta^2\\eta^{1/2 }",
    "\\bigr ) \\\\ & \\quad=1- 2\\varphi \\biggl ( -\\frac{2\\eta^{2/3}+\\vardelta^2\\eta ^{1/2}}{{\\vert}u(s , x){\\vert}\\eta^{1/2 } } \\biggr ) = 1- 2\\varphi \\biggl ( - \\frac{2\\eta^{1/6}+\\delta^2}{{\\vert}u(s , x){\\vert } } \\biggr),\\end{aligned}\\ ] ] where @xmath244 is the standard normal distribution function .",
    "thus , @xmath245 \\\\ & \\quad\\le1- 2\\varphi \\bigl ( -m \\bigl(2\\eta^{1/6}+\\delta^2 \\bigr ) \\bigr)\\le\\frac{m\\sqrt2}{\\sqrt{\\pi } } \\bigl(2\\eta ^{1/6}+\\delta ^2 \\bigr).\\end{aligned}\\ ] ] note that the definition of @xmath204 does not depend on @xmath246 .",
    "thus , we can assume without loss of generality that @xmath247 . finally , if @xmath248 then @xmath249 now we can fix @xmath250 , making all previous estimates to hold . combining with , we arrive at @xmath251 similarly , @xmath252 and hence @xmath253 plugging this estimate into , we arrive at the desired inequality .",
    "it is easy to modify the proof for the case where holds for all @xmath254 .",
    "indeed , the continuity would imply that holds in some neighborhood of @xmath255 , which is sufficient for the argument .",
    "as we have already mentioned , assumptions ( a3 ) and ( a4 ) are not needed in the case @xmath256 .",
    "indeed , we can set @xmath257 in the previous argument and skip the estimation of @xmath215 .",
    "nevertheless , these assumptions does not seem very restrictive , as we pointed out in remark  [ a34rem ] .      here",
    "we extend the results of the previous subsection to the case of infinite time horizon .",
    "let , as before , the stopping times @xmath120 , @xmath36 , be given by .",
    "we impose the following assumptions .",
    "* @xmath258 , and @xmath124 is locally lipschitz continuous in @xmath47 , that is ,  for all @xmath66 , @xmath58 , @xmath54 $ ] , and @xmath59 , @xmath259 * @xmath260 a.s .",
    "* for all @xmath64 and @xmath55 , @xmath261 * for all @xmath64 and @xmath58 , @xmath138\\times b_d(r ) } \\bigl{\\vert}\\varphi^n(t , x)- \\varphi ^0(t , x ) \\bigr{\\vert}\\to0 , \\quad n\\to\\infty.\\ ] ]    [ thm - con - infty ] assume ( a1 ) , ( a2 ) , ( c1 ) , ( c2 ) , ( h1)(h4 ) .",
    "then we have the following convergence in probability : @xmath152    fix arbitrary @xmath262 and @xmath263 . since @xmath264 a.s .",
    ", @xmath265 for some @xmath266 . for @xmath36 , @xmath267 $ ] , and @xmath55 , define  @xmath268 , @xmath269 .",
    "then the functions @xmath270 , @xmath36 , satisfy ( g1)(g3 ) and @xmath271 .",
    "therefore , in view of theorem  [ thm - convmoments ] , @xmath272 we estimate @xmath273 hence , @xmath274 letting @xmath275 , we arrive at the desired convergence .",
    "let @xmath276 and for all @xmath64 , @xmath277 , @xmath278 , @xmath279 , @xmath280 , where @xmath281 .",
    "then we have a sequence of lvy processes @xmath282 consider the following times : @xmath283 of crossing some curve @xmath284 .",
    "assume that @xmath4 , @xmath285 , @xmath6 , and @xmath7 as @xmath8 and , for any @xmath125 , @xmath286 } { \\vert}h^n(t)-h^0(t){\\vert}\\to0 $ ] as @xmath8 .",
    "then @xmath287 , @xmath8 .",
    "indeed , setting @xmath288 , we can check that all assumptions of theorem  [ thm - convmoments ] are in force .",
    "let @xmath276 .",
    "suppose that the coefficients @xmath289 , @xmath290 , @xmath291 satisfy ( a1 ) , ( a2 ) and that the convergence ( c1)(c3 ) takes place .",
    "assume that @xmath292 for all @xmath64 and @xmath293 .",
    "define @xmath294 it is not hard to check that , due to the nondegeneracy of @xmath295 , @xmath296 a.s .",
    "assume that @xmath297 , @xmath298 , @xmath8 .",
    "then , setting @xmath299 and using theorem  [ thm - con - infty ] , we get the convergence @xmath300 , @xmath8 .",
    "the author would like to thank the anonymous referee whose remarks led to a substantial improvement of the manuscript ."
  ],
  "abstract_text": [
    "<S> we investigate the convergence of hitting times for jump - diffusion processes . specifically , we study a sequence of stochastic differential equations with jumps . under reasonable assumptions , </S>",
    "<S> we establish the convergence of solutions to the equations and of the moments when the solutions hit certain sets .    </S>",
    "<S> ./style / arxiv - vmsta.cfg    stochastic differential equation , poisson measure , jump - diffusion process , stopping time , convergence 60h10,60g44,60g40 </S>"
  ]
}