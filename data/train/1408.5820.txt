{
  "article_text": [
    "the `` netflix prize ''  @xcite generated a significant interest in the _ matrix completion _ problem .",
    "the netflix data can be represented as a sparse matrix made up of ratings given by users ( rows ) to movies ( columns ) .",
    "to infer the missing entries is thus very helpful to propose sensible advertisement and improve the sales .",
    "however , it is totally impossible to recover an uncomplete matrix without any assumption .",
    "a suitable condition , popular in practice for this problem , is that the matrix has low - rank or approximately low - rank  @xcite . for the netflix problem ,",
    "this assumption is sensible as it means that many movies ( or users ) have similar profiles .",
    "let @xmath0 be an unknown matrix ( expected to be low - rank ) and @xmath1 be i.i.d random variables drawn from a joint distribution @xmath2 .",
    "we assume that @xmath3 the noise variables @xmath4 are independent from @xmath5 and @xmath6 we let @xmath7 denote the marginal distribution of @xmath8 when @xmath9 .",
    "remark that @xmath7 is a distribution on the set @xmath10 .",
    "then , the problem of estimating @xmath11 with @xmath12 is called the noisy matrix completion problem under general sampling distribution .",
    "a special instance of this problem is that the sampling distribution @xmath13 is uniform , this assumption is done for example in  @xcite .",
    "clearly , in practice , the observed entries are not always uniformly distributed : for example , some movies are more famous than others , and thus receive much more ratings .",
    "more importantly , the sampling distribution is not known in practice .",
    "more general sampling schemes than uniform distribution had been already studied , see e.g.  @xcite , but there are still some assumptions on @xmath7 in these papers . here , we do not impose any restriction on @xmath7 . from now , @xmath14 will denote the probability to observe the @xmath15-th entry .    for any matrix @xmath16 ,",
    "let @xmath17 denote the frobenius norm , i.e , @xmath18 .",
    "we define a `` generalized frobenius norm '' as follows @xmath19 note that when the sampling distribution @xmath13 is uniform , then @xmath20 for any matrix @xmath21 , we define the empirical risk as @xmath22 and the prediction risk @xmath23.\\ ] ] in this paper , the prediction problem is considered , i.e , the objective is to define an estimator @xmath24 such that @xmath25 is as small as possible .",
    "remark that @xmath26 for any @xmath27 ( using pythagorean theorem ) .",
    "when handing with this problem , most of the recent methods are often based on minimizing a criterion of the fit to the observations , such as @xmath28 , penalized by the nuclear - norm or the rank of the matrix .",
    "a first result can be found in by cands and recht  @xcite , cands and tao  @xcite for exact matrix completion ( noiseless case , i.e. @xmath29 ) .",
    "these results were then developed in the noisy case  @xcite .",
    "some efficient algorithms had also been proposed , for example see  @xcite .",
    "recently , some authors have studied a more general problem , the so - called _ trace regression _ problem : @xcite .",
    "this problem includes matrix completion , together with other well - known problems ( linear regression , reduced rank regression and multitask learning ) as special cases .",
    "they proposed nuclear - norm penalized estimators and provided reconstruction errors for their methods .",
    "they also proved that these errors are minimax - optimal ( up to a logarithmic factor ) .",
    "note that the average quadratic error on the entries of a rank-@xmath30 matrix size @xmath31 from @xmath32-observations can not be better than : @xmath33  @xcite .    on the other hand ,",
    "bayesian methods have been also considered  @xcite .",
    "most bayesian estimators are based on conjugate priors which allow to use gibbs sampling  @xcite or variational bayes methods  @xcite .",
    "these priors are discussed in details in  @xcite .",
    "these algorithms are fast enough to deal with large datasets like netflix or movielens , and are actually tested on these datasets in those papers .",
    "however , the theoretical understanding of bayesian algorithms is not satisfying . up to our knowledge ,",
    "the minimax - optimality - and even the consistency - of the bayesian estimator under conjugate prior is an open question .    in this paper , we design a new prior and prove an minimax - optimal oracle bound for the corresponding bayesian estimator .",
    "this is presented in section  [ section_theorem ] . in section  [ section_simulation ]",
    ", we discuss the implementation of our bayesian estimator .",
    "some experiments comparing our estimator to the one based on conjugate priors are done on simulated datasets .",
    "the proof of the main result is provided in the appendix .",
    "before we introduce our estimator , let us formulate some assumptions .",
    "[ bounded assume ] there is a known constant @xmath34 such that @xmath35    this is a mild assumption . in the netflix and movielens datasets ,",
    "the ratings belong to the set @xmath36 , so we can take @xmath37 .",
    "[ bruit - pac ] the noise variables @xmath38 are independent and independent of @xmath39 .",
    "there exist two known constants @xmath40 and @xmath41 such that @xmath42 @xmath43    assumption [ bruit - pac ] states that the noise is sub - exponential , it includes the cases where the noise is bounded or sub - gaussian ( and of course gaussian ) , see e.g. chapter 2 in  @xcite .",
    "we now describe a prior @xmath44 on matrices @xmath45 as follows .",
    "let @xmath46 and @xmath47 be a random variables taking value in the set @xmath48 with @xmath49 where @xmath50 for some constant @xmath51 and @xmath52 .",
    "now , assuming that @xmath53 and a matrix @xmath45 is drawn as @xmath54 where @xmath55 \\right )    &       \\text{when } \\gamma_{k,\\ell } = 1 , \\\\ \\mathcal{u } \\left ( \\left [ - \\kappa , \\kappa \\right ] \\right )                                  &       \\text{when } \\gamma_{k,\\ell } = 0 , \\end{cases }    \\quad \\ell =   1 , \\ldots , k\\ ] ] with @xmath56 and @xmath57 .",
    "note that , in this case , the entries of @xmath27 satisfy : @xmath58 .",
    "moreover , when a matrix @xmath59 is drawn from this prior , as @xmath60 is small , most columns of @xmath61 and @xmath62 are almost null .",
    "so the matrix @xmath63 is very close to a rank-@xmath64 matrix .",
    "actually , the choice @xmath65 leads to @xmath66 .",
    "we are now ready to define our estimator . for any @xmath67",
    ", we consider the conditional probability measure @xmath68 given by its density w.r.t .",
    "the probability measure @xmath44 : @xmath69 the aggregate @xmath70 is defined as follows @xmath71 note that , for @xmath72 , this corresponds exactly to the bayesian estimator that would be obtained for a gaussian noise @xmath73 .",
    "however , a slightly different choice for @xmath74 , denoted by @xmath75 below , will allow to obtain the optimality of the estimator under a wider class of noises .",
    "for any @xmath76 , define @xmath77 and @xmath78   \\vee   \\left [ 8\\sigma^{2 } + 2(3l)^2 \\right ] .",
    "$ ] hereafter , the main result is presented .",
    "we provide an oracle bound for our estimator @xmath79 .",
    "[ main ] let assumption [ bounded assume ] and [ bruit - pac ] be satisfied and take @xmath80 .",
    "then , for any @xmath81 , with probability at least @xmath82 and as soon as @xmath83 , one has @xmath84 where @xmath85 is a ( known ) numerical constant depending on @xmath86 and @xmath87 only .",
    "the proof of this theorem is given in the appendix .",
    "it follows an argument called  pac - bayesian inequality \" .",
    "pac - bayesian inequalities were introduced in  @xcite in order to provide empirical bounds on the prevision risk of bayesian - type estimators",
    ". however , our proof is closer to catoni s works  @xcite , where it is shown how to derive powerful oracle inequalities from pac - bayesian bounds .",
    "this approach has been used many times since then to prove oracle inequalities in many dimension - reduction problems like sparse regression estimation  @xcite or reduced - rank regression  @xcite .",
    "the choice @xmath88 comes from the proof of this theorem when optimizing an upper bound on the risk @xmath89 , see   page  .",
    "however , in practice , this choice may not be the best one .",
    "for example , in the experiments done in section 3 with gaussian noise @xmath90 , we take @xmath91 that was shown in  @xcite to behave very well in regression problems . also , in practice , to take @xmath92 smaller than @xmath93 improves significantly the speed of the algorithm with little consequence on the performance of the estimator  @xcite .    when @xmath94 , we can take @xmath95 , one gets @xmath96    the rate @xmath97 is minimax - optimal , or at least almost minimax - optimal : a lower bound in this problem is provided by theorems 5 and 7 in  @xcite , it is @xmath98 .",
    "the optimality of the @xmath99 term is , to our knowledge , an open question .",
    "note however that the upper bound in  @xcite is @xmath100 .",
    "so , our bound represents a slight improvement in the case @xmath101 .",
    "when the sampling distribution @xmath13 is uniform in theorem [ main ] , we obtain the following oracle bound for the frobenius norm @xmath102    finally , we want to mention that the rate of  @xcite is also reached , in a work parallel to ours , by suzuki  @xcite , in a bayesian framework .",
    "the main difference is that , while  @xcite provides a rate of convergence in a more general low - rank tensor estimation problem , his works do not bring an oracle inequality like theorem  [ main ] that can be used when @xmath103 is not exactly low - rank , but can be well approximated by a low - rank matrix .",
    "moreover , our result holds under any sampling distribution @xmath13 .",
    "as it has been shown in section 2 , our estimator @xmath105 satisfies a powerful oracle inequality .",
    "however , as mentioned in the introduction , the bayesian estimator using conjugate priors is popular in practice as it leads to a fast algorithm .",
    "the reason is that there is an explicit form for the conditional posterior distribution of the @xmath106-th row of @xmath61 , @xmath107 , given the other rowss of @xmath61 , @xmath108 , and given @xmath62 ( it is a multivariate normal distribution which parameters are known ) .",
    "this allows to use a gibbs sampler , with very good convergence properties .",
    "this is described for example in  @xcite and the references therein .    here ,",
    "straighforward but tedious computations lead to @xmath109 \\prod_{\\ell=1}^k \\mathbf{1}_{\\{|u_{i,\\ell}| \\leq \\delta\\ } } \\prod_{\\ell = k+1}^k \\mathbf{1}_{\\{|u_{i,\\ell}| \\leq \\kappa\\}}\\ ] ] where we use the notation @xmath110 , @xmath111 , @xmath112 , @xmath113 and @xmath114 is the density of the multivariate normal distribution with mean vector @xmath115 and variance - covariance matrix @xmath62 .",
    "so , the conditional posterior distribution of @xmath107 is a truncated multivariate normal . to sample from such a disitrubition",
    "is known as a very hard problem in general , see for example  @xcite . however , using the r package * tmvtnorm *  @xcite , it is possible to sample from a truncated multivariate normal fast enough to compute our estimator on reasonnably large datasets .",
    "finally , instead of including the hyperparameter @xmath116 in the simulations , we simulated @xmath92 chains simultaneously , one for every @xmath116 , and selected the realization of one of the chains at each round using the probabilities given by  .    also , note that the truncation procedure proposed by suzuki in  @xcite can not be implemented , to our understanding , using this procedure , as the truncation is done directly on the product @xmath117 rather than on @xmath61 and @xmath62 individually .",
    "we use the notation @xmath104 for our estimator , let us denote @xmath118 the estimator based on the gaussian prior for @xmath61 and @xmath62 with inverse gamma variance , described in  @xcite and in the aforementionned references . in order to compare both estimators , a series of experiments",
    "were done with simulated data :    * in the first series of simulations , the data are simulated as in  @xcite .",
    "more precisely , a rank-@xmath119 matrix @xmath120 ( so @xmath121 ) has been created as the product of two rank-@xmath119 matrices , @xmath122 , where the entries of @xmath123 and @xmath124 are i.i.d @xmath125 .",
    "only @xmath126 entries of the matrix @xmath11 are observed ( using a uniform sampling ) .",
    "this sampled set is then corrupted by noise as in  , where the @xmath127 are i.i.d @xmath128 .",
    "we consider the cases @xmath129 , @xmath130 , @xmath131 and @xmath132 .",
    "* the second series of simulations is similar to the first one , except that the matrix @xmath103 is no longer rank @xmath133 , but it can be well approximated by a rank @xmath133 matrix : @xmath134 where the entries of @xmath135 and @xmath136 are i.i.d @xmath125 . *",
    "the third series of experiments is similar to the first one , but the noise variables @xmath127 are now i.i.d from a uniform distribution on @xmath137 $ ] .",
    "note that , from a purely bayesian point of view , this corresponds to a mispecified model .",
    "however , the bound in theorem  [ main ] is still valid in this case . *",
    "finally , the fourth series of experiments is similar to the first one , noise variables @xmath127 are now i.i.d from a heavy - tailed distribution ( student , with parameter @xmath138 ) .",
    "this is another misspecified model , but in this case , theorem  [ main ] can not be used .",
    "the behavior of our estimator @xmath139 is computed through the root - mean - squared error ( rmse ) per entry , @xmath140^{1/2 }    = ( 1/m)\\|\\widehat{m}_{\\lambda }   -   m^0\\|_f .\\ ] ]    .__rmses in the first series of experiments ( low - rank matrix , gaussian noise ) _ _ [ cols=\"<,^,^,^,^\",options=\"header \" , ]     the parameters are given as follows : for both @xmath139 and @xmath118 , the parameter @xmath74 is set to @xmath141 , following  @xcite . following  @xcite we use for the parameters of the inverse gamma prior in @xmath118 the values @xmath142 , @xmath143 .",
    "finally , for @xmath139 , we used @xmath65 , @xmath144 , @xmath145 and @xmath146 on all the simulations apart from the heavy - tailed noise case , where we used @xmath147 . note that a proper optimization with respect to the parameters @xmath148 and @xmath74 could lead to better results , for example through cross - validation .",
    "the first conclusion is that the results of both methods are very close . in many situations , however , the variance of the estimator with uniform prior is larger than the variance of the estimator with gaussian prior .",
    "the evidence is that this is due to the fact that the mcmc algorithm used to compute the estimator with gaussian prior , @xmath118 , converges faster than the algorithm used to compute the estimator with uniform prior , @xmath104 .",
    "this is supported by figure  [ acfs ] page  .",
    "however , it seems that this difference is less and less significant when the dimension @xmath115 grows .",
    ", is in red while the acf of the gibbs sampler for the bayesian estimator with gaussian priors , @xmath118 , is in blue._,title=\"fig : \" ] , is in red while the acf of the gibbs sampler for the bayesian estimator with gaussian priors , @xmath118 , is in blue._,title=\"fig : \" ] , is in red while the acf of the gibbs sampler for the bayesian estimator with gaussian priors , @xmath118 , is in blue._,title=\"fig : \" ] , is in red while the acf of the gibbs sampler for the bayesian estimator with gaussian priors , @xmath118 , is in blue._,title=\"fig : \" ]    according to our main oracle inequality , our estimator is robust to misspecification in the low - rank assumption , see table  [ results2 ] , and in the noise , at least in the sub - gaussian case , see table  [ results3 ] . more importantly : despite the fact that the theoretical properties of @xmath118 are not known , this estimator is more robust than ours to heavy - tailed noise , as shown in table  [ results4 ] .",
    "this paper proposes a bayesian estimator for the noisy matrix completion problem under general sampling distribution .",
    "this estimator satisfies an optimal oracle inequality under any sampling scheme .",
    "based on simulations , it is also clear that this estimator performs well in practice , however , a faster algorithm for very large datasets is still an open issue .",
    "another important open question is the minimax - optimality of the estimator based on gaussian priors .",
    "we would like to thank the anonymous referees for their constructive comments and professor taiji suzuki for enlightening discussions .",
    "first , we state a version of bernstein s inequality useful in the proof of theorem [ main ] .",
    "this version is taken from  @xcite ( inequality 2.21 in the proof of proposition 2.9 page 24 ) .",
    "[ lemmemassart ] let @xmath149 , ",
    ", @xmath150 be independent real valued random variables .",
    "let us assume that there are two constants @xmath151 and @xmath152 such that @xmath153 \\leq v\\ ] ] and for all integers @xmath154 , @xmath155 \\leq v\\frac{k!w^{k-2}}{2}.\\ ] ] then , for any @xmath156 , @xmath157 \\right ]          \\leq \\exp\\left(\\frac{v\\zeta^{2}}{2(1-w\\zeta ) } \\right ) .\\ ] ]      the proof is divided in two steps . in the first step ,",
    "we establish a general pac - bayesian inequality for matrix completion , in the style of  @xcite . in the second step ,",
    "we derive the oracle inequality from the first step .",
    "let s define , for any matrix @xmath158 , the following random variables @xmath159 note that these variables are independent .",
    "we first check that the variables @xmath160 satisfy the assumptions of lemma  [ lemmemassart ] , in order to apply this lemma .",
    "we have @xmath161   &   = \\sum_{i=1}^{n } \\mathbb{e } \\left [         \\left ( 2y_{i } - m^0_{x_{i } } - m_{x_{i } } \\right)^{2 } \\left ( m^0_{x_{i } } -m_{x_{i } } \\right)^2              \\right ] \\\\ &   = \\sum_{i=1}^{n } \\mathbb{e } \\left [         \\left ( 2 \\mathcal{e}_{i } + m^0_{x_i }   - m_{x_{i } }   \\right)^{2 } \\left ( m^0_{x_{i } } -m_{x_{i } } \\right)^2              \\right ] \\\\ &     \\leq \\sum_{i=1}^{n } \\mathbb{e } \\left [          \\left [ 8 \\mathcal{e}_{i}^{2 } + 2(l + 2l)^2\\right ] \\left [ m^0_{x_{i } } -m_{x_{i } } \\right]^2              \\right ] \\\\ &      = \\sum_{i=1}^{n } \\mathbb{e } \\left [ 8 \\mathcal{e}_{i}^{2 } + 2(3l)^2 \\right ]       \\mathbb{e } \\left [ m^0_{x_{i } } -m_{x_{i } } \\right]^2 \\\\   &    \\leq n \\left [ 8 \\sigma^{2 } + 2(3l)^2\\right ]",
    "\\left [ r(m ) - r(m^0)\\right]=:v(m , m^0 ) = v.\\end{aligned}\\ ] ] next we have , for any integer @xmath154 , that @xmath162 \\leq & \\sum_{i=1}^{n } \\mathbb{e } \\left [         \\left| 2y_{i } - m^0_{x_{i } } - m_{x_{i } } \\right|^{k } \\left| m^0_{x_{i } } - m_{x_{i } } \\right|^k              \\right ] \\\\ \\leq & \\sum_{i=1}^{n } \\mathbb{e } \\left [         2^{2k-1}\\left [   |\\mathcal{e}_{i}|^{k } + ( l/2 + l)^{k } \\right ]   \\left| m^0_{x_{i } } - m_{x_{i } } \\right|^{k }              \\right ]",
    "\\\\ \\leq   &   \\sum_{i=1}^{n } \\mathbb{e } \\left [         2^{2k-1}\\left (   |\\mathcal{e}_{i}|^{k } + ( \\frac{3}{2 } l)^{k }",
    "\\right )         ( 3l)^{k-2 }   \\left| m^0_{x_{i } } - m_{x_{i } } \\right|^{2 }              \\right ] \\\\",
    "\\leq     &       2^{2k-1 } \\left [ \\sigma^{2}k!\\xi^{k-2 }               + \\left(\\frac{3}{2 } l \\right ) ^{k } \\right ]   ( 3l)^{k-2 } \\sum_{i=1}^{n}\\mathbb{e }   \\left| m^0_{x_{i } } - m_{x_{i } } \\right|^{2 } \\\\",
    "\\leq   &   \\frac { \\left [ \\sigma^{2}k!\\xi^{k-2 } + ( \\frac{3}{2 } l ) ^k \\right ] \\left [ 4 ( 3l ) \\right]^{k-2 } } { \\sigma^2 + ( \\frac{3}{2 } l)^2 } v \\\\",
    "\\leq   &    \\left [ k!\\xi^{k-2 } + \\left(\\frac{3}{2 } l \\right ) ^{k-2 }   \\right ] [ 4(3 l)]^{k-2 }",
    "\\leq    &    k !",
    "\\left ( \\xi + \\frac{3}{2 } l \\right)^{k-2 }   ( 12l)^{k-2 } v     \\leq v\\frac{k!w^{k-2}}{2},\\end{aligned}\\ ] ] with @xmath163 .    next , for any @xmath164 , applying lemma [ lemmemassart ] with @xmath165 gives @xmath166 \\leq \\exp\\left[\\frac{v\\lambda^{2}}{2n^{2}(1-\\frac{w\\lambda}{n})}\\right].\\ ] ] set @xmath167 $ ] . for the sake of simplicity let us put @xmath168 in order to understand what follows , keep in mind that @xmath169 is a constant and that our optimal estimator comes with @xmath170 , so @xmath171 is of order @xmath32 .    for any @xmath172",
    ", the last display yields @xmath173 \\leq \\frac{\\varepsilon}{2}.\\ ] ] integrating w.r.t .",
    "the probability distribution @xmath174 , we get @xmath175   \\pi",
    "( d m ) \\leq \\frac{\\varepsilon}{2}.\\ ] ] next , fubini s theorem gives @xmath176   \\pi ( d m )    \\hspace*{1.5 cm } \\\\",
    "=   \\mathbb{e } \\int \\exp   \\left\\lbrace    \\alpha     \\bigl ( r(m ) - r(m^0 ) \\bigr )                      + \\lambda\\bigl ( -r(m ) + r(m^0 ) \\bigr )       \\right .   -",
    "\\\\   \\left .              - \\log \\left[\\frac{d\\hat{\\rho}_{\\lambda}}{d \\pi } ( m )   \\right ]          - \\log\\frac{2}{\\varepsilon }          \\right\\rbrace           \\hat{\\rho}_{\\lambda}(d m ) \\leq \\frac{\\varepsilon}{2}.\\end{aligned}\\ ] ] jensen s inequality yields @xmath177 \\leq \\frac{\\varepsilon}{2},\\ ] ] where @xmath178 is the kullback ",
    "leibler divergence of @xmath179 from @xmath180 .",
    "now , using the basic inequality @xmath181 , we get @xmath182 \\geq 0 \\biggr\\ } \\leq \\frac{\\varepsilon}{2}.\\ ] ] using jensen s inequality again gives @xmath183 combining the last two displays we obtain @xmath184 } { \\frac{\\alpha}{\\lambda } } \\biggr\\ }   \\geq    1-\\frac{\\varepsilon}{2}.\\ ] ] + using donsker and varadhan s variational inequality ( lemma 1.1.3 in catoni  @xcite ) , we get @xmath185 } { \\frac{\\alpha}{\\lambda } } \\biggr\\ } \\geq 1-\\frac{\\varepsilon}{2},\\ ] ] where @xmath186 is the set of all positive probability measures over the set of @xmath187 matrices equiped with the borel @xmath188-algebra .",
    "+ we now want to bound from above @xmath189 by @xmath190 .",
    "we can use lemma  [ lemmemassart ] again , to @xmath191 and similar computations yield successively @xmath192 \\leq \\exp\\left[\\frac{v\\lambda^{2}}{2n^{2}(1-\\frac{w\\lambda}{n})}\\right],\\ ] ] and so for any ( data - dependent ) @xmath193 , @xmath194 \\leq \\frac{\\varepsilon}{2},\\ ] ] where @xmath195 here again , with the same spirit with @xmath196 in ( [ defalpha ] ) , @xmath197 is of order @xmath32 also .",
    "so : @xmath198 + \\frac{1}{\\lambda}\\left [ \\mathcal{k}(\\rho , \\pi ) + \\log \\frac{2}{\\varepsilon } \\right ] \\biggr\\}\\geq 1 - \\frac{\\varepsilon}{2}.\\ ] ] + combining ( [ interm4 ] ) and ( [ interm3bis ] ) with a union bound argument gives the general pac - bayesian bound @xmath199 + 2 \\left [ \\mathcal{k}(\\rho , \\pi ) + \\log \\frac{2}{\\varepsilon } \\right ] } { \\alpha   } \\biggr\\ }   \\geq   1-\\varepsilon.\\ ] ]      in the second step , we derive an explicit form for the upper bound in  .",
    "the idea is that , if we restrict the infimum in the upper bound in   to a small set of measures @xmath193 , we are able to provide an explicit bound for this infimum .",
    "this trick was introduced in  @xcite .",
    "let @xmath200 , it means that @xmath201 with @xmath202 .",
    "let us take , for any @xmath203 such that @xmath204 , the probability distribution @xmath205 note that , as @xmath206 , we have @xmath207 and so @xmath208 + thus , ( [ pac - bound ] ) becomes @xmath209        + 2 \\left [   \\mathcal{k } ( \\rho_{u , v , c } , \\pi ) + \\log \\frac{2}{\\varepsilon } \\right ] }        { \\alpha   } \\biggr\\ }   \\\\",
    "\\geq 1-\\varepsilon.\\end{aligned}\\ ] ]    let us fix @xmath210 .",
    "the end the proof consists in calculations to derive an upper bound for the two terms in  .",
    "firstly @xmath211 ( note that we use the notation @xmath212 ) . as @xmath213 and @xmath214",
    ", it can be seen that integral of the three scalar products in the previous equation vanish .",
    "moreover , @xmath215 ^ 2_{ij } \\pi_{ij }      \\leq     \\left (    \\sup\\limits_{ij }   \\left [ ( \\mu - u)\\nu^t \\right]_{ij }    \\right)^2 \\sum\\limits_{ij } \\pi_{ij } \\\\ & \\leq       \\left (    \\sup\\limits_{ij }   \\sum\\limits_{\\ell = 1}^k |\\mu - u|_{i\\ell }     |\\nu|_{j\\ell }     \\right)^2   \\leq       \\left (   k   \\sup\\limits_{i\\ell } |\\mu - u|_{i\\ell } \\,\\ ,   \\sup\\limits_{j\\ell }   |\\nu|_{j\\ell }     \\right)^2   \\\\   &   \\leq       \\left [   k c \\left (   c + \\sqrt { \\dfrac{l}{k } } \\right )   \\right]^2   = kc^2 ( \\sqrt{k } c + \\sqrt{l})^2    , \\end{aligned}\\ ] ] similarly @xmath216 . therefore , from",
    ", we have @xmath217     +    \\| uv^t - m^0\\|_{f , \\pi}^2 .\\end{aligned}\\ ] ]    so , we have an upper bound for the first term in  .",
    "we now deal with the kullback - leibler term : @xmath218 + note that , up to a reordering of the columns of @xmath219 and @xmath220 , we can assume that @xmath221 and @xmath222 , where @xmath223 .",
    "then @xmath224 and , as @xmath225 , @xmath226 + so , @xmath227 + by symmetry , @xmath228 plugging   and   into  , we obtain finally our upper bound for the kullback - leibler term : @xmath229 finally , substituting   and   into  , @xmath230 +    \\right .",
    "\\hspace*{1 cm }",
    "\\\\ +    \\left .",
    "\\| uv^t - m^0\\|_{f,\\pi}^2 \\right )       +   2(m + p ) k_0 \\log \\left ( \\dfrac{1}{c } \\sqrt{\\dfrac{2l}{k } }    \\right )    + \\\\ + 4   k_0 \\log(1/ \\tau )   +    4\\log \\left (    \\frac{\\tau}{1-\\tau } \\right )      +    2\\log \\frac{2}{\\varepsilon }               \\bigg ]           \\biggr\\ }                           \\geq 1-\\varepsilon.\\end{aligned}\\ ] ] let us put @xmath231 . note that as @xmath83 then @xmath232 and thus the condition @xmath233 is satisfied .",
    "so we have the following inequality with probability at least @xmath234 : @xmath235    +   \\dfrac{2}{\\lambda }       \\bigg [           ( m + p ) k_0   \\log \\left ( \\sqrt{\\frac{36n}{m+p } }   \\right ) + \\\\",
    "+ 2   k_0   \\log(1/ \\tau )   +    2\\log \\left (    \\frac{\\tau}{1-\\tau } \\right )      +    \\log \\frac{2}{\\varepsilon }             \\bigg ]              \\bigg\\ }     , \\end{aligned}\\ ] ] where @xmath196 and @xmath197 have been replaced by their definitions , see   and  .",
    "taking now @xmath236 with @xmath237 in the last above display , gives @xmath238 +   \\\\",
    "\\nonumber     +   \\dfrac{8 \\mathcal{c}}{n }       \\bigg [          \\frac{1}{2 } ( m + p ) { \\rm rank ( m ) }   \\log \\left ( \\frac{36n}{m+p }   \\right )                                                      +   \\log \\frac{2}{\\varepsilon }    + \\\\    + 2 { \\rm rank}(m )   \\log(1/ \\tau )   +    2\\log \\left (    \\frac{\\tau}{1-\\tau } \\right )                 \\bigg ]              \\bigg\\ }                       \\biggr\\ }           \\geq 1-\\varepsilon   , \\end{aligned}\\ ] ] where we have used that @xmath239 and @xmath240 . as @xmath241",
    "we have @xmath242 +   \\\\",
    "\\nonumber     +   \\dfrac{8 \\mathcal{c}}{n }       \\bigg [          \\frac{1}{2 } ( m + p ) { \\rm rank ( m ) }   \\log ( 36k )                                                      +   \\log \\frac{2}{\\varepsilon }    + \\\\    + 2 { \\rm rank}(m )   \\log(1/",
    "\\tau )   +    2\\log \\left (    \\frac{1}{1-\\tau }",
    "\\right )                 \\bigg ]              \\bigg\\ }                       \\biggr\\ }           \\geq 1-\\varepsilon   .\\end{aligned}\\ ] ] + moreover , @xmath243 for some constant @xmath244 depending on @xmath34 only .",
    "remind that @xmath87 is a constant in @xmath245 , we have @xmath246 for some constant @xmath247 depending on @xmath87 only . finally , from",
    ", we obtain @xmath248                     \\biggr\\ }           \\geq 1-\\varepsilon   , \\end{aligned}\\ ] ] for some constant @xmath249 depending only on @xmath250 and @xmath251 .",
    "however , as the constant @xmath251 also depends on @xmath86 then @xmath252 can be rewritten as @xmath253 as in the statement of the theorem .",
    "r.  foygel , o.  shamir , n.  srebro , and r.  salakhutdinov .",
    "learning with the weighted trace - norm under arbitrary sampling distributions . in _ advances in neural information processing systems _ , pages 21332141 , 2011 .",
    "p.  massart . , volume 1896 of _ lecture notes in mathematics_. springer , berlin , 2007 .",
    "lectures from the 33rd summer school on probability theory held in saint - flour , july 623 , 2003 , edited by jean picard .",
    "r.  salakhutdinov and a.  mnih .",
    "bayesian probabilistic matrix factorization using markov chain monte carlo . in _ proceedings of the 25th international conference on machine learning _ , pages 880887 .",
    "acm , 2008 ."
  ],
  "abstract_text": [
    "<S> bayesian methods for low - rank matrix completion with noise have been shown to be very efficient computationally  @xcite . while the behaviour of penalized minimization methods is well understood both from the theoretical and computational points of view ( see  @xcite among others ) in this problem , </S>",
    "<S> the theoretical optimality of bayesian estimators have not been explored yet . in this paper </S>",
    "<S> , we propose a bayesian estimator for matrix completion under general sampling distribution . </S>",
    "<S> we also provide an oracle inequality for this estimator . </S>",
    "<S> this inequality proves that , whatever the rank of the matrix to be estimated , our estimator reaches the minimax - optimal rate of convergence ( up to a logarithmic factor ) . </S>",
    "<S> we end the paper with a short simulation study . </S>"
  ]
}