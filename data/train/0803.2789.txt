{
  "article_text": [
    "in the last few decades it has become apparent that many problems in information theory , and coding problems in particular , can be mapped onto ( and interpreted as ) analogous problems in the area of statistical physics of disordered systems , most notably , spin glass models .",
    "such analogies are useful because physical insights , as well as statistical mechanical tools and analysis techniques ( like the replica method ) , can be harnessed in order to advance the knowledge and the understanding with regard to the information ",
    "theoretic problem under discussion ( and conversely , information ",
    "theoretic approaches to problems in physics may sometimes prove useful to physcists as well ) .",
    "a very small , and by no means exhaustive , sample of works along this line includes references [ 1][25 ] .    in particular ,",
    "sourlas @xcite,@xcite was the first to observe that there are strong analogies and parallisms between the behavior of ensembles of error correcting codes and certain spin glass models with quenched parameters , like the @xmath0spin glass model and derrida s random energy model ( rem ) @xcite,@xcite,@xcite at least as far as the mathematical formalism goes .",
    "in particular , the rem is an especially attractive model to adopt in this context , as it is , on the one hand , exactly solvable , and on the other hand , rich enough to exhibit phase transitions . as noted in ( * ? ? ? * chap .",
    "6 ) and @xcite , ensembles of error correcting codes ` inherit ' these phase transitions from the rem when viewed as physical systems whose phase diagram is defined in the plane of the coding rate vs.  decoding temperature . in @xcite",
    "this topic was further investigated and ensemble performance figures of error correcting codes ( random coding exponents ) were related to the free energies in the various phases of the phase diagram .",
    "while the above  described relation takes place between _ pure _ channel coding and the rem _ without _ any external magnetic field , in this work , we demonstrate that there are also intimate relationships between _ combined source / channel coding _ and the rem _ with _ such a magnetic field . in particular",
    ", it turns out that typical patterns of erroneously decoded messages in the source / channel coding problem have `` magnetization '' properties that are analogous to those of the rem in certain phases , where the non  uniformity of the distribution of the source in the joint source  channel coding system , plays the role of an external magnetic field applied to the spin glass modeled by the rem .",
    "we also relate the ensemble performance ( random coding exponents ) of joint source  channel codes to the free energy of the rem in its different phases .",
    "the outline of this paper is as follows . in section 2",
    ", we provide some background , both on the information theoretic aspect of this work , which is the problem of joint source channel coding , and the statistical mechanical aspect , which is the rem and its magnetic properties . in section 3",
    ", we present the phase diagram pertaining to finite  temperature decoding of an ensemble of joint source  channel codes and characterize the free energies in the various phases . finally ,",
    "in section 4 , we derive random coding exponents pertaining to this emsemble and demonstrate their relationships to the free energies .",
    "in this section , we give some very basic background which will be needed in the sequel . in subsection 2.1 , we provide a brief overview of shannon s fundamental coding theorems , the skeleton of information theory : the source coding theorem , the channel coding theorem , and finally the joint source  channel coding theorem . in subsection 2.2",
    ", we review a few models of spin glasses , with special emphasis on the rem .",
    "suppose we wish to compress a sequence of @xmath1 bits , @xmath2 , drawn from a stationary memoryless binary source , i.e. , each bit is drawn independently , where @xmath3 .",
    "shannon s _ source coding theorem _",
    "( see , e.g. , ( * ? ? ? * chap .  5 ) ) tells that if we demand that the source sequence would be perfectly reconstructable from the compressed data , then the best achievable compression ratio ( i.e. , the smallest average ratio between the compressed message length and the original source message length ",
    "@xmath1 ) , at the limit of large @xmath1 , is given by the entropy of the source , which in the binary memroyless case considered here , is given by : @xmath4 many practical coding algorithms are known to achieve @xmath5 asymptotically , e.g. , huffman coding , shannon coding , arithmetic coding , and lempel  ziv coding , to name a few @xcite .",
    "shannon s celebrated _ channel coding theorem _ ( see ,",
    "e.g. , ( * ? ? ?",
    "* chap .  7 ) ) is about reliable transmission of digital information across a noisy channel : suppose we wish to transmit a binary messsage of @xmath6 bits , indexed by @xmath7 ( @xmath8 ) , through a noisy binary symmetric channel , which flips the transmitted bit with probability @xmath0 or conveys it unaltered , with probability @xmath9 .",
    "if we wish to convey the message via the channel reliably ( i.e. , with very small probability of error ) , then before we transmit the message via the channel , we have to encode it , i.e. , map it in a sophisticated manner into a longer binary message of length @xmath10 ( @xmath11 ) and then transmit the encoded message @xmath12 .",
    "the ratio @xmath13 is called the _ coding rate_. it measures how efficiently the channel is used , i.e. , how many information bits are conveyed per one channel use .",
    "the corresponding channel output sequence , @xmath14 ( with some of the bits flipped by the channel ) , is received at the decoder .    the optimum decoder , in the sense of minimum probability of error , estimates the message @xmath7 by the _ maximum a  posteriori _ ( map ) decoder , i.e.",
    ", it selects the message @xmath8 which maximizes posterior probability given @xmath15 , that is , @xmath16 , or equivalently , it maximizes the product @xmath17 , where @xmath18 the prior probability of message @xmath7 and @xmath19 is the conditional probability of the observed @xmath15 given that @xmath20 was transmitted . in the important special case where all messages are _ a - priori _",
    "equiprobable , that is , @xmath21 for all @xmath7 , the map decoding rule boils down to the maximization of @xmath19 , which is the _ maximum likelihood _ ( ml )",
    "decoding rule .",
    "channel capacity @xmath22 is defined as the supremum of all coding rates @xmath23 for which there still exist encoders and decoders which make the probability of error arbitrarly small provided that @xmath10 is large enough ( keeping @xmath13 fixed ) .",
    "shannon s channel coding theorem provides a formula of the channel capacity , which in the binary case considered here , is given by @xmath24 one of the mainstream efforts in the information theory literature has evolved around devising practical coding and decoding schemes , in terms of computational complexity and storage , with rates close to capacity .      finally , we consider the problem of _ joint source  channel coding _ ( see ,",
    "e.g. , ( * ? ? ?",
    "* sect .  7.13 ) ) : suppose we have a binary memoryless source , as in the first paragraph above , and a binary memoryless channel , as in the second paragraph above .",
    "we assume that by the time that the source generates @xmath1 symbols , the channel can transmit @xmath25 bits ( @xmath26 is fixed ) .    a joint source  channel code maps the source sequence @xmath27 of length @xmath1 into a channel input sequence @xmath28 of length @xmath10 .",
    "the decoder , that receives the channel output vector @xmath15 , estimates @xmath29 either by the _ symbol map _ decoder , which minimizes the symbol error probability ( or the bit error probability ) or the _ word map _ decoder , which as mentioned earlier , minimizes the word error probability .",
    "the word map decoder works similarly to the above described map decoder for a channel code : it estimates the source sequence as a whole by seeking the vector @xmath29 that maximizes @xmath30 , where @xmath31 is the probability of the source vector @xmath29 .",
    "the symbol map decoder , on the other hand , estimates each bit @xmath32 of the source separately by seeking the symbol @xmath33 that maximizes @xmath34 , @xmath35 .",
    "these two decoders can be thought of as two special cases of a more general class of decoders , referred to as _ finite  temperature decoders _",
    "@xcite . a finite  temperature decoder estimates the @xmath36th symbol @xmath32 by @xmath37^\\beta,\\ ] ] where the parameter @xmath38 can be thought of as an inverse temperature parameter .",
    "the choice @xmath39 corresponds to the symbol map decoder , whereas @xmath40 gives us the word map decoder ( * ? ? ?",
    "* chap .  6 ) .",
    "the joint source  channel coding theorem asserts that a necessary and sufficient condition for the existence of codes , that for large enough @xmath10 and @xmath1 ( with @xmath41 fixed ) , @xmath29 can be decoded with aribrarily small probability of error ( both wordwise and symbolwise ) is given by @xmath42 one approach to achieve reliable communication , whenever this condition holds , is to apply separate source coding and channel coding : first compress the source to essentially @xmath5 bits per symbol , resulting in a binary compressed message of length about @xmath43 bits , as described in the first paragraph above , and then use a reliable channel code of rate @xmath44 to convey the compressed message , as described in the second paragraph .",
    "the decoder will first decode the message by the corresponding channel decoder and then decompress the resulting message .",
    "another approach is to map @xmath29 directly to a channel input vector @xmath28",
    ". it can be shown ( * ? ? ?",
    "* exercise 5.16 , p.  534 ) that by a random selection of a code from the uniform ensemble ( i.e. , by generating each codeword @xmath28 , @xmath45 , independently by a sequence of @xmath10 fair coin tosses ) , the average probability of error , over this ensemble of codes , tends to zero as the block length goes to infinity , as long as the above necessary and sufficient condition holds .",
    "consider a spin glass with @xmath10 spins , designated by a binary vector @xmath46 , @xmath47 , @xmath48 .",
    "the simplest model of this class is that of a _ paramagnetic _ solid , namely , the one where the only effect is that of the external magnetic field @xmath49 , whereas the effect of interactions is negligible ( cf .",
    "3 ) ) . assuming that the spin directions are all either parallel or antiparallel to the direction of the external magnetic field , the energy associated with a configuration @xmath50 is given ( in the appropriate units ) by : @xmath51 which means ( according to the boltzmann distribution ) that each spin is independently oriented upward ( + 1 ) with probability @xmath52 $ ] or downward ( -1 ) with probability @xmath53 $ ] .",
    "this means that the average ( net ) magnetic moment is @xmath54 and so the average internal energy per particle is @xmath55 and the free energy per particle is @xmath56 $ ] .",
    "more involved ( and more interesting ) situations occur , of course , when the effect of mutual interactions among the spins is appreciable .",
    "the simplest model that accounts for interactions is the _ ising model _ , given by @xmath57 where the second term is the contribution of the external magnetic field as before , and the in the first term , pertaining to the interaction , @xmath58 describes the intensity of the interaction with the summation being defined over pairs of neighboring spins ( depending on the geometry of the problem ) .",
    "more general models allow interactions not only with immediate neighbors , but also with more distant ones , and then there are different strengths of interaction , depending on the distance between the two spins . in this case , the first term is replaced , by the more general form @xmath59 , where now the sum can be defined over all possible pairs @xmath60 . here , in addition to the ferromagnetic case , where all @xmath61 , and the antiferromagnetic case , where all @xmath62 , there is also a mixed situation where some @xmath63 are positive and others are negative , which is the case of a _",
    "spin glass_. here , not all spin pairs can be in their preferred mutual position ( parallel / antiparallel ) , thus the system may be _ frustrated .",
    "_    to model situations of disorder , it is common to model @xmath64 as random variables ( rv s ) with , say , equal probabilities of being positive or negative .",
    "for example , in the edwards ",
    "anderson ( ea ) model @xcite , @xmath64 are taken to be i.i.d .",
    "zero  mean gaussian rv s when @xmath36 and @xmath65 are neighbors and set to zero otherwise .",
    "in the sherrington ",
    "kirkpatrick ( sk ) model @xcite , all @xmath64 are i.i.d .",
    "mean gaussian rv s .",
    "in the _ @xmath0spin  glass model _ , the interaction terms consist of all products of combinations of @xmath0 spins ( rather than just pairs ) with gaussian coefficients of the appropriate scaling ( cf .",
    "e.g. , @xcite ) .    in all these models ,",
    "the system has two levels of randomness : the randomness of the interaction coefficients and the randomness of the spin configuration given the interaction coefficients , according to the boltzmann distribution .",
    "however , the two sets of rv s are normally treated differently .",
    "the random coefficients are commonly considered _ quenched _",
    "rv s , namely , they are considered fixed in the time scale at which the spin configuration may vary .",
    "this is analogous to the model of coded communication in a random coding paradigm : a randomly drawn code should normally be thought of as a quenched entity , as opposed to the randomness of the source and/or the channel .      in @xcite,@xcite,@xcite , derrida took the above described idea of randomizing the ( parameters of the ) hamiltonian to an extreme , and suggested a model of spin glass with disorder under which the energy levels @xmath66 are simply i.i.d .",
    "rv s , without any structure in the form of ( [ ham ] ) or its above  described extensions .",
    "it can also be viewed , however , as the asymptotic behavior of the @xmath0spin  glass model when @xmath67 ( a limit to be taken after the limit @xmath68 , i.e. , @xmath69 ) @xcite . in particular , in the absence of a magnetic field , the @xmath70 rv s @xmath66 are taken to be i.i.d . , zero ",
    "mean gaussian rv s , all with variance @xmath71 , where @xmath58 is a parameter . to match the behavior of the hamiltonian ( [ ham ] ) with a limited number of interacting neighbors and random interaction parameters , which has a number of independent terms that is linear in @xmath10 . ]",
    "the beauty of the rem is in that on the one hand , it is very easy to analyze , and on the other hand , it consists of sufficient richness to exhibit phase transitions .",
    "the basic observation about the rem is that for a typical realization of the configurational energies @xmath66 , the density of number of configurations with energy about @xmath72 ( i.e. , between @xmath72 and @xmath73 ) , @xmath74 , is proportional ( up to sub  exponential terms in @xmath10 ) to @xmath75 , as long as @xmath76 , whereas energy levels outside this range are typically not populated by spin configurations ( @xmath77 ) , as the probability of having at least one configuration with such an energy decays exponentially with @xmath10 .",
    "thus , the asymptotic ( thermodynamical ) entropy per spin , which is defined by @xmath78 is given by @xmath79 the partition function of a typical realization of a rem spin glass is then @xmath80 where the notation @xmath81 designates asymptotic equivalence between two functions of @xmath10 in the exponential scale .",
    "means that @xmath82 . ]",
    "the exponential growth rate of @xmath83 , @xmath84 behaves according to @xmath85\\nonumber\\\\ & = & \\max_{|e|\\le e_0}\\left[\\ln 2- \\left(\\frac{e}{nj}\\right)^2-\\beta j \\cdot\\left(\\frac{e}{nj}\\right)\\right].\\end{aligned}\\ ] ] solving this simple optimization problem , one finds that @xmath86 is given by @xmath87 which means that the asymptotic free energy per spin , a.k.a .",
    "the _ free energy density _",
    ", is given by ( cf .",
    "* proposition 5.2 ) ) : @xmath88 thus , the free energy density is subjected to a phase transition at the inverse temperature @xmath89 . at high temperatures ( @xmath90 ) , which is referred to as the _ paramagnetic phase _ , the partition function is dominated by an exponential number of configurations with energy @xmath91 and the entropy grows linearly with @xmath10 .",
    "when the system is cooled to @xmath92 and beyond , which is the _ glassy phase _ , the system freezes but it is still in disorder  the partition function is dominated by a subexponential number of configurations of minimum energy @xmath93 .",
    "the entropy , in this case , grows sublinearly with @xmath10 , namely the entropy per spin vanishes , and the free energy density no longer depends on @xmath38 .",
    "further details about the rem can be found in @xcite and the references mentioned in the introduction .",
    "the random energy levels of the rem , as described above , represent the interaction energies among the various spins in the absence of an external magnetic field . in the presence of an external uniform magnetic field , @xmath49 ( cf .",
    "@xcite,@xcite,@xcite ) , the hamiltonian of the system should be supplemented with the term @xmath94 ( cf .",
    "( [ ham ] ) ) , where @xmath95=\\frac{2n_1(\\sigma)}{n}-1\\ ] ] is the magnetization associated with the configuration @xmath50 , and @xmath96 is the number of spins up , @xmath97 .",
    "as far as the statistical description of the rem goes , this shifts the expectation of the random energy level @xmath98 from zero to @xmath99 .",
    "equivalently , we can assign the same zero ",
    "mean gaussian distribution as before to the interaction energy , call it now @xmath100 , and add to each configuration @xmath50 the term @xmath99",
    ". the corresponding partition function would then be : @xmath101}\\nonumber\\\\ & = & \\sum_m\\left[\\sum_{\\sigma:~m(\\sigma)=m}e^{-\\beta{{\\cal e}}_i(\\sigma)}\\right]e^{\\beta nmh}\\nonumber\\\\ & { \\stackrel{\\delta } { = } } & \\sum_m \\zeta(\\beta , m)e^{\\beta nmh}\\end{aligned}\\ ] ] where @xmath102 , referred to as the _ partial partition function _ , contains only the contributions of configurations whose magnetization @xmath103 is equal to @xmath7 .",
    "the behavior of @xmath102 is exactly like that of the rem without a magnetic field , except that instead of @xmath70 configurations , it has only @xmath104 configurations , where @xmath105 is the binary entropy function . by carrying out a similar analysis as in the previous subsection to @xmath102 and then finding the dominant contribution of @xmath7 ( which is the typical magnetization )",
    ", one can show ( cf .",
    "@xcite,@xcite,@xcite ) that there exists a phase transition at @xmath106 , where @xmath107 is the unique solution to the equation @xmath108 it is not difficult to see that @xmath107 is a non  increasing function of @xmath109 and therefore @xmath110 is non  decreasing , with a minimum at @xmath111 , given by @xmath112 ( see fig .  1 ) . for high temperatures ( @xmath113 ) , where the effect of the interactions among the spins is relatively insignificant , one observes the ordinary paramagnetic behavior , with the average magnetization is @xmath114 whereas for low temperatures ( @xmath115 ) , the system is frozen in the spin glass phase where the magnetization no longer depends on the temperature : @xmath116 the free energy per spin is given by @xmath117/2)}{\\beta}+h\\tanh(\\beta h)\\right ] & \\beta < \\beta_c(h)\\\\ -\\left[j\\sqrt{h\\left(\\frac{1+\\tanh(\\beta_c(h)h)}{2}\\right)}+ h\\tanh(\\beta_c(h)h)\\right ] & \\beta \\ge \\beta_c(h ) \\end{array}\\right.\\ ] ] as can be seen , no sponteneous magnetization takes place under the rem , even at low temperatures ( @xmath118 implies @xmath119 ) . as for other thermodynamic quantities , we have the average internal energy per spin @xmath120=   \\left\\{\\begin{array}{ll } -h\\tanh(\\beta h)-\\frac{\\beta j^2}{2 } & \\beta <",
    "\\beta_c(h)\\\\ -h\\tanh(\\beta_c(h)\\cdot h)-\\frac{\\beta_c(h)j^2}{2 } & \\beta \\ge \\beta_c(h ) \\end{array}\\right.\\ ] ] the entropy per spin @xmath121=\\left\\{\\begin{array}{ll } h\\left(\\frac{1+\\tanh(\\beta h)}{2}\\right)-\\frac{\\beta^2 j^2}{4 } & \\beta < \\beta_c(h)\\\\ 0 & \\beta \\ge \\beta_c(h ) \\end{array}\\right.\\ ] ] and the magnetic susceptibility @xmath122_{h=0}=\\left\\{\\begin{array}{ll } \\beta & \\beta < \\beta_c(h)\\\\ \\beta_c(h ) & \\beta \\ge \\beta_c(h ) \\end{array}\\right.\\ ] ]    ( 0,0 )    # 1#2#3#4#5 @font    ( 4518,2814)(320,-2968 ) ( 2369,-355)(0,0)[lb ] ( 4594,-2486)(0,0)[lb ] ( 3184,-1671)(0,0)[lb ]      in this subsection , we analyze the behavior of a finite  temperature decoder for a typical randomly selected code using the tools of the analysis of the rem in a magnetic field . using the viewpoint of the magnetic properties of the rem",
    ", it will be seen that the source bits play the role of spins in a magnetic field whose intensity is @xmath123 , where @xmath124 is the probability that @xmath125 for each @xmath36 .",
    "accordingly , instead of the binary alphabet @xmath126 that we used before , it will prove more convenient to let each @xmath32 assume values in @xmath127 .",
    "another slight change in notation , that will take place mostly for the sake of convenience , is that instead of defining channel capacity and coding rates in terms of bits , we will define them in units of _ nats _ , where @xmath128 nat @xmath129 bits .",
    "this means that logarithms will be taken to the natural basis @xmath130 rather than the base 2 .",
    "accordingly , @xmath5 will be redefined hereafter as @xmath131 and the capacity of the binary channel considered in section 2 will be redefined as @xmath132 .      to see that @xmath152 behaves like the rem in a magnetic field , consider the following : first , denote by @xmath153 the number of @xmath154 s in @xmath29 , so that the magnetization , @xmath155 $ ] , pertaining to spin configuration @xmath29 , is given by @xmath156 .",
    "equivalently , @xmath157 , and then @xmath158^{n/2}\\left(\\frac{q}{1-q}\\right)^{nm({\\mbox{\\boldmath $ u$}}))/2}\\nonumber\\\\ & = & [ q(1-q)]^{n/2}e^{nm({\\mbox{\\boldmath $ u$}})h}\\end{aligned}\\ ] ] where @xmath49 is defined as above . by the same token , for the binary symmetric channel we have : @xmath159 where @xmath160 and @xmath161 is the hamming distance between @xmath162 and @xmath15 , namely , the number of places @xmath163 where @xmath164 .",
    "thus , @xmath165^{n\\beta/2}\\sum_m\\left[\\sum_{{\\mbox{\\boldmath $ x$}}({\\mbox{\\boldmath $ u$}}):~m({\\mbox{\\boldmath $ u$}})=m } e^{-\\beta\\ln[1/p({\\mbox{\\boldmath $ y$}}|{\\mbox{\\boldmath $ x$}}({\\mbox{\\boldmath $ u$}}))]}\\right]e^{n\\beta mh}\\nonumber\\\\ & = & [ q(1-q)]^{\\beta n/2}(1-p)^{n\\beta}\\sum_m\\left[\\sum_{{\\mbox{\\boldmath $ x$}}({\\mbox{\\boldmath $ u$}}):~m({\\mbox{\\boldmath $ u$}})=m } e^{-\\beta bd_h({\\mbox{\\boldmath $ x$}}({\\mbox{\\boldmath $ u$}}),{\\mbox{\\boldmath $ y$}})}\\right]e^{\\beta nmh}\\nonumber\\\\ & { \\stackrel{\\delta } { = } } & [ q(1-q)]^{n\\beta/2}(1-p)^{n\\beta}\\sum_m\\zeta(\\beta , m ) e^{\\beta nmh}\\end{aligned}\\ ] ] where the resemblance to eq .",
    "( [ rempart ] ) is self evident , with @xmath102 being redefined as the second bracketed term . in analogy to the above analysis of the rem , @xmath102 here behaves like in the rem without a magnetic field , namely , it contains exponentially @xmath166 terms , with the random energy levels of the rem being replaced now by random hamming distances @xmath167 that are induced by the random selection of the code @xmath168 .",
    "is also random , but this randomness does not play any essential role here .",
    "this discussion applies as well for every given @xmath15 .",
    "] using the same considerations as with the rem ( see also @xcite ) , @xmath102 can be represented as @xmath169 , where @xmath170 is the number of vectors @xmath171 with @xmath172 and @xmath173 . since @xmath170 is the sum of @xmath174 many i.i.d .",
    "binary random variables of the form @xmath175 ( again , with randomness induced by the random selection of @xmath28 ) , each with expectation given by @xmath176}$ ] , then @xmath170 is typically zero for all @xmath177 such that @xmath178 , and is typically around its expectation , @xmath179}$ ] , for all @xmath177 such that @xmath180 .",
    "defining now the gilbert ",
    "varshamov distance @xmath181 ( * ? ? ?",
    "6 ) as the solution @xmath182 to the equation @xmath183 , the condition @xmath180 is equivalent to the condition @xmath184 .",
    "thus , for a typical randomly selected code , @xmath185 } \\left[\\frac{1}{\\theta}h\\left(\\frac{1+m}{2}\\right)+h(\\delta)-\\ln 2-\\beta b\\delta\\right]\\nonumber\\\\ & = & \\left\\{\\begin{array}{ll } \\frac{1}{\\theta}h\\left(\\frac{1+m}{2}\\right)-\\ln 2 + h(p_\\beta)-\\beta bp_\\beta &   p_\\beta \\ge \\delta_{gv}\\left(\\frac{1}{\\theta}h\\left(\\frac{1+m}{2}\\right)\\right ) \\\\",
    "-\\beta b \\delta_{gv}\\left(\\frac{1}{\\theta}h\\left(\\frac{1+m}{2}\\right)\\right ) & p_\\beta <",
    "\\delta_{gv}\\left(\\frac{1}{\\theta}h\\left(\\frac{1+m}{2}\\right)\\right ) \\end{array}\\right . \\end{aligned}\\ ] ] where @xmath186 . the condition @xmath187 is equivalent to the condition @xmath188.\\ ] ] the exponential order of @xmath189 , as a function of @xmath1 is then @xmath190= \\max_m[\\theta\\phi(\\beta , m)+\\beta mh].\\ ] ] for small enough @xmath38 , the dominant value of @xmath7 is the one that maximizes @xmath191 $ ] , namely , the well  known paramagnetic magnetization @xmath192 .",
    "this is true as long as @xmath193 .",
    "consider then the equation @xmath194 where the unknown is @xmath38 , or equivalently , the equation @xmath195 now @xmath196 is decreasing with @xmath38 , while @xmath197 $ ] is increasing . at @xmath198 , @xmath199 whereas @xmath200 . as @xmath40 , @xmath201 whereas @xmath202 , provided that @xmath203 .",
    "thus , for @xmath203 , there must be a unique solution , which we shall denote by @xmath204 , where the subscript `` pg '' stands for the fact that this is the boundary curve between the paramagnetic phase and the glassy phase .",
    "since @xmath205 is decreasing with @xmath109 , @xmath204 is decreasing in @xmath109 , i.e. , the temperature @xmath206 is increasing in @xmath109 , as before ( see fig .  [ gen3 ] ) .",
    "as for the case @xmath111 , for @xmath207 , we have @xmath208.\\ ] ] for @xmath209 , @xmath210 , namely , @xmath211 , which means that there is no phase transition as the behavior is paramagnetic at all temperatures . in the same manner , it is easy to see that @xmath212 for all @xmath213 , which is another case where there are no phase transitions , but this time , it is a glassy behavior at all temperatures .    as long as @xmath214 , we have @xmath215 on the other hand , for @xmath216 , the system is in the glassy phase . in this case ,",
    "@xmath217\\ ] ] thus , the maximizing @xmath7 depends only on @xmath49 but not on @xmath38 . in this case",
    ", we have @xmath218 and so @xmath219\\nonumber\\\\ & = & \\beta\\left[h\\tanh(\\beta_{pg}(h)\\cdot h)- b\\theta p_{\\beta_{pg}(h)}\\right].\\end{aligned}\\ ] ] the free  energy density associated with erroneous messages is therefore given by @xmath220-\\theta\\ln(1-p)-\\frac{\\psi(\\beta , h)}{\\beta}\\ ] ] i.e. , @xmath221 where @xmath222-\\theta\\ln(1-p)-\\frac{1}{\\beta}\\left [ h\\left(\\frac{1+\\tanh(\\beta h)}{2}\\right)-\\theta(\\ln 2-h(p_\\beta))\\right ] + \\theta bp_\\beta - h\\tanh(\\beta h)\\ ] ] and @xmath223-\\theta\\ln(1-p)- h\\tanh(\\beta_{pg}(h)\\cdot h)+ b\\theta p_{\\beta_{pg}(h)}.\\ ] ]    the boundary between the ferromagnetic phase ( where @xmath144 is the dominant term in @xmath83 ) and the glassy phase is the vertical line ( see fig .  [ gen3 ] ) @xmath224 , where @xmath225 is the solution to the equation @xmath226-\\ln(1-p)- \\frac{h\\tanh(\\beta_{pg}(h)h)}{\\theta}+bp_{\\beta_{pg}(h)}\\ ] ] which after rearranging terms becomes @xmath227 whose solution in turn is achieved when @xmath228 , i.e. , @xmath229 which is nothing but the boundary of reliable communication ( [ relcond ] ) .",
    "thus , @xmath230 where @xmath231 is the inverse of the function @xmath105 in the range where the argument is in @xmath232 $ ] . the vertical line @xmath224 intersects the paramagnetic  glassy boundary curve @xmath233 at the triple point @xmath234 , namely , @xmath235 .",
    "the ferromagnetic region , pertaining to correct decoding ( where @xmath236 ) , is @xmath237 , where @xmath238 is paramagnetic ",
    "ferromagnetic boundary curve ( see fig .  [ gen3 ] ) given by the solution @xmath239 of the equation @xmath240 for every given @xmath49 which is larger than @xmath225 in absolute value .",
    "as can be seen , it also contains the point @xmath234 .",
    "* discussion : * we see that correct decoding occurs in a sufficiently strong magnetic field .",
    "this is not surprising as a strong magnetic field corresponds to a low  entropy source which can be transmitted reliably .",
    "the above exposition of the magnetization as a function of @xmath49 and @xmath241 is instructive for the understanding of typical error patterns in joint source  channel coding . at very low temperatures ( like in word map decoding , which corresponds to @xmath40 ) , the ( sub  exponentially few ) typical patterns of the erroneneously decoded vectors @xmath171 have magnetization dictated by the frozen phase , namely , @xmath242 , independently of the decoding temperature . for magnetic fields smaller than @xmath225 in absolute value ( namely , for sources with high entropy ) , @xmath243 , which means that the magnetization of a typical erroneously decoded sequence is _ higher _ than that of a typical ( correct ) source sequence which is @xmath244 .",
    "if the working temperature is lower than @xmath245 , this remains true no matter how small @xmath109 is .",
    "if , on the other hand , @xmath246 , then when the magnetic field is reduced , the magnetization of the ( exponentially many ) erroneously decoded vectors @xmath171 is given by @xmath247 , which is still higher than that of the typical source vector @xmath29 , but now it is temperature  dependent .",
    "( 0,0 )    # 1#2#3#4#5 @font    ( 6098,3794)(182,-3958 ) ( 3019,-432)(0,0)[lb ] ( 6027,-3312)(0,0)[lb ] ( 2941,-2374)(0,0)[lb ] ( 922,-2008)(0,0)[lb ] ( 1655,-3549)(0,0)[lb ] ( 4188,-1971)(0,0)[lb ] ( 2941,-3108)(0,0)[lb ] ( 3597,-3549)(0,0)[lb ]",
    "in this section , we provide bounds on the ensemble performance of joint source channel codes for the binary symmetric source and the binary symmetric channel . in particular , we examine the exponential decay rate of the average probability of correct decoding ( the correct decoding exponent , for short ) when the condition for reliable communication ( [ relcond ] ) is violated as well as the exponential decay rate of the average probability of error ( error exponent ) when this condition holds . as will be seen ,",
    "the former is intimately related to the free energy in the glassy phase , whereas the latter is strongly related to the free energy in the paramagnetic phase .",
    "the relationship between the previous derivations and both the correct decoding exponent and the error exponent stems from the fact both performance measures are bounded by expressions that are strongly related to the partition function @xmath146 .",
    "the probability of correct decoding pertaining to the word map decoder is well known ( and can easily be shown ) to be given by @xmath248\\nonumber\\\\ & = & \\sum_{{\\mbox{\\boldmath $ y$}}}\\lim_{\\beta\\to\\infty}\\left[\\sum_{{\\mbox{\\boldmath $ u$ } } } p^\\beta({\\mbox{\\boldmath $ u$}})p^\\beta({\\mbox{\\boldmath $ y$}}|{\\mbox{\\boldmath $ x$}}({\\mbox{\\boldmath $ u$}}))\\right]^{1/\\beta}\\nonumber\\\\ & = & [ q(1-q)]^{n/2}(1-p)^n\\sum_{{\\mbox{\\boldmath $ y$ } } } \\lim_{\\beta\\to\\infty}\\left[\\sum_m\\zeta(\\beta , m)e^{n\\beta mh}\\right]^{1/\\beta}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & [ q(1-q)]^{n/2}(1-p)^n\\sum_{{\\mbox{\\boldmath $ y$ } } } \\lim_{\\beta\\to\\infty}\\sum_m\\zeta^{1/\\beta}(\\beta , m)e^{nmh},\\end{aligned}\\ ] ] where with a slight abuse of notation , here @xmath102 is redefined to include _ all _ messages @xmath171 , including the correct one .",
    "now , taking the ensemble average : @xmath249^{n/2}(1-p)^n   \\sum_{{\\mbox{\\boldmath $ y$}}}\\lim_{\\beta\\to\\infty}\\sum_m{\\mbox{\\boldmath $ e$}}\\{\\zeta^{1/\\beta}(\\beta , m)\\}\\cdot e^{nmh}.\\ ] ] now , @xmath250^{1/\\beta}\\right\\}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & { \\mbox{\\boldmath $ e$}}\\left\\{\\left [ \\max_{\\delta}n_{{\\mbox{\\boldmath $ y$}},m}(\\delta)e^{-\\beta bn\\delta}\\right]^{1/\\beta}\\right\\}\\nonumber\\\\ & = & { \\mbox{\\boldmath $ e$}}\\left\\ { \\max_{\\delta}n_{{\\mbox{\\boldmath $ y$}},m}^{1/\\beta}(\\delta)e^{-",
    "bn\\delta}\\right\\}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & \\sum_{\\delta}{\\mbox{\\boldmath $ e$}}\\{n_{{\\mbox{\\boldmath $ y$}},m}^{1/\\beta}(\\delta)\\}\\cdot e^{-bn\\delta}\\end{aligned}\\ ] ] where again , @xmath170 is the number of codewords @xmath168 , corresponding to source words with @xmath172 , which fall at hamming distance @xmath251 from @xmath15 .",
    "now , as shown in @xcite , ( * ? ? ?",
    "* appendix ) , @xmath252\\right\\ } &   \\frac{1}{\\theta}h\\left(\\frac{1+m}{2}\\right)+h(\\delta)<\\ln 2\\\\ \\exp\\left\\{n\\left[\\frac{1}{\\theta}h\\left(\\frac{1+m}{2}\\right)+h(\\delta)-\\ln 2\\right]/\\beta\\right\\ } &   \\frac{1}{\\theta}h\\left(\\frac{1+m}{2}\\right)+h(\\delta)\\ge\\ln 2\\end{array}\\right.\\ ] ] thus , @xmath253\\right\\}\\ ] ] and so , @xmath254^{n/2}(1-p)^n \\sum_{{\\mbox{\\boldmath $ y$}}}\\sum_m \\left [ \\exp\\left\\{n\\left[\\frac{1}{\\theta}h\\left(\\frac{1+m}{2}\\right)-\\ln 2+\\right.\\right.\\right.\\nonumber\\\\ & & \\left.\\left.\\left .",
    "\\max_{\\delta\\le\\delta_{gv}(h((1+m)/2)/\\theta)}\\{h(\\delta)-b\\delta\\}\\right]\\right\\}\\right]e^{nmh}.\\end{aligned}\\ ] ] the dominant @xmath7 is the one that maximizes @xmath255 now , if @xmath256 , then the inner maximization is attained at @xmath257 and we get @xmath258 if @xmath259 , namely , the condition for reliable communication holds , this indeed happens . in this case",
    ", we get @xmath249^{n/2}(1-p)^n\\cdot 2^n\\cdot   \\exp\\left\\{n\\left[\\frac{h(q)}{\\theta}-\\ln 2+h(p)-bp+\\frac{2q-1}{2\\theta}\\ln \\frac{q}{1-q}\\right]\\right\\}=1,\\ ] ] as expected .",
    "otherwise , the maximum is attained at the boundary of the allowed range of @xmath177 , and we get @xmath260 = \\frac{h}{\\theta}\\tanh(\\beta_{pg}(h)\\cdot h)- b\\delta_{gv}\\left(\\frac{1}{\\theta}h\\left(\\frac{1+\\tanh(\\beta_{pg}(h)\\cdot h}{2}\\right)\\right)\\ ] ] and so , the correct decoding exponent is @xmath261-\\theta\\ln[2(1-p)]+\\nonumber\\\\ & & b\\theta\\delta_{gv } \\left(\\frac{1}{\\theta}h\\left(\\frac{1 + \\tanh(\\beta_{pg}(h)\\cdot h}{2}\\right)\\right)- h\\tanh(\\beta_{pg}(h)\\cdot h)\\nonumber\\\\ & = & f_g(h)-\\theta\\ln 2.\\end{aligned}\\ ] ] thus , we obtained a very simple relationship between the correct decoding exponent and the glassy free energy .",
    "the ferromagnetic  glassy phase transition is exactly the transition from @xmath262 to @xmath263 .",
    "the dominant magnetization of the correct decoding event is then @xmath242 , i.e. , the dominant ( rare ) event of correct decoding is when the source vector @xmath29 has the ( non  typical ) magentization @xmath264 .",
    "if the condition of reliable communication does not hold , i.e. , @xmath265 , then the word map decoder ( @xmath40 ) works in the glassy regime , but the symbol map decoder ( @xmath39 ) works in the paramagnetic regime .",
    "the computation of @xmath266 for the word map decoder is carried out also in the glassy regime .",
    "we begin by using gallager s techniques ( see ( * ? ? ?",
    "* problem 5.16 , pp .",
    "534535 ) ) : the probability of error for a given code and the word map decoder is given by @xmath267 now , it is easy to see that whenever an error occurs @xmath268^\\beta\\ge 1.\\ ] ] for every @xmath269 . thus , @xmath270^\\beta\\right)^\\rho\\ ] ] for every @xmath271 . substituting the right  hand side into the expression of @xmath272 ,",
    "we get the following upper bound : @xmath273^\\beta\\right)^\\rho~~~~~\\beta\\ge 0,~\\rho\\ge 0.\\ ] ] thus , the average error probability over the ensemble of codes is bounded by @xmath274^\\beta\\right)^\\rho\\right\\}.\\ ] ] in the binary symmetric case considered here , the first expectation is given by : @xmath275^n\\nonumber\\\\ & = & e^{-n\\gamma(1-\\rho\\beta)}\\end{aligned}\\ ] ] where @xmath276 $ ] .",
    "the second expectation is handled as follows . using the above derived relation : @xmath277^\\beta= [ q(1-q)]^{n\\beta/2}(1-p)^{n\\beta}\\sum_m\\zeta(\\beta , m ) e^{\\beta nmh},\\ ] ] we get @xmath278^\\beta\\right)^\\rho&= & \\left([q(1-q)]^{n\\beta/2}(1-p)^{n\\beta}\\sum_m\\zeta(\\beta , m)e^{\\beta nmh}\\right)^\\rho\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & [ q(1-q)]^{n\\beta\\rho/2}(1-p)^{n\\beta\\rho}\\sum_m\\zeta^\\rho(\\beta , m)e^{\\beta\\rho nmh}\\end{aligned}\\ ] ] and so , assuming @xmath279 $ ] , and using jensen s inequality @xmath280^\\beta\\right)^\\rho\\right\\}\\nonumber\\\\ & \\le&[q(1-q)]^{n\\beta\\rho/2}(1-p)^{n\\beta\\rho } \\sum_m[{\\mbox{\\boldmath $ e$}}\\{\\zeta(\\beta , m)\\}]^\\rho e^{\\beta\\rho mhn}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & [ q(1-q)]^{n\\beta\\rho/2}(1-p)^{n\\beta\\rho } \\sum_m\\sum_\\delta\\exp\\left\\{n\\rho\\left [ \\frac{1}{\\theta}h\\left(\\frac{1+m}{2}\\right)+h(\\delta)-\\ln 2- \\beta b\\delta\\right]\\right\\}\\cdot e^{\\beta\\rho mhn}\\nonumber\\\\ & { \\stackrel{\\cdot } { = } } & [ q(1-q)]^{n\\beta\\rho/2}(1-p)^{n\\beta\\rho}\\times\\nonumber\\\\ & & \\exp\\left\\{n\\rho\\left [ \\frac{1}{\\theta}h\\left(\\frac{1+\\tanh(\\beta h)}{2}\\right)+h(p_\\beta)-\\ln 2-\\beta bp_\\beta + \\frac{\\beta h}{\\theta}\\tanh(\\beta h)\\right]\\right\\}\\end{aligned}\\ ] ] we see that the magnetization that dominates the gallager bound is the paramagnetic magnetization . by plugging this expression back into the bound on @xmath281",
    ", we get the error exponent : @xmath282+\\theta[\\gamma(1-\\rho\\beta)-\\ln 2 ] -\\frac{\\beta\\rho}{2}\\ln[q(1-q)]-\\beta\\rho\\theta\\ln(1-p)-\\nonumber\\\\ & & \\rho\\left[h\\left(\\frac{1+\\tanh(\\beta h)}{2}\\right)+\\theta[h(p_\\beta)-\\ln 2- \\beta bp_\\beta ] + \\beta h\\tanh(\\beta h)\\right]\\nonumber\\\\ & = & -\\ln [ q^{1-\\rho\\beta}+(1-q)^{1-\\rho\\beta}]+ \\theta[\\gamma(1-\\rho\\beta)-\\ln 2]+\\rho\\beta f_p(\\beta , h)\\nonumber\\\\ & = & -\\ln \\{[p^{1-\\rho\\beta}+(1-p)^{1-\\rho\\beta}]^\\theta\\cdot[q^{1-\\rho\\beta}+(1-q)^{1-\\rho\\beta}]\\ } + \\rho\\beta f_p(\\beta , h)\\end{aligned}\\ ] ] here , unlike in the computation of the correct decoding exponent , there is a mismatch between the phase in the @xmath283 plane at which the decoder operatively works , and the phase at which @xmath281 is analyzed : while the former is ferromagnetic , the latter is paramagnetic regardless of the temperature .",
    "t.  c.  dorlas and j.  r.  wedagedera , `` phase diagram of the random energy model with higher order ferromagnetic term and error correcting codes due to sourlas , '' _ phys .",
    "_ , vol .",
    "83 , no .",
    "21 , pp .  44414444 ,",
    "november 1999 .",
    "a.  e.  allakhverdyan and d.  b.  saaskyan , `` finite  volume corrections to the magnetization in the spin  glass phase of the derrida model , '' _ theoretical and mathematical physics _ ,",
    "109 , no .  3 , pp .",
    "15741577 , 1996 .",
    "n.  merhav , `` relations between random coding exponents and the statistical physics of random codes , '' submitted to",
    "_ ieee trans .  inform .",
    "theory _ , august 2007 .",
    "available on  line at : [ http://www.ee.technion.ac.il/people/merhav/papers/p117.pdf ] .",
    "n.  merhav , `` error exponents of erasure / list decoding revisited via moments of distance enumerators , '' submitted to _",
    "ieee trans .  inform .  theory _ ,",
    "november 2007 . also , available on  line at : [ http://www.ee.technion.ac.il/people/merhav/papers/p119.pdf ] ."
  ],
  "abstract_text": [
    "<S> we demonstrate that there is an intimate relationship between the magnetic properties of derrida s random energy model ( rem ) of spin glasses and the problem of joint source  channel coding in information theory . </S>",
    "<S> in particular , typical patterns of erroneously decoded messages in the coding problem have `` magnetization '' properties that are analogous to those of the rem in certain phases , where the non  uniformity of the distribution of the source in the coding problem , plays the role of an external magnetic field applied to the rem . </S>",
    "<S> we also relate the ensemble performance ( random coding exponents ) of joint source  channel codes to the free energy of the rem in its different phases . </S>",
    "<S> + _ keywords _ : spin glasses , rem , phase transitions , magnetization , information theory , joint source  channel codes .    </S>",
    "<S> department of electrical engineering + technion - israel institute of technology + haifa 32000 , israel + </S>"
  ]
}