{
  "article_text": [
    "the smoothed particle hydrodynamics method ( sph ) is widely used to calculate three dimensional hydrodynamics with lagrange scheme ( lucy 1977 ; gingold & monaghan 1977 ) .",
    "it has been applied to many astrophysical problems .",
    "because of its lagrangian nature , it is suitable to the problem which has large density contrasts , e.g. the formation of galaxies(evrard 1988 ; hernquist & katz 1989 ; umemura 1993 ; steinmetz & mueller 1994 ) or a cloud - cloud collision ( lattanzio et al .",
    "1985 ; habe & ohta 1992 ) . in these calculation , in order to treat star particles and/or dark matter with gas particle , we have to solve sph with collision - less particles ( n - body system ) .",
    "various codes have been developed to combine sph and n - body system . in these codes ,",
    "gravitational forces are calculated in many different ways such as direct summations , particle - particle / particle - mesh methods ( evrard 1988 ) , tree methods ( hernquist & katz 1989 ; benz et al .",
    "1990 ) , and the method to use the special purpose computer grape ( umemura et al . 1993 ; steinmetz 1996 ) .",
    "grape ( gravity pipe ) is a special purpose computer for efficiently calculating gravitational force and potential ( sugimoto et al .",
    "we need a host computer , which is connected to grape board , to control it and conduct other calculations ( e.g. time integration ) .",
    "the use of grape for sph simulations ( hereafter grape - sph ) has many advantages , since grape can calculate not only gravitational force and potential but also construct lists of neighbor particles in a short time . in sph simulations we need the lists of neighbor particles to calculate hydrodynamical quantities .",
    "searching neighbor particles with grape is much more efficient than a direct search on the host computer . with grape - sph , therefore , we need to calculate only pure hydrodynamical part of sph on the host computer .",
    "although the cost of n - body part of sph can be significantly reduced by using grape , the speed of the hydrodynamical part of the code is limited by the speed of the host computer .",
    "the speed of workstation ( ws ) is being rapidly improved . to take full advantage of grape for sph calculations",
    ", we have to use the state - of - the - art ws as a host .",
    "if the host ws of grape is much slower than a fast ws available now , the total performance of grape - sph is lower than that of the sph simulation on the fast ws without grape . of course",
    ", by developing new interface to new ws , we could solve this problem .",
    "this approach , however , requires many human time .",
    "moreover , during the development of the new interface , an ever newer ws with different interface might become available .    here",
    "we take a different , novel approach to solve this problem .",
    "we use parallel virtual machine ( pvm ; geist et al .",
    "1994 ) , which is one of the most popular message - passing systems in parallel computing , to connect the ws which is directly connected to a grape board with another fast machine .",
    "the sph part is performed on the fast machine .",
    "in fact , all simulation code is run on the fast machine and the ws connected to grape board serves essentially as the intelligent communication interface .",
    "figure [ fig1 ] and [ fig2 ] shows the present grape system and our new approach , respectively . in our new approach ,",
    "the combination of the ws and grape ( figure [ fig1 ] ) behave as the `` remote grape '' system which is connected directly to the local area network ( lan ) .",
    "thus , any computer on the lan can be used as the host ( remote - host ) of the grape .",
    "we construct the library named remote - grape . in this paper",
    "we present the implementation of remote - grape and discuss its performance . in section 2 we summarize the grape system and the bottleneck of grape - sph . in section 3",
    "we describe how the remote - grape is implemented , analyze its performance , and compare it with the original grape system and other scheme . in section 4",
    "we show the performance of our grape - sph code and discuss possible improvement of our code . in section 5 conclusions are summarized .",
    "= 0.8    = 0.8    = 0.8",
    "the grape-3af system ( okumura et al .",
    "1993 ) has 8 processors on one board , and the grape board is connected to a host computer via vme ( versa module europe ) bus .",
    "the processor chip is designed to calculate gravitational force with a plummer softening , i.e. , @xmath1 during the force calculation , grape-3af can construct neighbor lists simultaneously ( fukushige et al . 1991 ; okumura et al . 1993 ) .",
    "the size of the buffer for neighbor lists is 1024@xmath24byte , which limits the maximum number of neighbors per board .",
    "the schematic diagram of the grape system is shown in figure [ fig1 ] .",
    "we present general performance of grape system in figures [ grape1]a and [ grape1]b .",
    "these figures show the relation between the cpu time and number of particles , @xmath3 , for different number of grape boards . in our site",
    ", the grape boards are connected to sun sparc classic ( hereafter classic ) via aval data sva-100 vme interface .",
    "figure [ grape1]a ( left ) shows the case that calculates only gravity ( hereafter case g ) , while figure [ grape1]b ( right ) shows the case that calculates both gravity and neighbor lists ( case gn ) .",
    "the particles used in these experiments are randomly distributed within the calculation region ( see figure [ data]a ) , and the size of the region changes with @xmath3 to make the number density almost constant .",
    "the average number of neighbor particles is almost constant ( @xmath0 40 ) for any @xmath3 .    in case g",
    ", the performance is dramatically improved by increasing the number of boards . in case gn , however , the use of larger number of boards does not significantly improve the performance .",
    "this is because the speed of the vme bus is too slow to read neighbor lists for large @xmath3 with many boards , and the calculations required to construct neighbor lists on the host computer is rather costly work for our machine .",
    "we compare the performances of grape system with different host computers .",
    "we use classic ( cf . spec int92/fp92 : 26.4/21.0 , which represent integer and floating performance of the machine ) and sun sparc station 10/40 ( hereafter ss-10 with int92/fp92 : 50.2/60.2 ) with the same interface and three grape boards .",
    "figure [ grape2]a and [ grape2]b show cases g and gn , respectively , where the solid and dotted lines represent the cases of classic and ss-10 , respectively . for case",
    "gn , the computation on host computer ( in short , construction of neighbor lists ) is longer than case g. thus the speed of the host computer apparently affect the performance .    in sph , physical quantities at one position",
    "are calculated by smoothly averaging over neighbor particles .",
    "thus , sph simulations are essentially equivalent to n - body simulation with complex , short range `` force '' . in grape - sph",
    "we use grape for calculating gravitational force and searching neighbor particles .",
    "steinmetz ( 1996 ) summarized the performance of grape - sph in his table 1 for one grape-3af board and sun sparc station 10 as a host computer .",
    "he showed that about 80 % ( @xmath4 ) of computing time is spent on the hydrodynamical and miscellaneous calculations , and only @xmath0 20 % is spent on the grape part .",
    "this implies that a higher performance is obtained if we can use a faster ws to calculate the sph part .",
    "= 0.8    [ cols=\"^,^,^,^,^\",options=\"header \" , ]      + the elapse time for one step is shown .",
    "@xmath5 .",
    "another possible method is to use the neighbor lists of the previous step , which is constructed by remote - grape , as possible neighbor lists ( case ps - rgn ) . in this case",
    ", we have to adopt larger neighbor radius than case s - rgn .",
    "figure [ sph]b indicates that the @xmath6 part of the calculation is dominant so that the parallel methods has less advantage for our configuration . since the @xmath6 part and the other parts of the calculation obey different @xmath3 dependence , it is worth examine if there exits an optimal number of particles that leads to the optimal performance for a given configuration in case ps - rgn . for the sph and misc part , we estimate their @xmath3 dependence in our sph code from figure [ sph]a .",
    "we get @xmath7 sec .",
    "therefore , @xmath6 always exceeds @xmath8 for any @xmath3 .    however , figure 9 and table [ table_sph1 ] are the results for pure sph calculations . because we are interested in the formation of galaxy and globular cluster , we include such physical processes in our sph code as cooling , heating , and star formation ( mori et al .",
    "if we include these physical processes , @xmath9 should be much longer , so that the parallel method has more advantage .",
    "for example , in the sph code calculation with non - equilibrium h@xmath10 cooling , it takes @xmath11 sec to solve rate equations for 10 species on our machine . in many cases ,",
    "the cooling time scale is shorter than the dynamical time scale , so that the rate equations need to be solved many times in one dynamical step , which leads to @xmath12 sec in our machine . for large @xmath3",
    ", @xmath9 is dominant than @xmath6 .",
    "we describe the remote - grape library and analyze its performance .",
    "it allows us to use grape-3a with the computer which is not directly connected to grape .",
    "thus , we can use the state - of - the - art computer as a host computer .      1 .",
    "if we calculate only gravity forces , the use of larger number of boards leads to higher performance .",
    "2 .   in calculating both gravity and neighbor lists , however",
    ", the computation required on host ws is larger than the former case , so that the increase of number of the boards is less advantageous .",
    "3 .   the speed of the host ws dramatically change the performance of grape system .      1 .",
    "if we calculate only gravity forces , the time required by remote - grape is almost the same as that of original grape .",
    "2 .   in calculating both gravity and neighbor lists by remote - grape , the time required to transferring data does not occupy a large fraction of total time but @xmath0 20 - 30 % for our configuration .",
    "3 .   compared with the tree method",
    ", the advantage of remote - grape is that its performance does not depend on the state of clustering , while the required time of the tree method is longer for higher degree of clustering .",
    "4 .   in the actual application , the performance of our sph code using remote - grape is 3 - 4 times faster than the sph code using original grape on the slave machine .",
    "5 .   we can get further high ( 10 times than the sph on slave machine ) performance by using the parallel method with remote - grape to calculating only gravity .",
    "we have completed the first version of remote - grape , which significantly improves the performance of grape - sph .",
    "however , there are several ways to improve further the performance of our library and grape - sph with remote - grape .    1 .",
    "if we use a larger number of grape boards with a fast bus and a fast slave machine , the use of grape in constructing neighbor lists is more advantageous than the use of tree structure .",
    "2 .   we can develop more efficient library . in the grape system",
    "all values are converted to the fixed floating point format in the original grape library and then sent to the grape board .",
    "the conversion from the floating point format to the fixed format may be done by the master side . in a relatively slow slave computer",
    ", this will significantly improve the performance .",
    "we would like to thank j. makino and t. suzuki for reading the manuscript and giving useful comments .",
    "we also thank t. miwa for providing us the result with ss-10 .",
    "those who would like to use our library can contact authors ( nakasato@astron.s.u-tokyo.ac.jp ) .",
    "this work has been supported in part by the grant - in - aid for scientific research ( 05242102 , 06233101 ) and coe research ( 07ce200 ) of ministry of education , science , and culture of japan ."
  ],
  "abstract_text": [
    "<S> we have developed remote - grape , a subroutine library to use the special purpose computer grape-3a . </S>",
    "<S> the grape-3a can efficiently calculate gravitational force between particles , and construct neighbor lists . </S>",
    "<S> all other calculations are performed on the host workstation ( ws ) which is directly connected to grape . </S>",
    "<S> the use of grape for smoothed particle hydrodynamics ( grape - sph ) can in principle greatly speed up the calculations on ws . </S>",
    "<S> however the current bottleneck of grape - sph is that its performance is limited by the speed of the host ws . to solve this problem , </S>",
    "<S> we implement remote - grape ; it allows us to run applications which use grape-3a hardware on the significantly faster computers than the physical host ws . </S>",
    "<S> thus , we can take advantage of the fast computers even though they can not physically be connected to grape . </S>",
    "<S> the remote - grape system is implemented on the parallel virtual machine ( pvm ) . </S>",
    "<S> the detail of implementation is described .    </S>",
    "<S> we analyze the performance of remote - grape and obtain the following results . </S>",
    "<S> 1 ) when remote - grape is used to calculate only gravitational forces , the overhead due to the network decrease according to the number of particles ( 20 % - 40 % of total time ) . </S>",
    "<S> 2 ) to calculate gravity and neighbor lists , the overhead due to the network does not occupy a large fraction of total time but only @xmath0 20 - 30 % , because the computation required on the slave machine ( see text ) is very large due to the property of grape system . </S>",
    "<S> 3 ) we also compare the performance of remote - grape with the tree method . </S>",
    "<S> the tree method requires more time for higher degree of clustering , while the required time for remote - grape does not much depend on it .    </S>",
    "<S> we then analyze the performance of grape - sph with remote - grape . </S>",
    "<S> its performance is about 4 times faster than grape - sph with usual grape for our configuration . using remote - grape </S>",
    "<S> , we can calculate the grape part and other parts in parallel . </S>",
    "<S> this parallel method leads to further speed up of our sph code . </S>",
    "<S> we estimate how the performance of remote - grape depends on the configuration . </S>",
    "<S> we also show that the performance of remote - grape can be further improved . </S>"
  ]
}