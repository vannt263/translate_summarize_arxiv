{
  "article_text": [
    "likelihood functions are a very flexible and powerful approach for estimation of model parameters .",
    "the asymptotic properties of likelihood functions , as shown by wilks ( 1938 ) , allow us to perform inferential tests even when the distribution of the statistics describing the parameters is unknown .",
    "the classical likelihood function ( which we will refer to as the fisher likelihood ) requires exact specification of the probability function @xmath0 . in most applications the true data distribution is unknown ,",
    "so an assumption must be made . in some cases where the data distribution can be described ,",
    "the likelihood function is still impossible to express mathematically due to the complexity of the probability density function .",
    "there are many alternatives to the fisher likelihood ; here we focus on two such methods : the composite likelihood and the empirical likelihood .",
    "composite likelihood ( lindsay , 1988 ) appropriately combines conditional and marginal densities in order to construct an approximation to the fisher likelihood . to demonstrate a basic example of the approach let @xmath1 and @xmath2 be random vectors in @xmath3 from some joint distribution @xmath4 . denote the density function as @xmath5 and the likelihood function as @xmath6 .",
    "we can construct the following composite likelihood objects : @xmath7 each composite likelihood is the product of proper likelihoods derived from the data .",
    "@xmath8 rewrites the true likelihood as the product of a conditional and marginal .",
    "@xmath9 and @xmath10 , however , are only equivalent to the true likelihood if @xmath1 and @xmath2 are independent .",
    "the theoretical properties and justifications for specific forms are explored in lindsay ( 1988 ) .",
    "besag ( 1974 ) introduces a multidimensional form of @xmath10 , which he calls the pseudolikelihood , to create an approximation to a likelihood for spatial lattice data .",
    "@xmath9 is commonly referred to as the independence likelihood and only permits inference on marginal parameters .",
    "the theoretical properties of independence likelihoods have been explored by cox & reid ( 2004 ) and varin ( 2008 ) .",
    "a general overview on composite likelihoods can be found in varin et .",
    "2011 ) .",
    "empirical likelihood is a nonparametric approach for the estimation of the likelihood function from the data .",
    "the general form and first - order asymptotics of the empirical likelihood are explored by qin & lawless ( 1994 ) . under mild regularity conditions empirical likelihoods",
    "inherit the asymptotic properties of the fisher likelihood ( wilks , 1938 ; qin & lawless , 1994 ) . empirical likelihood generally permits a bartlett correction ( diciccio et .",
    "al . , 1991 ) ; an exception is explored in lazar & mykland ( 1999 ) .",
    "empirical likelihood has been applied to univariate data ( owen , 1988 ) , bivariate data ( owen , 1990 ) , generalized linear models ( kolaczyk , 1994 ) and many other settings .",
    "the main drawback of empirical likelihood is computational ; in all but the simplest cases ( such as inference on a single mean parameter ) , empirical likelihood does not result in a closed form solution , hence computation of estimators and confidence intervals is non - trivial and time consuming .",
    "desirable features of any statistical method are robustness , flexibility and computational simplicity .",
    "a defining property of any likelihood method is its wide applicability .",
    "our proposed method , which combines the composite and empirical approaches , does not require any distributional assumptions like the empirical likelihood and maintains the flexibility of construction seen with composite likelihoods .",
    "we call this construct _ composite empirical likelihood _ since we are building a composite likelihood using empirical likelihoods .",
    "we show that under mild conditions the asymptotic distribution of this construction inherits the asymptotic properties seen in both empirical likelihood and parametric likelihood .",
    "these results place the composite empirical likelihood as a general case of many existing likelihood methods .",
    "before defining the composite empirical likelihood , we first formally define composite likelihood and empirical likelihood .",
    "let @xmath11 be a @xmath12-variate sample from some distribution @xmath13 .",
    "we define conditional or marginal events for which the likelihood @xmath14 can be written for @xmath15 , with the requirement that @xmath14 for all @xmath16 must be a fisher likelihood .",
    "the composite likelihood is @xmath17 where @xmath18 is a predetermined weight .",
    "if @xmath18 is equal for all @xmath16 the weights can be ignored for purposes of maximization . the fisher likelihood can also be viewed as a specific case of @xmath19 .",
    "if the true probability function is @xmath20 , we then have @xmath21 .",
    "let @xmath11 be @xmath12-variate independent identically distributed observations from some distribution @xmath13 .",
    "the empirical likelihood function is @xmath22    the empirical likelihood function is maximized by the empirical distribution function @xmath23 so the empirical likelihood ratio function @xmath24 can be written as @xmath25 suppose now we are interested in the estimation of a @xmath26 parameter @xmath27 .",
    "we add additional constraints in the form of @xmath28 unbiased estimating equations @xmath29 for @xmath30 .",
    "the profile empirical likelihood ratio function is @xmath31 provided that @xmath27 is inside the convex hull of the points @xmath11 a unique value of equation [ eqn : pelrf ] exists ( owen , 1988 ) . by definition @xmath32 for all @xmath27 not inside the convex hull .    the estimators using both composite likelihood and empirical likelihood are the parameter values which maximize the respective likelihood functions , and both estimators are asymptotically normal ( varin et .",
    "al . , 2011 ; qin & lawless , 1994 ) .",
    "furthermore the asymptotic distribution of the test statistic for hypothesis testing is @xmath33 using empirical likelihood and a weighted @xmath33 using composite likelihood ( see qin & lawless , 1994 for empirical likelihood and varin et .",
    "2011 for composite likelihood ) .    to define the composite empirical likelihood",
    "let @xmath34 be from some distribution @xmath13 .",
    "define @xmath35 as a ( univariate or multivariate ) subset of @xmath36 for @xmath37 .",
    "we will assume that all @xmath35 come from some distribution @xmath38 .",
    "for simplicity we will assume the sample sizes of each likelihood component are equal so @xmath39 for all @xmath16 .",
    "define @xmath40 for @xmath37 as the estimating equations for each subset and define the parameters as @xmath41 having dimension @xmath42 .",
    "the dimension of the parameter @xmath27 is @xmath43 , and finally assume @xmath44 for all @xmath16 , where @xmath45 is the dimension of @xmath40 . for each subset",
    "@xmath16 the component empirical likelihood is @xmath46 and the composite empirical likelihood function is @xmath47 note that the construction of the composite empirical likelihood consists of proper empirical likelihood components multiplied together .",
    "the composite likelihood allows for the addition of a weight on each component , but we do not explore that option here .    let @xmath48 $ ] where @xmath49 for @xmath50 are each univariate all with a sample size of @xmath51 .",
    "if we assume that @xmath52 and @xmath53 are correlated , @xmath54 and @xmath55 we build a composite empirical likelihood where the first likelihood component consists of @xmath52 and @xmath53 , the second likelihood component consists of @xmath56 and the third component consists of @xmath57 ( @xmath58 ) .",
    "the estimating equations are @xmath59^\\t\\\\ g_2(z_{i3},\\mu_1,\\mu_2 ) & = z_{i,3 } - \\mu_1\\\\ g_3(z_{i4},\\mu_1,\\mu_2 ) & = z_{i,4 } - \\mu_2\\end{aligned}\\ ] ] so the composite empirical likelihood for this example is @xmath60^\\t = [ \\mu_1,\\mu_2]^\\t ,   u_{i,1 } \\geq 0 , \\sum_{i=1}^n u_{i,1 } = 1          \\right . \\bigg)\\\\      & \\quad \\times \\bigg(\\sup_{\\textbf{u}_2 } \\prod_{i=1}^n u_{i,2 } \\left| \\sum_{i=1}^n u_{i,2}z_{i,3 } = \\mu_1 ,   u_{i,2 } \\geq 0 , \\sum_{i=1}^n u_{i,2 } = 1          \\right . \\bigg)\\\\      & \\quad \\times \\bigg(\\sup_{\\textbf{u}_3 } \\prod_{i=1}^n u_{i,3 } \\left| \\sum_{i=1}^n u_{i,3}z_{i,4 } = \\mu_2 ,   u_{i,3 } \\geq 0 , \\sum_{i=1}^n u_{i,3 } = 1          \\right . \\bigg).\\\\\\end{aligned}\\ ] ]    following the derivation in owen ( 1988 , 1990 ) and qin & lawless ( 1994 ) we express @xmath61 from equation [ eqn : lce ] separately for each @xmath16 in terms of the @xmath62 lagrange multipliers @xmath63 as @xmath64 with the following restrictions @xmath65 where the values of @xmath63 are determined based on the value of @xmath27 .",
    "furthermore @xmath63 for all @xmath16 and @xmath27 must satisfy @xmath66 since @xmath67 .",
    "the @xmath63 are differentiable functions of @xmath27 ( qin & lawless , 1994 ) .    the maximum composite empirical likelihood estimator is the value of @xmath27 which maximizes @xmath68 . we denote this by @xmath69 .",
    "our derivations and results follow from qin & lawless ( 1994 ) .",
    "we derive the asymptotic distribution of the maximum composite empirical likelihood estimator along with the asymptotic distribution of the log composite empirical likelihood ratio .",
    "first define @xmath70 then the negative log composite empirical likelihood function is @xmath71    [ as:1 ] let @xmath72 be the true value of @xmath27 . then for all @xmath16    1 .",
    "@xmath73 is positive definite .",
    "@xmath74 is continuous in a neighborhood of the true value @xmath72 .",
    "3 .   @xmath75 and @xmath76 are both bounded by some integrable function in the same neighborhood of @xmath72 .",
    "the rank of @xmath77 is @xmath78 .    under assumption 1",
    "@xmath79 attains its minimum value at some point @xmath69 in the interior of the ball @xmath80 with probability 1 as @xmath81 .    furthermore @xmath69 and @xmath82 for all @xmath16 satisfy @xmath83    where @xmath84    and @xmath85    where @xmath86    the proof of lemma ( 1 ) follows directly from qin & lawless ( 1994 , lemma 1 ) .",
    "[ as:2 ] let @xmath72 be the true value of @xmath27 . then for all @xmath16    1 .",
    "the second derivative @xmath87 is continuous in @xmath27 in a neighborhood of the true value @xmath72 .",
    "@xmath88 can be bounded by some integrable function in the neighborhood of @xmath72 .    for simplicity we will denote @xmath89 as @xmath40 .    under assumptions 1 and 2",
    "@xmath90 where @xmath91 v & = \\sum_{j=1}^j \\sum_{k=1}^j    \\edtj^\\t \\ejji e(g_jg_k^\\t ) \\ekki \\edtk.\\end{aligned}\\ ] ]    the covariance of @xmath69 shows similarities to the estimators of both empirical and composite likelihood .",
    "the matrix @xmath92 is identical to the covariance matrix shown in qin & lawless ( 1994 ) for empirical likelihood , while @xmath93 operates as a nonparametric equivalent of the variability matrix seen with composite likelihood ( varin et .",
    "al . , 2011 ) . also note that if @xmath94 for all @xmath95 then @xmath96 and the covariance matrix reduces to @xmath97 .",
    "analogous to other likelihood functions , we develop a composite empirical likelihood ratio statistic in order to find efficient estimators , and by extension confidence intervals and test statistics .",
    "the remaining results hold under assumptions 1 and 2 .",
    "let @xmath98^\\t$ ] be a @xmath99 dimensional vector where @xmath100 is a @xmath101 vector and @xmath102 is a @xmath103 vector .",
    "the profile composite empirical likelihood ratio test statistic for @xmath104 is @xmath105 where @xmath106 minimizes @xmath107 with respect to @xmath108 . under @xmath109 @xmath110 as @xmath81 .",
    "@xmath111 is a weighted @xmath33 random variable where @xmath112 for @xmath113 is the set of all non zero eigenvalues of    @xmath114              \\left[\\begin{array}{ccc }                  e(g_1g_1^\\t ) & \\cdots & e(g_1g_j^\\t)\\\\                  \\vdots & \\ddots & \\vdots\\\\                  e(g_jg_1^\\t ) & \\cdots & e(g_jg_j^\\t)\\end{array }              \\right]\\end{aligned}\\ ] ]    where @xmath115 w_{\\boldsymbol\\nu } & = \\sum_{j=1}^j   e   \\left ( \\frac{\\partial g_j}{\\partial{\\boldsymbol\\nu } } \\right)^\\t \\left\\{e(g_jg_j^\\t)\\right\\}^{-1 } e\\left(\\frac{\\partial g_j}{\\partial{\\boldsymbol\\nu } } \\right)\\end{aligned}\\ ] ] and @xmath116 is as defined in theorem ( 1 ) .",
    "the result of theorem ( 2 ) highlights that the composite empirical likelihood function ( like the composite likelihood ) is not inherently asymptotically equivalent to the true likelihood . as a consequence",
    "the asymptotic distribution of @xmath117 is not the @xmath33 seen in ordinary empirical likelihood ( owen , 1988 ; owen , 1990 ; qin & lawless , 1994 ) but rather the weighted @xmath33 derived from composite likelihood ( varin et .",
    "al . , 2011 ) .",
    "if we assume ( or know ) that @xmath118 for all @xmath119 then @xmath120 and @xmath121 are asymptotically independent for all @xmath95 .",
    "the following corollary shows that given this additional assumption the asymptotic distribution of the test statistic using the composite empirical likelihood reduces to the standard @xmath33 .",
    "in addition to assumptions 1 and 2 let @xmath122 for all @xmath95 .",
    "then @xmath123 as @xmath81 .",
    "@xmath117 is defined in theorem ( 2 ) .",
    "we give proofs of theorem ( 1 ) , ( 2 ) and corollary ( 1 ) in the appendix .",
    "the next two corollaries give the asymptotic distribution of the test statistic when there are no nuisance parameters .",
    "the composite empirical likelihood ratio statistic for testing @xmath124 is @xmath125 where @xmath126 is given by equation [ eqn : llce ] . under @xmath109 @xmath110 as @xmath81 .",
    "@xmath111 is a weighted @xmath33 random variable where @xmath112 for @xmath113 is the set of all non zero eigenvalues of @xmath127              \\left[\\begin{array}{ccc }                  e(g_1g_1^\\t ) & \\cdots & e(g_1g_j^\\t)\\\\                  \\vdots & \\ddots & \\vdots\\\\                  e(g_jg_1^\\t ) & \\cdots & e(g_jg_j^\\t)\\end{array }              \\right]\\end{aligned}\\ ] ] where @xmath128 and @xmath116 is as defined in theorem ( 1 ) .    assume that @xmath118 for all @xmath95 .",
    "then @xmath129 as @xmath81 when @xmath109 is true .",
    "@xmath117 is as defined in corollary ( 2 ) .",
    "we do not provide proofs for corollaries ( 2 ) and ( 3 ) as they follow from the proofs for theorem ( 2 ) and corollary ( 1 ) by noting that @xmath130 .",
    "the distributions of the test statistics from theorem ( 2 ) and corollary ( 2 ) require that we know the @xmath131 and @xmath132 matrices in order to determine the weighted @xmath33 .",
    "since these are generally unknown , we can estimate the matrices for a given value of @xmath133 using the sample data @xmath134 . for confidence intervals",
    "we use @xmath69 as the value of @xmath135 . for hypothesis testing",
    "we can use the null hypothesis values @xmath108 and @xmath136 , or for computational efficiency replace @xmath136 with @xmath137 since the maximum composite empirical likelihood is a consistent estimator of @xmath138 ( barndorff - nielsen & cox , 1994 , page 91 ) .",
    "the covariance between each @xmath40 and @xmath139 is estimated using @xmath140    each @xmath141th block of @xmath131 is @xmath142 and @xmath143 \\widehat{w}_{\\boldsymbol\\nu}(\\boldsymbol{\\tilde\\theta } )      & =   \\sum_{j=1}^j \\bigg\\{\\left(\\frac{1}{n } \\sum_{i=1}^{n}\\frac{\\partial g_j(\\mathbf{z}_{ij},\\boldsymbol{\\tilde\\theta})}{\\partial{\\boldsymbol\\nu}}\\right)^\\t                                                  \\left(\\frac{1}{n } \\sum_{i=1}^{n}g_j^\\t(\\mathbf{z}_{ij},\\boldsymbol{\\tilde\\theta})g_j(\\mathbf{z}_{ij } , \\boldsymbol{\\tilde\\theta})\\right)^{-1}\\\\                                                  & \\quad                                                  \\left(\\frac{1}{n } \\sum_{i=1}^{n}\\frac{\\partial g_j(\\mathbf{z}_{ij},\\boldsymbol{\\tilde\\theta})}{\\partial{\\boldsymbol\\nu}}\\right )                                            \\bigg\\}.%%%%\\\\[3\\jot]\\end{aligned}\\ ] ]",
    "in order to examine the performance of the proposed method given several different data distributions we generate data from bivariate normal , bivariate chi square , and bivariate uniform ( @xmath144 ) distributions .",
    "we use 500 replicates of varying sample size @xmath51 .",
    "the bivariate normal random variables are generated with parameter values of @xmath145 and @xmath146 .",
    "the bivariate chi square random variables are generated with @xmath147 .",
    "the bivariate uniform random variables are generated with a lower bound of @xmath148 and an upper bound of @xmath149 .",
    "we vary the values of the correlation @xmath150 .",
    "all data distributions have an expected value of 1 , variance of 2 and correlation of @xmath150 .",
    "the mean is the parameter of interest and we are assuming that the mean is the same for both variables , so @xmath151 and @xmath152 .",
    "using the result from theorem ( 1 ) our estimator will have an asymptotic mean of 1 and an asymptotic variance of @xmath153 .",
    "we use the sample mean and sample variance to empirically estimate these values , and we show the percentage of false rejections out of the 500 simulations at @xmath154 for the two sided test of @xmath155 .",
    "table [ tab : t0 ] shows the asymptotic variances at specified sample sizes and correlation .",
    ".theoretical variance of @xmath156 based on sample size and correlation .",
    "[ cols=\"^,^,^,^,^ \" , ]     figure [ fig : f1 ] shows that as the sample size increases the computation times when @xmath157 also significantly increases , and there is an increase in the variability of the computation time . as we increase the number of likelihood components ( which allows us to take advantage of the parallel computing environment ) the decrease in computation times is substantial . in this example splitting the likelihood into two pieces with a sample size of 100 cuts the median computation time down from 3.42 seconds to 0.864 seconds , while using four likelihood components brings the median computation time down to 0.373 seconds .",
    "an immediate question that arises from the construction of the composite empirical likelihood is how many components should one use .",
    "one principle is that variables that are correlated will be combined into a single component while independent variables can be separate components ( as in example 1 ) .",
    "this idea can also be applied to create a pairwise composite empirical likelihood where each component consists of two variables , allowing for inference on correlation between pairs .",
    "if all components are independent then the test statistic is a standard @xmath33 eliminating the need to compute the weights .",
    "a critical aspect to note is that independence of the likelihood components is based on @xmath118 , so there are cases where the variables themselves may be correlated but the likelihood components are not .    the simplest application of composite empirical likelihood is separation of large sample univariate data . in this case",
    "the balance between the number of components and the number of observations in each component is a matter of user preference and computing resources . for minimizing run time and reducing the possibility of an optimization algorithm failing to converge we suggest ( based on our own experiences ) having at least as many components as parallel pools ( which determines how many components can be computed simultaneously ) provided there is a minimum of 10 observations per parameter for each likelihood component . also the approach shown in section 4.3 can be applied to any empirical likelihood . since one of the assumptions of the empirical likelihood is the data are @xmath158 the test statistic will still be @xmath33 since each likelihood piece will also be independent .",
    "this work was partially supported by a national institutes of health grant .",
    "we would like to thank albert vexler for helpful conversations .",
    "first by taylor expansion of @xmath159 around @xmath72 and 0 @xmath160 where @xmath161 . solving for @xmath162 yields @xmath163 for all @xmath16 .",
    "now by taylor expansion of @xmath164 around @xmath165 @xmath166      & = q_2({\\boldsymbol\\theta}_0,0,\\ldots,0 ) + \\frac{\\partial}{\\partial{\\boldsymbol\\theta}}q_2({\\boldsymbol\\theta}_0,0,\\ldots,0)(\\hat{{\\boldsymbol\\theta}}_{ce } - { \\boldsymbol\\theta}_0)\\\\      & \\quad + \\sum_{j=1}^j \\frac{\\partial}{\\partial \\textbf{t}_j^\\t } q_2({\\boldsymbol\\theta}_0,0,\\ldots,0 )   ( \\hat{t}_j - 0 ) + o_p(\\delta)\\end{aligned}\\ ] ] where @xmath167 .",
    "the derivatives of @xmath172 and @xmath173 with respect to @xmath27 and @xmath63 are @xmath174 so @xmath175 ( \\hat{{\\boldsymbol\\theta}}_{ce } - { \\boldsymbol\\theta}_0 ) & = w_{\\boldsymbol\\theta}^{-1 } \\sum_{j=1}^j\\bigg ( \\edtj^\\t\\ejji\\left\\ { -q_{1j}({\\boldsymbol\\theta}_0,0)\\right\\ } \\bigg)+ o_p(1)\\end{aligned}\\ ] ] where @xmath176 for all @xmath16 @xmath177 so the variance of @xmath178 is @xmath179 \\times\\\\ & \\left[\\begin{array}{ccc } e(g_1g_1^\\t ) & \\cdots & e(g_1g_j^\\t)\\\\ \\vdots & \\ddots & \\vdots\\\\ e(g_jg_1^\\t )   & \\cdots & e(g_jg_j^\\t ) \\end{array}\\right]\\times \\left[\\begin{array}{c } \\left\\{e(g_1g_1^\\t)\\right\\}^{-1}e\\left(\\frac{\\partial g_1}{\\partial{\\boldsymbol\\theta } } \\right ) w_{\\boldsymbol\\theta}^{-1}\\\\ \\vdots \\\\ \\left\\{e(g_jg_j^\\t)\\right\\}^{-1}e\\left(\\frac{\\partial g_j}{\\partial{\\boldsymbol\\theta } } \\right ) w_{\\boldsymbol\\theta}^{-1 } \\end{array}\\right]\\\\[4\\jot ] & = \\sum_{k=1}^j    \\sum_{j=1}^j \\bigg (    w_{\\boldsymbol\\theta}^{-1 } e\\left ( \\frac{\\partial g_j}{\\partial{\\boldsymbol\\theta } } \\right)^\\t\\left\\{e(g_jg_j^\\t)\\right\\}^{-1}e(g_jg_k^\\t ) \\left\\{e(g_kg_k^\\t)\\right\\}^{-1}e\\left(\\frac{\\partial g_k}{\\partial{\\boldsymbol\\theta } } \\right ) w_{\\boldsymbol\\theta}^{-1 } \\bigg ) \\\\[4\\jot ] & = w_{\\boldsymbol\\theta}^{-1 } v w_{\\boldsymbol\\theta}^{-1 } % \\left\\{\\sum_{j=1}^j    \\sum_{k=1}^j     e\\left ( \\frac{\\partial g_j}{\\partial{\\boldsymbol\\theta } } \\right)^\\t\\left\\{e(g_jg_j^\\t)\\right\\}^{-1}e(g_jg_k^\\t ) \\left\\{e(g_kg_k^\\t)\\right\\}^{-1}e\\left(\\frac{\\partial g_k}{\\partial{\\boldsymbol\\theta } } \\right)\\right\\}\\end{aligned}\\ ] ] which completes the proof .    following qin & lawless ( 1994 ) for a given value of @xmath27 @xmath180 so using the notation from lemma ( 1 ) @xmath181 using the derivatives shown in the proof of lemma ( 1 ) we have by taylor expansion of @xmath120 @xmath182 and @xmath183 now @xmath184 which can be written as @xmath185    \\times   a    \\times     \\left[\\begin{array}{c}\\sqrt{n}q_{11}({\\boldsymbol\\theta}_0,0 )",
    "\\\\   \\vdots \\\\",
    "\\sqrt{n}q_{1j}({\\boldsymbol\\theta}_0,0 ) \\end{array}\\right ] % + o_p(1)%\\end{aligned}\\ ] ] where @xmath131 is defined in theorem ( 2 ) .",
    "@xmath186 converges to a normal with mean 0 and covariance @xmath187 which completes the proof .",
    "@xmath117 can be written as @xmath188 @xmath189 is asymptotically standard multivariate normal and @xmath190 for all @xmath95 .",
    "so we need only show that @xmath131 is idempotent .",
    "we can rewrite a as @xmath191 where the @xmath141th entries of each matrix are @xmath192 and @xmath193 both @xmath194 and @xmath195 are idempotent , with ranks @xmath99 and @xmath196 respectively . to establish that @xmath117 is @xmath33 with @xmath196 degrees of freedom we only need to show that @xmath131 is non - negative definite ( see rao , 1973 , page 187 ) .",
    "we have @xmath197          \\left [          \\begin{array}{cc }          0 & 0 \\\\          0 & w_{\\boldsymbol\\nu}^{-1 }          \\end{array }          \\right ]          \\left [          \\begin{array}{c }          e\\left(\\frac{\\partial g_k}{\\partial{\\boldsymbol\\phi}}\\right)^\\t\\\\e\\left(\\frac{\\partial g_k}{\\partial{\\boldsymbol\\nu}}\\right)^\\t          \\end{array }          \\right]\\\\          & = \\ednj w_{\\boldsymbol\\nu}^{-1 } \\ednj^\\t\\\\          & \\propto \\left(a^{\\boldsymbol\\nu}\\right)_{jk}\\end{aligned}\\ ] ] so @xmath131 is non negative definite with rank @xmath198 ."
  ],
  "abstract_text": [
    "<S> the likelihood function plays a pivotal role in statistical inference ; it is adaptable to a wide range of models and the resultant estimators are known to have good properties . however , these results hinge on correct specification of the data generating mechanism . </S>",
    "<S> many modern problems involve extremely complicated distribution functions , which may be difficult  if not impossible  to express explicitly . </S>",
    "<S> this is a serious barrier to the likelihood approach , which requires not only the specification of a distribution , but the correct distribution . </S>",
    "<S> non - parametric methods are one way to avoid the problem of having to specify a particular data generating mechanism , but can be computationally intensive , reducing their accessibility for large data problems . we propose a new approach that combines multiple non - parametric likelihood - type components to build a data - driven approximation of the true function . </S>",
    "<S> the new construct builds on empirical and composite likelihood , taking advantage of the strengths of each . </S>",
    "<S> specifically , from empirical likelihood we borrow the ability to avoid a parametric specification , and from composite likelihood we utilize multiple likelihood components . </S>",
    "<S> we will examine the theoretical properties of this composite empirical likelihood , both for purposes of application and to compare properties to other established likelihood methods . </S>"
  ]
}