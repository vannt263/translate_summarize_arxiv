{
  "article_text": [
    "to improve classification accuracy , more than one machine learning algorithm may be combined .",
    "this process is known with different names in different domains such as classifier fusion , classifier ensemble , classifier combination , mixture of experts , committees of neural networks , voting pool of classifiers , and others @xcite .",
    "ensembles can be categorized as weak and strong classifier ensemble according to used classifiers type .",
    "the weak classifiers are machine learning algorithms with fast training times and lower classification accuracy individually . due to fast training times ,",
    "weak classifier ensembles contain higher number of classifiers , such as 50 - 200 classifiers .",
    "on the other hand , strong classifiers have slow training times and higher generalization accuracy individually .",
    "due to slow training times , strong classifier ensembles contain lower number of classifiers , such as 3 - 7 classifiers .",
    "different methods are used to combine classifiers @xcite .",
    "one of the most simple methods to ensemble classifiers is majority voting . in the majority voting method ,",
    "every classifier in ensemble gets a single vote for result .",
    "the output is the most voted , majority voted , result @xcite .",
    "another common approach that uses majority voting in its decision stage is bootstrap aggregating algorithm ( bagging ) @xcite .",
    "bagging trains weak classifiers from same dataset using uniform sampling with replacement .    instead of using single vote for every classifier , weighted voting",
    "may be used @xcite .",
    "weighted majority voting ( wmv ) algorithm @xcite uses accuracy of individual classifiers to find weights .",
    "those classifiers that have better accuracy in training step gets better weights for their votes , and becomes more effective in voting .",
    "others researchers proposed different approaches to find suitable weights for combining classifiers .",
    "for example , heuristic optimization techniques were used for this purpose @xcite . while zhang et al @xcite proposed differential evolution , sylvester and chawla @xcite used genetic algorithms .",
    "these two methods do not model ensemble weight finding problem but they use output classification accuracy of whole ensemble for fitness function .",
    "another common approach to ensemble classifiers is boosting .",
    "most used boosting algorithm , adaboost @xcite , trains weak classifiers iteratively and adds them to ensemble .",
    "different from bagging , subset creation is not randomized in boosting . in each iteration",
    ", subsets are created according to previous iterations results , i.e miss - classified data in previous subsets are more likely included .",
    "in addition , every weak classifier is weighted according to their accuracy .",
    "other methods are also used to combine classifiers .",
    "verma and hassan @xcite proposed a multi - layered hybrid method for classifier ensemble .",
    "first two layers uses self organising map ( som ) and k - means to cluster data .",
    "last layer uses multi - layer perceptron ( mlp ) to fuse clusters .",
    "they generate parallel classifiers using this hybrid technique .",
    "final decision is generated using majority voting approach .",
    "maudes et al @xcite use different projection methods to generate new features .",
    "their new feature space size is generally higher than original feature space .",
    "they train base svm classifiers using these new features and ensemble them using boosting techniques .",
    "although using more classifiers increases generalization performance of ensemble classifier , this performance increase stops after a while and leads to accuracy reduction . to put it in another way , similar classifiers",
    "do not contribute to overall accuracy very much .",
    "this deficiency can be removed by increasing the classifier diversity @xcite .",
    "although classifier diversity has no accepted definition , it affects ensemble accuracy @xcite .",
    "however , the increase in ensemble size reduces classifier diversity which finally reduces classification accuracy @xcite .",
    "therefore , improvement of diversity of classifiers is a challenge in ensemble classifier studies .",
    "ahmad @xcite is among the researchers that studied classifier diversity .",
    "he proposes to use kernel function generated features in decision tree ensembles . in his study , using kernel functions , different kernel features are generated .",
    "decision tree classifiers are trained from randomly chosen generated kernel features and original features .",
    "these new features introduce diversity to decision tree ensembles .",
    "similarly , lee et al @xcite used hierarchical pair competition - based parallel genetic algorithms ( hfc - pga ) for ensembling neural networks . in normal genetic algorithm ,",
    "individual with high fitness values will saturate population pool with their descendants very quickly .",
    "hfc - pga prevents this phenomena using parallel populations with diversity .",
    "these parallel populations may have low fitness value but have high diversity among themselves . using different population with diverse features , lee et",
    "al @xcite trained diverse neural networks .",
    "after training , they combined neural networks in a ensemble using negative correlation rule .",
    "similarly , kim et al @xcite proposed ensemble approach for biological data .",
    "their approach were similar to boosting but they also used sparse features in their weak classifiers .    another method to improve ensemble performance is pruning ( ensemble selection ) . after training ensemble , classifiers are removed from whole ensemble such that overall performance is not reduced .",
    "liu et al @xcite have proposed to use greedy randomized adaptive search procedure ( grasp ) for ensemble selection .",
    "they constructed adaboost decision tree ensembles and prune it with grasp .",
    "they compared original adaboost results with their proposed method .",
    "zhang et al @xcite implemented grasp algorithm on extreme learning machine ensembles .",
    "zhang and dai @xcite improved their earlier work @xcite using path relinking .",
    "most of the above mentioned studies do not model ensemble problem as a mathematical model .",
    "but other researchers modeled ensemble weight finding as mathematical optimization problem and proposed different methods for solving these models .",
    "zhang and zhou @xcite formulated weight finding problem as linear programming problem and solved it accordingly .",
    "yin et al @xcite proposed weight finding cost function that encompasses data regularization term , sparsity term and diversity term .",
    "they solved this cost function using genetic algorithm . in a later study , they improved their approach @xcite , and finally they solved the same cost function using convex optimization techniques @xcite .",
    "mao et al @xcite proposed 01 matrix decomposition and quadratic form @xcite methods to find classifier weights .",
    "sen and erdogan @xcite proposed different cost function and different loss function to solve weight finding problem .",
    "inspired from mentioned studies @xcite and authors earlier work @xcite , sparsity - driven weighted ensemble classifier ( sdwec ) has been proposed .",
    "our proposed cost function and solution differs from previous studies . proposed cost function",
    "consists of following terms : ( 1 ) a data fidelity term with sign function aiming to decrease misclassification rate , ( 2 ) @xmath0-norm sparsity term aiming to decrease the number of classifiers , and ( 3 ) a non - negativity constraint on the weights of the classifiers .",
    "cost function proposed in sdwec is hard to solve since it is non - convex and non - differentiable ; thus , ( a ) the sign operation is convex relaxed using a novel approximation , ( b ) the non - differentiable @xmath1-norm sparsity term and the non - negativity constraint are approximated using log - sum - exp and taylor series .",
    "sdwec improves classification accuracy , while minimizing the number of classifiers used in ensemble . since number of classifiers used in ensemble decreases , testing time for whole ensemble decreases according to sparsity level of sdwec",
    "an ensemble consists of @xmath2 number of classifiers .",
    "classifiers are trained using training dataset .",
    "we aim to increase ensemble accuracy on test dataset by finding suitable weights for classifiers using validation dataset .",
    "ensemble weight finding problem is modeled with the following matrix equation . in this matrix equation ,",
    "classifiers predictions are weighted such that obtained prediction for each data row becomes approximately equal to expected results .",
    "@xmath3 @xmath4    matrix @xmath5 consists of @xmath2 classifier predictions for @xmath6 data rows that drawn from validation dataset .",
    "our aim is to find suitable weights for @xmath7 in a sparse manner while preserving condition of @xmath8 ( sign function ) . for this model , the following cost function",
    "is proposed :    @xmath9    p0.45p0.45 @xmath6 : number of samples & @xmath7 : classifier weights + @xmath5 : classifiers results @xmath10 & @xmath2 : number of individual classifiers + @xmath11 : data fidelity coefficient & @xmath12 : true labels @xmath13 +   +    in equation  [ eq - cost - function1 ] , first term acts as a data fidelity term and minimizes the difference between true labels and ensemble predictions .",
    "base classifiers of ensemble give binary predictions ( @xmath14 or @xmath15 ) and these predictions are multiplied with weights through sign function .",
    "to make this term independent from data size , it is divided to @xmath6 ( number of data rows ) .",
    "the second term is sparsity term @xcite that forces weights to be sparse @xcite ; therefore , minimum number of classifier is utilized . in sparsity term , any @xmath16-norm ( @xmath17 ) can be used . when @xmath18 , weights become more sparse as @xmath19 gets closer to @xmath20 . however",
    ", when ( @xmath21 ) , sparsity term becomes non - convex and thus solution becomes harder , and when p is 0 then solution of @xmath22-norm becomes np - hard @xcite . here , @xmath1-norm is used as a convex relaxation of @xmath16-norm @xcite .",
    "similar to data fidelity term , this term is also normalized with division to @xmath2 ( number of individual classifiers ) .",
    "the third term is used as non - negativity constraint .",
    "since base binary classifiers use @xmath23 for class labels , negative weights change sign prediction ; thus they change class label of prediction . to prevent this problem , this constraint term is used to force weights to be non - negative . using lagrange - multipliers and definition of @xmath24 , cost function",
    "is transformed into equation  [ eq - cost - function - transformed ] .",
    "@xmath25    in equation  [ eq - cost - function - transformed ] , @xmath26 constraint is better satisfied as @xmath27 becomes larger .",
    "equation  [ eq - cost - function - transformed ] is a non - convex function , since _ sgn _",
    "function creates jumps on cost function surface .",
    "in addition , _ max _ function is non - differentiable .",
    "due to these two functions , _ max _ and _ sgn _ , equation  [ eq - cost - function - transformed ] is hard to minimize .",
    "therefore , we propose a novel convex relaxation for _ sgn _ as given in equation  [ eq - approximation - sgn ] .",
    "figure  [ chapterensemblesd - signum - approximation - plot ] shows approximation of sign function using equation  [ eq - approximation - sgn ] .",
    "@xmath28    where    @xmath29     at various points . ]    in this equation , @xmath30 is a small positive constant .",
    "we also introduce a new constant @xmath31 as a proxy for @xmath7 .",
    "therefore , @xmath32 is also a constant .",
    "however , this sgn approximation is only accurate around introduced constant @xmath31 .",
    "therefore , the approximated cost function needs to be solved iteratively . additionally , _",
    "function is approximated with log - sum - exp @xcite as following :    @xmath33    accuracy of log - sum - exp approximation becomes better as @xmath34 , a positive constant , increases .",
    "double precision floating point can represent values up to @xmath35 in magnitude @xcite .",
    "this means that @xmath36 should be less than @xmath37 where @xmath38 , otherwise exponential function will produce infinity ( @xmath39 ) . at @xmath40 ,",
    "there is no danger of numerical overflow in exponential terms of log - sum - exp approximation ; thus , large @xmath34 values can be used .",
    "but as @xmath41 gets larger , there is danger of numerical overflow in exponential terms of log - sum - exp approximation since @xmath42 may be out of double precision floating point limits .    to remedy this numerical overflow problem ,",
    "a novel adaptive @xmath34 approximation is proposed , where @xmath43 is adaptive form of @xmath34 and defined as @xmath44 .",
    "one can decrease @xmath30 or increase @xmath34 to improve approximation accuracy .",
    "figure  [ chapterensemblesd - figure - adaptive - gamma - l1-approximation ] shows proposed adaptive @xmath34 and resulting approximations for two different @xmath30 and @xmath34 values .",
    "validity of the approximation can be checked by taking the limits at @xmath45 , and @xmath46 with respect to @xmath47 .",
    "these limits are @xmath48 , and @xmath49 at @xmath45 , and @xmath46 respectively .",
    "as @xmath50 gets larger , dependency to @xmath34 decreases ; thus , proposed adaptive @xmath34 approximation is less prone to numerical overflow compared to standard log - sum - exp approximation .    applying adaptive @xmath34 approximation leads to following equations :    @xmath51    max width=,keepaspectratio     this approximation leads to cost function in equation  [ eq - cost - function - final - version ] , where @xmath52 is the iteration number .",
    "@xmath53    to get a second - order accuracy and to obtain a linear solution after taking the derivative of the cost function , equation  [ eq - cost - function - final - version ] is expanded as a second - order taylor series centered on @xmath54 , and equation  [ eq - cost - function - after - taylor ] is obtained .",
    "@xmath55    in equation  [ eq - cost - function - after - taylor ] , @xmath56 contains constant terms , @xmath57 contains @xmath58 terms , and @xmath59 contains @xmath60 terms after taylor expansion : this approximation is accurate around @xmath54 where taylor is expanded .",
    "these @xmath54 values change at each iteration .",
    "if @xmath58 values changes significantly from constant point , @xmath54 , approximation diverges from true cost function . to ensure that @xmath58 changes slowly",
    ", a new regularization term , @xmath61 , is added into the cost function .",
    "refined cost function is given in equation  [ eq - cost - function - after - taylor - new - regularization - term ] .",
    "@xmath62    equation  [ eq - cost - function - after - taylor - new - regularization - term ] can be written in a matrix - vector form as :    @xmath63    width=,keepaspectratio    [ cols= \" < , < , < \" , ]",
    "in this article , a novel sparsity driven ensemble classifier method has been presented .",
    "efficient and accurate solution for original cost function ( hard to minimize , non - convex , and non - differentiable ) has been developed .",
    "proposed solution uses a novel convex relaxation technique for sign function , and a novel adaptive log - sum - exp approximation that reduces numerical overflows .",
    "sdwec has been compared with other ensemble methods in well - known uci datasets and nsl - kdd dataset . by tuning parameters of sdwec , a more sparse ensemble  thus , better",
    "testing time can be obtained with a small decrease in accuracy .",
    "sunghan kim , fabien scalzo , donatello telesca , and xiao hu . ensemble of sparse classifiers for high - dimensional biological data .",
    "_ international journal of data mining and bioinformatics _ , 120 ( 2):0 167183 , 2015 .",
    "xu - cheng yin , kaizhu huang , hong - wei hao , khalid iqbal , and zhi - bin wang .",
    "classifier ensemble using a heuristic learning with sparsity and diversity . in _",
    "neural information processing : 19th international conference , iconip 2012 _ , berlin , heidelberg , 2012 .",
    "s.  mao , l.  xiong , l.  c. jiao , s.  zhang , and b.  chen .",
    "weighted ensemble based on 0 - 1 matrix decomposition .",
    "_ electronics letters _ , 490 ( 2):0 116118 , january 2013 .",
    "issn 0013 - 5194 .",
    "doi : 10.1049/el.2012.3528 .",
    "thomas pock , daniel cremers , horst bischof , and antonin chambolle .",
    "an algorithm for minimizing the mumford - shah functional . in _",
    "2009 ieee 12th international conference on computer vision _ , pages 11331140 .",
    "ieee , 2009 .",
    "mahbod tavallaee , ebrahim bagheri , wei lu , and ali  a. ghorbani . a detailed analysis of the kdd cup 99 data set . in _ proceedings of the second ieee international conference on computational intelligence for security and defense applications _ ,",
    "cisda09 , pages 5358 , 2009 ."
  ],
  "abstract_text": [
    "<S> in this study , a novel weighted ensemble classifier that improves classification accuracy and minimizes the number of classifiers is proposed . </S>",
    "<S> proposed method uses sparsity techniques therefore it is named sparsity - driven weighted ensemble classifier ( sdwec ) . in sdwec , </S>",
    "<S> ensemble weight finding problem is modeled as a cost function with following terms : ( a ) a data fidelity term aiming to decrease misclassification rate , ( b ) a sparsity term aiming to decrease the number of classifiers , and ( c ) a non - negativity constraint on the weights of the classifiers . as the proposed cost function is non - convex and hard to solve , convex relaxation techniques and novel approximations are employed to obtain a numerically efficient solution . the efficiency of sdwec is tested on 11 datasets and compared with state - of - the art classifier ensemble methods . </S>",
    "<S> the results show that sdwec provides better or similar accuracy using fewer classifiers and reduces testing time for ensemble .    </S>",
    "<S> = 1 </S>"
  ]
}