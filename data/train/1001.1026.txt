{
  "article_text": [
    "network coding as a means of increasing throughput in networks has been extensively studied in @xcite .",
    "block network - error correction for coherent network codes has been studied in @xcite . in all of these , the sufficient field size requirement for designing good block network - error correcting codes ( bneccs ) is quite high . to be precise , the sufficient field size requirement for constructing a bnecc along with a network code which corrects network - errors due to any @xmath3 edges of the network being in error once in every @xmath4 network uses is such that @xmath5 where @xmath6 is the set of sinks .",
    "this requires every network - coding node of the network to perform multiplications of large degree polynomials over the base field each time it has to transmit , and therefore is computationally demanding .",
    "moreover , the bound increases with the size of the network .",
    "it is therefore necessary to study network - error correcting codes which work under small field size conditions .",
    "convolutional network - error correcting codes ( cneccs ) were introduced in @xcite in the context of coherent network coding for acyclic instantaneous networks .",
    "the field size requirement for the cneccs of @xcite is independent of the number of edges in the network and in general much smaller than what is demanded by bneccs .",
    "although the error correcting capability might not be comparable to that offered by bneccs , the reduction in field size is a considerable advantage in terms of the computation to be performed at each coding node of the network . also , the use of convolutional codes permits decoding using the viterbi decoder , which is readily available .",
    "cneccs with similar advantages for memory - free unit - delay acyclic networks were discussed in @xcite and the benefit obtained in the performance of such cneccs by using memory at the nodes of unit - delay networks was discussed in @xcite .",
    "the cneccs of @xcite were designed to correct network - errors which correspond to a set @xmath7 of error patterns ( subsets of the edge set ) once in a certain number of network uses ( a network use being the use of the edges of the network to transmit a number of symbols equal to the network code dimension ) . a similar error model ( with @xmath7 being all subsets of the edge set with @xmath3 edges )",
    "was considered in @xcite . while this error model allows code construction , it is less realistic because the errors corresponding to any error pattern in @xmath7 are assumed to occur with equal probabilities",
    ".    a more realistic error model would be to assume every edge @xmath8 in the network as a bsc with a certain cross - over probability ( @xmath0 ) and with errors across different edges to be i.i.d . in this paper , we assume such an error model ( with @xmath0 being the same for all edges ) and analyze cneccs over the binary field .",
    "binary network codes together with this error model were studied in @xcite .",
    "the decoding of bneccs under a similar probabilistic setting was discussed in @xcite . however , practical analysis and simulations of bneccs under a probabilistic error setting is difficult because of the large field size demanded . on the other hand , the cneccs developed in @xcite",
    "require small field sizes and thus facilitate analysis .",
    "the contributions and organization of this paper are as follows .",
    "* after briefly discussing cneccs for the network coding setup ( section [ sec2 ] ) , we present the error model for the network .",
    "if the edge cross - over probability @xmath9 then it is sufficient to compute only single edge network - error probabilities in the network thereby reducing the computations required to study the performance of cneccs . for any network with a given number of edges , we derive a bound on how small this @xmath0 should be so that this assumption of ignoring multiple edge network - errors can be made safely .",
    "( section [ sec3 ] ) * expressions for the upper bound on the bit error probability of cneccs are obtained based on a modified version of the augmented path generating function @xmath10 of the cnecc being used .",
    "( section [ sec4 ] ) * we analyze the performance of cneccs on networks with a probabilistic error model using simulations with the butterfly network ( fig .",
    "[ fig : butterfly ] ) as an example .",
    "simulations on the butterfly network indicate that different criteria apply for cneccs to be good under low and high @xmath0 conditions .",
    "we therefore suggest different types of cneccs under these two conditions .",
    "( section [ sec5 ] ) * for high @xmath0 conditions , it is seen that those codes perform better which have a high value of slope , which is defined as follows .",
    "+ given a minimal encoder of a rate @xmath11 convolutional code @xmath12 , the minimum normalized cycle weight @xmath13 among all cycles @xmath14(the set of all cycles ) in the state transition diagram of the encoder , except the zero cycle @xmath15 in the zero state , is called the slope @xmath16 of the convolutional code @xmath17 here @xmath18 indicates the hamming weight accumulated by the output sequence while traversing the cycle @xmath19 , and @xmath20 is the length of the cycle in @xmath21-tuples . *",
    "we derive a lower bound on the slope of any rate @xmath2 convolutional code over any finite field ( section [ sec6 ] ) , and conclude with a short discussion of the paper and several directions for future research ( section [ sec7 ] ) .",
    "while cneccs only over @xmath22 are considered for the analyses and simulations of this paper , cneccs over any field size can be studied using similar methods .",
    "an acyclic network can be represented as an acyclic directed multi - graph ( a graph that can have parallel edges between nodes ) @xmath23 = ( @xmath24 ) where @xmath25 is the set of all vertices and @xmath26 is the set of all edges in the network .",
    "every edge in the directed multi - graph representing the network has unit _ capacity _ ( can carry utmost one symbol from @xmath22 ) .",
    "let @xmath27 be the mincut between the source @xmath28 and the set of sinks @xmath29 and the dimension of the network code .",
    "an @xmath27-dimensional binary network code can be described by three matrices @xmath30 , @xmath31,and @xmath32 , each having elements from @xmath33 further details on the structure of these matrices can be found in @xcite .",
    "the network transfer matrix corresponding to a sink @xmath34 is an @xmath35 binary matrix @xmath36 such that for any input @xmath37 , the output at sink @xmath38 is @xmath39      for a given set of error patterns @xmath7 and for some @xmath40 a method of constructing rate @xmath41 convolutional codes was given in @xcite such that these cneccs will correct network - errors which correspond to the patterns in @xmath42 for a given network with a network code , the definitions for the input and output convolutional code are as follows .    an _ input convolutional code _ , @xmath43 , corresponding to an acyclic network is a convolutional code of rate @xmath44 with a _ input generator matrix _",
    "@xmath45 implemented at the source of the network .",
    "the _ output convolutional code _",
    "@xmath46 corresponding to a sink node @xmath34 in the acyclic network is the @xmath41 convolutional code generated by the _ output generator matrix _ @xmath47 which is given by @xmath48 with @xmath36 being the full rank network transfer matrix corresponding to an @xmath27-dimensional network code .    it was shown in @xcite that errors corresponding to @xmath7 can be corrected at all sinks as long as they are separated by a certain number of network uses .",
    "moreover , a sink can achieve this error correcting capability by choosing to decode on either the input or the output convolutional codes depending upon their distance properties .",
    "table [ tab1 ] shows the network transfer matrices of the butterfly network of fig .",
    "[ fig : butterfly ] and an example of a cnecc along with the output convolutional codes at the two sinks .    .butterfly network of fig .",
    "[ fig : butterfly ] with the input convolutional code @xmath49.$ ] [ cols=\"^,^ , < \" , ]     [ tab2 ]    it is seen that there are two regimes of operation ( for each pair of convolutional codes ) where the performance of the codes get interchanged .",
    "this was already noticed in @xcite in the context of awgn channels .",
    "the value of @xmath0 for which these regimes becomes separated is not only dependent on the cnecc - pair chosen , but also on the network and the network code , and would probably decrease with the increase in the size of the network .          ' '' ''    fig .",
    "[ fig : lowpecurve ] shows the performance of convolutional codes with different free distances on the butterfly network for low values of @xmath50 along with the bounds on the bit - error probability evaluated according to section [ sec4 ] .",
    "codes with better distance spectra are good in the low @xmath0 regime . according to fig .",
    "[ fig : completecurve ] , this behavior is seen upto @xmath51 however the bounds on the bit - error probability states become very loose beyond @xmath52 which is why the @xmath0 has been restricted to that value in fig .",
    "[ fig : lowpecurve ] .",
    "maximum distance separable ( mds ) convolutional codes thus seem to be a good choice .",
    "the design of such convolutional codes along with the bounds on the field size requirement was discussed in @xcite for a fixed set of error patterns .",
    "if the value of @xmath0 is low enough , one might follow the design given in @xcite assuming the set of errors to be all possible single or double edge network - errors alone .      from fig .",
    "[ fig : completecurve ] , it is seen that codes with higher slopes are good for the high @xmath0 regime .",
    "the definition of the slope @xmath16 of a convolutional code @xmath12 is as in ( [ slopedefinition ] ) . for a given memory @xmath53 and free distance @xmath54 ,",
    "a convolutional code is said to be a _",
    "maximum slope convolutional code _",
    "@xcite if there exists no other code with a higher slope for the same memory and same free distance .",
    "families of convolutional maximum slope convolutional codes were reported in @xcite , discovered using computer search .",
    "as seen in subsection [ subsec5b ] , codes with good slopes perform well in high @xmath0 conditions .",
    "it is therefore important to investigate the properties of the slope parameter and to come up with constructions which yield codes with good slopes .",
    "upper bounds on the slope of convolutional codes were given in @xcite .",
    "a lower bound on the slope of any rate @xmath55 convolutional code was given in @xcite . in this section ,",
    "we derive a lower bound on the slope of any rate @xmath2 convolutional code over any finite field .    a primer on the basics of convolutional codes can be found in appendix [ app1 ] . towards obtaining a bound on the slope @xmath16",
    ", we first give the following lemma .",
    "the proof of the following lemma is on the lines of lemma 1 in @xcite .",
    "[ deltazeroes ] let @xmath12 be a rate @xmath2 convolutional code with degree @xmath56 for some @xmath57 if there exists a @xmath58 length partial codeword sequence @xmath59 } : = \\left[\\boldsymbol{v}_i,\\boldsymbol{v}_{i+1 } , . .",
    ", \\boldsymbol{v}_{i+\\delta}\\right]\\ ] ] where @xmath60 for @xmath61 then @xmath62}$ ] has at least one cycle around the zero state of the corresponding minimal encoder of @xmath17    let @xmath63 be a minimal basic generator matrix of @xmath64 let the ordered forney indices ( row degrees of @xmath63 ) be @xmath65 , and therefore @xmath66 being the sum of these indices . then a systematic generator matrix(@xmath67 ) for @xmath12 that is equivalent to @xmath63 is of the form @xmath68 where @xmath69 is a full rank @xmath70 submatrix of @xmath63 with a delay - free determinant .",
    "we have the following observation .",
    "[ degree ] the degree of @xmath71 is utmost @xmath56 also , we have the @xmath72 element @xmath73 of @xmath74 as @xmath75 where @xmath76 $ ] is the cofactor of the @xmath77 element of @xmath78 the degree of @xmath79 is utmost @xmath80    let @xmath81 represent the @xmath72 element of @xmath82 where @xmath83 @xmath84 being @xmath85 element of @xmath86 therefore , the element @xmath87 can be expressed as @xmath88 where the degree of @xmath89 $ ] is utmost @xmath90 now if we divide @xmath91 by @xmath71 , we have @xmath92 where the degree of @xmath93 $ ] is utmost @xmath94 , and the degree of @xmath95 is utmost @xmath96 because every element of @xmath67 can be reduced to the form in ( [ eqn4 ] ) , we can have a realization of @xmath67 with utmost @xmath66 memory elements for each of the @xmath97 inputs .",
    "let this encoder realization be known as @xmath98        ' '' ''    now we shall prove the lemma by contradiction .",
    "let @xmath99 be a codeword which contains the partial codeword sequence @xmath100}$ ] as follows : @xmath101\\ ] ]    let @xmath102 be the information sequence which when encoded into @xmath103 by the systematic encoder @xmath98 because of the systematic property of @xmath104 , we must have that @xmath105    by observation [ degree ] , @xmath104 is an encoder which has utmost @xmath66 memory elements ( for each input ) , and hence the state vector @xmath106 at time instant @xmath107 becomes zero as a result of @xmath66 zero input vectors .",
    "[ fig : alphabound ] shows the scenario we consider .    with another zero at time instant @xmath108",
    "there is a zero cycle .",
    "but we need to prove it for a minimal encoder , not a systematic one .",
    "so , we consider the codeword @xmath109 which can now be written as a unique sum of two code words @xmath110 , where @xmath111\\ ] ] and @xmath112\\ ] ] where @xmath113 and the uniqueness of the decomposition holds with respect to the positions of the zeros indicated in the two code words @xmath114 and @xmath115    let @xmath116 be the information sequence which is encoded into @xmath103 by a minimal realization @xmath117 of a minimal basic generator matrix @xmath63 ( a minimal encoder )",
    ". then we have @xmath118 where @xmath119 and @xmath120 are encoded by @xmath117 into @xmath114 and @xmath121 respectively .    by the",
    "_ predictable degree property _ ( pdp ) @xcite of minimal basic generator matrices , we have that for any polynomial code sequence @xmath103 , @xmath122 where @xmath123 $ ] represents the information sequence corresponding to the @xmath124 input , and @xmath125 indicates the degree of the polynomial .",
    "therefore , by the pdp property , we have that @xmath126 , since @xmath127 .      with the minimal encoder @xmath117 which has @xmath134 memory elements",
    ", these @xmath58 consecutive zeros of @xmath116 would result in the state vector @xmath135 becoming zero for all time instants from @xmath136 to @xmath137 i.e. , @xmath138 with @xmath139 the path traced by @xmath62}$ ] traces at least one zero cycle on the trellis corresponding to the minimal encoder .",
    "this concludes the proof .",
    "we shall now prove the bound on @xmath140    the slope @xmath16 of a rate @xmath2 convolutional code @xmath12 with degree @xmath66 is lower bounded as @xmath141    first we note the fact that every path in the state transition diagram is either a cycle or a part of a cycle . by lemma [ deltazeroes ] , the path traced by any partial codeword sequence with @xmath58 consecutive zero components in the state transition diagram of the minimal encoder would have a cycled around in the zero state at least once .",
    "the definition of @xmath16 excludes a cycle around the zero state , and therefore paths ( partial codeword sequences ) which have @xmath58 consecutive zero components can not be considered to measure @xmath16 since they would ultimately result in a zero cycle .",
    "however , lemma [ deltazeroes ] also implies that any path in the state transition diagram which does not include the zero cycle must therefore accumulate at least 1 hamming weight in every @xmath58 transitions .",
    "thus we have proved that the slope is lower bounded as : @xmath141",
    "the performance of cneccs under the bsc edge error model has been analyzed using theoretical bounds and simulations .",
    "a sufficient upper bound on the edge cross - over probability @xmath0 has been obtained , so that if @xmath0 is below this bound , the complexity of analysis can be reduced greatly by considering only single edge network - errors .",
    "codes with better distance spectra and those with good slopes are seen to perform well under different conditions on the cross - over probability . a lower bound on the slope of any convolutional code is also obtained .",
    "several interesting problems remain in this context including the following .    * studying the soft - decision decoding performance of cneccs .",
    "* constructions of convolutional codes with good slopes . * in large networks ,",
    "error probabilities at the sinks could be large even for negligible @xmath0 values",
    ". it would be interesting to look at the existing network error correction schemes for such networks , and compare them with schemes which involve coding over smaller subnetworks .",
    "this work was supported partly by the drdo - iisc program on advanced research in mathematical engineering to b.  s.  rajan .",
    "160 r. ahlswede , n. cai , r. li and r. yeung , `` network information flow '' , ieee transactions on information theory , vol.46 , no.4 , july 2000 , pp .",
    "1204 - 1216 .",
    "n. cai , r. li and r. yeung , `` linear network coding '' , ieee transactions on information theory , vol .",
    "2003 , pp .",
    "371 - 381 .",
    "r. koetter and m. medard , `` an algebraic approach to network coding '' , ieee / acm transactions on networking , vol .",
    "2003 , pp .",
    "782 - 795 .",
    "raymond w. yeung and ning cai , `` network error correction , part 1 and part 2 '' , comm . in inform . and systems , vol .",
    "6 , 2006 , pp .",
    "19 - 36 .",
    "zhen zhang , `` linear network - error correction codes in packet networks '' , ieee transactions on information theory , vol .",
    "2008 , pp .",
    "209 - 218 .",
    "shenghao yang and yeung , r.w .",
    ", `` refined coding bounds for network error correction '' , itw on information theory for wireless networks , july 16 , 2007 , bergen , norway , pp",
    ". 1 - 5 .",
    "k. prasad and b. sundar rajan , `` convolutional codes for network - error correction '' , arxiv:0902.4177v3 [ cs.it ] , august 2009 , available at : http://arxiv.org/abs/0902.4177 .",
    "a shortened version of this paper is to appear in the proceedings of globecom 2009 , nov .",
    "4 , honolulu , hawaii , usa .    k. prasad and b. sundar rajan , `` network error correction for unit - delay , memory - free networks using convolutional codes '' , arxiv:0903.1967v3 [ cs.it ] , sep . 2009 , available at : http://arxiv.org/abs/0903.1967 .",
    "a shortened version of this paper is to appear in the proceedings of icc 2010 , may 23 - 27 , capetown , south africa .",
    "k. prasad and b. sundar rajan , `` single - generation network coding for networks with delay '' , arxiv:0909.1638v1 [ cs.it ] , sep .",
    "2009 , available at : http://arxiv.org/abs/0909.1638 .",
    "a shortened version of this paper is to appear in the proceedings of icc 2010 , may 23 - 27 , capetown , south africa .    ming xiao and aulin , t.m . , `` a physical layer aspect of network coding with statistically independent noisy channels '' , proceedings of icc 2006 , june 1 - 15 , istanbul , turkey , pp",
    ". 3996 - 4001 .",
    "bahramgiri , h. and lahouti , f. , `` block network error control codes and syndrome - based maximum likelihood decoding '' , proceedings of isit 2008 , july 6 - 11 , toronto , canada , pp . 807 - 811 .",
    "r. jordan , v. pavlushkov , and v.v .",
    "zyablov , `` maximum slope convolutional codes '' , ieee transactions of information theory , vol .",
    "2004 , pp .",
    "2511 - 2522    andrew j. viterbi , james k.omura , `` principles of digital communication and coding '' , mcgraw - hill , 1979 .",
    "r. johannesson and k.s zigangirov , `` fundamentals of convolutional coding '' , john wiley , 1999 .",
    "g. k. huth and c. l. weber , `` minimum weight convolutional codewords of finite length '' , ieee trans .",
    "theory , vol .",
    "it-22 , mar .",
    "1976 , pp .",
    "243 - 246 .",
    "g. d. forney , `` minimal bases of rational vector spaces with applications to multivariable linear systems '' , siam j. contr .",
    "13 , no . 3 , 1975 , pp .",
    "493 - 520 .",
    "we review the basic concepts related to convolutional codes , used extensively throughout the rest of the paper . for @xmath142 power of a prime ,",
    "let @xmath143 denote the finite field with @xmath144 elements , @xmath145 $ ] denote _ the ring of univariate polynomials _ in @xmath146 with coefficients from @xmath147 @xmath148 denote _ the field of rational functions _ with variable @xmath146 and coefficients from @xmath143 and @xmath149 $ ] denote _ the ring of formal power series _ with coefficients from @xmath143 .",
    "every element of @xmath149 $ ] of the form @xmath150 .",
    "thus , @xmath145 \\subset \\mathbb{f}_q[[z]]$ ] .",
    "we denote the set of @xmath27-tuples over @xmath149 $ ] as @xmath151 $ ] .",
    "also , a rational function @xmath152 with @xmath153 is said to be _",
    "realizable_. a matrix populated entirely with realizable functions",
    "is called a realizable matrix .    for a convolutional code , the _ information sequence _",
    "@xmath154(\\boldsymbol{u}_i\\in\\mathbb{f}_q^b)$ ] and the _ codeword sequence _ ( output sequence ) @xmath155\\left(\\boldsymbol{v}_i\\in\\mathbb{f}_q^c\\right)$ ] can be represented in terms of the delay parameter @xmath146 as @xmath156    a _ convolutional code _ , @xmath157 of rate @xmath158 is defined as @xmath159~|~ \\boldsymbol{v}(z)=\\boldsymbol{u}(z)g(z ) \\}\\ ] ] where @xmath160 is a @xmath161 _ generator matrix _ with entries from @xmath148 and rank @xmath97 over @xmath148 , and @xmath103 being the codeword sequence arising from the information sequence , @xmath162 $ ] .",
    "two generator matrices are said to be _ equivalent _ if they encode the same convolutional code .",
    "_ polynomial generator matrix__@xcite for a convolutional code @xmath12 is a generator matrix for @xmath12 with all its entries from @xmath145 $ ] .",
    "it is known that every convolutional code has a polynomial generator matrix @xcite .",
    "also , a generator matrix for a convolutional code is _ _",
    "catastrophic__@xcite if there exists an information sequence with infinitely many non - zero components , that results in a codeword with only finitely many non - zero components .    for a polynomial generator matrix @xmath160 ,",
    "let @xmath163 be the element of @xmath160 in the @xmath164 row and the @xmath165 column , and @xmath166 be the @xmath164 _ row degree _ of @xmath160 .",
    "let @xmath167 be the _ degree _ of @xmath168      forney in @xcite showed that the ordered set @xmath169 of row degrees ( indices ) is the same for all minimal basic generator matrices of @xmath12 ( which are all equivalent to one another ) .",
    "therefore the ordered row degrees and the degree @xmath66 can be defined for a convolutional code @xmath17 also , any minimal basic generator matrix for a convolutional code is non - catastrophic .    a _ convolutional encoder _ is a physical realization of a generator matrix by a linear sequential circuit .",
    "two encoders are said to be _ equivalent encoders _ if they encode the same code .",
    "minimal encoder _ is an encoder with the minimal number of memory elements among all equivalent encoders .",
    "given an encoder with @xmath170 memory elements for the code @xmath12 , we can associate a vector @xmath171 whose components indicate the states of the @xmath170 memory elements at time instant @xmath172"
  ],
  "abstract_text": [
    "<S> convolutional network - error correcting codes ( cneccs ) are known to provide error correcting capability in acyclic instantaneous networks within the network coding paradigm under small field size conditions . in this work , </S>",
    "<S> we investigate the performance of cneccs under the error model of the network where the edges are assumed to be statistically independent binary symmetric channels , each with the same probability of error @xmath0(@xmath1 ) . </S>",
    "<S> we obtain bounds on the performance of such cneccs based on a modified generating function ( the transfer function ) of the cneccs . for a given network , </S>",
    "<S> we derive a mathematical condition on how small @xmath0 should be so that only single edge network - errors need to be accounted for , thus reducing the complexity of evaluating the probability of error of any cnecc . </S>",
    "<S> simulations indicate that convolutional codes are required to possess different properties to achieve good performance in low @xmath0 and high @xmath0 regimes . for the low @xmath0 regime , </S>",
    "<S> convolutional codes with good distance properties show good performance . for the high @xmath0 regime , convolutional codes that have a good _ slope _ </S>",
    "<S> ( the minimum normalized cycle weight ) are seen to be good . we derive a lower bound on the slope of any rate @xmath2 convolutional code with a certain degree . </S>"
  ]
}