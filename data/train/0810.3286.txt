{
  "article_text": [
    "there is a rapidly growing interest in the recovery of an unknown low - rank or approximately low - rank matrix from very limited information .",
    "this problem occurs in many areas of engineering and applied science such as machine learning @xcite , control @xcite and computer vision , see @xcite . as a motivating example , consider the problem of recovering a data matrix from a sampling of its entries .",
    "this routinely comes up whenever one collects partially filled out surveys , and one would like to infer the many missing entries . in the area of recommender systems , users submit ratings on a subset of entries in a database , and the vendor provides recommendations based on the user s preferences . because users only rate a few items , one would like to infer their preference for unrated items ; this is the famous netflix problem @xcite . recovering a rectangular matrix from a sampling of its entries",
    "is known as the _ matrix completion _ problem .",
    "the issue is of course that this problem is extraordinarily ill posed since with fewer samples than entries , we have infinitely many completions .",
    "therefore , it is apparently impossible to identify which of these candidate solutions is indeed the `` correct '' one without some additional information .    in many instances ,",
    "however , the matrix we wish to recover has low rank or approximately low rank .",
    "for instance , the netflix data matrix of all user - ratings may be approximately low - rank because it is commonly believed that only a few factors contribute to anyone s taste or preference . in computer vision ,",
    "inferring scene geometry and camera motion from a sequence of images is a well - studied problem known as the structure - from - motion problem .",
    "this is an ill - conditioned problem for objects may be distant with respect to their size , or especially for `` missing data '' which occur because of occlusion or tracking failures .",
    "however , when properly stacked and indexed , these images form a matrix which has very low rank ( e.g.  rank 3 under orthography ) @xcite . other examples of low - rank matrix fitting abound ; e.g.  in control ( system identification ) , machine learning ( multi - class learning ) and so on .",
    "having said this , the premise that the unknown has ( approximately ) low rank radically changes the problem , making the search for solutions feasible since the lowest - rank solution now tends to be the right one .    in a recent paper @xcite ,",
    "cands and recht showed that matrix completion is not as ill - posed as people thought .",
    "indeed , they proved that most low - rank matrices can be recovered _",
    "exactly _ from most sets of sampled entries even though these sets have surprisingly small cardinality , and more importantly , they proved that this can be done by solving a simple _ convex _ optimization problem .",
    "to state their results , suppose to simplify that the unknown matrix @xmath5 is square , and that one has available @xmath6 sampled entries @xmath7 where @xmath8 is a random subset of cardinality @xmath6 .",
    "then @xcite proves that most matrices @xmath9 of rank @xmath10 can be perfectly recovered by solving the optimization problem @xmath11 provided that the number of samples obeys @xmath12 for some positive numerical constant @xmath13 .",
    "matrix of rank @xmath10 depends upon @xmath14 degrees of freedom . ] in , the functional @xmath15 is the nuclear norm of the matrix @xmath9 , which is the sum of its singular values .",
    "the optimization problem is convex and can be recast as a semidefinite program @xcite . in some sense , this is the tightest convex relaxation of the np - hard rank minimization problem @xmath16 since the nuclear ball @xmath17 is the convex hull of the set of rank - one matrices with spectral norm bounded by one .",
    "another interpretation of cands and recht s result is that under suitable conditions , the rank minimization program and the convex program are _ formally equivalent _ in the sense that they have exactly the same unique solution .",
    "because minimizing the nuclear norm both provably recovers the lowest - rank matrix subject to constraints ( see @xcite for related results ) and gives generally good empirical results in a variety of situations , it is understandably of great interest to develop numerical methods for solving . in @xcite ,",
    "this optimization problem was solved using one of the most advanced semidefinite programming solvers , namely , sdpt3 @xcite .",
    "this solver and others like sedumi are based on interior - point methods , and are problematic when the size of the matrix is large because they need to solve huge systems of linear equations to compute the newton direction .",
    "in fact , sdpt3 can only handle @xmath18 matrices with @xmath19 .",
    "presumably , one could resort to iterative solvers such as the method of conjugate gradients to solve for the newton step but this is problematic as well since it is well known that the condition number of the newton system increases rapidly as one gets closer to the solution . in addition , none of these general purpose solvers use the fact that the solution may have low rank .",
    "we refer the reader to @xcite for some recent progress on interior - point methods concerning some special nuclear norm - minimization problems .",
    "this paper develops the _ singular value thresholding _ algorithm for approximately solving the nuclear norm minimization problem and by extension , problems of the form @xmath20 where @xmath21 is a linear operator acting on the space of @xmath22 matrices and @xmath23 .",
    "this algorithm is a simple first - order method , and is especially well suited for problems of very large sizes in which the solution has low rank .",
    "we sketch this algorithm in the special matrix completion setting and let @xmath24 be the orthogonal projector onto the span of matrices vanishing outside of @xmath8 so that the @xmath25th component of @xmath26 is equal to @xmath27 if @xmath28 and zero otherwise .",
    "our problem may be expressed as @xmath29 with optimization variable @xmath30 .",
    "fix @xmath31 and a sequence @xmath32 of scalar step sizes .",
    "then starting with @xmath33 , the algorithm inductively defines @xmath34 until a stopping criterion is reached . in",
    ", @xmath35 is a nonlinear function which applies a soft - thresholding rule at level @xmath36 to the singular values of the input matrix , see section [ sec : alg ] for details .",
    "the key property here is that for large values of @xmath36 , the sequence @xmath2 converges to a solution which very nearly minimizes .",
    "hence , at each step , one only needs to compute at most one singular value decomposition and perform a few elementary matrix additions .",
    "two important remarks are in order :    1 .   _ sparsity .",
    "_ for each @xmath37 , @xmath1 vanishes outside of @xmath8 and is , therefore , sparse , a fact which can be used to evaluate the shrink function rapidly .",
    "2 .   _ low - rank property .",
    "_ the matrices @xmath38 turn out to have low rank , and hence the algorithm has minimum storage requirement since we only need to keep principal factors in memory .",
    "our numerical experiments demonstrate that the proposed algorithm can solve problems , in matlab , involving matrices of size @xmath39 having close to a billion unknowns in 17 minutes on a standard desktop computer with a 1.86 ghz cpu ( dual core with matlab s multithreading option enabled ) and 3 gb of memory . as a consequence ,",
    "the singular value thresholding algorithm may become a rather powerful computational tool for large scale matrix completion .",
    "the singular value thresholding algorithm can be adapted to deal with other types of convex constraints .",
    "for instance , it may address problems of the form @xmath40 where each @xmath41 is a lipschitz convex function ( note that one can handle linear equality constraints by considering pairs of affine functionals ) .",
    "in the simpler case where the @xmath41 s are affine functionals , the general algorithm goes through a sequence of iterations which greatly resemble .",
    "this is useful because this enables the development of numerical algorithms which are effective for recovering matrices from a small subset of sampled entries possibly contaminated with noise .",
    "the rest of the paper is organized as follows . in section",
    "[ sec : alg ] , we derive the singular value thresholding ( svt ) algorithm for the matrix completion problem , and recasts it in terms of a well - known lagrange multiplier algorithm . in section [ sec : general ] , we extend the svt algorithm and formulate a general iteration which is applicable to general convex constraints . in section [ sec : conv ] , we establish the convergence results for the iterations given in sections [ sec : alg ] and [ sec : general ] .",
    "we demonstrate the performance and effectiveness of the algorithm through numerical examples in section [ sec : num ] , and review additional implementation details .",
    "finally , we conclude the paper with a short discussion in section [ sec : discussion ] .    before continuing ,",
    "we provide here a brief summary of the notations used throughout the paper .",
    "matrices are bold capital , vectors are bold lowercase and scalars or entries are not bold . for instance , @xmath42 is a matrix and @xmath27 its @xmath25th entry .",
    "likewise , @xmath43 is a vector and @xmath44 its @xmath45th component .",
    "the nuclear norm of a matrix is denoted by @xmath15 , the frobenius norm by @xmath46 and the spectral norm by @xmath47 ; note that these are respectively the 1-norm , the 2-norm and the sup - norm of the vector of singular values .",
    "the adjoint of a matrix @xmath42 is @xmath48 and similarly for vectors .",
    "the notation @xmath49 , where @xmath43 is a vector , stands for the diagonal matrix with @xmath50 as diagonal elements .",
    "we denote by @xmath51 the standard inner product between two matrices ( @xmath52 ) .",
    "the cauchy - schwarz inequality gives @xmath53 and it is well known that we also have @xmath54 ( the spectral and nuclear norms are dual from one another ) , see e.g.  @xcite .",
    "this section introduces the singular value thresholding algorithm and discusses some of its basic properties .",
    "we begin with the definition of a key building block , namely , the singular value thresholding operator .",
    "consider the singular value decomposition ( svd ) of a matrix @xmath55 of rank @xmath10 @xmath56 where @xmath57 and @xmath58 are respectively @xmath59 and @xmath60 matrices with orthonormal columns , and the singular values @xmath61 are positive ( unless specified otherwise , we will always assume that the svd of a matrix is given in the reduced form above ) .",
    "for each @xmath62 , we introduce the soft - thresholding operator @xmath63 defined as follows : @xmath64 where @xmath65 is the positive part of @xmath66 , namely , @xmath67 . in words",
    ", this operator simply applies a soft - thresholding rule to the singular values of @xmath42 , effectively shrinking these towards zero .",
    "this is the reason why we will also refer to this transformation as the _ singular value shrinkage _ operator .",
    "even though the svd may not be unique , it is easy to see that the singular value shrinkage operator is well defined and we do not elaborate further on this issue . in some sense , this shrinkage operator is a straightforward extension of the soft - thresholding rule for scalars and vectors . in particular , note that if many of the singular values of @xmath42 are below the threshold @xmath36 , the rank of @xmath68 may be considerably lower than that of @xmath42 , just like the soft - thresholding rule applied to vectors leads to sparser outputs whenever some entries of the input are below threshold .",
    "the singular value thresholding operator is the proximity operator associated with the nuclear norm .",
    "details about the proximity operator can be found in e.g.  @xcite .",
    "[ thm : prox ] for each @xmath62 and @xmath69 , the singular value shrinkage operator @xmath70 obeys @xmath71    since the function @xmath72 is strictly convex , it is easy to see that there exists a unique minimizer , and we thus need to prove that it is equal to @xmath73 . to do this , recall the definition of a subgradient of a convex function @xmath74 .",
    "we say that @xmath75 is a subgradient of @xmath76 at @xmath77 , denoted @xmath78 , if @xmath79 for all @xmath42 .",
    "now @xmath80 minimizes @xmath81 if and only if @xmath82 is a subgradient of the functional @xmath81 at the point @xmath80 , i.e. @xmath83 where @xmath84 is the set of subgradients of the nuclear norm .",
    "let @xmath30 be an arbitrary matrix and @xmath85 be its svd .",
    "it is known @xcite that @xmath86    set @xmath87 for short . in order to show that @xmath80 obeys , decompose the svd of @xmath88 as @xmath89 where @xmath90 , @xmath91 ( resp .",
    "@xmath92 , @xmath93 ) are the singular vectors associated with singular values greater than @xmath36 ( resp .",
    "smaller than or equal to @xmath36 ) . with these notations",
    ", we have @xmath94 and , therefore , @xmath95 by definition , @xmath96 , @xmath97 and since the diagonal elements of @xmath98 have magnitudes bounded by @xmath36 , we also have @xmath99 .",
    "hence @xmath100 , which concludes the proof .",
    "we are now in the position to introduce the singular value thresholding algorithm .",
    "fix @xmath31 and a sequence @xmath101 of positive step sizes . starting with @xmath102 ,",
    "inductively define for @xmath103 , @xmath104 until a stopping criterion is reached ( we postpone the discussion this stopping criterion and of the choice of step sizes ) .",
    "this shrinkage iteration is very simple to implement . at each step",
    ", we only need to compute an svd and perform elementary matrix operations . with the help of a standard numerical linear algebra package",
    ", the whole algorithm can be coded in just a few lines .    before addressing further computational issues",
    ", we would like to make explicit the relationship between this iteration and the original problem . in section [ sec : conv ]",
    ", we will show that the sequence @xmath2 converges to the unique solution of an optimization problem closely related to , namely , @xmath105 furthermore , it is intuitive that the solution to this modified problem converges to that of as @xmath106 as shown in section [ sec : general ] .",
    "thus by selecting a large value of the parameter @xmath36 , the sequence of iterates converges to a matrix which nearly minimizes .    as mentioned earlier , there are two crucial properties which make this algorithm ideally suited for matrix completion .",
    "* _ low - rank property . _",
    "a remarkable empirical fact is that the matrices in the sequence @xmath2 have low rank ( provided , of course , that the solution to has low rank ) .",
    "we use the word `` empirical '' because all of our numerical experiments have produced low - rank sequences but we can not rigorously prove that this is true in general .",
    "the reason for this phenomenon is , however , simple : because we are interested in large values of @xmath36 ( as to better approximate the solution to ) , the thresholding step happens to ` kill ' most of the small singular values and produces a low - rank output .",
    "in fact , our numerical results show that the rank of @xmath107 is nondecreasing with @xmath108 , and the maximum rank is reached in the last steps of the algorithm , see section [ sec : num ] . + thus",
    ", when the rank of the solution is substantially smaller than either dimension of the matrix , the storage requirement is low since we could store each @xmath109 in its svd form ( note that we only need to keep the current iterate and may discard earlier values ) . * _ sparsity .",
    "_ another important property of the svt algorithm is that the iteration matrix @xmath1 is sparse . since @xmath110",
    ", we have by induction that @xmath111 vanishes outside of @xmath8 . the fewer entries available ,",
    "the sparser @xmath1 . because the sparsity pattern @xmath8 is fixed throughout",
    ", one can then apply sparse matrix techniques to save storage .",
    "also , if @xmath112 , the computational cost of updating @xmath1 is of order @xmath6 .",
    "moreover , we can call subroutines supporting sparse matrix computations , which can further reduce computational costs . + one such subroutine is the svd .",
    "however , note that we do not need to compute the entire svd of @xmath1 to apply the singular value thresholding operator .",
    "only the part corresponding to singular values greater than @xmath36 is needed .",
    "hence , a good strategy is to apply the iterative lanczos algorithm to compute the first few singular values and singular vectors .",
    "because @xmath1 is sparse , @xmath1 can be applied to arbitrary vectors rapidly , and this procedure offers a considerable speedup over naive methods .",
    "our algorithm is inspired by recent work in the area of @xmath4 minimization , and especially by the work on linearized bregman iterations for compressed sensing , see @xcite for linearized bregman iterations and @xcite for some information about the field of compressed sensing . in this line of work , linearized bregman iterations are used to find the solution to an underdetermined system of linear equations with minimum @xmath4 norm .",
    "in fact , theorem [ thm : prox ] asserts that the singular value thresholding algorithm can be formulated as a linearized bregman iteration .",
    "bregman iterations were first introduced in @xcite as a convenient tool for solving computational problems in the imaging sciences , and a later paper @xcite showed that they were useful for solving @xmath4-norm minimization problems in the area of compressed sensing . linearized bregman iterations were proposed in @xcite to improve performance of plain bregman iterations , see also @xcite .",
    "additional details together with a technique for improving the speed of convergence called _ kicking _ are described in @xcite . on the practical side ,",
    "the paper @xcite applied bregman iterations to solve a deblurring problem while on the theoretical side , the references @xcite gave a rigorous analysis of the convergence of such iterations .",
    "new developments keep on coming out at a rapid pace and recently , @xcite introduced a new iteration , the _ split bregman iteration _ , to extend bregman - type iterations ( such as linearized bregman iterations ) to problems involving the minimization of @xmath4-like functionals such as total - variation norms , besov norms , and so forth .    when applied to @xmath4-minimization problems ,",
    "linearized bregman iterations are sequences of soft - thresholding rules operating on vectors .",
    "iterative soft - thresholding algorithms in connection with @xmath4 or total - variation minimization have quite a bit of history in signal and image processing and we would like to mention the works @xcite for total - variation minimization , @xcite for @xmath4 minimization , and @xcite for some recent applications in the area of image inpainting and image restoration . just as iterative soft - thresholding methods are designed to find sparse solutions , our iterative singular value thresholding scheme is designed to find a sparse vector of singular values .",
    "in classical problems arising in the areas of compressed sensing , and signal or image processing , the sparsity is expressed in a known transformed domain and soft - thresholding is applied to transformed coefficients .",
    "in contrast , the shrinkage operator @xmath113 is adaptive .",
    "the svt not only discovers a sparse singular vector but also the bases in which we have a sparse representation . in this sense ,",
    "the svt algorithm is an extension of earlier iterative soft - thresholding schemes .    finally , we would like to contrast the svt iteration with the popular iterative soft - thresholding algorithm used in many papers in imaging processing and",
    "perhaps best known under the name of proximal forward - backward splitting method ( pfbs ) , see @xcite for example .",
    "the constrained minimization problem may be relaxed into @xmath114 for some @xmath115 .",
    "theorem [ thm : prox ] asserts that @xmath116 is the proximity operator of @xmath117 and proposition 3.1(iii ) in @xcite gives that the solution to this unconstrained problem is characterized by the fixed point equation @xmath118 for each @xmath119 .",
    "one can then apply a simplified version of the pfbs method ( see ( 3.6 ) in @xcite ) to obtain iterations of the form @xmath120 introducing an intermediate matrix @xmath111 , this algorithm may be expressed as @xmath121 the difference with may seem subtle at first  replacing @xmath107 in with @xmath122 and setting @xmath123 gives with @xmath124but has enormous consequences as this gives entirely different algorithms .",
    "first , they have different limits : while converges to the solution of the constrained minimization , converges to the solution of provided that the sequence of step sizes is appropriately selected .",
    "second , selecting a large @xmath125 ( or a large value of @xmath124 ) in gives a low - rank sequence of iterates and a limit with small nuclear norm .",
    "the limit , however , does not fit the data and this is why one has to choose a small or moderate value of @xmath125 ( or of @xmath126 ) .",
    "however , when @xmath125 is not sufficiently large , the @xmath38 may not have low rank even though the solution has low rank ( and one may need to compute many singular vectors ) , and @xmath1 is not sufficiently sparse to make the algorithm computationally attractive . moreover , the limit does not necessary have a small nuclear norm .",
    "these are reasons why is not suitable for matrix completion .      in this section",
    ", we recast the svt algorithm as a type of lagrange multiplier algorithm known as uzawa s algorithm .",
    "an important consequence is that this will allow us to extend the svt algorithm to other problems involving the minimization of the nuclear norm under convex constraints , see section [ sec : general ] .",
    "further , another contribution of this paper is that this framework actually recasts linear bregman iterations as a very special form of uzawa s algorithm , hence providing fresh and clear insights about these iterations .    in",
    "what follows , we set @xmath127 for some fixed @xmath31 and recall that we wish to solve @xmath128 the lagrangian for this problem is given by @xmath129 where @xmath69 .",
    "strong duality holds and @xmath130 and @xmath131 are primal - dual optimal if @xmath132 is a saddlepoint of the lagrangian @xmath133 , i.e.  a pair obeying @xmath134 ( the function @xmath135 is called the dual function . )",
    "uzawa s algorithm approaches the problem of finding a saddlepoint with an iterative procedure . from @xmath136 ,",
    "say , inductively define @xmath137 where @xmath32 is a sequence of positive step sizes .",
    "uzawa s algorithm is , in fact , a subgradient method applied to the dual problem , where each step moves the current iterate in the direction of the gradient or of a subgradient .",
    "indeed , observe that @xmath138 where @xmath139 is the minimizer of the lagrangian for that value of @xmath88 so that a gradient descent update for @xmath88 is of the form @xmath140    it remains to compute the minimizer of the lagrangian , and note that @xmath141 however , we know that the minimizer is given by @xmath142 and since @xmath143 for all @xmath37 , uzawa s algorithm takes the form @xmath144 which is exactly the update .",
    "this point of view brings to bear many different mathematical tools for proving the convergence of the singular value thresholding iterations . for an early use of uzawa s algorithm minimizing an @xmath4-like functional , the total - variation norm , under linear inequality constraints , see @xcite",
    "this section presents a general formulation of the svt algorithm for approximately minimizing the nuclear norm of a matrix under convex constraints .      set the objective functional @xmath145 for some fixed @xmath31 , and consider the following optimization problem : @xmath146 where @xmath147 is a linear transformation mapping @xmath148 matrices into @xmath149 ( @xmath150 is the adjoint of @xmath147 ) .",
    "this more general formulation is considered in @xcite and @xcite as an extension of the matrix completion problem .",
    "then the lagrangian for this problem is of the form @xmath151 where @xmath30 and @xmath152 , and starting with @xmath153 , uzawa s iteration is given by @xmath154 the iteration is of course the same as in the case where @xmath147 is a sampling operator extracting @xmath6 entries with indices in @xmath8 out of an @xmath155 matrix . to verify this claim ,",
    "observe that in this situation , @xmath156 , and let @xmath9 be any matrix obeying @xmath157 . then defining @xmath158 and substituting this expression in gives .",
    "one can also adapt the algorithm to handle general convex constraints .",
    "suppose we wish to minimize @xmath159 defined as before over a convex set @xmath160 . to simplify",
    ", we will assume that this convex set is given by @xmath161 where the @xmath41 s are convex functionals ( note that one can handle linear equality constraints by considering pairs of affine functionals ) .",
    "the problem of interest is then of the form @xmath162 just as before , it is intuitive that as @xmath163 , the solution to this problem converges to a minimizer of the nuclear norm under the same constraints as shown in theorem [ thm : largemu2 ] at the end of this section .",
    "put @xmath164 for short .",
    "then the lagrangian for is equal to @xmath165 where @xmath30 and @xmath152 is now a vector with nonnegative components denoted , as usual , by @xmath166",
    ". one can apply uzawa s method just as before with the only modification that we will use a subgradient method with projection to maximize the dual function since we need to make sure that the successive updates @xmath167 belong to the nonnegative orthant .",
    "this gives @xmath168_+ .",
    "\\end{cases}\\ ] ] above , @xmath169 is of course the vector with entries equal to @xmath170 .",
    "when @xmath171 is an affine mapping of the form @xmath172 so that one solves @xmath173 this simplifies to @xmath174_+ , \\end{cases}\\ ] ] and thus the extension to linear inequality constraints is straightforward .",
    "an interesting example concerns the extension of the dantzig selector @xcite to matrix problems .",
    "suppose we have available linear measurements about a matrix @xmath9 of interest @xmath175 where @xmath176 is a noise vector .",
    "then under these circumstances , one might want to find the matrix which minimizes the nuclear norm among all matrices which are consistent with the data @xmath177 .",
    "inspired by the work on the dantzig selector which was originally developed for estimating sparse parameter vectors from noisy data , one could approach this problem by solving @xmath178 where @xmath179 is an array of tolerances , which is adjusted to fit the noise statistics @xcite . above ,",
    "@xmath180 , for any two matrices @xmath181 and @xmath182 , means componentwise inequalities ; that is , @xmath183 for all indices @xmath184 .",
    "we use this notation as not to confuse the reader with the positive semidefinite ordering . in the case of the matrix completion problem where @xmath147 extracts sampled entries indexed by @xmath8",
    ", one can always see the data vector as the sampled entries of some matrix @xmath182 obeying @xmath185 ; the constraint is then natural for it may be expressed as @xmath186 if @xmath187 is white noise with standard deviation @xmath188 , one may want to use a multiple of @xmath188 for @xmath189 . in words , we are looking for a matrix with minimum nuclear norm under the constraint that all of its sampled entries do not deviate too much from what has been observed .",
    "let @xmath190 ( resp .",
    "@xmath191 ) be the lagrange multiplier associated with the componentwise linear inequality constraints @xmath192 ( resp .",
    "@xmath193 ) ) .",
    "then starting with @xmath194 , the svt iteration for this problem is of the form @xmath195_+,\\quad { \\bm{r}}^k = { \\bm{b}}^k -    \\mathcal{a}({\\bm{x}}^k ) , \\end{cases}\\ ] ] where again @xmath196_+$ ] is applied componentwise .",
    "we conclude by noting that in the matrix completion problem where @xmath197 and one observes @xmath198 , one can check that this iteration simplifies to @xmath199_+ . \\end{cases}\\ ] ] again , this is easy to implement and whenever the solution has low rank , the iterates @xmath38 have low rank as well .",
    "we now show that minimizing the proximal objective @xmath200 is the same as minimizing the nuclear norm in the limit of large @xmath36 s .",
    "the theorem below is general and covers the special case of linear equality constraints as in .",
    "[ thm : largemu2 ] let @xmath201 be the solution to and @xmath202 be the minimum frobenius - norm solution to defined as @xmath203 assume that the @xmath204 s , @xmath205 , are convex and lower semi - continuous .",
    "then @xmath206    it follows from the definition of @xmath201 and @xmath202 that @xmath207 summing these two inequalities gives @xmath208 which implies that @xmath209 is bounded uniformly in @xmath36 .",
    "thus , we would prove the theorem if we could establish that any convergent subsequence @xmath210 must converge to @xmath202 .",
    "consider an arbitrary converging subsequence @xmath211 and set @xmath212 .",
    "since for each @xmath205 , @xmath213 and @xmath41 is lower semi - continuous , @xmath214 obeys @xmath215 furthermore , since @xmath209 is bounded , yields @xmath216 an immediate consequence is @xmath217 and , therefore , @xmath218 .",
    "this shows that @xmath214 is a solution to .",
    "now it follows from the definition of @xmath202 that @xmath219 , while we also have @xmath220 because of .",
    "we conclude that @xmath221 and thus @xmath222 since @xmath223 is unique .",
    "this section establishes the convergence of the svt iterations .",
    "we begin with the simpler proof of the convergence of in the special case of the matrix completion problem , and then present the argument for the more general constraints .",
    "we hope that this progression will make the second and more general proof more transparent .",
    "we begin by recording a lemma which establishes the strong convexity of the objective @xmath224 .",
    "[ lem : alpha ] let @xmath225 and @xmath226 .",
    "then @xmath227    an element @xmath75 of @xmath228 is of the form @xmath229 , where @xmath230 , and similarly for @xmath231 .",
    "this gives @xmath232 and it thus suffices to show that the first term of the right - hand side is nonnegative . from , we have that any subgradient of the nuclear norm at @xmath42 obeys @xmath233 and @xmath234 . in particular , this gives @xmath235 whence , @xmath236 which proves the lemma .",
    "this lemma is key in showing that the svt algorithm converges .",
    "[ thm : converge ] suppose that the sequence of step sizes obeys @xmath237 .",
    "then the sequence @xmath2 obtained via converges to the unique solution of .",
    "let @xmath238 be primal - dual optimal for the problem .",
    "the optimality conditions give @xmath239 for some @xmath240 and some @xmath241 .",
    "we then deduce that @xmath242 and , therefore , it follows from lemma [ lem : alpha ] that @xmath243 we continue and observe that because @xmath244 , @xmath245 therefore , setting @xmath246 , @xmath247 since for any matrix @xmath42 , @xmath248 . under our assumptions about the size of @xmath249",
    ", we have @xmath250 for all @xmath251 and some @xmath252 and thus @xmath253 two properties follow from this :    1 .",
    "the sequence @xmath254 is nonincreasing and , therefore , converges to a limit .",
    "2 .   as a consequence , @xmath255 as @xmath256 .",
    "the theorem is established .",
    "our second result is more general and establishes the convergence of the svt iterations to the solution of under general convex constraints . from now",
    "now , we will only assume that the function @xmath257 is lipschitz in the sense that @xmath258 for some nonnegative constant @xmath259 .",
    "note that if @xmath260 is affine , @xmath261 , we have @xmath262 where @xmath263 is the spectrum norm of the linear transformation @xmath264 defined as @xmath265 .",
    "we also recall that @xmath266 where each @xmath41 is convex , and that the lagrangian for the problem is given by @xmath267 we will assume to simplify that strong duality holds which is automatically true if the constraints obey constraint qualifications such as slater s condition @xcite .",
    "we first establish the following preparatory lemma .",
    "[ teo : proj ] let @xmath268 be a primal - dual optimal pair for .",
    "then for each @xmath119 , @xmath269 obeys @xmath270_+.\\ ] ]    recall that the projection @xmath271 of a point @xmath43 onto a convex set @xmath272 is characterized by @xmath273 in the case where @xmath274 , this condition becomes @xmath275 and @xmath276    now because @xmath269 is dual optimal we have @xmath277 substituting the expression for the lagrangian , this is equivalent to @xmath278 which is the same as @xmath279 hence it follows that @xmath269 must be the projection of @xmath280 onto the nonnegative orthant @xmath281 .",
    "since the projection of an arbitrary vector @xmath43 onto @xmath281 is given by @xmath169 , our claim follows .",
    "we are now in the position to state our general convergence result .",
    "[ thm : converge3 ] suppose that the sequence of step sizes obeys @xmath282 , where @xmath259 is the lipschitz constant in .",
    "then assuming strong duality , the sequence @xmath2 obtained via converges to the unique solution of .",
    "let @xmath268 be primal - dual optimal for the problem .",
    "we claim that the optimality conditions give that for all @xmath42 @xmath283 for some @xmath240 and some @xmath241 .",
    "we justify this assertion by proving one of the two inequalities since the other is exactly similar . for the first",
    ", @xmath38 minimizes @xmath284 over all @xmath42 and , therefore , there exist @xmath240 and @xmath285 , @xmath205 , such that @xmath286 now because each @xmath41 is convex , @xmath287 and , therefore , @xmath288 this is .",
    "now write the first inequality in for @xmath130 , the second for @xmath38 and sum the two inequalities .",
    "this gives @xmath289 the rest of the proof is essentially the same as that of theorem [ thm : converge2 ] .",
    "it follows from lemma [ lem : alpha ] that @xmath290 we continue and observe that because @xmath291_+$ ] by lemma [ teo : proj ] , we have @xmath292_+ - [ { \\bm{y}}^\\star + \\delta_k { \\mathcal{f}}({\\bm{x}}^\\star)]_+\\|\\cr & \\le \\|{\\bm{y}}^{k-1 } - { \\bm{y}}^\\star + \\delta_k ( { \\mathcal{f}}({\\bm{x}}^k ) - { \\mathcal{f}}({\\bm{x}}^\\star))\\|\\end{aligned}\\ ] ] since the projection onto the convex set @xmath281 is a contraction . therefore , @xmath293 where we have put @xmath294 instead of @xmath259 for short . under our assumptions about the size of @xmath249",
    ", we have @xmath295 for all @xmath251 and some @xmath252 .",
    "then @xmath296 and the conclusion is as before .",
    "the problem with linear constraints can be reduced to by choosing @xmath297- \\left[\\begin{matrix}{\\mathcal{a}}\\cr      -{\\mathcal{a}}\\end{matrix}\\right]{\\bm{x}},\\ ] ] and we have the following corollary :    [ thm : converge2 ] suppose that the sequence of step sizes obeys @xmath298 . then the sequence @xmath2 obtained via converges to the unique solution of .",
    "let @xmath299 . with @xmath257",
    "given as above , we have @xmath300 and thus , theorem [ thm : converge3 ] guarantees convergence as long as @xmath301 .",
    "however , an argument identical to the proof of theorem [ thm : converge ] would remove the extra factor of two .",
    "we omit the details .",
    "this section provides implementation details of the svt algorithm  as to make it practically effective for matrix completion  such as the numerical evaluation of the singular value thresholding operator , the selection of the step size @xmath249 , the selection of a stopping criterion , and so on .",
    "this section also introduces several numerical simulation results which demonstrate the performance and effectiveness of the svt algorithm .",
    "we show that @xmath302 matrices of rank 10 are recovered from just about 0.4% of their sampled entries in a matter of a few minutes on a modest desktop computer with a 1.86 ghz cpu ( dual core with matlab s multithreading option enabled ) and 3 gb of memory .",
    "to apply the singular value tresholding operator at level @xmath36 to an input matrix , it suffices to know those singular values and corresponding singular vectors above the threshold @xmath36 . in the matrix completion problem ,",
    "the singular value thresholding operator is applied to sparse matrices @xmath303 since the number of sampled entries is typically much lower than the number of entries in the unknown matrix @xmath9 , and we are hence interested in numerical methods for computing the dominant singular values and singular vectors of large sparse matrices .",
    "the development of such methods is a relatively mature area in scientific computing and numerical linear algebra in particular .",
    "in fact , many high - quality packages are readily available .",
    "our implementation uses propack , see @xcite for documentation and availability .",
    "one reason for this choice is convenience : propack comes in a matlab and a fortran version , and we find it convenient to use the well - documented matlab version",
    ". more importantly , propack uses the iterative lanczos algorithm to compute the singular values and singular vectors directly , by using the lanczos bidiagonalization algorithm with partial reorthogonalization .",
    "in particular , propack does not compute the eigenvalues and eigenvectors of @xmath304 and @xmath305 , or of an augmented matrix as in the matlab built - in function `` svds ` ' for example .",
    "consequently , propack is an efficient  both in terms of number of flops and storage requirement  and stable package for computing the dominant singular values and singular vectors of a large sparse matrix . for information , the available documentation @xcite reports a speedup factor of about ten over matlab s `` svds ` ' .",
    "furthermore , the fortran version of propack is about 34 times faster than the matlab version . despite this significant speedup",
    ", we have only used the matlab version but since the singular value shrinkage operator is by - and - large the dominant cost in the svt algorithm , we expect that a fortran implementation would run about 3 to 4 times faster .    as for most svd packages ,",
    "though one can specify the number of singular values to compute , propack can not automatically compute only those singular values exceeding the threshold @xmath36 .",
    "one must instead specify the number @xmath306 of singular values ahead of time , and the software will compute the @xmath306 largest singular values and corresponding singular vectors . to use this package , we must then determine the number @xmath307 of singular values of @xmath122 to be computed at the @xmath108th iteration .",
    "we use the following simple method .",
    "let @xmath308 be the number of nonzero singular values of @xmath309 at the previous iteration .",
    "set @xmath310 and compute the first @xmath311 singular values of @xmath122 .",
    "if some of the computed singular values are already smaller than @xmath36 , then @xmath307 is a right choice .",
    "otherwise , increment @xmath307 by a predefined integer @xmath312 repeatedly until some of the singular values fall below @xmath36 . in the experiments ,",
    "we choose @xmath313 .",
    "another rule might be to repeatedly multiply @xmath307 by a positive number ",
    "2until our criterion is met .",
    "incrementing @xmath307 by a fixed integer works very well in practice ; in our experiments , we very rarely need more than one update .",
    "we note that it is not necessary to rerun the lanczos iterations for the first @xmath307 vectors since they have been already computed ; only a few new singular values ( @xmath312 of them ) need to be numerically evaluated .",
    "this can be done by modifying the propack routines .",
    "we have not yet modified propack , however .",
    "had we done so , our run times would be decreased .",
    "there is a large literature on ways of selecting a step size but for simplicity , we shall use step sizes that are independent of the iteration count ; that is @xmath123 for @xmath103 . from theorem [ thm : converge ] , convergence for the completion problem is guaranteed provided that @xmath314 .",
    "this choice is , however , too conservative and the convergence is typically slow . in our experiments , we use instead @xmath315 i.e.  @xmath316 times the undersampling ratio .",
    "we give a heuristic justification below .    consider a fixed matrix @xmath317 . under the assumption that the column and row spaces of @xmath181 are not well aligned with the vectors taken from the canonical basis of @xmath318 and @xmath319",
    "respectively  the _ incoherence assumption _ in @xcite  then with very large probability over the choices of @xmath8 , we have @xmath320 provided that the rank of @xmath181 is not too large .",
    "the probability model is that @xmath8 is a set of sampled entries of cardinality @xmath6 sampled uniformly at random so that all the choices are equally likely . in",
    ", we want to think of @xmath321 as a small constant , e.g.  smaller than 1/2 . in other words ,",
    "the ` energy ' of @xmath181 on @xmath8 ( the set of sampled entries ) is just about proportional to the size of @xmath8 .",
    "the near isometry is a consequence of theorem 4.1 in @xcite , and we omit the details .    now returning to the proof of theorem [ thm : converge ] , we see that a sufficient condition for the convergence of is @xmath322 compare , which is equivalent to @xmath323 since @xmath324 for any matrix @xmath30 , it is safe to select @xmath325 . but suppose that we could apply to the matrix @xmath326",
    ". then we could take @xmath327 inversely proportional to @xmath328 ; e.g.  with @xmath329 , we could take @xmath330 .",
    "below , we shall use the value @xmath331 which allows us to take large steps and still provides convergence , at least empirically .",
    "the reason why this is not a rigorous argument is that can not be applied to @xmath332 even though this matrix difference may obey the incoherence assumption .",
    "the issue here is that @xmath333 is not a fixed matrix , but rather depends on @xmath8 since the iterates @xmath2 are computed with the knowledge of the sampled set .",
    "the svt algorithm starts with @xmath110 , and we want to choose a large @xmath36 to make sure that the solution of is close enough to a solution of .",
    "define @xmath334 as that integer obeying @xmath335.\\ ] ] since @xmath110 , it is not difficult to see that @xmath336 to save work , we may simply skip the computations of @xmath337 , and start the iteration by computing @xmath338 from @xmath339 .    this strategy is a special case of a _ kicking device _ introduced in @xcite ; the main idea of such a kicking scheme is that one can ` jump over ' a few steps whenever possible . just like in the aforementioned reference",
    ", we can develop similar kicking strategies here as well . because in our numerical experiments the kicking",
    "is rarely triggered , we forgo the description of such strategies .      here , we discuss stopping criteria for the sequence of svt iterations , and present two possibilities .    the first is motivated by the first - order optimality conditions or kkt conditions tailored to the minimization problem . by and letting @xmath340 in , we see that the solution @xmath341 to must also verify @xmath342 where @xmath88 is a matrix vanishing outside of @xmath343 . therefore , to make sure that @xmath38 is close to @xmath344 , it is sufficient to check how close @xmath345 is to obeying . by definition ,",
    "the first equation in is always true .",
    "therefore , it is natural to stop when the error in the second equation is below a specified tolerance .",
    "we suggest stopping the algorithm when @xmath346 where @xmath321 is a fixed tolerance , e.g.  @xmath347 .",
    "we provide a short heuristic argument justifying this choice below .    in the matrix completion problem , we know that under suitable assumptions @xmath348 which is just applied to the fixed matrix @xmath9 ( the symbol",
    "@xmath349 here means that there is a constant @xmath321 as in ) .",
    "suppose we could also apply to the matrix @xmath350 ( which we rigorously can not since @xmath38 depends on @xmath8 ) , then we would have @xmath351 and thus @xmath352 in words , one would control the relative reconstruction error by controlling the relative error on the set of sampled locations .",
    "a second stopping criterion comes from duality theory .",
    "firstly , the iterates @xmath38 are generally not feasible for although they become asymptotically feasible .",
    "one can construct a feasible point from @xmath38 by projecting it onto the affine space @xmath353 as follows : @xmath354 as usual let @xmath355 and denote by @xmath356 the optimal value of .",
    "since @xmath357 is feasible , we have @xmath358 secondly , using the notations of section [ sec : uzawa ] , duality theory gives that @xmath359 therefore , @xmath360 is an upper bound on the duality gap and one can stop the algorithm when this quantity falls below a given tolerance .",
    "for very large problems in which one holds @xmath38 in reduced svd form , one may not want to compute the projection @xmath357 since this matrix would not have low rank and would require significant storage space ( presumably , one would not want to spend much time computing this projection either ) .",
    "hence , the second method only makes practical sense when the dimensions are not prohibitively large , or when the iterates do not have low rank .",
    "we conclude this section by summarizing the implementation details and give the svt algorithm for matrix completion below ( algorithm [ alg : svt ] ) . of course",
    ", one would obtain a very similar structure for the more general problems of the form and with linear inequality constraints . for convenience , define for each nonnegative integer @xmath361 , @xmath362_s , \\quad k = 1 , 2 , \\ldots,\\ ] ] where @xmath363 $ ] and @xmath364 $ ] are the first @xmath306 singular vectors of the matrix @xmath111 , and @xmath365 is a diagonal matrix with the first @xmath306 singular values @xmath366 on the diagonal .",
    "[ alg : svt ]        our implementation is in matlab and all the computational results we are about to report were obtained on a desktop computer with a 1.86 ghz cpu ( dual core with matlab s multithreading option enabled ) and 3 gb of memory . in our simulations , we generate @xmath18 matrices of rank @xmath10 by sampling two @xmath367 factors @xmath368 and @xmath369 independently , each having i.i.d .",
    "gaussian entries , and setting @xmath370 as it is suggested in @xcite .",
    "the set of observed entries @xmath8 is sampled uniformly at random among all sets of cardinality @xmath6 .",
    "the recovery is performed via the svt algorithm ( algorithm [ alg : svt ] ) , and we use @xmath371 as a stopping criterion . as discussed earlier ,",
    "the step sizes are constant and we set @xmath372 . throughout this section ,",
    "we denote the output of the svt algorithm by @xmath373 .",
    "the parameter @xmath36 is chosen empirically and set to @xmath374 .",
    "a heuristic argument is as follows .",
    "clearly , we would like the term @xmath375 to dominate the other , namely , @xmath376 . for products of gaussian matrices as above , standard random matrix theory asserts that the frobenius norm of @xmath9 concentrates around @xmath377 , and that the nuclear norm concentrates around about @xmath378 ( this should be clear in the simple case where @xmath379 and is generally valid ) .",
    "the value @xmath380 makes sure that on the average , the value of @xmath375 is about @xmath381 times that of @xmath382 as long as the rank is bounded away from the dimension @xmath383 .",
    ".experimental results for matrix completion .",
    "the rank @xmath10 is the rank of the unknown matrix @xmath9 , @xmath384 is the ratio between the number of sampled entries and the number of degrees of freedom in an @xmath18 matrix of rank @xmath10 ( oversampling ratio ) , and @xmath385 is the fraction of observed entries .",
    "all the computational results on the right are averaged over five runs . [",
    "cols=\"^,^,^,^,^,^,^ \" , ]     the results , reported in figure [ fig : ds ] , show that the algorithm behaves just as well with linear inequality constraints . to make this point , we compare our results with those obtained from noiseless data ( same unknown matrix and sampled locations ) . in the noiseless case",
    ", it takes about 150 iterations to reach the tolerance @xmath386 whereas in the noisy case , convergence occurs in about 200 iterations ( figure [ fig : ds](a ) ) .",
    "in addition , just as in the noiseless problem , the rank of the iterates is nondecreasing and quickly reaches the true value @xmath10 of the rank of the unknown matrix @xmath9 we wish to recover ( figure [ fig : ds](b ) ) . as a consequence the svt iterations take about the same amount of time as in the noiseless case ( figure [ fig : ds](c ) ) so that the total running time of the algorithm does not appear to be substantially different from that in the noiseless case .",
    "we close by pointing out that from a statistical point of view , the recovery of the matrix @xmath9 from undersampled and noisy entries by the matrix equivalent of the dantzig selector appears to be accurate since the relative error obeys @xmath387 ( recall that the noise ratio is about @xmath388 ) .",
    "this paper introduced a novel algorithm , namely , the singular value thresholding algorithm for matrix completion and related nuclear norm minimization problems .",
    "this algorithm is easy to implement and surprisingly effective both in terms of computational cost and storage requirement when the minimum nuclear - norm solution is also the lowest - rank solution .",
    "we would like to close this paper by discussing a few open problems and research directions related to this work .",
    "our algorithm exploits the fact that the sequence of iterates @xmath2 have low rank when the minimum nuclear solution has low rank .",
    "an interesting question is whether one can prove ( or disprove ) that in a majority of the cases , this is indeed the case",
    ".    it would be interesting to explore other ways of computing @xmath389in words , the action of the singular value shrinkage operator .",
    "our approach uses the lanczos bidiagonalization algorithm with partial reorthogonalization which takes advantages of sparse inputs but other approaches are possible .",
    "we mention two of them .    1 .",
    "a series of papers have proposed the use of randomized procedures for the approximation of a matrix @xmath88 with a matrix @xmath75 of rank @xmath10 @xcite .",
    "when this approximation consists of the truncated svd retaining the part of the expansion corresponding to singular values greater than @xmath36 , this can be used to evaluate @xmath389 .",
    "some of these algorithms are efficient when the input @xmath88 is sparse @xcite , and it would be interesting to know whether these methods are fast and accurate enough to be used in the svt iteration .",
    "2 .   a wide range of iterative methods for computing",
    "matrix functions of the general form @xmath390 are available today , see @xcite for a survey .",
    "a valuable research direction is to investigate whether some of these iterative methods , or other to be developed , would provide powerful ways for computing @xmath389 .    in practice",
    ", one would like to solve for large values of @xmath36 .",
    "however , a larger value of @xmath36 generally means a slower rate of convergence .",
    "a good strategy might be to start with a value of @xmath36 , which is large enough so that admits a low - rank solution , and at the same time for which the algorithm converges rapidly .",
    "one could then use a continuation method as in @xcite to increase the value of @xmath36 sequentially according to a schedule @xmath391 , and use the solution to the previous problem with @xmath392 as an initial guess for the solution to the current problem with @xmath393 ( warm starting ) .",
    "we hope to report on this in a separate paper .",
    "c.  is supported by the wavelets and information processing programme under a grant from dsta , singapore .",
    "e.  c.  is partially supported by the waterman award from the national science foundation and by an onr grant n00014 - 08 - 1 - 0749 .",
    "z.  s.  is supported in part by grant r-146 - 000 - 113 - 112 from the national university of singapore .",
    "e.  c.  would like to thank benjamin recht and joel tropp for fruitful conversations related to this project , and stephen becker for his help in preparing the computational results of section 5.2.2 ."
  ],
  "abstract_text": [
    "<S> this paper introduces a novel algorithm to approximate the matrix with minimum nuclear norm among all matrices obeying a set of convex constraints . </S>",
    "<S> this problem may be understood as the convex relaxation of a rank minimization problem , and arises in many important applications as in the task of recovering a large matrix from a small subset of its entries ( the famous netflix problem ) . </S>",
    "<S> off - the - shelf algorithms such as interior point methods are not directly amenable to large problems of this kind with over a million unknown entries .    </S>",
    "<S> this paper develops a simple first - order and easy - to - implement algorithm that is extremely efficient at addressing problems in which the optimal solution has low rank . </S>",
    "<S> the algorithm is iterative and produces a sequence of matrices @xmath0 and at each step , mainly performs a soft - thresholding operation on the singular values of the matrix @xmath1 . </S>",
    "<S> there are two remarkable features making this attractive for low - rank matrix completion problems . </S>",
    "<S> the first is that the soft - thresholding operation is applied to a sparse matrix ; the second is that the rank of the iterates @xmath2 is empirically nondecreasing . </S>",
    "<S> both these facts allow the algorithm to make use of very minimal storage space and keep the computational cost of each iteration low . on the theoretical side </S>",
    "<S> , we provide a convergence analysis showing that the sequence of iterates converges . on the practical side , </S>",
    "<S> we provide numerical examples in which @xmath3 matrices are recovered in less than a minute on a modest desktop computer . </S>",
    "<S> we also demonstrate that our approach is amenable to very large scale problems by recovering matrices of rank about 10 with nearly a billion unknowns from just about 0.4% of their sampled entries . </S>",
    "<S> our methods are connected with the recent literature on linearized bregman iterations for @xmath4 minimization , and we develop a framework in which one can understand these algorithms in terms of well - known lagrange multiplier algorithms .    * keywords . </S>",
    "<S> * nuclear norm minimization , matrix completion , singular value thresholding , lagrange dual function , uzawa s algorithm . </S>"
  ]
}