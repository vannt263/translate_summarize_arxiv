{
  "article_text": [
    "we consider the standard framework of supervised learning , that is nonparametric regression with random design . in this setting",
    ", there is an input - output pair @xmath0 with unknown probability distribution @xmath1 , and the goal is to find a prediction function @xmath2 , based on a training set @xmath3 @xmath4 of @xmath5 independent random pairs distributed as @xmath6 .",
    "a good solution @xmath7 is such that , given a new input @xmath8 , the value @xmath9 is a good prediction of the true output @xmath10 .",
    "when choosing the square loss to measure the quality of the prediction , as we do throughout this paper , this means that the expected risk @xmath11 is _ small _ , or , in other words , that @xmath7 is a _ good _ approximation of the regression function @xmath12 minimizing this risk .    in many learning problems , a major goal besides",
    "prediction is that of _ selecting the variables _ that are _ relevant to achieve good predictions_. in the problem of variable selection we are given a set @xmath13 of functions from the input space @xmath14 into the output space @xmath15 and we aim at selecting those functions which are needed to represent the regression function , where the _ representation _ is typically given by a linear combination .",
    "the set @xmath13 is usually called _ dictionary _ and its elements _",
    "features_. we can think of the features as measurements used to represent the input data , as providing some relevant parameterization of the input space , or as a ( possibly overcomplete ) dictionary of functions used to represent the prediction function . in modern applications , the number @xmath16 of features in the dictionary is usually very large , possibly much larger that the number @xmath5 of examples in the training set .",
    "this situation is often referred to as the `` large @xmath16 , small @xmath5 paradigm '' @xcite , and a key to obtain a meaningful solution in such case is the requirement that the prediction function @xmath7 is a linear combination of only a _",
    "few _ elements in the dictionary , i.e. that @xmath7 admits a _ sparse _ representation .",
    "the above setting can be illustrated by two examples of applications we are currently working on and which provide an underlying motivation for the theoretical framework developed in the present paper .",
    "the first application is a classification problem in computer vision , namely face detection @xcite .",
    "the training set contains images of faces and non - faces and each image is represented by a very large redundant set of features capturing the local geometry of faces , for example wavelet - like dictionaries or other local descriptors .",
    "the aim is to find a good predictor able to detect faces in new images .",
    "the second application is the analysis of microarray data , where the features are the expression level measurements of the genes in a given sample or patient , and the output is either a classification label discriminating between two or more pathologies or a continuous index indicating , for example , the gravity of an illness . in this problem , besides prediction of the output for examples - to - come , another important goal is the identification of the features that are the most relevant to build the estimator and would constitute a gene signature for a certain disease @xcite . in both applications ,",
    "the number of features we have to deal with is much larger than the number of examples and assuming sparsity of the solution is a very natural requirement .",
    "the problem of variable / feature selection has a long history in statistics and it is known that the brute - force approach ( trying all possible subsets of features ) , though theoretically appealing , is computationally unfeasible . a first strategy to overcome this problem",
    "is provided by greedy algorithms . a second route , which we follow in this paper , makes use of sparsity - based regularization schemes ( convex relaxation methods ) .",
    "the most well - known example of such schemes is probably the so - called _ lasso regression _",
    "@xcite  also referred to in the signal processing literature as basis pursuit denoising @xcite  where a coefficient vector @xmath17 is estimated as the minimizer of the empirical risk penalized with the @xmath18-norm , namely @xmath19 where @xmath20 , @xmath21 is a suitable positive regularization parameter and @xmath22 a given set of features .",
    "an extension of this approach , called _ bridge regression _ , amounts to replacing the @xmath18-penalty by an @xmath23-penalty @xcite .",
    "it has been shown that this kind of penalty can still achieve sparsity when @xmath16 is bigger , but very close to @xmath24 ( see @xcite ) .",
    "for this class of techniques , both consistency and computational aspects have been studied .",
    "non - asymptotic bounds within the framework of statistical learning have been studied in several papers @xcite .",
    "a common feature of these results is that they assume that the dictionary is finite ( with cardinality possibly depending on the number of examples ) and satisfies some assumptions about the linear independence of the relevant features ",
    "see @xcite for a discussion on this point   whereas @xmath15 is usually assumed to be @xmath25 .",
    "several numerical algorithms have also been proposed to solve the optimization problem underlying lasso regression and are based e.g. on quadratic programming @xcite , on the so - called lars algorithm @xcite or on iterative soft - thresholding ( see @xcite and references therein ) .    despite of its success in many applications ,",
    "the lasso strategy has some drawback in variable selection problems where there are highly correlated features and we need to identify all the relevant ones .",
    "this situation is of uttermost importance for e.g. microarray data analysis since , as well - known , there is a lot of functional dependency between genes which are organized in small interacting networks .",
    "the identification of such groups of correlated genes involved in a specific pathology is desirable to make progress in the understanding of the underlying biological mechanisms .    motivated by microarray data analysis , zou and hastie @xcite proposed the use of a penalty which is a weighted sum of the @xmath18-norm and the square of the @xmath26-norm of the coefficient vector @xmath27 .",
    "the first term enforces the sparsity of the solution , whereas the second term ensures democracy among groups of correlated variables . in @xcite",
    "the corresponding method is called _ ( naive ) elastic net_. the method allows to select groups of correlated features when the groups are not known in advance ( algorithms to enforce group - sparsity with _",
    "preassigned _ groups of variables have been proposed in e.g. @xcite using other types of penalties ) .    in the present paper",
    "we study several properties of the elastic - net regularization scheme for vector - valued regression in a random design .",
    "in particular , we prove consistency under some adaptive and non - adaptive choices for the regularization parameter . as concerns variable selection , we assess the accuracy of our estimator for the vector @xmath27 with respect to the @xmath26-norm , whereas the prediction ability of the corresponding function @xmath28 is measured by the expected risk @xmath11 .",
    "to derive such error bounds , we characterize the solution of the variational problem underlying elastic - net regularization as the fixed point of a contractive map and , as a byproduct , we derive an explicit iterative thresholding procedure to compute the estimator . as explained below , in the presence of highly collinear features , the presence of the @xmath26-penalty ,",
    "besides enforcing grouped selection , is crucial to ensure stability with respect to random sampling .    in the remainder of this section",
    ", we define the main ingredients for elastic - net regularization within our general framework , discuss the underlying motivations for the method and then outline the main results established in the paper .    as an extension of the setting originally proposed in @xcite , we allow the dictionary to have an infinite number of features . in such case , to cope with infinite sums , we need some assumptions on the coefficients .",
    "we assume that the prediction function we have to determine is a linear combination of the features @xmath29 in the dictionary and that the series @xmath30 converges absolutely for all @xmath8 and for all sequences @xmath31 satisfying @xmath32 , where @xmath33 are given positive weights .",
    "the latter constraint can be viewed as a constraint on the _ regularity _ of the functions @xmath34 we use to approximate the regression function . for infinite - dimensional sets , as for example wavelet bases or splines , suitable choices of the weights",
    "correspond to the assumption that @xmath34 is in a sobolev space ( see section 2 for more details about this point ) .",
    "such requirement of regularity is common when dealing with infinite - dimensional spaces of functions , as it happens in approximation theory , signal analysis and inverse problems .    to ensure the convergence of the series defining @xmath34",
    ", we assume that @xmath35 notice that for finite dictionaries , the series becomes a finite sum and the previous condition as well as the introduction of weights become superfluous .    to simplify the notation and the formulation of our results , and without any loss in generality , we will in the following rescale the features by defining @xmath36 , so that on this _ rescaled dictionary _ , @xmath37 will be represented by means of a vector @xmath38 belonging to @xmath39 ; the condition then becomes @xmath40 , for all @xmath41 . from now on , we will only use this rescaled representation and we drop the tilde on the vector @xmath27 .",
    "let us now define our estimator as the minimizer of the empirical risk penalized with a ( weighted ) elastic - net penalty , that is , a combination of the squared @xmath39-norm and of a weighted @xmath18-norm of the vector @xmath27 .",
    "more precisely , we define the elastic - net penalty as follows .    given a family @xmath42 of weights @xmath43 and a parameter @xmath44 , let @xmath45 $ ] be defined as which can also be rewritten as @xmath46 , where @xmath47 .",
    "the weights @xmath48 allow us to enforce more or less sparsity on different groups of features .",
    "we assume that they are prescribed in a given problem , so that we do not need to explicitly indicate the dependence of @xmath49 on these weights .",
    "the elastic - net estimator is defined by the following minimization problem .",
    "given @xmath50 , let @xmath51 $ ] be the empirical risk penalized by the penalty @xmath49 and let @xmath52 be _ the _ or _ a _ minimizer of on @xmath39    the positive parameter @xmath21 is a regularization parameter controlling the trade - off between the empirical error and the penalty .",
    "clearly , @xmath53 also depends on the parameter @xmath54 , but we do not write explicitly this dependence since @xmath54 will always be fixed .    setting @xmath55 in",
    ", we obtain as a special case an infinite - dimensional extension of the lasso regression scheme . on the other hand , setting @xmath56 , the method reduces to @xmath26-regularized least - squares regression  also referred to as _ ridge regression _  with a generalized linear model .",
    "the @xmath18-penalty has selection capabilities since it enforces sparsity of the solution , whereas the @xmath26-penalty induces a linear shrinkage on the coefficients leading to stable solutions .",
    "the positive parameter @xmath54 controls the trade - off between the @xmath18-penalty and the @xmath26-penalty .",
    "we will show that , if @xmath57 , the minimizer @xmath53 always exists and is unique . in the paper we will focus on the case @xmath57 .",
    "some of our results , however , still hold for @xmath55 , possibly under some supplementary conditions , as will be indicated in due time .    as previously mentioned",
    "one of the main advantage of the elastic - net penalty is that it allows to achieve stability with respect to random sampling . to illustrate this property more clearly",
    ", we consider a toy example where the ( rescaled ) dictionary has only two elements @xmath58 and @xmath59 with weights @xmath60 .",
    "the effect of random sampling is particularly dramatic in the presence of highly correlated features .",
    "to illustrate this situation , we assume that @xmath58 and @xmath59 exhibit a special kind of linear dependency , namely that they are linearly dependent on the input data @xmath61 : @xmath62 for all @xmath63 , where we have parametrized the coefficient of proportionality by means of the angle @xmath64 $ ] .",
    "notice that this angle is a random variable since it depends on the input data .",
    "observe that the minimizers of must lie at a tangency point between a level set of the empirical error and a level set of the elastic - net penalty .",
    "the level sets of the empirical error are all parallel straight lines with slope @xmath65 , as depicted by a dashed line in the two panels of figure  [ fig:2d ] , whereas the level sets of the elastic - net penalty are _ elastic - net balls _",
    "( _ @xmath54-balls _ ) with center at the origin and corners at the intersections with the axes , as depicted in figure  [ fig : balls ] .        when @xmath55 , i.e. with a pure @xmath18-penalty ( lasso ) , the @xmath54-ball is simply _ a square _",
    "( dashed line in figure  [ fig : balls ] ) and we see that the unique tangency point will be the _ top corner _ if @xmath66 ( the point @xmath67 in the two panels of figure  [ fig:2d ] ) , or the _ right corner _ if @xmath68 . for @xmath69  ( that is , when @xmath58 and @xmath59 coincide on the data ) ,",
    "the minimizer of is no longer unique since the level sets will touch along an edge of the square .",
    "now , if @xmath70 randomly tilts around @xmath71 ( because of the random sampling of the input data ) , we see that the lasso estimator is not stable since it randomly jumps between the top and the right corner .",
    "if @xmath72 , i.e. with a pure @xmath26-penalty ( ridge regression ) , the @xmath54-ball becomes a disc ( dotted line in figure  [ fig : balls ] ) and the minimizer is the point of the straight line having minimal distance from the origin ( the point @xmath73 in the two panels of figure  [ fig:2d ] ) .",
    "the solution always exists , is stable under random perturbations , but it is never sparse ( if @xmath74 ) .",
    "the situation changes if we consider the elastic - net estimator with @xmath57 ( the corresponding minimizer is the point @xmath1 in the two panels of figure  [ fig:2d ] ) .",
    "the presence of the @xmath26-term ensures a smooth and stable behavior when the lasso estimator becomes unstable .",
    "more precisely , let @xmath75 be the slope of the right tangent at the top corner of the elastic - net ball ( @xmath76 ) , and @xmath77 the slope of the upper tangent at the right corner ( @xmath78 ) .",
    "as depicted in top panel of figure  [ fig:2d ] , the minimizer will be the top corner if @xmath79 .",
    "it will be the right corner if @xmath80 . in both cases",
    "the elastic - net solution is sparse . on the other hand ,",
    "if @xmath81 the minimizer has both components @xmath82 and @xmath83 different from zero  see the bottom panel of figure  [ fig:2d ] ; in particular , @xmath84 if @xmath85 .",
    "now we observe that if @xmath70 randomly tilts around @xmath71 , the solution smoothly moves between the top corner and the right corner .",
    "however , the price we paid to get such stability is a decrease in sparsity , since the solution is sparse only when @xmath86 $ ] .",
    "+     [ fig:2d ]    the previous elementary example could be refined in various ways to show the essential role played by the @xmath26-penalty to overcome the instability effects inherent to the use of the @xmath18-penalty for variable selection in a random - design setting .",
    "we now conclude this introductory section by a summary of the main results which will be derived in the core of the paper . a key result will be to show that for @xmath57 , @xmath53 is the fixed point of the following contractive map    @xmath87 where @xmath88 is a suitable relaxation constant , @xmath89 is the matrix with entries @xmath90 , @xmath91 is the vector @xmath92 ( @xmath93 denotes the scalar product in the output space @xmath15 ) .",
    "moreover , @xmath94 is the soft - thresholding operator acting componentwise as follows @xmath95_\\g=\\left\\{\\begin{array}{lccl } \\bg -\\frac{\\rp\\wg}{2 } &        \\text{\\rm if } & \\bg>\\frac{\\rp \\wg}{2 }   \\\\ 0 & \\text{\\rm if } & |\\bg|\\leq \\frac{\\rp\\wg}{2 } \\\\",
    "\\bg+\\frac{\\rp\\wg}{2 } & \\text{\\rm if } & \\bg < -\\frac{\\rp\\wg}{2 } \\end{array}\\right .. \\ ] ] as a consequence of the banach fixed point theorem , @xmath53 can be computed by means of an iterative algorithm .",
    "this procedure is completely different from the modification of the lars algorithm used in @xcite and is akin instead to the algorithm developed in @xcite .",
    "another interesting property which we will derive from the above equation is that the non - zero components of @xmath53 are such that @xmath96 , where @xmath97 is a constant depending on the data . hence the only active features are those for which the corresponding weight lies below the threshold @xmath98 . if the features are organized into finite subsets of increasing complexity ( as it happens for example for wavelets ) and the weights tend to infinity with increasing feature complexity , then the number of active features is finite and can be determined for any given data set .",
    "let us recall that in the case of ridge regression , the so - called _ representer theorem _",
    ", see @xcite , ensures that we only have to solve in practice a finite - dimensional optimization problem , even when the dictionary is infinite - dimensional ( as in kernel methods ) .",
    "this is no longer true , however , with an @xmath18-type regularization and , for practical purposes , one would need to truncate infinite dictionaries .",
    "a standard way to do this is to consider only a finite subset of @xmath99 features , with @xmath99 possibly depending on @xmath5  see for example @xcite .",
    "notice that such procedure implicitly assumes some order in the features and makes sense only if the retained features are the most relevant ones .",
    "for example , in @xcite , it is assumed that there is a natural exhaustion of the hypothesis space with nested subspaces spanned by finite - dimensional subsets of features of increasing size . in our approach",
    "we adopt a different strategy , namely the encoding of such information in the elastic - net penalty by means of suitable weights in the @xmath18-norm .    the main result of our paper concerns the consistency for variable selection of @xmath53 .",
    "we prove that , if the regularization parameter @xmath100 satisfies the conditions @xmath101 and @xmath102 , then @xmath103 where the vector @xmath104 , which we call the _ elastic - net representation of @xmath34 _ , is the minimizer of @xmath105 the vector @xmath104 exists and is unique provided that @xmath57 and the regression function @xmath106 admits a _ sparse representation on the dictionary _ , i.e. @xmath107 for at least a vector @xmath108 such that @xmath109 is finite .",
    "notice that , when the features are linearly dependent , there is a problem of identifiability since there are many vectors @xmath27 such that @xmath110 .",
    "the elastic - net regularization scheme forces @xmath111 to converge to @xmath104 .",
    "this is precisely what happens for linear inverse problems where the regularized solution converges to the minimum - norm solution of the least - squares problem . as a consequence of the above convergence result ,",
    "one easily deduces the consistency of the corresponding prediction function @xmath112 , that is , @xmath113 with probability one .",
    "when the regression function does not admit a sparse representation , we can still prove the previous consistency result for @xmath7 provided that the linear span of the features is sufficiently rich .",
    "finally , we use a data - driven choice for the regularization parameter , based on the so - called balancing principle @xcite , to obtain non - asymptotic bounds which are adaptive to the unknown regularity of the regression function .",
    "the rest of the paper is organized as follows . in section 2",
    ", we set up the mathematical framework of the problem . in section 3",
    ", we analyze the optimization problem underlying elastic - net regularization and the iterative thresholding procedure we propose to compute the estimator .",
    "finally , section 4 contains the statistical analysis with our main results concerning the estimation of the errors on our estimators as well as their consistency properties under appropriate a priori and adaptive strategies for choosing the regularization parameter .",
    "in this section we describe the general setting of the regression problem we want to solve and specify all the required assumptions .",
    "we assume that @xmath14 is a separable metric space and that @xmath15 is a ( real ) separable hilbert space , with norm and scalar product denoted respectively by @xmath114 and @xmath93 . typically , @xmath14 is a subset of @xmath115 and @xmath15 is @xmath116 .",
    "recently , however , there has been an increasing interest for vector - valued regression problems @xcite and multiple supervised learning tasks @xcite : in both settings @xmath15 is taken to be @xmath117 .",
    "also infinite - dimensional output spaces are of interest as e.g. in the problem of estimating of glycemic response during a time interval depending on the amount and type of food ; in such case , @xmath15 is the space @xmath118 or some sobolev space .",
    "other examples of applications in an infinite - dimensional setting are given in @xcite .",
    "our first assumption concerns the set of features .",
    "the family of features @xmath119 is a countable set of measurable functions @xmath120 such that @xmath121 for some finite number @xmath122 .",
    "the index set @xmath123 is countable , but we do not assume any order .",
    "as for the convergence of series , we use the notion of summability : given a family @xmath124 of vectors in a normed vector space @xmath125 , @xmath126 means that @xmath124 is summable , there is a finite subset @xmath127 such that @xmath128 for all finite subsets @xmath129 . if @xmath130 , the notion of summability is equivalent to requiring the series to converge unconditionally ( i.e. its terms can be permuted without affecting convergence ) . if the vector space is finite - dimensional , summability is equivalent to absolute convergence , but in the infinite - dimensional setting , there are summable series which are not absolutely convergent . ] with sum @xmath131 .",
    "assumption  1 can be seen as a condition on the class of functions that can be recovered by the elastic - net scheme . as already noted in the introduction , we have at our disposal an arbitrary ( countable ) dictionary @xmath22 of measurable functions , and we try to approximate @xmath106 with linear combinations @xmath132 where the set of coefficients @xmath133 satisfies some _ decay condition _ equivalent to a _ regularity condition _ on the functions @xmath34 .",
    "we make this condition precise by assuming that there exists a sequence of positive weights @xmath134 such that @xmath135 and , for any of such vectors @xmath136 , that the series defining @xmath34 converges absolutely for all @xmath8 .",
    "these two facts follow from the requirement that the set of rescaled features @xmath137 satisfies @xmath138 .",
    "condition is a little bit stronger since it requires that @xmath139 , so that we also have that the functions @xmath34 are bounded . to simplify the notation , in the rest of the paper",
    ", we only use the ( rescaled ) features @xmath140 and , with this choice , the regularity condition on the coefficients @xmath133 becomes @xmath141 .",
    "an example of features satisfying the condition is given by a family of _ rescaled _ wavelets on @xmath142 $ ] .",
    "let @xmath143 be a orthonormal wavelet basis in @xmath144)$ ] with regularity @xmath145 , @xmath146 , where for @xmath147 @xmath148 is the orthonormal wavelet basis ( with suitable boundary conditions ) spanning the detail space at level @xmath149 . to simplify notation",
    ", it is assumed that the set @xmath150 contains both the wavelets and the scaling functions at level @xmath151 .",
    "fix @xmath152 such that @xmath153 and let @xmath154 .",
    "then @xmath155 where @xmath97 is a suitable constant depending on the number of wavelets that are non - zero at a point @xmath156 $ ] for a given level @xmath149 , and on the maximum values of the scaling function and of the mother wavelet ; see @xcite for a similar setting .",
    "condition allows to define the hypothesis space in which we search for the estimator .",
    "let @xmath26 be the hilbert space of the families @xmath157 of real numbers such that @xmath141 , with the usual scalar product @xmath158 and the corresponding norm @xmath159 .",
    "we will denote by @xmath160 the canonical basis of @xmath39 and by @xmath161 the support of @xmath27 . the cauchy - schwarz inequality and the condition",
    "ensure that , for any @xmath162 , the series @xmath163 is summable in @xmath15 uniformly on @xmath14 with later on , in proposition  [ rkhs ] , we will show that the hypothesis space @xmath164 is then a vector - valued reproducing kernel hilbert space on @xmath14 with a bounded kernel @xcite , and that @xmath165 is a normalized tight frame for @xmath166 . in the example of the wavelet features one can easily check that @xmath166 is the sobolev space @xmath167 on @xmath168 $ ] and @xmath169 is equivalent to @xmath170 .",
    "the second assumption concerns the regression model .",
    "[ statistical_model ] the random couple @xmath6 in @xmath171 obeys the regression model @xmath172 where @xmath173 and @xmath174 with @xmath175 .",
    "the family @xmath176 are the positive weights defining the elastic - net penalty @xmath49 in .",
    "observe that @xmath177 is always a bounded function by .",
    "moreover the condition is a further regularity condition on the regression function and will not be needed for some of the results derived in the paper .",
    "assumption is satisfied by bounded , gaussian or sub - gaussian noise . in particular",
    ", it implies @xmath178 see @xcite , so that @xmath179 has a finite second moment .",
    "it follows that @xmath180 has a finite first moment and implies that @xmath106 is the regression function @xmath181 .",
    "condition controls both the sparsity and the regularity of the regression function . if @xmath182",
    ", it is sufficient to require that @xmath183 is finite .",
    "indeed , the hlder inequality gives that if @xmath184 , we also need @xmath185 to be finite . in the example of the ( rescaled ) wavelet features a natural choice for the weights is @xmath186 for some @xmath187 , so that @xmath188 is equivalent to the norm @xmath189 , with @xmath190 , in the besov space @xmath191 on @xmath168 $ ] ( for more details , see e.g. the appendix in @xcite ) . in such a case , is equivalent to requiring that @xmath192 .",
    "finally , our third assumption concerns the training sample .",
    "[ sample_model ] the sequence of random pairs @xmath193 are independent and identically distributed ( _ i.i.d .",
    "_ ) according to the distribution of @xmath6 .    in the following ,",
    "we let @xmath194 be the probability distribution of @xmath6 , and @xmath195 be the hilbert space of ( measurable ) functions @xmath196",
    "with the norm @xmath197 with a slight abuse of notation , we regard the random pair @xmath6 as a function on @xmath198 , that is , @xmath199 and @xmath200 . moreover ,",
    "we denote by @xmath201 the empirical distribution and by @xmath202 the corresponding ( finite - dimensional ) hilbert space with norm @xmath203      the choice of a quadratic loss function and the hilbert structure of the hypothesis space suggest to use some tools from the theory of linear operators . in particular",
    ", the function @xmath34 depends linearly on @xmath27 and can be regarded as an element of both @xmath195 and of @xmath202 .",
    "hence it defines two operators , whose properties are summarized by the next two propositions , based on the following lemma .",
    "[ rankone ] for any fixed @xmath8 , the map @xmath204 defined by @xmath205 is a hilbert - schmidt operator , its adjoint @xmath206 acts as @xmath207 in particular @xmath208 is a trace - class operator with moreover , @xmath209 is a @xmath39-valued random variable with @xmath210 and @xmath211 is a @xmath212-valued random variable with @xmath213 where @xmath212 denotes the separable hilbert space of the hilbert - schmidt operators on @xmath26 , and @xmath214 is the hilbert - schmidt norm .    clearly @xmath215 is a linear map from @xmath39 to @xmath15 . since @xmath216 , we have @xmath217 so that @xmath215 is a hilbert - schmidt operator and @xmath218 by .",
    "moreover , given @xmath10 and @xmath219 @xmath220 which is .",
    "finally , since @xmath14 and @xmath15 are separable , the map @xmath221 is measurable , then @xmath222 is a real random variable and , since @xmath39 is separable , @xmath209 is @xmath39-valued random variable with @xmath223 a similar proof holds for @xmath211 , recalling that any trace - class operator is in @xmath212 and @xmath224 .",
    "the following proposition defines the distribution - dependent operator @xmath225 as a map from @xmath26 into @xmath195 .",
    "[ lemma1 ] the map @xmath226 , defined by @xmath227 , is a hilbert - schmidt operator and @xmath228    since @xmath34 is a bounded ( measurable ) function , @xmath229 and @xmath230 hence @xmath225 is a hilbert - schmidt operator with @xmath231 so that holds .",
    "by @xmath232 has a finite second moment and by @xmath233 is a bounded function , hence @xmath234 is in @xmath195 .",
    "now for any @xmath235 we have @xmath236 on the other hand , by , @xmath209 has finite expectation , so that follows . finally , given @xmath237 @xmath238 so that is clear , since @xmath211 has finite expectation as a consequence of the fact that it is a bounded @xmath212-valued random variable .",
    "replacing @xmath194 by the empirical measure we get the sample version of the operator .",
    "[ lemma0 ] the map @xmath239 defined by @xmath240 is hilbert - schmidt operator and @xmath241    the proof of proposition  [ lemma0 ] is analogous to the proof of proposition  [ lemma1 ] , except that @xmath194 is to be replaced by @xmath242 .    by with @xmath243",
    ", we have that the matrix elements of the operator @xmath208 are @xmath244 so that @xmath89 is the empirical mean of the gram matrix of the set @xmath165 , whereas @xmath245 is the corresponding mean with respect to the distribution @xmath1 .",
    "notice that if the features are linearly dependent in @xmath202 , the matrix @xmath89 has a non - trivial kernel and hence is not invertible .",
    "more important , if @xmath123 is countably infinite , @xmath89 is a compact operator , so that its inverse ( if it exists ) is not bounded . on the contrary ,",
    "if @xmath123 is finite and @xmath165 are linearly independent in @xmath202 , then @xmath89 is invertible .",
    "a similar reasoning holds for the matrix @xmath245 . to control whether these matrices have a bounded inverse or not",
    ", we introduce a lower spectral bound @xmath246 , such that @xmath247 and , with probability @xmath24 , @xmath248 clearly we can have @xmath249 only if @xmath123 is finite and the features @xmath165 are linearly independent both in @xmath202 and @xmath195 .    on the other hand , and ( [ tracetx ] )",
    "give the crude upper spectral bounds @xmath250 one could improve these estimates by means of a tight bound on the largest eigenvalue of @xmath245 .",
    "we end this section by showing that , under the assumptions we made , a structure of reproducing kernel hilbert space emerges naturally .",
    "let us denote by @xmath251 the space of functions from @xmath14 to @xmath15 .",
    "[ rkhs ] the linear operator @xmath252 , @xmath253 , is a partial isometry from @xmath39 onto the vector - valued reproducing kernel hilbert space @xmath166 on @xmath14 , with reproducing kernel @xmath254 the null space of @xmath255 is and the family @xmath165 is a normalized tight frame in @xmath166 , namely latexmath:[\\[\\sum_{\\gg }    conversely , let @xmath166 be a vector - valued reproducing kernel hilbert space with reproducing kernel @xmath257 such that @xmath258 is a trace - class operator for all @xmath8 , with trace bounded by @xmath259 . if @xmath165 is a normalized tight frame in @xmath166 , then holds .",
    "proposition  2.4 of @xcite ( with @xmath260 , @xmath261 , @xmath262 and @xmath263 ) gives that @xmath255 is a partial isometry from @xmath39 onto the reproducing kernel hilbert space @xmath166 , with reproducing kernel @xmath264 .",
    "since @xmath255 is a partial isometry with range @xmath166 and @xmath265 where @xmath160 is a basis in @xmath39 , then @xmath165 is normalized tight frame in @xmath166 .",
    "+ to show the converse result , given @xmath41 and @xmath10 , we apply the definition of a normalized tight frame to the function @xmath266 defined by @xmath267 .",
    "@xmath266 belongs to @xmath166 by definition of a reproducing kernel hilbert space and is such that the following reproducing property holds @xmath268 for any @xmath269 .",
    "then @xmath270 is a basis in @xmath15 and @xmath8 @xmath271",
    "in this section , we study the properties of the elastic net estimator @xmath53 defined by . first of all , we characterize the minimizer of the elastic - net functional as the unique fixed point of a contractive map .",
    "moreover , we characterize some sparsity properties of the estimator and propose a natural iterative soft - thresholding algorithm to compute it . our algorithmic approach is totally different from the method proposed in @xcite , where @xmath53 is computed by first reducing the problem to the case of a pure @xmath18 penalty and then applying the lars algorithm @xcite .    in the following we make use the of the following vector notation . given a sample of @xmath5 i.i.d .",
    "observations @xmath272 , and using the operators defined in the previous section , we can rewrite the elastic - net functional as      the main difficulty in minimizing is that the functional is not differentiable because of the presence of the @xmath18-term in the penalty .",
    "nonetheless the convexity of such term enables us to use tools from subdifferential calculus .",
    "recall that , if @xmath273 is a convex functional , the subgradient at a point @xmath274 is the set of elements @xmath275 such that @xmath276 the subgradient at @xmath277 is denoted by @xmath278 , see @xcite .",
    "we compute the subgradient of the convex functional @xmath49 , using the following definition of @xmath279    we first state the following lemma .",
    "[ penalty ] the functional @xmath280 is a convex , lower semi - continuous ( l.s.c . )",
    "functional from @xmath39 into @xmath281 $ ] . given @xmath235 , a vector @xmath282 if and only if @xmath283    define the map @xmath284 $ ] @xmath285 given @xmath219 , @xmath286 is a convex , continuous function and its subgradient is @xmath287 where we used the fact that the subgradient of @xmath288 is given by @xmath279 .",
    "since @xmath289 and @xmath290 is continuous , a standard result of convex analysis @xcite ensures that @xmath280 is convex and lower semi - continuous .",
    "+ the computation of the subgradient is standard . given @xmath235 and @xmath291 , by the definition of a subgradient , @xmath292 given @xmath219 , choose @xmath293 with @xmath294 , it follows that @xmath295 belongs to the subgradient of @xmath296 , that is , conversely , if holds for all @xmath219 , by definition of a subgradient @xmath297 by summing over @xmath219 and taking into account the fact that @xmath298 , then @xmath299    to state our main result about the characterization of the minimizer of , we need to introduce the soft - thresholding function @xmath300 , @xmath301 which is defined by and the corresponding nonlinear thresholding operator @xmath302 acting componentwise as we note that the soft - thresholding operator satisfies @xmath303 these properties are immediate consequences of the fact that @xmath304 notice that with @xmath305 ensures that @xmath306 for all @xmath235 .",
    "we are ready to prove the following theorem .",
    "[ repre_n ] given @xmath44 and @xmath301 , a vector @xmath235 is a minimizer of the elastic - net functional@xmath307 if and only if it solves the nonlinear equation or , equivalently , if @xmath57 the solution always exists and is unique . if @xmath55 , @xmath249 and @xmath308 , the solution still exists and is unique .",
    "if @xmath57 the functional @xmath309 is strictly convex , finite at @xmath310 , and it is coercive by @xmath311 observing that @xmath312 is continuous and , by lemma  [ penalty ] , the elastic - net penalty is l.s.c .",
    ", then @xmath309 is l.s.c . and",
    ", since @xmath39 is reflexive , there is a unique minimizer @xmath53 in @xmath39 . if @xmath55 , @xmath309 is convex , but the fact that @xmath249 ensures that the minimizer is unique .",
    "its existence follows from the observation that @xmath313 where we used . in both cases",
    "the convexity of @xmath309 implies that @xmath27 is a minimizer if and only if @xmath314 .",
    "since @xmath312 is continuous , corollary iii.2.1 of @xcite ensures that the subgradient is linear . observing that @xmath312 is differentiable with derivative @xmath315",
    ", we get @xmath316 eq . follows taking into account the explicit form of @xmath317 , @xmath318 and @xmath91 , given by lemma  [ penalty ] and proposition  [ lemma0 ] , respectively .",
    "+ we now prove , which is equivalent to the set of equations setting @xmath319 , we have @xmath320 if and only if @xmath321 that is , @xmath322 which is equivalent to .",
    "the following corollary gives some more information about the characterization of the solution as the fixed point of a contractive map .",
    "in particular , it provides an explicit expression for the lipschitz constant of this map and it shows how it depends on the spectral properties of the empirical mean of the gram matrix and on the regularization parameter @xmath21 .    [ fixed ] let @xmath44 and @xmath301 .",
    "pick up any arbitrary @xmath323 .",
    "then @xmath27 is a minimizer of @xmath309 in @xmath39 if and only if it is a fixed point of the following lipschitz map @xmath324 , namely with the choice @xmath325 , the lipschitz constant is bounded by @xmath326 in particular , with this choice of @xmath88 and if @xmath57 or @xmath249 , @xmath327 is a contraction .",
    "clearly @xmath27 is a minimizer of @xmath309 if and only if it is a minimizer of @xmath328 , which means that , in , we can replace @xmath21 with @xmath329 , @xmath330 by @xmath331 and @xmath180 by @xmath332 .",
    "hence @xmath27 is a minimizer of @xmath309 if and only if it is a solution of @xmath333 therefore , by , @xmath27 is a minimizer of @xmath309 if and only if @xmath334 . + we show that @xmath327 is lipschitz and calculate explicitly a bound on the lipschitz constant .",
    "by assumption we have @xmath335 ; then , by the spectral theorem , @xmath336 where @xmath337 denotes the operator norm of a bounded operator on @xmath26 .",
    "hence , using , we get @xmath338 the minimum of @xmath339 with respect to @xmath88 is obtained for @xmath340 that is , @xmath341 , and , with this choice , we get @xmath342    by inspecting the proof , we notice that the choice of @xmath325 provides the best possible lipschitz constant under the assumption that @xmath343 . if @xmath57 or @xmath249 , @xmath327 is a contraction and @xmath53 can be computed by means of the banach fixed point theorem . if @xmath55 and @xmath344 , @xmath327 is only non - expansive , so that proving the convergence of the successive approximation scheme is not straightforward and @xmath344 . ] .",
    "let us now write down explicitly the iterative procedure suggested by corollary  [ fixed ] to compute @xmath53 . define the iterative scheme by @xmath345 with @xmath346 .",
    "the following corollary shows that the @xmath347 converges to @xmath53 when @xmath348 goes to infinity .",
    "[ banach ] assume that @xmath57 or @xmath249 . for any @xmath349 the following inequality holds @xmath350",
    ".    since @xmath327 is a contraction with lipschitz constant @xmath351 , the banach fixed point theorem applies and the sequence @xmath352 converges to the unique fixed point of @xmath327 , which is @xmath53 by corollary  [ fixed ] .",
    "moreover we can use the lipschitz property of @xmath327 to write @xmath353 so that we immediately get @xmath354 since @xmath355 , @xmath356 and @xmath357 .",
    "let us remark that the bound provides a natural stopping rule for the number of iterations , namely to select @xmath348 such that @xmath358 , where @xmath359 is a bound on the distance between the estimator @xmath53 and the true solution .",
    "for example , if @xmath360 is bounded by @xmath361 and if @xmath344 , the stopping rule is @xmath362    finally we notice that all previous results also hold when considering the distribution - dependent version of the method .",
    "the following proposition summarizes the results in this latter case .",
    "[ minimizer ] let @xmath44 and @xmath301 .",
    "pick up any arbitrary @xmath363 .",
    "then a vector @xmath235 is a minimizer of @xmath364 if and only if it is a fixed point of the following lipschitz map , namely if @xmath57 or @xmath249 , the minimizer is unique .",
    "if it is unique , we denote it by @xmath365 :    we add a comment . under assumption  [ statistical_model ] and the definition of @xmath104 ,",
    "the statistical model is @xmath366 where @xmath232 has zero mean , so that @xmath365 is also the minimizer of @xmath367      the results of the previous section immediately yield a crude estimate of the number and localization of the non - zero coefficients of our estimator . indeed , although the set of features could be infinite , @xmath53 has only a finite number of coefficients different from zero provided that the sequence of weights is bounded away from zero .    [ monk ]",
    "assume that the family of weights satisfies @xmath368 , then for any @xmath235 , the support of @xmath369 is finite . in particular , @xmath53 , @xmath347 and @xmath365",
    "are all finitely supported .",
    "let @xmath370 .",
    "since @xmath371 , there is a finite subset @xmath372 such that @xmath373 for all @xmath374 .",
    "this implies that @xmath375 by the definition of soft - thresholding , so that the support of @xmath369 is contained in @xmath376 .",
    "equations , and the definition of @xmath347 imply that @xmath53 , @xmath365 and @xmath347 have finite support .",
    "however , the supports of @xmath347 and @xmath53 are not known a priori and to compute @xmath347 one would need to store the infinite matrix @xmath89 .",
    "the following corollary suggests a strategy to overcome this problem .",
    "[ trunc_dic ] given @xmath44 and @xmath301 , let @xmath377 then    if @xmath378 , clearly @xmath379 is a solution of . let @xmath380 ; the definition of @xmath53 as the minimizer of yields the bound @xmath381 , so that @xmath382 hence , for all @xmath219 , the second inequality gives that @xmath383 , and we have @xmath384 and , therefore , by , @xmath385 since @xmath386 when @xmath387 , this implies that @xmath388 if @xmath389 .",
    "now , let @xmath390 be the set of indexes @xmath391 such that the corresponding feature @xmath392 for some @xmath63 .",
    "if the family of corresponding weights @xmath393 goes to infinity goes to infinity , if for all @xmath394 there exists a finite set @xmath395 such that @xmath396 , @xmath397 . ] , then @xmath398 is always finite .",
    "then , since @xmath399 , one can replace @xmath123 with @xmath398 in the definition of @xmath330 so that @xmath89 is a finite matrix and @xmath91 is a finite vector .",
    "in particular the iterative procedure given by corollary  [ fixed ] can be implemented by means of finite matrices .",
    "finally , by inspecting the proof above one sees that a similar result holds true for the distribution - dependent minimizer @xmath365 .",
    "its support is always finite , as already noticed , and moreover is included in the following set @xmath400",
    "in this section we provide an error analysis for the elastic - net regularization scheme . our primary goal is the _ variable selection problem _ , so that we need to control the error @xmath401 , where @xmath402 is a suitable choice of the regularization parameter as a function of the data , and @xmath27 is an explanatory vector encoding the features that are relevant to reconstruct the regression function @xmath106 , that is , such that @xmath403 .",
    "although assumption implies that the above equation has at least a solution @xmath404 with @xmath405 , nonetheless , the operator @xmath225 is injective only if @xmath406 is @xmath39-linearly independent in @xmath195 . as usually done for inverse problems , to restore uniqueness we choose , among all the vectors @xmath27 such that @xmath403 , the vector @xmath104 which is the minimizer of the elastic - net penalty .",
    "the vector @xmath104 can be regarded as the _ best _ representation of the regression function @xmath106 according to the elastic - net penalty and we call it the _ elastic - net representation_. clearly this representation will depend on @xmath54 .",
    "next we focus on the following error decomposition ( for any fixed positive @xmath21 ) , where @xmath365 is given by .",
    "the first error term in the right - hand side of the above inequality is due to finite sampling and will be referred to as the _ sample error _ , whereas the second error term is deterministic and is called the _",
    "approximation error_. in section [ sec : sample_apprx ] we analyze the sample error via concentration inequalities and we consider the behavior of the approximation error as a function of the regularization parameter @xmath21 .",
    "the analysis of these error terms leads us to discuss the choice of @xmath21 and to derive statistical consistency results for elastic - net regularization . in section [ sec : param ] we discuss a priori and a posteriori ( adaptive ) parameter choices .",
    "the following proposition provides a way to define a unique solution of the equation @xmath403 .",
    "let @xmath407 where @xmath108 is given by in assumption  [ statistical_model ] and @xmath408    [ ora ] if @xmath57 or @xmath249 , there is a unique @xmath409 such that    if @xmath249 , @xmath410 reduces to a single point , so that there is nothing to prove . if @xmath57 , @xmath410 is a closed subset of a reflexive space .",
    "moreover , by lemma  [ penalty ] , the penalty @xmath280 is strictly convex , l.s.c .",
    "and , by of assumption  [ statistical_model ] , there exists at least one @xmath411 such that @xmath412 is finite .",
    "since @xmath413 , @xmath280 is coercive .",
    "a standard result of convex analysis implies that the minimizer exists and is unique .      the main result of this section is a probabilistic error estimate for @xmath414 , which will provide a choice @xmath415 for the regularization parameter as well as a convergence result for @xmath416 .",
    "we first need to establish two lemmas .",
    "the first one shows that the sample error can be studied in terms of the following quantities measuring the perturbation due to random sampling and noise ( we recall that @xmath417 denotes the hilbert - schmidt norm of a hilbert - schmidt operator on @xmath39 ) .",
    "the second lemma provides suitable probabilistic estimates for these quantities .",
    "let @xmath44 and @xmath301 . if @xmath57 or @xmath249 , then @xmath418    let @xmath325 and recall that @xmath53 and @xmath365 satisfy and , respectively .",
    "taking into account we get @xmath419 by assumption [ statistical_model ] and the definition of @xmath104 , @xmath420 , and @xmath421 and @xmath422 both coincide with the function @xmath106 , regarded as an element of @xmath195 and @xmath202 respectively . moreover by @xmath423 , so that @xmath424 moreover @xmath425 from the assumption on @xmath89 and the choice @xmath426 , we have @xmath427 , so that gives @xmath428 the bound is established by observing that @xmath429 .    the probabilistic estimates for are straightforward consequences of the law of large numbers for vector - valued random variables .",
    "more precisely , we recall the following probabilistic inequalities based on a result of @xcite ; see also th .",
    "3.3.4 of @xcite and @xcite for concentration inequalities for hilbert - space - valued random variables .",
    "[ pine ] let @xmath430 be a sequence of i.i.d .",
    "zero - mean random variables taking values in a real separable hilbert space @xmath166 and satisfying where @xmath431 and @xmath432 are two positive constants .",
    "then , for all @xmath433 and @xmath434 where @xmath435 or , for all @xmath436 ,    bound is given in @xcite with a wrong factor , see @xcite . to show , observe that the inverse of the function @xmath437 is the function @xmath438 so that the equation @xmath439 has the solution @xmath440    [ noises_est ] with probability greater than @xmath441 , the following inequalities hold , for any @xmath442 and @xmath57 , and    consider the @xmath26 random variable @xmath443 . from , @xmath444 and , for any @xmath445 , @xmath446 due to and . applying with @xmath447 and @xmath448 , and recalling the definition",
    ", we get that @xmath449 with probability greater than @xmath450 . + consider the random variable @xmath451 taking values in the hilbert space of hilbert - schmidt operators ( where @xmath214 denotes the hilbert - schmidt norm ) .",
    "one has that @xmath452 and , by @xmath453 hence @xmath454 by @xmath455 . applying with @xmath456 @xmath457 with probability greater than @xmath450 .",
    "the simplified bounds are clear provided that @xmath458 .    in both and , the condition @xmath458 allows to simplify the bounds enlightening the dependence on @xmath5 and the confidence level @xmath459 . in the following results we always assume that @xmath458 , but we stress the fact that this condition is only needed to simplify the form of the bounds .",
    "moreover , observe that , for a fixed confidence level , this requirement on @xmath5 is very weak  for example , to achieve a @xmath460 confidence level , we only need to require that @xmath461 .",
    "the next proposition gives a bound on the sample error .",
    "this bound is uniform in the regularization parameter @xmath21 in the sense that there exists an event independent of @xmath21 such that its probability is greater than @xmath462 and holds true .",
    "[ sample ] assume that @xmath57 or @xmath249 .",
    "let @xmath436 and @xmath463 such that @xmath458 , for any @xmath301 the bound holds with probability greater than @xmath462 , where @xmath464 .",
    "plug bounds and in , taking into account that @xmath465    by inspecting the proof , one sees that the constant @xmath466 in can be replaced by any constant @xmath467 such that @xmath468 where @xmath398 is the set of _ active features _ given by corollary  [ trunc_dic ] .",
    "if @xmath344 and @xmath469 , which means that @xmath398 is finite and the active features are linearly independent , one can improve the bound below . since we mainly focus on the case of linearly dependent dictionaries we will not discuss this point any further .",
    "+ the following proposition shows that the approximation error @xmath470 tends to zero when @xmath21 tends to zero .",
    "[ appr - prop ] if @xmath57 then @xmath471    it is enough to prove the result for an arbitrary sequence @xmath472 converging to 0 .",
    "putting @xmath473 , since @xmath474 , by the definition of @xmath475 as the minimizer of and the fact that @xmath104 solves @xmath476 , we get @xmath477 condition of assumption  1 ensures that @xmath478 is finite , so that @xmath479 since @xmath57 , the last inequality implies that @xmath480 is a bounded sequence in @xmath39 .",
    "hence , possibly passing to a subsequence , @xmath480 converges weakly to some @xmath481 .",
    "we claim that @xmath482 . since @xmath483 is l.s.c . @xmath484 that is @xmath485 . since @xmath280 is l.s.c .",
    ", @xmath486 by the definition of @xmath104 , it follows that @xmath482 and , hence , to prove that @xmath475 converges to @xmath104 in @xmath39 , it is enough to show that @xmath487 . since @xmath159 is l.s.c .",
    ", @xmath488 .",
    "hence we are left to prove that @xmath489 .",
    "assume the contrary .",
    "this implies that , possibly passing to a subsequence , @xmath490 and , using , @xmath491 however , since @xmath492 is l.s.c .",
    "@xmath493    from and the triangular inequality , we easily deduce that @xmath494 with probability greater that @xmath495 .",
    "since the tails are exponential , the above bound and the borel - cantelli lemma imply the following theorem , which states that the estimator @xmath53 converges to the generalized solution @xmath104 , for a suitable choice of the regularization parameter @xmath21 .",
    "[ teo_main ] assume that @xmath57 and @xmath344 .",
    "let @xmath402 be a choice of @xmath21 as a function of @xmath5 such that @xmath101 and @xmath496 .",
    "then @xmath497 if @xmath249 , the above convergence result holds for any choice of @xmath402 such that @xmath101 .",
    "the only nontrivial statement concerns the convergence with probability @xmath24 .",
    "we give the proof only for @xmath344 , being the other one similar .",
    "let @xmath498 be a sequence such that @xmath101 and @xmath496 .",
    "since @xmath499 , proposition  [ appr - prop ] ensures that @xmath500 .",
    "hence , it is enough to show that @xmath501 with probability 1 .",
    "let @xmath502 , which is finite since the approximation error goes to zero if @xmath503 tends to zero . given @xmath434 ,",
    "let @xmath504 for @xmath5 large enough , so that the bound holds providing that @xmath505 the condition that @xmath496 implies that the series @xmath506 converges and the borel - cantelli lemma gives the thesis .",
    "the two conditions on @xmath402 in the above theorem are clearly satisfied with the choice @xmath507 with @xmath508 . moreover , by inspecting the proof , one can easily check that to have the convergence of @xmath111 to @xmath104 in probability , it is enough to require that @xmath101 and @xmath509 .",
    "let @xmath510 .",
    "since @xmath511 and @xmath512 , the above theorem implies that @xmath513 with probability 1 , that is , the consistency of the elastic - net regularization scheme with respect to the square loss .",
    "let us remark that we are also able to prove such consistency without assuming in assumption  @xmath514 .",
    "to this aim we need the following lemma , which is of interest by itself .    instead of assumption",
    "@xmath514 , assume that the regression model is given by @xmath515 where @xmath516 is a bounded function and @xmath179 satisfies@xmath517 and@xmath518 . for fixed @xmath503 and @xmath57 , with probability greater than @xmath519",
    "we have where @xmath520 and @xmath521 .",
    "we notice that in , the function @xmath522 is regarded both as an element of @xmath202 and as an element of @xmath195 .",
    "consider the @xmath39-valued random variable @xmath523 a simple computation shows that @xmath524 and @xmath525 hence , for any @xmath526 , @xmath527 applying with @xmath528 and @xmath529 , we obtain the bound .",
    "observe that under assumption and by the definition of @xmath104 one has that @xmath530 , so that becomes @xmath531 since @xmath225 is a compact operator this bound is tighter than the one deduced from .",
    "however , the price we pay is that the bound does not hold uniformly in @xmath21 .",
    "we are now able to state the universal strong consistency of the elastic - net regularization scheme .",
    "assume that @xmath6 satisfy and that the regression function @xmath532 is bounded .",
    "if the linear span of features @xmath165 is dense in @xmath195 and @xmath57 , then @xmath533 provided that @xmath534 and @xmath496 .",
    "as above we bound separately the approximation error and the sample error .",
    "as for the first term , let @xmath535 .",
    "we claim that @xmath536 goes to zero when @xmath21 goes to zero . given @xmath434 , the fact that the linear span of the features @xmath165 is dense in @xmath195 implies that there is @xmath537 such that @xmath538 and @xmath539 let @xmath540 , then , for any @xmath541 , @xmath542 as for the sample error",
    ", we let @xmath543 ( so that @xmath544 ) and observe that @xmath545 we bound @xmath414 by observing that @xmath546 where @xmath547 is a suitable constant and where we used the crude estimate @xmath548 hence yields @xmath549 observe that the proof of does not depend on the existence of @xmath104 provided that we replace both @xmath550 and @xmath551 with @xmath106 , and we take into account that both @xmath552 and @xmath553 are equal to @xmath554 . hence , plugging and in we have that with probability greater than @xmath555 @xmath556 where @xmath547 is a suitable constant and @xmath458 .",
    "the thesis now follows by combining the bounds on the sample and approximation errors and repeating the proof of theorem  [ teo_main ] .    to have an explicit convergence rate",
    ", one needs a explicit bound on the approximation error @xmath557 , for example of the form @xmath558 .",
    "this is out of the scope of the paper .",
    "we report only the following simple result .",
    "assume that the features @xmath140 are in finite number and linearly independent .",
    "let @xmath559 and @xmath560 , then @xmath561 with the choice @xmath562 , for any @xmath436 and @xmath463 with @xmath458 @xmath563 with probability greater than @xmath462 , where @xmath564 and @xmath464 .",
    "observe that the assumption on the set of features is equivalent to assume that @xmath249 .",
    "first , we bound the approximation error @xmath557 .",
    "as usual , with the choice @xmath325 , eq . gives @xmath565- \\frac{\\eps\\rp}{\\tau+\\eps\\rp } \\beg.\\end{aligned}\\ ] ] property implies that @xmath566 since @xmath567 , @xmath568 and @xmath569 one has @xmath570 the bound is then an straightforward consequence of .",
    "let us observe this bound is weaker than the results obtained in @xcite since the constant @xmath466 is a _ global _ property of the dictionary , whereas the constants in @xcite are _",
    "local_.      in this section , we suggest an adaptive choice of the regularization parameter @xmath21 .",
    "the main advantage of this selection rule is that it does not require any knowledge of the behavior of the approximation error . to this aim , it is useful to replace the approximation error with the following upper bound @xmath571 the following simple result holds .",
    "[ decr ] given @xmath57 , @xmath572 is an increasing continuous function and @xmath573    first of all , we show that @xmath574 is a continuous function . fixed @xmath301 , for any @xmath575 such that @xmath576 , eq . with @xmath325 and corollary  [ fixed ] give @xmath577 where @xmath578 does not depend on @xmath575 and we wrote @xmath579 to make explicit the dependence of the map @xmath580 on the regularization parameter .",
    "hence @xmath581 the claim follows by observing that ( assuming for simplicity that @xmath582 ) @xmath583 which goes to zero if @xmath575 tends to zero .",
    "+   now , by the definition of @xmath365 and @xmath104 @xmath584 so that @xmath585 hence @xmath586 for all @xmath21 . clearly @xmath587 is an increasing function of @xmath21 ; the fact that @xmath557 is continuous and goes to zero with @xmath21 ensures that the same holds true for @xmath587 .",
    "notice that we replaced the approximation error with @xmath587 just for a technical reason , namely to deal with an increasing function of @xmath21 .",
    "if we have a monotonic decay rate at our disposal , such as @xmath588 for some @xmath589 and for @xmath590 , then clearly @xmath591 .",
    "now , we fix @xmath57 and @xmath592 and we assume that @xmath344",
    ". then we simplify the bound observing that @xmath593 where @xmath594 ; the bound holds with probability greater than @xmath462 uniformly for all @xmath301 .",
    "+ when @xmath21 increases , the first term in decreases whereas the second increases ; hence to have a tight bound a _ natural _ choice of the parameter consists in balancing the two terms in the above bound , namely in taking @xmath5950,\\infty[\\ \\mid \\app{\\rp}=\\frac{1}{\\sqrt{n}\\eps\\rp}}.\\ ] ] since @xmath587 is continuous , @xmath596 and the resulting bound is @xmath597 this method for choosing the regularization parameter clearly requires the knowledge of the approximation error . to overcome this drawback , we discuss a data - driven choice for @xmath21 that allows to achieve the rate requiring any prior information on @xmath587 .",
    "for this reason , such choice is said to be _",
    "adaptive_. the procedure we present is also referred to as an _ a posteriori _ choice since it depends on the given sample and not only on its cardinality @xmath5 .",
    "in other words , the method is purely data - driven .",
    "let us consider a discrete set of values for @xmath21 defined by the geometric sequence @xmath598 notice that we may replace the sequence @xmath599 be any other geometric sequence @xmath600 with @xmath601 ; this would only lead to a more complicated constant in .",
    "define the parameter @xmath602 as follows ( with the convention that @xmath603 ) .",
    "this strategy for choosing @xmath604 is inspired by a procedure originally proposed in @xcite for gaussian white noise regression and which has been widely discussed in the context of deterministic as well as stochastic inverse problems ( see @xcite ) . in the context of nonparametric regression from random design , this strategy has been considered in @xcite and the following proposition is a simple corollary of a result contained in @xcite .    provided that @xmath605 , the following bound holds with probability greater than @xmath462 @xmath606    the proposition results from theorem  2 in @xcite . for completeness , we report here a proof adapted to our setting .",
    "let @xmath607 be the event such that holds for any @xmath301 ; we have that @xmath608\\geq 1 -4 e^{-\\delta}$ ] and we fix a sample point in @xmath607 .",
    "+ the definition of @xmath609 and the assumption @xmath605 ensure that @xmath610 .",
    "hence the set @xmath611 is not empty and we can define @xmath612 the fact that @xmath613 is a geometric sequence implies that @xmath614 while with the definition of @xmath615 ensures that @xmath616 we show that @xmath617 . indeed , for any @xmath618 , using twice , we get @xmath619 where the last inequality holds since @xmath620 and @xmath621 for all @xmath622 . now @xmath623 for some @xmath624 , so that @xmath625 finally , recalling and , we get the bound : @xmath626    notice that the _ a priori _",
    "condition @xmath627 is satisfied , for example , if @xmath628 .    to illustrate the implications of the last proposition ,",
    "let us suppose that for some unknown @xmath6290,1]$ ] .",
    "one has then that @xmath630 and @xmath631 .",
    "we end noting that , if we specialize our analysis to least squares regularized with a pure @xmath26-penalty ( i.e. setting @xmath632 , @xmath633 ) , then our results lead to the error estimate in the norm of the reproducing kernel space @xmath166 obtained in @xcite . indeed , in such a case , @xmath104 is the generalized solution @xmath634 of the equation @xmath476 and the approximation error satisfies under the a priori assumption that the regression vector @xmath634 is in the range of @xmath635 for some @xmath636 ( the fractional power makes sense since @xmath245 is a positive operator ) . under this assumption",
    ", it follows that @xmath637 . to compare this bound with the results in the literature , recall that both @xmath638 and @xmath639 belongs to the reproducing kernel hilbert space @xmath166 defined in proposition  [ rkhs ] .",
    "in particular , one can check that @xmath640 if and only if @xmath641 , where @xmath642 is the integral operator whose kernel is the reproducing kernel @xmath257 @xcite . under this condition ,",
    "the following bound holds @xmath643 which gives the same rate as in theorem  2 of @xcite and corollary  17 of @xcite .",
    "we thank alessandro verri for helpful suggestions and discussions .",
    "christine de mol acknowledges support by the `` action de recherche concerte '' nb 02/07 - 281 , the vub - goa 62 grant and the national bank of belgium bnb ; she is also grateful to the disi , universit di genova for hospitality during a semester in which the present work was initiated .",
    "ernesto de vito and lorenzo rosasco have been partially supported by the firb project rbin04parl and by the the eu integrated project health - e - child ist-2004 - 027749 .",
    "a.  argyriou , t.  evgeniou , and m.  pontil .",
    "multi - task feature learning . in b.",
    "schlkopf , j.  platt , and t.  hoffman , editors , _ advances in neural information processing systems 19 _ , pages 4148 . mit press , cambridge ,",
    "ma , 2007 .",
    "l.  baldassarre , b.  gianesin , a.  barla , and m.  marinelli . a statistical learning approach to liver iron overload estimation .",
    "technical report disi - tr-08 - 13 , disi - universit di genova ( italy ) , 2008 .",
    "preprint available at http://slipguru.disi.unige.it/downloads/publications/disi-tr-08-13.pdf .",
    "a.  barla , s.  mosci , l.  rosasco , and a.  verri .",
    "a method for robust variable selection with significance assessment . in _",
    "esann 2008 _ , 2008 .",
    "preprint available at http://www.disi.unige.it/person/moscis/papers/esann.pdf .                        c.  de  mol , s.  mosci , m.  traskine , and a.  verri . a regularized method for selecting nested groups of relevant genes from microarray data .",
    "technical report disi - tr-07 - 04b , disi - universit di genova ( italy ) , 2007 .",
    "preprint available at http://www.disi.unige.it/person/moscis/papers/tr0704b.pdf ."
  ],
  "abstract_text": [
    "<S> within the framework of statistical learning theory we analyze in detail the so - called elastic - net regularization scheme proposed by zou and hastie @xcite for the selection of groups of correlated variables . to investigate on the statistical properties of this scheme and in particular on its consistency properties , we set up a suitable mathematical framework . </S>",
    "<S> our setting is random - design regression where we allow the response variable to be vector - valued and we consider prediction functions which are linear combination of elements ( _ features _ ) in an infinite - dimensional dictionary . under the assumption that the regression function admits a sparse representation on the dictionary </S>",
    "<S> , we prove that there exists a particular `` _ elastic - net representation _ '' of the regression function such that , if the number of data increases , the elastic - net estimator is consistent not only for prediction but also for variable / feature selection . </S>",
    "<S> our results include finite - sample bounds and an adaptive scheme to select the regularization parameter . </S>",
    "<S> moreover , using convex analysis tools , we derive an iterative thresholding algorithm for computing the elastic - net solution which is different from the optimization procedure originally proposed in @xcite . </S>"
  ]
}