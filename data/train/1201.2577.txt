{
  "article_text": [
    "let @xmath0 be i.i.d . zero mean vectors with unknown covariance matrix @xmath1 .",
    "our objective is to estimate the unknown covariance matrix @xmath2 when the vectors @xmath3 are partially observed , that is , when some of their components are not observed .",
    "more precisely , we consider the following framework .",
    "denote by @xmath4 the @xmath5__-th _ _ component of the vector @xmath6 .",
    "we assume that each component @xmath4 is observed independently of the others with probability @xmath7 $ ] .",
    "note that @xmath8 can be easily estimated by the proportion of observed entries .",
    "therefore , we will assume in this paper that @xmath8 is known .",
    "note also that the case @xmath9 corresponds to the standard case of fully observed vectors .",
    "let @xmath10 be a sequence of i.i.d .",
    "bernoulli random variables with parameter @xmath8 and independent from @xmath3 .",
    "we observe @xmath11 i.i.d .",
    "random vectors @xmath12 whose components satisfy @xmath13 we can think of the @xmath14 as masked variables . if @xmath15 , then we can not observe the @xmath5__-th _ _ component of @xmath6 and the default value @xmath16 is assigned to @xmath17 .",
    "our goal is then to estimate @xmath2 given the partial observations @xmath18 .",
    "the statistical problem of covariance estimation with missing observations is fundamental in multivariate statistics since it is often used as the first step to retrieve information in numerous applications where datasets with missing observations are common :    1 .",
    "climate studies : @xmath11 is the number of time points and @xmath19 the number of observations stations , which may sometimes fail to produce an observation due to break down of measure instruments . as a consequence , the generated datasets usually contain missing values .",
    "gene expression micro - arrays : @xmath11 is the number of measurements and @xmath19 the number of tested genes . despite the improvement of genes expression techniques ,",
    "the generated datasets frequently contain missing values with up to @xmath20 of genes affected .",
    "cosmology : @xmath11 is the number of images produced by a telescope and @xmath19 is the number of pixels per image . with the development of very large telescopes and wide sky surveys ,",
    "the generated datasets are huge but usually contain missing observations due to partial sky coverage or defective pixel .",
    "one simple strategy to deal with missing data is to exclude from the analysis any variable for which observations are missing , thus restricting the analysis to a subset of fully observed variables . in gene expression data where @xmath20 of the genes are affected by missing values",
    ", we would be left with too few variables so that the legitimacy of the statistical analysis becomes questionable .",
    "also , discarding variables with very few missing observations is a waste of available information .",
    "existing procedures involve complex imputation techniques to fill in the missing values through computationally intensive implementation of the em algorithm , see @xcite and the references cited therein for more details . in this paper",
    ", we propose a simple procedure computationally tractable in high - dimension that does not require to imput missing observations or to discard any available observation to recover the covariance matrix @xmath2 .",
    "contemporary datasets are huge with both large sample size @xmath11 and dimension @xmath19 and typically @xmath21 .",
    "consequently , a question of considerable practical interest is to perform dimension reduction , that is finding a good low - dimensional approximation for these huge datasets .",
    "this recent paradigm where high - dimensional objects of interest admit in fact a small intrinsic dimension has produced spectacular results in several fields , such as compressed sensing where it is possible to recover @xmath22-sparse vectors of dimension @xmath19 with only @xmath23 measurements provided these measurements are carried out properly , see @xcite and the references cited therein for more details .",
    "an analogous result holds in matrix completion where recovery of a low - rank matrix @xmath24 via nuclear norm minimization is possible with as few as @xmath25 observed entries where @xmath26 is the rank of @xmath27 , provided the matrix of interest @xmath27 satisfies some incoherence condition , see @xcite for more details . see also @xcite for rank minimization approach .",
    "a popular dimension reduction technique for covariance matrices is principal component analysis ( pca ) , which exploits the spectrum of the sample covariance matrix . in the high - dimensional setting",
    ", @xcite showed that the standard pca procedure is bound to fail since the sample covariance spectrum is too spread out .",
    "several alternatives have been studied in the literature to provide better estimates of the covariance matrix in the high - dimensional setting . a popular approach in gaussian graphical models",
    "consists in estimating the inverse of the covariance matrix ( called concentration matrix ) since it admits a naturally sparse ( or approximately sparse ) structure if the dependence graph is itself sparse .",
    "see @xcite and the references cited therein for more details .",
    "a limitation of this approach is that it does not apply to low rank matrices @xmath2 since the concentration matrix does not exist in this case .",
    "an other popular approach assumes that the unknown covariance matrix is sparse , that is most of the entries are exactly or approximately zero and then proposes to perform either entrywise thresholding or tapering of the sample covariance matrix @xcite .",
    "note that the sparsity notion adopted in this approach is not adapted to strongly correlated datasets with dense covariance matrix .    in random matrix theory , an important line of work , @xcite and the references cited therein studied the asymptotic distribution of the sample covariance matrix eigenvalues for different settings of @xmath11 and @xmath19 .",
    "see also @xcite for a very nice survey of existing non - asymptotic results on the spectral norm deviation of the sample covariance matrix from its population counterpart . in this paper",
    ", we adopt this approach and we will provide further details as we present our results .    note that the results derived in the works cited above do not cover datasets with missing observations .",
    "for instance , when the data contains no missing observation ( @xmath9 ) , @xcite established a non - asymptotic control on the stochastic deviation @xmath28 of the empirical covariance matrix @xmath29 provided some tails conditions are satisfied by the common distribution of @xmath3 . exploiting these results",
    ", it is possible to establish oracle inequalities for the covariance version of the matrix lasso estimator @xmath30 where @xmath31 is the set of @xmath32 positive - semidefinite symmetric matrices , @xmath33 and @xmath34 are respectively the frobenius and nuclear norm of @xmath35 and @xmath36 is a regularization parameter that should be chosen of the order of magnitude of @xmath28 .",
    "this estimator is the covariance version of the matrix lasso estimator initially introduced in the matrix regression framework , see @xcite and the references cited therein . to the best of our knowledge , the procedure ( [ matrixlasso ] ) has not been studied in the covariance estimation problem .",
    "when the data contains missing observations ( @xmath37 ) , we no longer have access to @xmath38 .",
    "given the observations @xmath18 , we can build the following empirical covariance matrix @xmath39 in this case , a naive approach to derive oracle inequalities consists in computing the matrix lasso estimator ( [ matrixlasso ] ) with @xmath38 replaced by @xmath40 .",
    "unfortunately this approach is bound to fail since @xmath40 is not a good estimator of @xmath2 when @xmath37 .",
    "indeed , some elementary algebra gives that @xmath41 with @xmath42 where @xmath43 is the @xmath32 diagonal matrix obtained by putting all the non - diagonal entries of @xmath2 to zero .",
    "when @xmath9 , we see that @xmath44 and @xmath45 .",
    "however , when observations are missing ( @xmath37 ) , @xmath46 can be very far from @xmath2 .",
    "hence , @xmath47 will be a poor estimator of @xmath2 since it concentrates around its mean @xmath46 under suitable tail conditions on the distribution of @xmath48 .",
    "consequently , the stochastic deviation @xmath49 will be too large and the matrix lasso estimator ( [ matrixlasso ] ) with @xmath38 replaced by @xmath40 , which requires @xmath50 to be of the order of magnitude of @xmath49 , will perform poorly since its rate of estimation grows with @xmath50 .",
    "we present now our reconstruction procedure based on the following simple observation @xmath51 therefore , we can define the following unbiased estimator of @xmath52 when the data set contains missing observations @xmath53 our estimator is then solution of the following penalized empirical risk minimization problem : @xmath54 where @xmath36 is a regularization parameter to be tuned properly .",
    "we note that this simple procedure can be computed efficiently in high - dimension since @xmath55 is solution of a convex minimization problem .",
    "the optimal choice of the tuning parameter @xmath50 is of the order of magnitude of the stochastic deviation @xmath56 .",
    "therefore , in order to order to establish sharp oracle inequalities for ( [ nuclearnormest ] ) , we need first to study the deviations of @xmath56 .",
    "this analysis is more difficult as compared to the study of @xmath28 since we need to derive the sharp scaling of @xmath57 with @xmath8 .",
    "the rest of the paper is organized as follows . in section [ tools ] , we recall some tools and definitions . in section [ oracle ] , we establish oracle inequalities for the frobenius and spectral norms for our procedure ( [ nuclearnormest ] ) and also propose a data - driven choice of the regularization parameter . in section",
    "[ lower ] , we establish minimax lower bounds for data with missing observations @xmath7 $ ] , thus showing that our procedures are minimax optimal up to a logarithmic factor . finally , section [ proof ] contains all the proofs of the paper .",
    "we emphasize that the results of this paper are non - asymptotic in nature , hold true for any setting of @xmath58 , are minimax optimal ( up to a logarithmic factor ) and do not require the unknown covariance matrix @xmath2 to be low - rank .",
    "we note also that to the best of our knowledge , there exists in the literature no minimax lower bound result for statistical problem with missing observations .",
    "the @xmath59-norms of a vector @xmath60 is given by @xmath61    denote by @xmath62 the set of @xmath32 symmetric positive - semidefinite matrices .",
    "any matrix @xmath63 admits the following spectral representation @xmath64 where @xmath65 is the rank of @xmath27 , @xmath66 are the nonzero eigenvalues of @xmath27 and @xmath67 are the associated orthonormal eigenvectors ( we also set @xmath68 ) .",
    "the linear vector space @xmath69 is the linear span of @xmath70 and is called support of @xmath27 .",
    "we will denote respectively by @xmath71 and @xmath72 the orthogonal projections onto @xmath69 and @xmath73 .",
    "the schatten @xmath74-norm of @xmath63 is defined by @xmath75 note that the trace of any @xmath76 satisfies @xmath77 .",
    "recall the _ trace duality _ property : @xmath78    we will also use the fact that the subdifferential of the convex function @xmath79 is the following set of matrices : @xmath80 ( cf .",
    "@xcite ) .",
    "we recall now the definition and some basic properties of sub - exponential random vectors .",
    "the @xmath81-norms of a real - valued random variable @xmath82 are defined by @xmath83 we say that a random variable @xmath82 with values in @xmath84 is sub - exponential if @xmath85 for some @xmath86 . if @xmath87 , we say that @xmath82 is sub - gaussian .",
    "we recall some well - known properties of sub - exponential random variables :    1 .",
    "for any real - valued random variable @xmath82 such that @xmath88 for some @xmath89 , we have @xmath90 where @xmath91 can depend only on @xmath92 .",
    "2 .   if a real - valued random variable @xmath82 is sub - gaussian , then @xmath93 is sub - exponential .",
    "indeed , we have @xmath94    a random vector @xmath95 is sub - exponential if @xmath96 are sub - exponential random variables for all @xmath97 .",
    "the @xmath81-norms of a random vector @xmath48 are defined by @xmath98    we recall the bernstein inequality for sub - exponential real - valued random variables ( see for instance corollary 5.17 in @xcite )    [ bernstein ] let @xmath18 be independent centered sub - exponential random variables , and @xmath99 .",
    "then for every @xmath100 , we have with probability at least @xmath101 @xmath102 where @xmath91 is an absolute constant .",
    "the following proposition is the matrix version of bernstein s inequality for bounded random matrices @xcite ( see also corollary 9.1 in @xcite ) .",
    "[ prop : bernstein_bounded ] let @xmath103 be symmetric independent random matrices in @xmath104 that satisfy @xmath105 and @xmath106 almost surely for some constant @xmath107 and all @xmath108 . define @xmath109 then , for all @xmath110 with probability at least @xmath101 we have @xmath111",
    "we can now state the main result for the procedure ( [ nuclearnormest ] ) .",
    "[ theomain1 ] let @xmath3 be i.i.d .",
    "vectors in @xmath112 with covariance matrix @xmath2 . for any @xmath113",
    ", we have on the event @xmath114 @xmath115 and @xmath116    as we see in theorem [ theomain1 ] , the regularization parameter @xmath50 should be chosen sufficiently large such that the condition @xmath114 holds with probability close to @xmath117 .",
    "the optimal choice of @xmath50 depends on the unknown distribution of the observations .",
    "we consider now the case of sub - gaussian random vector @xmath118 .",
    "[ assumption1 ] the random vector @xmath95 is sub - gaussian , that is @xmath119 .",
    "in addition , there exist a numerical constant @xmath120 such that @xmath121    note that gaussian distributions satisfy assumption [ assumption1 ] . under the above condition",
    ", we can study the stochastic quantity @xmath122 and thus properly tune the regularization parameter @xmath50 .",
    "the intrinsic dimension of the matrix @xmath2 can be measured by the effective rank @xmath123 see section 5.4.3 in @xcite .",
    "note that we always have @xmath124 .",
    "in addition , we can possibly have @xmath125 for approximately low - rank matrices @xmath2 , that is matrices @xmath2 with large rank but concentrated around a low - dimensional subspace .",
    "consider for instance the covariance matrix @xmath2 with eigenvalues @xmath126 and @xmath127 , then @xmath128    we have the following result , which requires no condition on the covariance matrix @xmath2 .    [ lem1 ]",
    "let @xmath129 be i.i.d .",
    "random vectors satisfying assumption [ assumption1 ] .",
    "let @xmath18 be defined in ( [ equationy ] ) with @xmath130 $ ] .",
    "then , for any @xmath131 , we have with probability at least @xmath101 @xmath132 and @xmath133 where @xmath91 is an absolute constant .    1",
    ".   the natural choice for @xmath134 is of the order of magnitude @xmath135 .",
    "then the conclusions of proposition [ lem1 ] hold true with probability at least @xmath136 .",
    "in addition , if the number of measurements @xmath11 is sufficiently large @xmath137 where @xmath138 is a sufficiently large numerical constant , then an acceptable choice for the regularization parameter @xmath50 is @xmath139 where the absolute constant @xmath91 is sufficiently large .",
    "2 .   as we claimed in the introduction , proposition [ lem1 ]",
    "requires no condition on @xmath2 whatsoever .",
    "however , for the result to be of any practical interest , we need the bound in ( [ prop1-bound-1 ] ) to be small , which is the case if the condition ( [ measurements ] ) is satisfied .",
    "this condition is interesting since it shows that the number of measurements sufficient to guarantee a precise enough estimation of the spectrum of @xmath2 grows with the effective rank @xmath140 . in particular , when no observation is missing ( @xmath141 ) ,",
    "if @xmath142 is approximately low - rank so that @xmath143 , then only @xmath144 measurements are sufficient to estimate precisely the spectrum of the @xmath32 covariance matrix @xmath2 .",
    "3 .   note that if we assume that @xmath145 a.s .",
    "for some constant @xmath146 , then we can eliminate the @xmath147 factor in ( [ prop1-bound-1 ] ) .",
    "consequently , we can replace the condition ( [ measurements ] ) on the number of measurements by the following less restrictive one @xmath148 for some absolute constant @xmath138 sufficiently large .",
    "when there is no missing observation ( @xmath9 ) , we obtain the standard condition on the number of measurements ( see remark 5.53 in @xcite ) .",
    "when some observations are missing ( @xmath37 ) , we have the additional quantity @xmath149 in the denominators of ( [ prop1-bound-1 ] ) and ( [ measurements ] ) . the bound ( [ prop1-bound-1 ] )",
    "is degraded in the case @xmath37 since we observe less entries per measurement .",
    "consequently , as we can see it in ( [ measurements ] ) , if we denote by @xmath150 the number of necessary measurements to estimate @xmath2 with a precision @xmath151 when no observation is missing ( @xmath141 ) , then we will need at least @xmath152 measurements in order to estimate @xmath2 with the same precision @xmath151 when some observations are missing ( @xmath37 ) . in theorem [ theomain2 ] , we prove in particular that the dependence of the bound ( [ prop1-bound-1 ] ) on @xmath8 is sharp by establishing a minimax lower bound .",
    "4 .   in the full observations case ( @xmath9 ) and for sub - gaussian distributions with low rank covariance matrix @xmath2 , a simple modification of the @xmath151-net argument used in @xcite to prove theorem 5.39 yields an inequality similar to ( [ prop1-bound-1 ] ) with an upper bound of the order @xmath153 without any logarithmic factor @xmath154 . note",
    "however that this bound is suboptimal when @xmath155 ( cf the discussion below assumption [ assumption1 ] on the intrinsic dimension of a matrix ) .",
    "in addition , in the missing observations framework @xmath37 , the matrix @xmath46 can have full rank even if the matrix @xmath2 is low rank .",
    "therefore the @xmath151-net argument will yield an upper bound of the order @xmath156 which is much larger than the bound derived in ( [ prop1-bound-1 ] ) .",
    "proposition [ lem1 ] and equation ( [ lambdaoptimal ] ) give some insight on the tuning of the regularization parameter : @xmath157 where @xmath91 is a sufficiently large absolute constant .",
    "we see that this choice of @xmath50 depends on @xmath158 and @xmath159 which are typically unknown",
    ". therefore we propose to use instead @xmath160 where @xmath91 is a large enough constant .",
    "note that the above choice of @xmath50 does not depend on the unknown quantities @xmath159 or @xmath158 and constitutes thus an interesting choice in practice .",
    "we prove in the next lemma that @xmath161 with probability at least @xmath162 .",
    "[ lem - lambda - datadriven ] let the assumptions of proposition [ lem1 ] be satisfied",
    ". assume in addition that ( [ measurements ] ) holds true .",
    "take @xmath50 as in ( [ lambdapractice ] ) with @xmath91 a large enough constant that can depend only on @xmath163 .",
    "then , we have with probability at least @xmath136 that @xmath164 where @xmath165 can depend only on @xmath163 .",
    "we obtain the following corollary of theorem [ theomain1 ] .",
    "[ cor22 ] let assumption [ assumption1 ] be satisfied .",
    "assume that ( [ measurements ] ) is satisfied .",
    "consider the estimator ( [ nuclearnormest ] ) with the regularization parameter @xmath50 satisfying ( [ lambdapractice ] ) .",
    "then we have , with probability at least @xmath136 that @xmath166 and @xmath167 where @xmath168 can depend only on @xmath163 .",
    "the proof of this corollary is immediate by combining theorem [ theomain1 ] with proposition [ lem1 ] and lemma [ lem - lambda - datadriven ] .",
    "for any integer @xmath169 , define @xmath170 we also introduce @xmath171 the class of probability distributions on @xmath112 with covariance matrix @xmath172 .    we now establish a minimax lower bound that guarantees the rates we obtained in corollary [ cor22 ] are optimal up to a logarithmic factor on the probability distribution class @xmath171 .",
    "in particular , the dependence of our rates on @xmath8 , @xmath159 and @xmath140 is sharp .",
    "[ theomain2 ] fix @xmath130 $ ] .",
    "let @xmath173 be integers such that @xmath174 .",
    "let @xmath3 be i.i.d .",
    "random vectors in @xmath112 with covariance matrix @xmath172 .",
    "we observe @xmath11 i.i.d .",
    "random vectors @xmath175 such that @xmath176 where @xmath177 is an i.i.d .",
    "sequence of bernoulli @xmath178 random variables independent of @xmath3 .",
    "then , there exist absolute constants @xmath179 and @xmath138 such that @xmath180 and @xmath181 where @xmath182 denotes the infimum over all possible estimators @xmath183 of @xmath2 based on @xmath18 .",
    "the proof of the first inequality adapts to covariance matrix estimation the arguments used in the trace regression problem to prove theorems 1 and 11 in @xcite .    by definition of @xmath184",
    ", we have for any @xmath185 @xmath186 if @xmath187 , we deduce from the previous display that @xmath188 next , a necessary and sufficient condition of minimum for problem ( [ nuclearnormest ] ) implies that there exists @xmath189 such that for all @xmath76 @xmath190 for any @xmath76 of rank @xmath26 with spectral representation @xmath191 and support @xmath69 , it follows from ( [ kkt ] ) that @xmath192 for an arbitrary @xmath193 .",
    "note that @xmath194 by monotonicity of subdifferentials of convex functions and that the following representation holds @xmath195 where @xmath196 is an arbitrary matrix with @xmath197 .",
    "in particular , there exists @xmath196 with @xmath197 such that @xmath198 for this choice of @xmath196 , we get from ( [ th1-interm1 ] ) that @xmath199 where we have used the following facts @xmath200 and @xmath201 for any @xmath24 define @xmath202 . set @xmath203 .",
    "we have @xmath204 using cauchy - schwarz s inequality and trace duality , we get @xmath205 the above display combined with ( [ th1-interm2 ] ) give @xmath206 a decoupling argument gives @xmath207 finally , we get on the event @xmath208 that @xmath209    we now prove the spectral norm bound .",
    "note first that the solution of ( [ nuclearnormest ] ) is given by @xmath210 where @xmath211 and @xmath212 admits the spectral representation @xmath213 with positive eigenvalues @xmath214 and orthonormal eigenvectors @xmath215 .",
    "indeed , the solution of ( [ nuclearnormest ] ) is unique since the functional @xmath216 is strictly convex",
    ". a sufficient condition of minimum is @xmath217 with @xmath218 .",
    "we consider the following choice of @xmath219 with @xmath220 it is easy to check that @xmath221 .",
    "next , we have on the event @xmath208 @xmath222      the delicate part of this proof is to obtain the sharp dependence on @xmath8 . as a consequence ,",
    "the proof is significantly more technical as compared to the case of full observations @xmath9 . to simplify the understanding of this proof , we decomposed it into three lemmas that we prove below .",
    "define @xmath223    we have @xmath224 now combining a simple union bound argument with lemmas [ lem3 ] , [ lem4 ] and [ lem5 ] , we get with probability at least @xmath225 that @xmath226 and @xmath227,\\end{aligned}\\ ] ] where @xmath91 is an absolute constant .",
    "noting finally that @xmath228 and @xmath229 , we can conclude , up to a rescaling of the absolute constant @xmath91 , that ( [ prop1-bound-1 ] ) and ( [ prop1-bound-2 ] ) hold true simultaneously with probability at least @xmath101 .",
    "[ lem3 ] under the assumptions of proposition [ lem1 ] , we have with probability at least @xmath101 that @xmath230 where @xmath91 is an absolute constant .",
    "we have @xmath231 next , since the random variables @xmath14 and @xmath232 are sub - gaussian for any @xmath233 , we have @xmath234 where we have used assumption 1 in the last inequality .",
    "we can apply bernstein s inequality ( see proposition [ bernstein ] in the appendix below ) to get for any @xmath235 with probability at least @xmath236 that @xmath237 where @xmath91 is an absolute constant .",
    "next , taking @xmath238 combined with a union bound argument we get the result .",
    "[ lem4 ] under the assumptions of proposition [ lem1 ] , we have with probability at least @xmath239 that @xmath240 where @xmath91 is a large enough absolute constant .    we have @xmath241 where @xmath242 define @xmath243 where @xmath244 are i.i.d .",
    "bernoulli random variables with parameter @xmath8 independent from @xmath48 and @xmath245 we want to apply the noncommutative bernstein inequality for matrices . to this end",
    ", we need to study the quantities @xmath246 and @xmath247 .",
    "we note first that @xmath248 .",
    "next , we set @xmath249 and @xmath250",
    ". some easy algebra yields that @xmath251 we now treat @xmath252 and @xmath253 separately .",
    "denote by @xmath254 and @xmath255 the expectations w.r.t . @xmath256 and @xmath48 respectively .",
    "we have @xmath257 .",
    "next , we have @xmath258 consequently , we get for any @xmath259 with @xmath260 that @xmath261\\right)\\notag\\\\ & \\leq \\delta^2 \\sqrt{\\mathbb e_x |x|_2 ^ 4 } \\left (   \\delta \\sqrt{\\mathbb e_x ( x^{\\top}u)^4 } + ( 1-\\delta ) \\sum_{j=1}^p \\sqrt{\\mathbb e_x ( x^{(j)})^4}u_j^2 \\right),\\end{aligned}\\ ] ] where we have applied cauchy - schwarz s inequality .",
    "we have again by cauchy - schwarz s inequality and assumption [ assumption1 ] that @xmath262^{1/2}\\left[\\mathbb e ( x^{(k)})^4\\right]^{1/2}\\\\    & \\leq & \\left(\\sum_{j=1}^p \\sqrt{\\mathbb e ( x^{(j)})^4}\\right)^2\\\\    & \\leq & c\\left(\\sum_{j=1}^p \\|x^{(j)}\\|_{\\psi_2}^2\\right)^2\\\\    & \\leq & cc_1^{-2}\\left(\\mathrm{tr}(\\sigma)\\right)^2,\\end{aligned}\\ ] ] for some absolute constant @xmath91 .",
    "we have also , in view of ( [ subexp1 ] ) , with the same absolute constant @xmath263 as above @xmath264 and @xmath265    combining the three above displays with ( [ interm - lem4 - 2 ] ) , we get @xmath266\\notag\\\\ & \\leq c c_1^{-2}\\delta^2 \\mathrm{tr}(\\sigma)\\|\\sigma\\|_\\infty,\\end{aligned}\\ ] ] and @xmath267 combining the two above displays with ( [ interm - lem4 - 1 ] ) , we get @xmath268 where @xmath91 is an absolute constant .    next , we treat @xmath269 .",
    "we have @xmath270 where we have used that @xmath271    in view of assumption [ assumption1 ] , we have @xmath272    then , combining proposition [ bernstein ] with a union bound argument gives for any @xmath131 @xmath273 where @xmath91 is an absolute constant .",
    "define @xmath274 and @xmath275 where @xmath165 is a large enough absolute constant .",
    "we have @xmath276 where we have used proposition [ prop : bernstein_bounded ] to get that @xmath277    [ lem5 ] under the assumptions of proposition [ lem1 ] , we have with probability at least @xmath101 that @xmath278 where @xmath91 is an absolute constant .    in view of assumption [ assumption1 ]",
    ", we have for any @xmath279 that @xmath280 and @xmath281    next , we have @xmath282 next , we have @xmath283 then we can apply proposition [ bernstein ] to get the result .      in view of proposition [ lem1 ]",
    ", we have on an event @xmath284 of probability at least @xmath136 that @xmath285 we assume further that ( [ measurements ] ) is satisfied with a sufficiently large constant @xmath286 so that we have , in view of ( [ prop1-bound-1 ] ) and ( [ prop1-bound-2 ] ) , on the same event @xmath284 that @xmath287 and @xmath288 we immediately get on the event @xmath284 that @xmath289 and @xmath290 combining these simple facts with ( [ lem - lambda - datadriven - interm1 ] ) , we get the result .",
    "this proof uses standard tools of the minimax theory ( cf .",
    "for instance @xcite ) . however , as for proposition [ lem1 ] , the proof with missing observations ( @xmath37 ) is significantly more technical as compared to case of full observations ( @xmath9 ) .",
    "in particular , the control of the kullback - leibler divergence requires a precise description of the conditional distributions of the random variables @xmath291 given the masked variables @xmath292 . to our knowledge",
    ", there exists no minimax lower bound result for statistical problem with missing observations in the literature .",
    "set @xmath293 where @xmath294 is a sufficiently small absolute constant .",
    "we consider first the case @xmath295 .",
    "define @xmath296 set @xmath297 for any @xmath298 .",
    "consider the associated set of symmetric matrices @xmath299 note that any matrix @xmath300 is positive - semidefinite if @xmath301 since we have by assumption @xmath302    by construction , any element of @xmath303 as well as the difference of any two elements of @xmath304 is of rank exactly @xmath26 .",
    "consequently , @xmath305 since @xmath306 for any @xmath307 .",
    "note also that for any @xmath308 , we have @xmath309 and @xmath310 provided that @xmath301 and consequently @xmath311 .",
    "indeed , we have @xmath312 in view of the condition @xmath313 .",
    "a similar reasoning gives the lower bound .",
    "denote by @xmath314 the @xmath32 block matrix with first block equal to @xmath315 .",
    "varshamov - gilbert s bound ( cf .",
    "lemma 2.9 in @xcite ) guarantees the existence of a subset @xmath316 with cardinality @xmath317 containing @xmath314 and such that , for any two distinct elements @xmath318 and @xmath319 of @xmath320 , we have @xmath321    let @xmath129 be i.i.d .",
    "@xmath322 with @xmath323 . for the sake of brevity , we set @xmath324 .",
    "recall that @xmath292 are random vectors in @xmath112 whose entries @xmath14 are i.i.d .",
    "bernoulli entries with parameter @xmath8 independent from @xmath325 and that the observations @xmath18 satisfy @xmath326 . denote by @xmath327 the distribution of @xmath328 and by @xmath329 the conditional distribution of @xmath328 given @xmath330 .",
    "next , we note that , for any @xmath331 , the conditional random variables @xmath332 are independent gaussian vectors @xmath333 where @xmath334",
    "thus , we have @xmath335 .",
    "denote respectively by @xmath336 and @xmath337 the probability distribution of @xmath338 and the associated expectation , and by @xmath339 the expectation w.r.t @xmath340 for any @xmath331 .",
    "we also denote by @xmath341 and @xmath342 the expectation and conditional expectation associated respectively with @xmath343 and @xmath344 .",
    "next , the kullback - leibler divergences @xmath345 between @xmath346 and @xmath347 satisfies @xmath348    using that @xmath349 with @xmath350 defined in ( [ proof - theo2-interm1 ] ) , we get for any @xmath331 , any @xmath351 and any realization @xmath352 that    1 .",
    "@xmath353 and hence @xmath354 .",
    "@xmath355 and @xmath356 are supported on a @xmath357-dimensional subspace of @xmath112 where @xmath358 .",
    "define @xmath359 .",
    "define the mapping @xmath360 as follows @xmath361 where for any @xmath362 , @xmath363 is obtained by keeping only the components @xmath364 with their index @xmath365 .",
    "we denote by @xmath366 the right inverse of @xmath367 .",
    "we note that @xmath368 and @xmath369    thus we get that @xmath370 denote by @xmath371 the eigenvalues of @xmath372 .",
    "note that @xmath373 for any @xmath374 in view of ( [ spectraltest ] ) if @xmath375 .",
    "we get , using the inequality @xmath376 for any @xmath377 , that @xmath378 taking the expectation w.r.t . to @xmath340 in the above display , we get for any @xmath331 that @xmath379 since @xmath380 . combining the above display with ( [ proof - theo2-interm2 ] )",
    ", we get @xmath381 thus , we deduce from the above display that the condition @xmath382 is satisfied for any @xmath383 if @xmath294 is chosen as a sufficiently small numerical constant depending on @xmath92 . in view of ( [ lower_2 ] ) and ( [ eq : condition c ] ) , ( [ eq : lower1 ] ) now follows by application of theorem 2.5 in @xcite .",
    "the lower bound ( [ eq : lower2 ] ) follows from ( [ eq : lower1 ] ) by the following simple argument .",
    "consider the set of matrices @xmath320 .",
    "for any two distinct matrices @xmath384 of @xmath320 , we have @xmath385 indeed , if ( [ eq : lower_spectral_1 ] ) does not hold , we get @xmath386 since @xmath387 by construction of @xmath320",
    ". this contradicts ( [ lower_2 ] ) .",
    "next , ( [ eq : condition c ] ) is satisfied for any @xmath383 if @xmath388 is chosen as a sufficiently small numerical constant depending on @xmath92 .    combining ( [ eq : lower_spectral_1 ] ) with ( [ eq : condition c ] ) and theorem 2.5 in @xcite gives the result .",
    "the case @xmath389 can be treated similarly and is actually easier .",
    "indeed if @xmath390 , then we have @xmath391 and @xmath392 .",
    "consequently , we can derive the lower bound by testing between the two hypothesis @xmath393 where @xmath394 and @xmath395 are @xmath32 covariance matrices with only one nonzero component on the first diagonal entry .",
    "for these covariance matrices , we have @xmath396 and @xmath397 .",
    "thus we have @xmath398 for some absolute constant @xmath138 .",
    "the rest of the proof is identical to the case @xmath295 .",
    "i wish to thank professor vladimir koltchinskii for suggesting this problem and the observation ( [ sigmarecons ] ) ."
  ],
  "abstract_text": [
    "<S> in this paper , we study the problem of high - dimensional approximately low - rank covariance matrix estimation with missing observations . we propose a simple procedure computationally tractable in high - dimension and that does not require imputation of the missing data . </S>",
    "<S> we establish non - asymptotic sparsity oracle inequalities for the estimation of the covariance matrix with the frobenius and spectral norms , valid for any setting of the sample size and the dimension of the observations . </S>",
    "<S> we further establish minimax lower bounds showing that our rates are minimax optimal up to a logarithmic factor . </S>"
  ]
}