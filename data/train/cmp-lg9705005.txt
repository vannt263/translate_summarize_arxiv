{
  "article_text": [
    "we are concerned here with the issue of classifying documents into categories .",
    "more precisely , we begin with a number of categories ( e.g. , ` tennis , soccer , skiing ' ) , each already containing certain documents . our goal is to determine into which categories newly given documents ought to be assigned , and to do so on the basis of the distribution of each document s words .",
    "many methods have been proposed to address this issue , and a number of them have proved to be quite effective ( e.g.,@xcite ) .",
    "the simple method of conducting hypothesis testing over word - based distributions in categories ( defined in section 2 ) is not efficient in storage and suffers from the _ data sparseness problem _ , i.e. , the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them . in order to address this difficulty ,",
    "@xcite have proposed using distributions based on what we refer to as _ hard clustering _ of words , i.e. , in which a word is assigned to a single cluster and words in the same cluster are treated uniformly .",
    "the use of hard clustering might , however , degrade classification results , since the distributions it employs are not always precise enough for representing the differences between categories .",
    "we propose here to employ _ soft clustering _ ,",
    "i.e. , a word can be assigned to several different clusters and each cluster is characterized by a specific word probability distribution .",
    "we define for each category a _ finite mixture model _ , which is a linear combination of the word probability distributions of the clusters .",
    "we thereby treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models . in order to accomplish hypothesis testing",
    ", we employ the em algorithm to efficiently and approximately calculate from training data the maximum likelihood estimates of parameters in a finite mixture model .",
    "our method overcomes the major drawbacks of the method using word - based distributions and the method based on hard clustering , while retaining their merits ; it in fact includes those two methods as special cases .",
    "experimental results indicate that our method outperforms them .",
    "although the finite mixture model has already been used elsewhere in natural language processing ( e.g. @xcite ) , this is the first work , to the best of knowledge , that uses it in the context of document classification .",
    "a simple approach to document classification is to view this problem as that of conducting hypothesis testing over word - based distributions . in this paper",
    ", we refer to this approach as the _ word - based method _",
    "( hereafter , referred to as wbm ) .    letting @xmath0 denote a vocabulary ( a set of words ) , and @xmath1 denote a random variable representing any word in it , for each category @xmath2 @xmath3",
    ", we define its _ word - based distribution _ @xmath4 as a histogram type of distribution over @xmath0 .",
    "( the number of free parameters of such a distribution is thus @xmath5 ) .",
    "wbm then views a document as a sequence of words : @xmath6 and assumes that each word is generated independently according to a probability distribution of a category .",
    "it then calculates the probability of a document with respect to a category as @xmath7 and classifies the document into that category for which the calculated probability is the largest .",
    "we should note here that a document s probability with respect to each category is equivalent to the _ likelihood _ of each category with respect to the document , and to classify the document into the category for which it has the largest probability is equivalent to classifying it into the category having the largest likelihood with respect to it .",
    "hereafter , we will use only the term likelihood and denote it as @xmath8 .",
    "notice that in practice the parameters in a distribution must be estimated from training data . in the case of wbm ,",
    "the number of parameters is large ; the training data size , however , is usually not sufficiently large for accurately estimating them .",
    "this is the _ data sparseness problem _ that so often stands in the way of reliable statistical language processing ( e.g.@xcite ) .",
    "moreover , the number of parameters in word - based distributions is too large to be efficiently stored .      in order to address the above difficulty , guthrie et.al .",
    "have proposed a method based on hard clustering of words @xcite ( hereafter we will refer to this method as hcm ) .",
    "let @xmath9 be categories .",
    "hcm first conducts hard clustering of words .",
    "specifically , it ( a ) defines a vocabulary as a set of words @xmath0 and defines as clusters its subsets @xmath10 satisfying @xmath11 and @xmath12 @xmath13 ( i.e. , each word is assigned only to a single cluster ) ; and ( b ) treats uniformly all the words assigned to the same cluster .",
    "hcm then defines for each category @xmath2 a distribution of the clusters @xmath14 @xmath15 .",
    "it replaces each word @xmath16 in the document with the cluster @xmath17 to which it belongs @xmath18 .",
    "it assumes that a cluster @xmath17 is distributed according to @xmath14 and calculates the likelihood of each category @xmath2 with respect to the document by @xmath19    .frequencies of words [ cols=\"<,^,^,^,^,^,^\",options=\"header \" , ]     we considered the following questions :    \\(1 ) the training data used in the experimentation may be considered sparse .",
    "will a word - clustering - based method ( fmm ) outperform a word - based method ( wbm ) here ?",
    "\\(2 ) is it better to conduct soft clustering ( fmm ) than to do hard clustering ( hcm ) ?",
    "\\(3 ) with our current method of creating clusters , as the threshold @xmath20 approaches 0 , fmm behaves much like wbm and it does not enjoy the effects of clustering at all ( the number of parameters is as large as in wbm ) .",
    "this is because in this case ( a ) a word will be assigned into all of the clusters , ( b ) the distribution of words in each cluster will approach that in the corresponding category in wbm , and ( c ) the likelihood value for each category will approach that in wbm ( recall case ( 2 ) in section 3 ) .",
    "since creating clusters in an optimal way is difficult , when clustering does not improve performance we can at least make fmm perform as well as wbm by choosing @xmath21 .",
    "the question now is `` does fmm perform better than wbm when @xmath20 is @xmath22 ? ''    in looking into these issues",
    ", we found the following :    \\(1 ) when @xmath23 , i.e. , when we conduct clustering , fmm does not perform better than wbm for the first data set , but it performs better than wbm for the second data set .    evaluating classification results on the basis of each individual category , we have found that for three of the nine categories in the first data set , fmm0.5 performs best , and that in two of the ten categories in the second data set fmm0.5 performs best .",
    "these results indicate that clustering sometimes does improve classification results _ when we use our current way of creating clusters_. ( fig .",
    "[ fig : corn ] shows the best result for each method for the category ` corn ' in the first data set and fig .",
    "[ fig : grain ] that for ` grain ' in the second data set . )",
    "\\(2 ) when @xmath23 , i.e. , when we conduct clustering , the best of fmm almost always outperforms that of hcm .",
    "\\(3 ) when @xmath24 , fmm performs better than wbm for the first data set , and that it performs as well as wbm for the second data set .    in summary , fmm always outperforms hcm ; in some cases it performs better than wbm ; and in general it performs at least as well as wbm .",
    "8.2 cm    8.2 cm    for both data sets , the best fmm results are superior to those of cos throughout .",
    "this indicates that the probabilistic approach is more suitable than the cosine approach for document classification based on word distributions .",
    "although we have not completed our experiments on the entire reuters data set , we found that the results with fmm on the second data set are almost as good as those obtained by the other approaches reported in @xcite .",
    "( the results are not directly comparable , because ( a ) the results in @xcite were obtained from an older version of the reuters data ; and ( b ) they used stop words , but we did not . )    we have also conducted experiments on the susanne corpus data and confirmed the effectiveness of our method .",
    "we omit an explanation of this work here due to space limitations .",
    "let us conclude this paper with the following remarks :    1 .",
    "the primary contribution of this research is that we have proposed the use of the finite mixture model in document classification .",
    "experimental results indicate that our method of using the finite mixture model outperforms the method based on hard clustering of words .",
    "experimental results also indicate that in some cases our method outperforms the word - based method _ when we use our current method of creating clusters_.    our future work is to include :    1 .   comparing the various methods over the entire reuters corpus and over other data bases , 2 .   developing better ways of creating clusters .    our proposed method is not limited to document classification ; it can also be applied to other natural language processing tasks , like word sense disambiguation , in which we can view the context surrounding a ambiguous target word as a document and the word - senses to be resolved as categories .",
    "we are grateful to tomoyuki fujita of nec for his constant encouragement .",
    "we also thank naoki abe of nec for his important suggestions , and mark petersen of meiji univ . for his help with the english of this text .",
    "we would like to express special appreciation to the six acl anonymous reviewers who have provided many valuable comments and criticisms ."
  ],
  "abstract_text": [
    "<S> we propose a new method of classifying documents into categories . we define for each category a _ finite mixture model _ based on _ soft clustering _ of words . </S>",
    "<S> we treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models , and employ the em algorithm to efficiently estimate parameters in a finite mixture model . </S>",
    "<S> experimental results indicate that our method outperforms existing methods . </S>"
  ]
}