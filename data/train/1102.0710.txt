{
  "article_text": [
    "we consider the problem of communicating over an unknown and arbitrarily varying channel , with the help of feedback .",
    "we would like to minimize the assumptions on the communication channel as much as possible , while using the feedback link to learn the channel .",
    "the main questions with respect to such channels are how to define the expected communication rates , and how to attain them universally , without channel knowledge .    the traditional models for unknown channels @xcite are compound channels , in which the channel law is selected arbitrarily out of a family of known channels , and arbitrarily varying channels ( avc s ) , in which a sequence of channel states is selected arbitrarily .",
    "the well known results for these models @xcite do not assume adaptation .",
    "therefore , the avc capacity , which is the supremum of the communication rates that can be obtained with vanishing error probability over any possible occurrence of the channel state sequence , is in essence a worst - case result .",
    "for example , if one assumes that @xmath0 , the channel output at time @xmath1 , is determined by the probability law @xmath2 where @xmath3 is the channel input , and @xmath4 is an arbitrary sequence of conditional distributions , clearly no positive rate can be guaranteed a - priori , as it may happen that all @xmath4 have zero capacity , and therefore the avc capacity is zero",
    ". this capacity may be non - zero only if a constraint on @xmath4 is defined . in this paper",
    "we use the term `` arbitrarily varying channel '' in a loose manner , to describe any kind of unknown and arbitrary change of the channel over time , and the acronym `` avc '' to refer to the traditional model @xcite .",
    "other communication models , which allow positive communication rates over such avc s were proposed by the authors and others @xcite .",
    "although the channel models considered in these papers are different , the common feature distinguishing them from the traditional avc setting is that the communication rate is adaptively modified using feedback .",
    "the target rate is known only a - posteriori , and is gradually learned throughout the communication process . by adapting the rate , one avoids worst case assumptions on the channel , and can achieve positive communication rates when the channel is good .",
    "however , in the aforementioned communication models , the distribution of the transmitted signal is fixed and independent of the feedback , and only the rate is adapted . specifically in the `` individual channel '' model @xcite for reasons explained therein , the distribution of the channel input is fixed to a predefined prior .",
    "likewise , eswaran  @xcite show that for a fixed prior , the mutual information of the averaged channel can be attained .",
    "clearly , with this limitation these systems are incapable of universally attaining the channel capacity in many cases of interest .",
    "for example , consider even the simple case where the channel is a compound memoryless channel , i.e. the conditional distributions @xmath5 are all constant but unknown .    in the last paper @xcite ,",
    "the problem of universal communication was formulated as that of a competition against a reference system , comprised of an encoder and a decoder with limited capabilities . for the case where the channel is modulo - additive with an individual , arbitrary noise sequence ,",
    "it was shown possible to asymptotically perform at least as well as any finite - block system ( which may be designed knowing the noise sequence ) , without prior knowledge of the noise sequence . however",
    ", this result crucially relies on the property of the modulo - additive channel , that the capacity achieving prior is the uniform i.i.d .",
    "prior for any noise distribution . to extend the result to more general models",
    ", we would like to be able to adapt the input behavior .",
    "the key parameter to be adapted is the `` prior '' , i.e. the distribution of the codebook ( or equivalently the channel input ) , since it plays a vital role in the converse as well as the attainability proof of channel capacity and is the main factor in adapting the message to the channel @xcite .    in a crude way",
    "we may say that previous works achieve various kinds of `` mutual information '' for a fixed prior and any channel from a wide class , by mainly solving problems of universal decoding and rate adaptation . however to obtain more than the `` mutual information '' , i.e. the `` capacity ''",
    ", one would need to select the prior in a universal way .",
    "prior adaptation using feedback is a well known practice for static or semi - static channels .",
    "two familiar examples are bit and power loading performed in digital subscriber lines ( dsl - s ) @xcite , and precoding for in multi - antenna systems @xcite which is performed in practice in wireless standards such as wifi , wimax and lte . if the channel can be assumed to be static for a period of time sufficient to close a loop of channel measurement , feedback and coding , then an input prior close to the optimal one can be chosen . in the theoretical setting of the compound memoryless channel where @xmath6 , where @xmath7 is unknown but fixed , a system with feedback can asymptotically attain the channel capacity of @xmath7 , without prior knowledge of it , by using an asymptotically small portion of the transmission time to estimate the channel , and using an estimate of the optimal prior and the suitable rate during the rest of the time @xcite .",
    "all models for prior adaptation that we are aware of , use the assumption that the knowledge of the channel at a given time yields non trivial statistical information about future channel states , but do not deal with arbitrary variation .",
    "the question that we deal with in this paper is : assuming a channel which is _ arbitrarily _ changing over time , is there any merit in using feedback to adapt the input distribution , and what rates can be guaranteed ? as a target , we would have liked to consider the most general variation of the channel ( as in the unknown vector channel model @xcite ) , however to start our exploration , we focus on channel models which are memoryless in the input , i.e. whose behavior at a certain time does not depend on any previous channel _ inputs_.",
    "the most general model that does not include memory of the input is that of an unknown sequence of memoryless channels ( which is in essence an avc without constraints ) and this is the main model considered in this paper .",
    "the motivation for avoiding memory of the input can be appreciated by considering the negative examples in @xcite .",
    "we now give a brief overview of the structure and the results of this paper . in section  [ sec : problem_statement_and_notation ] we state the problem , and define several communication rates ( as a function of the channel sequence ) that would be of interest . in order to focus thoughts on questions related to the problem of determining the _ prior _ , we initially adopt an abstract model of the communication system , stripping off the details of communication , such as decoding , channel estimation , overheads , error probability , etc .",
    "we begin by presenting an easier synthetic problem , in which all previous channels are known ( section  [ sec : toy_problem ] ) .",
    "this problem may represent a channel which changes its behavior in a block - wise manner and remains i.i.d .",
    "memoryless during each block ( a subset of the original problem ) .",
    "this problem is related to standard prediction problems ( section  [ sec : toy_categorization ] ) , and used as a tool to gain insight into the prediction problem involved , present bounds on what can be achieved universally , and develop the techniques that will be used later on .",
    "furthermore , we show that even for this easier problem there is no hope to attain the channel capacity universally and we would have to settle for lower rates ( section  [ sec : regret_lb ] ) .",
    "the attained rate is the maximum over the prior , of the averaged mutual information ( theorem  [ theorem : prior_predictor_exp ] ) . in section  [ sec : arbitrary_channel_var ] , we return to the main problem , and show that the rate that can be attained when the past channel is not known , but is estimated from the output , is lower .",
    "we focus on the capacity of the time - averaged channel .",
    "we show this rate is the best achievable rate that does not depend on the order of the channel sequence ( theorem  [ theorem : c_overlinew_optimality ] ) , and present the main result showing that this rate is indeed achievable ( theorem  [ theorem : c_overlinew_achievability ] ) .",
    "furthermore , this rate meets or exceeds the avc capacity , and essentially equals the `` empirical capacity '' defined by eswaran  @xcite .",
    "we present a scheme based on rateless coding and combines a prior predictor that attains this rate . in section  [ sec : arbitrary_var_exp_prior_predictor ] , the prior predictor is developed under abstract assumptions regarding the channel estimation and decoding rate . in section  [ sec :",
    "mainproof ] , we present and analyze the full communication system and prove the main result .",
    "finally , section  [ sec : discussion ] is devoted to discussion and comments .",
    "we denote random variables by capital letters and vectors by boldface . however for probabilities",
    "which are sometimes treated as vectors we use regular capital letters .",
    "we apply superscript and subscript indices to vectors to define subsequences in the standard way , i.e. @xmath8 , @xmath9    @xmath10 denotes the mutual information obtained when using a prior @xmath11 over a channel @xmath7 , i.e. it is the mutual information @xmath12 between two random variables with the joint probability @xmath13 .",
    "@xmath14 denotes the channel capacity @xmath15 . for discrete channels ,",
    "the channel @xmath16 is sometimes presented as a matrix where @xmath16 is in the @xmath17-th column and the @xmath18-th row .",
    "logarithms and all information quantities are base @xmath19 unless specified otherwise .",
    "we denote by @xmath20 the unit simplex @xmath21 , i.e. the set of all probability measures on @xmath22 .",
    "@xmath23 denotes a bernoulli random variable with probability @xmath24 to be @xmath25 .",
    "@xmath26 denotes an indicator function of an event or a condition , and equals @xmath25 if the event occurs and @xmath27 otherwise .",
    "we use `` @xmath28 '' to denote simple mathematical inductions , where the same rule is repeatedly applied , for example @xmath29 .",
    "a hat @xmath30 denotes an estimated value , and a line @xmath31 denotes an average value .",
    "the empirical distribution of a vector @xmath32 of length @xmath33 is a function representing the relative frequency of each letter , @xmath34 where the subscript identifies the vector .",
    "the conditional empirical distribution of two equal length vectors @xmath35 is defined as @xmath36      let @xmath37 be sets defining the input and output alphabets , respectively .",
    "both @xmath37 are assumed to be finite , unless stated otherwise.,[sec : arbitrary_channel_var ] do not require @xmath38 to be finite ] let @xmath39 be a sequence of memoryless channels over @xmath33 channel uses .",
    "each @xmath4 is a conditional distribution @xmath40 where @xmath41 and @xmath42 represent an input and output symbol respectively .",
    "the conditional distribution of the output vector @xmath43 given the input vector @xmath44 is given by : @xmath45 the sequence of channels @xmath4 is arbitrary and unknown to the transmitter and the receiver .",
    "we assume the existence of common randomness ( i.e. that the transmitter and the receiver both have access to some random variable of choice ) .",
    "there exists a feedback link between the receiver and the transmitter . to simplify",
    ", we assume the feedback is completely reliable , has unlimited bandwidth and is instantaneous , i.e. arrives to the encoder before the next symbol .",
    "we assume the system is rate adaptive , which means that the message is represented by an infinite bit sequence @xmath46 , and the system may choose how many bits to send .",
    "the error probability is measured only over the bits which were actually sent ( i.e. over the first @xmath47 bits , where @xmath48 is the rate reported by the receiver ) .",
    "the system setup is presented in figure  [ fig : system_adaptive ] .",
    "( 140 , 30 ) ( 23,16)transmitter(20,10)(1,0)20(40,10)(0,1)15(40,25)(-1,0)20(20,25)(0,-1)15 ( 63,20)channel(60,19)(1,0)20(80,19)(0,1)5(80,24)(-1,0)20(60,24)(0,-1)5 ( 103,16)receiver(100,10)(1,0)20(120,10)(0,1)15(120,25)(-1,0)20(100,25)(0,-1)15 ( 0,17.5)(1,0)20(6,18.5)@xmath49(0,14)(message ) ( 40,21.5)(1,0)20(42,22.5)@xmath50 ( 80,21.5)(1,0)20(82,22.5)@xmath51 ( 100,15)(-1,0)60(55,10)@xmath52 ( feedback ) ( 120,21.5)(1,0)20(125,22.5)@xmath48 ( rate ) ( 120,15)(1,0)20(125,16)@xmath53 ( message ) ( 30,5)(0,1)5(30,0)@xmath54 ( common randomness ) ( 110,5)(0,1)5(110,0)@xmath54    to simplify , we assume that there are no constraints on the channel input ( such as power constraints ) . if such constraints exist they can be accommodated by changing the set of potential priors .",
    "since the channel sequence is arbitrary there is no positive rate which can be guaranteed a - priori .",
    "instead , we define a target rate @xmath55 as a function of the channel sequence @xmath56 .",
    "[ def : attainability_of_rw ] we say that a sequence of rate functions @xmath55 is asymptotically attinable , if for every @xmath57 there is @xmath33 large enough such that there is a system with feedback and common randomness over @xmath33 channel uses , in which , for _ every _ sequence @xmath39 , the rate is @xmath58 or more , with probability of at least @xmath59 , while the probability of error is at most @xmath60 .    in the next section",
    "we propose several potential target rates and then we would ask which of these are attainable .      with respect to the sequence @xmath61",
    "we can define various meaningful information theoretic measures .",
    "the maximum possible rate of reliable communication is the capacity when the sequence is known a - priori ( in other words , the capacity with full , non causal , channel state information at the transmitter and the receiver ) and is given by : @xmath62 note that if constraints on the sequence @xmath63 existed , then we would have an equality @xcite . the maximum rate that can be obtained with a single _ fixed _ prior when the sequence is known is : @xmath64",
    "lastly , the capacity of the time - averaged channel is : @xmath65 where we define the time - averaged channel as @xmath66    clearly , @xmath67 where the first inequality results from the order of maximization and the other results from the convexity of the mutual information with respect to the channel .",
    "for each of the above target rates we would like to find out whether it is achievable under the definitions above .",
    "as we shall see , @xmath68 is not achievable , @xmath69 is achievable , and @xmath70 is achievable only under further constraints imposed on the problem .    a rigorous proof that @xmath68 is the capacity of the channel sequence is left out of the scope of this paper . for our purpose , it is sufficient to observe that @xmath68 is an upper bound on the achievable rate , because the mutual information between channel input and output is maximized by a memoryless ( not i.i.d . ) input distribution @xmath71 . to see intuitively how @xmath68 can be achieved ,",
    "consider that since @xmath33 can be arbitrarily large while the input and output alphabets , and thus the set of channels , remain constant , we may sort the channels into groups of similar channels , and apply block coding to each group .",
    "a close result pertaining to stationary ergodic channels appears in ( * ? ? ?",
    "* ( 3.3.5 ) ) .",
    "in this section we present a synthetic problem , which will help us examine the achievability of the target rates defined above in a simplified scenario , draw the links to universal prediction , and introduce the techniques that will be used in the sequel .",
    "we focus on the problem of setting a prior @xmath72 at time @xmath1 .",
    "we assume that at each time instance @xmath1 , the system has full knowledge of the sequence of past channels @xmath73 .",
    "the prior prediction mechanism sets @xmath72 based on the knowledge of @xmath73 .",
    "then , we assume that @xmath74 bits are conveyed during time instance @xmath1 .",
    "a predictor @xmath75 attains a given target rate @xmath55 if for all sequences @xmath56 we have @xmath76 , and @xmath77 .",
    "this abstract problem can apply to a situation where the channel sequence is constant during long blocks , and changes its value only from block to block , or from one transmission to another . in this case",
    "@xmath1 denotes the block index , and denoting by @xmath78 the constant block length , at most @xmath79 bits can be sent in block @xmath1 .",
    "if the channel is constant over long blocks it is reasonable to assume that past channels can be estimated .",
    "note that in addition we made the assumption that @xmath74 is achievable , although this communication rate is unknown to the transmitter in advance , i.e. we ignored the problem of rate adaptation .",
    "therefore the synthetic problem is a subset of the original problem and upper bounds that we show here apply also to the original problem .",
    "we begin by discussing the achievability of @xmath70 for the synthetic problem .",
    "the target rate @xmath70 is special in being an additive function for each value of @xmath11 . universally attaining @xmath70 under the conditions specified above , falls into a widely studied category of universal prediction problems @xcite .",
    "below , we present this class of problems and review some results that will be important for our discussion .",
    "these prediction problems have the following form : let @xmath80 be a strategy in a set of possible strategies @xmath81 , and @xmath41 be a state of nature . a loss function @xmath82 associates a loss with each combination of a strategy and a state of nature .",
    "the total loss over @xmath33 occurrences is defined as @xmath83 . the universal predictor @xmath84 assigns the next strategy given the past values of the sequence , and before seeing the current value .",
    "there is a set of reference strategies @xmath85 ( sometimes called experts ) , which are visible to the universal predictor .",
    "the target of universal prediction is to provide a predictor @xmath86 which is asymptotically and universally better than any of the reference strategies , in the sense defined below .    for a given sequence @xmath87 , denote the losses of the universal predictor and the reference strategies as @xmath88 and @xmath89 , respectively .",
    "denote the regret of the universal predictor with respect a specific reference strategy as the excessive loss : @xmath90 @xmath91 is a function of the sequence @xmath87 and the predictor .",
    "the target of the universal predictor is to minimize the worst case regret , i.e. attain @xmath92    the reference strategies may be defined in several different ways . in the simplest form of the problem",
    "the competition is against the set of fixed strategies @xmath93 .",
    "the exact minimax solution is known only for very specific loss functions @xcite , and a solution guaranteeing @xmath94 is not known for general loss functions .",
    "however there are many prediction schemes which perform well for a wide range of loss functions ( see references above ) .    in the information theoretic framework",
    ", the log - loss @xmath95 , where @xmath96 is a probability distribution over @xmath22 is the most familiar loss function , and used in analyzing universal source encoding schemes @xcite , since @xmath82 represents the optimal encoding length of the symbol @xmath17 when assigned a probability @xmath96 .",
    "it exhibits an asymptotical minimax regret of @xmath97 .",
    "however in the more general setting the asymptotical minimax regret decreases in a slower rate of @xmath98 .",
    "there are several loss functions which are characterized by a `` smoother '' behavior for which better minimax regret is obtained ( * ? ? ?",
    "* theorem 3.1 , proposition 3.1 ) .",
    "for some of these loss functions , a simple forecasting algorithm termed `` follow the leader '' ( fl ) can be used @xcite ( * ? ? ?",
    "* theorem 1 ) . in fl ,",
    "the universal forecaster picks at every iteration @xmath1 the strategy that performed best in the past , i.e. minimizes the cumulative loss over the instances from @xmath25 to @xmath99 .",
    "the archetype of loss functions for which it is not possible to obtain a better convergence rate than @xmath100 is the absolute loss @xmath101 , where @xmath102 and @xmath103 $ ] .",
    "the proof for the lower bound on the minimax regret ( * ? ? ? * theorem 3.7 ) is based on generating the sequence @xmath87 randomly , and calculating the minimum _ expected _ regret ( over @xmath32 ) .",
    "this value is a lower bound for the minimum - maximum regret . to show that the regret is @xmath104 it is enough to consider only two competitors ",
    "one forecasting a constant zero , and one a constant one , and observe that since the cumulative losses of the two competitors always sum up to @xmath33 , the minimum loss of the two competitors is a random variable with a standard deviation of @xmath105 which is upper bounded by @xmath106 , and therefore its expected value is @xmath107 , whereas the expected loss of the best single strategy over the random sequence can not be better than @xmath106 .",
    "we will use a similar idea to prove lower bounds on the regret in the current problems . for general loss functions , and specifically for the absolute loss , the simple fl strategy does not converge .",
    "the problem of asymptotically attaining @xmath108 is analogous to the standard prediction problem , where the prior @xmath109 represents a strategy , and the channel @xmath4 represents a state of nature .",
    "our problem is given in terms of gains rather than losses , so we may consider the loss to be @xmath110 .",
    "the regret is therefore : @xmath111 note that the regret is defined in terms of bits rather than rates ( i.e. it is not normalized ) , from technical reasons .      a natural question to ask is , then",
    ": what is the asymptotical form of the minimax regret expected in our case ? as we will show , the prior prediction problem we posed , includes as a special case the prediction problem with the absolute loss function .",
    "therefore , the asymptotical behavior can not be better than @xmath105 , and it is not possible to apply the simple fl strategy .    the following example",
    "shows why the problem of attaining @xmath70 includes as a particular case the absolute loss function :    [ example : prediction_channel1 ] consider the quaternary to binary channel ( @xmath112 ) , which may be in one of two states @xmath113 , which define two conditional probability functions ( shown as @xmath114 matrices below ) : @xmath115 \\\\ & w_{1}(y|x ) = \\left [ \\begin{array}{cccc } \\half & \\half & 1 & 0 \\\\   \\half & \\half & 0 & 1 \\end{array } \\right ] .",
    "\\end{split}\\ ] ]    ( 206.79 , 112.54)(0,0 ) ( 0,0 ) example channels @xmath116,title=\"fig : \" ] ( 5.67,87.18)@xmath27 ( 5.67,65.36)@xmath25 ( 5.67,45.51)@xmath19 ( 5.67,25.67)@xmath117 ( 23.53,99.09)@xmath118 ( 63.21,81.23)@xmath119 ( 77.10,67.34)@xmath27 ( 77.10,45.51)@xmath25 ( 25.51,7.81)@xmath120 ( 114.80,87.18)@xmath27 ( 114.80,65.36)@xmath25 ( 114.80,45.51)@xmath19 ( 114.80,25.67)@xmath117 ( 132.66,99.09)@xmath118 ( 172.35,81.23)@xmath119 ( 186.24,67.34)@xmath27 ( 186.24,45.51)@xmath25 ( 134.65,7.81)@xmath121    ( 206.79 , 112.54)(0,0 ) ( 0,0 ) example channels @xmath116,title=\"fig : \" ] ( 5.67,87.18)@xmath27 ( 5.67,65.36)@xmath25 ( 5.67,45.51)@xmath19 ( 5.67,25.67)@xmath117 ( 23.53,99.09)@xmath118 ( 63.21,81.23)@xmath119 ( 77.10,67.34)@xmath27 ( 77.10,45.51)@xmath25 ( 25.51,7.81)@xmath120 ( 114.80,87.18)@xmath27 ( 114.80,65.36)@xmath25 ( 114.80,45.51)@xmath19 ( 114.80,25.67)@xmath117 ( 132.66,99.09)@xmath118 ( 172.35,81.23)@xmath119 ( 186.24,67.34)@xmath27 ( 186.24,45.51)@xmath25 ( 134.65,7.81)@xmath121    by writing the input as two binary digits @xmath122 $ ] , the channel can be defined as follows : if @xmath123 then @xmath124 , otherwise , @xmath125 .",
    "these channels are depicted in figure  [ fig : w0w1_example_channels ] , where transitions are denoted by solid lines for probability @xmath25 , and dashed lines for probability @xmath126 .",
    "we consider the same prediction problem , under the simplifying assumption that the channel @xmath127 is chosen only between the two channels above , and the forecaster knows this limitation , i.e. only the sequence of states @xmath128 is unknown .",
    "it is clear from convexity of the mutual information , and the symmetry with respect to @xmath129 ( interchanging the values of @xmath129 leads to the same mutual information ) , that any solution can only be improved by taking a uniform distribution over @xmath129 .",
    "therefore , without loss of generality , the input distribution @xmath11 can be defined by a single value @xmath130 $ ] , and be written @xmath131 $ ] . for this choice",
    "the output will always be uniformly distributed @xmath132 .",
    "we have : @xmath133 and similarly @xmath134 , therefore we can write : @xmath135 hence , even under this limited scenario , the loss function @xmath136 behaves like the absolute loss function , and therefore the normalized minimax regret ( and the redundancy in attaining @xmath70 ) is at least @xmath137 .",
    "note that the relation to the absolute loss implies that the simple fl predictor @xmath138 , can not be applied to our problem .",
    "an example to illustrate this and some further details are given in appendix  [ sec : fl_failure_example ] .",
    "since in the rest of the paper we will focus on the rate function @xmath69 , it is interesting to note that , although this rate is smaller , in general , than @xmath70 , the minimum redundancy in obtaining it can not be better than @xmath137 . to show this , we only need to show that in the context of the counter - example shown above , @xmath139 . for a specific sequence of channels , denote by @xmath24 the relative frequency with which channel @xmath140 appears .",
    "the averaged channel is @xmath141 .",
    "it is easy to see that the capacity of this channel is obtained by placing the entire input probability on the two useful inputs of the channel that appears most of the time .",
    "that is , if @xmath142 we place the input probability on the useful inputs of @xmath140 and obtain the rate @xmath143 , and otherwise obtain @xmath144 .",
    "hence the capacity of the averaged channel is @xmath145 .",
    "on the other hand , @xmath146 } \\left ( ( 1-p ) \\cdot ( 1-q ) + p q \\right ) = \\max(p,1-p ) .",
    "\\end{split}\\ ] ] using the example above , we can also see why @xmath68 is not universally achievable with an asymptotically vanishing normalized regret by a sequential predictor . in the example",
    ", the capacities of the two channels are @xmath147 .",
    "suppose the sequence of channel states @xmath148 is generated randomly i.i.d . @xmath132 .",
    "then for any sequential predictor of @xmath149 , the expected loss in each time instance is @xmath150 = \\half ( 1-q ) + \\half q = \\half$ ] , while the target rate is @xmath151 .",
    "therefore the expected normalized regret with respect to @xmath68 is @xmath126 , and the maximum regret ( maximum over the sequence @xmath61 ) is lower bounded by the expected regret .    to summarize ,",
    "we have seen why @xmath68 is not universally achievable , and therefore @xmath70 constitutes a reasonable target .",
    "furthermore , the minimax regret with respect to @xmath70 is at least @xmath137 , and the simple fl predictor following the best a - posteriori strategy does yield a vanishing regret .",
    "the prediction algorithm proposed below is based on a well known technique of a weighted average predictor , using exponential weighting @xcite .",
    "a minor difference with respect to known results is the extension to a continuous set of reference strategies .    a weight function @xmath152 is any non - negative function @xmath153 with @xmath154 .",
    "all integrals in the sequel are by default over @xmath20 .",
    "define the following weight function : @xmath155 and the predictor : @xmath156 the weighting function gives a higher weight to priors that succeeded in the past and the predictor averages the potential priors with respect to the weight .",
    "this is illustrated in fig .",
    "[ fig : exponential_weighting_illustration ] .",
    "the following theorem gives a bound on the regret of this predictor , which is proven in the next section .",
    "( 152.28 , 85.26)(0,0 ) ( 0,0 ) an illustration of exponential weighting .",
    "the triangle represents the unit simplex .",
    "the two peaks represent two priors @xmath11 which have a relatively large gain @xmath157 .",
    "the weight function @xmath158 combines them exponentialy , and the predictor @xmath72 ( represented as a black spot ) is the weighted average .",
    ", title=\"fig : \" ] ( 70.31,14.00)@xmath159 ( 119.06,62.36)@xmath158 ( 43.77,22.71)@xmath72    ( 152.28 , 85.26)(0,0 ) ( 0,0 ) an illustration of exponential weighting .",
    "the triangle represents the unit simplex .",
    "the two peaks represent two priors @xmath11 which have a relatively large gain @xmath157 .",
    "the weight function @xmath158 combines them exponentialy , and the predictor @xmath72 ( represented as a black spot ) is the weighted average .",
    ", title=\"fig : \" ] ( 70.31,14.00)@xmath159 ( 119.06,62.36)@xmath158 ( 43.77,22.71)@xmath72    [ theorem : prior_predictor_exp ] let @xmath160 be bounded function @xmath161 which is concave in its first argument . then for @xmath33 large enough so that @xmath162 , the predictor defined by and with @xmath163 yields @xmath164 with @xmath165    note that the theorem applies to gain functions more general than the mutual information , since it uses only the properties of concavity and boundness .",
    "in the case of mutual information we have @xmath166    we obtained a convergence rate of @xmath167 which is slightly worse than the asymptotic bound of @xmath168 from section  [ sec : regret_lb ] .",
    "the additional @xmath169 may be attributed to the fact the space of reference predictors is continuous ( it results from lemma  [ lemma : f_exp_weight_ub ] stated below ) , but we do not know if this is the best convergence rate .      in this section",
    "we analyze the performance of the predictor and prove theorem  [ theorem : prior_predictor_exp ] .",
    "define the instantaneous regret @xmath170 and the cumulative regret @xmath171 as functions of @xmath11 : @xmath172 @xmath173    these functions express the regret with respect to a fixed competing prior @xmath11 .",
    "the claim of the theorem is equivalent to the claim that for all @xmath11 , @xmath174 .",
    "we sometimes omit the dependence on @xmath11 for brevity .    for @xmath175 of our choice , we define the following potential function : @xmath176 where @xmath177 is an arbitrary function defined over the unit simplex",
    ". note that for large values of @xmath178 , @xmath179 approximates @xmath180 . as customary in this prediction technique",
    ", the proof consists of two parts :    1 .   bounding the growth rate of @xmath181 over @xmath182 for any @xmath11 .",
    "relating @xmath183 to @xmath184 .",
    "the techniques we use are based on cesa - bianchi and lugosi s @xcite ( see theorem 2.1 , corollary 2.2 , theorem 3.3 ) .    from the concavity of @xmath10 with respect to @xmath11",
    "we have that for any weight function @xmath152 and any @xmath4 : @xmath185 following @xcite we term this inequality the `` blackwell condition '' .",
    "the meaning of this condition is that by choice of @xmath152 we can prevent an increase of @xmath171 in a chosen direction ( @xmath152 can be thought of as a unit vector in the hilbert space of functions over @xmath20 ) . for the specific choice of the weight function ,",
    "this direction is proportional to the gradient of @xmath186 with respect to @xmath48 , thus preventing any growth in this direction and leaving only second order terms that contribute to the increase of @xmath184 .",
    "since the factor @xmath187 in does not depend on @xmath11 , the weight function can be alternatively written as : @xmath188 @xmath158 is indifferent to any constant addition to @xmath189 due to the normalization .",
    "the growth of the potential can be bounded as follows : @xmath190 notice that @xmath191 .",
    "we take @xmath192 small enough that @xmath193 and use the following inequality ( proven in appendix  [ sec : proofs_of_small_lemmas ] ) :    [ lemma : exp_second_order_bound ] for @xmath194 $ ] : @xmath195    returning to we have : @xmath196    therefore recursively applying : @xmath197 notice that @xmath198 .",
    "this completes the first part of showing that the increase in @xmath199 is bounded . for the second part we shall use the following lemma which relates the exponential weighting of a function to its maximum , and",
    "is proven in appendix  [ sec : proof_of_lemma1 ] :    [ lemma : f_exp_weight_ub ] let @xmath200 be a real non - negative bounded function @xmath201 $ ] concave in @xmath54 , where @xmath54 is a closed convex vector region of dimension @xmath202 , and let @xmath192 satisfy @xmath203 , then @xmath204 + \\frac{d}{\\eta } \\ln \\left ( \\frac{\\eta e ( b - a)}{d } \\right ) \\\\ & = \\frac{1}{\\eta } \\ln \\left [ \\frac{\\phi(f)}{\\phi(0 ) } \\right ] + \\frac{d}{\\eta } \\ln \\left ( \\frac{\\eta e ( b - a)}{d } \\right ) .",
    "\\end{split}\\ ] ]    let @xmath205 . in this case",
    "the convex region is @xmath20 and therefore @xmath206 . by we",
    "can bound @xmath207 by : @xmath208 where the factor @xmath209 is constant in @xmath11 .",
    "we have @xmath210 .",
    "assuming @xmath211 to satisfy the conditions of lemma  [ lemma : f_exp_weight_ub ] , we obtain from : @xmath212 where in the last inequality we assumed @xmath213 ( this would hold for @xmath192 small enough ) .",
    "we use the following lemma to optimize the rhs of with respect to @xmath192 :    [ lemma : ab_alphabeta ] the unique minimum over @xmath214 of @xmath215 ( @xmath216 ) is obtained at @xmath217 and equals @xmath218 \\cdot a^{\\frac{\\beta}{\\alpha+\\beta } } \\cdot b^{\\frac{\\alpha}{\\alpha+\\beta } } .\\ ] ] particularly , for @xmath219 , i.e. @xmath220 we have @xmath221 and @xmath222 .    the proof of the lemma is simple by a direct derivation ( see appendix  [ sec : proofs_of_small_lemmas ] ) . applying the lemma to the optimization of @xmath192 in we",
    "obtain : @xmath223 and @xmath224    we now verify the assumptions we made along the way . in",
    "we assumed that @xmath225 .",
    "if the contrary holds @xmath226 then considering the first term in the rhs of , we have @xmath227 , and therefore the theorem holds in a void way . to apply lemma  [ lemma : f_exp_weight_ub ] we required @xmath211 .",
    "if the opposite is true , i.e. @xmath228 then the second term the rhs of becomes @xmath229 , and so for @xmath230 we would have again @xmath227 and the theorem will hold in a void way .",
    "thus for the two last conditions , it is enough that @xmath231 , since in this case if either of the conditions does not hold , the theorem becomes true automatically ( in a void way ) .",
    "lastly , in we assumed @xmath213 . substituting @xmath232 we have @xmath233 , which becomes smaller than @xmath25 for @xmath33 large enough .",
    "the last condition supersedes @xmath230 , and is specified as a requirement in the theorem .",
    "in this section we return to the problem defined in section  [ sec : problem_setting ] and present the main results of the paper : the achievability of the capacity of the averaged channel , and a converse showing that this is the best rate , under some conditions .",
    "we give the outline of the communication system attaining this rate , while leaving out some of the technical details , such as decoding and channel estimation ( these will be completed in the next section ) .",
    "we show that under abstract assumptions , the system achieves the desired rate .",
    "the same communication scheme and predictor will be used , with slight modifications , to prove the main result in section  [ sec : mainproof ] .",
    "the synthetic problem differs from the problem defined in section  [ sec : problem_setting ] , in two main aspects :    1 .",
    "it assumes that the sequence of past channels is fully known .",
    "since the receiver observes only one output sample from each channel , this assumption is not realistic . on the other hand , the time - averaged channel over `` large '' chunks of symbols can be measured .",
    "it assumes that a rate corresponding to a sum of the per - symbol mutual information can be attained , whereas with an arbitrarily varying channel , the amount of mutual information between the input and output vectors is potentially lower .",
    "therefore , as we shall see , @xmath70 is no longer achievable in the context of the arbitrarily varying channel defined in section  [ sec : problem_setting ] . in appendix",
    "[ sec : example_prediction_channel2 ] we show that even imposing on the synthetic problem only the limitation that the past channels are not given , but need to be estimated , leads to the conclusion that @xmath70 is not attainable",
    ". therefore we compromise on an alternative target : obtaining @xmath234 , i.e. the capacity of the averaged channel . as we shall show in this section and the next , this rate is indeed asymptotically achievable .",
    "the rate @xmath235 is certainly not the maximum achievable target rate . as an example , if @xmath235 is achievable for large @xmath33 then by operating the same scheme on two halves of the transmission time one could attain @xmath236 , where @xmath237 denote the averaged channels on the two halves .",
    "this rate is in general higher , because due to the convexity of the mutual information with respect to the channel @xmath238 \\leq r$ ] .    on the other hand",
    ", @xmath235 is the maximum achievable rate which is independent of the order of the sequence @xmath61 , or , in other words , which is fixed under permutation of the sequence .",
    "this observation is formalized in the following theorem :    [ theorem : c_overlinew_optimality ] let @xmath55 ( for @xmath239 ) be a sequence of rate functions , which are oblivious to the order of @xmath56 . if the sequence is asymptotically attainable according to definition  [ def : attainability_of_rw ] , then there exists a sequence @xmath77 such that @xmath240 .",
    "note that @xmath235 depends on @xmath33 through the average over @xmath33 channels @xmath241 .",
    "since both @xmath68 and @xmath70 are oblivious to the order of @xmath56 , theorem  [ theorem : c_overlinew_optimality ] implies they are not achievable .",
    "following is a rough outline of the proof .",
    "consider the channel generated by uniformly drawing a random permutation @xmath242 of the indices @xmath243 , using the channels @xmath4 in a permuted order .",
    "if a system guarantees a rate @xmath55 , which is fixed under permutation , then this rate would be fixed for all drawing of @xmath242 , and therefore for the channel we described , the system can guarantee the rate @xmath55 a - priori .",
    "hence , the capacity of this channel must be at least @xmath55 .",
    "the next stage is to show that the feedback capacity of this channel is at most @xmath235 . due to",
    "the fact we select the channels from the set @xmath39 without replacement , the proof is a little technical and will be deferred to appendix  [ sec : proof_cw_max_rate ] .",
    "however to give an intuitive argument , if we replace the channel described above , by a similar channel , obtained by randomly drawing at each time instance one of @xmath39 , this time _ with _ replacement , then this new channel is simply the dmc with channel law @xmath244 .",
    "therefore feedback does not increase the capacity and its feedback capacity is simply @xmath235 .",
    "the main point in the proof is to show there is no difference in feedback - capacity between the two channels , and the main tool is hoeffding s bounds on sampling without replacement @xcite .",
    "another interesting property of the rate @xmath235 is that it meets or exceeds the random - code capacity of any memoryless avc defined over the same alphabet , and thus attaining @xmath235 yields universality over all avc s ( see section  [ sec : discussion_avc ] ) . through the relation to avc capacity",
    "we can see that common randomness is essential to obtain @xmath235 , as it is essential for obtaining the random - code capacity @xcite . after settling for @xmath235 ,",
    "the next question that naturally arises is : what is the best convergence rate of the regret , with respect to this target ? in section  [ sec : regret_lb ] we have shown that even in the context of the synthetic problem of section  [ sec : toy_problem ] ( with full knowledge of past channels ) , the regret with respect to @xmath69 is at least @xmath245 , and this lower bound naturally holds in the current problem , where only partial knowledge of past channels is available .",
    "the following theorem formalizes claim that @xmath235 is achievable according to definition  [ def : attainability_of_rw ] :    [ theorem : c_overlinew_achievability ] for every @xmath246 there exists @xmath247 and a constant @xmath248 , such that for any @xmath249 there is an adaptive rate system with feedback and common randomness , where for the problem of section  [ sec : problem_setting ] , over any sequence of channels",
    "@xmath250 :    1 .",
    "the probability of error is at most @xmath60 2 .",
    "the rate satisfies @xmath251 with probability at least @xmath59 3 .",
    "@xmath252    [ corollary : symbolwise_random_numerical ] specific values for @xmath253 can be obtained as follows .",
    "let @xmath254 be parameters of choice .",
    "then the constants @xmath255 and @xmath248 are given in the proof , by , , where constants used in these equations are defined in , , , - , .",
    "for any @xmath256 , @xmath257 and @xmath258 .",
    "[ corollary : symbolwise_random_prior_predictor1 ] the same holds if @xmath4 is determined ( e.g. by an adversary ) as a function of the message and all previous channel inputs and outputs @xmath259 .",
    "a numerical example is given after the proof ( example  [ example : symbolwise_random_prior_predictor ] ) .",
    "the proof of the theorem is given in section  [ sec : mainproof ] .      in this section give the communication scheme , up to some details which will be completed later on ( section  [ sec : mainproof_decoding_cond ] ) .",
    "one of the issues that we ignored in the synthetic problem is the determination of the rate @xmath48 before knowing the channel . to solve this problem we use rateless codes @xcite .",
    "we divide the available time into multiple such blocks as done by eswaran  @xcite and in @xcite .",
    "we fix a number @xmath260 of bits per block . in each block",
    ", @xmath260 bits from the message string are sent . at each block",
    "@xmath261 , a codebook of @xmath262 codewords is generated randomly and i.i.d .",
    "( in time and message index ) according to the prior @xmath263 .",
    "@xmath263 is determined by a prediction scheme which is specified below .",
    "the random drawing of the codewords is carried out by using the common randomness , and the codebook is known to both sides .",
    "the relevant codeword matching the message sub - string is sent to the receiver symbol by symbol . at each symbol of the block and for each codeword",
    "@xmath264 in the codebook , the receiver evaluates a decoding condition that will be specified later on . roughly speaking",
    ", the condition measures whether there is enough information from the channel output to reliably decode the message .",
    "the receiver decides to terminate the block if the condition holds , and informs the transmitter .",
    "when this happens , the receiver determines the decoded codeword as one of the codewords that satisfied .",
    "then , using the known channel output @xmath265 , and the decoded input @xmath32 over the block which was decoded , the receiver computes an estimate of the averaged channel over the block .",
    "the specific estimation scheme will be specified in section  [ sec : mainproof_decoding_cond ] .",
    "the receiver calculates a new prior for the next block according to the prediction scheme that will be specified below .",
    "the receiver sends the new prior to the transmitter .",
    "alternatively , the receiver may send the estimated channel , and the new prior can be calculated at each side separately .",
    "the new block @xmath266 starts at the next symbol , and the process continues , until symbol @xmath33 is reached .",
    "the last block may terminate before decoding .",
    "( 229.77 , 131.75)(0,0 ) ( 0,0 ) an illustration of the combination of a rateless scheme with prior prediction .",
    "each box represents a rateless block in which @xmath260 bits are transmitted.,title=\"fig : \" ] ( 75.28,118.30 ) ( 45.52,118.30 ) ( 122.90,118.30 ) ( 190.37,118.30 ) ( 123.56,66.71 ) ( 125.33,41.63 ) ( 71.53,12.69)predictor ( 160.84,12.18 ) ( 75.34,41.63 ) ( 73.58,66.71 ) ( 46.78,41.63 ) ( 45.02,66.71 )    ( 229.77 , 131.75)(0,0 ) ( 0,0 ) an illustration of the combination of a rateless scheme with prior prediction .",
    "each box represents a rateless block in which @xmath260 bits are transmitted.,title=\"fig : \" ] ( 75.28,118.30 ) ( 45.52,118.30 ) ( 122.90,118.30 ) ( 190.37,118.30 ) ( 123.56,66.71 ) ( 125.33,41.63 ) ( 71.53,12.69)predictor ( 160.84,12.18 ) ( 75.34,41.63 ) ( 73.58,66.71 ) ( 46.78,41.63 ) ( 45.02,66.71 )      in this section we present the prediction algorithm .",
    "we denote by @xmath1 the index of the block , and by @xmath267 the averaged channel over the block , i.e. if the block @xmath1 starts at symbol @xmath268 and ends at @xmath269 , then @xmath270 .",
    "the length of the @xmath1-th block is denoted @xmath271 .",
    "we use an exponentially weighted predictor mixed with a uniform prior .",
    "the motivation for using the uniform prior is explained in the next section .",
    "let @xmath272 be the uniform prior over @xmath22 .",
    "we define the predictor as : @xmath273 where @xmath274 where @xmath275 is an estimate of the mutual information of the averaged channel over block @xmath1 , @xmath276 , and is interpreted as an estimate of the number of bits that would have been sent with the alternative prior @xmath11 .",
    "this estimate is defined later on in section  [ sec : mainproof_attained_rate ] .",
    "the parameters @xmath277 and @xmath260 will be chosen later on .",
    "@xmath278 is the potential function defined in .",
    "the term @xmath279 normalizes @xmath158 to @xmath280 .",
    "the following lemma formalizes the claim that the predictor resulting of - , asymptotically achieves a rate @xmath281 :    [ lemma : rateless_prior_predictor_lemma ] let @xmath275 , @xmath282 be a set of @xmath283 non - negative concave functions of the prior @xmath159 , let @xmath284 denote a set of non - negative numbers , and @xmath285 be arbitrary positive constants satisfying @xmath286 and @xmath287 .",
    "define the target rate @xmath288 define the actual rate @xmath48 over @xmath33 channel uses as : @xmath289 define the sequential predictor @xmath72 as the result of and .",
    "let @xmath284 satisfy : @xmath290 then for the value of @xmath192 specified below it is guaranteed that : @xmath291 where @xmath292 and @xmath293 the value of @xmath192 attaining the result above is : @xmath294    the lemma is proven in appendix  [ sec : proof_rateless_prior_predictor_lemma ] .",
    "the proof uses similar techniques to those introduced in section  [ sec : toy_performance ] , however , different from the previous analysis , due to mixing with the uniform prior , the `` blackwell '' condition ( in the previous case ) only approximately holds . on the other hand ,",
    "the use of the uniform prior enables relating @xmath295 to @xmath275 for any other @xmath11 , and thus obtain from an upper bound on the gain @xmath296 related to an alternative prior @xmath11 .",
    "the trade - off between the two is expressed in the two last factors in , one of which is increasing with @xmath297 and the other decreasing .    since by , @xmath298 , the claim of the lemma appears similar to theorem  [ theorem : prior_predictor_exp ] , with @xmath296 taking the place of the function @xmath299 .",
    "however two important properties of the lemma , distinguishing it from the rather standard claim of theorem  [ theorem : prior_predictor_exp ] are that the bound does not depend on the number of blocks ( i.e. the number of prediction steps ) , and that no upper bound on @xmath275 is assumed .    the rate @xmath300 represents a bound on mutual information , but in the context of the lemma it enough to consider it as an arbitrary rate that caps @xmath301 .",
    "it affects the setting of @xmath192 and the resulting loss .",
    "also , @xmath33 does not have to correspond to the actual number of symbols and serves here merely as a scaling parameter for the communication rate .",
    "the lemma sets a value of @xmath192 but not for @xmath297 , since @xmath297 will have additional roles in the next section .      in this section a motivation for the prediction algorithm , and especially for the use of the uniform prior",
    "under abstract assumptions it is shown to achieve the capacity of the averaged channel .",
    "this section is intended merely to give motivation and is not formally necessary for the proof of theorem  [ theorem : c_overlinew_achievability ] .    to simplify the discussion ,",
    "let us make abstract assumptions regarding the decoding condition and the channel estimation :    1 .",
    "the decoding condition yields block lengths satisfying : @xmath302 with an equality for all blocks except the last one which is not decoded .",
    "this implies the rate @xmath303 equals the mutual information of the averaged channel .",
    "2 .   the averaged channels over all previous blocks are known and available for the predictor    with these assumptions , the prediction problem can be considered separately from decoding and channel estimation issues . supposing that @xmath304 blocks were transmitted , the achieved rate is @xmath305 . since @xmath306 , using this can be written as @xmath307 .",
    "the target is to find a prediction scheme for @xmath72 , such that for any sequence @xmath4 , one will have @xmath308 with @xmath309 .",
    "there are two main difficulties compared to the prediction problem discussed in section  [ sec : toy_problem ] :    1 .",
    "the problem is not directly posed as a prediction problem with an additive loss .",
    "the loss is not bounded : if for some @xmath1 , @xmath310 then the rate becomes zero regardless of other blocks .",
    "the first issue is resolved by posing an alternative problem which has an additive loss , and using the convexity of the mutual information with respect to the channel ( as will be exemplified below in the abstract case ) . regarding the second issue ,",
    "notice that if the channel has zero capacity ( always , or from some point in time onward ) , it is possible that one of the blocks will extend forever and will never be decoded .",
    "however we must avoid a situation where the channel has non - zero capacity ( which our competition enjoys ) , while a badly chosen prior yields @xmath311 .",
    "this may happen for example in the channels of example  [ example : prediction_channel1 ] , if the predictor selects to use the pair of inputs that yield zero capacity .",
    "if this happens then the scheme will get stuck since the block will never be decoded , and hence there will be no chance to update the prior .",
    "in addition , notice that selecting some inputs with zero probability makes the predictor blind to the channel values over these inputs . to resolve these difficulties",
    "we construct the predictor as a mixture between an exponentially weighted predictor and a uniform prior .",
    "we use a result by shulman and feder @xcite , which bounds the loss from capacity by using the uniform prior @xmath312 : @xmath313{shulman_prior } } \\geq c   \\cdot   \\beta(c ) \\stackrel{\\cite[(17)]{shulman_prior}}{\\geq } \\frac{c}{|\\mathcal{x}| \\cdot ( 1-e^{-1})},\\ ] ] where @xmath314 is the channel capacity and @xmath315 is defined therein .",
    "this guarantees that if the capacity is non - zero , then the uniform prior will yield a non - zero rate , and hence the block will not last indefinitely .    under the abstract assumptions made here , the following @xmath316 is known and can be substituted in lemma  [ lemma : rateless_prior_predictor_lemma ] :",
    "@xmath317 this yields the following result :    [ lemma : c_overlinew_achievability_w_side_info ] for the scheme of section  [ sec : arbitrary_var_rateless_scheme ] under the abstraction specified above , with @xmath231 and @xmath287 and properly chosen @xmath318 , the following holds : for any sequence of channels , the rate satisfies : @xmath319 where @xmath235 is the capacity of the averaged channel and @xmath320 where @xmath321 .",
    "the parameters of the scheme @xmath318 required to attain the result are specified in and respectively .",
    "note that the bound is increasing with @xmath260 , so it appears that that it can be improved by taking the minimal value of @xmath260 .",
    "however in the actual system , there are be fixed overheads related to the communication scheme , and a large block size would be needed to overcome them . taking any fixed and large enough @xmath260 ,",
    "the normalized regret is bounded by @xmath322 , which converges to zero , but at a worse rate than we had in section  [ sec : toy_exp_prior_predictor ] .",
    "note that the claims of lemma  [ lemma : c_overlinew_achievability_w_side_info ] are stronger than the claims that appeared in the conference paper on the subject @xcite , for the same problem , mainly in terms of the improved convergence rate with @xmath33 . also , the scheme used here is slightly different than the one in the conference paper ( in equation  )",
    ". the proof corresponding to the scheme presented in the conference paper can be found in an early version uploaded to arxiv @xcite .    to prove lemma  [ lemma : c_overlinew_achievability_w_side_info ] , lemma  [ lemma : rateless_prior_predictor_lemma ]",
    "is used with @xmath316 defined in .",
    "the rate guaranteed by lemma  [ lemma : rateless_prior_predictor_lemma ] is approximately @xmath323 .",
    "using convexity of the mutual information with respect to the channel this is at least @xmath324 , and since this is true for any @xmath11 , the rate is at least @xmath325 .",
    "the detailed proof appears in appendix  [ sec : proof_c_overlinew_achievability_w_side_info ] .",
    "in this section we prove theorem  [ theorem : c_overlinew_achievability ] , regarding the attainability of @xmath235 . the principles of the prediction scheme have been laid in the previous section , and here we plug - in a suitable decoding condition and a channel estimator .",
    "suppose that during a certain block of length @xmath78 we have used the i.i.d .",
    "prior @xmath326 .",
    "in order to estimate the channel after the block has ended and @xmath32 was decoded , we use the following estimate : @xmath327 where here and throughout the current section , @xmath35 denote the @xmath78-length input and output vectors over the block , and @xmath328 is the empirical distribution of the pair @xmath329 ( for @xmath330 ) .",
    "the estimator is the joint empirical distribution divided by the ( known ) marginal distribution of the input @xmath118 .",
    "since we mix a uniform prior into @xmath326 , all @xmath326 are bounded away from zero , which makes the estimator statistically stable , in comparison with the more natural estimator given by the empirical conditional distribution : @xmath331 in which the denominator may turn out to be zero .",
    "a drawback of the proposed estimator is , that it does not generally yield a legitimate probability distribution , i.e. @xmath332 . the result of using this estimator is that in the calculations , we will see values that formally appear like probabilities but are not . to distinguish them from legitimate probabilities we term these values `` false '' probabilities , and",
    "mark them with a @xmath333 .",
    "these functions usually approximate or estimate a legitimate probability .",
    "formally , a false probability @xmath334 or @xmath335 can be any non - negative function of @xmath18 or @xmath336 ( respectively ) .",
    "note that until this point we did not need the assumption that the output alphabet @xmath38 is finite , since the channel was given to the predictor rather than being estimated , and it is the first time this assumption is used .",
    "the function that we use as an optimization target for selecting the prior for the next block is , as before , the mutual information .",
    "the reason is that since our aim is to achieve the capacity of the averaged channels , the `` competing '' schemes , for each prior @xmath11 , achieve the mutual information of the averaged channel .",
    "however , since the estimates of past channels are false - probabilities , we need to define how to apply the mutual information to them .",
    "we do this by simply plugging - in the false channel into the standard formula of @xmath10 .",
    "this substitution results in what we define as the _ false mutual information _ @xmath337 : @xmath338 where cases of @xmath339 or @xmath340 are resolved using the convention @xmath341 .",
    "the following lemma shows that most of the properties of the mutual information function @xmath342 needed for our previous analysis in section  [ sec : arbitrary_var_exp_prior_predictor ] are maintained .",
    "[ lemma : fmi_properties ] the function @xmath337 defined in is    1 .",
    "non negative 2 .",
    "concave with respect to @xmath11 3 .",
    "convex with respect to @xmath343 4 .",
    "upper bounded by @xmath344 , where @xmath345 $ ] .",
    "the proof is technical and appears in appendix  [ sec : proof_of_lemma_fmi_properties ] .",
    "in addition to the properties above , our proof relies on the next property which is more surprising .",
    "when the prior @xmath11 used for estimating the channel in is the same prior @xmath11 used as input in , the false mutual information attains a form which is familiar from @xcite as a prototype of the zero order rate function . as in @xcite",
    ", we use this form to obtain a bound on the probability of @xmath337 to exceed a threshold for a random drawing of @xmath32 .",
    "this bound , in turn , allows us to construct the rate - adaptive system attaining a block length @xmath346 that depends on @xmath337 .",
    "following @xcite , we define conditional empirical _ probability _ of the discrete sequence @xmath32 given the sequence @xmath265 as @xmath347 , i.e. the probability of the sequence @xmath32 under the conditionally i.i.d .",
    "distribution @xmath348 .",
    "also , when vectors are substituted into @xmath11 we explicitly extend @xmath11 in an i.i.d .",
    "fashion , i.e. @xmath349 .",
    "we will use the following result :    [ lemma : fmi_as_remp ] the false mi with prior @xmath326 and @xmath350 where @xmath35 are @xmath78-length vectors can be written as : @xmath351 furthermore , for any @xmath11 and any @xmath265 , when @xmath44 is distributed i.i.d .",
    "@xmath352 , @xmath353 where @xmath354    note that from the results in ( * ? ? ?",
    "* theorem 9 ? )",
    "( by using the result of the theorem and the definition of intrinsic redundancy therein ) we can obtain a tighter upper bound with @xmath355 ( @xmath356 where @xmath357 is explicitly stated in ( * ? ? ?",
    "* theorem 9 ? ) ) . for the sake of simplicity",
    "we prove here a looser result above , as this does not change the asymptotical results significantly .",
    "_ proof of lemma  [ lemma : fmi_as_remp ] : _ the first part is shown by direct substitution . when @xmath358 we have @xmath359 therefore @xmath360    as for the second claim , by markov bound we have : @xmath361 \\\\ & \\stackrel{(a)}{= } \\exp(-m t ) \\sum_{\\vr x \\in \\mathcal{x}^m } \\frac{\\hat",
    "p(\\vr x | \\vr y)}{q(\\vr x ) } q(\\vr x ) \\\\ & = \\exp(-m t ) \\sum_{\\vr x \\in \\mathcal{x}^m } \\hat p(\\vr x | \\vr y ) , \\end{split}\\ ] ] where in ( a ) we have used the fact @xmath44 is distributed @xmath11 independently of @xmath265 . to bound the sum above",
    "we split the set of sequences @xmath32 to sub - sets having the same conditional empirical probability @xmath362 ( i.e. same conditional type @xcite@xcite ) . in a subset having @xmath363",
    ", the empirical probability @xmath364 equals the ( legitimate ) probability of the sequence under the i.i.d .",
    "distribution @xmath24 , and as a result we have @xmath365 .",
    "the number of subsets is upper bounded ( similarly to bounds on the number types ( * ? ? ?",
    "* theorem 11.1.1 ) ) by which is upper bounded by @xmath366 , since @xmath367 is completely defined by @xmath368 integers in @xmath369 .",
    "@xmath370    substituting in and using @xmath371 yields the desired result .",
    "when the communication scheme was described in section  [ sec : arbitrary_var_rateless_scheme ] , the details of the decoding condition and channel estimation were omitted .",
    "these are specified below .",
    "at each symbol of the block and for each codeword @xmath264 in the codebook , the receiver evaluates the following decoding condition : @xmath372 where @xmath373 is a parameter to be specified later on , and the vectors @xmath374 and @xmath265 are taken over the symbols of the block .",
    "equivalently , by lemma  [ lemma : fmi_as_remp ] , the decoding condition can be written as : @xmath375 where @xmath78 is the number of the symbol in the block and @xmath340 is a channel estimate according to , where @xmath32 is substituted with the hypothesized input @xmath374 and @xmath265 is known output vector over the block .    after decoding , the receiver sets the estimated channel @xmath376 as the false channel @xmath340 measured according to , where @xmath35 are the @xmath78 length vectors denoting the ( hypothesized ) input and output vectors over the duration of the block .    to produce the next prior",
    ", this false channel is fed into the prediction scheme of lemma  [ lemma : rateless_prior_predictor_lemma ] , with @xmath377 and where @xmath346 denotes the length of block @xmath1 .",
    "the parameters @xmath378 ( the latter are required for the prediction scheme of lemma  [ lemma : rateless_prior_predictor_lemma ] ) will be determined in the course of the proof .",
    "the following proof outline conveys the main ideas in the proof , while some details were intentionally dropped , for simplicity .    1 .   using the results of lemma  [ lemma : fmi_as_remp ]",
    "we show that the block lengths can satisfy the inequality required by lemma  [ lemma : rateless_prior_predictor_lemma ] , up to a small overhead term in @xmath260 , while still attaining a small probability of error .",
    "2 .   operating the prior prediction scheme of lemma  [ lemma : rateless_prior_predictor_lemma ] , with @xmath379 as the metric with @xmath376 the _ measured _ channels , guarantees that if no errors were made , the rate achieved by the system exceeds @xmath380 up to vanishing factors , where @xmath304 is the number of blocks that were sent .",
    "3 .   due to the convexity of the false mutual information with respect to the channel",
    ", the rate above exceeds @xmath381 where @xmath382 .",
    "since the rate above exceeds @xmath383 for any @xmath11 , it exceeds @xmath384 .",
    "all is left is to show the convergence in probability of @xmath385 to the true average channel @xmath244 , and by using the continuity of the capacity this proves the convergence in probability of @xmath386 to the capacity of the averaged channel @xmath235 . 6 .   in order to attain explicit bounds on the convergence rate",
    "we develop bounds relating the difference in capacity to the difference in the channels , and optimize the system parameters .",
    "note that there are several delicate issues caused by the relations between @xmath376 , @xmath346 and @xmath72 .",
    "for example , the correct operation of the prior predictor relies on the assumption of correct decoding which is required to obtain the correct channel estimators ( i.e. that @xmath32 used in is the true channel input ) .",
    "however , conditioning on the event of correct decoding changes the distribution of the average estimated channel @xmath385 .",
    "another example is that , although the convergence of @xmath387 to @xmath244 appears to be trivial at first sight , the proof is complicated by the fact that the block lengths @xmath346 are random variables , which themselves depend on the estimated channels @xmath376 .",
    "one embodiment of this dependence is that the block would never end with an estimated channel which has zero capacity .",
    "another dependence is between @xmath388 of different blocks , created through the prior prediction @xmath72 .",
    "we start with a set of definitions and propositions formalizing the claims made in the proof outline above .",
    "we use @xmath389 to denote the symbol index and @xmath1 to denote the block index .",
    "we denote by @xmath390 the block index of a certain symbol ( i.e. @xmath390 if symbol @xmath389 belongs to block @xmath1 ) .",
    "we define @xmath346 ( @xmath282 ) as the length of each block including the last one .",
    "the last block is not accounted for in the rate , even if it is decoded .",
    "[ prop : symbolwise_scheme_err_probability ] for the value of @xmath373 given below , the probability of any decoding error occurring in any of the blocks is at most @xmath60 .    _",
    "proof : _ consider a specific block and denote by @xmath78 the number of the symbol inside the block . since codewords other than the one which is actually transmitted are independent of @xmath35 , the probability to decide in favor of a specific erroneous codeword @xmath391 , at any specific symbol @xmath389 ( i.e. that will hold with respect to it ) , is upper bounded using by : @xmath392 where @xmath393 are defined in lemma  [ lemma : fmi_as_remp ] . and by taking expected value over @xmath43 we have that the same bound holds when not conditioning on @xmath265 .",
    "since there are @xmath394 competing codewords , and @xmath33 symbols , the probability to decide in favor of any erroneous codeword at any symbol ( i.e. to make any decoding error ) , is upper bounded using the union bound , by : @xmath395 where we replaced @xmath396 by @xmath397 .",
    "we now determine @xmath373 so as to make the rhs equal @xmath60 , and thereby guarantee the error probability is at most @xmath60 : @xmath398 note that with a suitable choice of @xmath260 we would have @xmath399 .",
    "the following lemma relates the rate to the averaged estimated channel @xmath385 :    [ prop : symbolwise_scheme_predictor_rate ] if there are no decoding errors , the rate of the scheme satisfies : @xmath400 where @xmath401 is the false capacity , @xmath385 is the averaged estimated channel @xmath402 @xmath403 is defined in lemma  [ lemma : rateless_prior_predictor_lemma ] ( for the relevant parameters @xmath404 ) , and @xmath405 .\\ ] ]    _ proof : _ denote by @xmath406 the channel estimate according to , taken over the symbols of the @xmath1-th block , with respect to the hypothesized input sequence @xmath374 . by our definition of @xmath376 ( section  [ sec : mainproof_decoding_cond ] ) , @xmath407",
    "when @xmath408 is the index of the correct codeword .",
    "denote by @xmath409 the value of @xmath406 when @xmath408 is the index of the hypothesized codeword . when there are no errors , @xmath410 .",
    "we use the prediction scheme of lemma  [ lemma : rateless_prior_predictor_lemma ] with @xmath411 . by lemma  [ lemma : fmi_properties ] , this choice satisfies the conditions of the lemma with respect to @xmath275 . assuming there are no errors , we can equivalently write @xmath377 .",
    "we now use the decoding condition to show the requirements of lemma  [ lemma : rateless_prior_predictor_lemma ] with respect to the block length hold .",
    "denote by @xmath412 and @xmath413 , the channel estimates taken with respect to the true @xmath32 over the first @xmath414 symbols of the block @xmath1 , and over the last symbol of the block , respectively .",
    "in other words , if block @xmath1 spans symbols @xmath415 $ ] where @xmath416 then @xmath417 where in the equations above we wrote the empirical distribution in explicitly as a normalized sum of indicator functions .",
    "we currently assume @xmath418 and we ll return to the case of @xmath419 at the end . from the above we have that : @xmath420    since at symbol @xmath414 in the block , which is one symbol before decoding , none of the codewords satisfies the decoding condition , including the correct codeword ( which corresponds to the true channel input @xmath44 ) , we have @xmath421 the same holds for the last block @xmath422 . as for @xmath413 ,",
    "from we have that @xmath423 and because @xmath413 is measured on a single symbol , we can bound : @xmath424 the equality above can be obtained using lemma  [ lemma : fmi_as_remp ] , or by definition , using the fact that only for a single pair @xmath425 , @xmath426 . combining and using we have : @xmath427 in the case of @xmath419 , @xmath428 and holds due to",
    ". the last inequality means the conditions of lemma  [ lemma : rateless_prior_predictor_lemma ] with respect to @xmath346 are satisfied , with @xmath260 replaced by @xmath429 . under the conditions of the lemma",
    ", it guarantees that : @xmath430 where @xmath431 is the offset defined in the lemma , with @xmath260 replaced by @xmath429 .",
    "we use the convexity of @xmath432 with respect to the channel ( lemma  [ lemma : fmi_properties ] ) in order to relate the sum above to the capacity of the estimated averaged channel @xmath385 : @xmath433 substituting in we obtain : @xmath434 because the actual rate that the scheme achieves is not @xmath435 but @xmath436 , we have : @xmath437 considering the second term , notice that the expression for @xmath438 in lemma  [ lemma : rateless_prior_predictor_lemma ] , is sublinear in @xmath260 , i.e. @xmath439 is decreasing with @xmath260 , and therefore @xmath440 , and we can replace the offset term in by @xmath438 .    as for the factor @xmath441 we have @xmath442}_{\\delta_1 } .",
    "\\end{split}\\ ] ] and by using @xmath443 we have the desired result .",
    "we would now like to show the convergence of @xmath385 to @xmath244 .",
    "as mentioned above , @xmath346 and @xmath376 are statistically dependent . to avoid conditioning on @xmath346",
    ", we first write @xmath385 in an alternative form . plugging the explicit form of @xmath376 from into the definition of @xmath385",
    ", we have :    @xmath444    recall that the averaged channel is @xmath445 we would like to show that @xmath446 . define @xmath447 , \\ ] ] then @xmath448 although @xmath449 are not i.i.d .",
    ", they constitute a bounded martingale difference sequence , where the martingale is @xmath450 , as we will show below .",
    "first , by , each component @xmath449 is bounded @xmath451 , so they be bounded in absolute value by @xmath452 .",
    "on average over the common randomness , each symbol @xmath453 is generated @xmath454 independent of the past ( given @xmath455 ) .",
    "in other words , for someone not knowing the specific codebook , the knowledge of past values of @xmath456 does not yield any information about @xmath453 when @xmath455 is given . define the state variable @xmath457 .",
    "note that @xmath458 is only generated as a function of past symbols and therefore can be considered as part of the state at time @xmath389 .",
    "we have : @xmath459 & = \\frac{\\pr(x_k = x , y_k = y | s_{k-1})}{n \\cdot \\hat q_{b_k}(x ) } - \\frac{w_k(y|x)}{n } \\\\ & = \\frac{\\hat q_{b_k}(x ) \\cdot w_k(y|x)}{n \\cdot \\hat q_{b_k}(x ) } - \\frac{w_k(y|x)}{n } = 0 .",
    "\\end{split}\\ ] ] now , since the previous value of the sum @xmath460 is only a function of @xmath461 , by applying the iterated expectations law we have @xmath462 \\\\ & = \\e \\left\\ { \\e \\left [ \\gamma_k(x , y ) \\bigg| s_{k-1 } , \\sum_{j=1}^{k-1 } \\gamma_j \\right ] \\bigg| \\sum_{j=1}^{k-1 } \\gamma_j \\right\\ } = 0 , \\end{split}\\ ] ] which shows @xmath463 is a martingale",
    ". we can now apply hoeffding - azuma inequality ( * ? ? ?",
    "* a.1.3)@xcite@xcite and obtain : @xmath464 the above holds for each value of @xmath425 separately . to bound the @xmath465 norm we use the union",
    "bound : @xmath466 \\right\\ } \\\\ & \\leq \\sum_{x , y } \\pr \\left\\ { \\left| { \\breve{w}}_a(y|x ) - \\overline{w}(y|x ) \\right| > t \\right\\ } \\\\ & \\stackrel{\\eqref{eq:1580}}{\\leq } 2 |\\mathcal{x}| \\cdot |\\mathcal{y}| \\cdot e^{-\\frac{2 n \\lambda^2 t^2}{|\\mathcal{x}|^2 } } .",
    "\\end{split}\\ ] ] to guarantee the above holds with probability at most @xmath467 we choose @xmath468 to make the rhs equal @xmath467 : @xmath469 this is summarized in the following proposition :    [ prop : symbolwise_scheme_channel_convergence2 ] for any @xmath470 , and for @xmath471 defined above , @xmath472    observe that a large @xmath297 improves the channel estimate convergence ( reduces @xmath471 ) , since it increases the minimum rate at which each input symbol is sampled .",
    "this is the additional role of @xmath297 that we did not have in lemma  [ lemma : c_overlinew_achievability_w_side_info ] .",
    "the final step is to link the difference in the channels @xmath473 to the difference in capacities . for this purpose",
    "we use the following lemma :    [ lemma : fmi_lp_bound ] let @xmath326 be an input distribution on the discrete alphabet @xmath22 , @xmath474 a conditional distribution , and @xmath340 a false conditional distribution .",
    "define @xmath475 where @xmath476 assuming @xmath477 we have : @xmath478 and @xmath479 where @xmath480 for @xmath481 , by convention @xmath482 .",
    "furthermore @xmath483 is concave and monotonically non - decreasing for @xmath484 .",
    "note that the lemma is also true with respect to legitimate distributions .",
    "the proof of the lemma is based on cover and thomas @xmath485 bound on entropy @xcite , and s  inequality , and appears in appendix  [ sec : lp_bound_on_capacity ] .",
    "we now combine the results above as follows : choose a value of @xmath467 .",
    "we denote by @xmath486 the event of any decoding error occurring in any of the blocks , and by @xmath487 the event @xmath488 .",
    "we use and overline @xmath31 to denote complementary events .    consider the event @xmath489 . in this case",
    ", we have @xmath490 and from lemma  [ lemma : fmi_lp_bound ] this implies @xmath491 where @xmath492 .",
    "from proposition  [ prop : symbolwise_scheme_predictor_rate ] we have that : @xmath493 to summarize , if @xmath489 then @xmath251 . by the union",
    "bound and propositions  [ prop : symbolwise_scheme_channel_convergence2],[prop : symbolwise_scheme_err_probability ] , we have : @xmath494    note that although lemma  [ lemma : fmi_lp_bound ] is stated for general @xmath495 norms , we have used it here only with respect to the @xmath465 norm , since it is relatively simple to obtain bounds on the convergence of @xmath496 by using the well known hoeffding - azuma inequality per channel element ( @xmath336 ) and the union bound .",
    "however as the distribution of @xmath385 tends to a mutlivariate gaussian distribution , using @xmath497 norm seems to be more suited .",
    "indeed , applying lemma  [ lemma : fmi_lp_bound ] with @xmath497 norm , together with the ( yet unpublished ) bound on the @xmath497 convergence of vector martingales due to hayes @xcite yields tighter bounds on the probability of having a small difference @xmath498 for large alphabet sizes .",
    "we now substitute the numerical expressions for the various overheads , and set the parameters of the scheme to optimize the convergence rate .",
    "@xmath499 are parameters of choice , and together with @xmath500 they determine @xmath501 .",
    "our purpose is to choose @xmath500 that will approximately minimize @xmath501 .",
    "this part is rather tedious .",
    "we write @xmath501 and collect all the relations below :    @xmath502 \\\\ \\delta_c   & = & - 2 \\delta_w \\cdot |\\mathcal{y}| \\log ( \\delta_w ) \\\\",
    "\\delta_w & = & \\frac{|\\mathcal{x}|}{\\lambda } \\sqrt{\\frac{1}{2n } \\ln \\left ( \\frac{2 |\\mathcal{x}| \\cdot |\\mathcal{y}|}{\\delta_0 } \\right ) } \\\\ \\dpred & = & \\frac{k}{n } + i_{\\max } \\cdot \\lambda + c_1 \\sqrt{\\frac{\\ln ( n)}{n } } \\lambda^{-\\half }   .",
    "\\\\ c_1 & = & 2 \\sqrt{k \\cdot |\\mathcal{x}| ( |\\mathcal{x}|-1 ) \\cdot i_{\\max}}\\end{aligned}\\ ] ]    since @xmath503 , @xmath504 , therefore @xmath505 . to make @xmath506 we need @xmath507 , and making this assumption",
    ", we have that the last element in @xmath508 is bounded by @xmath509 . further assuming that @xmath510 ( this holds trivially for the values of @xmath393 of lemma  [ lemma : fmi_as_remp ] when @xmath511 ) , and @xmath512 ( for some arbitrary polynomial decay rate @xmath513 ) we have @xmath514 \\\\&= \\frac{\\log n}{k } ( d_{\\epsilon } + \\tfrac{5}{4 } k_0 + \\tfrac{3}{2 } ) .",
    "\\end{split}\\ ] ] using these bounds and extracting the constants we can upper bound @xmath501 by : @xmath515 where element @xmath516 stems from @xmath508 , @xmath517 from @xmath518 and @xmath519 from @xmath403 , and the constants are : @xmath520 as we shall see , element @xmath521 is negligible .",
    "therefore we first optimize the sum of @xmath516 and @xmath522 with respect to @xmath260 , using lemma  [ lemma : ab_alphabeta ] .",
    "we write the sum as @xmath523 with @xmath524 .",
    "since @xmath260 is required to be integer , we write it as a function of a real valued parameter @xmath468 : @xmath525 , and assume @xmath526 .",
    "then @xmath527 , and therefore @xmath528 . by optimizing the bound with respect to @xmath468 using lemma  [ lemma : ab_alphabeta ]",
    ", we obtain @xmath529 where we defined @xmath530    @xmath531    substituting in ( and upper bounding element @xmath521 by @xmath532 ) , we obtain : @xmath533 to determine @xmath297 we notice that it is a trade - off between element @xmath534 which is increasing in @xmath297 and either @xmath535 or @xmath517 which are decreasing . minimizing any combination separately ( i.e. @xmath536 or @xmath537 ) using lemma  [ lemma : ab_alphabeta ] , yields the same decay rate @xmath538 , and @xmath297 of the form @xmath539 therefore this determines the best decay rate possible for .",
    "note that we do not have to worry about the case @xmath540 , since in this case the term @xmath541 in will exceed @xmath300 and theorem  [ theorem : c_overlinew_achievability ] will be true in a void way .",
    "substituting @xmath297 we have : @xmath542 \\cdot \\left ( \\frac{\\ln^2 ( n)}{n } \\right)^{\\tfrac{1}{4 } } + c_5 \\cdot \\left ( \\frac{\\ln n}{n^2 } \\right)^{\\frac{1}{3 } } \\\\&\\leq \\left [ \\tfrac{3}{2 } \\cdot \\left ( \\tfrac{5}{2 } \\frac{c_2 c_4 ^ 2}{c_{\\lambda } }   \\right)^{\\tfrac{1}{3 } } + \\frac{c_3}{c_{\\lambda } } + i_{\\max } \\cdot c_{\\lambda }   + 1\\right ] \\cdot \\left ( \\frac{\\ln^2 ( n)}{n } \\right)^{\\tfrac{1}{4 } } \\\\&= c_{\\delta } \\cdot \\left ( \\frac{\\ln^2 ( n)}{n } \\right)^{\\tfrac{1}{4 } } , \\end{split}\\ ] ] where in the last inequality we substituted the expression for @xmath543 and assumed @xmath544 . in the last step we defined @xmath545    we now revisit the assumptions we have made along the way .    * in , we assumed @xmath544 .",
    "this requires that @xmath546 , and a sufficient condition is @xmath547 . * for we assumed @xmath507 . substituting @xmath297 leads to @xmath548 , and a sufficient condition is @xmath549 * for we assumed @xmath512 .",
    "we may simply determine @xmath513 and set @xmath550 .",
    "* for we assumed @xmath510 , i.e. @xmath551 * the application of lemma  [ lemma : rateless_prior_predictor_lemma ] to obtain proposition  [ prop : symbolwise_scheme_predictor_rate ] requires that @xmath230 and @xmath552 . since @xmath553",
    "it is sufficient that @xmath287 , or @xmath554 .",
    "furthermore for we assumed @xmath555 , so we require @xmath556 . substituting @xmath557 leads to the sufficient condition : @xmath558 + to summarize , the results holds for @xmath256 where @xmath255 is the maximum of the conditions of , , and of @xmath230 : @xmath559 . \\end{split}\\ ] ]    this proves corollary  .",
    "the claims of the theorem are milder and are easily deduced from this corollary . given @xmath560 , let @xmath561 , and choose any @xmath562 and @xmath563 .",
    "choose @xmath247 large enough so that the error probability given by the corollary satisfies @xmath564 , and @xmath565 .",
    "this guarantees that for @xmath249 , the requirements of the corollary are met the error probability is @xmath566 , and the probability to fall short of the rate is at most @xmath567 .",
    "this concludes the proof of theorem  [ theorem : c_overlinew_achievability ] .",
    "following is a numerical example for the calculation of @xmath248 and @xmath255 in theorem  [ theorem : c_overlinew_achievability ] .",
    "[ example : symbolwise_random_prior_predictor ] for @xmath568 , @xmath569 and @xmath570 we obtain @xmath571 and @xmath572 .",
    "choosing @xmath573 we obtain @xmath574 and @xmath575 .",
    "the convergence rate is rather slow and we have @xmath576 only for @xmath577 .      during the proof of theorem  [",
    "theorem : c_overlinew_achievability ] we assumed the channel sequence is unknown but fixed .",
    "it is easy to see that the same proof holds even if the channel sequence is determined by an online adversary .",
    "the error probability ( proposition  [ prop : symbolwise_scheme_err_probability ] ) is maintained regardless of channel behavior , because the probabilistic assumptions made refer to the distribution of codewords that were _ not _ transmitted .",
    "proposition  [ prop : symbolwise_scheme_predictor_rate ] does not make any assumptions on the channel as it connects the communication rate with the _ measured _ channel .",
    "the main difference is with respect to channel convergence . for the proof of proposition  [ prop : symbolwise_scheme_channel_convergence2 ] to hold we need to show that @xmath578 remains a bounded martingale difference sequence , which boils down to verifying still holds , i.e. that @xmath578 has zero mean conditioned on the past . adding the message to the state variable @xmath461 defined before , i.e. redefining @xmath579 , where @xmath580 is the message bit sequence , we have that holds even when the channel @xmath581 is a function of @xmath461 .",
    "although channels with memory of the input are not considered in this paper , the scheme presented above can be used over such channels as well . in this case",
    ", the performance of the scheme can be characterized as follows :    [ lemma : prior_prediction_channel_with_memory ] when the scheme of theorem  [ theorem : c_overlinew_achievability ] is operated over a general channel @xmath582 , the results of the theorem hold if the averaged channel is redefined as follows : @xmath583    note that for each pair @xmath336 , @xmath584 is a random variable depending on the history @xmath585 , and therefore , different from the main setting considered in this paper , @xmath244 is also a random variable .",
    "the definition above coincides with the previous definition of @xmath244 when the channel is memoryless in the input .",
    "this lemma is used in @xcite to show competitive universality for channels with memory of the input .    _",
    "proof : _ as in the proof of corollary  [ corollary : symbolwise_random_prior_predictor1 ] it is easy to see that assumptions on the channel apply only to proposition  [ prop : symbolwise_scheme_channel_convergence2 ] showing the convergence of the average estimated channel @xmath385 to @xmath244 . to show proposition  [ prop : symbolwise_scheme_channel_convergence2 ] holds ,",
    "we need to show that @xmath578 remains a bounded martingale difference sequence , where now @xmath578 is defined as : @xmath586 . \\end{split}\\ ] ] as in , we have @xmath587 .",
    "equation now becomes @xmath588 & = \\frac{\\pr(x_k = x , y_k = y | s_{k-1})}{n \\cdot \\hat q_{b_k}(x ) } \\\\ & \\qquad - \\frac{1}{n } \\pr(y_k = y | x_k = x , \\vr x^{k-1 } , \\vr y^{k-1 } ) \\\\ & = \\frac{\\hat q_{b_k}(x ) \\cdot \\pr(y_k = y | x_k = x , \\vr x^{k-1 } , \\vr y^{k-1})}{n \\cdot \\hat q_{b_k}(x ) } \\\\ & \\qquad - \\frac{1}{n } \\pr(y_k = y | x_k = x , \\vr x^{k-1 } , \\vr y^{k-1 } ) \\\\ & = 0 .",
    "\\end{split}\\ ] ] the rest of the proof of proposition  [ prop : symbolwise_scheme_channel_convergence2 ] remains the same .",
    "in this section we discuss the relation of the current results to existing results pertaining to unknown channels and make some comments on schemes presented here .",
    "it is interesting to compare the target rate @xmath235 with the avc capacity",
    ". we will give a short background on the avc and the relation to the current problem .",
    "in the traditional avc setting @xcite , the channel model is similar to the setting assumed here , but slightly more constrained .",
    "the channel in each time instance is assumed to be chosen arbitrarily out of a set of channels , each of which is determined by a state .",
    "frequently , constrains on the state sequence ( such as maximum power , number of errors ) are defined .",
    "the avc capacity is the maximum rate that can be transmitted reliably , for every sequence of states that obeys the constraints .",
    "the avc capacity may be different depending on whether the maximum or the average error probability over messages is required to tend to zero with block length , on the existence of feedback , and on whether common randomness is allowed , i.e. whether the transmitter and the receiver have access to a shared random variable .",
    "the last factor has a crucial effect on the achievable rate as well as on the complexity of the underlying mathematical problem : the characterization of avc capacity with randomized codes is relatively simple and independent on whether maximum or average error probability is considered , while the characterization of avc capacity for deterministic codes is , in general , still an open problem .",
    "randomization has a crucial role , since we consider the worst - case sequence of channels .",
    "this sequence of channels is chosen after the deterministic code was selected ( and therefore sometimes viewed as an adversary ) , enabling the worst - case sequence of channels to exploit vulnerabilities that exist in the specific code .",
    "as an example , for every symmetrizable avc ( * ? ? ?",
    "* definition 2 ) , the avc capacity for deterministic codes is zero ( * ? ? ?",
    "* theorem 1 ) .",
    "when randomization does exist , the random seed is selected `` after '' the channel sequence was selected ( mathematically , the probability over random seeds is taken after the maximum error probability over all possible sequences ) , and therefore prevents tuning the channel to the worst - case code .",
    "when randomization exists , the channel inputs may be made to appear independent from the point of view of the adversary , thus limiting effective adversary strategies .",
    "therefore the results in the current paper assume common randomness exists .",
    "we would now like to compare the target rate @xmath235 with the randomized avc capacity .",
    "the discrete memoryless avc capacity without constraints may be characterized as follows : let @xmath589 be the set of possible channels that are realized by different channel states ( for example in a binary modulo - additive channel with an unknown noise sequence , there are two channels in the set  one in which @xmath590 and another in which @xmath591 ) . this set is traditionally assumed to be finite , i.e. there is a finite number of `` states '' , however this constraint is immaterial for the comparison .",
    "the randomized code capacity of the avc is ( * ? ? ?",
    "* theorem 2 ) : @xmath592 where @xmath593 is the convex hull of @xmath589 , which represents all channels which are realizable by a random drawing of channels from @xmath589 . over channel states in @xcite . ] in the example , @xmath593 would be the set of all binary symmetric channels .",
    "when input or state constraints exist , they affect simply by including in the set of @xmath11-s and in @xmath594 only those priors , or channels , that satisfy the constraints ( respectively ) .",
    "the converse of is obtained by choosing the worst - case channel @xmath595 and implementing a discrete memoryless channel ( dmc ) where the channel law is @xmath596 , by a random selection of channels from @xmath589 .",
    "hence it is clear that the randomized code capacity can not be improved by feedback .",
    "in contrast , the deterministic code avc capacity can be improved by feedback , and in some cases made to equal to the randomized code capacity @xcite@xcite@xcite . therefore , most existing works on feedback in avc deal with the deterministic case .    since by definition @xmath597 , we have from , @xmath598 , i.e. our target rate meets or exceeds the avc capacity . while in the traditional setting ,",
    "a - priori knowledge of @xmath589 or state constraints on the channel is necessary in order to obtain a positive rate , here we attain a rate possibly higher than the avc capacity , without prior knowledge of @xmath589 . this is important since without such constraints , i.e. when the channel sequence is completely arbitrary , the avc capacity is zero .",
    "this property makes the system presented here universal , with respect to the avc parameters , a universality which also holds in an online - adversary setting ( corollary  [ corollary : symbolwise_random_prior_predictor1 ] ) .",
    "we can view the difference between @xmath599 and @xmath235 as the difference between the capacities of the worst realizable channel @xmath600 , and the specific channel @xmath597 representing the average of the sequence of channels that actually occurred .",
    "this difference is obtained by adapting the communication rate to the capacity of the average channel , and adapting the input prior to the prior that achieves this capacity , whereas in the avc setting , the rate and the prior are determined a - priori , based on the worst - case realizable channel .    as we noted above",
    ", feedback can not improve the randomized avc capacity .",
    "therefore the improvement is attained not merely by the use of feedback , but by allowing the communication rate to vary , whereas in the traditional avc setting , one looks for a fixed rate of communication which can be guaranteed a - priori ( note that the improvement is not in the worst case ) . in allowing the rate to vary ,",
    "we have lost the formal notion of capacity ( as the supremum of achievable rates ) , thereby making the question of setting the target rate more ambiguous , but nevertheless improved the achieved rates .",
    "the capacity of the averaged channel @xmath235 is a slight generalization of the notion of _ empirical capacity _ defined by eswaran  @xcite .",
    "the only difference is releasing the assumption made there , that the set of channel states is finite .",
    "the empirical capacity of eswaran is in itself a generalization of the empirical capacity for modulo additive channels defined by shayevitz and feder  @xcite .",
    "eswaran  @xcite assume the prior @xmath11 is given a - priori and attain the empirical mutual information @xmath601 .",
    "the scheme used here is similar to the scheme they presented in its high level structure .",
    "we can view the current result ( theorem  [ theorem : c_overlinew_achievability ] ) as an improvement over the previous work , i.e. attaining the capacity @xmath602 , rather than the mutual information , by the addition of the universal predictor .",
    "our result answers the question raised there @xcite , whether the empirical capacity is attainable .",
    "another small extension is in corollary  [ corollary : symbolwise_random_prior_predictor1 ] , showing that the result holds in an adversarial setting .",
    "this extension is outside our main focus of communicating over unknown channels , and is only used to strengthen the claim on universality with respect to the avc parameters .",
    "the main result ( theorem  [ theorem : c_overlinew_achievability ] ) could be derived in a conceptually simpler but crude scheme , by combining the results of eswaran @xcite or our previous paper @xcite with theorem  [ theorem : prior_predictor_exp ] .",
    "the transmission time @xmath33 may be divided into multiple fixed - size blocks @xmath603 , and in each block , one of these schemes is operated , with an i.i.d .",
    "prior chosen by a predictor . using eswaran s result , for example , and ignoring some details such as finite - state assumptions",
    ", one would obtain the rate @xmath604 over each block , where @xmath267 is the averaged channel over the block .",
    "the channel @xmath267 can be well estimated ( e.g. using training symbols or using the communication scheme itself ) .",
    "assuming it is known , if the prediction scheme of theorem  [ theorem : prior_predictor_exp ] is operated over @xmath267 it will guarantee the average rate over the @xmath247 blocks will be asymptotically at least @xmath605 for any @xmath11 , and using convexity , @xmath606 . since this holds for any @xmath11 this achieves the capacity of the average channel .",
    "note that here it appears that there is no need for the uniform prior , however this is somewhat hidden in the assumption that the channel is known .",
    "furthermore there is no need to worry about rateless blocks extending `` forever '' since the commnication scheme is re - started on each of the @xmath247 blocks .      in a related paper @xcite we presented the concept of the iterated finite block capacity @xmath607 of an infinite vector channel , which is similar in spirit to the finite state compressibility defined by lempel and ziv @xcite . roughly speaking , this value is the maximum rate that can be reliably attained by any block encoder and decoder , constrained to apply the same encoding and decoding rules over sub - blocks of finite length .",
    "the positive result is that @xmath607 is universally attainable for all modulo - additive channels ( i.e. over all noise sequences ) .",
    "the result is obtained by a system similar to the one described in section  [ sec : arbitrary_var_rateless_scheme ] , while the input prior is fixed to the uniform prior .",
    "the result uses two key properties of the modulo additive channel :    1 .",
    "the channel is memoryless with respect to the input @xmath3 ( i.e. current behavior is not affected by previous values of the input ) .",
    "the capacity achieving prior is fixed for any noise sequence .",
    "the current work is a step toward removing the second assumption .",
    "the capacity of the averaged channel is a bound on the rate that can be obtained reliably by a transmitter and a receiver operating on a single symbol , since the channel that this system `` sees '' can be modeled as a random uniform selection of a channel out of @xmath39 , which we term the `` collapsed channel '' @xcite . by combining @xmath389 symbols into a single super - symbol",
    ", we can extend the result and obtain a rate which is equal to or better from the rate obtained by block encoder and decoder operating over chunks of @xmath389 symbols .",
    "therefore the current result suggests that it is possible to attain @xmath607 for all vector channels that are memoryless in the input , i.e. that have the form defined in , for an arbitrary sequence of channels @xmath4 ( compared to only an arbitrary noise sequence , in the previous result ) .",
    "it is interesting to consider the converse ( theorem  [ theorem : c_overlinew_optimality ] ) from the following point of view : suppose a competitor is given the entire sequence of channels @xmath56 , but is allowed to take from this sequence only the `` histogram '' ( a list of channels and how many times they occurred ) , and devise a communication system based on this information .",
    "the rate that can be guaranteed in this case is limited by @xmath235 . on the other hand , assuming common randomness exists , it is enough to know @xmath244 in order to attain @xmath235 without feedback .",
    "to see this intuitively , we may apply a random interleaver and use the fact the interleaved channel is similar to a dmc with the channel law @xmath244 .",
    "therefore even if one knows the entire histogram of the sequence , the average channel @xmath244 , which contains less information , contains all information necessary for communication .    to illustrate this , consider the deterministic setting , where instead of a sequence of channel laws @xmath40 we have a sequence of deterministic functions @xmath608",
    "this is a particular case of our problem , with @xmath609 . even in this case , according to theorem  [ theorem : c_overlinew_optimality ] , a competitor knowing the list of functions up to order , will not be able to guarantee a rate better than @xmath235 , where @xmath610 , i.e. a channel created by counting for each @xmath17 , the normalized number of times a certain @xmath18 would appear as output .    comparing the amount of information in the channel histogram and the averaged channel in this case ,",
    "there are @xmath611 functions , and therefore the distribution is given by @xmath612 real numbers .",
    "on the other hand , the average channel is a probability distribution from @xmath613 to @xmath614 and is specified by @xmath615 real numbers .",
    "an interesting property revealed through the example , is that although the setting is deterministic , the result is given in terms of probability functions .",
    "these `` probabilities '' are only averages related to the deterministic function sequence , but this shows that the formulation via probabilities ( or frequencies ) is more natural than by specifying the function @xmath616 between the input and output .",
    "we assumed the feedback channel has unlimited rate , and is free of delays and errors .",
    "this was done mainly to focus the discussion and simplify the results .",
    "it is clear from the scheme presented , that because the amount of information required to be fed back to the transmitter can be made small , the capacity of the average channel could be attained even if the feedback link has any small positive rate and a fixed delay .",
    "if the feedback channel is such that errors can be mitigated by coding with finite delay , then errors can be accommodated as well .",
    "specifically , we show in appendix  [ sec : zero_feedback_rate ] that when the feedback rate is limited , or there is a fixed delay , the penalty is a gap of at most @xmath617 symbols between the blocks , and that the normalized loss from this effect tends to zero .",
    "therefore we have @xmath618 ( with the notation of theorem  [ theorem : c_overlinew_achievability ] ) , with any positive feedback rate and any fixed delay .",
    "the gap may be reduced by using the time of the @xmath1-th block to transfer the channel information from block @xmath99 and use it only in block @xmath266 ( i.e. insert a delay of one block in the prediction scheme ) , however this approach is not analyzed here .      throughout the course of this paper , as we have gradually made",
    "our assumptions more realistic , we have seen a deterioration of the rate of convergence , of the achieved rate to the target rate .",
    "we denote by @xmath619 the gap between the guaranteed rate and the target rate , and focus on the dominant polynomial power @xmath620 , while ignoring the @xmath621 terms .",
    "we have @xmath622 in the synthetic problem of section  [ sec : toy_problem ] ( assuming `` block - wise '' variation )  [ sec : toy_problem ] , @xmath623 when using the rateless scheme under assumptions of perfect average channel knowledge  [ sec : arbitrary_channel_var ] , and @xmath624 when releasing the abstract assumptions  [ sec : mainproof ] . the first deterioration ( between @xmath126 and @xmath625 ) is mainly attributed to the rateless coding scheme .",
    "more specifically , it stems from mixing with the uniform prior , which is necessary to bound the regret per block when the blocks have variable lengths . the second deterioration ( between @xmath625 and @xmath626 )",
    "can be attributed mainly to the fact that the number of bits per block @xmath260 has to increase in a certain rate in order to balance overheads created by the universal decoding procedure ( and reduces the rate of adaptation ) . while the rate of convergence which was achieved deteriorates , the only upper bound we presented on the convergence rate is @xmath627 (  [ sec : regret_lb ] ) , which is tight only for the first case .",
    "we do not know whether better convergence rates can be attained in theorems  [ lemma : c_overlinew_achievability_w_side_info],[theorem : c_overlinew_achievability ] .",
    "variation '' ) & arbitrarily varying channel , with side information on average channel and without communication overheads & arbitrarily varying channel & notes + reference &  [ sec : toy_problem ] , theorem  [ theorem : prior_predictor_exp ] &  [ sec : arbitrary_var_exp_prior_predictor ] , lemma  [ lemma : c_overlinew_achievability_w_side_info ] &  [ sec : problem_setting],[sec : mainproof ] , theorem  [ theorem : c_overlinew_achievability ] & + @xmath68 attainability & no & no & no & @xmath68 = capacity of @xmath628 = mean capacity @xmath629 + @xmath70 attainability & yes & no & no & @xmath70 = mean mutual information with fixed prior @xmath630 + @xmath69 attainability & yes & yes & yes & @xmath69 = capacity of the time - averaged channel @xmath631    1 .",
    "best attainable rate not using time structure ( theorem  [ theorem : c_overlinew_optimality ] ) .",
    "2 .   @xmath632 ( section  [ sec : discussion_avc ] )     + normalized regret lower bound & @xmath633 & @xmath633 & @xmath633 & + normalized regret attained & @xmath634 & @xmath635 & @xmath636 & +      the results in this paper were obtained by exponential weighting .",
    "this scheme was selected mainly due to its simplicity and elegance .",
    "unfortunately , the exponential weighting is performed over a continuous domain ( of probabilities ) , and therefore it is not immediately implementable . of course",
    ", the simplest practical solution could be discrete sampling of the unit simplex and replacement of the integrals by sums .",
    "since the mutual information is continuous , it is possible to bound the error resulting from this discretization .",
    "an alternative way is to quantize the set of priors . instead of competing against a continuum of reference schemes",
    ", we first reduce the number of reference schemes to a finite one , by creating a `` codebook '' of priors @xmath637 .",
    "this codebook is designed so that the penalty in the mutual information resulting from rounding to the nearest codeword , is small .",
    "this quantization is useful in terms of the feedback link , which now only has to convey the index @xmath78 .",
    "having quantized the priors , we may replace the predictors shown here by standard schemes used for competition against a finite set of references @xcite,@xcite .",
    "see a rough analysis of this approach in appendix  [ sec : prior_quantization_analysis ] .",
    "an alternative approach is to bypass the explicit calculation of the predictor @xmath72 and use a rejection - sampling based algorithm to generate a random variable @xmath638 .",
    "this approach is demonstrated in appendix  [ sec : rejection_sampling_predictor ] .",
    "zinkevich @xcite proposed a computationally efficient online algorithm , based on gradient descent , to solve a problem of minimizing the sum of convex functions , each revealed to the forecaster after the decision was made ( a similar setting to that of lemma  [ lemma : rateless_prior_predictor_lemma ] ) . to apply zinkevich s results to our problem",
    ", some modifications are required .",
    "the mutual information does not have a bounded gradient ( which is required by @xcite ) , but this could be bypassed by keeping away from the boundary of @xmath20 , i.e. from these points for which one of the elements of @xmath11 is @xmath27 or @xmath25 .",
    "one way to accomplish this is by mixing with the uniform prior when defining the target rate , and use @xmath639 as a target , and then bounding the loss induced by this mixture . in the rateless scheme ,",
    "a bound on the maximum value of @xmath296 ( of lemma  [ lemma : rateless_prior_predictor_lemma ] ) is required and can be obtained using the same methods presented here .",
    "another application of sequential algorithms to solve problems related to avc s was proposed by buchbinder  @xcite who used a sequential algorithm to solve a problem of dynamic transmit power allocation , where the current channel state is known but future states are arbitrary .      in the communication scheme proposed in section  [",
    "sec : arbitrary_var_rateless_scheme ] we chose to use an i.i.d . prior during each block , and update the prior only at the end of the block .",
    "this choice is motivated by the following considerations :    * assuming no explicit training symbols are transmitted , the estimation of the channel @xmath640 is done based on the encoded sequence , which is known to the receiver only after decoding ( at the end of the block ) . * varying the prior throughout the block inserts memory into the channel input , which complicates the analysis .",
    "the result of this is a relatively slow update of the prior , essentially limited by the block size , which is determined based on communication related considerations ( overheads and error probabilities ) .",
    "an alterative would be learning the channel through random training symbols ( see for example @xcite ) , and updating the prior from time to time , without relation to the rateless blocks .      in section",
    "[ sec : regret_lb ] we have shown a lower bound on the redundancy in attaining @xmath70 by using a counter example with @xmath112 .",
    "it is worth mentioning that for the set of binary channels @xmath641 , the normalized regret is not necessarily @xmath137 .",
    "for this set of channels , the optimal prior does not reach the boundaries of @xmath642 $ ] : the two input probabilities @xmath643 are always in @xmath644 $ ] @xcite .",
    "it is possible to show that the loss function @xmath645 satisfies conditions 1,2,4 in cesa - bianchi and lugosi s book @xcite theorem 3.1 ( but not condition 3 ) .",
    "this fact together with experimental results showing convergence of the fl predictor , suggests that the normalized minimax regret in this case may converge like @xmath646 .",
    "in the prediction scheme of theorems  [ lemma : c_overlinew_achievability_w_side_info],[theorem : c_overlinew_achievability ] , we mixed a uniform prior with an exponentially weighted predictor .",
    "this mixing has two advantages :    1 .   enabling to bound the instantaneous regret caused by a large block due to a low mutual information 2 .",
    "enabling channel estimation by making sure all input symbols have a non zero probability .",
    "note that alternative solutions are use of training symbols at random locations and termination and re - transmission of blocks whose length exceeds a threshold .    mixing the exponentially weighted predictor with a uniform distribution is a technique used in prediction problems with partial monitoring , where the predictor only has access to its own loss ( or a function of it ) and not to the loss of the competitors @xcite , and effectively assigns some time instances for sampling the range of strategies . in our problem the uniform prior plays two roles .",
    "one , is related to the rateless communication scheme , which required to relate the gains of the predictor to the gain of any alternative prior @xmath11 in order to have an upper bound on the latter .",
    "the second role is in the convergence of the estimated channel ( proposition  [ prop : symbolwise_scheme_channel_convergence2 ] ) .",
    "the second role is similar to the role of uniform distribution in partial monitoring problems : the channel @xmath16 can not be estimated for input values @xmath17 that occur with zero probability .",
    "note that even without the explicit uniform component @xmath647 , the exponential weighting element @xmath648 in includes a small uniform component . particularly , since referring to , @xmath649 , @xmath650 and @xmath651 however this value is too small for our purpose .      in the current paper we assumed the input and output alphabets are finite .",
    "in general it is not possible to universally attain @xmath70 or @xmath69 , even in the context of the synthetic problem of section  [ sec : toy_problem ] , when the alphabet size @xmath22 is infinite .",
    "this is since in the continuous case one is trying to assign a probability @xmath11 to an infinite set of values , where the values producing the capacity may be a small subgroup .",
    "consider the following example :    let the channel @xmath652 , with input @xmath17 and output @xmath18 ( @xmath653 ) be defined by the arbitrary sequence @xmath654 , @xmath655 , with all @xmath656 .",
    "the channel rule is defined by : @xmath657 for any sequential predictor ( even randomized ) we can find a sequence of channels @xmath658 such that the values of the sequence @xmath659 at each step have total probability zero ( since the input distribution may have at most a countable group of discrete values with non zero probability ) . therefore we can always find a sequence of channels where the rate obtained by the predictor would be zero . on the other hand , each channel @xmath660 has infinite capacity ( since it can transmit noiselessly any integer number ) .",
    "therefore the value of @xmath70 is infinite ( it is enough to choose a prior suitable for one of the channels in the sum ) .",
    "it stands to reason that under suitable continuity conditions on @xmath16 and input constraints on @xmath326 , we may convert the problem to a discrete one , while bounding the loss in this conversion , by discretization of the input ",
    "i.e. by selecting the input from a finite grid , or alternatively assuming a parametrization of the channel .",
    "we considered the problem of adapting an input prior for communication over an unknown and arbitrarily varying channel , comprised of an arbitrary sequence of memoryless channels , using feedback from the receiver .",
    "we showed that it is possible to asymptotically approach the capacity of the time - averaged channel universally for every sequence of channels .",
    "this rate equals or exceeds the randomized avc capacity of any memoryless channel with the same inputs , and thus the system is universal with respect to the avc model .",
    "the result holds also when the channel sequence is determined adversatively .",
    "we also presented negative results showing which communication rates or minimax regret convergence rates can not be attained universally ( see a summary in table  [ tbl : prior_pred_summary ] ) , and presented a simplified synthetic problem relating to prediction of the communication prior , which may have applications for block - fading channels .",
    "when examining the role of feedback in combating unknown channel , previous works mainly focused on the gains of rate adaptation , and here we have seen an additional aspect , namely selection of the communication prior , in which feedback improves the communication rate .",
    "the results have implications on competitive universality in communication , and suggest that with feedback , it would be possible for any memoryless avc , to universally achieve a rate comparable to that of any finite block system , without knowing the channel sequence .",
    "when comparing the results to the traditional avc results , the former setting was prevailed by the notion of capacity , and thus , even when feedback was assumed , it was not used for adapting the communication rate .",
    "here we have shown for the first time , that rates equal to or better from the avc capacity can be attained universally , when releasing the constraint of an a - priori guaranteed rate .",
    "this demonstrates the validity of the alternative `` opportunistic '' problem setting that has been considered in the last decade for feedback communication over unknown channels , a setting which does not focus on capacity .",
    "we thank yishay mansour for helpful discussions on the universal prediction problem .        _",
    "proof : _ let @xmath663 denote a global maximum of @xmath200 in @xmath54 ( which exists since @xmath207 is concave and @xmath54 is closed ) . then from the concavity of @xmath207 for any @xmath664 $ ] we have : @xmath665 note that the rhs is a constant .",
    "denote @xmath666 . then due to convexity @xmath667 and due to the shrinkage @xmath668 . furthermore by ( [ eq:2600 ] ) , @xmath669 .",
    "we have : @xmath670 therefore , @xmath671",
    "\\geq f(\\vr x^ * ) -\\lambda ( b - a ) + \\frac{d \\ln \\lambda}{\\eta } .\\ ] ] maximizing the rhs with respect to @xmath297 we obtain @xmath672 where @xmath673 by the assumptions of the lemma , and substituting @xmath297 we have : @xmath674 rearranging yields the desired result .",
    "during the course of the derivation below we attempt to optimize the asymptotical form of the loss ( up to constant factors ) , and thus we make simplifying assumptions on the parameters , which hold asymptotically for large enough @xmath33 . for finite @xmath33",
    "these assumptions might lead to suboptimal results .",
    "we do not discuss the assumptions during the course of the derivation and we collect them at the end .",
    "all integrals below are by default over the unit simplex @xmath159 .    in the block - wise variation",
    "setting ( section  [ sec : toy_problem ] ) , our target was to control the growth rate of the regret . here",
    ", at each block @xmath1 , by the gain of the competitor using prior @xmath11 is @xmath296 ( bits ) , while the universal scheme sends a fixed number of bits @xmath260 .",
    "therefore the gain of the competitor @xmath296 and the instantaneous regret @xmath675 are related by a constant , and it is more convenient to base the derivation on the gain rather than the regret .",
    "the potential function @xmath278 will be used as an approximation of the @xmath676 in .",
    "denote the cumulative gain of the competitor with prior @xmath11 as : @xmath677 and the potential function of @xmath678 as : @xmath679 note that @xmath680 is not a function of @xmath11 due to the integration over @xmath11 performed by @xmath681 .",
    "we can now write @xmath158 as : @xmath682 the growth of the potential is bounded by : @xmath683 d q \\\\ & = \\phi_{i-1 } \\left [ 1 + \\eta \\int w_i m_i f_{i } d q + \\eta^2 \\int w_i m_i^2 f_{i}^2 d q \\right ] , \\end{split}\\ ] ] where in the last inequality we used lemma  [ lemma : exp_second_order_bound ] and assumed @xmath684 .",
    "the dependence of @xmath316 and @xmath685 on @xmath11 is suppressed for brevity .",
    "we now bound the integrals @xmath686 and @xmath687 .",
    "the property that a badly chosen prior may cause the iterative system to get stuck ( not transmitting any block ) translates into the fact that without placing any limitations on @xmath72 , the competitor s gain , @xmath296 may be unbounded , since @xmath346 might be indefinitely large while @xmath275 can be any positive value .",
    "this is prevented by mixing with the uniform prior , which enables us to link @xmath275 with @xmath295 .",
    "since in the context of the lemma we do not assume @xmath275 is the mutual information , we use a bound which is slightly looser than shulman and feder s , but is based on the same technique @xcite , and only assumes concavity .    define @xmath688 as modulo - addition over the set @xmath22 , and write @xmath689 for any @xmath11 , i.e. express the uniform prior as the mean of all cyclic rotations of @xmath11 . using concavity and non - negativity of @xmath207 : @xmath690 because the prior has the structure @xmath691 , by the concavity of @xmath316 : @xmath692    using in conjunction with we have @xmath693 which yields a bound on the competitor gain in each block .",
    "we now bound the two integrals appearing in .",
    "starting with the first integral , using the concavity of @xmath316 : @xmath694 from which we obtain @xmath695      recall that in the classical weighted average predictor @xcite , the product of the instantaneous regret and the weighting function is guaranteed to be non positive ( blackwell condition ) .",
    "similarly in the previous section we obtained @xmath697 ( see ) . in the present case ,",
    "if we define @xmath698 , then by we have @xmath699 , i.e. due to the inclusion of the uniform prior ( which is needed for @xmath700 to be bounded ) , this integral may be positive , although arbitrarily small .",
    "thus , we pay a price in the first order term in order to be able to bound the second order term .",
    "plugging the bounds , into we have : @xmath701 \\\\ & \\stackrel{(\\ref{eq:845}),(\\ref{eq:850b})}{\\leq } \\phi_{i-1 } \\bigg [ 1 + \\eta \\cdot \\frac{k}{1-\\lambda } + \\eta^2 \\frac{|\\mathcal{x}|}{\\lambda } k \\cdot \\frac{k}{1-\\lambda } \\bigg ] \\\\ & \\stackrel{\\eqref{eq : ex483}}{\\leq } \\phi_{i-1 } e^{\\eta \\cdot \\frac{k}{1-\\lambda } \\left ( 1 + \\frac { \\eta \\cdot   k \\cdot |\\mathcal{x}|}{\\lambda }   \\right ) } \\\\ & \\leq \\ldots \\leq \\phi_0 e^{\\eta \\cdot \\frac{k \\cdot i}{1-\\lambda } \\left ( 1 + \\frac { \\eta \\cdot   k \\cdot |\\mathcal{x}|}{\\lambda }   \\right ) } .",
    "\\end{split}\\ ] ] in the last step we applied the same relation inductively . using",
    "we can obtain a bound on @xmath702 , and we now use lemma  [ lemma : f_exp_weight_ub ] to relate this bound to @xmath703 and to the target rate .",
    "the dimension is @xmath705 . by",
    "we have @xmath706 the reason for setting the upper bound as @xmath707 rather than just @xmath708 , is technical , as this simplifies the conditions required to meet the requirements of the lemma . to satisfy @xmath203",
    "we only require @xmath709 . by lemma",
    "[ lemma : f_exp_weight_ub ] and we have : @xmath710 where @xmath711 is the redundancy term introduced by lemma  [ lemma : f_exp_weight_ub ] . bounding @xmath301 using , while substituting @xmath712 , we obtain : @xmath713 after rearrangement we have the following bound on @xmath48 : @xmath714 where @xmath715 the rest of the proof of lemma  [ lemma : rateless_prior_predictor_lemma ] is an algebraic derivation focused on simplifying and optimizing the bound above .",
    "the lower bound on @xmath48 in the rhs of is increasing with respect to @xmath301 .",
    "this is since @xmath716 is zero for @xmath717 and for @xmath718 the derivative @xmath716 is @xmath719 , which by our assumption @xmath709 is smaller than @xmath25",
    ". therefore @xmath720 .",
    "in order to optimize the parameters , we assume for now that @xmath717 and bound the difference @xmath721 . using @xmath722 we have @xmath723 from , under the assumption @xmath717 we have : @xmath724 we further simplify @xmath725 by making the assumption that @xmath726 and therefore @xmath727 , and @xmath728 . using these simplifications we further bound the rhs of by @xmath729 where @xmath730 and @xmath731 .",
    "applying lemma  [ lemma : ab_alphabeta ] to the optimization of the two terms depending on @xmath192 in ( marked @xmath732 , with powers @xmath733 ) we have : @xmath734 and @xmath735 substituting @xmath736 yields @xmath403 and @xmath192 stated in the lemma .",
    "now , the derivation involving equations  assumes @xmath717 .",
    "since the lower bound on @xmath48 is increasing with respect to @xmath301 , in the case that @xmath737 we are guaranteed to obtain a better lower bound on @xmath48 than the lower bound @xmath738 attained for @xmath739 ( in other words , the rhs of for @xmath739 is at least @xmath740 ) .",
    "therefore the bound can be stated as @xmath741 .",
    "we now collect the various assumptions we have made along the way .",
    "we use the same technique used in the proof of theorem  [ theorem : prior_predictor_exp ] , of showing that if the assumptions do not hold then ( possibly under some simple conditions ) , @xmath742 and therefore the lemma holds in a void way ( since the rhs of becomes non - positive ) .",
    "in we assumed @xmath684 . using the upper bound of we have the sufficient condition @xmath743 .",
    "if this condition does nt hold , i.e. @xmath744 , then the second term in satisfies @xmath745 , so @xmath746 and the lemma holds in a void way .",
    "before we assumed @xmath726 . when the opposite is true , then second term in satisfies the @xmath747 . by requiring @xmath287",
    "we have that in this case the lemma will also be true in a void way . to use lemma  [ lemma : f_exp_weight_ub ] we required @xmath748 .",
    "if the opposite is true , then the third term in satisfies @xmath749 , and thus if @xmath230 , @xmath746 . therefore , by requiring @xmath286 and @xmath287 , we have that if any of the assumptions we made does not hold , the lemma is true in a void way .",
    "this concludes the proof of lemma  [ lemma : rateless_prior_predictor_lemma ] .      in this proof",
    "we use nats ( @xmath750-s are natural base ) . this does not change the results since all values scale according to the base of the @xmath750-s . also , we assume all probabilities and false probabilities are non - zero . it is easy to check that the results for zero probabilities follow by replacing zeros with small probabilities and taking the limit using @xmath751 .",
    "* concavity with respect to @xmath11 * : denote as above @xmath752 and write : @xmath754 the left hand term is linear with respect to @xmath11 .",
    "the function @xmath755 is convex in @xmath468 ( for all @xmath756 ) , and @xmath334 is linear in @xmath11 , therefore the right hand term is convex in @xmath11 , and so @xmath432 is concave with respect to @xmath11 .",
    "[ [ sec : lp_bound_on_capacity ] ] proof of lemma  [ lemma : fmi_lp_bound ] and @xmath495 bounds on differences of entropies and capacities ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    in this section we prove lemma  [ lemma : fmi_lp_bound ] , relating the @xmath495 norm difference of two channels ( one of which may be a false distribution ) to the difference in capacities .",
    "two intermediate results that are captured in lemmas  [ lemma : false_entropy_l1_bound],[lemma : false_entropy_lp_bound ] are an extension of the @xmath485 bound of cover & thomas to false distributions and a trivial extension of the same bound to @xmath495 norms .        also note that the function @xmath769 is monotonous non decreasing for @xmath770 , as can be verified by differentiation .",
    "our first step is to extend the lemma to a case where one of @xmath771 is a false distribution . in cover and thomas proof ,",
    "the first step is to write entropy as @xmath772 where @xmath773 and to show that for all @xmath774 and @xmath775 , the difference in @xmath776 is bounded by @xmath777 . here",
    "@xmath468 represents the minimum of @xmath778 for a certain @xmath18 , @xmath779 the absolute difference , and @xmath780 the maximum of @xmath778 .",
    "then , the difference in entropy is bounded by the sum of the absolute values , this bound is substituted in the summation , and convexity arguments are use to bring it to the desired form .",
    "the only step that needs to be modified is showing that @xmath777 , where now @xmath468 is no longer bounded to @xmath781 .",
    "it can be verified by differentiating the function @xmath782 with respect to @xmath468 that the derivative is always negative for @xmath783 .",
    "in addition , @xmath784 , therefore the maximum absolute of this function , which is the absolute value of either the the maximum or the minimum , occurs at either end of the region to which @xmath468 is limited . in the original proof",
    "this yields @xmath785 ( notice that @xmath786 ) . here , since one of @xmath771 is a legitimate distribution , @xmath787 ( as the minimum of the two ) and we have instead : @xmath788 .",
    "as we will show below , if we limit @xmath789 we have @xmath790 , and therefore the bound @xmath791 applies as in the original proof and cover & thomas result holds . to show this , consider the function @xmath792 .",
    "this function is @xmath27 for @xmath793 , and the derivative is @xmath794 , it is positive in a certain interval @xmath795 and negative for @xmath796 , and therefore it crosses @xmath27 only once . calculating this function for @xmath797 yields a positive value , therefore it is positive for all @xmath789 .",
    "we capture this variation of cover & thomas result in the following lemma :    [ lemma : false_entropy_l1_bound ] let @xmath798 be a distribution on the finite alphabet @xmath38 and @xmath799 be a false distribution on the same alphabet , with @xmath800 , then @xmath801 where the false entropy @xmath802 is defined as @xmath803    we first convert the bound to the @xmath495 norm ( @xmath804 ) . to relate the norms we use s inequality : for two vectors @xmath805 , @xmath806 , where @xmath807 is the  conjugate of @xmath24 and by convention for @xmath481 we define @xmath482 ( note that @xmath808 and the conjugate of @xmath481 is @xmath809 ) .",
    "we have @xmath810 assuming @xmath811 we can use the monotonicity of the bound of lemma  [ lemma : false_entropy_l1_bound ] , and write : @xmath812 where we defined @xmath813 @xmath483 is concave with respect to @xmath468 ( because @xmath814 is concave in @xmath756 ) , and is monotonically non decreasing for @xmath815 , as can be verified by differentiation .",
    "furthermore , to meet the requirement @xmath816 of lemma  [ lemma : false_entropy_l1_bound ] , it is sufficient that @xmath817 ( by ) , and in addition prior to we have assumed @xmath818 , however it is easy to see that this condition is dominated by the previous one . since @xmath819 , and @xmath820 ,",
    "it is sufficient to require @xmath821 . in summary",
    ", we have the following result :    [ lemma : false_entropy_lp_bound ] let @xmath804 , @xmath798 be a distribution on the finite alphabet @xmath38 , and @xmath799 be a false distribution on the same alphabet with @xmath822 , then : @xmath823 where @xmath824 is defined in , and it is concave and monotonically non - decreasing for @xmath484 .",
    "we now write the false mutual information as a difference of false entropies : @xmath825 the above is analogous to the equality @xmath826 . for",
    "the channels @xmath827 define the difference as @xmath828 and define the output distributions as @xmath829 and @xmath830 , then by the triangle inequality : @xmath831 we begin with the difference @xmath832 . by the @xmath495 bound of lemma  [ lemma : false_entropy_lp_bound ] we have : @xmath833 using the triangle inequality : @xmath834 where the notation @xmath835 is used to emphasize that the norm operation is with respect to @xmath18 only .",
    "using s inequality , @xmath836 assuming @xmath837 , @xmath824 is monotonously increasing , and combining the inequalities above we have : @xmath838    for the second part of , by the @xmath495 bound we have : @xmath839 using the concavity and monotonicity of @xmath824 : @xmath840 where the monotonicity of @xmath824 is again guaranteed by the condition @xmath837 . plugging and into we have : @xmath841 which proves the bound on mutual information .",
    "the bound on capacity is trivially obtained from above by writing @xmath842 and maximizing both sides with respect to @xmath11 ( and similarly for the other direction ) .",
    "_ proof of lemma  [ lemma : exp_second_order_bound ] : _ we would like to prove that @xmath843 . using a finite tailor series",
    "we have : @xmath844 where @xmath845 \\cup [ x , 0]$ ] is a point between @xmath27 and @xmath17 .",
    "this proves the lower bound .",
    "also , for @xmath846 since @xmath847 this also proves the upper bound . for @xmath848 ,",
    "the right inequality can be made tighter , by writing the full tailor expansion : @xmath849    _ proof of lemma  [ lemma : ab_alphabeta ] : _",
    "@xmath850 is continuous and differentiable therefore @xmath851 at the maximum .",
    "derivation yields @xmath852 , and @xmath851 yields the single solution @xmath853 stated in the lemma .",
    "this is a single maximum since @xmath854 is positive for @xmath855 and negative for @xmath856 .",
    "[ [ sec : proof_cw_max_rate ] ] proof of theorem  [ theorem : c_overlinew_optimality ] : the optimality of averaged channel capacity ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    in this section we prove theorem  [ theorem : c_overlinew_optimality ] presented in  [ sec : arbitrary_var_target_rate ] ( regarding the optimality of @xmath235 ) . for a given sequence @xmath56 ,",
    "consider the `` permutation '' channel generated by uniformly selecting a random permutation @xmath857 of the indices @xmath243 , rearranging the sequence @xmath56 to a permuted sequence @xmath858 , and applying the channel @xmath859 to the input ( i.e. using the channels @xmath4 in permuted order ) .",
    "suppose there is a system achieving the rate @xmath58 with probability @xmath59 and error probability @xmath60 .",
    "since this rate is fixed for all drawing of @xmath857 , the system can guarantee the rate @xmath860 a - priori ( with probability @xmath59 ) , and we can convert the rate - adaptive system to a fixed - rate system , delivering a message @xmath861 of @xmath862 bits , with probability of error at most @xmath863 .",
    "once we constrain the discussion to the permutation channel induced by the deterministic sequence @xmath56 , we can assume this sequence is known to the transmitter and the receiver .",
    "in the main part of the proof we will show that approximately , @xmath867 . note that",
    "because of feedback , @xmath868 may be a function of @xmath861 and @xmath869 , and therefore @xmath870 does not give a tight bound on the rate .",
    "as noted in the outline presented in section  [ sec : arbitrary_var_target_rate ] , if the channels @xmath871 were selected from @xmath56 with replacement , this result would be obvious , since feedback would not be helpful . in the permuted channel ,",
    "a system with feedback can use past channel outputs to gain some knowledge about the future behavior of the channel .",
    "the point of the proof is to show that there is no considerable gain from this knowledge , and even a knowledge of the actual list of channels that were already picked does not change the mutual information considerably .",
    "we denote by @xmath857 the random permutation and by @xmath242 a specific instance of the permutation .",
    "we bound the mutual information as follows : @xmath872 where ( a ) is because conditioning reduces entropy ( used twice ) , and ( b ) is since @xmath873 ( in other words , @xmath874 gives all relevant information on @xmath875 ) .",
    "this can be seen from the functional dependence graph in fig.[fig : dependence_graph_for_converse_channel ] .",
    "let @xmath876 be a random variable generated by passing @xmath868 through the channel @xmath244 ( i.e. @xmath877 ) . next we show that @xmath878 and @xmath879 .",
    "( 220.24 , 130.39)(0,0 ) ( 0,0 ) a dependence graph for the variables of the permutation channel in appendix  [ sec : proof_cw_max_rate ] .",
    "each node is a ( potentially random ) function of the nodes with arrows pointing toward it.,title=\"fig : \" ] ( 5.67,63.37)@xmath78 ( 43.37,7.81)@xmath868 ( 41.39,116.95)@xmath880 ( 194.17,63.37)@xmath242 ( 118.77,116.95)@xmath881 ( 79.09,116.95)@xmath869 ( 81.07,7.81)@xmath875 ( 122.74,7.81)@xmath871 ( 162.43,116.95)@xmath882 ( 164.41,7.81)@xmath883 ( 134.65,61.39)@xmath56    ( 220.24 , 130.39)(0,0 ) ( 0,0 ) a dependence graph for the variables of the permutation channel in appendix  [ sec : proof_cw_max_rate ] .",
    "each node is a ( potentially random ) function of the nodes with arrows pointing toward it.,title=\"fig : \" ] ( 5.67,63.37)@xmath78 ( 43.37,7.81)@xmath868 ( 41.39,116.95)@xmath880 ( 194.17,63.37)@xmath242 ( 118.77,116.95)@xmath881 ( 79.09,116.95)@xmath869 ( 81.07,7.81)@xmath875 ( 122.74,7.81)@xmath871 ( 162.43,116.95)@xmath882 ( 164.41,7.81)@xmath883 ( 134.65,61.39)@xmath56    given @xmath884 , the channel law between @xmath868 and @xmath875 is a random pick from the group of @xmath885 channels that are not included in @xmath886 : @xmath887 the average channel given the past indices @xmath888 is an average of @xmath885 values @xmath889 .",
    "note that the indices @xmath389 belong to @xmath890 , so the notation may be confusing , but it is used to stress the causal dependence on @xmath884 .    considering the random variable @xmath891 generated by calculating this channel over all drawings of @xmath857 ,",
    "the set @xmath892 becomes a random set of @xmath885 distinct indices from @xmath893 , chosen uniformly from all such sets .",
    "@xmath891 is an average of @xmath885 values @xmath889 , sampled uniformly without replacement from the set @xmath894 ( for any specific @xmath336 ) .",
    "it was shown by hoeffding @xcite that averages of variables sampled without replacement obey the same bounds ( * ? ? ?",
    "* theorem  1 ) with respect to the probability to deviate from their mean , as independent random variables . specifically ,",
    "applying hoeffding s bounds ( combining theorem  1 with section  6 in @xcite ) , and since @xmath895 = \\overline w$ ] , we have : @xmath896 using the union bound over all @xmath897 values of @xmath336 ( see the proof of proposition  [ prop : symbolwise_scheme_channel_convergence2 ] ) , we have : @xmath898 where the @xmath465 norm is over @xmath336 .",
    "to further simplify , we pick a small value @xmath899 , and from now on we assume @xmath900 . substituting in , we have : @xmath901 since @xmath902 is uniformly continuous ( see lemma  [ lemma : false_entropy_lp_bound ] ) , for any @xmath899 there is a @xmath468 such that if @xmath903 then @xmath904 . for a given @xmath899",
    "we choose the value of @xmath468 such that this requirement is satisfied , so that together with we have : @xmath905    we use the following relation to translate proximity in probability to proximity of the expected values : if @xmath906 $ ] are two random variables satisfying @xmath907 ( for some @xmath908 $ ] ) , then @xmath909 - \\e [ b ] \\big| & = \\big| \\e [ ( a - b ) \\cdot \\ind(|a - b| \\leq \\epsilon ) ] \\\\ & \\qquad + \\e [ ( a - b ) \\cdot \\ind(|a - b| > \\epsilon ) ] \\big| \\\\ & \\leq \\e [ |a - b| \\cdot \\ind(|a - b| \\leq \\epsilon ) ] \\\\ &",
    "\\qquad + \\e [ |a - b| \\cdot \\ind(|a - b| > \\epsilon ) ] \\\\ & \\leq \\epsilon + \\e [ a_{\\max } \\cdot \\ind(|a - b| > \\epsilon ) ] \\\\ & \\leq \\epsilon + a_{\\max } \\cdot p . \\end{split}\\ ] ]",
    "applying this inequality to bound @xmath910 we have : @xmath911 \\\\ & \\stackrel{\\eqref{eq:2580 } , \\eqref{eq:2592}}{\\geq } \\e \\left [ h(\\overline w(\\cdot|x_i ) ) \\right ] - \\epsilon_0 - \\log |\\mathcal{y}| \\cdot p \\\\ & = h(z_i|x_i ) - \\epsilon_0 - \\log |\\mathcal{y}| \\cdot p . \\end{split}\\ ] ]    we now show that the distributions of @xmath875 and @xmath876 are similar ( note that they are not equal , due to the possible dependence of @xmath868 on @xmath884 ) .",
    "@xmath912 - \\e \\left [ \\pr(z_i = y | x_i ) \\right ] \\right| \\\\&\\stackrel{\\eqref{eq:2528}}{= } \\left| \\e \\left [ \\overline w_{\\pi^{i-1}}(y|x_i ) \\right ] - \\e \\left [ \\overline w(y | x_i ) \\right ] \\right| \\stackrel{\\eqref{eq:2543b } , \\eqref{eq:2592}}{\\leq } t + p . \\end{split}\\ ] ] since for any @xmath913 we have @xmath914 , we can choose @xmath33 large enough such that @xmath915 and we have @xmath916 .",
    "then , by our selection of @xmath468 ( before ) , we shall have : @xmath917 returning to , and treating the first @xmath918 and the last @xmath919 symbols separately , we have : @xmath920 \\\\ &",
    "\\qquad +   \\epsilon_0 \\cdot n \\cdot \\log |\\mathcal{y}| \\\\ & \\leq \\sum_{i=1}^{n } i(z_i ; x_i ) + n \\underbrace { ( 2 \\epsilon_0 + ( \\epsilon_0 + p ) \\cdot \\log |\\mathcal{y}|)}_{\\delta_0 } \\\\ & \\leq n \\cdot c(\\overline w ) + n \\delta_0 . \\end{split}\\ ] ] because @xmath899 is a parameter of choice , and for any @xmath913 we have @xmath914 , we can make @xmath467 a small as desired for @xmath33 large enough .",
    "returning to we have : @xmath921 where @xmath300 is defined in .",
    "since by definition  [ def : attainability_of_rw ] , the above must hold for every @xmath922 , for @xmath33 large enough , and @xmath923 ( see and the discussion following it ) , we can make @xmath508 as small as desired by taking @xmath924 .",
    "this concludes the proof of theorem  [ theorem : c_overlinew_optimality ] .",
    "clearly , using the assumptions of this section , @xmath925 satisfies the conditions of the lemma .",
    "the lemma assumes there are @xmath283 blocks and the rate is @xmath926 , which corresponds to a case where the last block was not decoded , however it holds as a lower bound even if the last block was decoded .",
    "we now optimize the value of @xmath297 . starting from : @xmath927 we determine @xmath297 using lemma  [ lemma : ab_alphabeta ] ( with @xmath928 ) and obtain : @xmath929 \\cdot i_{\\max}^{\\frac{\\beta}{\\alpha+\\beta } } \\cdot b_2^{\\frac{\\alpha}{\\alpha+\\beta } } + \\frac{k}{n } \\\\&= 3 \\cdot 2^{-\\frac{2}{3 } } \\cdot i_{\\max}^{\\frac{1}{3 } } \\cdot \\left ( c_1 \\sqrt{\\frac{\\ln ( n)}{n } } \\right)^{\\frac{2}{3 } } + \\frac{k}{n } \\\\ & \\stackrel{\\eqref{eq:795}}{= } 3 \\left ( k \\mathcal{x}| ( |\\mathcal{x}|-1 )   \\right)^{\\frac{1}{3 } } i_{\\max}^{\\frac{2}{3 } } \\cdot \\left ( \\frac{\\ln ( n)}{n } \\right)^{\\frac{1}{3 } } + \\frac{k}{n } \\\\ & \\leq \\left(\\frac{k}{n}\\right)^{\\tfrac{1}{3 } } \\cdot \\left [ 3   \\left ( |\\mathcal{x}|   \\cdot i_{\\max } \\right)^{\\tfrac{2}{3 } } \\ln^{\\tfrac{1}{3}}(n ) + \\left(\\frac{k}{n}\\right)^{\\tfrac{2}{3 } } \\right ] \\\\ & \\stackrel{(a)}{\\leq } \\left(\\frac{k}{n}\\right)^{\\tfrac{1}{3 } } \\cdot   4 \\cdot \\left ( |\\mathcal{x}|   \\cdot i_{\\max } \\right)^{\\tfrac{2}{3 } } \\ln^{\\tfrac{1}{3}}(n ) \\\\ & = 4 \\cdot k^{\\tfrac{1}{3 } } \\cdot |\\mathcal{x}|^{\\tfrac{2}{3 } } \\cdot i_{\\max}^{\\tfrac{2}{3 } } \\cdot \\left ( \\frac{\\ln ( n)}{n } \\right)^{\\frac{1}{3 } } \\defeq \\dpred , \\end{split}\\ ] ] where in ( a ) we assumed @xmath930 .",
    "if the contrary is true , the first term in yields @xmath931 and the theorem is true in a void way . similarly , we do not have to worry about the case @xmath932 since also in this case , due to the second term in , @xmath746 .",
    "if the conditions of lemma  [ lemma : rateless_prior_predictor_lemma ] are satisfied , we have for all @xmath11 : @xmath933 where we used the convexity of @xmath10 with respect to the channel @xmath7 . maximizing both sides of with respect to @xmath11 yields the desired result .",
    "the conditions of lemma  [ lemma : rateless_prior_predictor_lemma ] on @xmath934 remain as conditions of the theorem .",
    "the application of lemma  [ lemma : ab_alphabeta ] in yields the following value of @xmath297 : @xmath935 this concludes the proof of lemma  [ lemma : c_overlinew_achievability_w_side_info ] .      in this section",
    "we demonstrate the claim made in section  [ sec : arbitrary_var_target_rate ] , that even imposing on the synthetic problem only the limitation that the past channels are not given , but need to be estimated , leads to the conclusion @xmath70 is not attainable .    to show this we use an example , based on randomization of the channel sequence .",
    "as in section  [ sec : toy_problem ] , we assume @xmath74 bits are transmitted in time instance @xmath1 ( in other words , this is the gain obtained in retrospect for choosing @xmath72 ) , however , instead of knowing the full channel sequence , the predictor is only allowed to base its decisions on measurements of the channel input and output , i.e. on the values of @xmath936 where @xmath875 is the result of @xmath4 operated on @xmath868 . it would make sense to also require that @xmath868 be distributed @xmath263 but this assumption is not required for the counter example .",
    "[ example : prediction_channel2 ] consider a ternary input binary output channel .",
    "we will choose the channel randomly , and consider the average gain of the predictor and the reference ( since the average regret is a lower bound for the maximum regret ) .",
    "the basic channels are @xmath937 $ ] , @xmath938 $ ] . note that in the two channels , the first input is useless , and using only the two last inputs yields a rate of @xmath25 bit / use .",
    "we add to this family of channels all 3 possible cyclic rotations of the inputs , and term the channel @xmath939 ( @xmath940 ) .",
    "the resulting channels are depicted in fig .",
    "[ fig : example_prediction_channel2 ] .",
    "now we generate the sequence of channels as follows : choose @xmath941 randomly ( one for the entire sequence ) , and choose a random ( uniform , i.i.d . ) sequence of @xmath942-s .",
    "the competitor , knowing @xmath941 , easily selects a prior that optimizes @xmath943 , since @xmath944 and @xmath945 have the same optimizer for each @xmath941 , and achieves a rate of 1 . because of the random generation of the sequence @xmath942 , for any value of @xmath941 , the channel output @xmath946 is uniform i.i.d . over @xmath947 and independent of the input .",
    "therefore the predictor can not infer any information on @xmath941 from the input - output distribution .",
    "therefore the best the predictor can do ( in terms of optimizing for the worst - case @xmath941 ) , is place a uniform prior over all 3 inputs , and therefore obtain a rate of @xmath948 , i.e. a regret of @xmath949 bit per channel use . by increasing the size of the channel input , this gap can be increased indefinitely .",
    "( 248.46 , 134.36)(0,0 ) ( 0,0 ) an illustration of the generation of the channels @xmath950 in example  [ example : prediction_channel2].,title=\"fig : \" ] ( 5.67,77.26)@xmath27 ( 5.67,55.43)@xmath25 ( 5.67,35.59)@xmath19 ( 27.50,93.14)@xmath118 ( 214.02,79.25)@xmath119 ( 227.91,65.36)@xmath27 ( 227.91,43.53)@xmath25 ( 89.01,7.81)@xmath951 ( 75.12,120.91 ) ( 144.57,120.91 ) ( 73.13,110.99 ) ( 140.60,110.99 )    ( 248.46 , 134.36)(0,0 ) ( 0,0 ) an illustration of the generation of the channels @xmath950 in example  [ example : prediction_channel2].,title=\"fig : \" ] ( 5.67,77.26)@xmath27 ( 5.67,55.43)@xmath25 ( 5.67,35.59)@xmath19 ( 27.50,93.14)@xmath118 ( 214.02,79.25)@xmath119 ( 227.91,65.36)@xmath27 ( 227.91,43.53)@xmath25 ( 89.01,7.81)@xmath951 ( 75.12,120.91 ) ( 144.57,120.91 ) ( 73.13,110.99 ) ( 140.60,110.99 )        in section  [ sec : discussion_prediction_scheme ] we mentioned an alternative of using a `` codebook '' of priors , instead of the exponential weighting scheme over the continuum of priors , which was used in this paper .",
    "following is a rough analysis of this approach , for the block - wise variation setting .",
    "we first determine the accuracy required of the codebook .",
    "suppose we have two priors @xmath952 with @xmath953 , and for a certain channel @xmath7 the resulting output distributions are @xmath954 respectively ( @xmath955 ) .",
    "we write @xmath956 ( output entropy minus output entropy given the input ) .",
    "since by definition @xmath957 , by using lemma  [ lemma : false_entropy_lp_bound ] we have @xmath958 , and since the second term in @xmath959 may change by at most @xmath960 , we have @xmath961 .",
    "therefore , in order to bound the loss due to the codebook quantization to @xmath962 , we need to have @xmath963 ( here , @xmath964 represents the any prior , and @xmath965 represents the closest point in the codebook ) . to have a density of @xmath245 per dimension ,",
    "@xmath966 points are required .",
    "now , since @xmath967 differs from @xmath968 by at most @xmath969 , we can now consider the problem of competing against the @xmath247 priors ( considered as @xmath247 experts ) .",
    "the best normalized redundancy than can be attained is @xmath970 ( see the lower bound ( * ? ? ?",
    "* theorem 3.7 ) and the upper bound ( * ? ? ?",
    "* corollary 2.2 ) in cesa - bianchi and lugosi s book ) .",
    "note that since the predictor loss and the codebook loss are balanced , we can not gain by changing the codebook density .",
    "however , we have not shown that the bound on @xmath969 is tight .        1 .   in order to report reception of a rateless block ( we use 1 bit per channel use ) 2 .   in order to send the estimated averaged channel @xmath376 after the end of each block ( or alternatively , the next prior @xmath971 ) .",
    "suppose feedback is limited to rate @xmath972 . instead of reporting successful reception on each symbol",
    ", we report it each @xmath973 symbols .",
    "the price would be wasting up to @xmath974 symbols per block , which essentially form an unused `` gap '' between successful decoding of block @xmath1 and the start of block @xmath266 .",
    "we now give a coarse bound on the number of bits required to represent the estimated averaged channel @xmath975 .",
    "@xmath975 is completely specified to the transmitter by specifying the empirical distribution @xmath328 which takes at most @xmath976 values for a block of length @xmath78 . since @xmath977 ,",
    "the number of bits is at most @xmath978 .",
    "these bits can be sent over @xmath979 channel uses at the end of each block , thus forming another unused `` gap '' between the blocks .",
    "overall the gap between blocks is @xmath980 .",
    "since the maximum number of blocks grows sub - linearly in @xmath33 , the overall loss can be made negligible .",
    "specifically , the effect of the additional gap on the rate can be analyzed using the same technique used to analyze the loss in the last symbol ( the transition between and ) , and would effectively increase the term @xmath981 in @xmath508 by a factor of the gap @xmath617 . since @xmath982 it is easy to see that under the same setting of of the parameters of the scheme , we would still have @xmath983 and @xmath618 , and nearly at the same convergence rate .",
    "as mentioned , implementation of the prediction methods described in this paper , which are based on weighted average over the unit simplex , require the calculation of integrals . in the below",
    ", we show an alternative method to generate the same results , using a method based on rejection sampling . instead of explicitly calculating the predictor @xmath984",
    ", we describe an algorithm that generates a random variable @xmath985 ( which can be used to generate a letter in the random codebook ) , based on multiple drawings of uniform random variables .",
    "the number of random drawings required in this algorithm is polynomial in @xmath33 , but still prohibitively large , so unfortunately it is not practical .    first ,",
    "any scalar random variable can be derived from a uniform @xmath642 $ ] random variable by the inverse transform theorem .",
    "a generation of the mixture of an exponentially weighted and a uniform distribution such as in , only requires to toss a coin with probability @xmath297 , which determines whether @xmath118 is generated using the exponentially weighted distribution or using a uniform distribution .",
    "therefore the problem of generating the predictors described here , , boils down to the following problem : we would like to generate a random variable @xmath118 distributed according to @xmath986 where @xmath987 and where @xmath988 is a concave function and is bounded @xmath989 .",
    "@xmath990 is the unit simplex ( which implicitly refers to the alphabet @xmath22 ) .",
    "all integrals below are over the unit simplex .",
    "furthermore , we would like to accomplish this without computing any integrals .",
    "the first observation is that instead of generating an @xmath118 from @xmath984 it is enough to generate a the probability vector @xmath11 randomly with the probability distribution @xmath152 and then generate an @xmath118 from the ( specific ) probability distribution @xmath11 .",
    "the last step can be accomplished using the inverse transform theorem . in this case",
    "we have : @xmath991 \\\\&= \\underset{q \\sim w(q)}{\\e } \\left [ q(x ) \\right ] = \\int q(x ) w(q ) dq .",
    "\\end{split}\\ ] ] this leaves us with the problem of generating @xmath992 .",
    "this is accomplished by rejection sampling .",
    "i.e. we first generate a random variable with a different distribution , and if it does not satisfy a given condition , we `` reject it '' and re - generate it , until the condition is satisfied .",
    "we first generate a probability distribution @xmath798 uniformly over the unit simplex @xmath990 .",
    "there are several algorithms for uniform sampling over the unit simplex @xcite .",
    "a simple algorithm , for example , is normalizing a vector of i.i.d . exponential random variables .",
    "define @xmath993 , and @xmath994 .",
    "we will determine @xmath995 later on such that @xmath996 .",
    "having generated @xmath798 , we toss a coin with probability @xmath997 for `` accept '' .",
    "if @xmath798 is accepted , this is the resulting random variable and we set @xmath998 . otherwise , we draw @xmath798 again and repeat the process .",
    "let @xmath999 denote the event of acceptance , and @xmath1000 denote the distribution of @xmath798 which is the uniform distribution over the simplex .",
    "the distribution of @xmath11 equals the distribution of @xmath798 given that it was accepted .",
    "i.e. : @xmath1001 which is the desired distribution .    to determine @xmath995 ,",
    "suppose we know the maximum of @xmath988 .",
    "this is usually possible since it is a convex optimization problem .",
    "even if this value is not known , a bound on this value will be sufficient .",
    "suppose that @xmath1002 is the maximizer of @xmath988 and therefore also of @xmath1003 .",
    "then it is enough to set @xmath1004 .",
    "an important question from implementation perspective is the average number of iterations required . since the probability of acceptance @xmath1005 in each iteration is fixed , the number of iterations is a geometrical random variable , with mean @xmath1006 . by lemma  [ lemma : f_exp_weight_ub ]",
    "we can relate @xmath1007 to @xmath1008 and bound the average number of iterations . using the lemma we have : @xmath1009 + \\frac{d}{\\eta } \\ln \\left ( \\frac{\\eta e n   g_0}{d } \\right ) \\\\ & \\leq \\frac{1}{\\eta } \\ln \\left (   \\e   \\left [ g(p ) \\right ] \\right ) + \\frac{d}{\\eta } \\ln \\left ( \\frac{\\eta e n g_0}{d } \\right ) , \\end{split}\\ ] ] where @xmath1010 is the dimension of the unit simplex .",
    "we obtain the following bound on @xmath995 : @xmath1011 } \\cdot \\left ( \\frac{\\eta e n g_0}{d } \\right)^{-d } , \\ ] ] and the average number of iterations can be bounded : @xmath1012 } \\\\&= \\frac{1}{\\e \\left [ a(p ) \\right ] } = \\frac{1}{\\alpha \\e \\left [ g(p ) \\right ] } \\leq \\left ( \\frac{\\eta e n g_0}{d } \\right)^{d } .",
    "\\end{split}\\ ] ] since @xmath192 is polynomial in @xmath33 and tends to @xmath27 , @xmath1013 grows slower than @xmath1014 , however this number is still prohibitively large .        1 .",
    "compute the maximum of @xmath988 ( a convex optimization problem ) , or a bound on it .",
    "2 .   set @xmath1015 .",
    "3 .   draw @xmath11 uniformly over the unit simplex @xcite .",
    "[ line:2488 ] 4 .",
    "toss a coin and with probability @xmath1016 return to step [ line:2488 ] .",
    "draw @xmath118 randomly according to the distribution @xmath326 .        as noted in section  [ sec : toy_categorization ] , the relation of the synthetic prediction problem to prediction under the absolute loss function , implies that the fl predictor can not be applied to our problem .",
    "here we give a specific example to see why fl fails , based on the channel defined in section  [ sec : toy_categorization ] .",
    "we construct the following sequence of channels : the channel at @xmath1017 is a mixture of @xmath1018 with probability @xmath126 and a completely noisy channel @xmath1019 . for this channel @xmath1020 . at time @xmath1021 ,",
    "the best a - posteriori strategy is @xmath1022 .",
    "the sequence of channels from time @xmath1021 onward is the alternating sequence @xmath1023 .",
    "it is easy to see that the resulting cumulative rates are linear functions of @xmath149 and thus the optimum is attained at the boundaries of @xmath642 $ ] and @xmath1024 . at each time , since the channel that slightly dominates the past is opposite of the channel that is about to appear , the fl predictor chooses the prior that yields the _ least _ mutual information , and ends up having a zero rate in time instances @xmath1025 . on the other hand , by using a uniform fixed prior , a competitor may achieve an average rate of @xmath126 over these symbols .",
    "therefore the normalized regret of fl would be at least @xmath126 , and does not vanish asymptotically .",
    "p.  chow , j.  cioffi , and j.  bingham , `` a practical discrete multitone transceiver loading algorithm for data transmission over spectrally shaped channels , '' _ ieee trans .",
    "communications _ , vol .",
    "773 775 , apr .",
    "d.  love , r.  heath , v.  lau , d.  gesbert , b.  rao , and m.  andrews , `` an overview of limited feedback in wireless communication systems , '' _ ieee journal on selected areas in communications _ , vol .",
    "26 , no .  8 , pp .",
    "13411365 , oct .",
    "a.  mahajan and s.  tatikonda , `` a training based scheme for communicating over unknown channels with feedback , '' in _ communication , control , and computing , 2009 .",
    "allerton 2009 .",
    "47th annual allerton conference on _ , 30 2009-oct . 2 2009 ,",
    "1549 1553 .",
    "r.  ahleswede and n.  cai , `` the avc with noiseless feedback and maximal error probability : a capacity formula with a trichotomy , '' _ numbers , information and complexity _ , pp .",
    "151176 , 2000 , special volume in honour of r. ahlswede on occasion of his 60th birthday .",
    "s.  onn and i.  weissman , `` generating uniform random vectors over a simplex with implications to the volume of a certain polytope and to multivariate extremes , '' _ annals of operations research _ , vol .",
    "189 , pp . 331342 , 2011 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1007/s10479-009-0567-7"
  ],
  "abstract_text": [
    "<S> we consider the problem of universally communicating over an unknown and arbitrarily varying channel , using feedback . </S>",
    "<S> the focus of this paper is on determining the input behavior , and specifically , a prior distribution which is used to randomly generate the codebook . </S>",
    "<S> we pose the problem of setting the prior as a sequential universal prediction problem , that attempts to approach a given target rate , which depends on the unknown channel sequence . </S>",
    "<S> the main result is that , for a channel comprised of an unknown , arbitrary sequence of memoryless channels , there is a system using feedback and common randomness that asymptotically attains , with high probability , the capacity of the time - averaged channel , universally for every sequence of channels . </S>",
    "<S> while no prior knowledge of the channel sequence is assumed , the rate achieved meets or exceeds the traditional arbitrarily varying channel ( avc ) capacity for every memoryless avc defined over the same alphabets , and therefore the system universally attains the random code avc capacity , without knowledge of the avc parameters . </S>",
    "<S> the system we present combines rateless coding with a universal prediction scheme for the prior . </S>",
    "<S> we present rough upper bounds on the rates that can be achieved in this setting and lower bounds for the redundancies . </S>"
  ]
}