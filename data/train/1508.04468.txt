{
  "article_text": [
    "the alternating directions method of multipliers ( admm ) has received a great deal of attention recently for large - scale problems involving constraints on the image of the unknowns under some linear mapping .",
    "the analysis has focused on either global complexity estimates @xcite or sufficient conditions for local linear convergence @xcite .",
    "the closely related douglas ",
    "rachford algorithm has also been the focus of recent studies showing global complexity @xcite and ( local linear ) convergence in increasingly inhospitable settings @xcite .",
    "a survey of results on proximal methods in general can be found in @xcite . in the convex",
    "setting , the convergence studies for both admm and douglas  rachford share a common thread through the well - known duality between these algorithms @xcite .",
    "studies of admm frequently invoke strong convexity .",
    "studies of douglas  rachford , on the other hand have , until very recently , focused on _ feasibility _ problems and corresponding notions of regularity of intersections .",
    "we combine an analysis of the admm algorithm with facts learned from the local convergence of douglas  rachford to provide sufficient conditions for local linear convergence of sequences generated by admm without strong convexity .",
    "while this paper was under review we became aware of two recent studies that also combine the analysis of admm and douglas - rachford to improve and generalize many local and global results @xcite .",
    "while our theoretical results are general and abstract , our motivation for the current study comes from the application of statistical multiscale image denoising / deconvolution following @xcite for fluorescence microscopic images ( see also @xcite for a review of fluorescence microscopy techniques and statistical methods for them ) .",
    "we demonstrate the analysis for image denoising / deconvolution of stimulated emission depletion ( sted ) images @xcite .",
    "though many of the arguments presented here work equally well for infinite dimensional hilbert spaces , to avoid technicalities , _ it will be assumed throughout that @xmath0 and @xmath1 are euclidean spaces . _",
    "the norm @xmath2 denotes the euclidean norm .",
    "we denote the extended reals by @xmath3}{:=}{\\mathbb{r}}\\cup\\{+\\infty\\}$ ] and the nonnegative orthant by @xmath4 . the closed unit ball centered at the origin",
    "is denoted by @xmath5 . in the usual notation for the natural numbers",
    "@xmath6 we include @xmath7 .",
    "the mapping @xmath8 is linear and the functional @xmath9}$ ] is proper ( not everywhere @xmath10 and nowhere @xmath11 ) , convex and lower semicontinuous ( lsc ) , as is the functional @xmath12}$ ] .",
    "the level set of @xmath13 corresponding to @xmath14 is defined by @xmath15 .",
    "the domain of a function @xmath16}\\,}$ ] is defined by @xmath17}. we use the notation @xmath18 to denote a set - valued mapping from @xmath0 to @xmath1 .",
    "a proper function @xmath16}\\,}$ ] is _ strongly convex _ if there is a constant @xmath19 such that @xmath20 for all @xmath21 and @xmath22 and @xmath23 .",
    "we will not assume smoothness of functions and so will require the _",
    "subdifferential_. the subdifferential of a function @xmath16}\\,}$ ] at a point @xmath24 is defined by @xmath25 when @xmath26 the subdifferential is defined to be empty .",
    "elements from the subdifferential are called _",
    "subgradients_. the subdifferential of a proper , lsc convex function is a _ maximally monotone _ set - valued mapping ( * ? ? ?",
    "* theorem 12.17 ) .",
    "the _ fenchel conjugate _ of a function @xmath27 is denoted by @xmath28 and defined by @xmath29 a mapping @xmath30 is said to be _",
    "@xmath31-inverse strongly monotone _",
    "* corollary 12.55 ) if for all @xmath32 @xmath33 the mapping @xmath34 is said to be _ polyhedral _ ( or piecewise polyhedral @xcite ) if its graph is the union of finitely many sets that are polyhedral convex in @xmath35 @xcite .",
    "we denote the _ resolvent _ of @xmath34 by @xmath36 where @xmath37 denotes the identity mapping and the inverse is defined as @xmath38 the corresponding _ reflector _ is defined by @xmath39 .",
    "notions of _ continuity _ of set - valued mappings have been thoroughly developed over the last @xmath40 years .",
    "readers are referred to the monographs @xcite for basic results .",
    "a mapping @xmath18 is said to be _ lipschitz continuous _ if it is closed valued and for all @xmath41 there exists a @xmath42 such that @xmath43 lipschitz continuity is , however , too strong a notion for set - valued mappings .",
    "a key property of set - valued mappings that we will rely on is _ metric subregularity _ , which can be understood as the property corresponding to a lipschitz - like continuity of the inverse mapping relative to a specific point .",
    "as the name suggests , it is a weaker property than _ metric regularity _ which , in the case of an @xmath44 matrix for instance , is equivalent to surjectivity . our definition follows the characterization of this property given in ( * ? ? ?",
    "* exercise 3h.4 ) .",
    "[ d : metric subregularity ] @xmath45    a.   [ d : metric subregularity i ] the mapping @xmath46 is called _ metrically subregular at @xmath47 for @xmath48 relative to @xmath49 _ if @xmath50 and there is a constant @xmath51 and neighborhoods @xmath52 of @xmath47 such that @xmath53 b.   [ d : metric subregularity ii ] the mapping @xmath34 is called _ strongly metrically subregular at @xmath47 for @xmath48 relative to @xmath49 _ if @xmath50 and there is a constant @xmath51 and neighborhoods @xmath52 of @xmath47 such that @xmath54    the constant @xmath55 measures the stability under perturbations of inclusion @xmath56 .",
    "an important instance where metric subregularity comes for free is for polyhedral mappings .",
    "[ t : polyhedrality - strong msr ] let @xmath57 be an affine subspace and @xmath58 . if @xmath59 is polyhedral and @xmath60 is an isolated point , @xmath61 , then @xmath62 is strongly metrically subregular , hence metrically subregular , at @xmath63 for @xmath7 relative to @xmath64 .",
    "if @xmath59 is polyhedral , so is @xmath65 .",
    "now by ( * ? ? ?",
    "* propositions 3i.1 and 3i.2 ) , since @xmath66 is polyhedral and @xmath63 is an isolated point of @xmath67 ,",
    "then @xmath68 is _ strongly metrically subregular _ at @xmath63 for @xmath7 with constant @xmath55 on the neighborhood @xmath52 of @xmath63 restricted to @xmath64 .    one prevalent source of polyhedral mappings is the subdifferential of piecewise linear - quadratic functions ( see proposition [ t : polyhedral subdiff ] below ) .",
    "[ d : plq ] a function @xmath69 $ ] is called _ piecewise linear - quadratic _ if @xmath70 can be represented as the union of finitely many polyhedral sets , relative to each of which @xmath71 is given by an expression of the form @xmath72 for some scalar @xmath73 vector @xmath74 , and symmetric matrix @xmath75 .    a notion related to metric regularity",
    "is that of _ weak - sharp solutions_. this will be used in the development of error bounds ( theorem [ t : dist to sinf ] ) .",
    "[ d : weak sharp ] the solution set @xmath76 for a nonempty closed convex set @xmath77 , is weakly sharp if , for @xmath78 , there exists a positive number @xmath79 ( sharpness constant ) such that @xmath80 similarly , the solution set @xmath81 is weakly sharp of order @xmath82 if there exists a positive number @xmath79 ( sharpness constant ) such that , for each @xmath83 , @xmath84      to conclude this section we present general results about types of ( firmly ) nonexpansive operators that clarify the underlying mechanisms yielding linear convergence of many algorithms .",
    "the operative definitions are given here .",
    "[ d : q(f)ne ] let @xmath85 and @xmath86 be nonempty subsets of @xmath0 and let @xmath59 be a ( multi - valued ) mapping from @xmath85 to @xmath0 .",
    "a.   [ d : qne ] @xmath59 is called _",
    "@xmath87-nonexpansive on @xmath85 _ if @xmath88 if holds with @xmath89 then we say that @xmath59 is _ @xmath86-nonexpansive _ on @xmath85 . b.   [ d : qfne ] @xmath59 is called _",
    "@xmath87-firmly nonexpansive _ on @xmath85 if @xmath90 if holds with @xmath89 then we say that @xmath59 is _",
    "@xmath86-firmly nonexpansive _ on @xmath85 .",
    "if , in addition , @xmath91 , then @xmath59 is said to be _ quasi - firmly nonexpansive_.    [ t : gen convergence ] let @xmath57 be an affine subspace and @xmath58 be quasi - firmly nonexpansive on @xmath64 .",
    "let @xmath60 be an isolated point , @xmath61 .",
    "if @xmath62 is metrically subregular at @xmath63 for @xmath7 , then there is a neighborhood @xmath52 of @xmath63 such that @xmath92 where @xmath93 for @xmath55 a constant of metric subregularity of @xmath94 at @xmath63 for the neighborhood @xmath52 .",
    "consequently , the fixed point iteration @xmath95 converges linearly to @xmath96 with rate @xmath97 for all @xmath98 .",
    "define @xmath99 and note that @xmath100 , hence @xmath101 suppose that @xmath34 is metrically subregular at @xmath96 for @xmath7 .",
    "then by definition [ d : metric subregularity ] we have , for all @xmath102 and for all @xmath103 , @xmath104 which is the coercivity condition of ( * ? ? ?",
    "* eq.(3.1 ) , lemma 3.1 ) . by assumption ,",
    "@xmath59 is @xmath105-firmly nonexpansive ( i.e. , quasi - firmly nonexpansive ) on @xmath64 ( definition [ d : q(f)ne ] ) .",
    "the result then follows from ( * ? ? ?",
    "* lemma 3.1 ) with rate @xmath97 for @xmath106 .    the constant @xmath107 in the above theorem",
    "can always be chosen to be less than or equal to @xmath108 . to see this , note that for any metrically subregular mapping @xmath34 , there is a constant @xmath109 and hence a @xmath110 so that the rate constant given in theorem [ t : gen convergence ] will always hold whenever the fixed point is a ( relatively ) isolated point .    [",
    "eg : example ] consider two lines , @xmath111 and @xmath112 , in @xmath113 intersecting orthogonally at the origin and let @xmath59 be the douglas  rachford operator for the projections onto each line .",
    "in this example @xmath114 where @xmath115 for the projection onto the line @xmath111 denoted by @xmath116 , and likewise for @xmath117 . in the context of what follows",
    ", @xmath116 is the resolvent of the subdifferential of the indicator function of the line @xmath111 and likewise for @xmath118 .",
    "it is elementary to verify that @xmath59 is firmly nonexpansive , has a unique fixed point , and @xmath119 for all @xmath120 .",
    "moreover @xmath121 , which has a constant of metric subregularity @xmath122 .",
    "theorem [ t : gen convergence ] then predicts that the douglas  rachford algorithm converges linearly with rate constant @xmath7 in this case , i.e. it converges in one step .",
    "the reader can verify that this indeed is the case .    to see the importance of the restriction to the affine subspace @xmath64 , consider instead of two lines in @xmath113 two lines in @xmath123 intersecting at the origin .",
    "it can be shown that the fixed points of the douglas  rachford operator consist of the axis ",
    "let s call it the @xmath124 axis  extending from the origin , perpendicular to the linear hull of the two lines @xcite .",
    "it is elementary to verify that , from any starting point @xmath125 in @xmath123 , the douglas  rachford algorithm converges in one step to the intersection of the @xmath124 axis with the affine subspace containing @xmath125 and parallel to the plane containing the lines @xmath111 and @xmath112 . clearly , the fixed points of the mapping @xmath59 are not isolated points , but they are isolated points relative to the affine subspace containing @xmath125 and parallel to @xmath111 and @xmath112 , so theorem [ t : gen convergence ] applies and predicts , correctly , that the douglas - rachford algorithm converges to a fixed point in one step .",
    "the projection of this fixed point onto the set @xmath112 is the solution to the problem of finding the point of intersection .",
    "[ t : polyhedral convergence ] let @xmath57 be an affine subspace and @xmath58 be quasi - firmly nonexpansive on @xmath64 .",
    "let @xmath60 be an isolated point , @xmath61 .",
    "if @xmath59 is polyhedral , then there is a neighborhood @xmath52 of @xmath63 such that @xmath126 where @xmath93 for @xmath55 a constant of metric subregularity of @xmath94 at @xmath63 for the neighborhood @xmath127 .",
    "consequently , the fixed point iteration @xmath95 converges linearly to @xmath96 with rate @xmath97 for all @xmath98 .",
    "the result follows immediately from proposition [ t : polyhedrality - strong msr ] and theorem [ t : gen convergence ] .",
    "the requirement that the fixed point set is a singleton can be viewed as a uniqueness assumption , which is common in the inverse problems literature .",
    "it is well known , however , that , even if the solution to a given problem is unique , the set of fixed points of the numerical method ( of interest to us , the douglas  rachford operator ) need not be solutions to the given problem , much less be unique @xcite .",
    "recent work has shown , however , that the set of fixed points need only consist of singletons relative to appropriate affine subspaces where the iterates lie @xcite . this feature has been exploited in the analysis of the douglas  rachford algorithm applied to problems with polyhedral and quadratic structure @xcite .",
    "metric ( sub)regularity , on the other hand , is one of the central assumptions of well - posedness of inverse problems @xcite .",
    "other useful equivalent characterizations of metric subregularity can be found in @xcite .",
    "polyhedrality can be quite easy to verify , as we will see below .",
    "we consider problems in the following format : @xmath128 there are many possibilities for solving such problems .",
    "we focus our attention on one of the more prevalent methods , the alternating direction method of multipliers , abbreviated as admm ( primary sources include @xcite ) .",
    "this method is one of many _ splitting methods _ which are the principle approach to handling the computational burden of large - scale , separable problems @xcite .",
    "admm belongs to a class of _ augmented lagrangian methods _ whose original motivation was to regularize lagrangian formulations of constrained optimization problems .    introducing a new variable @xmath129 ,",
    "our problem is to solve @xmath130    the augmented lagrangian @xmath131 for is given by @xmath132 where @xmath133 , @xmath134 is a fixed penalty parameter .",
    "the admm algorithm for solving is , given @xmath135 , @xmath136 , compute @xmath137 by @xmath138 using @xmath139 , the algorithm - can be written equivalently as    the penalty parameter @xmath140 need not be a constant , and indeed evidence indicates that the choice of @xmath140 can greatly impact the complexity of the algorithm , but this is beyond the scope of this investigation , so we have left this parameter fixed .",
    "we do not specify how the argmin in steps - should be calculated , and indeed , the analysis that follows assumes that these can be computed exactly .",
    "this is , of course , not true in practice . in an attempt to circumvent this fact",
    ", the standard approach in numerical analysis is to accommodate summable errors .",
    "the generalization to summable errors is , however , tantamount to _ eventual _ exact evaluation of - and thus , for all practical purposes , is no different from _ immediate _ exact evaluation , the latter involving errors that sum to zero .    even if we do assume infinite precision , a few remarks about the computational complexity of the individual steps of algorithm [ a : admm ] are warranted .",
    "inspection of shows that an implicit method involving computation of the inverse of @xmath141 may not be feasible if this is very large or does not otherwise enjoy a structure that allows for efficient inversion . if @xmath13 is smooth , a number of classical quasi - newton methods , with error bounds , are available @xcite . if @xmath13 is nonsmooth , then a forward - backward - type method such as fista @xcite could be applied . in the latter case",
    "new results on convergence of the iterates to a solution open the door to error bounds at this stage @xcite .",
    "the second step does not involve any matrix inversion , but will , for exact penalization , involve a nonsmooth penalty @xmath142 .",
    "again , one has recourse to fast first - order methods that , as of very recently , permit error bounds .",
    "our goal is to determine the rate of convergence of these algorithms so that they may be used as inner routines in an iteratively regularized procedure . knowing that an algorithm converges linearly , for instance , yields rational stopping criteria with computable estimates for the distance of the current iterate to the solution set .",
    "we present sufficient conditions for _ linear _ convergence of algorithm [ a : admm ] by showing the same for the douglas - rachford algorithm which is more amenable to the tools of abstract fixed point theory presented in section [ s : abstract ] .",
    "it is well known @xcite that the admm algorithm can be derived from the douglas  rachford algorithm , and vice versa , and therefore sufficient conditions for convergence of douglas  rachford also apply here . the first convergence result for douglas ",
    "rachford is due to lions and mercier @xcite , under the assumption of strong convexity and lipschitz continuity of @xmath13 .",
    "recent published work in this direction includes @xcite .",
    "convergence rates with respect to objective values under various assumptions on the objective , all of which involving strong convexity , was established in @xcite which is conservative .",
    "local linear convergence of the iterates to a _",
    "solution _ was established in @xcite for linear and quadratic programs using spectral analysis . in the first main result ,",
    "theorem [ t : convergence ] , we describe two conditions that guarantee linear convergence of the admm iterates to a solution .",
    "the first of these conditions follows from classical results of lions and mercier @xcite .",
    "the second condition is based on work of more recent vintage @xcite , is much more prevalent in applications and generalizes the results of @xcite .",
    "the ( fenchel - legendre ) dual problem corresponding to the problem is ( see , for instance @xcite ) @xmath143 here @xmath144 and @xmath145 are the fenchel conjugates of @xmath13 and @xmath142 respectively . instead of working with this dual",
    ", we work with the following equivalent form with the change of variable @xmath146 : @xmath147 under the assumption that the solutions @xmath148 and @xmath149 of the primal and dual problems exist and that the dual gap is zero , the following two inclusions characterize the solutions of the problems and respectively : @xmath150 @xmath151 in both cases , one has to solve an inclusion of the form @xmath152 for general set - valued mappings @xmath112 and @xmath85 . for any @xmath134 , the douglas  rachford algorithm @xcite for solving ( [ problem inclusion ] )",
    "is given by @xmath153 where @xmath154 and @xmath155 are the _ resolvents _ of @xmath156 and @xmath157 respectively .",
    "the connection between the admm algorithm - and the douglas  rachford algorithm was first discovered by gabay @xcite and is derived for convenience in the appendix .    given @xmath158 and @xmath159 , following @xcite , define the new variable @xmath160 so that @xmath161 .",
    "we thus arrive at an alternative formulation of the douglas  rachford algorithm : @xmath162 where @xmath163 and @xmath164 are the reflectors of the respective resolvents .",
    "this is exactly the form of douglas  rachford considered in @xcite .",
    "[ r : resolvents sv ] note that for our application @xmath165 and so the resolvent mappings are the proximal mappings of the convex functions @xmath166 and @xmath145 respectively , and hence the resolvent mappings and corresponding fixed point operator @xmath59 are single - valued @xcite .",
    "[ t : pre convergence ] let @xmath167 and @xmath168 be proper , lsc and convex .",
    "let @xmath169 be linear and suppose there exists a solution to @xmath170 for @xmath112 and @xmath85 defined by",
    ". for fixed @xmath134 , given any initial points @xmath125 and @xmath171 such that @xmath172 , the sequences @xmath173 , @xmath174 and @xmath175 defined respectively by , and @xmath176 converge to points @xmath177 , @xmath178 and @xmath179 .",
    "the point @xmath180 is a solution to , and @xmath181 .",
    "if , in addition , @xmath111 has full column rank , then the sequence @xmath182 corresponds exactly to the sequence of points generated in steps and of algorithm [ a : admm ] and the sequence @xmath183 generated by converges to @xmath148 , a solution to .",
    "following @xcite , we rewrite the douglas  rachford iteration [ e : drsalg1 ] in two steps : given @xmath184 , for @xmath136 do    [ a : drsalg1 ] @xmath185    the existence and uniqueness in the above steps follows from the representation lemma ( * ? ? ?",
    "* corollary 3.6.3 ) .",
    "the mappings @xmath186 are maximal monotone operators as the subdifferentials of proper lsc convex functions .",
    "this together with the fact that the solution set of ( [ problem inclusion ] ) is non - empty yields that the sequence @xmath187 defined by the algorithm converges to some @xmath188 such that @xmath189 and @xmath149 solves ( * ? ? ?",
    "* theorem 1 ) . by the change of variables @xmath190",
    ", it follows that @xmath191 for @xmath59 given by .    for these definitions of @xmath112 and @xmath85 ,",
    "the sequence @xmath173 generated by @xmath192 for @xmath193 generated by corresponds exactly to the sequence @xmath173 generated by . moreover , if @xmath111 is full column rank , then by the discussion in @xcite ( see the appendix ) both @xmath173 and the sequence @xmath175 generated by @xmath194 correspond exactly to the sequences of points @xmath195 and @xmath196 generated by - . consequently , by ( * ? ? ?",
    "* proposition 3.42 ) and @xmath197 and the update rule , @xmath198 , from which the claim follows  see the appendix . ] the sequence @xmath199 defined by converges to a solution of .",
    "we now state sufficient conditions guaranteeing linear convergence of the admm and the douglas  rachford algorithms .",
    "the first conditions of theorem [ t : convergence ] are classical .",
    "the second conditions are new .",
    "[ t : convergence ] let @xmath167 and @xmath168 be proper , lsc and convex .",
    "suppose there exists a solution to @xmath200 for @xmath112 and @xmath85 defined by where @xmath169 is an injective linear mapping .",
    "let @xmath201 for @xmath59 defined by .",
    "for fixed @xmath134 and any given triplet of points @xmath202 satisfying @xmath160 , with @xmath159 , generate the sequence @xmath203 by - and the sequence @xmath204 by .",
    "a.   [ t : convergence_i ] let @xmath205 be a neighborhood of @xmath206 on which @xmath142 is strongly convex with constant @xmath207 and @xmath208 is @xmath31-inverse strongly monotone for some @xmath209 .",
    "then , for any @xmath210 satisfying @xmath211 , the sequences @xmath204 and @xmath203 converge linearly to the respective points @xmath212 and @xmath213 with rate at least @xmath214 .",
    "b.   [ t : convergence_ii ] suppose that @xmath215 for some affine subspace @xmath49 with @xmath216 . on the neighborhood @xmath52 of @xmath206 relative to @xmath64 , that is",
    "@xmath127 , suppose there is a constant @xmath217 such that @xmath218 then the sequences @xmath204 and @xmath203 converge linearly to the respective points @xmath219 and @xmath213 with rate bounded above by @xmath97 .    in either case , the limit point @xmath220 is a solution to , @xmath189 and the sequence @xmath199 given by of algorithm [ a : admm ] converges to @xmath148 , a solution of .",
    "the final statement of the theorem and the statements about the sequence @xmath221 follows from proposition [ t : pre convergence ] where it is shown that the sequence @xmath203 generated by - corresponds to sequences @xmath173 and @xmath175 generated respectively by and @xmath222 for @xmath174 generated by . the linear convergence of the iterates of algorithm [ a : admm ] claimed in statements and follows from the properties of the operators @xmath223 and @xmath59 defined respectively by and .    part . since @xmath142 is assumed to be strongly convex with @xmath19 the modulus of convexity on @xmath52",
    ", @xmath224 is strongly monotone with modulus of monotonicity @xmath207 ( * ? ? ?",
    "* example 22.3 ) . since @xmath208 is also maximally monotone , using the identity @xmath225 ( see , for example , ( * ? ? ?",
    "* corollary 3.49 ) ) we conclude that @xmath226 is lipschitz continuous with constant @xmath227 . moreover , since @xmath208 is @xmath31-inverse strongly monotone on @xmath52 , we have for any @xmath228 @xmath229 hence @xmath230 is strongly monotone with modulus @xmath31 and proposition 4 of @xcite applies to yield linear convergence of the sequences @xmath231 and @xmath232 to the respective limit points @xmath63 and @xmath149 @xmath233 where @xmath234 is some constant , @xmath235 and @xmath236 is the lipschitz constant for the set - valued map @xmath230 on @xmath52 .",
    "now , since @xmath237 , we have for @xmath238 with the same rate as @xmath193 and @xmath195 , modulo a constant : @xmath239 this completes the proof of the first statement .",
    "@xmath240    part . since @xmath112 and @xmath85 are maximal",
    "monotone operators the reflected resolvents @xmath164 and @xmath163 are nonexpansive ( * ? ? ?",
    "* proposition 23.7 ) .",
    "the composition @xmath241 is nonexpansive which implies that the mapping @xmath59 is firmly nonexpansive ( * ? ? ?",
    "* proposition 4.2 ) , and hence quasi - firmly nonexpansive on @xmath64 .",
    "condition is the coercivity condition ( b ) of ( * ? ? ?",
    "* lemma 3.1 ) which guarantees local linear convergence of fixed - point iterations for @xmath242-firmly nonexpansive mappings ( @xmath243 ) .",
    "quasi - firmly nonexpansive mappings , under consideration here , are @xmath244-firmly nonexpansive .",
    "thus , by ( * ? ? ?",
    "* lemma 3.1 ) the sequence @xmath204 converges linearly on the neighborhood @xmath52 with rate @xmath97 .",
    "nonexpansiveness of the resolvent @xmath154 and the relations @xmath245 and @xmath246 then complete the proof of the second statement .",
    "the strong convexity assumption of theorem [ t : convergence ] fails in a wide range of applications , and in particular for feasibility problems ( minimizing the sum of indicator functions ) . by theorem [ t : gen convergence ] ,",
    "case of theorem [ t : convergence ] , in contrast , holds in general for mappings @xmath59 for which @xmath94 is _ metrically subregular _ and the fixed point sets are _ isolated points _ with respect to an affine subspace to which the iterates are confined . the restriction to the affine subspace",
    "@xmath64 is a natural generalization for the douglas  rachford algorithm , where the iterates are known to stay confined to affine subspaces orthogonal to the fixed point set @xcite",
    ". it would be far too restrictive to require that @xmath96 be a singleton on the entire ambient space @xmath1 rather than with respect to just the affine hull of the iterates .",
    "we show that metric subregularity with respect to this affine subspace holds in many applications .",
    "( see also example [ eg : example ] . )",
    "proposition [ t : pre convergence ] and theorem [ t : convergence ] and their proofs also hold in infinite dimensional hilbert spaces .",
    "lemma 3.1 of @xcite is stated for euclidean spaces , but the proof holds also on general hilbert spaces .",
    "[ t : polyhedral subdiff ] let @xmath167 and @xmath168 be proper , lsc and convex .",
    "suppose , in addition , that @xmath13 and @xmath142 are piecewise linear - quadratic .",
    "the operator @xmath247 defined by with @xmath134 fixed , is polyhedral for @xmath112 and @xmath85 given by where @xmath169 is a linear mapping .",
    "since the functions @xmath13 and @xmath248 are proper , lsc , convex and piecewise linear - quadratic , by ( * ? ? ?",
    "* theorem 11.14 ) so are the fenchel conjugates , @xmath144 and @xmath145 .",
    "the subdifferentials @xmath249 and @xmath250 and their resolvents , therefore , are polyhedral mappings ( * ? ? ?",
    "* proposition 12.30 ) . since the graphs of reflectors @xmath164 and @xmath163 correspond to the graphs of their respective resolvents @xmath155 and @xmath154 through a linear transformation , @xmath164 and @xmath163 are also polyhedral mappings . since by remark [ r : resolvents sv ] the resolvents @xmath155 and @xmath154 are single - valued , the reflectors @xmath164 and @xmath163 are also single - valued .",
    "therefore @xmath251 is polyhedral as the composition of single - valued polyhedral mappings .",
    "[ t : convergence - polyhedrality ] let @xmath167 and @xmath168 be proper , lsc , convex , piecewise linear - quadratic functions ( see definition [ d : plq ] ) . define the operator @xmath247 by with @xmath134 fixed and @xmath112 and @xmath85 given by where @xmath169 is a linear mapping .",
    "suppose that there exists a solution to @xmath200 , that @xmath215 for @xmath64 some affine subspace of @xmath1 and that @xmath60 is an isolated point @xmath61 .",
    "then there is a neighborhood @xmath52 of @xmath63 such that , for all starting points @xmath252 with @xmath253 for @xmath254 so that @xmath255 , the sequence @xmath204 generated by converges linearly to @xmath63 where @xmath256 is a solution to",
    ". the rate of linear convergence is bounded above by @xmath97 , where @xmath257 , for @xmath55 a constant of metric subregularity of @xmath94 at @xmath63 for the neighborhood @xmath52 . moreover",
    ", the sequence @xmath182 generated by algorithm [ a : admm ] converges linearly to @xmath213 with @xmath258 , and the sequence @xmath199 defined by of algorithm [ a : admm ] converges to a solution to .    by proposition [ t : polyhedral subdiff ] the douglas",
    " rachford operator @xmath59 is polyhedral and thus the first statement follows from corollary [ t : polyhedral convergence ] .",
    "the statement about the sequences generated by algorithm [ a : admm ] follows as in theorem [ t : convergence ] .",
    "in this section , we study an iteratively regularized algorithmic scheme for solving the problems of the form @xmath259 where @xmath9}$ ] is proper lsc and convex , the mapping @xmath260 is linear , for all @xmath261 the nonnegative - valued function @xmath262 is convex and smooth ( at least at points that matter ) and @xmath263 .",
    "we refer to the inequality constraints as _",
    "structured constraints_. it will be convenient to introduce the following notation that will help to reduce clutter .",
    "we collect the constraints into a vector - valued function so that we can write the problem as @xmath264 where @xmath265 here the vector inequality is understood as holding element - wise .",
    "a common approach to solving problems of the type arising from inverse problems is to apply _ implicitly _ the structured constraint by adding some ( usually smooth ) quantification of the constraint violation into the objective function : @xmath266 where @xmath267}\\,}$ ] is a proper , lsc convex function and @xmath268 .",
    "this places us in the context of the previous section since problem is the specialization of with @xmath269 .    as is often seen in the inverse problems literature , the constraint violation parameter @xmath270 ( @xmath271 ) , essentially penalizing divergence from the origin .",
    "a prominent instance of this form of regularization is the squared norm : @xmath272 there are many efficient methods available for solving .",
    "it is clear that for a certain value of @xmath273 the optimal solution to , @xmath274 , will satisfy @xmath275 with the _ effective _ error @xmath276 depending on @xmath273 .",
    "what is _ not _ true in general , however , is that the solution to corresponds to the solution to for the constraint error @xmath277 .",
    "moreover , for our intended applications , @xmath0 is a finite dimensional euclidean space with dimension @xmath278 and the dimensionality of the constraints @xmath279 grows superlinearly as a function of @xmath278 , so we would like to consolidate the constraints somehow while exploiting the phenomenon that , at the solution to relatively few of the constraints are in fact tight or _ active_.    we consider convex penalties that reduce the dimensionality of the constraint structure and have the property that @xmath280 if and only if @xmath281 . of particular interest among penalties with this property are _ exact penalties _ , that is penalties @xmath282 with the property that solutions to correspond to solutions to for all values of @xmath273 beyond a certain threshold @xmath283 . for more background on exact penalization",
    "see , for example , @xcite .",
    "we point also to friedlander and tseng @xcite for a connection between exact penalization and what they call _ exact regularization _ as this fits well with our viewpoint that the structured constraints @xmath284 constitute a regularization of the _ model _ with regularization parameter @xmath285 .",
    "this illustrates the distinction between _ model - based _ regularization , that is , regularization of the constraints motivated by external ( eg . statistical ) considerations , versus _ numerical _ regularization motivated solely on the grounds of enabling efficient ( approximate ) numerical solutions to .    while it is nice to know that , with exact penalization , one can achieve an exact correspondence between the original constrained optimization problem and the penalized problem , the whole point of relaxing the constraints",
    "is to reduce the computational burden of strictly enforcing the constraints .",
    "as is often done in practice , one gradually strengthens the constraints , finding intermediate points that nearly solve the relaxed problem and using these as starting points for solving a more strictly penalized problem .",
    "together with theorem [ t : dist to sinf ] below , the linear convergence rate established in theorems [ t : convergence ] and [ t : convergence - polyhedrality ] of the previous section yield estimates on the distance of intermediate points to the solution set of the relaxed problem as well as estimates on the distance to feasibility for the unrelaxed problem .",
    "define @xmath286 this is a closed convex set since the @xmath287 are lsc and convex . if there exists some @xmath14 such that @xmath288 is nonempty and bounded then has a solution ( * ? ? ?",
    "* theorem 11.9 ) .",
    "this will happen , for instance , if @xmath289 and @xmath13 is _ coercive _ ( * ? ? ?",
    "* proposition 11.12 ) , that is @xmath13 satisfies @xmath290 such assumptions are naturally satisfied in many applications .",
    "moreover , @xmath291 , the lower level - set of @xmath13 corresponding to the optimal value @xmath292 in , is convex and so the set of optimal solutions to is also convex . define @xmath293 for the convex , lsc function @xmath282 satisfying @xmath294 for all @xmath295 and @xmath296 if and only if @xmath297 .",
    "then @xmath298 is convex , lsc and corresponds exactly to @xmath13 on the set @xmath299 .",
    "otherwise @xmath298 increases pointwise to @xmath10 at points outside @xmath299 as @xmath300 . for @xmath301 with @xmath302 ,",
    "the sequence of functions @xmath303 epi - converges ( see ( * ? ? ?",
    "* definition 7.1 ) ) to @xmath304 as @xmath305 where @xmath306 is the indicator function of the set @xmath299 .",
    "as we will allow _ approximate _ solution of problems it will be helpful to recall the set of @xmath307-minimizers : @xmath308 .",
    "the relation between the solution sets to and is detailed in the following , which is a direct application of ( * ? ? ?",
    "* theorem 7.33 ) .",
    "[ t : epi - convergence ] let @xmath9}$ ] , @xmath309 and @xmath310 be proper , lsc and convex , and let @xmath260 be linear .",
    "let @xmath13 be coercive with @xmath311 for @xmath299 defined by .",
    "suppose further that @xmath294 and that @xmath296 if and only if @xmath297 .",
    "define @xmath312 where @xmath313 as @xmath314 . then @xmath315 .",
    "moreover , for any sequence of errors @xmath316 and corresponding points @xmath317 , the sequence @xmath318 is bounded , and all its cluster points belong to @xmath319 .    _",
    "proof sketch .",
    "_ the property of the convex penalty @xmath282 that @xmath294 and @xmath296 if and only if @xmath297 yields epi - convergence of @xmath320 to @xmath321 .",
    "coercivity of @xmath13 guarantees that @xmath298 is level bounded for all values of @xmath268 .",
    "these two properties , together with lower semicontinuity and the fact that @xmath13 and @xmath298 are proper , are all that is needed to prove the result . @xmath322    if the regularization were _ exact _ , then we would know that for all parameter values @xmath273 large enough , the solutions to coincide with solutions to .",
    "we return to this later .",
    "we now turn our attention to solution of the problem for a fixed value of @xmath323 .",
    "the admm algorithm discussed in section [ s : siadpmm ] is useful for solving this problem in the sense that it has an error bound under specific assumptions which gives a stopping rule .",
    "this is not unique to algorithm [ a : admm ] , but we focus on this method due to its prevalence in practice .",
    "recall the exact problem : @xmath324 it will be convenient to rewrite the penalized problem as @xmath325 consider also the _ limiting _ problem @xmath326 we view problem as the regularized version of with @xmath13 as the regularizing functional and @xmath327 as the regularization parameter .",
    "denote the solution sets to these problems by @xmath328    if the penalization @xmath282 satisfies @xmath280 if and only if @xmath281 , then it is immediately clear that @xmath329 corresponds to the feasible set of problem hence @xmath330 .",
    "what is more remarkable is that , if a lagrange multiplier for exists , then @xmath331 for all @xmath273 large enough , that is , the penalty @xmath282 is _",
    "exact_.    [ t : exact penalty ] suppose that @xmath86 is nonempty and compact , and that there exist lagrange multipliers @xmath332 for .",
    "let the penalization @xmath282 in be convex .",
    "assume , moreover , that @xmath282 satisfies the condition @xmath280 if and only if @xmath281 .",
    "then the solution set to the penalized problem , @xmath333 , coincides with the solution set to the exact problem , @xmath86 , for all @xmath334 where @xmath335 is the polar function of @xmath282 given by @xmath336 .",
    "it is easy to check whether a solution @xmath337 is in fact feasible for ( and hence also in @xmath86 ) by simply evaluating the value of @xmath338 .",
    "more generally , one would check whether the first order optimality conditions for are satisfied at @xmath274 , namely @xmath339 an explicit formula for the subdifferential in for image denoising and deconvolution is given in section [ s : application ] as this will be needed for computing step of algorithm [ a : admm ] .",
    "if , in addition , @xmath329 is _ weakly sharp _ ( see definition [ d : weak sharp ] ) , then one can obtain an upper bound for the distance of solutions to to _ feasible _ solutions to , even in the absence of lagrange multipliers for .",
    "[ a : assumption - freidlandertseng]@xmath45    a.   [ a : assumption - freidlandertseng 1 ] the solution set @xmath329 of problem is nonempty .",
    "b.   [ a : assumption - freidlandertseng 2 ] @xmath340 is bounded for each @xmath341 and @xmath342 . c.   [ a : assumption - freidlandertseng 3 ] the solution set @xmath329 of is weakly sharp of order @xmath343 .    [",
    "t : dist to sinf ] suppose assumption [ a : assumption - freidlandertseng]- hold .",
    "a.   [ t : dist to sinf i ] for any @xmath344 , @xmath345 is bounded .",
    "b.   [ t : dist to sinf ii ] if , in addition , assumption [ a : assumption - freidlandertseng ] holds with modulus of sharpness @xmath346 , then for any @xmath344 there exists @xmath347 such that @xmath348 c.   [ t : dist to sinf iii ] if , in addition , assumption [ a : assumption - freidlandertseng ] holds and the penalization @xmath282 is exact , then for all @xmath273 large enough , @xmath349 and @xmath350 .    and . under the assumption [ a : assumption - freidlandertseng ] , theorem 5.1 in @xcite directly applies to yield the result .",
    "@xmath240    . if the penalization @xmath282 is exact , then @xmath280 if and only if @xmath281 , hence @xmath351 for all @xmath273 large enough , and @xmath329 corresponds exactly to the feasible set in .",
    "the error bound holds independent of the existence of lagrange multipliers for , hence , for exact penalization under assumption [ a : assumption - freidlandertseng ] , theorem [ t : dist to sinf ] yields an upper bound on the distance of solutions to to feasible points for .",
    "we specialize the above results to the application of optimization with statistical multiscale side constraints .",
    "all of the examples considered in this section satisfy the requirements of theorem [ t : convergence - polyhedrality ] , and thus for each fixed value of the penalty parameter @xmath273 local linear convergence to a solution of is guaranteed .",
    "moreover , the penalty function @xmath282 that we use is _ exact _ and hence by theorem [ t : exact penalty ] , for @xmath273 large enough , the computed solution to is also a solution to .",
    "what is not known _ a priori _ is what value of @xmath273 yields the correspondence .",
    "moreover , since the whole point of the relaxation is to remove the burden of satisfying the constraints , we approach a solution to via a sequence of solutions to for progressively larger values of @xmath273 .",
    "this is described precisely in the following sequentially penalized algorithm .",
    "the outer iteration , indexed by @xmath352 , consists of numerical approximations to solutions of for the penalty parameter @xmath323 .",
    "the the inner iteration proceeds with the current value of @xmath323 until the step size between successive iterates @xmath353 and @xmath354 drops in a linear fashion below a given tolerance @xmath355 . from theorem [ t : convergence - polyhedrality ] one can then obtain a posteriori estimates on the distance of the iterate @xmath356 to the true solution .",
    "then @xmath323 is increased by a constant factor . since , for this model the penalization @xmath282 is exact , once the constraints appear to be satisfied ( as determined by monitoring the value of @xmath357 ) , it is reasonable to conclude that the correspondence between problems and holds , and the penalty @xmath323 no longer needs to be updated ; the inner loop of the algorithm then can be run to the desired accuracy . as indicated in figures [ fig",
    ": s - data](b ) and [ fig : performance ] , the constraints appear to be satisfied when the penalty term @xmath358 ( green plot ) drops suddenly to machine precision .",
    "the application problem involves image deconvolution and denoising with statistical multiscale estimation as presented in @xcite .",
    "we are well aware that there are many ways to model such problems that permit much less computationally intensive numerical solutions than the technique we present here .",
    "our interest in multiresolution deconvolution / denoising is two - fold : first , it is one of the few techniques available that has the potential to yield quantitative ( i.e. statistical ) guarantees for the recovered images , and secondly , it is an important instance of convex optimization problems where the number of constraints grows superlinearly as a function of the number of unknowns .",
    "our numerical demonstration addresses the first issue of quantitative image denoising : if the numerics do not permit estimates for the distance to the model solution , then the quantitative assurances of the model are irrelevant .",
    "unlike the numerical approach proposed in @xcite , the numerical approach we present here permits error bounds to within machine accuracy of our numerical solution to the true model solution .",
    "following the approach proposed in @xcite we quantify the difference between an estimate @xmath359 and the data @xmath360 via the maximum absolute value of all weighted inner products of the residual function @xmath361 : @xmath362 the residual function used in @xcite @xmath240 is simply @xmath363 .",
    "the weights @xmath364 are scaled window functions so that the set @xmath365 is the index set corresponding to all collections of these subsets of the image .",
    "the statistical multiscale analysis requires that , on each window , @xmath366 the same error @xmath367 is specified at all scales .",
    "hence @xmath368 in specializes to @xmath369 for @xmath370 defined by ( @xmath271 ) and @xmath371 ( here we are expanding the original @xmath368 by the constant function @xmath372 . )",
    "the max function is a standard tool in exact penalization methods @xcite and falls naturally into the context of piecewise linear - quadratic functions .",
    "algorithm [ a : dadmm ] does not specify how the iterates @xmath373 and @xmath374 are calculated .",
    "the linear convergence of the inner iterations predicted in theorem [ t : convergence - polyhedrality ] , from which error bounds can be determined , as well as the numerical convergence of the outer iterates to problem is discussed next .",
    "computation of @xmath375 and @xmath376 in algorithm [ a : dadmm ] involves minimizing the sum of a convex quadratic function and ( in general ) a convex , nonsmooth , piecewise linear - quadratic function .",
    "this can be solved via any number of techniques ranging from first order methods like fista @xcite to higher - order nonlinear optimization methods like quasi - newton methods studied in @xcite .",
    "in order to take advantage of the relative sparsity of the active constraints , we propose the following ( exact ) algorithm .",
    "algorithm [ a : sp ] is an _ active set method _ and the set @xmath377 defined by is the set of _ active indexes at @xmath378_. another helpful interpretation is as a steepest subgradient descent method for solving @xmath379 the steepest descent step is @xmath380 for @xmath381 with @xmath382 and @xmath383 .",
    "the choice of the step length @xmath384 ensures that , at each step @xmath385 , the active set is growing ; specifically , @xmath386 at termination , the subdifferential @xmath387 is large enough that it contains the residual @xmath388 .",
    "the terminal point of algorithm [ a : sp ] , @xmath389 , is a point in since it satisfies the first - order optimality conditions : @xmath390 where @xmath391 .",
    "replacing @xmath392 and @xmath393 with @xmath375 and @xmath394 respectively yields the update for @xmath395 in algorithm [ a : dadmm ] .",
    "the expression for the subdiffferential @xmath396 is particularly simple in this case .",
    "note that @xmath397 for all @xmath378 . applying the ( convex ) calculus of subdifferentials to the objective @xmath398 , as permitted by the regularity of @xmath282 and @xmath399 ( see , for instance ( * ? ? ?",
    "* section 2.3 ) ) , yields @xmath400 where @xmath401 denotes the convex hull of a set of points .",
    "this , of course , assumes that @xmath287 is differentiable at @xmath378 for those @xmath402 .",
    "inspection of shows that this is not the case in general , in particular at points @xmath403 where @xmath404 .",
    "however , such points will never be in the active set @xmath405 since @xmath406 for all @xmath407 , so we can safely apply formula without further ado .",
    "this yields the following specialization for @xmath408 given by : @xmath409      fig .",
    "[ fig : s - data ] shows a set of synthetic exact data @xmath410 ( shown in blue ) and corresponding noisy data @xmath411 ( shown in green ) with @xmath412 data points , as well as the reconstructed / denoised signal @xmath413 ( shown in red ) . in this example",
    "we consider only denoising , that is , the imaging operator @xmath111 is the identity so @xmath414 .",
    "the noisy data @xmath360 was generated by adding i.i.d .",
    "gaussian random noise with standard deviation @xmath415 to each original data point of @xmath416 .",
    "\\(a )   of algorithm [ a : dadmm ] showing solutions , the constraint violation , the active set size and the objective value for the penalized problem for successively larger values of the penalty parameter @xmath273 .",
    "( c ) inner iterates of algorithm [ a : dadmm ] with @xmath417 : step sizes , constraint violation , objective value and gap between the primary , domain - space variables @xmath418 .",
    ", title=\"fig : \" ] + ( b )   of algorithm [ a : dadmm ] showing solutions , the constraint violation , the active set size and the objective value for the penalized problem for successively larger values of the penalty parameter @xmath273 .",
    "( c ) inner iterates of algorithm [ a : dadmm ] with @xmath417 : step sizes , constraint violation , objective value and gap between the primary , domain - space variables @xmath418 .",
    ", title=\"fig : \" ] ( c )   of algorithm [ a : dadmm ] showing solutions , the constraint violation , the active set size and the objective value for the penalized problem for successively larger values of the penalty parameter @xmath273 .",
    "( c ) inner iterates of algorithm [ a : dadmm ] with @xmath417 : step sizes , constraint violation , objective value and gap between the primary , domain - space variables @xmath418 .",
    ", title=\"fig : \" ]    in our specialization of problem we use the total variation penalty @xmath419 where @xmath420 is the ( discrete ) gradient operator .",
    "the structured constraints are given by .",
    "the weights @xmath421 are scaled window functions of all intervals of lengths between @xmath108 and @xmath422 pixels , and @xmath365 is the index set corresponding to all collections of successive pixels in @xmath423 of cardinality  or length  from @xmath108 to @xmath422 . the same error @xmath367",
    "is specified at all scales .    for a signal length @xmath412 with interval lengths from @xmath108 to @xmath422",
    "the number of windows is @xmath424 .",
    "the constant @xmath79 is , strictly speaking , redundant but was introduced as an additional means to balance the contributions of the individual terms to make the most of limited numerical accuracy ( double precision ) .",
    "we chose @xmath425 .",
    "the constant @xmath367 was taken to be @xmath426 .",
    "figure [ fig : s - data](a ) shows very good correspondence of the reconstructed signal to the original .",
    "the multi - resolution constraint prevents the usual `` blocky '' artifacts common to image denoising with tv - regularization .",
    "the eventual ( starting from around iteration @xmath427 ) linear convergence of the algorithm can be seen in figure [ fig : s - data](c ) . under the assumption that the latter iterates are indeed in the region of local linear convergence , the observed convergence rate is @xmath428 , which yields an a posteriori upper bound on the distance of the @xmath429th iterate to the true solution : @xmath430 .",
    "since the signal length is @xmath431 , this amounts to @xmath432 digits of accuracy in the pointwise value of the signal .      for our main demonstration ,",
    "we are presented with an image @xmath433 ( figure [ fig : edata](a ) ) generated from a stimulated emission depletion ( sted ) microscopy experiment @xcite conducted at the laser - laboratorium gttingen examining tubulin , represented as the `` object '' @xmath434 .",
    "the imaging model is simple linear convolution , @xmath435 where @xmath111 is a convolution matrix with a nonsymmetric experimentally measured point - spread function ( @xmath436 ) .",
    "the measurement @xmath360 is _ noisy _ or otherwise inexact , and thus an exact solution @xmath437 is not desirable .",
    "although the noise in such images is usually modeled by poisson noise , a gaussian noise model with constant variance suffices as the photon counts are of the order of @xmath438 per pixel and do not vary significantly across the image .",
    "figure  [ fig : edata](b ) shows a close - up which we used as the noisy data @xmath439 with @xmath440 data points .",
    "we calculate the numerically reconstructed tubulin density @xmath148 shown in figure [ fig : soln](a ) via algorithm [ a : dadmm ] for the problem with the qualitative objective @xmath441    [ cols= \" < , < , < , < \" , ]     for the image size @xmath440 with the window system of squares of lengths @xmath108 and @xmath442 , the number of windows is @xmath443 .",
    "the constant @xmath79 in is , strictly speaking , redundant but was introduced as an additional means to balance the contributions of the individual terms to make the most of limited numerical accuracy ( double precision ) .",
    "we chose @xmath425 .",
    "the constant @xmath367 was chosen so that the model solution would be no more than @xmath444 standard deviations from the noisy data on each interval of each scale .",
    "we emphasize that , since this is experimental data , there is no `` truth '' for comparison - the constraint , together with the error bounds on the numerical solution to the model solution provide statistical guarantees on the numerical reconstruction @xcite .",
    "the numerical `` image '' generated from the reconstructed tubulin density , @xmath148 , is given by @xmath445 and is shown in figure [ fig : soln](b ) ; this figure is a denoised version of the measured data shown in figure [ fig : edata](b ) . in figure  [ fig : performance](a ) a sample run of the algorithm shows a succession of outer iterations .",
    "the inner iteration is shown in figure [ fig : performance](b ) with the value of @xmath446 for which the constraints are exactly satisfied ( to within machine precision ) , indicating the correspondence of the computed solution of problem to a solution to the exact model problem .",
    "the eventual ( starting from around iteration @xmath447 ) linear convergence of the algorithm can be seen in figure [ fig : performance](c ) . under the assumption that the latter iterates are indeed in the region of local linear convergence , the observed convergence rate is @xmath448 , which yields an a posteriori upper estimate of the pixelwise error of about @xmath449 , or @xmath444 digits of accuracy at each pixel .",
    "we have focused our attention on the admm algorithm due partly to its prevalence in practice , and partly its amenability to our theoretical techniques .",
    "the parameter @xmath140 in algorithm [ a : admm ] was left constant . how to choose this parameter in the context of minimization is a perplexing question and worthy of further study .",
    "our theoretical framework can also be adapted to krasnoselski - mann relaxations of the douglas  rachford algorithm .",
    "statements about this will appear in work underway studying more generally _ averaged mappings_.    the statistical interpretation of the reconstruction in figure [ fig : soln](b ) as described in @xcite opens the door to a quantitative approach to image processing , but this is only valid when one can estimate the distance of the numerical approximation to the exact solution to the underlying model optimization problem .",
    "determining quantitative estimates for how close the numerical solution shown in figure [ fig : soln](a ) is to an exact solution to problem under the assumption of exact evaluation of the associated prox operators of has been the topic of our study .    what is needed and largely missing in the current treatment of algorithms in the literature is a complete error analysis accounting for accumulated errors at each stage of algorithms  due to finite precision or finite termination of iterative procedures  together with statements about how close one can get to the _ solution _ to a given optimization problem , as opposed to its _ optimal value _ , the latter having in general no necessary connection to the former .",
    "this is a monumental project that has not received as much attention in the literature as studies of complexity based upon function values .",
    "as we argued , the standard approach for handling inexact computation by assuming summable errors does not solve the problem , it just distributes it over infinitely many iterates .",
    "an alternative to this was suggested in ( * ? ? ?",
    "* section 6 ) and applied in @xcite which allows a fixed error over all iterations without compromising local linear convergence .",
    "more work in this direction would narrow the gap between theory and practice .",
    "_ duality of admm and the douglas  rachford algorithm . _ consider the sequence @xmath450 of the douglas  rachford iteration [ e : drsalg1 ] , for the case @xmath451 ; @xmath452 . recalling the two - step implementation , denote @xmath453 and @xmath454 . then is the proximal step @xmath455 on the operator @xmath456 .",
    "if @xmath111 has full column rank , by ( * ? ?",
    "* proposition 3.32(iv ) ) , this step can be performed by @xmath457 indeed , since @xmath111 has full rank , @xmath458 is a proper strongly convex function of @xmath392 and has a unique minimizer @xmath459 . from the optimality condition for , @xmath460",
    "hence , @xmath461 which implies @xmath462 .",
    "this gives @xmath463 using , @xmath464 substituting @xmath465 in - yields @xmath466    similarly , if we denote @xmath467 and @xmath468 , is the proximal step @xmath469 on the operator @xmath470 which can be performed via @xmath471 substituting @xmath472 , @xmath473 now , - and - together yield @xmath474 this is the admm algorithm - for the primal problem ( @xmath475 ) . @xmath322",
    "we thank jennifer schubert of the laser - laboratorium gttingen for providing us with the sted measurements shown in fig .  [",
    "fig : edata ] . thanks also to jalal fadili for fruitful discussions and helpful comments during the preparation of this work ."
  ],
  "abstract_text": [
    "<S> we consider the problem of minimizing the sum of a convex function and a convex function composed with an injective linear mapping . </S>",
    "<S> for such problems , subject to a coercivity condition at fixed points of the corresponding picard iteration , iterates of the alternating directions method of multipliers converge locally linearly to points from which the solution to the original problem can be computed . </S>",
    "<S> our proof strategy uses duality and strong metric subregularity of the douglas  rachford fixed point mapping </S>",
    "<S> . our analysis does not require strong convexity and yields error bounds to the set of model solutions . </S>",
    "<S> we show in particular that convex piecewise linear - quadratic functions naturally satisfy the requirements of the theory , guaranteeing eventual linear convergence of both the douglas  rachford algorithm and the alternating directions method of multipliers for this class of objectives under mild assumptions on the set of fixed points . </S>",
    "<S> we demonstrate this result on quantitative image deconvolution and denoising with multiresolution statistical constraints .     </S>",
    "<S> research of t. aspelmeier , c. charitha and d. r. luke was supported in part by the german research foundation grant sfb755-a4 . </S>",
    "<S> +    * 2010 mathematics subject classification : * primary 49j52 , 49m20 , 90c26 ; secondary 15a29 , 47h09 , 65k05 , 65k10 , 94a08 .    </S>",
    "<S> * keywords : * augmented lagrangian , admm , douglas  rachford , exact penalization , fixed point theory , image processing , inverse problems , metric regularity , statistical multiscale analysis , piecewise linear - quadratic , linear convergence </S>"
  ]
}