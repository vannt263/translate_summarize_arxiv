{
  "article_text": [
    "magnetohydrodynamic ( mhd ) equations can be used in modeling phenomena in a wide range of applications including laboratory  @xcite , astrophysical  @xcite , and space plasmas  @xcite .",
    "for example , 3d mhd simulations have been widely adopted in space weather simulations .",
    "the historical review and current status of the existing popular 3d mhd models can be found in  @xcite and  @xcite , respectively . however , mhd equations form a nonlinear system of hyperbolic conservation laws , which is so complex that high - resolution methods are necessary to solve them in order to capture shock waves and other discontinuities .",
    "these high - resolution methods are in general computationally expensive and parallel computational resources such as beowulf clusters or even supercomputers are often utilized to run the codes that implemented these methods  @xcite  @xcite  @xcite  @xcite  @xcite .    in the last few years , the rapid development of graphics processing units ( gpus ) makes them more powerful in performance and more programmable in functionality . by comparing the computational power of gpus and cpus ,",
    "gpus exceed cpus by orders of magnitude .",
    "the theoretical peak performance of the current consumer graphics card nvidia geforce gtx 295 ( with two gpus ) is 1788.48 g floating - point operations per second ( flops ) per gpu in single precision while a cpu ( core 2 quad q9650  3.0 ghz ) gives a peak performance of around 96gflops in single precision .",
    "the release of the _ compute unified device architecture ( cuda ) _",
    "@xcite hardware and software architecture is the culmination of such development .",
    "with cuda , one can directly exploit a gpu as a data - parallel computing device by programming with the standard c language and avoid working with a high - level shading language such as cg  @xcite , which requires a significant amount of graphics specific knowledge and was previously used for performing computation on gpus .",
    "detailed performance studies on gpus with cuda can be found in  @xcite and  @xcite .",
    "cuda is a general purpose parallel computing architecture developed by nvidia .",
    "it includes the cuda instruction set architecture ( isa ) and the parallel compute engine .",
    "an extension to c programming language and its compiler are provided , making the parallelism and high computational power of gpus can be used not only for rendering and shading , but also for solving many computationally intensive problems in a fraction of the time required on a cpu .",
    "cuda also provides basic linear algebra subroutines ( cublas ) and fast fourier transform ( cufft ) libraries to leverage gpus capabilities .",
    "these libraries release developers from rebuilding the frequently used basic operations such as matrix multiplication .",
    "graphics cards from g8x series support the cuda programming mode ; and the latest generation of nvidia gpus ( gt2x0 series or later ) unifies vertex and fragment processors and provides shared memory for interprocessor communication .",
    "a increasing number of new gpu implementations with cuda in different astrophysical simulations have been proposed .",
    "et al . _",
    "@xcite re - implemented the direct gravitational @xmath0-body simulations on gpus using cuda . for @xmath1",
    ", they reported a speedup of about 100 compared to the host cpu and about the same speed as the grape-6af .",
    "a library ` sapporo ` for performing high precision gravitational @xmath0-body simulations was developed on gpus by gaburov _",
    "et al . _",
    "this library achieved twice as fast as commonly used grape6a",
    "/ grape6-blx cards .",
    "et al . _",
    "@xcite  @xcite implemented a particle - in cell ( pic ) code on gpus for plasmas simulations and visualizations and demonstrated a speedup of 11 - 22 for different grid sizes .",
    "sainio  @xcite presented an accelerated gpu cosmological lattice program for solving the evolution of interacting scalar fields in an expanding universe , achieving speedups between one and two orders of magnitude in single precision . in the above works , no discussion on using double precision on gpus",
    "was reported . in mhd simulations ,",
    "the support of double precision is important , especially for nonlinear problems .",
    "we will evaluate the performance and accuracy of double precision on gpus in this work .",
    "in this paper , we present an efficient implementation to accelerate computation of mhd simulations on gpus , called _ gpu - mhd_. to our knowledge , this is the first work describing mhd simulations on gpus in detail . the goal of our work is to perform a pilot study on numerically solving the ideal mhd equations on gpus .",
    "in addition , the trend of today s chip design is moving to streaming and massively parallel processor models , developing new mhd codes to exploit such architecture is essential . _",
    "gpu - mhd _ can be easily ported to other many - core platforms such as intel s upcoming larrabee  @xcite , making it more flexible for the user s choice of hardware .",
    "this paper is organized as follows : a brief description of the cuda programming model is given in section 2 . the numerical scheme in which _ gpu - mhd _ adopted is presented in section 3 . in section 4 , we present the gpu implementation in detail .",
    "numerical tests are given in section 5 .",
    "accuracy evaluation by comparing single and double precision computation results is given in section 6 .",
    "performance measurements are reported in section 7 and visualization of the simulation results is described in section 8 .",
    "we conclude our work and point out the future work in section 9 .",
    "the compute unified device architecture ( cuda ) was introduced by nvidia as a general purpose parallel computing architecture , which includes gpu hardware architecture as well as software components ( cuda compiler and the system drivers and libraries ) .",
    "the cuda programming model  @xcite  @xcite  @xcite consists of functions , called _",
    "kernels _ , which can be executed simultaneously by a large number of lightweight _ threads _ on the gpu .",
    "these threads are grouped into one- , two- , or three - dimensional _ thread blocks _ , which are further organized into one- or two - dimensional _",
    "grids_. only threads in the same block can share data and synchronize with each other during execution .",
    "thread blocks are independent of each other and can be executed in any other . a graphics card that supports cuda , for example , the gt200 gpu  @xcite , consisting of 30 streaming multiprocessors ( sms ) .",
    "each multiprocessor consists of 8 streaming processors ( sps ) , providing a total of 240 sps .",
    "threads are grouped into batches of 32 called _ warps _ which are executed in single instruction multiple data ( simd ) fashion independently .",
    "threads within a warp execute a common instruction at a time .    for memory access and usage ,",
    "there are four types of memory , namely , _ global memory _ , _ constant memory _ , _ texture memory _ as well as _",
    "shared memory_. global memory has a separate address space for obtaining data from the host cpu s main memory through the pcie bus , which is about 8 gb / sec in the gt200 gpu .",
    "any valued stored in global memory can be accessed by all sms via load and store instructions .",
    "constant memory and texture memory are cached , read - only and shared between sps .",
    "constants that are kept unchanged during kernel execution may be stored in constant memory .",
    "built - in linear interpolation is available in texture memory . shared memory is limited ( 16 kb for gt200 gpu ) and shared between all sps in a mp . for detailed information concerning memory optimizations , we refer the reader to  cuda best practice guide \"  @xcite .",
    "double precision is one important concern in many computational physics applications , however , support of double precision is limited to the nivdia cards having compute capability 1.3 ( see appendix a in  @xcite ) such as the gtx 260 , gtx 280 , quadro fx 5800 ( contains one gt200 gpu ) , and tesla c1060 ( contains one gt200 gpu ) and s1070 ( contains four gt200 gpus ) . in gt200 gpu , there are eight single precision floating point ( fp32 ) arithmetic logic units ( alus ) ( one per sp ) in sm , but only one double precision floating point ( fp64 ) alu ( shared by eight sps ) .",
    "the theoretical peak performance of gt200 gpu is 936 gflops in single precision and 78 gflops in double precision . in cuda",
    ", double precision is disabled by default , ensuring that all double numbers are silently converted into float numbers inside kernels and any double precision calculations computed are incorrect . in order to use double precision floating point numbers ,",
    "we need to call ` nvcc ` :  ` -arch ` `` = ` sm_13 ` \" . the flag  ` -arch ` `` = ` sm_13 ` \" in the command tells  `",
    "nvcc ` \" to use the compute capability 1.3 which means enabling the double precision support .",
    "the recent fermi architecture  @xcite ( gtx 480 , for example ) significantly improves the performance of double precision calculations by introducing better memory access mechanisms .    in sections 6 and 7",
    "we compare the accuracy and actual performance of _ gpu - mhd _ in single and double precision on both gt200 and fermi architectures .",
    "the ideal mhd equations with the assumption of the magnetic permeability @xmath2 can be represented as hyperbolic system of conservation laws as follows  @xcite @xmath3    here , @xmath4 is the mass density , @xmath5 the momentum density , @xmath6 the magnetic field , and @xmath7 the total energy density .",
    "the total pressure @xmath8 where @xmath9 is the gas pressure that satisfies the equation of state , @xmath10 .",
    "in addition , the mhd equations should obey the divergence - free constraint @xmath11 .    over the last few decades",
    ", there has been a dramatic increase in the number of publications on the numerical solution of ideal mhd equations .",
    "in particular the development of shock - capturing numerical methods for ideal mhd equations .",
    "we do not provide an exhaustive review of the literature here .",
    "a comprehensive treatment of numerical solution of mhd equations can be found in  @xcite , for example .",
    "et al . _",
    "@xcite proposed a free , fast , simple , and efficient total variation diminishing ( tvd ) mhd code featuring modern high - resolution shock capturing on a regular cartesian grid .",
    "this code is second - order accuracy in space and time and enforces the @xmath12 constraint to machine precision and it was successfully used for studying nonradiative accretion onto the supermassive black hole  @xcite and fast magnetic reconnection  @xcite . due to these advantages and convenience for gpu verse cpu comparison ,",
    "the underlying numerical scheme in _ gpu - mhd _ is based on this work . a detailed comparison of shock capturing mhd codes",
    "can be found in  @xcite , for example .",
    "we plan to explore other recent high - order godunov schemes such as  @xcite and  @xcite for _ gpu - mhd _ as our future work .",
    "we briefly review the numerical scheme  @xcite we adopted in _ gpu - mhd _ here . in this numerical scheme ,",
    "the magnetic field is held fixed first and then the fluid variables are updated .",
    "a reverse procedure is then performed to complete a one time step .",
    "three dimensional problem is split into one - dimensional sub - problems by using a strang - type directional splitting  @xcite .",
    "firstly , we describe the fluid update step in which the fluid variables are updated while holding the magnetic field fixed .",
    "the magnetic field is interpolated to cell centers for second - order accuracy . by considering the advection along the @xmath13 direction ,",
    "the ideal mhd equations can be written in flux - conservative vector form as @xmath14    where the flux vector is given by @xmath15    equation ( [ eq : euler ] ) is then solved by jin & xin s relaxing tvd method  @xcite . with this method ,",
    "a new variable @xmath16 is defined , where @xmath17 is a free positive function called the _ flux freezing speed_. for ideal mhd equations , we have @xmath18 and equations @xmath19 \\frac{\\partial { { { \\mbox{\\boldmath $ w$}}}}}{\\partial t } + \\frac{\\partial}{\\partial x}(c{{{\\mbox{\\boldmath $ u$ } } } } ) & = 0\\end{aligned}\\ ] ]    these equations can be decoupled through a change of left- and right - moving variables @xmath20 and @xmath21 @xmath22 \\frac{\\partial { { { \\mbox{\\boldmath $ u$}}}}^l}{\\partial t } - \\frac{\\partial}{\\partial x}(c{{{\\mbox{\\boldmath $ u$}}}}^l ) & = 0\\end{aligned}\\ ] ]    the above pair of equations is then solved by an upwind scheme , separately for right- and left - moving waves , using cell - centered fluxes",
    ". second - order spatial accuracy is achieved by interpolating of fluxes onto cell boundaries using a monotone upwind schemes for conservation laws ( muscl )  @xcite with the help of a flux limiter .",
    "runge - kutta scheme is used to achieve second - order accuracy of time integration .",
    "we denote @xmath23 as the cell - centered values of the cell @xmath24 at time @xmath25 , @xmath26 as the cell - centered flux in cell @xmath24 , as an example , we consider the positive advection velocity , negative direction can be obtained in a similar way .",
    "we obtain the first - order upwind flux @xmath27 from the averaged flux @xmath28 in cell @xmath24 .",
    "two second - order flux corrections can be defined using three local cell - centered fluxes as follows @xmath29 \\delta{{\\mbox{\\boldmath $ f$}}}_{n + 1/2}^{r , t } & = \\frac{{{\\mbox{\\boldmath $ f$}}}_{n + 1}^t - { { \\mbox{\\boldmath $ f$}}}_n^t}{2}\\end{aligned}\\ ] ]    when the corrections have opposite signs , there is no second - order correction in the case of near extrema . with the aid of a flux limiter @xmath30",
    "we then get the second - order correction @xmath31 the van leer limiter  @xcite @xmath32 is used in _ gpu - mhd_. by adding the second - order correction to the first - order fluxes we obtain second - order fluxes .",
    "for example , the second - order accurate right - moving flux @xmath33 can be calculated @xmath34    the time integration is performed by calculating the fluxes @xmath35 and the freezing speed @xmath36 in the first half time step is given as follows @xmath37 where@xmath38 is computed by the first - order upwind scheme . by using the second - order tvd scheme on @xmath39 , we obtain the full time step @xmath40 @xmath41    to keep the tvd condition , the flux freezing speed @xmath42 is the maximum speed information can travel and should be set to @xmath43 as the maximum speed of the fast mhd wave over all directions is chosen . as the time integration",
    "is implemented using a second - order runge - kutta scheme , the time step is determined by satisfying the cfl condition @xmath44\\\\ \\delta t & = cfl / c_{max } \\end{aligned}\\ ] ] where @xmath45 is the courant - number and @xmath46 is generally set to @xmath47 for stability , and @xmath48 is the magnitude of the magnetic field . constrained transport ( ct )  @xcite is used to keep the @xmath49 to machine precision .",
    "therefore , the magnetic field is defined on cell faces and it is represented in arrays  @xcite @xmath50 where the cell centers are denoted by @xmath51 , and faces by @xmath52 , @xmath53 , and @xmath54 , etc .",
    "the cells have unit width for convenience .",
    "secondly , we describe the update of the magnetic field in separate two - dimensional advection - constraint steps along @xmath13-direction while holding the fluid variables fixed .",
    "the magnetic field updates along @xmath55 and @xmath56-directions can be handled in a similar matter .",
    "we follow the expressions used in  @xcite .",
    "for example , we can calculate the averaging of @xmath57 along @xmath13 direction as follows @xmath58\\ ] ] a first - order accurate flux is then obtained by @xmath59 where the velocity average is @xmath60\\ ] ]    @xmath61 is updated by constructing a second - order - accurate upwind electromotive force ( emf ) @xmath62 using jin & xin s relaxing tvd method  @xcite in the advection step .",
    "then this same emf is immediately used to update @xmath63 in the constraint step .",
    "extension to three dimensions can be done through a strang - type directional splitting  @xcite .",
    "equation ( [ eq : euler ] ) is dimensionally split into three separate one - dimensional equations . for a time step @xmath64 , let @xmath65 be the fluid update along @xmath13 , @xmath66 be the update of @xmath61 along @xmath55 , and @xmath67 be the update operator of @xmath68 to @xmath69 by including the flux along @xmath70 direction .",
    "each @xmath67 includes three update operations in sequence , for example , @xmath71 includes @xmath65 , @xmath72 , and @xmath73 . a forward sweep and a reverse sweep",
    "are defined as @xmath74 and @xmath75 , respectively .",
    "a complete update combines a forward sweep and reverse sweep .",
    "the dimensional splitting of the relaxing tvd can be expressed as follows  @xcite @xmath76 { { \\mbox{\\boldmath $ u$}}}^{t_3 } & = { { \\mbox{\\boldmath $ u$}}}^{t_2 + 2\\delta t_2 } = l_zl_xl_yl_yl_xl_z{{\\mbox{\\boldmath $ u$}}}^{t_2}\\\\[8pt ] { { \\mbox{\\boldmath $ u$}}}^{t_4 } & = { { \\mbox{\\boldmath $ u$}}}^{t_3 + 2\\delta t_3 } = l_yl_zl_xl_xl_zl_y{{\\mbox{\\boldmath $ u$}}}^{t_3}\\end{aligned}\\ ] ] where @xmath77 , @xmath78 , and @xmath79 are sequential time steps after each double sweep . for cartesian coordinate system , it is easy to apply strang - type directional splitting  @xcite on a high - dimensional problem and split it into one - dimensional sub - problems in cartesian coordinate system  @xcite . in principle , we can also apply directional splitting for cylindrical or spherical coordinate systems .",
    "we may need to split the edges of grid in any direction into equal - distance pieces and determine the positions of the cell centers and face centers .",
    "similar techniques from li and li  @xcite can be utilized to extend the usage of directional splitting for cylindrical or spherical coordinate systems .",
    "this extension will be left as our future work .",
    "in this section , we provide the implementation details of _ gpu - mhd_. with _ gpu - mhd _ , all computations are performed entirely on gpus and all data is stored in the gram of the graphics card .",
    "currently , _",
    "gpu - mhd _ works on a regular cartesian grid and supports both single and double precision modes .",
    "considering the rapid development of graphics hardware , our gpu implementation was design general enough for gt200 architecture ( gtx 295 in our study ) and fermi architecture ( gtx 480 in our study ) .",
    "therefore , _ gpu - mhd _ can be used on newer architectures without significant modification .",
    "before we explain our gpu implementation in detail , the consideration and strategy of our design is presented first . during the computational process ,",
    "the tvd numerical scheme for solving the mhd equations will generate many intermediate results such as the  flux \" and some interpolated values of each grid point .",
    "these intermediate results will then be used in the next calculation step .",
    "one important thing is not only these intermediate results of the current grid point but also those of the neighboring grid points are needed to be stored .",
    "this means the intermediate results of the neighboring grid points have to be calculated before going to the next calculation step . as a result ,",
    "each calculation step in the algorithm was designed with one or several kernels and huge amount of data should be stored . in order to avoid the data transmission between cpu and gpu during the computation , _ gpu - mhd _ was designed to be run entirely on gpus . to reduce the memory usage , the storage for the intermediate results will be reused to store the intermediate results generated by the next step .",
    "the eight components ( @xmath80 ) for solving the mhd equations are stored in the corresponding eight arrays .",
    "each component of a grid point is stored close to the same component of the neighboring gird points . in any calculation step ,",
    "only the necessary component of a calculation ( kernel ) will be accessed , thus providing more effective i / o . the strategy of our design is summarized as follows :    * each step of the numerical scheme is handled with one or several kernels to exploit the parallelism of gpus ; + * storage of the intermediate results are reused to reduce memory usage ; + * components of the mhd equations are stored in separate arrays to provide effective memory access .",
    "although shared memory provides much faster access rate than global memory , its size is very limited ( 16 kb in gtx 295 and 48 kb in gtx 480 ) .",
    "as we have to process many intermediate results in each calculation step , shared memory is too small to fit in our gpu implementation .",
    "of course there are some techniques of using shared memory , the basic idea is to copy the data from global memory to the shared memory first , and then use the data in shared memory to do calculations .",
    "after the calculations have been completed , write these results back to global memory .",
    "this will benefit those computations that need many data accesses during the calculation period .",
    "however , as we mentioned in the beginning of this section , due to the nature of the algorithm , _ gpu - mhd _ was designed with many separated cuda kernels .",
    "calculation of each kernel actually is simple and variables of grid points in each kernel are mostly accessed only once ( read ) or twice ( read and then write the result ) . in order to provide fast access speed , parameters and temporary results ( generated and used only within kernel ) in each kernel are stored with registers .",
    "the parameters for the whole simulation such as the data size and size of dimensions are stored using constant memory .",
    "thus in our case , shared memory does not show its advantages . on the other hand ,",
    "the size of shared memory is too small for our problem , especially when double precision is used in the calculations .",
    "we did try to use shared memory in _ gpu - mhd _ by coping the capable amount of data to shared memory for the calculations , but there is no speedup compared to our current approach .",
    "therefore , our code mainly uses global memory .",
    "there are three phases in our code : transform of data from the host memory into the global memory , execution of the kernels , and transfer of data from the gpu into the host memory .    for global memory ,",
    "if the data is well organized in global memory with the form that a load statement in all threads in a warp accesses data in the same aligned 128-byte block , then the threads can efficiently access data from the global memory .",
    "the process of organizing the data in such a form is so called coalescing  @xcite  @xcite . actually , gt200 architecture",
    "( with compute capability 1.2 or 1.3 ) has more flexible in handling data in global memory than those cards with compute capability 1.1 or lower .",
    "coalescing of loading and storing data that are not aligned perfectly to 128-byte boundaries is handled automatically on this architecture ( see appendix g.3.2.2 in  @xcite ) .",
    "we illustrate this new feature in figure  [ fig : gtx200_coalescing ] .",
    "gt200 architecture supports 32 bytes memory block and has less limitation to memory address , which is accessed by the header ( first ) thread . even without  shifting \" the address to aligned 64 bytes or 128 bytes",
    ", the gpu kernels can still keep good performance , especially when we only process with @xmath81 data .",
    "the memory arrangement of _ gpu - mhd _ is presented here . the most intuitive way to write a parallel program to solve",
    "a multidimensional problem is to use multidimensional arrays for data storage and multidimensional threads for computation .",
    "however , the ability of the current cuda is limited in supporting multidimensional threads , therefore , we could not implement our code in such a straightforward way . especially in three dimensions or higher dimensions , there are still some limitations in handling multidimensional arrays and multidimensional threads . as a result ,",
    "the most primitive way is to store data in one - dimension and perform parallel computation with one - dimension threads . by using an indexing technique ,",
    "our storage and threading method can be extended to to solve multidimensional problems .",
    "our data storage arrangement is expressed in fig .",
    "[ fig : matrix ] and in equations ( [ index1 ] ) to ( [ index2 ] ) .",
    "@xmath82 } / size_{z } \\\\",
    "= index & \\mathbf{mod } & size_{z } \\end{array}\\right .\\ ] ]    @xmath83\\label{index2 } index_{y } \\pm 1 & = index \\pm size_{z}\\\\[8pt ] index_{z } \\pm 1 & = index \\pm 1\\label{index3}\\end{aligned}\\ ] ]    here @xmath84 , @xmath85 , and @xmath86 are the indexes of a 3d matrix .",
    "@xmath87 is the 1d index used in _ gpu - mhd _ ,",
    "@xmath88 , and @xmath89 are the matrix size ( number of grid points in our study ) of a 3d matrix .",
    "equation ( [ index1 ] ) expresses the mapping of three - dimensional ( 3d ) indexes to one - dimensional ( 1d ) indexes .",
    "equations ( [ index2 ] ) to ( [ index3 ] ) express the shift operations .",
    "shift operations are very important in numerical solution of conservation laws because some calculations are based on the neighboring grid points .",
    "the above indexing technique is used to prepare suitable values ( vectors ) as input values for the calculation kernels we implemented in cuda . as an example",
    ", we give a conceptual calculation kernel for a calculation in @xmath13-dimension to show how the indexing technique works for this task in the following .",
    "this kernel calculates the result with the grid point itself and neighboring gird points in @xmath13-dimension .",
    "the calculations in @xmath55- or @xmath56-dimension have the similar form .",
    "calculate_x(data , result )    index = getid ( ) ; //self - increment index for multi - threading grid_point_index = index ; //(x , y , z ) neighbor_1 = grid_point_index + ( sizey * sizez ) ; //(x + 1 , y , z ) neighbor_2 = grid_point_index - ( sizey * sizez ) ; //(x - 1 , y , z )    calculate_kernel(data , result , grid_point_index , neighbor_1 , neighbor_2 , ... ) ; ......    the indexing technique is a common way to represent multidimensional arrays using 1d arrays by mapping a 3d index @xmath90 to an 1d index @xmath91 .",
    "the gpu kernels of tvd were designed such that each kernel calculates using the actual index of a particular grid point and its neighbors .",
    "for example , if the calculation needs the information in a particular gird point and its neighboring grid points in @xmath56-dimension , then the indexing operation will retrieve @xmath92 $ ] , @xmath93 $ ] and @xmath94 $ ] and proceed the calculation .",
    "if the calculation needs the information in a particular grid point and its neighboring grid points in @xmath55-dimension , then the indexing operation will retrieve @xmath95 $ ] , @xmath93 $ ] and @xmath96 $ ] .",
    "then these resulting indexes from indexing operation will pass to the gpu kernels of tvd for performing the calculation . as a result , for @xmath0-dimension problem , what we need are @xmath0-dimension indexing operation kernels while only one tvd kernel is needed at all the time .    for efficiency , _",
    "gpu - mhd _ only processes the problem with the number of grid points satisfying @xmath81 condition .",
    "one reason is that the size of a warp of gpu contains 32 threads , problems with grid point number of @xmath81 are easier to determine the number of threads and blocks to fit in multiple of a warp before the gpu kernel is called .",
    "that means we do not need to check if the i d of the grid point being processed ( calculated by the block i d and thread i d ) is out of the range .",
    "it is very helpful in making the gpu code run more efficient . on the other hand , it is also effective to reduce logical operations in a gpu kernel , which is known to be a little bit slow in the current gpu architecture . as a result ,",
    "warp divergence caused by the number of the data is avoided ( there is still a little bit warp divergence caused by  if \" operation in the calculation of our algorithm ) .",
    "similar method is used in the cuda sdk code sample  reduction \" .",
    "the actual memory pattern used in _ gpu - mhd _ will be presented at the end of next subsection after introducing our algorithm .",
    "a  cuda kernel \" is a function running on gpu  @xcite  @xcite  @xcite",
    ". noted that the cuda kernel will process all grid points in parallel , therefore , a ` for ` instruction is not needed for going through all grid points . _",
    "gpu - mhd _ includes the following steps :    1 .",
    "cuda initialization 2 .",
    "setup the initialize condition for the specified mhd problem : + @xmath97 of all grid points , @xmath98 of cell faces , and set parameters such as time @xmath25 , etc .",
    "3 .   copy the the initialize condition @xmath99 , @xmath100 to device memory ( cuda global memory ) 4 .   for all grid points ,",
    "calculate the @xmath101 by equation ( [ equat_cfl ] ) ( implemented with a cuda kernel ) 5 .",
    "use ` cublasisamax ` ( in single precision mode ) function or ` cublasidamax ` ( in double precision mode ) function of the cublas library to find out the maximum value of all @xmath101 , and then determine the @xmath64 6 .",
    "since the value of @xmath64 is stored in device memory , read it back to host memory ( ram ) 7 .",
    "sweeping operations of the relaxing tvd ( calculation of the @xmath102 , implemented with several cuda kernels , will be explained in the next subsection ) 8 .",
    "@xmath103 9 .   `",
    "if ` @xmath25 reaches the target time , go to next step + ` else ` repeats the procedure from step ( 4 ) 10 . read back data @xmath99 , @xmath100 to host memory 11 .",
    "output the result    the program flow of _ gpu - mhd _ is shown in fig .",
    "[ fig : flowchart ] .",
    "after the calculation of the cfl condition , the sweeping operations will be performed .",
    "the sweeping operation @xmath104 will update both the fluid variables and orthogonal magnetic fields along @xmath70 dimension .",
    "this is a core computation operation in the relaxing tvd scheme described in section  [ mhd ] .",
    "the cfl condition for the three - dimensional relaxing tvd scheme is obtained by equation ( [ equat_cfl ] ) .",
    "the procedure is to calculate all the @xmath101 of each grid point and find out the maximum value . in _ gpu - mhd",
    "_ , parallel computation power of cuda is exploited to calculate the @xmath101 of each grid point in parallel and all the @xmath101 values are stored in a matrix .",
    "then the ` cublasisamax ` function is used ( in double precision mode , the ` cublasidamax ` function is used ) to find out the maximum @xmath101 of the matrix in parallel ( called the reduction operation ) .",
    "the ` cublasisamax ` function is provided in the cublas library  a set of basic operations for vector and matrix provided by nvidia with the cuda toolkit  @xcite .",
    "the reason we read the @xmath64 back and store both @xmath64 and @xmath25 in host memory is due to the data in device memory can not be printed out directly in the current cuda version",
    ". this information is useful for checking if there is any problem during the simulation processing .",
    "the implementation of sweeping operations will be explained in the next subsection .",
    "before we start to describe the sweeping operations , consideration of memory arrangement is presented first in the following .",
    "implementing parallel computation using cuda kernels is somewhat similar to parallel implementation on a cpu - cluster , but it is not the same .",
    "the major concern is the memory constrain in gpus .",
    "cuda makes parallel computation process on gpus which can only access their graphics memory ( gram ) .",
    "therefore , data must be stored in gram in order to be accessed by gpus .",
    "there are several kinds of memory on graphics hardware including registers , local memory , shared memory , and global memory , etc . ,",
    "and they have different characteristics and usages  @xcite , making memory management of cuda quite different compared to parallel computation on a cpu - cluster .",
    "in addition , even the size of gram in a graphics card increases rapidly in newer models ( for example , the latest nvidia graphics card  geforece gtx 295 has 1.75 g gram ) , but not all the capacity of gram can be used to store data arbitrarily . shared memory and local memory are flexible to use , however , their sizes are very limited in a block and thus they can not be used for storing data with large size . in general , numerical solution of conservation laws will generate many intermediate results ( for example , @xmath105 , @xmath106 , @xmath42 , @xmath107 , etc . ) during the computation process , these results should be stored for subsequent steps in the process .",
    "therefore , global memory was mainly used in _ gpu - mhd_.    after the maximum value of @xmath101 in equation ( [ equat_cfl ] ) is found , we can get the @xmath108 by determining the courant - number ( @xmath45 ) .",
    "the sequential step is the calculation of @xmath104 ( @xmath109 ) .",
    "the implementation of @xmath104 includes two parts : update the fluid variables and update the orthogonal magnetic fields . as an example , the process for calculating @xmath71 is shown in fig .  [",
    "fig : lx ] where each block was implemented with one or several cuda kernels .",
    "the process for calculating @xmath110 or @xmath111 is almost the same as @xmath71 except that the dimensional indexes are different .",
    ".,width=384 ]    the first part of the @xmath71 calculation process is @xmath65 .",
    "the fluid variables will be updated along @xmath13 .",
    "* algorithm 1 * shows the steps and gpu kernels of this process ( the data of @xmath99 and @xmath100 are already copied to device memory ) , all the steps are processed on all grid points with cuda kernels in parallel .",
    "load @xmath99 , @xmath100 and @xmath64    memory allocation for the storage of the intermediate results : @xmath112 , @xmath113 , @xmath114 , @xmath115 , ( @xmath115 includes the storage of @xmath106 , @xmath42 , @xmath107 , etc )    @xmath116 results obtained by equation ( [ equat_b ] ) with @xmath100 , ( @xmath100 stored the magnetic field of the cell faces )    @xmath117 results obtained by equations ( [ eq : flux ] ) and ( [ eq : lr ] ) with @xmath99    @xmath118 the flux of a half time step : difference calculation (  first - order upwind scheme of fluid \" cuda kernels in fig .",
    "[ fig : lx ] ) obtained by equation ( [ eq : delta_u_half ] ) using @xmath115    @xmath119 calculate the intermediate result ( @xmath120 ) using equation ( [ eq : delta_u_half ] ) with @xmath99 and @xmath114    @xmath117 results obtained by equations ( [ eq : flux ] ) and ( [ eq : lr ] ) with @xmath113 ( the same algorithm and same cuda kernels in step 4 )    @xmath118 the flux of another half time step : difference calculation (  second - order tvd scheme of fluid \" cuda kernels in fig .",
    "[ fig : lx ] ) obtained by equation ( [ eq : delta_u_full ] ) and the limiter ( equation ( [ eq : limiter ] ) ) using @xmath115    calculate the result of @xmath121 with @xmath114 using equation ( [ eq : delta_u_full ] ) and save it back to @xmath99    free the storage of the intermediate results    ( continue to the second part of @xmath122 , update the orthogonal magnetic fields )    in this process , we have to calculate the magnetic fields of the grid point ( equation [ equat_b ] ) first because all the magnetic fields are defined on the faces of the grid cell  @xcite . to update the fluid variables of @xmath71 , the main process , which includes one or even several cuda kernels , is to calculate the affect of the orthogonal magnetic fields to the fluid variables of equations ( [ eq : flux ] ) , ( [ eq : lr ] ) and ( 10 ) .",
    "one such main process gives the flux of the @xmath123 step .",
    "after two main processes of flux calculation and the other difference calculations , the value of fluid  @xmath99 is updated from @xmath124 to @xmath125 in one @xmath122 process .",
    "the second part of the @xmath71 calculation process is to update the orthogonal magnetic fields in @xmath55-dimension ( @xmath126 ) , and @xmath56-dimension ( @xmath73 ) with the fluid along @xmath13-dimension .",
    "the strategy and implementation are similar to those in the first part but with a different algorithm for the orthogonal magnetic fields .",
    "( after the processes of fluid , we obtain an updated @xmath99 )    load @xmath127 ( density @xmath4 ) , @xmath128 ( @xmath129 ) , @xmath100 and @xmath64    memory allocation for the intermediate results : @xmath112 , @xmath114 , @xmath130 and @xmath131    @xmath132 determine the fluid speed with the updated @xmath127 and @xmath128 in @xmath65 , with the difference calculated in @xmath55-dimension    @xmath133 results obtained by equation ( [ eq : averaging_x ] )    @xmath118 the flux of a half time step : difference calculation of  flux of magnetic field in @xmath55-dimension \" (  first - order upwind scheme of magnetic field \" cuda kernels in fig .",
    "[ fig : lx ] ) obtained by equations ( [ eq : delta_u_half ] ) and ( [ eq : first_flux ] ) )    @xmath116 calculate the intermediate result ( @xmath120 ) by applying equation ( [ eq : delta_u_half ] ) to @xmath134 ( not by applying equation ( [ eq : delta_u_half ] ) to @xmath99 ) with @xmath63 and @xmath114    @xmath118 the flux of another half time step : difference calculation (  second - order tvd scheme of magnetic field \" cuda kernels in fig .  [ fig : lx ] ) obtained by equation ( [ eq : delta_u_half ] ) , the limiter of equation ( [ eq : limiter ] ) and equation ( [ eq : first_flux ] )    calculate the result of @xmath135 and @xmath136 with @xmath137 by applying equation ( [ eq : delta_u_full ] ) ) to @xmath134 , and save it back to @xmath100    ( the following steps is similar to above steps but the affected orthogonal magnetic field is changed from @xmath55 to @xmath56 )    @xmath132 determine the fluid speed with the updated @xmath127 and @xmath128 in @xmath65 , with the difference calculated in @xmath56-dimension    @xmath133 results obtained with equation ( [ eq : averaging_x ] ) using index of @xmath70 , @xmath138 , @xmath139    @xmath118 the flux of a half time step : difference calculation of  flux of magnetic field in @xmath56-dimension \" (  first - order upwind scheme of magnetic flied \" cuda kernels in fig .",
    "[ fig : lx ] ) obtained by equations ( [ eq : delta_u_half ] ) and ( [ eq : first_flux ] )    @xmath140 calculate the intermediate result ( @xmath141 ) by applying equation ( [ eq : delta_u_half ] ) to @xmath142 ( not by applying equation ( [ eq : delta_u_half ] ) to @xmath99 ) with @xmath143 and @xmath114    @xmath118 the flux of another half time step : difference calculation (  second - order tvd scheme of magnetic flied \" cuda kernels in fig .",
    "[ fig : lx ] ) obtained by equation ( [ eq : delta_u_half ] ) , the limiter of equation ( [ eq : limiter ] ) and equation ( [ eq : first_flux ] )    calculate the results of @xmath144 and @xmath145 with @xmath137 by applying equation ( [ eq : delta_u_full ] ) ) to @xmath142 , and save it back to @xmath100    free the storage of the intermediate results    in * algorithm 1 * , the calculations in steps ( 4 ) to ( 9 ) are the steps for @xmath126 , and steps ( 11 ) to ( 16 ) are the steps for @xmath73 .",
    "the steps for @xmath126 and @xmath73 are almost the same , and the only different parts are the dimensional indexes of the difference calculations , and the affected magnetic fields : @xmath134 and @xmath142 . after the first part of @xmath122 the fluid @xmath124 is updated to @xmath125 .",
    "this change of the fluid affects to the orthogonal magnetic fields .",
    "therefore , the corresponding change ( flux ) of orthogonal magnetic fields can be calculated with the density and velocity of the updated fluid @xmath146 .",
    "then the orthogonal magnetic fields are also updated to @xmath147 and @xmath148 , and also , these changes give effects to @xmath149 .",
    "after one process of @xmath122 , both fluid and magnetic fields are updated to @xmath150 with the affect of the flow in @xmath13-dimension .",
    "and a sweeping operation sequence includes two @xmath122 , @xmath151 , and @xmath152 ( see equations ( [ eq : l_sequence ] ) ) .",
    "so we actually get the updated fluid and magnetic fields of @xmath153 after one sweeping operation sequence .",
    "note that the second @xmath122 in the sequence is a reverse sweeping operation , the order of @xmath65 , @xmath154 and @xmath73 has to be reversed : @xmath126 and @xmath155 first , and @xmath65 second .",
    "as we mentioned before , numerical solution of conservation laws needs lots of memory because there are many intermediate results generated during the computation process .",
    "these intermediate results should be stored for the next calculation steps which need the information of the neighboring grid points obtained in the previous calculation steps .",
    "otherwise , in order to avoid the asynchronous problem in parallel computation , we have to do many redundant processes .",
    "this is due to the processors on gpus will not automatically start or stop working synchronously . without storing the intermediate results , it will be hard to guarantee the values of the neighboring grid points updated synchronously . with the purpose to minimizing the memory usage , not only the calculation process of @xmath71 is divided into several steps ( cuda kernels ) , but also the intermediate results are stored as little as possible .",
    "the processes dealing with the difference calculations are also divided into several steps to minimize the storage of the intermediate results and to guarantee there is no wrong result caused by asynchronous problem .",
    "it should be realized that most of the processes in the three - dimensional relaxing tvd scheme with the dimensional splitting technique is similar .",
    "et al . _",
    "@xcite swapped the data of @xmath13 , @xmath55 , and @xmath56-dimensions while _ gpu - mhd _ used one - dimensional arrays .",
    "but the similar swapping technique can be applied in our case with some indexing operations . instead of transposing or swapping the data",
    ", we implemented each calculation part of the flux computation with two sets of cuda kernels : one set is the cuda kernels for calculating the relaxing tvd scheme ( we call it tvd kernel here ) and the other set is the cuda kernels actually called by @xmath104 operations ( we call them @xmath104 kernels here ) .",
    "indexing operations are contained in all @xmath104 kernels .",
    "after the index is calculated , tvd kernels are called and the indexes are passed to the tvd kernels , letting the tvd kernels calculate the flux of corresponding dimension . therefore , the difference among @xmath122 , @xmath151 , and @xmath152 is the dimensional index .",
    "the flux computation of _ gpu - mhd _ is shown in fig .",
    "[ fig : indexingkernels ] .        the indexing operation swaps the target that will be updated and the neighboring relationship will also be changed accordingly . for example , the calculation that uses @xmath156 as the neighboring element in @xmath71",
    "will be changed to @xmath157 in @xmath110 .",
    "as transposing the data in a matrix needs more processing time , it is efficient and flexible to extend the code to multidimensional by dividing the indexing operation and flux calculation .    as we mentioned in section 4.1 ,",
    "the data is stored in 1d array , the data accesses of @xmath71 , @xmath110 , and @xmath111 are depicted in fig .",
    "[ fig : dataaccess ] . in @xmath71 ,",
    "the data of @xmath158 are used to calculate and update the data of @xmath159 .",
    "the data of @xmath160 are used to calculate and update the data of @xmath161 , and so on .",
    "similarly , in @xmath110 , the data of @xmath162 are used to calculate and update the data of @xmath90 .",
    "the data of @xmath163 are used to calculate and update the data of @xmath164 , and so on . in @xmath111 ,",
    "the data of @xmath165 are used to calculate and update the data of @xmath90 .",
    "the data of @xmath166 are used to calculate and update the data of @xmath167 , and so on .",
    "it seems that the data accesses of @xmath71 and @xmath110 will slow down the performance since these accesses are not in so called  coalescing \" pattern .",
    "however , experimental results show that the computational times spending on calculating each dimensional component such as @xmath65 and @xmath126 in @xmath71 , @xmath110 , and @xmath111 are very close in our current arrangement ( see tables  [ table_1d_part ]  [ table_2d_part ] , and  [ table_3d_part ] in section 7 ) .",
    "this is due to the fact that gt200 and fermi gpu are more flexible to handle the data access that is not perfectly coalesced ( see section 4.1 ) .",
    "thus we did not further perform the coalescing to make these data accesses in optimal coalescing pattern .    ,",
    "@xmath110 and @xmath111,width=480 ]    after the whole pipeline of fig .",
    "[ fig : flowchart ] is completed , the mhd simulation results will be stored in gram and these results are readily to be further processed by the gpu for visualization or read back to the cpu for other usage . due to the data - parallel nature of the algorithm and its high arithmetic intensity",
    ", we can expect our gpu implementation will exhibit a relatively good performance on gpus .",
    "in this section , several numerical tests in one - dimensional ( 1d ) , two - dimensional ( 2d ) , and three - dimensional ( 3d ) for validation of _ gpu - mhd _ are given .",
    "two graphics cards nvidia geforce gtx 295 and gtx 480 were used .",
    "gtx 295 has two gpus inside but only one was used in these numerical tests .",
    "the results shown in this section are computed with single precision mode in _ gpu - mhd _ on gtx 295 .",
    "the difference between single precision and double precision computation results will be discussed in section  [ sec : accuracy ] .",
    "1d brio - wu shock tube problem  @xcite which is a mhd version of the sod problem  @xcite , consists of a shock tube with two initial equilibrium states as follows    left side @xmath168 @xmath169 @xmath170 @xmath171    right side @xmath172 @xmath173 @xmath174 @xmath175    constant value of @xmath176 was used and the problem was solved for @xmath177 $ ] with 512 grid points .",
    "numerical results are presented at @xmath178 in fig .",
    "[ fig : briowu ] and fig .",
    "[ fig : briowu2 ] , which include the density , the pressure , the energy , the @xmath55- and @xmath56-magnetic field components , and the @xmath13- , @xmath55- and @xmath56-velocity components .",
    "the results are in agreement with those obtained by brio and wu  @xcite and zachary _ et al . _",
    "@xcite .    .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ]    .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ]      the second 1d test is the mhd shock tube problem considered in  @xcite .",
    "left side @xmath168 @xmath179 @xmath180 @xmath181    right side @xmath172 @xmath182 @xmath183 @xmath184    constant value of @xmath185 was used and the problem was solved for @xmath177 $ ] with 512 grid points .",
    "numerical results are presented at @xmath186 in fig .",
    "[ fig : rj952a ] and fig .",
    "[ fig : rj952a2 ] , which include the density , the pressure , the energy , the @xmath55- and @xmath56-magnetic field components , and the @xmath13- , @xmath55- and @xmath56-velocity components .",
    "the results are in agreement with those obtained by  @xcite and  @xcite .    .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] hfill    .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ] .",
    "the result computed with 512 grid points is shown with circles and solid line shows reference high resolution result of 4096 grid points.,title=\"fig:\",width=240 ]        the first 2d test is orszag - tang problem  @xcite , which is used to study incompressible mhd turbulence . in our test ,",
    "the boundary conditions are periodic everywhere .",
    "the density @xmath4 , pressure @xmath187 , initial velocities @xmath188 , and magnetic field @xmath189 are given by @xmath190 @xmath191 @xmath192    the orszag - tang vertex test was performed in a two - dimensional periodic box with 512 @xmath193 512 grid points .",
    "the results of the density and gas pressure evolution of the orszag - tang problem at @xmath194 and @xmath195 are shown in fig .",
    "[ fig : ot ] , where the complex pattern of interacting waves is perfectly recovered .",
    "the results agree well with those in lee _",
    "et al . _",
    "@xcite .",
    "( left ) and @xmath195 ( right ) computed with 512 @xmath193 512 grid points.,title=\"fig:\",width=240 ]   ( left ) and @xmath195 ( right ) computed with 512 @xmath193 512 grid points.,title=\"fig:\",width=240 ]   ( left ) and @xmath195 ( right ) computed with 512 @xmath193 512 grid points.,title=\"fig:\",width=240 ]   ( left ) and @xmath195 ( right ) computed with 512 @xmath193 512 grid points.,title=\"fig:\",width=240 ]      the second 2d test is the mhd blast wave problem .",
    "the mhd spherical blast wave problem of zachary _ et al . _",
    "@xcite is initiated by an over pressured region in the center of the domain .",
    "the result is a strong outward moving spherical shock with rarified fluid inside the sphere .",
    "we followed the test suite  @xcite of athena  @xcite .",
    "the condition for 2d mhd blast wave problem is listed as follows  @xcite @xmath196 @xmath197 @xmath198 @xmath199 in fig .",
    "[ fig:2dblast ] , we present images of the density and gas pressure at @xmath200 computed with 512 @xmath193 512 grid points .",
    "the results are in excellent agreement with those presented in  @xcite .    , computed with 512 @xmath193 512 grid points.,title=\"fig:\",width=240 ] ,",
    "computed with 512 @xmath193 512 grid points.,title=\"fig:\",width=240 ]      the third 2d test is the mhd rotor problem .",
    "the problem was taken from  @xcite .",
    "it initiates a high density rotating disk with radius @xmath201 of fluid measured from the center point @xmath202 .",
    "the ambient fluid outside of the spherical region of @xmath203 has low density and @xmath204 , and the fluid between the high density disk fluid and ambient fluid ( @xmath205 where @xmath206 ) has linear density and angular speed profile with @xmath207 and @xmath208 where @xmath209 .",
    "two initial value sets of @xmath210 and @xmath211 provided in  @xcite and  @xcite were tested . the initial condition for 2d mhd rotor problem",
    "is listed as follows @xmath212 @xmath213 @xmath214 @xmath215 @xmath216 @xmath217 @xmath218 @xmath219 first rotor problem : @xmath220 second rotor problem : @xmath221 in fig .",
    "[ fig:2drotor ] , we present images of the density , gas pressure of the two rotor problems computed with 512 @xmath193 512 grid points .",
    "the results are in excellent agreement with those presented in  @xcite and  @xcite .    , results of the density ( bottom - left ) , gas pressure ( bottom - right ) of the second mhd rotor test at @xmath222 , both computed with 512 @xmath193 512 grid points.,title=\"fig:\",width=240 ] , results of the density ( bottom - left ) , gas pressure ( bottom - right ) of the second mhd rotor test at @xmath222 , both computed with 512 @xmath193 512 grid points.,title=\"fig:\",width=240 ] , results of the density ( bottom - left ) , gas pressure ( bottom - right ) of the second mhd rotor test at @xmath222 , both computed with 512 @xmath193 512 grid points.,title=\"fig:\",width=240 ] , results of the density ( bottom - left ) , gas pressure ( bottom - right ) of the second mhd rotor test at @xmath222 , both computed with 512 @xmath193 512 grid points.,title=\"fig:\",width=240 ]      the 3d version of mhd spherical blast wave problem was also tested .",
    "the condition is listed as follows  @xcite @xmath223 @xmath224 @xmath225 @xmath226    fig .",
    "[ fig:3dblast01 ] and fig .",
    "[ fig:3dblast02 ] show the results of 3d blast wave problem , which include the density , gas pressure , and magnetic pressure at @xmath227 and @xmath186 sliced along @xmath13-@xmath55 plane at @xmath56 = 0.5 .",
    "the test was computed with 128 @xmath193 128 @xmath193 128 grid points .",
    "due to the scarcity of published 3d test results , we do not make direct contact with results presented in the literature here .",
    "considering only the @xmath99 and @xmath100 , the memory requirement of @xmath228 mhd problem is about 512 mb gram for single precision and 1024 mb gram for double precision , respectively . if the storage of intermediate results such as @xmath112 , @xmath113 , @xmath114 and @xmath229 etc .",
    "( see section  [ sec : sweeping ] ) are considered , the amount of memory requirement will be about 2.25 gb ( single precision ) .",
    "as we mentioned in section  [ sec : sweeping ] , not all the capacity of gram can be used to store data arbitrarily .",
    "as we said in the beginning of this section , there are actually two gpus inside the gtx 295 and the 1.75 gb gram is the total amount of the gram shared by two gpus , so that only less than @xmath230 gb gram can be used . as a result , the test of 3d problem with @xmath228 resolution are not able to be provided on a graphics card .     sliced along @xmath13-@xmath55 plane at @xmath56 = 0.5 and computed with 128 @xmath193 128 @xmath193 128 grid points.,title=\"fig:\",width=240 ]   sliced along @xmath13-@xmath55 plane at @xmath56 = 0.5 and computed with 128 @xmath193 128 @xmath193 128 grid points.,title=\"fig:\",width=240 ]   sliced along @xmath13-@xmath55 plane at @xmath56 = 0.5 and computed with 128 @xmath193 128 @xmath193 128 grid points.,title=\"fig:\",width=240 ]     sliced along @xmath13-@xmath55 plane at @xmath56 = 0.5 and computed with 128 @xmath193 128 @xmath193 128 grid points.,title=\"fig:\",width=240 ]   sliced along @xmath13-@xmath55 plane at @xmath56 = 0.5 and computed with 128 @xmath193 128 @xmath193 128 grid points.,title=\"fig:\",width=240 ]   sliced along @xmath13-@xmath55 plane at @xmath56 = 0.5 and computed with 128 @xmath193 128 @xmath193 128 grid points.,title=\"fig:\",width=240 ]",
    "in mhd simulations , accuracy is always to be considered since the error may increase fast and crash the simulation if low precision is used for computation .",
    "scientific computations such as mhd simulation mostly use double precision to reduce errors . in this section , the results generated by _",
    "gpu - mhd _ using single precision and double precision modes are shown and compared .",
    "the difference between the results of double precision and single precision computation of the @xmath231 one - dimensional brio - wu shock tube problem is shown in fig .  [",
    "fig : briowu_diff ] .",
    "two curves are almost the same but there are actually some differences with the amount of @xmath232 :     ( top ) , @xmath233 ( middle ) and @xmath234 ( bottom ) of 1d brio - wu shock tube problem at @xmath178 with 512 grid points , title=\"fig:\",width=240 ]   ( top ) , @xmath233 ( middle ) and @xmath234 ( bottom ) of 1d brio - wu shock tube problem at @xmath178 with 512 grid points , title=\"fig:\",width=240 ]   ( top ) , @xmath233 ( middle ) and @xmath234 ( bottom ) of 1d brio - wu shock tube problem at @xmath178 with 512 grid points , title=\"fig:\",width=240 ]   ( top ) , @xmath233 ( middle ) and @xmath234 ( bottom ) of 1d brio - wu shock tube problem at @xmath178 with 512 grid points , title=\"fig:\",width=240 ]   ( top ) , @xmath233 ( middle ) and @xmath234 ( bottom ) of 1d brio - wu shock tube problem at @xmath178 with 512 grid points , title=\"fig:\",width=240 ]   ( top ) , @xmath233 ( middle ) and @xmath234 ( bottom ) of 1d brio - wu shock tube problem at @xmath178 with 512 grid points , title=\"fig:\",width=240 ]    in 2d cases , the absolute difference between the results of double precision and single precision computation of mhd rotor test ( @xmath235 ) and orszag - tang vortex test ( @xmath236 and @xmath195 ) are shown in fig .",
    "[ fig : rotor_diff ] and fig  [ fig : ot_diff ] , respectively .",
    "the double precision computation results of both tests are also shown in the left - hand side of these figures .",
    "for the mhd rotor test , even the resulting image ( left in fig .",
    "[ fig : rotor_diff ] ) looks similar to the single precision resulting image ( top - left of fig .",
    "[ fig:2drotor ] ) , the high differences at the dense region can be found .",
    "experimental result shows that the maximum @xmath237 is larger than @xmath238 .",
    "( left ) and @xmath239 ( right ) of mhd rotor problem at @xmath235 with @xmath240 grid points , title=\"fig:\",width=240 ]   ( left ) and @xmath239 ( right ) of mhd rotor problem at @xmath235 with @xmath240 grid points , title=\"fig:\",width=240 ]    fig .",
    "[ fig : ot_diff ] shows the absolute difference between the results of double precision and single precision computation of orszag - tang test at @xmath236 and @xmath195 . as the simulation time increases , the maximum @xmath237 increases from about @xmath241 to @xmath242 .",
    "( left ) and @xmath239 ( right ) of orszag - tang problem at @xmath236 ( top ) and @xmath195 ( bottom ) with @xmath240 grid points , title=\"fig:\",width=240 ]   ( left ) and @xmath239 ( right ) of orszag - tang problem at @xmath236 ( top ) and @xmath195 ( bottom ) with @xmath240 grid points , title=\"fig:\",width=240 ]   ( left ) and @xmath239 ( right ) of orszag - tang problem at @xmath236 ( top ) and @xmath195 ( bottom ) with @xmath240 grid points , title=\"fig:\",width=240 ]   ( left ) and @xmath239 ( right ) of orszag - tang problem at @xmath236 ( top ) and @xmath195 ( bottom ) with @xmath240 grid points , title=\"fig:\",width=240 ]    fig .",
    "[ fig:3dblast1_diff ] and fig .",
    "[ fig:3dblast2_diff ] show the resulting images of the simulation using double precision and the contours of the absolute differences between the results of double precision and single precision computation of 3d blast wave test with @xmath243 grid points at @xmath227 and @xmath186 . as it is a high dimension computation in low resolution ,",
    "the differences between them are clear .",
    "the number of grid points having higher difference value increases , and the @xmath237 is still less than @xmath244 .",
    "small difference value makes the double precision resulting images ( fig .",
    "[ fig:3dblast1_diff ] and fig .  [ fig:3dblast2_diff ] ) looked similar to the single precision resulting images ( fig .",
    "[ fig:3dblast01 ] and fig .",
    "[ fig:3dblast02 ] ) .",
    "( top - left ) , @xmath245 ( bottom - left ) and @xmath246 ( top - right ) , @xmath239 ( bottom - right ) of 3d blast wave problem at @xmath227 with @xmath243 grid points , title=\"fig:\",width=240 ]   ( top - left ) , @xmath245 ( bottom - left ) and @xmath246 ( top - right ) , @xmath239 ( bottom - right ) of 3d blast wave problem at @xmath227 with @xmath243 grid points , title=\"fig:\",width=240 ]   ( top - left ) , @xmath245 ( bottom - left ) and @xmath246 ( top - right ) , @xmath239 ( bottom - right ) of 3d blast wave problem at @xmath227 with @xmath243 grid points , title=\"fig:\",width=240 ]   ( top - left ) , @xmath245 ( bottom - left ) and @xmath246 ( top - right ) , @xmath239 ( bottom - right ) of 3d blast wave problem at @xmath227 with @xmath243 grid points , title=\"fig:\",width=240 ]     ( top - left ) , @xmath245 ( bottom - left ) and @xmath246 ( top - right ) , @xmath239 ( bottom - right ) of 3d blast wave problem at @xmath186 with @xmath243 grid points , title=\"fig:\",width=240 ]   ( top - left ) , @xmath245 ( bottom - left ) and @xmath246 ( top - right ) , @xmath239 ( bottom - right ) of 3d blast wave problem at @xmath186 with @xmath243 grid points , title=\"fig:\",width=259 ]   ( top - left ) , @xmath245 ( bottom - left ) and @xmath246 ( top - right ) , @xmath239 ( bottom - right ) of 3d blast wave problem at @xmath186 with @xmath243 grid points , title=\"fig:\",width=240 ]   ( top - left ) , @xmath245 ( bottom - left ) and @xmath246 ( top - right ) , @xmath239 ( bottom - right ) of 3d blast wave problem at @xmath186 with @xmath243 grid points , title=\"fig:\",width=259 ]    an important point can be realized that not only the grid points at the high density region has high difference value , but also the number of grid points having high difference value and the amount of the difference values are increasing along with the increase of the simulation time .",
    "higher dimension is another factor to introduce noticeable differences between the computation results with different precisions because higher dimension means a grid point has more neighbors and more neighbors need more computation steps in one time step . as a result",
    "the differences become more obvious . therefore , for a long - term simulation , double precision computation is a must .",
    "the original fortran code  @xcite is a second - order accurate high - resolution tvd mhd code .",
    "theoretically , we consider that _ gpu - mhd _ is sufficient to capture forward and reverse shocks as well as any other discontinuities such as contact discontinuities which are important in space physics . as _",
    "gpu - mhd _ is a dimensional - splitting based code , there are two drawbacks : ( i ) the code is unable to evolve the normal ( to the sweep direction ) magnetic field during each sweep direction  @xcite , and ( ii ) splitting errors will generally be introduced due to the linearized jacobian flux matrices do not commute in most of the nonlinear multidimensional problems  @xcite .",
    "the performance measurements of the gpu and cpu implementations as well as the computation using double precision and single precision are carried out in this section .",
    "different numbers of grid points and different dimensions were used in the performance tests .",
    "we run both _",
    "gpu - mhd _ and pen _ et al . _",
    "s fortran / cpu mhd code  @xcite to perform the simulations on a pc with intel core i7 965 3.20 ghz cpu , 6 g main memory , running microsoft windows xp 64-bit professional .",
    "two graphics cards were tested : nvidia geforce gtx 295 with 1.75 g video memory and gtx 480 ( fermi ) with 1.5 g video memory . the fortran compiler and gpu development toolkit we used are g95 stable version 0.92 and nvidia cuda 3.2 , respectively .",
    "_ gpu - mhd _ was designed for three - dimensional problems , thus the dimensions are expressed in three - dimensional form in all figures . for 1d test , 1d brio - wu shock tube problem ( see section  [ sub : brio - wu ] ) was used . for 2d test",
    ", 2d orszag - tang problem ( see section  [ sub : o - t ] ) was used was used . for 3d test ,",
    "3d blast wave problem ( see section  [ sub:3dblast ] ) was used .",
    "[ fig : speedup1d ] reports the comparison of _ gpu - mhd _ and the fortran / cpu code of 1d test with different numbers of grid points in single and double precisions . in single precision mode",
    ", basically there is only about 10 times speedup ( @xmath247 case ) since the number of grid points is small . and it should be realized that the amount of speedup is increased as long as the resolution is increased but dropped when the resolution reaches @xmath248 .",
    "it is because the `` max threads per block '' of gtx 295 is 512 , all the computations are handled within one block and a very high processing speed can be archived .",
    "on gtx 480 , there is about 80 times speedup ( @xmath247 case ) and the amount of speedup is increased linearly due to the higher floating point capability ( 512 mad ops / clock ) . in double precision mode ,",
    "around 10 times and 60 times speedup ( @xmath247 case ) is achieved on gtx 295 and gtx 480 , respectively .",
    "table  [ table_1d_precision ] gives the comparison of _ gpu - mhd _ using single precision and double precision of 1d test with different numbers of grid points .",
    "on gtx 295 , a similar speed drop happened both in single and double precision modes , but it occurred in different resolutions : @xmath248 in single precision and @xmath249 in double precision .",
    "this is not strange and not difficult to understand since the double precision has double size of data to be handled by the processors .",
    "except the special case of @xmath248 resolution , the processing speed in both modes are very closed .",
    "on gtx 480 , the performance between single and double precision is quite close .",
    ".the performance results of 1d test between single precision and double precision of _ gpu - mhd _ at different resolutions [ cols=\"^,^,^,^,^ \" , ]",
    "there is a need to visualize the mhd simulation data , for examples , daum  @xcite developed a toolbox called _ visan mhd _ in matlab for mhd simulation data visualization and analysis . with the help of gpus , stantchev _ et al . _",
    "@xcite used gpus for computation and visualization of plasma turbulence . in _ gpu - mhd",
    "_ , using the parallel computation power of gpus and cuda , the simulation results of one time step can be computed in dozens or hundreds milliseconds .",
    "according to the efficiency of _ gpu - mhd _ , near real - time visualization is able to be provided for 1d and 2d problems .",
    "the motion or attributes of the magnetic fluid can be computed and rendered on the fly .",
    "so the changes of the magnetic fluid during the simulation can be observed in real - time .    by adding the real - time visualization , the flow of _ gpu - mhd _ , fig .",
    "[ fig : flowchart ] is extended as fig .",
    "[ fig : flowchart_v ] :        _ gpu - mhd _ provides different visualization methods for one - dimensional , two - dimensional and three - dimensional problems .    to visualize one - dimensional problems for each time step",
    ", the simulation results are copied to the cuda global memory that mapped to the vertex buffer object ( vbo )  @xcite .",
    "for all grid points , one grid point is mapped to one vertex .",
    "the position of each grid point is mapped as the @xmath13-position of the vertex and the selected physical value ( @xmath4 , @xmath187 , etc . )",
    "is mapped as the @xmath55-position of the vertex .",
    "then a curve of these vertices is drawn . since the vbo is mapped to cuda global memory and simulation results are stored in gram , the copying and mapping operations are fast .",
    "experimental result shows that _ gpu - mhd _ with real - time visualization can achieve 60 frame per second ( fps ) in single precision mode and 30 fps in double precision mode on gtx 295 .",
    "on gtx 480 , around 60 fps in both single and double precisions is achieved .",
    "[ fig:1dvis ] shows two example images of 1d visualizations using _ gpu - mhd_.    ) of brio - wu shock tube problem with @xmath248 grid points using _ gpu - mhd_.,title=\"fig:\",width=240 ] ) of brio - wu shock tube problem with @xmath248 grid points using _",
    "gpu - mhd_.,title=\"fig:\",width=240 ]    the operational flow of visualization of 2d problems is similar to that in 1d visualization .",
    "however , instead of vertex buffer object ( vbo ) , pixel buffer object(pbo )  @xcite is used . for each time step ,",
    "the simulation results are copied to the cuda global memory that are then mapped to the pbo .",
    "for all grid points , one grid point is mapped to one pixel .",
    "the @xmath13 and @xmath55 position of each grid point are mapped as the corresponding @xmath13-position and the @xmath55-position of the vertex and the selected physical value ( @xmath4 , @xmath187 , etc . )",
    "is mapped as the color of the pixel to form a color image . to render this color image ,",
    "a transfer function is set to map the physical value to the color of the pixel and then the resulting image is drawn .",
    "similar to vbo , pbo is also mapped to cuda global memory and the simulation results are stored in gram , so the copying and mapping operations are also fast and do not affect too much to the performance . although the number of grid points in 2d problem is much larger than those in the one - dimension problem , the fps still reaches 10 in single precision mode and 6 in double precision mode on gtx 295 when the number of grid points is @xmath240 , still giving acceptable performance to the user .",
    "on gtx 480 , 22 fps in single precision and 17 fps in double precision are achieved and thus interactive rates are available .",
    "[ fig:2dvis ] shows two example images of 2d visualizations using _ gpu - mhd_.    ) of orszag - tang vortex problem with @xmath240 grid points using _ gpu - mhd_.,title=\"fig:\",width=240 ] ) of orszag - tang vortex problem with @xmath240 grid points using _ gpu - mhd_.,title=\"fig:\",width=240 ]    however , visualization of 3d problem is different to 1d and 2d problems .",
    "gpu - based volume visualization method  @xcite and texture memory ( or video memory ) are used .",
    "unfortunately , the current version ( version 2.3 ) of cuda does not provide the feature to copy the data from the cuda global memory to texture memory directly , even both of them are in gram . on the other hand ,",
    "texture memory is readable but is not rewritable in cuda .",
    "so the simulation results have to be copied to the main memory first , and then be copied to texture memory .",
    "in addition , the number of grid points is usually large compared to 2d problems and volume visualization techniques are somewhat time - consuming . as a result , on gtx 295 ,",
    "_ gpu - mhd _ only gets 2 fps in single precision mode and 1 fps in double precision mode when the number of grid points is @xmath243 , and it is far from real - time .",
    "nevertheless , we still get 10 fps ( single precision mode ) and 6 fps ( double precision mode ) for performing the simulation of problems with resolution of @xmath250 and about 20 fps ( single and double precision modes ) for problems with resolution of @xmath251 . on gtx 480",
    ", we can get 60 fps for both single and double precision for @xmath251 grid points , 20 fps ( single ) and 16 fps ( double ) for @xmath250 grid points , and 6.1 fps ( single ) and 3.6 fps ( double ) for @xmath243 grid points .",
    "[ fig:3dvis ] shows two example images of 2d visualizations using _ gpu - mhd_.    ) of 3d blast wave problem with @xmath243 grid points using _",
    "gpu - mhd_.,title=\"fig:\",width=240 ] ) of 3d blast wave problem with @xmath243 grid points using _ gpu - mhd_.,title=\"fig:\",width=240 ]",
    "in this paper we present , to the author s knowledge , the first implementation of mhd simulations entirely on gpus with cuda , named _ gpu - mhd _ , to accelerate the simulation process .",
    "the aim of this paper is to present a gpu implementation in detail , demonstrating how a tvd based mhd simulations can be implemented efficiently for nvidia gpus with cuda . a series of numerical tests have been performed to validate the correctness of our code .",
    "accuracy evaluation by comparing single and double precision computation results is also given , indicating that double precision support on gpus is a must for long - term mhd simulation .",
    "performance measurements of both single and double precision modes of _ gpu - mhd _ are conducted on gt200 architecture ( gtx 295 ) and fermi architecture ( gtx 480 ) .",
    "these measurements show that our gpu - based implementation achieves between one and two orders of magnitude depending on the used graphics card , problem size , and precision when comparing to the original serial cpu mhd implementation . in order to provide the user better understanding of the problems being investigated during the simulation process",
    ", we have extended _ gpu - mhd _ to support visualization of the simulation results . with _",
    "gpu - mhd _ , the whole mhd simulation and visualization process can be performed entirely on gpus .",
    "there are two directions in our future work , firstly , we are going to extend _ gpu - mhd _ for multiple gpus and gpu cluster  @xcite to fully exploit the power of gpus .",
    "secondly , we will investigate implementing other recent high - order godunov mhd algorithms such as  @xcite and  @xcite on gpus .",
    "these gpu - based algorithms will be served as the base of our gpu framework for simulating large - scale mhd problems in space weather modeling .",
    "this work has been supported by the science and technology development fund of macao sar ( 03/2008/a1 ) and the national high - technology research and development program of china ( 2009aa122205 ) .",
    "xueshang feng is supported by the national natural science foundation of china ( 40874091 and 40890162 ) .",
    "the authors would like to thank dr .",
    "ue - li pen and bijia pang at the canadian institute for theoretical astrophysics , university of toronto for providing the fortran mhd code .",
    "thanks to dr .",
    "yuet ming lam for his suggestions on the revision of the paper .",
    "special thanks to anonymous reviewers for their constructive comments on the paper .",
    "d. s. balsara , d. s. spicer , a staggered mesh algorithm using high order godunov fluxes to ensure solenoidal magnetic fields in magnetohydrodynamic simulations , _ j. comput",
    ". phys . _ * 149 * ( 1999 ) 270 - 292 .",
    "r. g. belleman , j. bdorf , s. f. portegies zwart , high performance direct gravitational n - body simulations on graphics processing units ii : an implementation in cuda , _ new astronomy _ * 13 * ( 2008 ) 103 - 112 .",
    "s. che , m. boyer , j. meng , d. tarjan , j. w. sheaffer , k. skadron , a performance study of general - purpose applications on graphics processors using cuda , _ j. parallel distrib .",
    "* 68 * ( 2008 ) 1370 - 1380 .",
    "a. ciardi , s. lebedev , a. frank , e. blackman , d. ampleford , c. jennings , j. chittenden , t. lery , s. bland , s. bott , g. hall , j. rapley , f. vidal , a. marocchino , 3d mhd simulations of laboratory plasma jets , _ astrophysics and space science _ * 307 * ( 2007 ) 17 - 22 .",
    "m. dryer , space weather simulations in 3d mhd from the sun to earth and beyond to 100 au : a modeler s perspective of the present state of the art ( invited review ) , _ asia j. of physics _",
    "* 16 * ( 2007 ) 97 - 121 .",
    "j. c. hayes , m. l. norman , r. a. fiedler , j. o. bordner , p. s. li , s. e. clark , a. ud - doula , m .-",
    "m. low , simulating radiating and magnetized flows in multiple dimensions with zeus - mp , _ astrophys .",
    "j. supp . _",
    "* 165 * ( 2006 ) 188 - 228 .",
    "d. lee , a. e. deane , a parallel unsplit staggered mesh algorithm for magnetohydrodynamics , in _",
    "parallel computational fluid dynamics  theory and applications _",
    ", edited by a. deane _ et al .",
    "_ , elsevier ( 2006 ) 243 - 250 .",
    "p. mininni , e. lee , a. norton , j. clyne , flow visualization and field line advection in computational fluid dynamics : application to magnetic fields and turbulent flows , _ new j. phys .",
    "_ * 10 * ( 2008 ) 125007 .",
    "l. seiler , d. carmean , e. sprangle , t. forsyth , m. abrash , p. dubey , s. junkins , a. lake , j. sugerman , r. cavin , r. espasa , e. grochowski , t. juan , p. hanrahan , larrabee : a many - core x86 architecture for visual computing , _ acm trans .",
    "_ * 27 * ( 2008 ) article 18 .",
    "g. stantchev , d. juba , w. dorland , a. varshney , using graphics processors for high - performance computation and visualization of plasma turbulence , _ computing in science and engineering _ * 11 * ( 2009 ) 52 - 59 .",
    "g. tth , d. odstr@xmath252il , comparison of some flux corrected transport and total variation diminishing numerical schemes for hydrodynamic and magnetohydrodynamic problems , _",
    "j. comput .",
    "* 128 * ( 1996 ) 82 - 100 ."
  ],
  "abstract_text": [
    "<S> magnetohydrodynamic ( mhd ) simulations based on the ideal mhd equations have become a powerful tool for modeling phenomena in a wide range of applications including laboratory , astrophysical , and space plasmas . in general , </S>",
    "<S> high - resolution methods for solving the ideal mhd equations are computationally expensive and beowulf clusters or even supercomputers are often used to run the codes that implemented these methods . with the advent of the </S>",
    "<S> compute unified device architecture ( cuda ) , modern graphics processing units ( gpus ) provide an alternative approach to parallel computing for scientific simulations . in this paper </S>",
    "<S> we present , to the author s knowledge , the first implementation of mhd simulations entirely on gpus with cuda , named _ gpu - mhd _ , to accelerate the simulation process . _ </S>",
    "<S> gpu - mhd _ supports both single and double precision computation . </S>",
    "<S> a series of numerical tests have been performed to validate the correctness of our code . </S>",
    "<S> accuracy evaluation by comparing single and double precision computation results is also given . </S>",
    "<S> performance measurements of both single and double precision are conducted on both the nvidia geforce gtx 295 ( gt200 architecture ) and gtx 480 ( fermi architecture ) graphics cards . </S>",
    "<S> these measurements show that our gpu - based implementation achieves between one and two orders of magnitude depending on the used graphics card , problem size , and precision when comparing to the original serial cpu mhd implementation . </S>",
    "<S> in addition , we extend _ gpu - mhd _ to support the visualization of the simulation results and thus the whole mhd simulation and visualization process can be performed entirely on gpus .    </S>",
    "<S> mhd simulations , gpus , cuda , parallel computing </S>"
  ]
}