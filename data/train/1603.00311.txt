{
  "article_text": [
    "after the invention of the monte carlo ( mc ) paradigm by s.  ulam in the late 1940s , it has become extremely popular in numerous application areas such as physics , biology , economics , social sciences , and other areas .",
    "as far as mathematics is concerned , monte carlo methods showed themselves exceptionally efficient in the simulation of various probability distributions , numerical integration , estimation of the mean values of the parameters , etc . @xcite .",
    "the salient feature of this approach to solution of various problems of this sort is that `` often , '' it is dimension - free in the sense that , given  @xmath0 samples , the accuracy of the result does not depend on the dimension of the problem .    on the other hand ,",
    "applications of the mc paradigm in the area of optimization are not that triumphant . in this regard ,",
    "problems of global optimization deserve special attention .",
    "as explained in  @xcite ( see beginning of chapter 1.2 ) , `` _ _ in global optimization , randomness can appear in several ways .",
    "the main three are : ( i )  the evaluations of the objective function are corrupted by random errors ; ( ii )  the points @xmath1 are chosen on the base of random rules , and ( iii )  the assumptions about the objective function are probabilistic .",
    "_ _ '' pertinent to the exposition of this paper is only case ( ii ) .",
    "monte carlo is the simplest , brute force example of randomness - based methods ( in @xcite it is referred to as `` pure random search '' ) . with this method ,",
    "one samples points uniformly in the feasible domain , computes the values of the objective function , and picks the record value as the output .    of course , there are dozens of more sophisticated stochastic methods such as simulated annealing , genetic algorithms , evolutionary algorithms , etc . ;",
    "e.g. , see @xcite for an incomplete list of relevant references .",
    "however most of these methods are heuristic in nature ; often , they lack rigorous justification , and the computational efficiency is questionable .",
    "moreover , there exist pessimistic results on `` insolvability of global optimization problems . ''",
    "this phenomenon has first been observed as early as in the monograph  @xcite by a.  nemirovskii and d.  yudin , both in the deterministic and stochastic optimization setups ( see theorem , section 1.6 in @xcite ) . specifically , the authors of  @xcite considered the minimax approach to the minimization of the class of lipschitz functions and proved that , no matter what is the optimization method , it is possible to construct a problem which will require exponential ( in the dimension ) number of function evaluations . the `` same '' number of samples is required for the simplest mc method .",
    "similar results can be found in  @xcite , theorem 1.1.2 , where the construction of `` bad '' problems is exhibited .",
    "below we present another example of such problems ( with very simple objective functions , close to linear ones ) which are very hard to optimize . concluding this brief survey",
    ", we see that any advanced method of global optimization can not outperform monte carlo when optimizing `` bad '' functions .",
    "this explains our interest in the mc approach as applied to the optimization setup . in spite of the pessimistic results above",
    ", there might be a belief that , if monte carlo is applied to a `` good '' optimization problem ( e.g. , a convex one ) , the results would not be so disastrous .",
    "the goal of the present paper is to clarify the situation .",
    "we examine the `` best '' optimization problems ( the minimization of a linear function on a ball or on a cube ) and estimate the accuracy of the monte carlo method .",
    "unfortunately , the dependence on the dimension remains exponential , and practical solution of these simplest problems via such an approach is impossible for high dimensions .",
    "the second goal of the paper is to exhibit the same situation with multiobjective optimization  @xcite .",
    "we treat methods for the pareto set discovery via the monte carlo technique and estimate their accuracy , which happens to be poor for large dimensions .",
    "these results are instructive for multiobjective optimization , because there exist many methods based on a similar approach ( with regular grids instead of random grids ) , see @xcite .",
    "an intuitive explanation of the effects under consideration can rely on the geometric nature of multidimensional spaces . numerous facts of this sort can be found in chapter  2 of the book  @xcite , which is available in the internet .",
    "the titles of sections in  @xcite are very impressive : `` the volume is near the equator , '' `` the volume is in a narrow annulus , '' `` the surface area is near the equator . ''",
    "some of the results in the present paper clarify these statements by providing rigorous closed - form estimates for the minimum number of random points in the ball - shaped sets required to assess , with a given probability , the optimum of a linear function with given accuracy .",
    "these estimates are based on our previous results on the properties of the uniform distribution over a ball ,  @xcite ( see section 2.2 ) .",
    "as far as the geometry of many - dimensional spaces is concerned , the highly advanced monograph  @xcite is worth mentioning ; it deals with the geometrical structure of finite dimensional normed spaces , as the dimension grows to infinity , and presents numerous deep mathematical results in the area .",
    "the rest of the paper is organized as follows . in section  [",
    "s : statement ] , we propose a motivating example , formulate the two optimization problems , scalar and multiobjective , considered within the monte carlo setup , and present two known theorems on the uniform distribution over the @xmath2-norm ball . these theorems will be used in section  [ s : ball ] to derive new results related to the two optimization problems of interest for the case of the @xmath2-norm ball .",
    "section  [ s : box ] deals with the scalar optimization problem for the case where  @xmath3 is a box ; use of various deterministic grids are also discussed .",
    "brief concluding remarks are given in the last section .",
    "a preliminary version of this paper is  @xcite .",
    "several refinements are performed in the current text ; first , we changed the overall structure of the exposition ; then , we provide a new result on the probability of the empirical maximum of a linear function on a ball ( section  [ ssec : scalar ] ) , next , we add a result on the expected value ( end of section  [ ssec : multiobj ] ) , present closed - form results for the @xmath4- and @xmath5-norm balls ( section  [ ssec : boxmc ] ) , discuss deterministic grids over a box ( section  [ ssec : lptau ] ) , and accompany these new results with numerical illustrations .",
    "finally , the introduction section and the bibliography list are considerably modified and extended and various typos and inaccuracies are corrected .",
    "in this section , we propose a motivation for the research performed in this paper , formulate the problems of interest , and present two known facts which form the basis for deriving the new results in section  [ s : ball ] .      to motivate our interest in the subject of this paper , we present a simple example showing failure of stochastic global optimization methods in high - dimensional spaces .",
    "this example is constructed along the lines suggested in  @xcite ( also , see  @xcite , theorem 1.1.2 ) and is closely related to one of the central problems discussed below , the minimization of a linear function over a ball in @xmath6 .",
    "consider an unknown vector @xmath7 , @xmath8 , and the function @xmath9 to be minimized over the euclidean ball @xmath10 of radius @xmath11 centered at the origin . obviously , the function has one local minimum @xmath12 , with the function value @xmath13 , and one global minimum @xmath14 , with the function value @xmath15 .",
    "the objective function is lipschitz with lipschitz constant equal to @xmath16 , and @xmath17 .",
    "any standard ( not problem - oriented ) version of stochastic global search ( such as multistart , simulated annealing , etc . ) will miss the domain of attraction of the global minimum with probability @xmath18 , where @xmath19 is the volume of the ball @xmath3 , and @xmath20 is the volume of the set @xmath21 .",
    "in other words , the probability of success is equal to @xmath22 where @xmath23 is the regularized incomplete beta function with parameters  @xmath24 and  @xmath25 ( for use of this function , also see theorem  [ th : ball_maxlin ] in section  [ s : ball ] ) , and @xmath26 is the height of the spherical cap  @xmath27 ; in this example , @xmath28 .",
    "this probability quickly goes to zero as the dimension of the problem grows ; say , for @xmath29 it is of the order of @xmath30 .",
    "hence , any `` advanced '' method of global optimization will find the minimum with relative error not less @xmath31 ; moreover , such methods are clearly seen to be no better than a straightforward monte carlo sampling .",
    "the same is true if our goal is to estimate the minimal value of the function @xmath32 ( not the minimum point @xmath33 ) .",
    "various methods based on ordered statistics of sample values ( see section 2.3 in @xcite ) fail to reach the set @xmath27 with high probability , so that the prediction will be close to @xmath13 instead of @xmath15 .",
    "these observations motivate our interest in the analysis of the performance of the mc schemes in optimization .",
    "let  @xmath34 denote a unit ball in one or another norm and let @xmath35 be a multisample of size  @xmath0 from the uniform distribution @xmath36 .",
    "we are targeted at solving the problems of the following sort .",
    ".1 in * i. scalar optimization : * given the scalar - valued linear function @xmath37 defined on the unit ball  @xmath34 , estimate its maximum value from the multisample .    more specifically ,",
    "let @xmath38 be the true maximum of @xmath39 on  @xmath3 and let @xmath40 be the empirical maximum ; we say that @xmath41 approximates  @xmath38 _ with accuracy at least  @xmath42 _ if @xmath43    then the problem is : _ given a probability level  @xmath44 and accuracy  @xmath45 , determine the minimal length  @xmath46 of the multisample such that , with probability at least  @xmath47 , the accuracy of approximation is at least  @xmath42 ( i.e. , with high probability , the empirical maximum nicely evaluates the true one ) .",
    "_    these problems are the subject of discussion in sections [ ssec : scalar ] and  [ ssec : boxmc ] .    , 1 in * ii .",
    "multiobjective optimization : * consider now @xmath48 scalar functions @xmath49 , and the image of  @xmath3 under these mappings .",
    "the problem is to `` characterize '' the boundary of the image set @xmath50 via the multisample @xmath51 from  @xmath3 .    in rough terms , the problem is : _ determine the minimal sample size  @xmath46 which guarantees , with high probability , that the image of at least one sample fall close to the boundary of  @xmath52_.    for the case where  @xmath3 is the euclidean ball , the mappings @xmath53 are linear , and @xmath54 , this problem is discussed in section  [ ssec : multiobj ] ; various statistics ( such as the cumulative distribution function , mathematical expectation , mode ) of a specific random variable associated with image points are evaluated .",
    "the results presented in section  [ s : ball ] below are based on the following two facts established in  @xcite ; they relate to the probability distribution of a specific linear or quadratic function of the random vector uniformly distributed on the euclidean ball .",
    "* fact 1 * @xcite .",
    "_ let the random vector @xmath55 be uniformly distributed on the unit euclidean ball  @xmath34 .",
    "assume that a matrix  @xmath56 has rank @xmath57 .",
    "then the random variable @xmath58 has the beta distribution  @xmath59 with probability density function @xmath60 where  @xmath61 is the euler gamma function . _",
    ".1 in alternatively , the numerical coefficient in   writes @xmath62 , where @xmath63 is the beta function .",
    ".1 in the second fact is an asymptotic counterpart of fact  1 .",
    ".1 in * fact 2 * @xcite .",
    "_ assume that for every @xmath64 , the matrix @xmath65 has rank  @xmath66 , and @xmath67 is a random vector uniformly distributed over the unit ball  @xmath3 in  @xmath68 .",
    "then , as @xmath69 , the random vector @xmath70 tends in distribution to the standard gaussian vector @xmath71 , where @xmath72 is the identity @xmath73-matrix . _    note that for  @xmath74 fixed , we have @xmath75 i.e. , facts  2 and  1 characterize the asymptotic distribution of the vector  @xmath76 and exact distribution of its squared norm ( normalized by the dimension ) .",
    "in this section we analyse the two optimization settings formulated in section  [ two_settings ] for @xmath3 being the @xmath74-dimensional unit @xmath2-ball .",
    "we consider the scalar case   and discuss first a qualitative result that follows immediately from fact  1 .    without loss of generality ,",
    "let @xmath77 , so that the function  @xmath78 takes its values on the segment @xmath79 $ ] , and the true maximum of @xmath39 on  @xmath3 is equal to  @xmath16 ( respectively ,  @xmath80 for the minimum ) and is attained with @xmath81 ( respectively , @xmath82 ) .",
    "let us compose the random variable @xmath83 which is the squared first component  @xmath84 of  @xmath85 .",
    "by fact  1 with @xmath86 ( i.e. , @xmath87 ) , for the probability density function ( pdf ) of @xmath88 we have @xmath89    straightforward analysis of this function shows that , as dimension grows , the mass of the distribution tends to concentrate closer the origin , meaning that the random variable ( r.v . )",
    "@xmath88 is likely to take values which are far from the maximum , equal to unity . to illustrate , fig .",
    "[ fig : pdf_rho ] depicts the plot of the pdf   for @xmath90 .",
    ".,width=264 ]    we next formulate the following rigorous result .    [",
    "th : ball_maxlin ] let @xmath85 be a random vector uniformly distributed over the unit euclidean ball  @xmath3 and let @xmath78 , @xmath91 . given @xmath92 and @xmath93 , the minimal sample size  @xmath46 that guarantees , with probability at least  @xmath47 , for the empirical maximum of  @xmath39 to be at least a  @xmath42-accurate estimate of the true maximum ,",
    "is given by @xmath94}\\,,\\ ] ] where @xmath23 is the regularized incomplete beta function with parameters  @xmath24 and  @xmath25 .",
    ".1 in clearly , a correct notation should be @xmath95 , i.e. , rounding toward the next integer ; we omit it , but it is implied everywhere in the sequel .",
    ".1 in _ proof _ we specify sample size  @xmath0 , and let @xmath51 be a multisample from the uniform distribution on  @xmath3 ; also introduce the random variable @xmath96 the empirical maximum of the function @xmath97 , @xmath98 , from this multisample .",
    "we now estimate the probability @xmath99 .",
    "by fact  1 , the pdf of the r.v .",
    "@xmath100 is given by  , and its cumulative distribution function ( cdf ) is known to be referred to as the _ regularized incomplete beta function _",
    "@xmath101 with parameters @xmath102 and @xmath103 , @xcite",
    ". due to the symmetry of the distribution of  @xmath84 , we have @xmath104 , so that @xmath105 .",
    "respectively , @xmath106 and @xmath107^n$ ] , so that finally @xmath108^n.\\ ] ] letting @xmath109 and inverting the last relation , we arrive at  .",
    ".1 in numerical values of the function @xmath23 can be computed via use of the matlab routine betainc .",
    "for instance , with modest values @xmath110 , @xmath111 , and @xmath112 , this gives @xmath113 , and this quantity grows quickly as the dimension  @xmath74 increases .",
    "since we are interested in small values of  @xmath42 , i.e. , in @xmath114 close to unity , a `` closed - form '' lower bound for @xmath46 can be computed as formulated below .    in the conditions of theorem",
    "[ th : ball_maxlin ] @xmath115}\\,,\\ ] ] where @xmath116 .",
    "_ proof _ we have @xmath117}\\\\ & = &    1 - \\beta_n x^{-1/2}\\int_0^{1-x}v^{(n-1)/2}{\\rm d}v   \\qquad\\quad \\mbox { [ $ v=1-t$ ] } \\\\ & = &    1 - \\beta_n\\frac{2}{n+1}x^{-1/2}(1-x)^{(n+1)/2},\\end{aligned}\\ ] ] so that from   we obtain @xmath118^n\\ ] ] and @xmath119 } < n_{\\min}.\\ ] ] proof is complete .",
    ".1 in further simplification of the lower bound can be obtained : @xmath120 this is doable by noting that @xmath121 for small @xmath122 and using the approximation @xmath123 for the beta function with large  @xmath25 .",
    "these lower bounds are quite accurate ; for instance , with @xmath110 , @xmath111 , and @xmath112 , we have @xmath124 , while @xmath125 and @xmath126 .      consider a ( possibly nonlinear ) mapping @xmath127 , @xmath128 ; the goal is to characterize the boundary of the image of a set  @xmath34 under the mapping  @xmath129 .",
    "apart from being of independent interest , this problem emerges in numerous applications . in particular , if a special part of the boundary , the _ pareto front _",
    "@xcite is of interest , we arrive at a multiobjective optimization problem .",
    "numerous examples ( e.g. , see  @xcite ) show that , for  @xmath74 large , the images of the points sampled randomly uniformly in  @xmath3 may happen to fall deep inside the true image set , giving no reasonable description of the boundary and the pareto front of @xmath52 .",
    "we first present a qualitative explanation of this phenomenon by using the setup of fact  2 ; i.e. , the set  @xmath3 is the unit euclidean ball and the mappings are linear .",
    "since the squared norm of a standard gaussian vector in @xmath130 has the @xmath131-distribution @xmath132 with  @xmath66 degrees of freedom  @xcite , from fact  2 and we obtain @xmath133 in distribution as @xmath134 .",
    "this is in compliance with the well - known result in the probability theory , namely , @xmath135 in distribution as @xmath136 , @xcite ; here , @xmath137 stands for the r.v . having the beta distribution with shape parameters @xmath138 . for @xmath139 ( i.e. , @xmath54 , the case most relevant to applications ) , fig .",
    "[ fig : beta_distr ] depicts the plots of the cumulative distribution functions @xmath137 ( see   below for the explicit formula ) for @xmath140 ( i.e. , @xmath141 ) .     for @xmath140.,width=340 ]",
    "hence , fact  2 immediately implies the following important conclusion : linear transformations essentially change the nature of the uniform distribution on a ball .",
    "namely , as the dimension of the vector  @xmath85 grows , with the rank of the transformation matrix  @xmath142 being unaltered , the distribution of the vector  @xmath143 tends to `` concentrate closer the center '' of the image set .",
    "we now turn to fact  1 and provide quantitative estimates ; to this end , consider the simple case where @xmath54 and the two mappings are linear : @xmath144 ( i.e. , @xmath145 in the notation of fact  1 ) ; for instance , @xmath146 may be any two different unit coordinate vectors , so that @xmath147 and @xmath148 , @xmath149 , are the two different components of  @xmath114 .",
    "then the image of  @xmath3 is the unit circle centered at the origin .",
    "introduce now the random variable @xmath150 the squared norm of the image of  @xmath151 under mapping   ( i.e. , @xmath152 ) .",
    "then , by fact  1 with @xmath54 , we have the closed - form expressions for the cdf  @xmath153 and pdf  @xmath154 of the r.v .",
    "@xmath88 : @xmath155 @xmath156    with these in mind , let us evaluate the minimal length  @xmath0 of the multisample that guarantees a given accuracy with a given probability . to this end ,",
    "recall that , given a multisample @xmath157 from the scalar cdf  @xmath158 with pdf  @xmath159 , the random variable @xmath160 has the cumulative distribution function @xmath161 with pdf @xmath162 which is , in our case",
    " writes @xmath163    we next evaluate several statistics of the r.v .",
    "@xmath164 .",
    "the theorem below determines the minimal sample size  @xmath46 that guarantees , with high probability , that a random vector @xmath36 be mapped close to the boundary of the image set .",
    "[ th : mapping_prob ] letting @xmath85 be the random vector uniformly distributed over the unit euclidean ball @xmath34 , consider the linear mapping @xmath165 as in  . given @xmath92 and @xmath93 , the minimal sample size @xmath46 that guarantees , with probability at least  @xmath47 , that at least one sample be mapped at least  @xmath42-close to the boundary of the image set , is given by @xmath166 for small @xmath42 we have @xmath167    _ proof _",
    "let us specify sample size  @xmath0 and estimate the probability for a sample to be mapped close to the boundary of the image set .",
    "to this end , denote the image of  @xmath36 under mapping   by @xmath168 and introduce the r.v . @xmath169 the maximum of @xmath170 over the multisample  @xmath51 . also , consider the r.v .",
    "@xmath171   for which we have @xmath172 and the r.v . @xmath173 , the maximum of @xmath88 over the multisample  @xmath51 , for which we have @xmath174    hence , noting that @xmath175 , for a small  @xmath176 ( i.e. , letting @xmath177 )",
    ", we see that the probability for at least one sample  @xmath178 to be mapped at least @xmath42-close to the boundary is equal to @xmath179    let @xmath92 be a desired confidence level ; then , letting @xmath180 and inverting the last relation , we obtain the minimal required length of the multisample .",
    "the simple approximation for @xmath46 follows from the fact that @xmath121 for small @xmath122 .",
    "@xmath181    .1 in to illustrate , for modest dimension @xmath110 , accuracy @xmath111 , and probability @xmath112 , one has to generate approximately @xmath182 random samples to obtain , with probability  @xmath183 , a point which is @xmath42-close to the boundary of the image set . a sharper illustration of this phenomenon for @xmath184 is given in fig .",
    "[ fig : twodmap ] , which depicts the images of @xmath185 samples of  @xmath36 under mapping  .",
    "none of them falls closer than @xmath186 to the boundary of the image set .",
    "-dimensional ball and the result of the monte carlo sampling , width=302 ]    qualitatively , such a behavior can be explained by using geometric considerations and simple projection - type arguments discussed in  @xcite .",
    "the pdf   can be shown to be unimodular , and we find its mode by straightforward differentiating .",
    "letting @xmath187 , for the pdf we have @xmath188 then @xmath189 writes @xmath190 simplifying , we obtain @xmath191 hence , @xmath192    we thus arrive at the following result .    [",
    "th : mode ] letting @xmath85 be the random vector uniformly distributed over the unit euclidean ball @xmath34 , consider the linear mapping @xmath165 as in   and the random variable @xmath193 the empirical maximum of the function @xmath194 from the multisample @xmath51 of size  @xmath0 .",
    "the mode of the distribution of @xmath41 is given by @xmath192 for large  @xmath74 we have an approximation @xmath195    the quantity @xmath196 is seen to be essentially less than unity for large  @xmath74 , even if the sample size  @xmath0 is huge .",
    "this means the r.v .",
    "@xmath41 takes values far from the boundary of the image .     and @xmath197 , @xmath198,width=302 ]    for instance , let @xmath90 ; then , to `` obtain a point in the @xmath199 vicinity of the boundary , ''",
    "one has to generate @xmath200 random samples in  @xmath3 .",
    "the family of the pdfs   is plotted in fig .",
    "[ fig : eta_pdf ] .",
    "we now estimate the mathematical expectation  @xmath201 of the empirical maximum .",
    "[ th : expect ] under the conditions of theorem  [ th : mode ] we have @xmath202 where @xmath63 is the beta function .",
    "_ proof _ if a r.v .",
    "@xmath203 is positive and defined on @xmath204 $ ] , then the expectation @xmath205 where @xmath158 is the cdf of  @xmath203 .",
    "hence , having @xmath0 samples @xmath178 of @xmath36 and the respective r.v . @xmath171   with support @xmath206 $ ] , for the r.v .",
    "@xmath207 we have @xmath208 where @xmath209 is given by  . by change of variables",
    "@xmath210 , we arrive at .@xmath181    for large @xmath74 and @xmath0 , numerical values of the expectation are close to those observed for the mode ; this is seen from the shape of the pdf   depicted in fig .",
    "[ fig : eta_pdf ] .",
    "more formally , having the approximation @xmath211 for large  @xmath0 , from   we obtain @xmath212    for instance , with @xmath90 and @xmath213 , we have @xmath214 for the expectation and @xmath215 for the mode .",
    "in this section , we consider the scalar optimization problem , however , for box - shaped sets , i.e , , not related to facts  1 and  2 .",
    "we consider the scalar setup described in section  [ two_settings ] along with the deterministic approach based on use of regular grids .",
    "consider the linear scalar optimization problem for the case where @xmath216^n$ ] .",
    "clearly , the results heavily depend on the vector  @xmath217 in the optimized function @xmath218 ; we consider two extreme cases .",
    ".1 in * case 1 .",
    "* first , let @xmath219 and consider the empirical maximum @xmath220 where @xmath221 is the first component of the random vector @xmath151 .",
    "specifying  @xmath93 , we obtain @xmath222 this quantity is seen to be independent of the dimension ( which is obvious as it is ) .",
    "now , specifying a probability level @xmath223 $ ] , we obtain that the minimal required sample size that guarantees accuracy  @xmath42 with probability @xmath47 is equal to @xmath224 for instance , with @xmath225 and @xmath226 , one has to generate just @xmath227 points to obtain a @xmath228-accurate estimate of the maximum with probability  @xmath183 , independently of the dimension .",
    ".1 in * case 2 .",
    "* now let @xmath229 ; i.e. , the optimized function is @xmath230 , so that the maximum is attained at @xmath231 and is equal to @xmath232 .",
    "in contrast to case  1 , monte carlo sampling exhibits a totally different behavior .",
    "below , @xmath233 stands for the volume of a set .",
    "[ th : box_diag ] letting @xmath85 be the random vector uniformly distributed over the unit @xmath4-norm ball @xmath234^n$ ] , consider the linear function @xmath230 . given @xmath92 and @xmath93 , @xmath235 , the minimal sample size  @xmath46 that guarantees , with probability at least  @xmath47 , for the empirical maximum of  @xmath39 to be at least a  @xmath42-accurate estimate of the true maximum , is given by @xmath236 for small  @xmath42 and large  @xmath74 we have @xmath237    _ proof _ let us specify a small @xmath238 and define @xmath239 so that the maximum of @xmath39 , over @xmath240 is equal to @xmath241 . for @xmath235 ,",
    "the set @xmath242 is seen to be the simplex with @xmath243 vertices at the points @xmath244 and @xmath245 with @xmath246 . since @xmath247 ,",
    "for @xmath36 we have @xmath248 so that @xmath249 equating this probability to  @xmath47 and inverting this relation leads to  .",
    "the lower bound   follows immediately from stirling s formula and the fact that @xmath121 for small @xmath122.@xmath181    for @xmath110 and the same values @xmath250 and @xmath112 , we obtain a huge @xmath251 . even for @xmath252 , an `` unexpectedly '' large number @xmath253 of samples",
    "are to be drawn .",
    ".1 in * @xmath5-norm ball : * the setup of case  2 is of the same flavor as the one where the set  @xmath3 is the unit @xmath5-norm ball , and the optimized function is @xmath254 with @xmath255 .",
    "we have a result similar to those in theorems  [ th : ball_maxlin ] and  [ th : box_diag ] .",
    "[ th : l_one_ball ] letting @xmath85 be the random vector uniformly distributed over the unit @xmath5-norm ball @xmath256 , consider the linear function @xmath257 . given @xmath92 and @xmath93 , the minimal sample size  @xmath46 that guarantees , with probability at least  @xmath47 , for the empirical maximum of  @xmath39 to be at least a  @xmath42-accurate estimate of the true maximum ,",
    "is given by @xmath258 for small @xmath42 we have @xmath259    _ proof _ the true maximum of @xmath39 on  @xmath3 is equal to unity ; we specify accuracy @xmath93 and consider the set @xmath260 we then have @xmath261 so that for @xmath36 we obtain @xmath262 and the rest of the proof is the same as that of the previous theorem .",
    "@xmath181    .1 in    to compare complexity associated with evaluating the optimum of a linear function over the @xmath2- , @xmath4- , and @xmath5-balls , we present a table showing the minimal required number of samples for @xmath111 , @xmath112 and various dimensions , as per formulae  , , and , respectively .",
    ".@xmath263-balls : minimal required number of samples for @xmath111 and @xmath112 . [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ t : box ]    it is seen that the uniform mesh exhibits a very poor relative performance as dimension grows , while the results of @xmath264 and monte carlo approaches are much better and similar to each other . clearly , the absolute values are very far from the true maxima equal to  @xmath74 , since @xmath265 samples are not sufficient to obtain reasonable accuracy for dimensions @xmath266 .    instead of computing the sample size for fixed values of the accuracy  @xmath42 as in table  1 , here we fix the sample size and compute the empirical maxima .",
    "the reason for such an `` inversion '' is that , given  @xmath42 and the specific linear function @xmath39 , it is not quite clear how to estimate the required length of the @xmath264-sequence . to overcome this problem",
    ", one might fix a reasonable value of accuracy , compute the minimal sample size  @xmath46 required for monte carlo , run the @xmath264-approach with this length , and compare the results .",
    "however , for large dimensions  @xmath74 , the values of  @xmath46 are huge , leading to very large computation times or even memory overflow .",
    "the values obtained for the uniform mesh were computed by inverting relation   and using the actual grid with cardinality @xmath267 , so the quantities presented in row `` uniform grid '' of table  2 are overly optimistic .",
    "more importantly , the routine sobolset has several parameters , such as @xmath268 ( choice of the initial point in the sequence ) and @xmath269 ( selection of every @xmath270th point ) .",
    "these play the similar role as the seed value in matlab pseudorandom number generators does , and may happen to be crucial for the quality of the resulting @xmath264 sequence . in the experiments , we used the default values @xmath271",
    "( i.e. , the first  @xmath0 points ) , though for different values of @xmath272 , the respective estimates of the empirical maximum may differ a lot .",
    "for instance , adopting @xmath273 , for @xmath274 we obtain a much better estimate @xmath275 , while taking ( intentionally bad ) @xmath276 leads to very poor @xmath277 .",
    "finally , note that applications of uniform grids and @xmath264 sequences are limited to box - shaped sets .",
    "sets different from boxes can in principle be embedded in tight enclosing boxes with subsequent use of rejection techniques ; however , the rejection rate usually grows dramatically as the dimension increases .",
    "the main contribution of the paper is a rigorous explanation of the reason why does a direct monte carlo approach show itself inefficient in high - dimensional optimization problems when estimating the maximum value of a function from a random sample in the domain of definition .",
    "first , attention was paid to linear functions and ball - shaped sets ; using known results on the uniform distribution over the ball , we characterized the accuracy of the estimates obtained via a specific random variable associated with the function value . also , a multiobjective optimization setup was discussed .",
    "the results obtained testify to a dramatic growth of computational complexity ( required number of samples ) as the dimension of the ball increases .",
    "same flavor results are obtained for box - shaped sets ; these also include analysis of deterministic grids .",
    "deb , k. : multiobjective optimization . in : burke ,",
    "e.k . and kendall , g. ( eds . ) : search methodologies : introductory tutorials in optimization and decision support techniques , pp .  444-463 .",
    "springer science+business media , new york ( 2014 )              polyak , b.t . , shcherbakov , p.s . : failure of the monte carlo methods in optimization problems of high dimensions . in : granichin ,",
    "( ed . ) : stochastic optimization in informatics , vol .",
    "10 , no .  1 ,",
    "89100 ( in russian ) .",
    "petersburg state university ( 2014 )      polyak , b. , shcherbakov , p. , khlebnikov , m. : quadratic image of a ball : towards efficient description of the boundary . in : international conference on system theory , control , and computing ( icstcc 2014 ) , pp .",
    "ieee css ( 2014 )"
  ],
  "abstract_text": [
    "<S> the paper proposes an answer to the question formulated in the title .    </S>",
    "<S> .2 in    institute for control science , ras , moscow , russia + e - mails : boris@ipu.ru ; cavour118@mail.ru    july 5 , 2016 </S>"
  ]
}