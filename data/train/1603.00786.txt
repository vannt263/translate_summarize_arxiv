{
  "article_text": [
    "entity mention detection , and more specifically named entity recognition ( ner ) @xcite , has become a popular task for social media analysis @xcite .",
    "many downstream applications that use social media , such as relation extraction @xcite and entity linking @xcite , rely on first identifying mentions of entities . not surprisingly , accuracy of ner systems in social media trails state - of - the - art systems for news text and other formal domains .",
    "while this gap is shrinking in english @xcite , it remains large in other languages , such as chinese @xcite .",
    "one reason for this gap is the lack of robust up - stream nlp systems that provide useful features for ner , such as part - of - speech tagging or chunking .",
    "annotated twitter data for these systems to improve a twitter ner tagger , however , these systems do not exist for social media in most languages .",
    "another approach has been that of and , who relied on training unsupervised lexical embeddings in place of these upstream systems and achieved state - of - the - art results for english and chinese social media , respectively .",
    "the same approach was also found helpful for ner in the news domain @xcite    in asian languages like chinese , japanese and korean , word segmentation is a critical first step @xcite .",
    "showed the value of word segmentation to chinese ner in social media by using character positional embeddings .    in this paper",
    ", we investigate better ways to incorporate word boundary information into an ner system for chinese social media .",
    "we combine state - of - the - art chinese word segmentation @xcite with the best chinese social media ner system @xcite . since both systems used learned representations , we propose an integrated model that allows for joint training of learned representations , providing more information to the ner system about what is learned from word segmentation , as compared to features based on segmentation output .",
    "our integrated model achieves nearly a 5% absolute improvement over the previous best results on both ner and nominal mention for chinese social media .",
    "we propose a model that integrates the best chinese word segmentation system @xcite using an lstm neural model that learns representations , with the best ner model for chinese social media @xcite , that supports training neural representations by a log - bilinear crf .",
    "we begin with a brief review of each system .",
    "proposed a single layer , left to right lstm for chinese word segmentation .",
    "an lstm is a recurrent neural network ( rnn ) which uses a series of gates ( input , forget and output gate ) to control how memory is propagated in the hidden states of the model . for the chinese word segmentation task",
    ", each chinese character is initialized as a @xmath0 dimensional vector , which the lstm will modify during its training .",
    "besides , for each input character , the model learns a hidden vector @xmath1 .",
    "these vectors are then used with a biased - linear transformation to predict the output labels , which in this case are * b*egin , * i*nside , * e*nd , and * s*ingleton . a prediction for position @xmath2",
    "is given as : @xmath3 where @xmath4 are the transformation parameters , @xmath5 the bias parameter , and @xmath6 the hidden state at position @xmath2 . to model the tag dependencies , they introduced the transition score @xmath7 to measure the probability of jumping from tag @xmath8 to tag @xmath9 .",
    "we used the same model as trained on the same data ( segmented chinese news article ) we employed a different training objective . employed a max - margin , however , while they found this objective yielded better results , we observed that maximum - likelihood yielded better segmentation results in our experiments .",
    "additionally , we sought to integrate their model with a log - bilinear crf , which uses a maximum - likelihood training objective . for consistency",
    ", we trained the lstm with a maximum - likelihood training objective as well .",
    "the maximum - likelihood crf objective function for predicting segmentations is : @xmath10 \\end{split } \\label{eq : segment}\\ ] ]    example pairs ( @xmath11 ) are word segmented sentences , @xmath12 indexes examples , and @xmath13 indexes positions in examples .",
    "@xmath14 are standard transition probabilities learned by the crf in model . ] .",
    "the lstm parameters @xmath15 are used to produce @xmath16 , the emission probability of the label at position @xmath13 for input sentence @xmath12 , which is obtained by taking a soft - max of .",
    "we use a first - order markov model .      proposed a log - bilinear model for chinese social media ner .",
    "they used standard ner features along with additional features based on lexical embeddings . by fine - tuning these embeddings , and jointly training them with a word2vec @xcite objective , the resulting model is log - bilinear .",
    "typical lexical embeddings provide a single embedding vector for each word type .",
    "however , chinese text is not word segmented , making the mapping between input to embedding vector unclear .",
    "explored several types of representations for chinese , including pre - segmenting the input to obtain words , using character embeddings , and a combined approach that learned embeddings for characters based on their position in the word .",
    "this final representation yielded the largest improvements .",
    "we use the same idea but augmented it with lstm learned representations , and we enables interaction between the crf and the lstm parameters .",
    "more details are described in (  [ sec : joint ] ) .",
    "the improvements provided by character position embeddings demonstrated by indicates that word segmentation information can be helpful for ner . embeddings aside , a simple way to include this information in an ner system would be to add features to the crf using the predicted segmentation labels as features .",
    "however , these features alone may overlook useful information from the segmentation model .",
    "previous work showed that jointly learning different stages of the nlp pipeline helped for chinese @xcite .",
    "we thus seek approaches for deeper interaction between word segmentation and ner .",
    "the lstm word segmentation learns two different types of representations : 1 ) embeddings for each characters and 2 ) hidden vectors for predicting segmentation tags .",
    "compressing these rich representations down to a small feature set imposes a bottleneck on using richer segmentation related information for ner .",
    "we experiment with including both of these information sources directly into the ner model .",
    "since the log - bilinear crf already supports joint training of lexical embeddings , we can also incorporate the lstm output hidden vectors as dynamic features using a joint objective function .",
    "first , we augment the crf with the lstm parameters as follows : @xmath17 ,   \\end{split } \\label{eq : ner}\\ ] ] where @xmath12 indexes instances , @xmath18 positions , and @xmath19 represents the feature functions .",
    "these features now depend on the embeddings learned by the lstm ( @xmath20 ) and the lstm s output hidden vectors ( @xmath21 ) .",
    "note that by including @xmath21 alone we create a dependence on all lstm parameters on which the hidden states depend ( i.e. the weight matrixes ) .",
    "we experiment with including input embeddings and output hidden vectors independently , as well as both parameters together .    [",
    "[ joint - training ] ] joint training + + + + + + + + + + + + + +    in our integrated model , the lstm parameters are used for both predicting word segmentations and ner .",
    "therefore , we consider a joint training scheme . we maximize a ( weighted ) joint objective : @xmath22 where @xmath23 trades off between better segmentations or better ner , and @xmath24 includes all parameters used in both models .",
    "since we are interested in improving ner we consider settings with @xmath25 .",
    "we train all of our models using stochastic gradient descent ( sgd . )",
    "we train for up to 30 epochs , stopping when ner results converged on dev data .",
    "we use a separate learning rate for each part of the joint objective , with a schedule that decays the learning rate by half if dev results do not improve after 5 consecutive epochs .",
    "dropout is introduced in the input layer of lstm following .",
    "we optimize two hyper - parameters using held out dev data : the joint coefficient @xmath23 in the interval @xmath26 $ ] and the dropout rate in the interval @xmath27 $ ] .",
    "all other hyper - parameters were set to the values given by for the lstm and for the crf .",
    "we train the joint model using an alternating optimization strategy . since the segmentation dataset is significantly larger than the ner dataset , we subsample the former at each iteration to be the same size as the ner training data , with different subsamples in each iteration .",
    "we found subsampling critical and it significantly reduced training time and allowed us to better explore the hyper - parameter space .",
    "we initialized lstm input embeddings with pre - trained character - positional embeddings trained on 112,971,734 weibo messages to initialize the input embeddings for lstm .",
    "we used word2vec @xcite with the same parameter settings as to pre - train the embeddings .",
    "we use the same training , development and test splits as for word segmentation and for ner .    [ [ word - segmentation ] ] word segmentation + + + + + + + + + + + + + + + + +    the segmentation data is taken from the sighan 2005 shared task .",
    "we used the pku portion , which includes 43,963 word sentences as training and 4,278 sentences as test .",
    "we did not apply any special preprocessing .",
    "[ [ ner ] ] ner + + +    this dataset contains 1,890 sina weibo messages annotated with four entity types ( person , organization , location and geo - political entity ) , including named and nominal mentions .",
    "we note that the word segmentation dataset is significantly larger than the ner data , which motivates our subsampling during training (  [ sec : parameter_estimation ] ) .",
    "table  [ tab : all_results ] shows results for ner in terms of precision , recall and f1 for named ( left ) and nominal ( right ) mentions on both dev and test sets .",
    "the hyper - parameters are tuned on dev data and then applied on test .",
    "we now explain the results .",
    "we begin by establishing a crf baseline ( # 1 ) and show that adding segmentation features helps ( # 2 ) .",
    "however , adding those features to the full model ( with embeddings ) in ( # 3 ) did not improve results ( # 4 ) .",
    "this is probably because the character - positional embeddings already carry segmentation information .",
    "replacing the character - positional embeddings with character embeddings ( # 5 ) gets worse results but benefits from adding segmentation features ( # 6 ) .",
    "this demonstrates both that word segmentation helps and that character - positional embeddings effectively convey word boundary information .",
    "we now consider our model of jointly train the character embeddings ( # 9 ) , the lstm hidden vectors ( # 10 ) and both ( # 11 ) .",
    "they all improve over the best published results ( # 3 ) . jointly train the lstm hidden vectors ( # 10 )",
    "does better than the embeddings ( # 9 ) probably because they carry richer word boundary information . using both representations",
    "achieves the single best result ( # 11 ) : @xmath28 improvement on named and @xmath29 improvement on nominal mentions .",
    "finally , we examine how much of the gain is from joint training versus from pre - trained segmentation representations .",
    "we first train an lstm for word segmentation , then use the trained embeddings and hidden vectors as inputs to the log - bilinear crf model for ner , and fine tunes these representations .",
    "this ( # 7 ) improved test f1 by 2% , about half of the overall improvements from joint training .",
    "different interpretations of our results suggest directions for future work .    first , we can view our method as multi - task learning @xcite , where we are using the same learned representations ( embeddings and hidden vectors ) for two tasks : segmentation and ner , which use different prediction and decoding layers .",
    "result # 8 shows the result of excluding the additional ner features and just sharing jointly trained lstm . while this does not perform as well as adding the additional ner features ( # 11 ) , it is impressive that this simple architecture achieved similar f1 as the best results in . while we may expect both ner and word segmentation results to improve , we found the segmentation performances of the best joint model tuned for ner lose to the stand alone word segmentation model ( f1 90.7% v.s .",
    "this lies in the fact that tuning @xmath23 means choosing between the two tasks ; no single setting achieved improvements for both , which suggests further work is needed on better model structures and learning .",
    "second , our segmentation data is from the news domain , whereas the ner data is from social media .",
    "while it is well known that segmentation systems trained on news do worse on social media @xcite , we still show large improvements in applying our model to these different domains",
    ". it may be that we are able to obtain better results in the case of domain mismatch because we integrate the representations of the lstm model directly into our crf , as opposed to only using the predictions of the lstm segmentation model .",
    "we plan to consider expanding our model to explicitly include domain adaptation mechanisms @xcite .",
    "razvan  c bunescu and raymond  j mooney .",
    "a shortest path dependency kernel for relation extraction . in _ empirical methods in natural language processing ( emnlp ) _ , pages 724731 .",
    "association for computational linguistics .",
    "xinchi chen , xipeng qiu , chenxi zhu , pengfei liu , and xuanjing huang .",
    "long short - term memory neural networks for chinese word segmentation . in _ proceedings of the conference on empirical methods in natural language processing ( emnlp)_.    colin cherry and hongyu guo .",
    "the unreasonable effectiveness of word representations for twitter named entity recognition . in _ north america chapter of association for computational linguistics ( naacl)_. association for computational linguistics .",
    "ronan collobert and jason weston .",
    "2008 . a unified architecture for natural language processing : deep neural networks with multitask learning . in",
    "international conference on machine learning ( icml ) _ , pages 160167 .",
    "acm .      huiming duan , zhifang sui , ye  tian , and wenjie li .",
    "the cips - sighan clp 2012 chineseword segmentation onmicroblog corpora bakeoff . in _",
    "second cips - sighan joint conference on chinese language processing _ , pages 3540 , tianjin , china , december .",
    "association for computational linguistics .",
    "tim finin , william murnane , anand karandikar , nicholas keller , justin martineau , and mark dredze .",
    "annotating named entities in twitter data with crowdsourcing . in _",
    "naacl workshop on creating speech and language data with mechanical turk_.      jinlan fu , jie qiu , yunlong guo , and li  li .",
    "entity linking and name disambiguation using svm in chinese micro - blogs . in _",
    "11th international conference on natural computation , icnc 2015 , zhangjiajie , china , august 15 - 17 , 2015 _ , pages 468472 .",
    "zhengyan he , houfeng wang , and sujian li .",
    "the task 2 of cips - sighan 2012 named entity recognition and disambiguation in chinese bakeoff . in _",
    "second cips - sighan joint conference on chinese language processing _ , pages 108114 , tianjin , china , december .",
    "association for computational linguistics .",
    "guangjin jin and xiao chen .",
    "the fourth international chinese language processing bakeoff : chinese word segmentation , named entity recognition and chinese pos tagging . in _ sixth sighan workshop on chinese language processing _",
    ", page  69 .",
    "citeseer .",
    "chenliang li , jianshu weng , qi  he , yuxia yao , anwitaman datta , aixin sun , and bu - sung lee . 2012 .",
    "twiner : named entity recognition in targeted twitter stream . in _ sigir conference on research and development in information retrieval _",
    ", sigir 12 , pages 721730 , new york , ny , usa . acm .",
    "xiaohua liu , shaodian zhang , furu wei , and ming zhou .",
    "2011 . recognizing named entities in tweets . in _",
    "association for computational linguistics ( acl ) _",
    ", pages 359367 .",
    "association for computational linguistics .",
    "xiaohua liu , ming zhou , furu wei , zhongyang fu , and xiangyang zhou .",
    "joint inference of named entity recognition and normalization for tweets . in _",
    "association for computational linguistics ( acl ) _ , acl 12 , pages 526535 , stroudsburg , pa , usa . association for computational linguistics .",
    "xiaohua liu , ming zhou , furu wei , zhongyang fu , and xiangyang zhou .",
    "joint inference of named entity recognition and normalization for tweets . in _ proceedings of the 50th annual meeting of the association for computational linguistics : long papers - volume 1 _ , acl 12 , pages 526535 , stroudsburg , pa , usa .",
    "association for computational linguistics .",
    "andrew mccallum and wei li .",
    "early results for named entity recognition with conditional random fields , feature induction and web - enhanced lexicons . in _ north america chapter of association for computational linguistics ( naacl)_.    tomas mikolov , ilya sutskever , kai chen , greg  s corrado , and jeff dean . 2013 . distributed representations of words and phrases and their compositionality . in _ neural information processing systems ( nips ) _ , pages 31113119 .",
    "lev ratinov , dan roth , doug downey , and mike anderson .",
    "local and global algorithms for disambiguation to wikipedia . in _ association for computational linguistics ( acl ) _ , pages 13751384 .",
    "association for computational linguistics .",
    "alan ritter , sam clark , oren etzioni , et  al .",
    "2011 . named entity recognition in tweets : an experimental study . in _ empirical methods in natural language processing ( emnlp ) _ , pages 15241534 .",
    "association for computational linguistics .",
    "joseph turian , lev ratinov , and yoshua bengio .",
    "word representations : a simple and general method for semi - supervised learning . in _",
    "association for computational linguistics ( acl ) _ , pages 384394 .",
    "association for computational linguistics .",
    "suxiang zhang , ying qin , juan wen , and xiaojie wang .",
    "word segmentation and named entity recognition for sighan bakeoff3 . in _",
    "fifth sighan workshop on chinese language processing _ , pages 158161 ,",
    "sydney , australia , july .",
    "association for computational linguistics .    xiaoqing zheng , hanyang chen , and tianyu xu .",
    "deep learning for chinese word segmentation and pos tagging . in _ proceedings of the 2013 conference on empirical methods in natural language processing _ , pages 647657 , seattle , washington , usa , october .",
    "association for computational linguistics ."
  ],
  "abstract_text": [
    "<S> named entity recognition , and other information extraction tasks , frequently use linguistic features such as part of speech tags or chunkings . for languages where word boundaries are not readily identified in text , </S>",
    "<S> word segmentation is a key first step to generating features for an ner system . while using word boundary tags as features are helpful , the signals that aid in identifying these boundaries may provide richer information for an ner system . </S>",
    "<S> new state - of - the - art word segmentation systems use neural models to learn representations for predicting word boundaries . </S>",
    "<S> we show that these same representations , jointly trained with an ner system , yield significant improvements in ner for chinese social media . in our experiments , jointly training ner and word segmentation with an lstm - crf model yields nearly 5% absolute improvement over previously published results .    </S>",
    "<S> = 1 </S>"
  ]
}