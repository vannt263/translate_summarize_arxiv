{
  "article_text": [
    "this paper considers the problem of estimating a vector of parameter @xmath0 from the linear problem @xmath1 where @xmath2 is a vector of observations , @xmath3 an @xmath4 predictor matrix , and @xmath5 a vector of independent normal random variables . the goal is to find a relevant parametric vector @xmath6 among many potential candidates and obtain high prediction accuracy .    the @xmath7 penalized least squares estimator for problem   has been the focus of a great deal of attention for variable selection and estimation in high - dimensional linear regression when the number of variables is much larger than the sample size @xcite .",
    "recently the dantzig selector was proposed for problem   in @xcite .",
    "the dantzig selector @xmath8 is a solution to the optimization problem @xmath9 with a fixed parameter @xmath10 and a diagonal matrix @xmath11 where the diagonal entries are equal to the @xmath12 norm of the columns of @xmath3 . here , we write @xmath13 for the @xmath14 norm of @xmath15 , @xmath16 .",
    "optimal @xmath12 rate properties for @xmath17 were established under a sparsity scenario and impressive empirical performance on real world problems involving large values of @xmath18 was shown in @xcite . since",
    "then the dantzig selector has received a considerable amount of attention .",
    "discussions on the dantzig selector can be found in @xcite . in @xcite ,",
    "an algorithm was proposed for fitting the entire coefficient path of the dantzig selector with a similar computational cost to the least angle algorithm that is used to compute the @xmath7 minimization via the lasso technique .",
    "the dantzig selector is a convex , but not strictly convex , optimization problem .",
    "unique solutions are in general not guaranteed .",
    "conditions ensuring the uniqueness of the dantzig selector were presented in @xcite . in @xcite a new class of dantzig selectors for linear regression problems for right - censored outcomes was proposed .",
    "the importance of the dantzig selector in linear regressions has been demonstrated in the aforementioned work .",
    "efficient methods for solving problem  , which however were not emphasized in the current literature , are highly needed . in @xcite",
    ", the problem is cast as a linear program which is solved by using a primal - dual interior point algorithm  @xcite .",
    "as it is well known , interior point methods are not efficient for large - scale problems . in @xcite ,",
    "the problem is cast as linear cone programming problem for which a smooth approximation to its dual problem is solved by an optimal first - order method @xcite .",
    "recently , an alternating direction method ( adm ) for finding the dantzig selector was studied in @xcite .",
    "numerical experiments showed that this method usually outperforms the method in @xcite in terms of cpu time while producing solutions of comparable quality .",
    "the problem was rewritten in @xcite in a form to which adm can be easily applied .",
    "adm itself is an iterative algorithm . in each iterate ,",
    "two subproblems are needed to be solved successively .",
    "one of the subproblems has a closed form solution , while the other does not and is approximated by a nonmonotone gradient method proposed in @xcite . to alleviate the difficulty caused by the subproblem without a closed form solution , a linearized adm was proposed for the dantzig selector and was shown to be efficient for solving both synthetic and real world data sets in @xcite .    in this paper ,",
    "the dantzig selectors for problem   are found by an algorithm based upon proximity operators .",
    "we first rewrite the problem as an unconstrained structural optimization problem via an indicator function .",
    "the resulting problem is then solved by a primal - dual algorithm . in comparison with the one given in @xcite ,",
    "our proposed algorithm is easy to implement .",
    "ours achieves comparable quality results while consuming much less cpu time .",
    "the outline of the paper is organized as follows . in section [ sec : alg ] we present our fixed - point theory based proximity operator algorithm for solving problem . in section",
    "[ sec : experiments ] , we present numerical experiments comparing the accuracy and efficiency of the proposed algorithm with adm proposed in  @xcite .",
    "the first set of experiments uses simulated sparse signals and the second set uses samples of biomarker data to predict the diagnosis of leukemia patients .",
    "section  [ sec : conclusion ] concludes the paper .    the following notation will be used in the rest of the paper . for any vector @xmath19 ,",
    "let @xmath20 and @xmath21 both denote the @xmath22-th component of @xmath23 .",
    "also for any vector @xmath24 , @xmath25 is the component - wise absolute values of @xmath23 , that is the @xmath22-th component of @xmath25 is @xmath26 , while @xmath27 is the vector whose @xmath22-th component is @xmath28 if @xmath29 and @xmath30 otherwise . given two vectors @xmath23 and @xmath31 in @xmath32",
    ", @xmath33 denotes the hadamard ( component - wise ) product of @xmath23 and @xmath31 , @xmath34 denotes the vector whose @xmath22-th entry is @xmath35 , and @xmath36 denotes the vector whose @xmath22-th entry is @xmath37 .",
    "let @xmath38 denote the vector of all ones whose dimension should be clear from the context .",
    "the natural numbers are given by @xmath39 .",
    "for the usual @xmath40-dimensional euclidean space denoted by @xmath32 we define @xmath41 , for @xmath42 , the standard inner product in @xmath32 .",
    "we denote by @xmath43 , @xmath44 , and @xmath45 the @xmath7 norm , @xmath12 norm , and the @xmath46 norm of a vector , respectively . the class of all lower semicontinuous convex functions @xmath47 $ ] such that @xmath48 is denoted by @xmath49 . for a closed convex set @xmath50 of @xmath32 , its indicator function",
    "@xmath51 is in @xmath49 and is defined as @xmath52 for a function @xmath53 , @xmath54 is the set of points of the given argument in @xmath55 for which @xmath56 attains its minimum value , i.e. , @xmath57 .",
    "in this section , we develop a proximity algorithm for solving the optimization problem  .",
    "we begin with reviewing two existing works on this problem , namely the alternating direction method ( adm ) proposed in @xcite and the linearized alternating direction method of multipliers ( ladm ) proposed in @xcite .",
    "both methods work on the reformulated optimization problem   with @xmath58 as follows : @xmath59 where @xmath60 is an auxiliary variable .",
    "the augmented lagrangian function for problem   is @xmath61 where @xmath62 is the lagrange multiplier and @xmath63 is a penalty parameter .",
    "the iterative scheme of adm for optimization problem   is @xmath64 which , with some elementary manipulations , can equivalently be written as @xmath65 the @xmath66-related subproblem in has a closed form solution , but the @xmath67-related subproblem does not and is solved approximately by using the nonmonotone gradient method in @xcite .",
    "the iterative scheme of ladm for optimization problem   is @xmath68 where @xmath69 is a proximal parameter and @xmath70 .",
    "note that the order of updating @xmath71 and @xmath72 in adm is reversed in ladm .",
    "for the @xmath67-subproblem in ladm , the last two terms in the objective function can be viewed as the linearization of the quadratic term @xmath73 with respect to @xmath67 at @xmath74 after dropping a constant .",
    "furthermore , the @xmath67-subproblem , after completing the square of these two terms and ignoring the resulting constant term , is the same as @xmath75 which has a closed form solution .",
    "the @xmath66-subproblem has a closed form solution as in adm .",
    "therefore , ladm can be easily and efficiently implemented .",
    "it was shown in @xcite that for any @xmath63 and @xmath76 and any initial iterate @xmath77 , the sequence @xmath78 converges .",
    "furthermore , the limit of the sequence @xmath79 is a solution of the dantzig selector problem .    in the following , we present our fixed - point theory based proximity operator algorithm for solving the optimization problem  . for simplicity of exposition , with the matrices @xmath3 and @xmath11 , the vector @xmath80 , and the constant @xmath81 appearing in problem  , we set @xmath82    then the optimization problem   can be rewritten as @xmath83 the objective function of this problem is convex and coercive thanks to the @xmath7-norm being coercive .",
    "hence a solution to problem   exists and can be characterized in terms of proximity operator . to this end , we review the definition of proximity operator .    for a function @xmath53 ,",
    "the proximity operator of @xmath56 with parameter @xmath84 , denoted by @xmath85 , is a mapping from @xmath32 to itself , defined for a given point @xmath86 by @xmath87    now , we can present a characterization of solutions of problem   that is simply derived from fermat s rule .",
    "[ thm : char ] let the @xmath88 matrix @xmath89 and the vector @xmath90 be given in . if @xmath0 is a solution to problem  , then for any @xmath91 and @xmath92 there exists a vector @xmath60 such that @xmath93 conversely , if there exist @xmath91 and @xmath92 such that @xmath94 satisfy equations   and , then @xmath67 is a solution of problem  .",
    "the proof of the result follows straightforwardly a general result in ( * ? ? ?",
    "* proposition 1 ) . for completeness",
    ", we present its proof here .",
    "first , we assume that @xmath67 is a solution to problem  . by fermat s rule and the chain rule of subdifferentiation , @xmath95 .",
    "then for any @xmath91 and @xmath92 there exists @xmath96 such that @xmath97 , that is , in terms of proximity operator , equation  .",
    "since the set @xmath98 is a cone , then @xmath96 implies @xmath99 which is essentially equivalent to equation  .",
    "conversely , if equations   and are satisfied , we then have @xmath97 and @xmath99 accordingly . using the fact that the set @xmath98 is a cone again , the second inclusion @xmath99 implies @xmath100 .",
    "multiplying @xmath101 to both sides of the previous inclusion and using the chain rule @xmath102 , we have that @xmath103 .",
    "since @xmath104 , we obtain @xmath105 .",
    "this shows that @xmath67 is a solution to problem  .",
    "we comment on the computation of the proximity operators @xmath106 and @xmath107 appearing in equations   and .",
    "the proximity operator @xmath106 at any @xmath108 is the well - known soft - thresholding operator given as follow : @xmath109    [ lemma : proj - prox ] let @xmath81 be a constant , let @xmath110 be a vector in @xmath111 , and let the set @xmath50 be given in . then for any vector @xmath112 , @xmath113 and @xmath114    it is well - known that the proximity operator @xmath107 is the projection operator onto the set @xmath50 . since the set @xmath115 is the cube with @xmath110 as its center and @xmath116 as the length of its side in @xmath111 .",
    "hence , @xmath117 the projection of the vector @xmath118 is given by .",
    "further , it holds that @xmath119 . from this identity",
    ", we can directly check that for each @xmath22 from @xmath28 to @xmath120 @xmath121 which , by using equation  , is @xmath122 .",
    "this completes the proof .    as a result of lemma  [ lemma : proj - prox ] , equation",
    "can be rewritten as follows : @xmath123 therefore , by theorem  [ thm : char ] , finding a solution @xmath67 to problem   amounts to solving the coupled fixed - point equations   and .",
    "two iterative schemes can be derived from equations   and .",
    "let us write equation   as @xmath124 . with any initial estimates",
    "@xmath125 and @xmath126 , the first iterative scheme based upon equations   and is as follows : @xmath127 we would like to comment the connection of this scheme with some existing ones .",
    "the dual formulation of , as derived in @xcite , is @xmath128 applying the primal - dual hybrid gradient method ( see ( * ? ? ?",
    "* equation 2.18 ) ) to the above dual formulation yields exactly the iterative scheme .",
    "it was further pointed out in @xcite that the iterative scheme is essentially the same as the linearized adm applying to problem  . in other words ,",
    "the iterative scheme is the same as in the case of @xmath58 .",
    "now , let us introduce the second iterative scheme for problem  .",
    "let us write equation   as @xmath129 . with any initial estimates",
    "@xmath130 and @xmath131 , the second iterative scheme based upon equations   and is as follows : @xmath132    the sequence @xmath79 generated by the iterative schemes   and will converge for any initial seeds when @xmath133 .",
    "the proof of this convergence result can be found in @xcite .",
    "hence , the limit of the sequence @xmath79 is a fixed - point of equations   and . in particular , the limit of the sequence @xmath134 is a solution to problem .",
    "as noted in @xcite , the dantzig selector often slightly underestimates the true values of the nonzero parameters . to correct this bias and increase performance in practical settings ,",
    "a postprocessing procedure was proposed in @xcite .",
    "assume that @xmath135 is the limit of the sequence @xmath134 that is generated through the iterative scheme  .",
    "this postprocessing consists of two steps .",
    "the first step is to estimate @xmath136 , the support of the vector @xmath135 .",
    "let @xmath137 be the @xmath138 submatrix obtained by extracting the columns of @xmath3 corresponding to the indices in @xmath139 , and let @xmath140 be the @xmath141-dimensional vector obtained by extracting the coordinates of @xmath8 corresponding to the indices in @xmath139 .",
    "the second step of the postprocessing is to construct the estimator @xmath8 such that @xmath142 and set the other coordinates to zero",
    ". if the matrix @xmath143 is invertible then @xmath144 .    putting all above discussion together , a complete two - stage procedure for finding a solution of problem",
    "is described in algorithm  [ alg : matrix - final ] .",
    "* input : * set the fixed parameters @xmath145 * initialization : * set the initial parameters @xmath146    * stage - i : * generate the sequence @xmath147 using equations   and  .",
    "+   + @xmath148    * stage - ii : * let @xmath149 be the last set of parameters computed in stage - i .",
    "* approximate @xmath150 by @xmath151 . *",
    "compute @xmath152 . * extend @xmath153 to form the dantzig selector @xmath154 on @xmath139 : @xmath155    stage - i of algorithm  [ alg : matrix - final ] terminates once the sequence @xmath147 reaches a stationary point . to estimate when this occurs , terminate the iterations when either of the following stopping criteria",
    "are met :    1 .",
    "the relative change between successive terms in the sequence @xmath156 falls below a specified tolerance ; @xmath157 for some @xmath158 , or 2 .",
    "the support of the sequence @xmath156 is stationary for a specified number of successive iterations ; @xmath159 for a fixed @xmath160 and some positive integer @xmath161 .",
    "stage - i is the largest contributor to the computational complexity of algorithm  [ alg : matrix - final ] , with each iteration having complexity @xmath162 . in comparison , each outer loop of adm computing the @xmath66 and @xmath163-related subproblems has complexity @xmath162 , while each inner loop of adm approximating the @xmath67-related subproblem has complexity @xmath164 .",
    "in general , algorithm  [ alg : matrix - final ] and adm will use a different number of iterations to terminate their iterative stages , so their overall complexities can not be directly compared .",
    "however , the numerical experiments in the next section indicate that algorithm  [ alg : matrix - final ] tends to have less overall complexity than adm since algorithm  [ alg : matrix - final ] has a shorter runtime even in situations where it requires more iterations .",
    "in the following experiments , we apply the proposed proximity operator based approach presented in algorithm  [ alg : matrix - final ] and the alternating direction method ( adm ) presented in  @xcite to solve the dantzig selector problem   using both synthetic and real data sets .",
    "the experiments using synthetic data are performed in matlab r2013a on single nodes of the condor supercomputer , hosted at afrl / rit affiliated resource center .",
    "the full capabilities of condor were not taken advantage of ; we ran the algorithms in serial using single nodes to emulate a typical high end consumer workstation .",
    "each utilized node is equipped with an intel xeon x5650 6 core cpu , with 2.67 ghz and 6@xmath1658 gb ram .",
    "the experiments using the real data set are performed in matlab r2014a on a pc with an intel core i7 - 3630qm 2.40 ghz processor and 16 gb ram running windows 7 enterprise .",
    "synthetic data set    in this series of simulations , sparse coefficient vectors are generated then recovered from noisy random linear observations using both algorithm  [ alg : matrix - final ] and adm .",
    "the parameters used are @xmath166 and @xmath167 for @xmath168 , and @xmath169 corresponding to 1% , 5% , 10% and 15% noise levels . for each combination of @xmath170 and @xmath171 , 100 simulations each of algorithm",
    "[ alg : matrix - final ] and adm are performed .",
    "all other parameters for adm are selected following the guidelines in  ( * ? ? ?",
    "* section 3 ) and the parameters selected for the initialization stage of algorithm  [ alg : matrix - final ] are @xmath172 , @xmath173 and @xmath174 .",
    "the parameters for the stopping criteria are @xmath175 and @xmath176 .",
    "the parameters @xmath177 and @xmath178 depend on the noise level @xmath171 , which in practice may not be known a priori .",
    "however , the noise level may be well - approximated using existing methods . in the event that the noise level is not accurately approximated",
    ", the speed of convergence of algorithm  [ alg : matrix - final ] will be affected , but the accuracy should not suffer much .",
    "the stopping criteria    the @xmath179 sensing matrices @xmath3 are generated for each simulation with independent gaussian entries normalized so each column has unit @xmath12 norm . to generate the coefficient vector , for each simulation a support set @xmath180 of size @xmath181",
    "is selected uniformly at random .",
    "then the vector @xmath67 with indices in @xmath180 is defined according to @xmath182 , where @xmath183 is a collection of independently and identically distributed random variables sampled from the standard normal distribution and @xmath184 is a collection of independently and identically distributed random variables sampled from the uniform distribution on @xmath185 . for @xmath186 ,",
    "set @xmath187 .",
    "then algorithm  [ alg : matrix - final ] and adm are used to approximate the dantzig selector @xmath154 from the observations @xmath188 , where @xmath5 is a collection of independent and identically distributed random variables sampled from the normal distribution with mean zero and standard deviation @xmath171 .     and adm with and without postprocessing for a single simulation of experiment 3.1 with parameters @xmath189 with @xmath190 .",
    ", title=\"fig:\",scaledwidth=48.0% ]   and adm with and without postprocessing for a single simulation of experiment 3.1 with parameters @xmath189 with @xmath190 .",
    ", title=\"fig:\",scaledwidth=48.0% ]     and adm with and without postprocessing for a single simulation of experiment 3.1 with parameters @xmath189 with @xmath190 .",
    ", title=\"fig:\",scaledwidth=48.0% ]   and adm with and without postprocessing for a single simulation of experiment 3.1 with parameters @xmath189 with @xmath190 .",
    ", title=\"fig:\",scaledwidth=48.0% ]    the accuracy of the dantzig selector recovered in the simulations is measured by @xmath191 where @xmath67 denotes the true parameter and @xmath154 denotes the parameter recovered using either algorithm  [ alg : matrix - final ] or adm .",
    "the denominator term of equation   is the expected mean squared - error of the ideal estimator @xcite .",
    "therefore , @xmath192 , and a smaller @xmath193 implies a more accurate estimator .    , computed as in equation   which measures the accuracy of the approximated dantzig selectors , for algorithm  [ alg : matrix - final ] and adm for noise levels @xmath194 and @xmath195 in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means . ,",
    "title=\"fig:\",scaledwidth=48.0% ] , computed as in equation   which measures the accuracy of the approximated dantzig selectors , for algorithm  [ alg : matrix - final ] and adm for noise levels @xmath194 and @xmath195 in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means .",
    ", title=\"fig:\",scaledwidth=48.0% ]    , computed as in equation   which measures the accuracy of the approximated dantzig selectors , for algorithm  [ alg : matrix - final ] and adm for noise levels @xmath194 and @xmath195 in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means . ,",
    "title=\"fig:\",scaledwidth=48.0% ] , computed as in equation   which measures the accuracy of the approximated dantzig selectors , for algorithm  [ alg : matrix - final ] and adm for noise levels @xmath194 and @xmath195 in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means .",
    ", title=\"fig:\",scaledwidth=48.0% ]    the effects of stage - ii of algorithm  [ alg : matrix - final ] and the postprocessing step of adm are illustrated in figure  [ accuracy plot med noise ] .",
    "the figure displays values of the exact simulated vector @xmath67 and of the dantzig selector @xmath154 approximated by each algorithm , first without performing postprocessing ( the left column of figure  [ accuracy plot med noise ] ) and then with postprocessing ( the right column of figure  [ accuracy plot med noise ] ) for one simulation with parameters @xmath190 and noise @xmath196 .",
    "one can clearly see that the postprocessing not only corrects the underestimated magnitudes of nonzero components of the estimates , but also eliminates unwanted nonzero components .     and",
    "adm for noise levels @xmath194 and @xmath195 as in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means.,title=\"fig:\",scaledwidth=48.0% ]   and adm for noise levels @xmath194 and @xmath195 as in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means.,title=\"fig:\",scaledwidth=48.0% ]     and adm for noise levels @xmath194 and @xmath195 as in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means.,title=\"fig:\",scaledwidth=48.0% ]   and adm for noise levels @xmath194 and @xmath195 as in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means.,title=\"fig:\",scaledwidth=48.0% ]     and adm for noise levels @xmath194 and @xmath195 in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means.,title=\"fig:\",scaledwidth=48.0% ]   and adm for noise levels @xmath194 and @xmath195 in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means.,title=\"fig:\",scaledwidth=48.0% ]     and adm for noise levels @xmath194 and @xmath195 in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means.,title=\"fig:\",scaledwidth=48.0% ]   and adm for noise levels @xmath194 and @xmath195 in example  3.1 . in each plot , the points along the curve represent the mean number of iterations required for each parameter @xmath170 over 100 simulations , and the points on the vertical lines represent one standard deviation away from the means.,title=\"fig:\",scaledwidth=48.0% ]    the results of the above simulations suggest that algorithm  [ alg : matrix - final ] has less overall complexity than adm , since the accuracy of the dantzig selectors approximated by each method are similar yet algorithm  [ alg : matrix - final ] completes much faster than adm , even when requiring more iterations .",
    "figure  [ fig : accuracy ] displays the mean and standard deviation of @xmath193 over 100 simulations for each parameter @xmath170 and @xmath171 and for both algorithm  [ alg : matrix - final ] and adm . note that the accuracy of the dantzig selector approximated by the two algorithms are very similar across all parameter levels .",
    "figure  [ fig : time ] displays the mean and standard deviation of the cpu time , and figure  [ fig : iterations ] displays the mean and standard deviation of the total number of iterations performed by algorithm  [ alg : matrix - final ] and the total number of iterations performed in the inner loop of adm for 100 simulations for each parameter @xmath170 and @xmath171 . from the figures , one can see that although algorithm  [ alg : matrix - final ] requires more iterations than adm , algorithm  [ alg : matrix - final ] completes significantly faster .",
    "leukemia data set    in this experiment , the dantzig selectors produced by algorithm  [ alg : matrix - final ] and by adm are used with a collection of biomarker data to indicate whether a patient may be diagnosed with a specific type of cancer .",
    "the biomarker dataset , first introduced in  @xcite and studied in  @xcite , contains the measurements of 7128 genes related to leukemia diagnoses .",
    "the dataset is split into a training set and a testing set .",
    "the training set is sampled from 38 patients , 27 of whom were diagnosed with acute lymphocytic leukemia ( all ) and 11 with acute mylogenous leukemia ( aml ) .",
    "the testing set is sampled from 34 patients , 20 diagnosed with all and 14 with aml .",
    "let @xmath197 contain the biomarker data in the training set , where each row is all  7128 gene measurements of a single patient and each column has been normalized to have unit @xmath12 norm .",
    "let @xmath198 be the column vector indicating the diagnosis of each patient in the training set : @xmath199 similarly define @xmath200 and @xmath201 from the data in the testing set .",
    "this experiment has a training phase and a testing phase . in the training phase , a sparse vector @xmath154",
    "is found such that @xmath202 . to preprocess the data , only the biomarkers with the largest variance are used to train the parameter @xmath154 . to this end , select a positive integer @xmath203 , and let @xmath139 be the @xmath203 indices of columns from @xmath204 with largest variance .",
    "let @xmath205 be the submatrix of @xmath204 with columns in @xmath139 .",
    "form the reduced problem @xmath206 the dantzig selector @xmath207 satisfying problem   is computed using algorithm  [ alg : matrix - final ] and adm , then extended to form @xmath208 via @xmath209    in the testing phase , the trained parameter @xmath154 is used to predict the diagnoses of patients in the testing set .",
    "the predictive indicator vector @xmath210 is computed from @xmath211 by thresholding and clustering values near the threshold boundary .",
    "set @xmath212 let @xmath213 and @xmath214 . for values of @xmath215 such that @xmath216 , set @xmath217 the @xmath218 patient in the testing set is predicted to have a diagnosis of all if @xmath219 and a diagnosis of aml if @xmath220 .",
    "0.48   and adm .",
    ", title=\"fig : \" ]    0.48   and adm .",
    ", title=\"fig : \" ]    0.48   and adm .",
    ", title=\"fig : \" ]    0.48   and adm . ,",
    "title=\"fig : \" ]    the above procedure was used to predict the diagnoses of patients in the testing set using the dantzig selector @xmath140 computed using both algorithm  [ alg : matrix - final ] and adm with parameters @xmath221 , @xmath222 and @xmath223 and stopping criteria parameters @xmath224 and @xmath175 for each @xmath81 in @xmath225 .",
    "figure  [ fig : leukemia ] displays the results of these simulations regarding the accuracy of the recovered indicator vector @xmath226 in predicting the leukemia diagnoses of patients in the testing set , as well as the number of iterations and cpu runtime used by algorithm  [ alg : matrix - final ] and adm . as shown in figure  [ fig : leukemia](a ) , algorithm  [ alg : matrix - final ] typically predicted the diagnoses of patients with higher acuracy than adm .",
    "moreover , for each parameter @xmath81 , algorithm  [ alg : matrix - final ] used fewer iterations than adm and the time used by algorithm  [ alg : matrix - final ] was several orders of magnitude less than the time used by adm , as shown in figures  [ fig : leukemia](c ) and ( d ) .",
    "figure  [ fig : leukemia](b ) illustrates the tendency of algorithm  [ alg : matrix - final ] to predict the diagnosis of patients in the testing set with higher accuracy than adm .",
    "this plot displays the values of @xmath227 recovered using algorithm  [ alg : matrix - final ] and by adm prior to the thresholding step , along with the true values of @xmath228 .",
    "since the values recovered by algorithm  [ alg : matrix - final ] tend to be more spread out , it is easier to accurately separate them into two distinct clusters .",
    "in this paper , we have developed an iterative algorithm to compute the dantzig selector , the solution to the minimization problem in problem .",
    "the algorithm is based on the proximity operator and its relationship to problem .",
    "the two - stage algorithm we proposed is an improvement over some other recently proposed methods to find the dantzig selector , which require the use of inner loop to estimate parameters within each step of the algorithm .",
    "additionally , our proposed method uses a novel stopping criterion based upon the support of the approximated parameters .",
    "we compare the proposed algorithm to the alternating direction method proposed in  @xcite .",
    "theoretically , two methods produce results of similar quality , however each iteration of stage - i of algorithm  [ alg : matrix - final ] has less computational complexity than each iteration of the inner loop of the alternating direction method .",
    "the numerical experiments demonstrate that the proposed method and the alternating direction method typically approximate the dantzig selectors with similar accuracy , yet algorithm  [ alg : matrix - final ] produces results in significantly less time , whether it uses more iterations than the alternating direction method , as in experiment  3.1 , or fewer iterations than the alternating direction method , as in experiment  3.2 .",
    "the authors are grateful to the anonymous reviewers for their helpful comments .",
    "the authors also would like to thank drs .",
    "x.  wang and x.  yuan for providing the matlab code to approximate the dantzig selector using the alternating direction method and for sharing the real dataset used in example 3.2 ."
  ],
  "abstract_text": [
    "<S> in this paper , we study a simple iterative method for finding the dantzig selector , which was designed for linear regression problems . </S>",
    "<S> the method consists of two main stages . </S>",
    "<S> the first stage is to approximate the dantzig selector through a fixed - point formulation of solutions to the dantzig selector problem . </S>",
    "<S> the second stage is to construct a new estimator by regressing data onto the support of the approximated dantzig selector . </S>",
    "<S> we compare our method to an alternating direction method , and present the results of numerical simulations using both the proposed method and the alternating direction method on synthetic and real data sets . </S>",
    "<S> the numerical simulations demonstrate that the two methods produce results of similar quality , however the proposed method tends to be significantly faster .    </S>",
    "<S> * key words : * dantzig selector , proximity operator , fixed - point algorithm , alternating direction method </S>"
  ]
}