{
  "article_text": [
    "discretized differential equations lie at the heart of many simulation algorithms in physics .",
    "a large variety of solution algorithms like conjugate gradient , overrelaxation , or multigrid exist to deal efficiently with such problems @xcite .",
    "the convergence of these algorithms usually depends on the condition number of the problem operator , i.e.  the quotient of its largest and smallest eigenvalue .",
    "( for many simple problems multigrid methods will always converge well .",
    "here we are not interested in such cases . )",
    "when the number of eigenmodes with very small eigenvalues is not large , each of these methods could be accelerated if an additional method for dealing with these modes would be applied .    in this paper",
    "we want to study a method that can be used to do exactly this .",
    "it is partly based on the multigrid idea and relies on a surprisingly simple principle , called the _ principle of indirect elimination _ or _",
    "pie_.    we will explain this principle in general context and then apply it to a case where the occurence of almost - zero modes spoils the convergence of standard methods , namely the dirac equation in a gauge field background with instantons @xcite .",
    "we will also show connections to an idea by kalkreuter somewhat similar in spirit , called the _ updating on the last point _ @xcite , and explain why our method is more general . finally , we will briefly remark on the connections to the iteratively smoothing unigrid algorithm @xcite .",
    "consider a linear operator  @xmath0 which may arise from a discretized differential equation . here and in the following",
    "we assume  @xmath0 to be _ positive definite _ , if it were not , we could use the operator  @xmath1 instead .",
    "the general form of the equation to be solved is then @xmath2 let us call the lowest eigenvalue of the operator  @xmath3 .. in section  [ dual ] we will explain what is meant by such a statement . ]",
    "its value determines the _ criticality _ of the operator because the smaller it is the larger the condition number ( quotient of largest and smallest eigenvalue ) of the operator will be .",
    "if @xmath4 the problem is ill - posed because the contribution of this zero - mode to the solution is not determined . for small @xmath3 standard iterative methods will converge only slowly , the convergence time  @xmath5 ( the number of iterations needed to reduce the error by a factor of  e ) behaving like @xmath6 , where @xmath7 is the condition number of @xmath0 and @xmath8 is the critical exponent .",
    "this behaviour is called _ critical slowing down _ because the more critical the problem gets the slower the algorithm will be . for relaxation methods ,",
    "one usually finds @xmath9 , conjugate gradient has a critical exponent of @xmath10 .",
    "an optimal algorithm should have a critical exponent of  0 .    at each time - step",
    ", any iterative method will yield an approximate solution  @xmath11 .",
    "we introduce two important quantities : the _ error _  @xmath12 which is the difference between the true and the actual solution and is of course not known , and the _ residual _",
    "@xmath13 , the difference between the true and the actual righthandside . with these definitions we can recast the fundamental equation  ( [ fundamental_eq ] ) as @xmath14 called the _ error equation_.    for a linear method",
    ", we can also introduce the _ iteration matrix _",
    "@xmath15 which tells us what the new error after the next iteration step will be , given the old one : @xmath16 the concrete structure of the iteration matrix is irrelevant for the following discussion , see @xcite for examples .",
    "the important point here is that the iteration matrix is the reduction matrix for the error .",
    "its eigenvalues should lie between minus one and one and convergence is governed by the eigenmode of  @xmath15 with absolute value of the eigenvalue closest to one .    in the following sections we will usually assume the algorithm to be linear because the existence of an iteration matrix eases the analysis .",
    "nevertheless the method presented here could be applied to the conjugate gradient algorithm as well , see also section  [ instantons ]      for the analysis it is important to distinguish between a vector space and its dual @xcite . the differential operator  @xmath0 maps a vector @xmath17 to a vector in the dual space @xmath18 . to see this ,",
    "consider the laplace equation in electrodynamics as an example : @xmath19 .",
    "the laplace operator maps a potential onto a charge density .",
    "these two objects can be regarded as dual vectors because there is a unique way of assigning a real number to them , namely the energy @xmath20 .",
    "the laplace operator therefore provides us with a bilinear form @xmath21 .",
    "however , there is no natural identification between the vector space and its dual besides that given by this scalar product .",
    "we will later see an example where one is easily drawn to wrong conclusions if this distinction is not taken into account .",
    "it is not really meaningful to speak about eigenvectors or -values of bilinear forms . on the other hand",
    ", the iteration matrix of relaxational methods maps the error to another error and is therefore a map @xmath22 , possessing eigenvectors .",
    "it are the eigenvalues of this matrix that determine the convergence .",
    "the standard identification of eigenvectors of @xmath15 with eigenvectors of @xmath0 is done using additional structure .",
    "this is given by the matrix @xmath23 which is defined through the relation @xmath24 .",
    "( standard relaxation methods arise from splitting the fundamental operator @xmath25 , where @xmath23 is chosen such that it approximates @xmath0 as good as possible but is `` easy to invert '' . )",
    "@xmath23 is an additional bilinear form and furnishes us with a scalar product in addition to the scalar product given by @xmath0 .    for conjugate gradient ,",
    "the situation is similar : conjugate gradient updating steps require computations of scalar products , e.g.  @xmath26 , where @xmath27 is the search vector . here",
    "we need another scalar product than the @xmath0-product .",
    "it is therefore only correct to speak of eigenvectors of @xmath0 when we have chosen a basis that is in some sense natural .",
    "for example , if we use the standard site - wise basis and find that the eigenvectors of @xmath0 in this basis agree with those of @xmath15 , the sloppy way of speach is justified",
    ". this will be the case for the example we will study below .",
    "nevertheless , in the theoretical parts of this paper we will be more strict .",
    "after these preliminaries we formulate the    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * principle of indirect elimination ( pie ) : * it is easier to calculate the shape of a bad - converging mode for a certain algorithm than to reduce it directly using this algorithm .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    to see this , consider the case where there is only one bad - converging mode and all others are reduced efficiently by the algorithm .",
    "we now use the algorithm to _ try _ to solve an equation of which we already know the solution , for example the equation @xmath28 . in this case",
    "we have @xmath29 , so we know the error as well . remembering equation  ( [ itererror ] ) we see that we can now directly investigate how the iteration matrix acts .",
    "after  @xmath30 iterations we have @xmath31 where @xmath32 is the initial guess we started with and @xmath33 is the approximate solution after the @xmath30-th iteration . for @xmath34 @xmath35 projects onto the eigenvector of @xmath15 with the largest absolute value of the eigenvalue , which is the slowest - converging mode . for finite @xmath30",
    "the accuracy of the projection depends on quotient between the largest and the second - largest eigenvalue : the larger this is , the better the projection will be .",
    "( this can be seen easily by imagining @xmath15 to be diagonalized . ) in the model case considered here , where there is only one bad - converging mode , this quotient will be large and so @xmath36 will converge rapidly against the bad - converging mode .",
    "if the number of bad - converging modes is larger than one , but still small , we can use the same technique to calculate them if we take care of orthogonalizing the approximations to the already known modes . by this",
    "it is obvious that this method will only be useful if this number is not too large , otherwise the calculations will take too much time .",
    "we will later comment on how the principle of indirect elimination can be applied locally and used to construct a multigrid algorithm .",
    "let us come back to the case of only one bad - converging mode .",
    "if we have calculated this using the principle of indirect elimination , how can we apply this knowledge to improve convergence ?",
    "the answer relies on multigrid ideas and is in fact very simple .",
    "let us call the bad - converging mode @xmath37 .",
    "we define an operator @xmath38 that creates a vector on the fundamental lattice from a number .",
    "this cumbersome notation has a two - fold purpose : first it stresses the similarity to multigrid ideas , where @xmath39 would be called an interpolation operator , second it will later allow us to study the case where @xmath39 is not exactly equal to the bad - converging mode @xmath37 to see how this will affect the convergence .    to solve the inhomogenuous equation , we first apply our standard iterative solver a few times .",
    "this will reduce all components of the error appreciably except for a part proportional to  @xmath37 : @xmath40 . inserting this knowledge into the error equation  ( [ error_eq ] ) or using the fact that @xmath41 we get @xmath42 in other words ,",
    "we have transformed the fundamental equation , living on a large lattice , into an equation for scalars ( or simple matrices in the case of a gauge theory , see below ) .",
    "this new equation can be considered to live on a lattice with only _",
    "one point_. in multigrid language this is often called the `` last - point lattice '' as we have there a whole tower of coarser and coarser lattices of which the last consists of only one point .",
    "the equation on the last point can be solved easily to get  @xmath43 and afterwards we correct our approximation : @xmath44 .",
    "thus we have reduced that part of the error corresponding to  @xmath39 .",
    "it is well - known from the multigrid context that using the largest mode of  @xmath15 as interpolation operator will yield the best convergence ( greenbaum criterion @xcite ) .",
    "if the iterative method used before has not been perfect , i.e.  if the error still contains contributions from other modes , we now have to start the iteration again to act on the remaining parts",
    ". this may again introduce error - components proportional to @xmath39 which are then reduced by another `` last - point updating '' .",
    "we can now understand the reason why the principle has been called principle of indirect elimination : direct elimination of the bad - converging mode using the iterative solver does not work efficiently , but an indirect approach , first trying to solve an auxilliary equation and only afterwards addressing the real problem , works fine .    in practice the situation will not be the idealized one described above .",
    "we now want to study two situations : what will the result of the correction be when the error is not an exact multiple of the zero - mode  @xmath37 , and what happens when @xmath39 deviates from @xmath37 ?    in the first case it is easy to prove that after the correction @xmath11 and @xmath39 will be @xmath0-orthogonal even if the error before was not a multiple of @xmath39 , but contained an additional contribution @xmath45 : @xmath46 @xmath47 @xmath48 @xmath49 @xmath50 @xmath51 now we want to investigate the second question , namely how well the approximation of the zero - mode has to be . to do so we can prove the following rather trivial    we have an algorithm consisting of two parts .",
    "the first part is able to eliminate completely all components of the error except one single mode  @xmath37 , so we have @xmath52 .",
    "the second updating then consists of an updating on the last point as described above using an approximation @xmath39 of @xmath37 .",
    "we can split the bad - converging mode  @xmath37 into two @xmath0-orthogonal parts : @xmath53 then the iteration matrix @xmath54 of the full algorithm consisting of both steps has the ( squared ) energy norm ( with respect to  @xmath0 ) @xmath55    the energy norm is defined as @xmath56 let @xmath15 be the iteration matrix of the first part of the algorithm . as it eliminates all parts of an arbitrary error except the mode  @xmath37 , it is clear that the supremum in the definition will be reached for  @xmath57 . @xmath58",
    "does not affect  @xmath37 .",
    "after the iteration only that part of @xmath37 that is @xmath0-orthogonal to @xmath39 will remain , see the calculation above .",
    "so we get @xmath59 .",
    "thus we have @xmath60    it is also useful to look at this geometrically : the angle  @xmath61 between the vector  @xmath37 and @xmath62 with respect to the scalar product defined by  @xmath0 is given by @xmath63 the reduction works by first projecting @xmath37 onto the direction given by @xmath64 and then taking the @xmath0-orthogonal part of this .",
    "this orthogonal part is the vector  @xmath45 ; it is all that remains after the coarse - grid correction step .",
    "the length of this vector is given by @xmath65 .",
    "the reduction factor , which is equal to the norm of the iteration matrix , is  @xmath66 :    using pythagoras theorem we get @xmath67 and inserting the split of the vector @xmath37 and again using the orthogonality property , we finally arrive at @xmath68    what are the implications of this theorem ?",
    "first it must be understood that the energy norm of the iteration matrix will be equal to the spectral radius provided the matrix  @xmath58 is @xmath0-symmetric , i.e.  @xmath69 . as an endomorphism , not as a bilinear form , we would transform it using the wrong relation and _ loose _ the @xmath0-symmetry property after the transformation : we have @xmath70 . now",
    "if we transform both using the transformation for endomorphisms , we get @xmath71 ! here there is no cancellation as it would be with the correct transformation : @xmath72 .",
    "only if @xmath73 is orthogonal or unitary do we get the same cancellation .",
    "] this is true when the operator  @xmath0 does not mix the mode  @xmath37 with the other modes .",
    "we can then regard @xmath37 as an eigenmode of @xmath0 because the matrix  @xmath58 provides us with an identification of the mode @xmath37 with the corresponding mode in the dual space .",
    "hence the energy norm directly tells us about the convergence rate of the algorithm .",
    "@xmath15 will also be @xmath0-symmetric for standard iterative methods like jacobi - relaxation and can be made so as well for sor or gau - seidel relaxation .    by choosing  @xmath37 as the zero - mode the theorem shows how important the correct treatment of this mode is .",
    "the closer the range of the interpolation operator @xmath64 is to the zero - mode and the smaller the energy norm of the residual part @xmath45 the better the convergence will be .",
    "( in the limiting case where the two are identical , the difference vector is zero and the error of the zero - mode is eliminated perfectly , as expected . )",
    "it is not only important that the zero - mode is approximated well by the interpolation operator on the last point , the convergence will also be better when the difference vector between zero - mode and the mode used for the interpolation has a small energy norm and is as smooth as possible .",
    "the theorem also serves to explain a finding by kalkreuter @xcite .",
    "he found that it is possible to eliminate critical slowing down in a multigrid algorithm ( actually a two - grid ) for the standard laplace equation even with interpolation operators that are not able to represent the zero - mode ( which is a constant in this case ) exactly , but only approximately . in the light of our theorem",
    "this could be understood if the difference vector has a small energy norm .",
    "this , however , has not been tested .",
    "thus we have seen the importance of the correct treatment of the zero - mode .",
    "other methods to remove convergence problems caused by the zero - mode can be thought of .",
    "kalkreuter @xcite proposed a simple rescaling of the approximate solution  @xmath11 in addition to a multigrid or a relaxation algorithm to improve the convergence .",
    "this method completely eliminates critical slowing down in the simplest model problem , the laplace equation on a two - point grid",
    ". the rescaling amounts to using the approximate solution @xmath11 itself as interpolation operator  @xmath64 .",
    "the motivation for this updating scheme can be found in the following argument : consider again the equation @xmath74 .",
    "solving this gives @xmath75 where we have inserted a unit matrix .",
    "let us fix the righthandside and increase the criticality of the problem .",
    "the more critical it gets the smaller the lowest eigenvalue @xmath3 of @xmath76 will be .",
    "the solution @xmath77 will then have larger and larger contributions from the lowest eigenmode of @xmath76 .",
    "therefore @xmath77 itself will be a good approximation to the bad - converging mode and can be used as interpolation operator .",
    "we see that this method is very similar to the principle of indirect elimination .",
    "however , the principle of indirect elimination will always provide us with an approximation to the zero - mode without any contribution from a given righthandside , whereas kalkreuter s method will only work well for large criticality : the interpolation operators used by the methods are @xmath78 for the principle of indirect elimination and @xmath79 for kalkreuter s method .",
    "even more important , the principle of indirect elimination can be used several times to remove more than just one mode , this is impossible with the other approach . on the other hand for large criticality and the case of only one bad - converging mode , kalkreuter s method has the advantage of not needing auxilliary iterations to calculate @xmath39 because @xmath11 is used .",
    "kalkreuter used this method in addition to a usual multigrid or relaxation method .",
    "he found that there is no strong improvement for a multigrid algorithm , but for standard local relaxation the asymptotical critical slowing down ( i.e.  critical slowing down for fixed grid size and infinitely many iterations ) was eliminated for the laplace equation with periodic boundary conditions .",
    "this is what we expect for a method that treats the lowest mode of the problem correctly because in this case it is the eigenvalue of the second - lowest mode that determines the convergence and this scales with the size of the grid , not with the lowest eigenvalue .",
    "so for increasing grid sizes critical slowing down should still be present ; this in agreement with kalkreuter s results .",
    "our example for a discretized differential equation with a small number of bad - converging modes is taken from theoretical high - energy physics , namely the two - dimensional dirac equation on the lattice in a gauge field background with periodic boundary conditions .",
    "for an introduction to lattice gauge theory consult @xcite .",
    "here we only present the framework : consider a regular , @xmath80-dimensional ( hyper-)cubic lattice @xmath81 with lattice constant  @xmath82 , lattice points  @xmath8 and directed links  @xmath83 .",
    "the opposite link is then denoted by @xmath84 , where @xmath85 means the next neighbour of @xmath8 in @xmath86-direction .",
    "the direction index  @xmath86 runs from @xmath87 to @xmath80 .",
    "usually , the lattices used will be finite with an extension of @xmath88 points in each dimension so that the number of degrees of freedom is  @xmath89 .",
    "a lattice gauge theory is defined by a gauge group  @xmath90 which might for example be u(1 ) or su(2 ) .",
    "elements of the gauge group act on a vector space  @xmath91 which for the examples above would be @xmath92 and @xmath93 , respectively .",
    "the computations presented below were done in two dimensions with gauge group u(1 ) , so that instantons can occur . a lattice gauge field ( in this case )",
    "assigns a u(1)-``matrix '' @xmath94 ( which is simply a phase ) to every link of the lattice , subject to the condition @xmath95 .",
    "these matrices are distributed randomly with a boltzmannian probability distribution @xmath96 , where @xmath97 is the standard wilson action of lattice gauge theory @xmath98 for a plaquette  @xmath99 of the lattice with links @xmath100 at its boundary .",
    "this distribution leads to a correlation between the gauge field matrices with finite correlation length @xmath101 for finite @xmath102 .",
    "the case @xmath103 corresponds to a completely random choice of the matrices ( @xmath104 ) , for @xmath105 all matrices are @xmath106 ( @xmath107 ) . in this sense , @xmath102 is a disorder parameter , the smaller @xmath102 the shorter the correlation length and the larger the disorder .",
    "the dirac operator acts on matter fields  @xmath77 living on the nodes of the lattice . in kogut - susskind formulation",
    "@xcite it is defined as @xmath108 here the  @xmath109 are the remnants of the dirac matrices  @xmath110 in the continuum .    as the dirac operator itself is not positive definite , we will use its square  @xmath111 in the following .",
    "the squared dirac has the property of totally decoupling the even and odd parts of the lattice ; if we color the lattice points in checkerboard fashion , any red point is only coupled to other red points , so that we can restrict our attention to one of the sub - lattices",
    ". this will be especially useful because it lifts the degeneracy of the eigenvalues : usually each eigenvalue of the dirac operator is degenerated twice , but we can choose the eigenvectors to live separated on the sub - lattices .",
    "for the sake of brevity we will generally speak of the dirac operator even when we mean the squared dirac .",
    "many algorithms for solving the dirac equation become problematic in the presence of instantons .",
    "instantons are gauge field configurations that are topologically non - trivial but possess zero energy .",
    "such configurations are only possible for certain choices of the dimension and the gauge group , in two dimensions instantons can occur when the gauge group is u(1 ) , see @xcite for an introduction .",
    "the atiyah - singer theorem states that at instanton charge  @xmath112 the spectrum in the continuum will possess @xmath113 exact zero - modes  @xcite ; these become modes with extremely small eigenvalues on the lattice @xcite . for the purpose of this paper",
    "it is not necessary to have an understanding of what an instanton is , it is only important that they are special gauge field configurations giving rise to almost - zero - modes on the lattice .",
    "figure  [ qtopspectrum ] shows the lower part of the spectrum of the squared dirac operator on an @xmath114-lattice at @xmath115 for different instanton charges  @xmath112 , taking only one of the two sublattices into account .",
    "clearly the atiyah - singer theorem is nicely reflected on the lattice .",
    "when instantons are present , the condition number of the dirac operator becomes very large and the problem is ill - posed . in @xcite",
    "this problem is investigated in detail for the parallel transported multigrid . in this section ,",
    "we want to use the principle of indirect elimination to show how an algorithm which converges well in the absence of instantons can be adapted to a case with instantons .",
    "the idea is very simple : if  @xmath116 bad - converging modes are present , use the principle of indirect elimination @xmath116 times to calculate approximations @xmath117 to these modes .",
    "the method presented here could be applied to a conjugate gradient algorithm . for this algorithm",
    ", dilger @xcite has found that the number of iterations needed strongly increasses with the instanton charge , therefore conjugate gradient would benefit from the application of the method described here .",
    "however , we will choose the isu algorithm on small lattices and at quite large values of  @xmath102 for a u(1 ) gauge field as an example .",
    "this algorithm has been described in detail in  @xcite . as it is in some parts based on the principle of indirect elimination",
    ", some remarks will be made on this method in the next section .",
    "for this section it is not necessary to understand how isu works , it suffices to know that for the parameters chosen the isu algorithm converges well for instanton charges 0 or @xmath118 at large @xmath102 but badly for larger instanton charges .",
    "the reason is that the algorithm in its standard form contains one interpolation operator on the last point ( which is calculated as an approximation to the zero - mode ) and so it is able to eliminate one zero - mode , but not more .    in the improved algorithm one",
    "tries to solve the equation @xmath119 with the given algorithm .",
    "as it eliminates all other modes quickly the approximate solution will converge against a linear combination of the bad - converging modes .",
    "then we start the procedure again , but now orthogonalizing the approximate solution to the interpolation operator we already know , doing this successively for all  @xmath116 bad - converging modes .",
    "( as the instanton charge can be easily measured , one usually knows beforehand how many operators are needed ; if one does not for some reason , a dynamical approach can be chosen : simply proceed calculating the next interpolation operator until the convergence rate of the trivial equation becomes good enough . )",
    "the overall work for this procedure is proportional to the square of the number of bad - converging modes , as is the work of actually applying the interpolation operators to eliminate them .",
    "( the number gets squared because of the need to calculate an effective operator on the last point layer .",
    "however , the effective operator only needs to be calculated once for each configuration . )",
    "this restricts the method to cases where the number of bad - converging modes is not too large , which usually is the case for instanton charges .",
    "figure  [ killingpic ] shows the performance of the usual isu method compared to the improved version for the dirac operator in a u(1 ) gauge field with different instanton charges .",
    "we measured the asymptotic convergence time , i.e.  the number of iterations asymptotically needed to reduce the error by a factor of  e. the improved version of isu used a number of interpolation operators on the last point equal to the instanton charge which equals the number of bad - converging modes .",
    "the data were generated on one sub - lattice of an @xmath114-lattice at @xmath115 .",
    "for this high value of @xmath102 , the convergence in the absence of instantons is good , as can be seen from the value at @xmath120 .",
    "the standard method works well for instanton charge  0 or 1 , as explained above , and its sensitivity to higher instanton charges is striking .",
    "the improved method shows no dependence on the instanton charge , the convergence is good in all cases .",
    "note also that the standard deviation is much higher for the usual method because it is affected by fluctuations in the eigenvalues of the bad - converging modes .",
    "clearly the improved method is superior  the cost of calculating the instanton modes is about 10  iterations for each instanton plus the cost of the orthogonalization , whereas the saving in the solution of the final equations is of the order of hundreds of iterations depending on how much we want to reduce the residual .",
    "we have seen above that the principle of indirect elimination will only be helpful when the number of bad - converging modes is not too large . in the case of simple relaxation methods ,",
    "however , this number is of order @xmath121 , so storing them would cost @xmath122 , where @xmath30 is the number of degrees of freedom in the system .",
    "so it seems that the method is useless in such cases . in this section",
    "we want to explain how the iteratively smoothing unigrid algorithm ( isu ) presented in @xcite can be regarded as the local application of the principle of indirect elimination .",
    "we will only present the basic idea here .",
    "some familiarity with the basic multigrid idea is assumed in this section , see @xcite for introductions .",
    "we can associate a length scale ( e.g.  a wavelength ) with each modes of our system . because they are local",
    ", relaxation methods eliminate all those modes corresponding to a length scale of the order of one lattice spacing .",
    "usually there will be @xmath123 of these .",
    "of the remaining modes @xmath124 will be associated with length scale @xmath125 ( @xmath82 is the lattice spacing ) , @xmath126 with scale @xmath127 and so on .",
    "so there will be many bad - converging modes with a small length scale and only a few corresponding to a large scale .",
    "the isu algorithm is a method to calculate interpolation operators that are able to span the space of these modes .",
    "these operators are restricted to parts of the lattice , the size of the domain being determined by the scale of the mode that is to be approximated by the operator .    to be more specific ,",
    "let us start with the smallest scale @xmath125 .",
    "as in usual multigrid methods , we divide the hypercubic lattice into ( overlapping ) hypercubes or blocks @xmath128 $ ] of side length  3 .",
    "then we try to solve the equation @xmath129}}{{\\cal a}}_x^{[1]}(z ) = 0 $ ] using a relaxation method . here",
    "@xmath129}}$ ] is the restriction of @xmath0 to the block @xmath128 $ ] using dirichlet boundary conditions .",
    "what remains after a few iterations will be the slowest - converging mode on this scale and can therefore be used as interpolation operator on the first block - lattice . repeating this for all the small hypercubes , we know the shapes of the bad - converging modes on scale @xmath125 .",
    "now we do the same on the next scale , dividing the lattice into larger blocks ( of side length  7 , agreeing with the formula @xmath130 ) .",
    "again we try to solve the equation @xmath131}}{{\\cal a}}_x^{[2]}(z ) = 0 $ ] , where @xmath128 $ ] now denotes the larger blocks .",
    "the important point is that we use the interpolation operators on the smaller scale that are already known for this calculation to eliminate contributions from the bad - converging modes on the smaller scale . in this way",
    "we proceed to larger and larger hypercubes , always using the interpolation operators already known .",
    "this method would only fail if a large number of bad - converging modes lived on a large length - scale .",
    "it has been found that this algorithm is able to eliminate critical slowing down completely for the case of the two - dimensional laplace equation in an su(2 ) gauge field background at arbitrarily large values of the gauge field disorder and the lattice size .",
    "an improved version has been shown to do the same for the two - dimensional squared dirac equation , except for extremely large disorder ( @xmath132 or smaller ) .",
    "see @xcite for details .",
    "an idea that is similar in spirit to the principle of indirect elimination has been discussed in @xcite .",
    "brandt proposes to do relaxations on the fundamental lattice with arbitrary starting vectors to determine `` typical shapes of a slow - to - converge error '' which could then be used to determine good multigrid interpolation operators .",
    "unfortunately , this idea suffers from a severe disease : the number of modes that converge badly under simple relaxation is huge ( about half of the number of grid points ) .",
    "what one will get by this procedure is a mixture of low - lying eigenmodes with contributions depending on their eigenvalues .",
    "the time needed to arrive at a function that consists only of the lowest eigenmodes will be proportional to the lattice size , so the method will not work without critical slowing down .",
    "the difference to the isu algorithm is that this is a so - called _ unigrid _ method .",
    "it allows for interpolation operators living on different length - scales , whereas standard multigrid algorithms only use interpolation operators living on small domains . on each length scale",
    "we need not represent all modes that converge badly on this and on all higher scales ; only the modes that belong to the scale corresponding to a certain lattice constant have to be dealt with .",
    "the next - coarser length - scale will then take care of the modes corresponding to this scale and in their computation the smaller scales are already taken into account .",
    "we have presented a simple method to improve the convergence of solution algorithms for discretized differential equations when the number of bad - converging modes is small .",
    "the principle of indirect elimination used to do this is based on the general idea that an algorithm can be used to identify its own bad - converging modes .",
    "conceptionally , the method is similar to the general idea of accelerating algorithms described in @xcite : one tries to find out what the slow modes of the algorithm are and uses this knowledge to improve the algorithm .",
    "for example , multigrid methods are based on the fact that the slow modes are the smooth modes that can be obtained by smooth interpolation .",
    "the principle of indirect elimination serves to automatize this process in the case when the number of slow modes is small so that it suffices to know them without doing further analysis of their structure .",
    "the method has been studied for the case of the dirac equation in a gauge field background with instantons and worked extremely well . applying it locally leads to a multigrid method called the iteratively smoothing unigrid .",
    "i wish to thank gerhard mack and alan sokal for stimulating discussions .",
    "hermann dilger provided me with a copy of his program for generating u(1 ) gauge field configurations .",
    "financial support by the deutsche forschungsgemeinschaft is gratefully acknowledged .",
    "99 m. atiyah , i. singer , ann .  math .  * 87 * , 1968 , 484 m. bker , int .  j.  mod",
    ".  phys .",
    "* c6 * , 1995 , 85 + m. bker , _ a multiscale view of propagators in gauge fields _ , ph .",
    "d. thesis , hamburg 1995 , desy 95 - 134 a. brandt , nucl .  phys .",
    "* b 26 * , ( proc.suppl . ) , 1992 , 137 a. brandt , _ multigrid techniques : 1984 guide with applications to fluid dynamics _ ,",
    "gmd  studie nr .",
    "85 w. l. briggs , _ a multigrid tutorial _ , siam , philadelphia 1987 m. creutz , _ quarks , gluons , and lattices _ , cambridge university press , cambridge 1983 h. dilger , int .",
    "j.  mod .",
    "* c 6 * , 1995 , 123 + h. dilger , nucl .",
    "* b 434 * , 1995 , 321 a. greenbaum , siam j. numer .",
    "21 * , 1984 , 657 w. hackbusch , _ multigrid methods and applications _ , springer series in computational mathematics 4 , 1985 m. harmatz , p. lauwers , s. solomon , t. wittlich , nucl .",
    "* b 30 * ( proc .  supp . ) , 1993 , 192 t. kalkreuter , phys .",
    "* d 48 * , 1993 , 1926 + t. kalkreuter , ph.d .",
    "thesis , preprint desy-92 - 158  a shortened version has appeared in int .",
    "j.  mod .",
    "* c 5 * , 1994 , 629 j. kogut , l. susskind , phys .",
    "* d 11 * , 1975 , 395 + t. banks , j. kogut , l. susskind , phys .",
    "rev .  * d 13 * , 1976 , 1043 + l. susskind , phys .",
    "* d 16 * , 1977 , 3031 + h. sharatchandra , h. thun , p. wei , nucl .",
    "* b 192 * , 1981 , 205 m. nakahara , _ geometry , topology and physics _ , bristol , uk : hilger , 1990 w. h. press , b. p. flannery , s. a. teukolsky , w. t. vetterling , _ numerical recipes _ , cambridge up , 1989 a.  sokal , phys .  lett .",
    "* b317 * , 1993 , 399 s. solomon , to appear in _ annual reviews of computational physics _ , + @xmath133bulletin board : hep-lat@ftp.scri.fsu.edu - 9411073@xmath134 r. varga , _ matrix iterative analysis _ , prentice hall , englewood cliffs , nj , 1962 d. young , _ iterative solutions of large linear systems _ , academic press , new york 1971"
  ],
  "abstract_text": [
    "<S> the principle of indirect elimination states that an algorithm for solving discretized differential equations can be used to identify its own bad - converging modes . </S>",
    "<S> when the number of bad - converging modes of the algorithm is not too large , the modes thus identified can be used to strongly improve the convergence . </S>",
    "<S> the method presented here is applicable to any standard algorithm like conjugate gradient , relaxation or multigrid . </S>",
    "<S> an example from theoretical physics , the dirac equation in the presence of almost - zero modes arising from instantons , is studied . using the principle , </S>",
    "<S> bad - converging modes are removed efficiently . applied locally , the principle is one of the main ingredients of the iteratively smooting unigrid algorithm .    </S>",
    "<S>  * s * # 1*#1 * # 1#2#1 , # 2    # 1 </S>"
  ]
}