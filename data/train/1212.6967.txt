{
  "article_text": [
    "our subject is entropic inference . the method of maximum entropy ( whether in its original maxent version or its generalization me , the method for updating probabilities ) has been successful in many applications but there are cases where it has either failed or led to paradoxes .",
    "it has been suggested that these are symptoms of irreparable flaws .",
    "i disagree .",
    "my assessment is considerably more optimistic : the paradoxes provide us with valuable opportunities for learning how to use the method and equally valuable warnings about pitfalls we can and should avoid .",
    "first , some background .",
    "the objective of the method of maximum entropy ( me ) is to update from a prior distribution @xmath0 to a posterior distribution when information is given that the posterior @xmath1 is constrained to belong to a certain family of distributions : @xmath2 .",
    "the selected posterior @xmath1 is that which maximizes the ( relative ) entropy @xmath3=-\\int dx\\,p(x)\\log \\frac{p(x)}{q(x)}~   \\label{s[pq]}\\]]subject to the constraint @xmath4 . justifying the method revolves to a large extent around justifying the particular choice of the functional @xmath5 $ ] .",
    "the criteria involved in designing the functional @xmath6 $ ] are purely pragmatic :    * ( 1 ) * we seek a method of universal applicability .",
    "it is conceivable that different situations could require different induction methods but _ _ what we want is a general - purpose method that captures what all those other problem - specific methods have in common . _ _ reflects an underlying measure or ( up to a normalization factor ) a uniform distribution .",
    "bayesian inference is recovered when the goal is to infer parameters @xmath7 on the basis of information about data @xmath8 and the relation between @xmath8 and @xmath7 as given by a known likelihood function @xmath9 .",
    "@xcite@xcite ]    * ( 2 ) * we want a parsimonious method that recognizes the value of information .",
    "what has been laboriously learned in the past should not be disregarded unless rendered obsolete by new information . _",
    "priors matter : rational beliefs should be updated but only to the minimal extent demanded by the new information .",
    "_    * ( 3 ) * the method must be useful in practice .",
    "in particular , in order to do science we must be able to understand parts of the universe without having to understand the universe as a whole .",
    "this implies that the notion of statistical independence must play a central and privileged role .",
    "this idea  that some things can be neglected , that not everything matters  is implemented by imposing a criterion that tells us how to handle independent systems . the design criterion",
    "we adopt is quite natural : _",
    "_ whenever two systems are a priori believed to be independent and we receive information about one it should not matter if the other is included in the analysis or not . _ _    the subtleties of how these criteria are implemented by imposing locality , coordinate invariance , and independence and the proofs showing that they lead to the entropy functional ( [ s[pq ] ] ) are discussed in caticha 2012 .",
    "a noteworthy feature is that it is not necessary to provide an interpretation for @xmath6 $ ] be it in terms of heat , or disorder , or amounts of information .",
    "entropy is a tool for updating that requires no further interpretation .",
    "the task of entropic inference  to update rational beliefs when information becomes available  immediately points both to the question what is information ?  and to its answer .",
    "a fully bayesian information theory demands a close relation between information and the beliefs of an ideally rational agent and , accordingly , the answer is : _ information is that which affects rational beliefs , and thus , constraints are information . _ which leads us to the main topic of this paper :  how to deploy constraints that correctly capture the information that is relevant to a problem . in the next section",
    "i argue that we ought to distinguish four epistemically different cases and that the failure to recognize the distinctions among them is a prime source of mistakes and paradoxes .    over the years",
    "a number of objections have been raised against the method of maximum entropy .",
    "i  believe some of these objections were quite legitimate at the time they were raised .",
    "they uncovered conceptual pitfalls with the old maxent as it was understood at the time .",
    "i also believe that in the intervening decades our understanding of entropic inference has evolved to the point that all these concerns can now be addressed satisfactorily .",
    "i explicitly discuss two examples of such mistakes .",
    "one concerns the dangers of replacing expected values with sample averages ( a good discussion of this so - called constraint rule  is _",
    "the other revolves around misunderstanding ignorance .",
    "these are objections of the type raised by the friedman - shimony paradox @xcite@xcite as it is manifested in the three - sided die problem @xcite@xcite and also in its original thermodynamic formulation .",
    "to fix ideas consider the standard maxent problem : to assign the probability of a discrete variable @xmath10 assuming a uniform underlying measure @xmath11 const and information in the form of a single linear constraint , @xmath12 .",
    "maxent requires us to maximize the shannon entropy @xmath13=-\\textstyle\\sum_{i } p_{i}\\log p_{i}$ ] subject to @xmath14 and @xmath15 which yields @xmath16 for an appropriately chosen @xmath17 .",
    "for example , the canonical distribution that describes the state of thermodynamic equilibrium is obtained maximizing @xmath18 $ ] subject to a constraint on the expected energy @xmath19 .",
    "this yields the boltzmann distribution , @xmath20 , where @xmath21 is the inverse temperature .",
    "the questions that concern us here are : how do we decide which is the right constraint function @xmath22 to choose ? how do we decide the numerical value @xmath23 of its expectation ? when can we expect the inferences to be reliable ?    when using the maxent method to obtain , say , the boltzmann distribution it has been common to adopt the following language :    * * : :    we seek the probability distribution that codifies the information we    actually have ( _ e.g. _ , the expected energy ) and is maximally unbiased    ( _ i.e. _ maximally ignorant or maximum entropy ) about all the other    information we do not possess .",
    "this justification has stirred considerable controversy .",
    "some of the objections that have been raised are the following :    * o1 * : :    objective reality is independent of our subjective knowledge about it .",
    "the observed spectrum of black body radiation is what it is    independently of whatever information happens to be available to us . * o2 * : :    in most realistic situations the expected value of the energy is not a    quantity we happen to know .",
    "how , then , can we justify using it as    information we actually have ? * o3 * : :    even when the expected values of some quantities happen to be known ,    there is no guarantee that the resulting inferences will be any good    at all .",
    "how can we justify the success of thermodynamics ?",
    "these objections deserve our consideration .",
    "the issue raised by * o1 * concerns the very essence of what physical theories are meant to be .",
    "let us grant for the sake of argument that there is such a thing as an external reality , that real phenomena are what they are independently of our thoughts about them .",
    "then the issue raised by * o1 * is whether the purpose of our theories is to provide _ models that faithfully mirror this external reality _ or whether the connection to reality is considerably more indirect and the _ models are pragmatic tools for manipulating information about reality _ for the purposes of prediction , control , explanation , etc . in the former case theories mirror reality and",
    "there is no logical room for subjectivity . in",
    "the latter case theories deal with _ our _ information about reality and some subjective elements are inevitable .",
    "the evidence in favor of the latter alternative is already considerable .",
    "it includes the successful derivation and a host of insights into statistical mechanics and thermodynamics achieved by jaynes maxent .",
    "it also includes the more recent entropic / bayesian derivations of both quantum and classical mechanics .    the objection * o1 * originates in a failure to recognize that while those epistemic judgments that must be made when assigning probabilities are inevitably subjective , they can nevertheless still be objectively right or wrong depending on whether they achieve empirical success or not . thus , we can evade * o1 * by refusing to wear a straightjacket that forces us into a strict subjective / objective dichotomy .",
    "subjective judgments do not preclude inferences that are objectively correct or incorrect .    to address objections * o2 * and * o3 * it is useful to distinguish four epistemically different types of constraints :    ( a ) : :    we know that the expected value of the function @xmath22    captures information that happens to be relevant to the particular    problem at hand and we also know its numerical value ,    @xmath14 .",
    "the ideal situation is one in which the set of available constraints of type * a * is complete in the sense that all the information that is necessary to obtain reliable answers to the questions that interest us is available . only then are we guaranteed reliable predictions",
    ". both requirements of relevance and completeness are crucial : an incomplete set of type * a * constraints has predictive value in that it leads to the best predictions that one can achieve under the circumstances but there is no guarantee that the predictions will be any good .",
    "thus we see that , properly understood , objection * o3 * is not a flaw of the entropic method ; it is a legitimate warning that reasoning with incomplete information is a risky business .",
    "note that a particular piece of evidence can be relevant and complete for some questions but not for others . for example , the expected energy @xmath24 is both relevant and complete for the question will system 1 be in thermal equilibrium with another system 2 ?  or alternatively , what is the temperature of system 1 ?",
    "but the same expected energy is far from complete for the vast majority of other possible questions such as , for example , where can we expect to find molecule # 23 in this sample of ideal gas ?",
    "( b ) : :    we know that @xmath25 captures information that    happens to be relevant to the problem at hand but its actual numerical    value @xmath23 is not known .",
    "this is the most common situation in physics .",
    "the answer to objection * o2 * hinges on the observation that whether the value of the expected energy @xmath26 is known or not , it is nevertheless still true that maximizing entropy subject to the energy constraint @xmath27 leads to the objectively correct _ family _ of distributions that describe thermal equilibrium ( including , for example , the observed black - body spectral distribution ) .",
    "thus , the justification behind imposing a constraint on the expected energy is not that the quantity @xmath26 happens to be known  because of the brute fact that its value is never actually known  but rather that it is a quantity that _ should _ be _ _",
    "_ _ known .",
    "even when the actual numerical value @xmath26 is unknown , the epistemic situation described in case * b * is one in which we know that the expected energy @xmath28 is the _ relevant _ information without which no successful predictions are possible .",
    "the question of how a particular @xmath22 is singled out as relevant has to be tackled on a case by case basis . in @xcite",
    "we discuss the problem of thermal equilibrium and show that the relevant quantity is indeed the expected energy @xmath28 and not some other conserved quantity such as @xmath29 or @xmath30 .",
    "type * b * information is processed by allowing maxent to proceed with the numerical value of @xmath19 handled as a free parameter .",
    "this leads us to the correct _ family _ of distributions @xmath31 containing the multiplier @xmath32 as a free parameter .",
    "the actual value of the parameter @xmath33 is at this point unknown and the standard approach is to seek additional information and infer @xmath33 either by a direct measurement using a thermometer , or to infer it indirectly by bayesian parameter estimation from other empirical data . the additional information has the net effect of transforming the type * b * constraint into a type * a*.    ( c ) : :    there is nothing special about the function @xmath22 except    that we happen to know its expected value ,    @xmath14 . in particular , we do not know    whether information about @xmath25 is complete    or whether it is at all relevant to the problem at hand .",
    "we do know something and this information , although limited , has some predictive value because it serves to constrain our attention to the subset of probability distributions that agree with it .",
    "maximizing entropy subject to such a constraint will yield predictions that are the best possible under the circumstances but since we do not know that @xmath22 captures information that is relevant and complete there is absolutely no guarantee that the predictions will be any good .",
    "induction is risky and objection * o3 * is a healthy reminder .",
    "( d ) : :    we know neither that @xmath25 captures relevant    information nor do we know its numerical value @xmath23 .",
    "this is an epistemic situation that reflects complete ignorance .",
    "case * d * applies to any arbitrary function @xmath22 and therefore it applies equally to all functions .",
    "since no specific @xmath22 is singled out a type * d * constraint provides no information at all and the correct procedure is to maximize @xmath18 $ ] subject to the single constraint of normalization .",
    "the result is as it should be : extreme ignorance is described by a uniform distribution .",
    "what distinguishes type * c * from * d * is that in * c * the value of @xmath23 is actually known .",
    "this fact singles out a specific variable @xmath22 and justifies using @xmath14 as a constraint .",
    "what distinguishes * d * from * b * is that in * b * there is actual knowledge that singles out the variable @xmath22 as being _",
    "relevant_.    objection * o2 * arises from a failure to distinguish constraints of type * b * from those of type * c * and * d*.    * summary : * between one extreme of ignorance ( type * d * , we know neither which variables are relevant nor their expected values ) , and the other extreme of useful knowledge ( a complete set of type * a * constraints in which we know all the relevant variables that need to be included in the analysis and we also know their expected values ) , there are _ _  intermediate states of knowledge _ _ ( involving constraints of types * b * and * c * ) and these constitute the rule rather than the exception .",
    "( it is , of course , also possible to encounter situations that mix constraints of different types . )",
    "type * b * is the more common and important situation in which a relevant variable has been correctly identified even though its actual expected value might be unknown .",
    "the situation described as type * c * is less common because information about expected values is not usually available .",
    "( what might be easily available is information in the form of sample averages which is not in general quite the same thing  see the next section . )",
    "type * d * constraints carry no information at all and should be ignored .",
    "the considerations above also apply to the problem of entropic updating from a generic prior @xmath0 to a posterior by maximizing @xmath6 $ ] subject to information in the form of constraints .",
    "let us return to the question if constraints refer to the expectation of certain variables , how do we decide their numerical magnitudes ?  and explore some pitfalls .",
    "here is a common temptation : the numerical values of expectations are seldom known and it is tempting to follow the constraint ruleand replace expected values by sample averages because it is the latter that are directly available from experiment .",
    "but the two are not the same : _ sample averages are experimental data .",
    "expected values are not experimental data . _    for very large samples such",
    "a replacement can be justified by the law of large numbers  there is a high probability that sample averages will approximate the expected values .",
    "however , for small samples using one as an approximation for the other can lead to incorrect inferences .",
    "it is important to realize that these incorrect inferences do not represent an intrinsic flaw of the entropic method ; they are merely an indication of how the method should not to be used .",
    "* example  just data : *    here is a variation on the same theme .",
    "suppose data @xmath34 have been collected .",
    "how do we process such information ?",
    "suppose we do not have a likelihood function so bayes rule is not an option .",
    "we might be tempted to maximize @xmath6 $ ] subject to a constraint @xmath35 where @xmath36 is unknown and then try to estimate @xmath36 as a sample average .",
    "this is a dangerous move .",
    "the reason is that _ in the absence of additional information _ we know neither that @xmath8 constitutes relevant information nor do we know its expected value @xmath36 and therefore this is what we identified above as a type * d * constraint  no information at all .    the mistake becomes apparent when we realize that if we know the data @xmath37 then we also know their squares @xmath38 and their cubes and also any arbitrary function of them @xmath39 .",
    "and we also know the corresponding sample averages . which of these should we use as an expected value constraint ? or should we use all of them ?",
    "the answer is that the entropic method is not designed to tackle problems where the _ only _ information is data @xmath40 .",
    "it is not that it gives a wrong answer ; it gives no answer at all because there is no constraint to impose ; the entropic engine can not even get started .",
    "but there is a possible exception :  surely the data @xmath41 must be relevant to inferences about the quantity @xmath8 itself . more generally , whether a given piece of data turns out to be relevant or not depends on what is the question being asked .",
    "if we want to make inferences about a particular function @xmath42 then we know that information about @xmath43 must surely be relevant  in which case we deal with a type * b * constraint . whether the information captured by @xmath43 is sufficient for reliable inferences , or whether higher moment constraints , @xmath44 must also be included is a question that must be addressed on a case by case basis .",
    "* example  a type b constraint plus data : *    suppose then , that in addition to the data @xmath40 collected in @xmath45 independent experiments we have information described as type * b * in the previous section : the expectation @xmath25 captures relevant information . then we can proceed to maximize the entropy @xmath5 $ ] , where @xmath46 is a ( possibly uniform ) prior distribution , subject to the constraint @xmath14 where the unknown @xmath23 is treated as a free parameter .",
    "if the variable @xmath8 can take @xmath47 discrete values labeled by @xmath10 we let @xmath48 and @xmath49 and the result is a canonical distribution @xmath50where@xmath51with an unknown multiplier @xmath17 that can be estimated from the data @xmath52 using standard bayesian methods .",
    "assuming the @xmath45 experiments are independent then bayes rule gives , @xmath53where @xmath54 is the prior for @xmath17 .",
    "it is convenient to consider the logarithm of the posterior , @xmath55the value of @xmath17 that maximizes the posterior @xmath56 is such that @xmath57where @xmath58 is the sample average , @xmath59using ( [ z and ef ] ) we see that the expected value @xmath25 ( and its corresponding @xmath17 ) can be estimated from the data as @xmath60as @xmath61 the second term on the right hand side vanishes and we see that the optimal @xmath17 is such that @xmath62 .",
    "this is to be expected : as is usual in bayesian inference for large @xmath45 the data @xmath52 overwhelms the prior @xmath54 and @xmath58 tends to @xmath63 ( in probability ) .",
    "but the result eq.([ef ] ) also shows that when @xmath45 is not large then the prior can make a non - negligible contribution .",
    "in general one should not assume that @xmath64 .",
    "let us emphasize that this analysis holds only when there is additional knowledge to the effect that the specific variable @xmath22 captures relevant information . in the absence of such knowledge we are back to the previous example  just data  and we have no reason to prefer the function @xmath42 over any other function @xmath65 and accordingly we have no reason to prefer the distribution ( [ canon dist ] ) over any other canonical distribution @xmath66 . [ the second term on the right of eq.([ef ] ) ] .",
    "his conclusion that we are always justified to set @xmath67 is therefore doubly wrong  it is both necessary that @xmath22 reflect relevant information and that the correction due to the prior be negligible . ]",
    "to set the stage for the issues involved consider a three - sided die .",
    "its faces are labeled by the number of spots @xmath68 and have probabilities @xmath69 which will be collectively denoted by @xmath70 .",
    "the space of distributions is the simplex @xmath71 with @xmath72 as shown in the figure .",
    "a fair die is one for which @xmath73 which lies at the very center of the simplex .",
    "the expected number of spots for a fair die is @xmath74 .",
    "having @xmath74 is no guarantee that the die is fair but if @xmath75 the die is necessarily biased .",
    "the paradox discussed by friedman and shimony @xcite and further explored in @xcite , @xcite and uffink 1996 arises from analyzing a situation of complete ignorance in two ways that are ( mistakenly ) thought to be equivalent .",
    "here is the first way to express complete ignorance :    * * ignorance**@xmath76 *  * nothing is known about the die ; we do not know that it is fair but on the other hand there is nothing that induces us to favor one face over another . on the basis of this minimal information we can use maxent . maximize @xmath77subject to @xmath78 and",
    "the resulting maximum entropy distribution is @xmath79 .    to set the background for the second way to represent complete ignorance consider constraints of the form @xmath80 with @xmath81 .",
    "in the figure the constraints corresponding to @xmath82 and to a generic @xmath83 are shown as vertical dashed lines .",
    "maximizing @xmath84 subject to @xmath85 and normalization leads us to assign @xmath86 at the center of the simplex .",
    "maximizing @xmath84 subject to @xmath80 and normalization leads to the point where the @xmath83 line crosses the dotted line .",
    "the dotted curve is the set of maxent distributions @xmath87 as @xmath83 spans the range from @xmath88 to @xmath89 .",
    "here is the second way to express complete ignorance :    * * ignorance**@xmath90 *  * it is tempting ( but wrong ! ) to pursue the following line of thought .",
    "we have a die but we do not know much about it .",
    "we do know , however , that the quantity @xmath91 must have some value , call it @xmath83 , about which we are ignorant too .",
    "now , the most ignorant distribution given @xmath83 is the maxent distribution @xmath92 .",
    "but @xmath83 is itself unknown so a more honest assignment for @xmath93 would be an average over @xmath83 , @xmath94where @xmath95 reflects our uncertainty about @xmath83 .",
    "it may , for example , make sense to pick a uniform distribution over @xmath83 but the precise choice is not important for our purposes .",
    "the point is that since the maxent dotted curve is concave the point @xmath96 necessarily lies below @xmath97 so that @xmath98 .",
    "and we have a paradox : we started assuming complete ignorance and through a process that claims to express full ignorance at every step we reach the conclusion that the die is biased against @xmath99 .",
    "where is the mistake ?",
    "another way to expose the problem is to force the issue and impose that the two descriptions of complete ignorance be equivalent by fiat , @xmath100since the dotted line is concave this equality can only be achieved if @xmath101 . and , again , we have a paradox : we started admitting complete ignorance about the die and therefore also about the value of @xmath83 and we end with complete certainty that @xmath82 .",
    "where is the mistake ?    before we blame the entropic method of inference it is best to take a closer look at * * ignorance**@xmath90 .",
    "one clue is symmetry .",
    "a situation of complete ignorance ought to treat the outcomes @xmath68 symmetrically but the end result is a distribution that is biased against @xmath99 .",
    "the symmetry must have been broken somewhere and it is clear that this happened at the moment we imposed the constraint on @xmath80 which is shown as _",
    "lines on the simplex .",
    "had we chosen to express our ignorance not in terms of the unknown value of @xmath80 but in terms of some other function @xmath102 then we could have easily broken the symmetry in some other direction .",
    "for example , let @xmath103 be a cyclic permutation of @xmath10 , @xmath104then repeating the analysis above would lead us to conclude that @xmath105 , which represents a die biased against @xmath106 .",
    "thus , the question becomes : what leads us to choose a constraint on @xmath91 rather than a constraint on @xmath107 when we are equally ignorant about both ?",
    "the discussion in section 2 is relevant here .",
    "the paradox with the three - sided die arises because a constraint of type * d * has been treated as if it were a constraint of type * b*. the correct approach is to recognize that we do not know whether it is the constraint @xmath108 or any other function @xmath25 that captures relevant information and their numerical values @xmath83 are also unknown  clearly a type * d * constraint",
    ". there is nothing to single out @xmath109 or any other @xmath25 and therefore the correct inference consists of maximizing @xmath110 imposing the only constraint _",
    "we actually know _ , namely , normalization .",
    "the result is as it should be  a uniform distribution ( @xmath79 ) which agrees with * * ignorance**@xmath76 .    on the other hand ,",
    "the * * ignorance**@xmath90 * *  * * argument that led to the assignment of @xmath96 in eq.([thetabar ] ) and to @xmath111 would have been correct if we actually had knowledge that it is the particular variable @xmath91  and not any other @xmath63  that captures information that is relevant to this very particular die .",
    "thus , imposing the type * b * constraint @xmath112 when @xmath83 is unknown and then averaging over @xmath83 represents a situation in which _ we know something_. there is some ignorance here  we do not know @xmath83  but this is not extreme ignorance .",
    "we can summarize as follows : knowing that the die is biased against @xmath99 but not knowing by how much ( * * ignorance**@xmath90 ) is not the same as not knowing anything about the die ( * * ignorance**@xmath76 ) .",
    "a thermodynamic version of the paradox is discussed in @xcite .",
    "here is the background : a physical system can be in any of @xmath45 microstates labeled @xmath113 .",
    "when we know absolutely nothing about the system ( * * ignorance**@xmath76 ) maximizing entropy subject to the single constraint of normalization leads to a uniform probability distribution , @xmath114    a different ( and wrong ! ) way to express complete ignorance ( * * ignorance**@xmath90 ) is to argue that the expected energy @xmath28 must have some value @xmath26 about which we are ignorant . maximizing entropy subject to both @xmath19 and normalization leads to the usual boltzmann distributions , @xmath115since the inverse temperature @xmath116 is itself unknown we must average over @xmath33 , @xmath117to the extent that both distributions are thought to reflect complete ignorance we must impose @xmath118which can be shown ( see @xcite ) to imply that@xmath119indeed , setting the lagrange multiplier @xmath120 in @xmath121 leads to the uniform distribution @xmath122 .",
    "and now we have a paradox : complete ignorance about the system ( * * ignorance**@xmath76 ) implies we are ignorant about its temperature .",
    "in fact , the system might not be in thermal equilibrium in which case it may not even have a temperature at all .",
    "but we also have the second way of expressing ignorance ( * * ignorance**@xmath90 ) and if we impose that the two agree we are led to conclude that @xmath33 has the value @xmath120 so that the temperature is precisely known . from complete ignorance",
    "we have ( wrongly ) concluded that the system must be infinitely hot  confusion about ignorance is hell .",
    "the paradox is dissolved once we realize that , just as with the die problem , a type * d * constraint has been treated as type * b*. knowing nothing about a system means we do not know whether it is in equilibrium or not .",
    "we do not know whether it is isolated and whether its energy is conserved and therefore it is not clear that @xmath28 might even be relevant information .",
    "this is the kind of non - information we earlier called a type * d * constraint that ought to be ignored .    on the other hand",
    "if we were to have actual knowledge that the system is in thermal equilibrium then it would be legitimate to impose a constraint on the expected energy @xmath28  as discussed in caticha 2012 thermal equilibrium is the physical condition that singles out the expected energy @xmath28 as being the relevant piece of information . since the temperature is unknown",
    "this is a type * b * constraint .    to summarize : knowing that a system is in thermal equilibrium while being ignorant about its temperature is not the same as knowing nothing about the system",
    ".    it may be worthwhile to rephrase this important point in yet another way .",
    "let @xmath123 and @xmath124 where @xmath125 is the space of microstates and @xmath126 is the space of some arbitrary quantities @xmath33 . the rules of probability theory allow us to write @xmath127paradoxes will easily arise if we fail to distinguish a situation of complete ignorance from a situation where we have actual knowledge about the conditional probability @xmath121 , which is what gives the parameter @xmath32 its meaning as inverse temperature .",
    "in entropic inference information is represented by constraints . in this tutorial we have argued that when constraints are expressed in terms of expected values the same formal expression @xmath14 can be used to represent four different types of available information and failure to distinguish between them can lead to errors .    in decreasing order of useful information",
    "the types range from type * a * , in which we know that the quantity @xmath22 is relevant to the inference at hand and its expected value @xmath23 is known ; to type * b * , in which @xmath22 is known to be relevant but @xmath23 is unknown ; and type * c * , in which @xmath22 is of interest only because @xmath23 happens to be known",
    ". constraints of type * a * and * b * are common and therefore important ; type * c * constraints are less so because information about expected values ( as opposed to sample averages ) is less directly available",
    ".    finally , there are situations of no information at all , labeled as type * d * , in which we know neither that @xmath22 is relevant nor its expected value @xmath23 .",
    "the point of identifying and labeling the non - informative type * d * is precisely as to avoid the errors that arise from confusing * d * with the more informative types * a * , * b * , and * c*.    the usefulness of distinguishing these four types was illustrated by discussing the constraint rule and the friedman - shimony paradox .",
    "* acknowledgements : * i am grateful to nstor caticha , r. fischer , a. giffin , a. golan , r. preuss , c. rodrguez , t. seidenfeld , j. skilling and j. uffink for many useful discussions on entropic inference .",
    "a. caticha and a. giffin , updating probabilities , _ bayesian inference and maximum entropy methods in science and engineering _ , ali mohammad - djafari ( ed . ) , aip conf . proc . * 872 * , 31 ( 2006 ) ( arxiv.org/abs/physics/0608185 ) .",
    "a. caticha , _ entropic inference and the foundations of physics _",
    "( monograph commissioned by the 11th brazilian meeting on bayesian statistics  ebeb-2012 ( usp press , so paulo , brazil 2012 ) ( http://www.albany.edu/physics/acaticha-eifp-book.pdf ) ."
  ],
  "abstract_text": [
    "<S> the method of maximum entropy has been very successful but there are cases where it has either failed or led to paradoxes that have cast doubt on its general legitimacy . </S>",
    "<S> my more optimistic assessment is that such failures and paradoxes provide us with valuable learning opportunities to sharpen our skills in the proper way to deploy entropic methods .    </S>",
    "<S> the central theme of this paper revolves around the different ways in which constraints are used to capture the information that is relevant to a problem . </S>",
    "<S> this leads us to focus on four epistemically different types of constraints . </S>",
    "<S> i propose that the failure to recognize the distinctions between them is a prime source of errors .    </S>",
    "<S> i explicitly discuss two examples . </S>",
    "<S> one concerns the dangers involved in replacing expected values with sample averages . </S>",
    "<S> the other revolves around misunderstanding ignorance . </S>",
    "<S> i discuss the friedman - shimony paradox as it is manifested in the three - sided die problem and also in its original thermodynamic formulation . </S>"
  ]
}