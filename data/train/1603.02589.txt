{
  "article_text": [
    "entropy is one measure of uncertainty within a system , and is often used to describe the disorder of sequences of quantized random variables .",
    "however , entropy can also be extended to methods within optimization , in which the disorder of a system of interest may be maximized or imized .",
    "such methods are prevalent within statistics , the physical sciences , and econometrics .    [ [ section ] ]",
    "the concerns of a statistician observing a sequence of outcomes include the validity of an explanatory hypothesis , its degree of significance , and any assumptions underlying the statistical tests .",
    "while linear hypothesis testing is often sufficient , larger sequences exhibit large deviations in behavior that should receive separate treatment .",
    "traditional linear hypothesis testing trivially assigns a constant multiple of an explanatory parameter @xmath0 to an observation when forg a hypothesis ( @xmath1 ) .",
    "optimal entropy , in which disorder is locally imized or maximized , can be used to construct _ asymptotic _ , non - linear hypothesis tests . unlike linear hypothesis testing",
    ", error probability can be imized .",
    "optimizing entropy extends to thermodynamic systems .",
    "the _ third law of thermodynamics _ states that the entropy of a closed system , i.e.  one in which no mass or energy is added or removed , must be bounded from below by zero . achieving a non - entropic system",
    "is nearly impossible , except within a perfect crystal lattice .",
    "a more probable state is one for which the entropy of a system is maximized , and observations follow a boltzmann distribution .    in our study of optimal entropy",
    ", we will use classical statistics and statistical mechanics as a lens .",
    "we will demonstrate the concept of imal entropy through two statistical tests : the univariate optimality test defined by _",
    "stein s lemma _ , and a multivariate optimality test , as defined by the _",
    "chernoff bounds_. we will see that there exists a distribution , the boltzmann distribution , that approaches maximum entropy as temperature goes to infinity .",
    "finally , we will apply the concept of asymptotic hypothesis testing to statistical mechanics .",
    "in particular , we will test and observe the evolution of error probability with a growing sample size .",
    "we will also compare q - function error probability with that of the chernoff bound .",
    "the remainder of the paper is organized as follows . to better understand the atypicality of sequences , we will study the method of types .",
    "we will then learn how such sequences behave through the large deviation theory .",
    "the focus of the paper will then shift to hypothesis testing , in which we will develop tools for recognizing the asymptotic optimality of entropy .",
    "illustrative examples of this will include stein s lemma and chernoff bounds .",
    "for the interest of the physical sciences , we will rigorously derive the boltzmann distribution , for which entropy is nearly maximized .",
    "finally , we will converge the aforementioned topics through simulations of asymptotic statistical testing .",
    "statisticians are often concerned with not just observed data , but the several possible underlying explanations .",
    "a few examples include : testing for the effectiveness of a drug , detering whether or not a coin is biased , and the effect of gender on wage growth .",
    "we begin with a simple case in which we decide between two hypothesis , each of which is represented by an independent and identical distribution , or i.i.d .",
    "let @xmath2 be i.i.d .",
    "@xmath3 . for an observed outcome , we have two possible explanations :    * @xmath4 * @xmath5    we now define a general decision function , whose value reflects the acceptance and rejection of the above hypothesis .",
    "namely , for general decision @xmath6 , @xmath7 indicates that @xmath8 is accepted . in the binary case ,",
    "the set @xmath9 over which @xmath7 is complemented by the set @xmath10 , over which @xmath11 .",
    "[ [ section-1 ] ]    quite often , statisticians are concerned with accepting incorrect hypotheses and rejecting correct ones .",
    "such occurrences , recognized as type i / ii errors , often occur when sequences exhibit atypicality and large deviating behavior ( see appendix ) .",
    "error probabilities are reflected through the decision function using weights @xmath12 : @xmath13 notice that the general decision function takes on values contradicting those implied by the conditional hypothesis .",
    "the first implies that @xmath14 was accepted even though @xmath15 was true , and the second implies that @xmath15 was accepted even though @xmath14 was true .",
    "type i ( reject true ) and type ii ( accept false ) errors similarly prove detrimental to experiments , and so we wish to minimize probabilities @xmath16 and @xmath0 . minimizing @xmath16 increases @xmath0 , and minimizing @xmath0 increases @xmath16",
    ". we will now explore methodology to minimize the overall probability of error by optimizing entropy as a weighted sum of @xmath16 and @xmath0 .",
    "we first fix either @xmath16 or @xmath0 , and manipulate the other to imize the probability of error .",
    "let @xmath2 be i.i.d . @xmath17 .",
    "further , let @xmath18 represent the kullback - leibler distance , or relative entropy between the probability densities . consider the hypothesis test between two alternatives @xmath19 and @xmath20 where @xmath21",
    "let @xmath22 be an acceptance region for hypothesis 1 .",
    "let the probabilities of error be @xmath23 and for @xmath24 , define @xmath25 then , @xmath26    see appendix    thus , no sequence of sets @xmath27 has an exponent better than @xmath28 .",
    "but the sequence @xmath29 achieves the exponent @xmath28 .",
    "thus @xmath29 is asymptotically optimal , and the best error exponent is @xmath28 .      thus far ,",
    "@xmath16 and @xmath0 have been treated separately .",
    "the approach underlying stein s lemma was to set one error probability to be infinitesimally small , and measure the effect on the resulting probability .",
    "we saw that setting @xmath30 achieved @xmath31 .",
    "however , the distribution of error amongst @xmath16 and @xmath0 may be highly asymmetrical , in which case univariate optimization may not suffice .",
    "we now explore methodoy for a bivariate optimization .",
    "[ [ section-2 ] ]    an alternative approach is to imize the weighted sum of @xmath16 and @xmath0 .",
    "the resulting error exponent is known as the _ chernoff information_. consider a distribution of i.i.d .",
    "random variables : @xmath2 representative of the decision function .",
    "we assign @xmath32 to q with probability @xmath33 and @xmath34 to q with probability @xmath35 . upholding the definition of @xmath16 and @xmath0 , the overall probability of error",
    "is @xmath36    the best achievable exponent in the bayesian probability of error is @xmath37 , where @xmath38 where @xmath39 and @xmath40 the value of @xmath41 such that @xmath42    see appendix",
    "claude shannon first proposed that the uncertainty due to possible errors in a message could be encapsulated by @xmath43 where w is the number of possible ways ( state space ) of encoding random information .",
    "intuitively , the uncertainty increases with increasing w , and is zero if w=1 .",
    "the concept of entropy provides a deep - rooted link between information theory and statistical mechanics .",
    "the state with the least information available , or greatest entropy occurs when the set of all states are equiprobable .",
    "this is also the state with maximum uncertainty .",
    "an information theoretic perspective dictates that explicit knowledge of various probabilities associated with the system constitutes greater information .",
    "similarly , the thermodynamics of a system of isolated particles indicate that entropy is directly correlated with expected energy level .",
    "below is a molecular orbital diagram that illustrates the possible energy states , all of which depend on the position an electron occupies .",
    "[ [ section-3 ] ]    entropy may also be observed in macroscopic states . the _ second law of thermodynamics _ states that in equilibrium , changes in entropy are proportional to changes in system heat per unit temperature .",
    "we have @xmath44",
    ". we can better understand this through an illustration .",
    "consider a system of gas particles that may be expanded or compressed .",
    "we can study the system under various entropic regimes .",
    "the diagram below illustrates how available work decreases ( increases ) for gaseous expansion ( compression ) under bivariate states of pressure and volume .",
    ", compressing the gas requires the least relative energy . that is , when when the change in entropy in imized , a system can be most naturally expanded / compressed .",
    "to reach imum work available , we move down the gradient of steepest descent until entropy is globally imized . ]",
    "[ [ section-4 ] ]    consider a perfectly structured crystal lattice structure , in which the positions of each contributing molecule is fixed .",
    "if we observe such a system , depart , and return after @xmath45 periods , the position of each molecule within the crystal will have remained the same _ almost surely_. if the particles did not displace , then they also carried zero kinetic energy , which is representative of a zero temperature system .",
    "this near certainty of a thermodynamic system is an example of an optimization in which entropy is imized .",
    "if instead , the system consisted of a fair coin toss , with no extra information , entropy would be maximized .",
    "the distribution that maximizes the state space for a fixed energy level is the boltzmann distribution .",
    "we will now derive such a distribution , and show that it uniquely maximizes entropy on each energy level . consider a crystal containing @xmath46particles , each of which has available energy levels , @xmath47 .",
    "the state space , w , is the number of ways the total energy @xmath48 can be distributed amongst the the particles in each energy level , across all energy levels , @xmath49 .",
    "the expected number of particles in each energy level , @xmath50 , is the product of the probability that a particle is at an energy level , @xmath51 , and the total number of particles , n. the only consideration for such a distribution is the number of particles in each energy levels , not necessarily the amount of energy allocated to each particle .",
    "energy is conserved amongst states and across energy levels .",
    "while there exist several ways of assigning the number of particles in each energy level @xmath47 , we wish to find the state with the distribution achievable in the most number of ways for fixed energy levels .",
    "our first constraint is that the total state space w is the sum of individual states occupied , @xmath52 for all possible distributions @xmath53 amongst all possible distributions of particles , there exists one that can be achieved in more ways than any other . a distribution that approaches maximum entropy for fixed energy levels is the boltzmann distribution .",
    "@xmath54    to find the most probable distribution that maximizes w , we first note that each particle in the crystal can be distinguished from the others because it occupies a defined position in space .",
    "therefore , such a setting allows us to number the particles @xmath55 .",
    "we assume a large n , to maintain consistency with typical non - deficient states .",
    "s a particular microstate of the crystal will place particle 1 in energy level @xmath56 , particle 2 in energy level @xmath57 , and so on .",
    "initially , we seek the number of @xmath52 microstates in a distribution for which there are @xmath58 particles in @xmath59 , @xmath60 particles in @xmath61 , and so on .",
    "we choose , at random , particles from the crystal , @xmath62 , and assign them to energy levels , @xmath56 .",
    "the number of ways this can be done is equal to the number of different orders in which the particles can be chosen from the crystal .",
    "the first particle can be chosen from a group of n. with @xmath63 particles remaining , the second can be chosen in @xmath63 ways .",
    "we see that the number of ways for selecting the first two particles is @xmath64 . following this procedure ,",
    "we then we see that the number of ways for selecting the @xmath46 particles is @xmath65 , or @xmath66 .",
    "[ [ section-5 ] ]    we have over counted the ways of achieving a given distribution , and have assumed that all states are distinguishable .",
    "consider the placement of the first two particles into energy level @xmath59 .",
    "it makes no difference whether the first particle is placed into @xmath59 prior to or following particle 2 .",
    "that is , the states are @xmath67 .",
    "this relaxes the strictness on order , and so permutation are ignored .",
    "thus , the state space , @xmath52 , for a given distribution , is @xmath66 divided by the product of all @xmath66 @xmath68 to find the distribution that maximizes @xmath52 , we note stirling s approximation for n ! @xmath69",
    "finding the maximum of @xmath52 is equivalent to finding the maximum of @xmath70 , so we combine equations ( 13 ) and ( 14 ) , and re - arrange as follows @xmath71 however , the set of particles is conserved .",
    "moreover , the net energy within the system is conserved .",
    "this provides the following two constraints @xmath72 we now use lagrange s method of undetered multipliers .",
    "when @xmath52 is maximized , its differential must be zero @xmath73 we multiply the constraints on particle count and energy by constants @xmath12 , and then take the differential to obtain @xmath74 subtracting these two constraints from the -entropy , we obtain @xmath75 through use of -properties and algebraic manipulation ( see appendix ) , the expression above is reduced to @xmath76 which , after exponentiating both sides is @xmath77 the significance of this result is that it shows the occupancy of en energy state @xmath57 is proportional to @xmath78 .      to account for energy states ,",
    "thermodynamicists often make use of temperature , an intrinsic quantity .",
    "temperature is equivalent to the average kinetic energy of a system of particles . because this varies across systems",
    ", we normalize .",
    "for a temperature t , and the boltzmann constant , @xmath79 , @xmath80 inducting on the one particle case , in which , @xmath81 , and combining the preceding three expressions , providing the desired result @xmath82 we have now found the boltzmann distribution , for which entropy is maximized .",
    "the boltzmann probability above expresses the fraction of particles placed in each quantum state @xmath45 to maximize entropy @xmath52 of the distribution over each energy level , @xmath57 .",
    "thus far , we have studied various methods of statistical testing , highlighting the importance of asymptotic tests such as the chernoff information bound .",
    "we have also ( briefly ) explored statistical mechanics , in which we show that entropy is maximized for a boltzmann distribution .",
    "we now demonstrate the importance of our learnings through a representative example .",
    "[ [ section-6 ] ]    robust methods of signal interpretation allow for communication , and involve the separation of _ signal _ and _ noise_. a simple signal will follow a gaussian distribution , for which entropy is maximized . a hypothesis consists of assigning observations as either signals , or as noise .",
    "such hypotheses carry error probabilities , and should be studied with both linear testing , as well as asymptotic testing .",
    "the classical binary detection problem involves the reception of finite - length signals realized as a random process @xmath83 , n=1,2 , \\ldots,$ ] [ 3 ] .",
    "the signal can be attributed to either gaussian white noise @xmath84 $ ] or a deterministic signal @xmath85 $ ] .",
    "basic studies involve the interpretation of the signal - to - noise ratio , a measure of quality .",
    "consider a binary detection problem : @xmath86    * detections are composed of signals and noise * n : n - dimensional noise vector * i.i.d .",
    "gaussian random variables and @xmath87 * @xmath88 * @xmath89    evaluate the error probability for both @xmath90 and @xmath91 when @xmath92 . + traditionally , error probability is evaluated through the q - function , which represents the probability that a normal random variable will obtain a value larger than @xmath93 standard deviations above the mean .",
    "it can also be thought of as the `` tail '' probability of the standard normal distribution , and is useful for linear hypothesis testing .",
    "@xmath94    given a signal - to - noise ratio , the q - function can be used to detere the error probability @xmath95    we also know that any error probability is bounded from above by the chernoff information bound . for the q - function , @xmath96 and so ,",
    "@xmath97 the figure below illustrates the growth of error in both forms of testing .",
    "optimizing entropy demonstrates the applicability of information theory beyond computing .",
    "asymptotic testing captures error probability in atypical sequences , and a boltzmann distribution of particles approaches maximum entropy , as temperature goes to infinity .",
    "motivation for asymptotic testing arises from atypicality and large deviations in sequences . we _ briefly _ review this , and encourage the ambitious reader to study further .",
    "the asymptotic equipartition property formalizes that although there exist several possible outcomes of a stochastic process , there exists a set from which sequences are _ typical _ , or most frequently observed . the centric approach underlying the aep involves defining an almost sure convergence in probability between the expectation of a sequence to its entropy .",
    "similarly , the method of types defines strong bounds on the number of sequences of a particular distribution , as well as the probability of each such sequence being observed .",
    "recall that _ type _ of a sequence @xmath98 is representative of its empirical distribution @xmath99 where : @xmath100 a distribution p on a is called an _",
    "n - type _ if it is the type of some @xmath101 .",
    "the set of all @xmath101 of type p is called the _ type class of the n - type _ p and is denoted by @xmath102 .",
    "first , we prove the upper bound using @xmath107 .",
    "@xmath108 consequently , @xmath109 . for the lower",
    "bound , using the fact that @xmath110 has the highest probability amongst all type classes in p , we can bound the ratio of probabilities @xmath111 using the identity @xmath112 , we see @xmath113 so @xmath114 .",
    "the lower bound can now be found as @xmath115    to connect the theory of types with general probability theory , we must develop a sense of relative entropy . for any distribution p on a , let @xmath116 denote the distribution of n independent drawings from p , that is , @xmath117 .      for probability @xmath119 , distribution @xmath120 , the probability of type class @xmath110 under @xmath121 is @xmath122 .",
    "we see @xmath123 replacing @xmath124 with the result from lemma 3 , we see the result .",
    "sanov s theorem let @xmath131 be a set of distributions on a whose closure is equal to the closure of its interior . then for the empirical distribution of a sample from a strictly positive distribution p on a , @xmath132    sanov s theorem let @xmath133 be the set of possible n - types and let @xmath134 .",
    "the previous lemma implies that @xmath135 since @xmath136 is continuous in q , the hypothesis on @xmath131 implies that @xmath137 is arbitrarily close to @xmath138 if n is large .      to prove the theorem",
    ", we construct a sequence of acceptance regions @xmath139 such that @xmath140 and @xmath141 .",
    "we then show that no other sequence of tests has an asymptotically better exponent .",
    "+ first , we define @xmath142 then , we have the following properties :    1 .   @xmath143 .",
    "this follows from : @xmath144 by the strong lln , since @xmath145 .",
    "hence , for sufficiently large n , @xmath140 .",
    "2 .   @xmath146 . using the definition of @xmath29",
    ", we have @xmath147 similarly , @xmath148 . and",
    "so , @xmath149 , and @xmath150 , hence @xmath151    the optimum hypothesis test is a likelihood ratio test , which follows the form : @xmath152 the test divides the probability simplex into regions corresponding to hypothesis 1 and hypothesis 2 , respectively .",
    "this is illustrated below :      let a be the set of types associated with hypothesis 1 . from the preceding discussions , it follows that the closest point in the set @xmath10 to @xmath32 is on the boundary of a , and is of the form given by @xmath154 $ ] .",
    "then , it is clear that @xmath153 is the distribution in a that is closest to @xmath34 .",
    "it is also the distribution in @xmath10 that is closest to @xmath32 . by sanov s theorem",
    ", we can calculate the associated probabilities of error : @xmath155 in the bayesian case , the overall probability of error is the weighted sum of the two probabilities of error , @xmath156 since the exponential rate is detered by the worst exponent .",
    "since @xmath157 increases with @xmath41 and @xmath158 decreases with @xmath41 , the maximum value of the imum of @xmath159 is attained when they are equal .",
    "we choose @xmath41 so that @xmath160 thus @xmath161 is the highest achievable exponent for the probability of error , and is called the chernoff information .",
    "the closest point in the set @xmath10 to @xmath32 is on the boundary of a , and is of the form given by [ ] . then from the previous discussion , it is clear that @xmath153 is the distribution in a that is closest to @xmath34 ; it is also the distribution in @xmath10 that is closest to @xmath32 . by sanov s theorem",
    ", we can calculate the associated probabilities of error : @xmath162 in the bayesian case , the overall probability of error is the weighted sum of the individual two probabilities of error , @xmath163 since the exponential rate is detered by the worst exponent .",
    "since @xmath164 increases with @xmath41 and @xmath165 decreases with @xmath41 , the maximum value of the imum of @xmath166 is attained when they are equal ."
  ],
  "abstract_text": [
    "<S> the method of optimizing entropy is used to ( i ) conduct asymptotic hypothesis testing and ( ii ) detere the energy distribution for which entropy is maximized . </S>",
    "<S> this paper focuses on two related applications of information theory : statistics and statistical mechanics . </S>"
  ]
}