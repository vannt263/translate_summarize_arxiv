{
  "article_text": [
    "matlab @xcite is the dominant programming language for implementing numerical computations and is widely used for algorithm development , simulation , data reduction , testing and system evaluation .",
    "the popularity of matlab is driven by the high productivity that is achieved by users because one line of matlab code can typically replace ten lines of c or fortran code .",
    "many matlab programs can benefit from faster execution on a parallel computer , but achieving this goal has been a significant challenge .",
    "there have been many previous attempts to provide an efficient mechanism for running matlab programs on parallel computers @xcite .",
    "these efforts of have faced numerous challenges and none have received widespread acceptance .    in the world of parallel computing the message passing interface ( mpi ) @xcite is the de facto standard for implementing programs on multiple processors .",
    "mpi defines c and fortran language functions for doing point - to - point communication in a parallel program .",
    "mpi has proven to be an effective model for implementing parallel programs and is used by many of the worlds most demanding applications ( weather modeling , weapons simulation , aircraft design , and signal processing simulation ) .",
    "matlabmpi consists of a set of matlab scripts that implements a subset of mpi and allows any matlab program to be run on a parallel computer .",
    "the key innovation of matlabmpi is that it implements the widely used mpi `` look and feel '' on top of standard matlab file i / o , resulting in a `` pure '' matlab implementation that is exceedingly small ( @xmath0250 lines of code ) .",
    "thus , matlabmpi will run on any combination of computers that matlab supports .",
    "the next section describes the implementation and functionality provided by matlabmpi .",
    "section three presents results on the bandwidth performance of the library from several parallel computers .",
    "section four uses an image processing application to show the scaling performance and the very high software price performance that can be achieved using matlabmpi .",
    "section five presents our conclusions and plans for future work .",
    "the central innovation of matlabmpi is the use of matlab s built in file i / o capabilities , which allow any matlab variable ( matrices , arrays , structures , ... ) to be written and read by matlab running on any machine , and eliminates the need to write complicated buffer packing and unpacking routines .",
    "the approach used in matlabmpi is extremely simple and is illustrated in figure  [ fig : filebasedcomm ] .",
    "the sender saves a matlab variable to a data file and when the file is complete the sender touches a lock file .",
    "the receiver continuously checks for the existence of the lock file ; when it is exists the receiver reads in the data file and then deletes both the data file and the lock file .",
    "an example of a basic send and receive matlab program is shown below    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .... mpi_init ;                            % initialize mpi .",
    "comm = mpi_comm_world ;               % create communicator .",
    "comm_size = mpi_comm_size(comm ) ;     % get size .",
    "my_rank = mpi_comm_rank(comm ) ;       % get rank .",
    "source = 0 ;                          % set source .",
    "dest = 1 ;                            % set destination .",
    "tag = 1 ;                             % set message tag .",
    "if(comm_size = = 2 )                   % check size .",
    "if ( my_rank = = source )             % if source .",
    "data = 1:10 ;                     % create data .",
    "mpi_send(data , dest , tag , comm ) ;    % send data",
    ".    end    if ( my_rank = = dest )               % if destination .",
    "data = mpi_recv(source , tag , comm ) ; % receive data .",
    "end end mpi_finalize ;                        % finalize matlab mpi .",
    "exit ;                                % exit matlab .... _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the structure of the above program is very similar to mpi programs written in c or fortran , but with the convenience of a high level language .",
    "the first part of the program sets up the mpi world ; the second part differentiates the program according to rank and executes the communication ; the third part closes down the mpi world and exits matlab . if the above program were written in a matlab file called sendreceive.m , it would be executed by calling the following command from the matlab prompt :    ....      eval ( mpi_run('sendreceive',2,machines ) ) ; ....    where the machines argument can be any of the following forms :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ machines = \\ { } ; : :    run on the local host .",
    "machines = \\{machine1 machine2 } ; : :    run on a multi processors .",
    "machines = \\{machine1:dir1 machine2:dir2 } ; : :    run on a multi processors and communicate using dir1 and dir2 , which    must be visible to both machines . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the mpi_run command launches a matlab script on the specified machines with output redirected to a file . if the rank=0 process is being run on the local machine , mpi_run returns a string containing the commands to initialize matlabmpi , which allows matlabmpi to be invoked deep inside of a matlab program in a manner similar to fork - join model employed in openmp .",
    "the sendreceive example illustrates the basic six mpi functions ( plus mpi_run ) that have been implemented in matlabmpi    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ mpi_run : :    runs a matlab script in parallel .",
    "mpi_init : :    initializes mpi .",
    "mpi_comm_size : :    gets the number of processors in a communicator .",
    "mpi_comm_rank : :    gets the rank of current processor within a communicator .",
    "mpi_send : :    sends a message to a processor ( non - blocking ) .",
    "mpi_recv : :    receives message from a processor ( blocking ) .",
    "mpi_finalize : :    finalizes mpi .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    for convenience ,",
    "three additional mpi functions have also been implemented along with two utility functions that provide important matlabmpi functionality , but are outside the mpi specification    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ mpi_abort : :    function to kill all matlab jobs started by matlabmpi .",
    "mpi_bcast : :    broadcast a message ( blocking ) .",
    "mpi_probe : :    returns a list of all incoming messages .",
    "matmpi_save_messages : :    matlabmpi function to prevent messages from being deleted ( useful for    debugging )",
    ". matmpi_delete_all : :    matlabmpi function to delete all files created by matlabmpi .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    matlabmpi handles errors the same as matlab , however running hundreds of copies does bring up some additional issues .",
    "if an error is encountered and the matlab script has an `` exit '' statement then all the matlab processes will die gracefully . if a matlab job is waiting for a message that will never arrives than it needs to be killed with the mpi_abort command .",
    "in this situation , matlabmpi can leave files which need to be deleted with the matmpi_delete_all command .    on shared memory systems ,",
    "matlabmpi only requires a single matlab license since any user is allowed to launch many matlab sessions . on a distributed memory system , matlabmpi requires one matlab license per machine .",
    "because matlabmpi uses file i / o for communication , there must also be a directory that is visible to every machine ( this is usually also required in order to install matlab ) .",
    "this directory defaults to the directory that the program is launched from .",
    "the vast majority of potential matlab applications are `` embarrassingly '' parallel and require minimal performance out of matlabmpi .",
    "these applications exploit coarse grain parallelism and communicate rarely ( if at all ) .",
    "never - the - less , measuring the communication performance is useful for determining which applications are most suitable for matlabmpi .",
    "matlabmpi has been run on several unix platforms .",
    "it has been benchmarked and compared to the performance of c mpi on the sgi origin2000 housed at boston university .",
    "these results indicate that for large messages ( @xmath01 mbyte ) matlabmpi is able to match the performance of c mpi ( see figure  [ fig : bu_o2000_bandwidth ] ) . for smaller messages ,",
    "matlabmpi is dominated by its latency ( @xmath035 milliseconds ) , which is significantly larger than c mpi .",
    "these results have been reproduced using a sgi origin2000 and a hewlett packard workstation cluster ( connected with 100 mbit ethernet ) at ohio state university ( see figure  [ fig : osu_hp_o2000_bandwidth ] ) .",
    "the above bandwidth results were all obtained using two processors engaging in bi - directional sends and receives .",
    "such a test does a good job of testing the individual links on an a multi - processor system . to more broadly test the interconnect the send receive benchmark is run on an eight node ( 16 cpu ) linux cluster connected with gigabit ethernet ( see figure  [ fig : linux_bandwidth ] ) .",
    "these results are shown for one and 16 cpus .",
    "matlabmpi is able to maintain high bandwidth even when multiple processors are communicating by allowing each processor to have its own receive directory . by cross",
    "mounting all the disks in a cluster each node only sees the traffic directed to it , which allows communication contention to be kept to a minimum .",
    "to further test matlabmpi a simple image filtering application was written .",
    "this application abstracts the key computations that are used in many of our dod sensor processing applications ( e.g. wide area synthetic aperture radar ) .",
    "the application executed repeated 2d convolution on a large image ( 1024 x 128,000 @xmath02  gbytes ) .",
    "this application was run on a large shared memory parallel computer ( the sgi origin2000 at boston university ) and achieved speedups greater than 64 on 64 processors ; showing the classic super - linear speedup ( due to better cache usage ) that comes from breaking a very large memory problem into many smaller problems ( see figure  [ fig : bu_o2000_speedup ] ) .    to further test the scalability , the image processing application was run with a constant load per processor ( 1024 x 1024 image per processor ) on a large shared / distributed memory system ( the ibm sp2 at the maui high performance computing center ) . in this test ,",
    "the application achieved a speedup of @xmath0300 on 304 cpus as well achieving @xmath015% of the theoretical peak ( 450 gigaflops ) of the system ( see figure  [ fig : mhpcc_ibmsp2_speedup ] ) .",
    "the ultimate goal of running matlab on parallel computers is to increase programmer productivity and decrease the large software cost of using hpc systems .",
    "figure  [ fig : prod_vs_perf ] ) plots the software cost ( measured in software lines of code or slocs ) as a function of the maximum achieved performance ( measured in units of single processor peak ) for the same image filtering application implemented using several different libraries and languages ( vsipl , mpi , openmp , using c++ , c , and matlab @xcite ) .",
    "these data show that higher level languages require fewer lines to implement the same level of functionality . obtaining increased peak performance ( i.e. exploiting more parallelism ) requires more lines of code .",
    "matlabmpi is unique in that it achieves a high peak performance using a small number of lines of code .",
    "two useful metrics we have developed for measuring software productivity are gigaflops / sloc and cpus / sloc .",
    "gigaflops / sloc will increase in a given application if it can be made to run faster or can be implemented with fewer lines of code .",
    "cpus / sloc will increase if the number of processors that can be effectively used in an application can be increased without increasing the number of lines of code .",
    "the test application does extremely well in both of these measures , achieving 0.85 gigaflops / sloc and 4.4 cpus / sloc , both of which are the highest values recorded for any application .",
    "the use of file i / o as a parallel communication mechanism is not new and is now increasingly feasible with the availability of low cost high speed disks .",
    "the extreme example of this approach are the now popular storage area networks ( san ) , which combine high speed routers and disks to provide server solutions .",
    "although using file i / o increases the latency of messages it normally will not effect the bandwidth .",
    "furthermore , the use of file i / o has several additional functional advantages which make it easy to implement large buffer sizes , recordable messages , multi - casting , and one - sided messaging .",
    "finally , the matlabmpi approach is readily applied to any language ( e.g. idl , python , and perl ) .",
    "matlabmpi demonstrates that the standard approach to writing parallel programs in c and fortran ( i.e. using mpi ) is also valid in matlab .",
    "in addition , by using matlab file i / o , it was possible to implement matlabmpi entirely within the matlab environment , making it instantly portable to all computers that matlab runs on .",
    "most potential parallel matlab applications are trivially parallel and do nt require high performance .",
    "never - the - less , matlabmpi can match c mpi performance on large messages . the simplicity and performance of matlabmpi",
    "makes it a very reasonable choice for programmers that want to speed up their matlab code on a parallel computer .",
    "matlabmpi provides the highest productivity parallel computing environment available .",
    "however , because it is a point - to - point messaging library , a significant amount code of must be added to any application in order to do basic parallel operations . in the test application presented here ,",
    "the number of lines of matlab code increased from 35 to 70 .",
    "while a 70 line parallel program is extremely small , it represents a significant increase over the single processor case .",
    "our future work will aim at creating higher level objects ( e.g. distributed matrices ) that will eliminate this parallel coding overhead ( see figure  [ fig : layeredarch ] ) . the resulting `` parallel matlab toolbox ''",
    "will be built on top of the matlabmpi communication layer , and will allow a user to achieve good parallel performance without increasing the number lines of code .",
    "the authors would like to thank the sponsorship of the dod high performance computing modernization office , and the staff at the supercomputing centers at boston university , ohio state university and the maui high performance computing center .",
    ".... mpi_init ;   % initialize mpi .",
    "comm = mpi_comm_world ;   % create communicator .",
    "comm_size = mpi_comm_size(comm ) ;   % get size .",
    "my_rank = mpi_comm_rank(comm ) ;   % get rank .",
    "% do a synchronized start .",
    "starter_rank = 0 ; delay = 30 ;   % seconds synch_start(comm , starter_rank , delay ) ; n_image_x = 2.^(10 + 1)*comm_size ;   % set image size ( use powers of 2 ) .",
    "n_image_y = 2.^10 ; n_point = 100 ;   % number of points to put in each sub - image .",
    "% set filter size ( use powers of 2 ) .",
    "n_filter_x = 2.^5 ; n_filter_y = 2.^5 ; n_trial = 2 ;   % set the number of times to filter .",
    "% computer number of operations .",
    "total_ops = 2.*n_trial*n_filter_x*n_filter_y*n_image_x*n_image_y ; if(rem(n_image_x , comm_size ) ~= 0 )   disp('error : processors need to evenly divide image ' ) ;   exit ; end disp(['my_rank : ' , num2str(my_rank ) ] ) ;   % print rank .",
    "left = my_rank - 1 ;   % set who is source and who is destination .",
    "if ( left < 0 )    left = comm_size - 1 ; end right = my_rank + 1 ; if ( right > = comm_size )    right = 0 ; end tag = 1 ;   % create a unique tag i d for this message .",
    "start_time = zeros(n_trial ) ;   % create timing matrices .",
    "end_time = start_time ; zero_clock = clock ;   % get a zero clock .",
    "n_sub_image_x = n_image_x./comm_size ;   % compute sub_images for each processor .",
    "n_sub_image_y = n_image_y ; % create starting image and working images ..",
    "sub_image0 = rand(n_sub_image_x , n_sub_image_y).^10 ; sub_image = sub_image0 ; work_image = zeros(n_sub_image_x+n_filter_x , n_sub_image_y+n_filter_y ) ; % create kernel . x_shape = sin(pi.*(0:(n_filter_x-1))./(n_filter_x-1)).^2 ; y_shape = sin(pi.*(0:(n_filter_y-1))./(n_filter_y-1)).^2 ; kernel = x_shape . ' * y_shape ; % create box indices .",
    "lboxw = [ 1,n_filter_x/2,1,n_sub_image_y ] ; cboxw = [ n_filter_x/2 + 1,n_filter_x/2+n_sub_image_x,1,n_sub_image_y ] ; rboxw = [ n_filter_x/2+n_sub_image_x+1,n_sub_image_x+n_filter_x,1,n_sub_image_y ] ; lboxi = [ 1,n_filter_x/2,1,n_sub_image_y ] ; rboxi = [ n_sub_image_x - n_filter_x/2 + 1,n_sub_image_x,1,n_sub_image_y ] ; start_time = etime(clock , zero_clock ) ;   % set start time .",
    "% loop over each trial . for i_trial = 1:n_trial    % copy center sub_image into work_image .",
    "work_image(cboxw(1):cboxw(2),cboxw(3):cboxw(4 ) ) = sub_image ;    if ( comm_size > 1 )      ltag = 2.*i_trial ;     % create message tag .",
    "rtag = 2.*i_trial+1 ;      % send left sub - image .",
    "l_sub_image = sub_image(lboxi(1):lboxi(2),lboxi(3):lboxi(4 ) ) ;      mpi_send (   left , ltag , comm , l_sub_image ) ;      % receive right padding .",
    "r_pad = mpi_recv ( right , ltag , comm ) ;      work_image(rboxw(1):rboxw(2),rboxw(3):rboxw(4 ) ) = r_pad ;      % send right sub - image .",
    "r_sub_image = sub_image(rboxi(1):rboxi(2),rboxi(3):rboxi(4 ) ) ;      mpi_send ( right , rtag , comm , r_sub_image ) ;      % receive left padding .",
    "l_pad = mpi_recv ( left , rtag , comm ) ;      work_image(lboxw(1):lboxw(2),lboxw(3):lboxw(4 ) ) = l_pad ;    end    work_image = conv2(work_image , kernel,'same ' ) ;    % compute convolution .",
    "% extract sub_image .",
    "sub_image = work_image(cboxw(1):cboxw(2),cboxw(3):cboxw(4 ) ) ; end end_time = etime(clock , zero_clock ) ;   % get end time for the this message .",
    "total_time = end_time - start_time   % print the results .",
    "% print compute performance .",
    "gigaflops = total_ops / total_time / 1.e9 ; disp(['gigaflops : ' , num2str(gigaflops ) ] ) ; mpi_finalize ;   % finalize matlab mpi .",
    "exit ; ....    99 matlab , the mathworks , inc .",
    ", http://www.mathworks.com/products/matlab/ message passing interface ( mpi ) , http://www.mpi-forum.org/ matlab*p , a. edelman , mit , http://www-math.mit.edu/@xmath0edelman/ a parallel linear algebra server for matlab - like environments , g. morrow and robert van de geijn , 1998 , supercomputing 98 http://www.supercomp.org/sc98/techpapers/sc98_fullabstracts/morrow779/index.htm automatic array alignment in parallel matlab scripts , i. milosavljevic and m. jabri , 1998 parallel matlab development for high performance computing with rtexpress , http://www.rtexpress.com/ matlab parallel example , kadin tseng , http://scv.bu.edu/scv/origin2000/matlab/matlabexample.shtml multimatlab : matlab on multiple processors a. trefethen et al , paramat , http://www.alphadata.co.uk/dsheet/paramat.html investigation of the parallelization of aew simulations written in matlab , don fabozzi 1999 , hpec99 matpar : parallel extensions to matlab , http://hpc.jpl.nasa.gov/ps/matpar/ mpi toolbox for matlab ( mpitb ) , http://atc.ugr.es/javier-bin/mpitb_eng a matlab compiler for parallel computers .",
    "m. quinn , http://www.cs.orst.edu/@xmath0quinn/matlab.html cornell multitask toolbox for matlab ( cmtm ) , http://gremlin.tc.cornell.edu/er/media/2000/cmtm.html j. kepner , a multi - threaded fast convolver for dynamically parallel image filtering , 2002 , accepted journal of parallel and distributed computing"
  ],
  "abstract_text": [
    "<S> the true costs of high performance computing are currently dominated by software . </S>",
    "<S> addressing these costs requires shifting to high productivity languages such as matlab . </S>",
    "<S> matlabmpi is a matlab implementation of the message passing interface ( mpi ) standard and allows any matlab program to exploit multiple processors . </S>",
    "<S> matlabmpi currently implements the basic six functions that are the core of the mpi point - to - point communications standard . </S>",
    "<S> the key technical innovation of matlabmpi is that it implements the widely used mpi `` look and feel '' on top of standard matlab file i / o , resulting in an extremely compact ( @xmath0250 lines of code ) and `` pure '' implementation which runs anywhere matlab runs , and on any heterogeneous combination of computers . </S>",
    "<S> the performance has been tested on both shared and distributed memory parallel computers ( e.g. sun , sgi , hp , ibm and linux ) . </S>",
    "<S> matlabmpi can match the bandwidth of c based mpi at large message sizes . a test image filtering application using matlabmpi achieved a speedup of @xmath0300 using 304 cpus and @xmath015% of the theoretical peak ( 450 gigaflops ) on an ibm sp2 at the maui high performance computing center . </S>",
    "<S> in addition , this entire parallel benchmark application was implemented in 70 software - lines - of - code ( sloc ) yielding 0.85 gigaflop / sloc or 4.4 cpus / sloc , which are the highest values of these software price performance metrics ever achieved for any application . </S>",
    "<S> the matlabmpi software will be made available for download . </S>"
  ]
}