{
  "article_text": [
    "in this paper , we revisit the problem of _ maximum inner product search ( mips ) _ , which was studied in a recent technical report  @xcite . in this report",
    "the authors present the first provably fast algorithm for mips , which was considered hard  @xcite .",
    "given an input query point @xmath0 , the task of mips is to find @xmath1 , where @xmath2 is a giant collection of size @xmath3 , which maximizes ( approximately ) the * inner product @xmath4 * : @xmath5 the mips problem is related to the problem of _ near neighbor search ( nns)_. for example , l2-nns @xmath6 or , correlation - nns @xmath7",
    "these three problems are equivalent if the norm of every element @xmath8 is constant .",
    "clearly , the value of the norm @xmath9 has no effect for the argmax . in many scenarios ,",
    "mips arises naturally at places where the norms of the elements in @xmath2 have significant variations  @xcite .",
    "as reviewed in  @xcite , examples of applications of mips include recommender system  @xcite , large - scale object detection with dpm  @xcite , structural svm  @xcite , and multi - class label prediction  @xcite .    * asymmetric lsh ( alsh ) * : locality sensitive hashing ( lsh )  @xcite is popular in practice for efficiently solving nns . in the prior work  @xcite",
    ", the concept of `` asymmetric lsh '' ( alsh ) was proposed that one can transform the input query @xmath10 and data in the collection @xmath11 independently , where the transformations @xmath12 and @xmath13 are different . @xcite",
    "developed a particular set of transformations to convert mips into l2-nns and then solved the problem by standard l2-hash  @xcite . in this paper , we name the scheme in  @xcite as * l2-alsh*. asymmetry in hashing has become popular recently , and it has been applied for hashing higher order similarity  @xcite , data dependent hashing  @xcite , sketching  @xcite etc .    * our contribution * : in this study , we propose another scheme for alsh , by developing a new set of asymmetric transformations to convert mips into a problem of correlation - nns , which is solved by `` sign random projections ''  @xcite .",
    "we name this new scheme as * sign - alsh*. our theoretical analysis and experimental study show that sign - lsh is more advantageous than l2-alsh for mips .",
    "the problem of efficiently finding nearest neighbors has been an active research since the very early days of computer science  @xcite .",
    "approximate versions of the near neighbor search problem  @xcite were proposed to break the linear query time bottleneck .",
    "the following formulation for approximate near neighbor search is often adopted .    * definition : * ( @xmath14-approximate near neighbor or @xmath14-nn )  _ given a set of points in a @xmath15-dimensional space @xmath16 , and parameters @xmath17 , @xmath18 , construct a data structure which , given any query point @xmath19 , does the following with probability @xmath20 : if there exists an @xmath21-near neighbor of @xmath19 in @xmath2 , it reports some @xmath22-near neighbor of @xmath19 in @xmath2 . _    _ locality sensitive hashing _ ( lsh )",
    "@xcite is a family of functions , with the property that more similar items have a higher collision probability .",
    "lsh trades off query time with extra ( one time ) preprocessing cost and space .",
    "existence of an lsh family translates into provably sublinear query time algorithm for c - nn problems .    *",
    "definition : * ( locality sensitive hashing ( lsh ) )     _ a family @xmath23 is called @xmath24-sensitive if , for any two points @xmath25 , @xmath26 chosen uniformly from @xmath23 satisfies : _    * if @xmath27 then @xmath28 * if @xmath29 then @xmath30    for efficient approximate nearest neighbor search , @xmath31 and @xmath32 is needed .    *",
    "fact 1 * :  given a family of @xmath24 -sensitive hash functions , one can construct a data structure for @xmath14-nn with @xmath33 query time and space @xmath34 , where @xmath35 .",
    "lsh is a generic framework and an implementation of lsh requires a concrete hash function .",
    "@xcite presented an lsh family for @xmath36 distances .",
    "formally , given a fixed window size @xmath37 , we sample a random vector @xmath38 with each component from i.i.d .",
    "normal , i.e. , @xmath39 , and a scalar @xmath40 generated uniformly at random from @xmath41 $ ] .",
    "the hash function is defined as : @xmath42 where @xmath43 is the floor operation .",
    "the collision probability under this scheme can be shown to be @xmath44 where @xmath45 and @xmath46 is the euclidean distance between the vectors @xmath47 and @xmath48 .",
    "another popular lsh family is the so - called `` sign random projections ''  @xcite .",
    "again , we choose a random vector @xmath38 with @xmath39 .",
    "the hash function is defined as : @xmath49 and collision probability is @xmath50 this hashing scheme is also popularly known as _ signed random projections ( srp ) _",
    "in  @xcite , it was shown that the framework of locality sensitive hashing is restrictive for solving mips .",
    "the inherent assumption of the same hash function for both the transformation as well as the query was unnecessary in the classical lsh framework and it was the main hurdle in finding provable sub - linear algorithms for mips with lsh . for the theoretical guarantees of lsh to work",
    "there was no requirement of symmetry . incorporating asymmetry in the hashing schemes was the key in solving mips efficiently .",
    "* definition  @xcite : * ( * _ asymmetric _ * locality sensitive hashing ( alsh ) )   a family @xmath23 , along with the two vector functions @xmath51 ( * query transformation * ) and @xmath52 ( * preprocessing transformation * ) , is called @xmath24-sensitive if for a given @xmath14-nn instance with query @xmath19 , and the hash function @xmath26 chosen uniformly from @xmath23 satisfies the following :    * if @xmath53 then @xmath54 * if @xmath55 then @xmath56    here @xmath47 is any point in the collection @xmath2 . note that the query transformation @xmath12 is only applied on the query and the pre - processing transformation @xmath13 is applied to @xmath8 while creating hash tables . by letting @xmath57 , we can recover the vanilla lsh . using different transformations ( i.e. , @xmath58 ) , it is possible to counter the fact that self similarity is not highest with inner products which is the main argument of failure of lsh .",
    "we only just need the probability of the new collision event @xmath59 to satisfy the conditions of definition of alsh for @xmath60 .",
    "@xcite[theo : extendedlsh ] given a family of hash function @xmath23 and the associated query and preprocessing transformations @xmath13 and @xmath12 , which is @xmath24 -sensitive , one can construct a data structure for @xmath14-nn with @xmath33 query time and space @xmath34 , where @xmath61 .",
    "@xcite also provided an explicit construction of alsh , which we call * l2-alsh*. without loss of generality , one can always assume @xmath62 .",
    "if this is not the case , then we can always scale down the norms without altering the @xmath63 .",
    "since the norm of the query does not affect the @xmath63 in mips , for simplicity it was assumed @xmath64 .",
    "this condition can be removed easily ( see section  [ sec : remove ] for details ) . in l2-alsh ,",
    "two vector transformations @xmath65 and @xmath66 are defined as follows : @xmath67\\\\ \\label{eq : q}q(x ) & = [ x ; 1/2 ; 1/2 ; .... ; 1/2],\\end{aligned}\\ ] ] where [ ; ] is the concatenation .",
    "@xmath11 appends @xmath68 scalers of the form @xmath69 at the end of the vector @xmath47 , while q(x ) simply appends @xmath68 `` 1/2 '' to the end of the vector @xmath47 . by observing @xmath70 one",
    "can obtain the following key equality : @xmath71 since @xmath72 , we have @xmath73 at the tower rate ( exponential to exponential ) .",
    "thus , as long as @xmath68 is not too small ( e.g. , @xmath74 would suffice ) , we have @xmath75 this scheme is the first connection between solving un - normalized mips and approximate near neighbor search .",
    "transformations @xmath13 and @xmath12 , when norms are less than 1 , provide correction to the l2 distance @xmath76 making it rank correlate with the ( un - normalized ) inner product .",
    "the general idea of alsh was partially inspired by the work on three - way similarity search  @xcite , where they applied different hashing functions for handling query and data in the repository .",
    "asymmetric transformations give us enough flexibility to modify norms without changing inner products . the transformation provided in  @xcite used this flexibility to convert mips to standard near neighbor search in @xmath36 space for which we have standard hash functions . signed random projections are popular hash functions widely adopted for correlation or cosine similarity .",
    "we use asymmetric transformation to convert approximate mips into approximate maximum correlation search . the transformations and the collision probability of the hashing functions determines the efficiency of the obtained alsh algorithm .",
    "we show that the new transformation with srp is better suited for alsh compared to the existing l2-alsh .",
    "note that in the recent work on _ coding for random projections _",
    "@xcite , it was already shown that sign random projections ( or 2-bit random projections ) can outperform l2lsh .",
    "we assume for simplicity that @xmath64 as the norm of the query does not change the ordering , we show in the next section how to get rid of this assumption . without loss of generality let @xmath77 as it can always be achieved by scaling the data by large enough number .",
    "we define two vector transformations @xmath65 and @xmath66 as follows : @xmath78\\\\ \\label{eq : q}q(x ) & = [ x ; 0 ; 0 ; .... ; 0],\\end{aligned}\\ ] ] using @xmath79 , @xmath80 , and @xmath81 we obtain the following key equality : @xmath82 the term @xmath83 again vanishes at the tower rate .",
    "this means we have approximately @xmath84 this provides another solution for solving mips using known methods for approximate correlation - nns .",
    "( [ eq : mipsnns ] ) shows that mips reduces to the standard approximate near neighbor search problem which can be efficiently solved by sign random projections , i.e. , @xmath85 ( defined by eq .",
    "( [ eq : signhash ] ) ) .",
    "formally , we can state the following theorem .",
    "[ theo : collprobnew ] given a @xmath14-approximate instance of mips , i.e. , @xmath86 , and a query @xmath19 such that @xmath64 along with a collection @xmath2 having @xmath87",
    "@xmath88 let @xmath13 and @xmath12 be the vector transformations defined in eq .",
    "( [ eq : p ] ) and eq .",
    "( [ eq : q ] ) , respectively .",
    "we have the following two conditions for hash function @xmath85 ( defined by eq .",
    "( [ eq : signhash ] ) )    * if @xmath89 then @xmath90 \\\\\\notag & \\ge 1-\\frac{1}{\\pi}\\cos^{-1}\\left(\\frac{s_0}{\\sqrt{m/4 + u^{2^{m+1}}}}\\right)\\end{aligned}\\ ] ] * if @xmath91 then @xmath92 \\\\\\notag & \\le 1-\\frac{1}{\\pi}\\cos^{-1}\\left(\\frac{\\min\\{cs_0,z^*\\}}{\\sqrt{m/4 + \\left(\\min\\{cs_0,z^*\\}\\right)^{2^{m+1}}}}\\right)\\end{aligned}\\ ] ] where @xmath93 .    *",
    "proof : *  when @xmath94 , we have , according to eq .",
    "( [ eq : signcollprob ] ) @xmath95 \\\\",
    "\\notag&=1-\\frac{1}{\\pi}\\cos^{-1}\\left(\\frac{q^tx}{\\sqrt{m/4+||x||_2^{2^{m+1}}}}\\right ) \\\\ \\notag&\\geq 1-\\frac{1}{\\pi}\\cos^{-1}\\left(\\frac{q^tx}{\\sqrt{m/4+u^{2^{m+1}}}}\\right)\\end{aligned}\\ ] ] when @xmath96 , by noting that @xmath97 , we have @xmath95\\\\ \\notag & = 1-\\frac{1}{\\pi}\\cos^{-1}\\left(\\frac{q^tx}{\\sqrt{m/4+||x||_2^{2^{m+1}}}}\\right ) \\\\\\notag & \\leq 1-\\frac{1}{\\pi}\\cos^{-1}\\left(\\frac{q^tx}{\\sqrt{m/4+(q^tx)^{2^{m+1}}}}\\right)\\end{aligned}\\ ] ] for this one - dimensional function @xmath98 , where @xmath99 , @xmath100 and @xmath101 , we know @xmath102 one can also check that @xmath103 for @xmath104 , i.e. , @xmath105 is a concave function .",
    "the maximum of @xmath105 is attained at @xmath106if @xmath107 , then we need to use @xmath108 as the bound .",
    "@xmath109    therefore , we have obtained , in lsh terminology , @xmath110 theorem  [ theo : extendedlsh ] allows us to construct data structures with worst case @xmath33 query time guarantees for @xmath14-approximate mips , where @xmath111 . for any given @xmath112 , there always exist @xmath113 and @xmath68 such that @xmath114 .",
    "this way , we obtain a sublinear query time algorithm for mips .",
    "because @xmath115 is a function of 2 parameters , the best query time chooses @xmath116 and @xmath68 , which minimizes the value of @xmath115 . for convenience",
    ", we define @xmath117 see figure  [ fig : optrho ] for the plots of @xmath118 , which also compares the optimal @xmath115 values for l2-alsh in the prior work  @xcite .",
    "the results show that sign - alsh is noticeably better .",
    "figure  [ fig : fixedrho ] presents the @xmath115 values for two sets of selected parameters : @xmath119 and @xmath120 .",
    "we can see that even if we use fixed parameters , the performance would not degrade much .",
    "this essentially frees practitioners from the burden of choosing parameters .",
    "changing norms of the query does not affect the @xmath121 , and hence , in practice for retrieving top-@xmath122 , normalizing the query should not affect the performance .",
    "but for theoretical purposes , we want the runtime guarantee to be independent of @xmath9 .",
    "note , both lsh and alsh schemes solve the @xmath14-approximate instance of the problem , which requires a threshold @xmath123 and an approximation ratio @xmath14 . for this given @xmath14-approximate instance",
    "we choose optimal parameters @xmath124 and @xmath125",
    ". if the queries have varying norms , which is likely the case in practical scenarios , then given a @xmath14-approximate mips instance , normalizing the query will change the problem because it will change the threshold @xmath21 and also the approximation ratio @xmath14 .",
    "the optimal parameters for the algorithm @xmath124 and @xmath125 , which are also the size of the data structure , change with @xmath21 and @xmath14 .",
    "this will require re - doing the costly preprocessing with every change in query .",
    "thus , the query time which is dependent on @xmath115 should be independent of the query",
    ".    transformations @xmath13 and @xmath12 were precisely meant to remove the dependency of correlation on the norms of @xmath47 but at the same time keeping the inner products same .",
    "realizing the fact that we are allowed asymmetry , we can use the same idea to get rid of the norm of @xmath19 .",
    "let @xmath126 be the upper bound on all the norms i.e. @xmath127 .",
    "in other words @xmath126 is the radius of the space .",
    "let @xmath113 , define the transformations , @xmath128 as @xmath129 and transformations @xmath130 are the same for the sign - alsh scheme as defined in eq ( [ eq : p ] ) and ( [ eq : q ] ) .",
    "given the query @xmath19 and any data point @xmath47 , observe that the inner products between @xmath131 and @xmath132 is @xmath133    @xmath131 appends first m zeros components to @xmath134 and then @xmath68 components of the form @xmath135 .",
    "@xmath136 does the same thing but in a different order .",
    "now we are working in @xmath137 dimensions .",
    "it is not difficult to see that the norms of @xmath131 and @xmath136 is given by latexmath:[\\[\\begin{aligned }     the transformations are very asymmetric but we know that it is necessary .",
    "therefore the correlation or the cosine similarity between @xmath131 and @xmath132 is @xmath139 note @xmath140 , therefore both @xmath141 and @xmath142 converge to zero at a tower rate and we get approximate monotonicity of correlation with the inner products .",
    "we can apply sign random projections to hash @xmath131 and @xmath136 .",
    "using the fact @xmath143 and @xmath144 , it is not difficult to get @xmath145 and @xmath146 for sign - alsh , without any conditions on any norms . simplifying the expression , we get the following value of optimal @xmath147 ( u for unrestricted ) .",
    "@xmath148 with this value of @xmath149 , we can state our main theorem .",
    "[ theo : main ] for the problem of @xmath14-approximate mips in a bounded space , one can construct a data structure having @xmath150 query time and space @xmath151 , where @xmath152 is the solution to constraint optimization ( [ eq : optrho_u ] ) .",
    "note , for all @xmath32 , we always have @xmath152 because the constraint @xmath153 is always true for big enough @xmath68 . the only assumption for efficiently solving mips that we need",
    "is that the space is bounded , which is always satisfied for any finite dataset .",
    "@xmath149 depends on @xmath126 , the radius of the space , which is expected .",
    "in  @xcite , the l2-alsh scheme was shown to outperform the lsh for l2 distance in retrieving maximum inner products . since our proposal is an improvement over l2-alsh",
    ", we focus on comparisons with l2-alsh . in this section ,",
    "we compare l2-alsh with sign - alsh based on ranking .",
    "we use the two popular collaborative filtering datasets * movielens 10 m * and * netflix * , for the task of item recommendations .",
    "these are also the same datasets used in  @xcite .",
    "each dataset is a sparse * user - item matrix * @xmath154 , where @xmath155 indicates the rating of user @xmath156 for movie @xmath157 . for getting the latent feature vectors from user item matrix , we follow the methodology of  @xcite .",
    "they use puresvd procedure described in  @xcite to generate user and item latent vectors , which involves computing the svd of @xmath154 @xmath158 where @xmath159 is @xmath160 matrix and @xmath161 is @xmath162 matrix for some chosen rank @xmath163 also known as latent dimension .    after the svd step ,",
    "the rows of matrix @xmath164 are treated as the user characteristic vectors while rows of matrix @xmath161 correspond to the item characteristic vectors .",
    "this simple procedure has been shown to outperform other popular recommendation algorithms for the task of top item recommendations in  @xcite , on these two datasets .",
    "we use the same choices for the latent dimension @xmath163 , i.e. , @xmath165 for movielens and @xmath166 for netflix as  @xcite .      in this section , we show how the ranking of the two alsh schemes , l2-alsh and sign - alsh , correlates with the top-@xmath167 inner products . given a user @xmath156 and its corresponding user vector @xmath168 , we compute the top-@xmath167 gold standard items based on the actual inner products @xmath169 , @xmath170 .",
    "we then generate @xmath124 different hash codes of the vector @xmath168 and all the item vectors @xmath171s and then compute @xmath172 where @xmath173 is the indicator function and the subscript @xmath174 is used to distinguish independent draws of @xmath26 . based on @xmath175 we rank all the items .",
    "ideally , for a better hashing scheme , @xmath175 should be higher for items having higher inner products with the given user @xmath168 .",
    "this procedure generates a sorted list of all the items for a given user vector @xmath168 corresponding to the each hash function under consideration .    for l2-alsh",
    ", we used the same parameters used and recommended in  @xcite . for sign - alsh , we used the two recommended choices shown in section  [ sec_parameter ] , which are @xmath176 , @xmath177 and @xmath178 , @xmath179 .",
    "it should be noted that sign - alsh does not have the parameter @xmath37 .",
    "we compute the precision and recall of the top-@xmath167 items for @xmath180 , obtained from the sorted list based on @xmath181 . to compute this precision and recall , we start at the top of the ranked item list and walk down in order .",
    "suppose we are at the @xmath182 ranked item , we check if this item belongs to the gold standard top-@xmath167 list .",
    "if it is one of the top-@xmath167 gold standard item , then we increment the count of _ relevant seen _ by 1 , else we move to @xmath183 . by @xmath182 step ,",
    "we have already seen @xmath122 items , so the _ total items seen _ is @xmath122 .",
    "the precision and recall at that point is then computed as : @xmath184 we show performance for @xmath185 .",
    "note that it is important to balance both precision and recall .",
    "the method which obtains higher precision at a given recall is superior .",
    "higher precision indicates higher ranking of the relevant items .",
    "we report averaged precisions and recalls over 2000 randomly chosen users .",
    "the plots for movielens and netflix datasets are shown in figure  [ fig_movielensranking ] and figure  [ fig_netflixranking ] respectively .",
    "we can clearly see , that our proposed sign - alsh scheme gives significantly higher precision recall curves than the l2-alsh scheme , indicating better correlation of the top neighbors under inner products with sign - alsh compared to l2-alsh .",
    "in addition , there is not much difference in the two different combinations of the parameters @xmath116 and @xmath68 in sign - alsh .",
    "the results are very consistent across both datasets .",
    "in this section , we evaluate the actual savings in the number of inner product evaluations for recommending top-@xmath167 items for the movielens dataset .",
    "for this , we implemented the standard @xmath186 algorithms in  @xcite , where @xmath124 is number of hashes in each hash table and @xmath125 is the total number of tables . for each query point , the returned results are the union of matches in all @xmath125 tables . to find the top-@xmath167 items , we need to compute the actual inner products only on the candidate items retrieved by the bucketing procedure .",
    "in this experiment , we choose @xmath187 and compute the recall value for each combination of @xmath188 for every query .",
    "for example , given query @xmath19 and a @xmath186-lsh scheme , if @xmath189 and only 5 of the true top-10 data points are retrieved , the recall will be @xmath190 for this @xmath188 . at the same time",
    ", we can also compute the * fip * ( fraction of inner products ) : @xmath191 which is basically the total number of inner products evaluation ( where @xmath192 represents the cost of hashing ) , normalized by the total number of items in the repository .",
    "thus , for each @xmath19 and @xmath188 , we can compute two values : recall and fip .",
    "we also need to figure out a way to aggregate the results for all queries .",
    "typically the performance of bucketing algorithm is very sensitive to the choice of hashing parameters @xmath124 and @xmath125 . ideally , to find best @xmath124 and @xmath125",
    ", we need to know the operating threshold @xmath21 and the approximation ratio @xmath14 in advance .",
    "unfortunately , the data and the queries are very diverse and therefore for retrieving top-@xmath167 near neighbors there is no common fixed threshold @xmath21 and approximation ratio @xmath14 that works for different queries .",
    "our goal is to compare the hashing schemes , and minimize the effect of @xmath124 and @xmath125 on the evaluation . to get away with the effect of @xmath124 and @xmath125",
    ", we perform rigorous evaluations of various @xmath124 and @xmath125 which includes optimal choices at various thresholds . for both the hashing schemes , we then select the best performing @xmath124 and @xmath125 and report the performance .",
    "this involves running the bucketing experiments for thousands of combinations and then choosing the best @xmath124 and @xmath125 to marginalize the effect of parameters in the comparisons .",
    "this all ensures that our evaluation is fair .",
    "we choose the following scheme . for each @xmath188",
    ", we compute the averaged recall and averaged fip , over all queries .",
    "then for each `` target '' recall level ( and @xmath167 ) , we can find the @xmath186 which produces the best ( lowest ) averaged fip . this way , for each @xmath167 , we can compute a `` fip - recall '' curve , which can be used to compare sign - alsh with l2-alsh .",
    "we use @xmath193 and @xmath194 .",
    "the results are summarized in figure  [ fig_movielenslsh ] .",
    "we can clearly see from the plots that for achieving the same recall for top-@xmath167 , sign - alsh scheme needs to do less computations compared to l2-alsh .",
    "the mips ( maximum inner product search ) problem has numerous important applications in machine learning , databases , and information retrieval .",
    "@xcite developed the framework of asymmetric lsh and provided an explicit scheme ( l2-alsh ) for approximate mips in sublinear time . in this study , we present another asymmetric transformation scheme ( sign - alsh ) which converts the problem of maximum inner products into the problem of maximum correlation search , which is subsequently solved by sign random projections . theoretical analysis and experimental study demonstrate that _ sign - alsh _ can be noticeably more advantageous than _",
    "the research is supported in part by onr - n00014 - 13 - 1 - 0764 , nsf - iii-1360971 , afosr - fa9550 - 13 - 1 - 0137 , and nsf - bigdata-1419210 . the method and theoretical analysis for _ sign - alsh _",
    "were conducted right after the initial submission of our first work on alsh  @xcite in february 2014 .",
    "the intensive experiments ( especially the lsh bucketing experiments ) , however , were not fully completed until june 2014 due to the demand of computational resources , because we exhaustively experimented a wide range of @xmath124 ( number of hashes ) and @xmath125 ( number of tables ) for implementing @xmath186-lsh schemes . here",
    ", we also would like to thank the computing supporting team ( lcsr ) at rutgers cs department as well as the it support staff at rutgers statistics department , for setting up the workstations especially the server with 1.5 tb memory .",
    "p.  cremonesi , y.  koren , and r.  turrin . performance of recommender algorithms on top - n recommendation tasks . in",
    "_ proceedings of the fourth acm conference on recommender systems _ , pages 3946 .",
    "acm , 2010 .",
    "t.  dean , m.  a. ruzon , m.  segal , j.  shlens , s.  vijayanarasimhan , and j.  yagnik .",
    "fast , accurate detection of 100,000 object classes on a single machine . in _",
    "computer vision and pattern recognition ( cvpr ) , 2013 ieee conference on _ , pages 18141821 .",
    "ieee , 2013 .",
    "a.  shrivastava and p.  li .",
    "asymmetric lsh ( alsh ) for sublinear time maximum inner product search ( mips ) .",
    "technical report , arxiv:1405.5869 ( to appear in nips 2014 . initially submitted to kdd 2014 ) ,"
  ],
  "abstract_text": [
    "<S> recently it was shown that the problem of maximum inner product search ( mips ) is efficient and it admits provably sub - linear hashing algorithms . </S>",
    "<S> asymmetric transformations before hashing were the key in solving mips which was otherwise hard . in  @xcite , </S>",
    "<S> the authors use asymmetric transformations which convert the problem of approximate mips into the problem of approximate near neighbor search which can be efficiently solved using hashing . in this work , </S>",
    "<S> we provide a different transformation which converts the problem of approximate mips into the problem of approximate cosine similarity search which can be efficiently solved using signed random projections . </S>",
    "<S> theoretical analysis show that the new scheme is significantly better than the original scheme for mips . </S>",
    "<S> experimental evaluations strongly support the theoretical findings . </S>"
  ]
}