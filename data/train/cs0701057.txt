{
  "article_text": [
    "optimization is a core problem both in mathematics and computer science .",
    "it is a very active research area with many international conferences every year , a large amount of literature , and many researchers and users across many fields for a wide range of applications .",
    "combinatorial optimization  @xcite is a branch of optimization where the set of feasible solutions of problems is discrete , countable , and of a finite size .",
    "the general methods for combinatorial optimization are 1 ) local search  @xcite , 2 ) simulated annealing  @xcite , 3 ) genetic algorithms  @xcite , 5 ) ant colony optimization  @xcite , 4 ) tabu search  @xcite , 5 ) branch - and - bound  @xcite 6 ) dynamic programming  @xcite .",
    "the successful applications of different combinatorial optimization methods have been reported in solving a large variety of optimization problems in practice .",
    "optimization is important in the areas of computer vision , pattern recognition , and image processing .",
    "for example , stereo matching is one of the most active research problems in computer vision @xcite .",
    "the goal of stereo matching is to recover the depth image of a scene from a pair of 2-d images of the same scene taken from two different locations . like many other problems from these areas",
    ", it can be formulated as a combinatorial optimization problem , which is np - hard @xcite in computational complexity in general .",
    "the researchers in computer vision have developed a number of search techniques which have been proven effective in practice for finding good solutions for combinatorial optimization problems .",
    "two well - known ones are the cooperative algorithm proposed by d.  marr and t.  poggio in @xcite for stereo matching and the probabilistic relaxation proposed by a.  rosenfield et al  @xcite for scene labeling .",
    "recently , there are some remarkable progresses in discovering new optimization methods for solving computer vision problems .",
    "graph cuts  @xcite is a powerful specialized optimization technique popular in computer vision .",
    "it has the best known results in energy minimization in the two recent evaluations of stereo algorithms  @xcite , more powerful than the classic simulated annealing method .",
    "however , graph cuts has a limitation in its scope because it is only applicable when the energy minimization of a vision problem can be reduced into a problem of finding the minimum cut in a graph  @xcite .    the second optimization method is so called the sum - product algorithm  @xcite , a generalized belief propagation algorithm developed in ai  @xcite .",
    "the sum - product algorithm is the most powerful optimization method ever found so far for attacking hard optimization problems raised from channel decoding in communications .",
    "the min - sum algorithm and max - product algorithm  @xcite are its variations .",
    "it has also been successful applied to solve several computer vision problems with promising experimental results  @xcite .    the third method proposed recently",
    "is so called max - product tree - reweighted message passing  @xcite .",
    "it is based on a lower bounding technique called linear programming relaxation .",
    "its improvement has been proposed recently and its successful applications in computer vision have been reported  @xcite .",
    "the cooperative optimization is a newly discovered general optimization method for attacking hard optimization problems  @xcite .",
    "it has been found in the experiments  @xcite that cooperative optimization has achieved remarkable performances at solving a number of real - world np - hard problems with the number of variables ranging from thousands to hundreds of thousands .",
    "the problems span several areas , proving its generality and power .",
    "for example , cooperative optimization algorithms have been proposed for dna image analysis  @xcite , shape from shading  @xcite , stereo matching  @xcite , and image segmentation  @xcite . in the second case",
    ", it significantly outperformed the classic simulated annealing in finding global optimal solutions . in the third case ,",
    "its performance is comparable with graph cuts in terms of solution quality , and is twice as faster as graph cuts in software simulation using the common evaluation framework for stereo matching  @xcite . in the fourth case ,",
    "it is ten times faster than graph cuts and has reduced the error rate by two to three factors . in all these cases ,",
    "its memory usage is efficient and fixed , its operations are simple , regular , and fully scalable .",
    "all these features make it suitable for parallel hardware implementations .",
    "this paper is organized in three major themes as 1 ) a formal presentation for cooperative optimization , 2 ) design issues , and 3 ) a case study .",
    "they are the generalization and the extension of the previous papers on cooperative optimization . in the case study ,",
    "another cooperative optimization algorithm for stereo matching besides the one proposed before  @xcite is offered to demonstrate the power and flexibility of cooperative optimization .",
    "compared with the previous one for stereo matching , the new one lowers the energy levels of solutions further and is more than ten times faster . just like the previous one , the new one is also simple in computation and fully parallel in operations , suitable for hardware implementations .",
    "different forms of cooperative optimization can be derived from different cooperation schemes .",
    "the basic form defines an important collection of cooperative optimization algorithms .",
    "there are two different ways to derive it ; namely , 1 ) as a cooperative multi - agent system for distributed optimization and 2 ) as a lower bounding technique for finding global optimums .",
    "each way offers its own inspirations and insights to understand the algorithms .",
    "this section describes the first way . the following section offers the description for the second way .",
    "readers who are not interested in them can directly jump to section  [ sec_gc_form ] for a general description of cooperative optimization .",
    "those three sections are relatively independent to each other .",
    "team playing is a common social behavior among individuals of the same species ( or different ) where the team members working together can achieve goals or solve hard problems which are beyond the capability of any member in the team .",
    "often times , team playing is achieved through competition and cooperation among the members in a team .",
    "usually , competition or cooperation alone can hardly lead to good solutions either for a team or for the individuals in the team . without competition , individuals in a team may lose motivation to pursue better solutions . without cooperation , they might directly conflict with each other and poor solutions might be reached both for the team and themselves . through properly balanced competition and cooperation , individuals in a team can find the best solutions for the team and possibly good solutions for themselves at the same time .    in the terms of computer science , we can view a team of this kind as a cooperative system with multiple agents . in the system",
    ", each agent has its own objective .",
    "the collection of all the agent s objectives form the objective of the system .",
    "we can use a cooperative system to solve a hard optimization problem following the divide - and - conquer principle .",
    "we first break up the objective function of the optimization problem into a number of sub - objective functions of manageable sizes and complexities . following that",
    ", we assign each sub - objective function to an agent in a system as the agent s own objective function and ask those agents in the system to optimize their own objective functions through competition and cooperation .",
    "( throughout this paper , we use the term `` objective '' and `` objective function '' interchangeably since the objective of an optimization problem is defined by an objective function and this paper focuses only on optimizing objective functions . )",
    "specifically , the competition is achieved by asking each agent to optimize its own objective function by applying problem - specific optimization methods or heuristics .",
    "however , the objectives of agents may not be always aligned with each other . in other words ,",
    "the best solutions of the agents for optimizing their own objective functions may conflict with each other . to resolve the conflicts",
    ", each agent passes its solution to its neighbors through local message passing . after receiving its neighbor s solutions",
    ", each agent compromises its solution with the solutions of its neighbors .",
    "the solution compromising is achieved by modifying the objective function of each agent to take into account its neighbors solutions .",
    "it is important to note that solution compromising among agents is a key concept for understanding the cooperation strategy introduced by cooperative optimization .",
    "let the objective of the individual @xmath0 be @xmath1 .",
    "let the solution of the individual @xmath0 at time @xmath2 be @xmath3 .",
    "let the collection of solutions of the neighbors of the individual @xmath0 at time @xmath2 be @xmath4 .",
    "the basic operations of a cooperative system are organized as a process shown in figure  [ fig_cooperative_system ] .",
    "123456789012345678901   + for each individual @xmath0 in the system , find the initial solution , + @xmath5 ; + _ iteration _",
    "+ for each individual @xmath0 in the system , + modify its original objective by including its neighbors solutions , + @xmath6 + @xmath7 ; + find solutions of the modified objective , + @xmath8 ;    the process of a cooperative system of this kind is iterative and self - organized and each agent in the system is autonomous .",
    "the system is also inherently distributed and parallel , making the entire system highly scalable and less vulnerable to perturbations and disruptions on individuals than a centralized system . despite of its simplicity",
    ", it has many interesting emerging behaviors and can attack many challenging optimization problems .      in light of the cooperative multi - agent system for distributed optimization described in fig .",
    "[ fig_cooperative_system ] , we can derive the basic form of cooperative optimization now .",
    "it is based on a direct way for defining the solution of each agent and a simple way to modify the objective of each agent .",
    "the derivation can be generalized further in a straightforward way to any other definitions of solutions and modifications of objectives .    given",
    "a multivariate objective function @xmath9 of @xmath10 variables , or simply denoted as @xmath11 , where each variable @xmath12 is of a finite domain @xmath13 of size @xmath14 .",
    "assume that @xmath11 can be decomposed into @xmath10 sub - objective functions @xmath15 , denoted as @xmath16 , satisfying    1 .",
    "@xmath17 [ condition_1 ] , 2 .",
    "@xmath15 , for @xmath18 , contains at least variable @xmath12 , 3 .",
    "the minimization of @xmath15 , for @xmath18 , is computationally manageable in complexity .",
    "let us assign @xmath15 as the objective of agent @xmath0 , @xmath19 there are @xmath10 agents in the system , one agent for each sub - objective function .",
    "let the initial solution of agent @xmath0 be the minimization result of @xmath15 defined as follows , @xmath20 where @xmath21 is the set of variables contained in @xmath15 , and @xmath22 stands for minimizing with respect to all variables in @xmath21 excluding @xmath12 .",
    "the solution is an unary function on variable @xmath12 , denoted as @xmath23 .",
    "assume that the system takes discrete - time with iteration step @xmath24 . to simplify notations ,",
    "let @xmath25 be the modified objective function of agent @xmath0 at iteration @xmath26 , i.e. , @xmath27 it is also referred to as the @xmath0-th modified sub - objective of the system . the agent s solution at the iteration",
    "is defined as @xmath28 the solution is an unary function on variable @xmath12 , denoted as @xmath29 .",
    "it is the state of agent @xmath0 at iteration @xmath26 .",
    "it can be represented as a vector of real values of size @xmath14 , the domain size of variable @xmath12 .",
    "the @xmath0-th equation in ( [ solution_i ] ) defines the dynamics of agent @xmath0 .",
    "all the @xmath10 equations define the dynamics of the system .",
    "as described in the previous subsection , the cooperation among the agents in the system is introduced by solution compromising via modifying the objective of each agent .",
    "let agent @xmath0 define its modified objective function @xmath25 at iteration @xmath26 as a linear combination of its original objective @xmath15 and the solutions of its neighbors at the previous iteration @xmath30 as follows , @xmath31 where @xmath32 and @xmath33 are coefficients of the linear combination .",
    "agent @xmath34 is the neighbor of agent @xmath0 if variable @xmath35 of the same index @xmath34 is contained in the agent @xmath0 s objective function @xmath15 .",
    "( based on this definition , the agent @xmath0 is also a neighbor of itself .",
    "such a generalization is necessary because there is no restriction to have agent @xmath0 modify its objective using its own solution . )",
    "the neighbors of agent @xmath0 is denoted as @xmath36 , i.e. , @xmath37 .",
    "specifically , it is defined as the set of indices as @xmath38    substituting eq .",
    "( [ modified_objective_function ] ) into eq .",
    "( [ solution_i ] ) and letting @xmath39 if @xmath40 , the dynamics of the cooperative system can be written as the following @xmath10 difference equations , @xmath41 such a set of difference equations defines a basic cooperative optimization system ( algorithm ) for minimizing an objective function of the form @xmath42 .    at iteration @xmath26 , variable @xmath12 , for @xmath18",
    ", has a value in the solution for minimizing the @xmath0-th modified sub - objective function @xmath25 .",
    "it is denoted as @xmath43 , i.e. , @xmath44 from ( [ solution_i ] ) , we have @xmath45 the agent @xmath0 is responsible for assigning that value to variable @xmath12 .",
    "the assignments of other variables are taken care of by other agents .",
    "all these values together form a solution of the system at iteration @xmath26 , denoted as @xmath46 .",
    "putting everything together , we have the pseudo code of the algorithm is given in figure  [ fig_cooperative_optimization_algorithm ] . the global optimality condition mentioned in the line",
    "@xmath47 will be discussed in detail later in this paper .",
    "* procedure * basic cooperative optimization algorithm + 123456789012345678901   1 initialize the soft assignment function @xmath23 , for each @xmath0 ; + 2 @xmath48 to @xmath49 * do * + 3 each @xmath0 * do * + / * modify the @xmath0-th sub - objective function @xmath15 * / + 4@xmath50 ; + / * minimize the modified sub - objective function * / + 5@xmath51 ; + / * find the best value for @xmath12 * / + 6@xmath52 ; + 7 @xmath46 is a global optimal solution * return * @xmath46 ; + 8 @xmath46 ; / * as an approximate solution * /      the coefficient @xmath32 in ( [ cooperative_optimization ] ) controls the level of the cooperation among the agents at iteration @xmath26 .",
    "it is so called the cooperation strength , satisfying @xmath53 . from ( [ cooperative_optimization ] )",
    "we can see that , for each agent , a high value for @xmath32 will weigh the solutions of the other agents more than its own objective @xmath15 . in other words ,",
    "the agents in the system tend to compromise more with their solutions . as a consequence , a strong level of cooperation",
    "is reached in this case .",
    "if the cooperation strength @xmath32 is of a small value , the cooperation among the agents is weak .",
    "particularly , if it is equal to zero , there is no cooperation among the agents and each agent minimizes its own objective function independently ( see ( [ cooperative_optimization ] ) ) .",
    "the coefficients @xmath33 control the propagation of solutions @xmath54 , for @xmath55 , as messages among the agents in the system .",
    "all @xmath33s together form a @xmath56 matrix called the propagation matrix . to have @xmath42 as the objective function to be minimized , it is required  @xcite that the propagation matrix @xmath57 is non - negative and @xmath58 to have solutions @xmath54 uniformly propagated among all the agents , it is required  @xcite that the propagation matrix @xmath59 is irreducible .",
    "a matrix @xmath59 is called reducible if there exists a permutation matrix @xmath60 such that @xmath61 has the block form @xmath62    the role of propagation matrices in basic cooperative optimization algorithms is exactly same as the one of transition matrices in markov chains ( or random walks over directed graphs ) . in a markov chain ,",
    "a transition matrix governs the distribution of states over time . in a basic cooperative optimization algorithm ,",
    "a propagation matrix governs the distribution of solutions among agents .",
    "the mathematical foundation for analyzing markov chains has been well established .",
    "they can be directly applied to analyze the message propagation of cooperative optimization .      as mentioned before , the solution @xmath29 of agent @xmath0 at iteration @xmath26 is an unary function on @xmath12 storing the solution of minimizing the @xmath0-th modified sub - objective function @xmath25 ( see ( [ solution_i ] ) ) .",
    "given a value of @xmath12 , @xmath29 is the minimal value of @xmath25 with the variable @xmath12 fixed to that value . to minimize @xmath25 , the values of @xmath12 which have smaller function values @xmath29 are preferred more than those of higher function values .",
    "the best value for assigning the variable @xmath12 is the one of the minimal function value @xmath29 ( see ( [ cop_solution ] ) ) .",
    "therefore , @xmath29 is inversely related to the preferences over different values of @xmath12 for minimizing @xmath25 .",
    "it is so called the assignment constraint on variable @xmath12 , an algorithm introduced constraint on the variable .",
    "it can also be viewed as a soft decision made by the agent for assigning the variable @xmath12 at iteration @xmath26 .",
    "in particular , a soft decision of agent @xmath0 falls back to a hard decision for assigning the variable @xmath12 when the agent accept only one value and reject all the rest values .",
    "such a hard decision can be represented by the assignment constraint @xmath29 as @xmath63 , for some @xmath64 , and @xmath65 for any @xmath66 .    with that insight , it can be understood now that the messages propagated around among the agents in a basic cooperative optimization system are the soft decisions for assigning variables .",
    "an agent can make a better decision using soft decisions propagated from its neighbors than using the hard ones instead .",
    "it is important to note that soft decision making is a critical feature of cooperative optimization , which makes it fundamentally different from many classic optimization methods where hard decisions are made for assigning variables .      given an objective function of the following form @xmath67 where each variable is of a finite domain .",
    "the goal is to seek values ( labels ) of the five variables such that the objective function is minimized .",
    "let us simply denote the function as @xmath68    to design a basic cooperative optimization algorithm to minimize the objective function , we first decompose it into the following five sub - objective functions , @xmath69    a propagation matrix @xmath59 of dimensions @xmath70 can be chosen as @xmath71    with the decomposition and the propagation matrix , substituting them into ( [ cooperative_optimization ] ) we have a basic cooperative optimization algorithm with five difference equations for minimizing the five sub - objective functions in an iterative and cooperative way .",
    "replacing @xmath72 by @xmath73 in the difference equations  ( [ cooperative_optimization ] ) , we have the basic canonical form of cooperative optimization as @xmath74    the basic form of cooperative optimization  ( [ cooperative_optimization ] ) has its cooperation strength @xmath32 restricted to @xmath53 .",
    "it is because its difference equations  ( [ cooperative_optimization ] ) do not make sense when @xmath75 .",
    "however , such a restriction can be relaxed to @xmath76 for the basic canonical form  ( [ cooperative_optimization_canonical ] ) . often in times in practice ,",
    "the basic canonical form is preferred over the basic one because the cooperation strength @xmath32 in the former has a broader range to choose from to maximize performance .",
    "in principle , a basic cooperative optimization algorithm can be understood as a lower bounding technique for finding global minima .",
    "it first initializes a function of some form as a lower bound function to an objective function .",
    "one may intentionally choose a form for the lower bound function such that the minimization of the function is simple in computation .",
    "following that , the algorithm progressively tightens the lower bound function until its global minimum touches the global minimum of the original objective function .",
    "the latter is then found by searching the former instead ( see the illustration in fig .",
    "[ fig_lowerbound ] ) .",
    "specifically , let the objective function to be minimized be @xmath11 .",
    "assume that the initial lower bound function be @xmath77 , @xmath78 . from @xmath77",
    ", assume that the algorithm progressively tightens the function in an iterative way such that @xmath79 where @xmath26 is the iteration number .",
    "let the global minimum of the lower bound function @xmath80 at iteration @xmath26 be @xmath46 . finding @xmath46 is simple in computation due to the simple form of the lower bound function @xmath80 . at iteration @xmath26 , if the algorithm found that the lower bound function @xmath80 at the solution @xmath46 has the same function value as the original objective function @xmath11 , i.e. , @xmath81 in other words , the two functions touch each other at the point where @xmath82 in the search space",
    ". then @xmath46 must also be the global minimum of @xmath11 simply because @xmath83 such a condition implies that the lower bound function @xmath80 has been tightened enough such that its global minimum @xmath46 touches the global minimum of the original objective function @xmath11 .",
    "the latter is thus found by searching the former instead .",
    "such a lower bounding technique is so called the bound function tightening technique for optimization .",
    "there are other lower bounding techniques based on principles different from this one .",
    "examples are lagrangian relaxation techniques , cutting plane techniques , branch - and - bound algorithms , and branch - and - cut algorithms .      in light of the bound function tightening technique described in the previous subsection , we can derive the basic form of cooperative optimization based on a simple form of lower bound functions .",
    "the derivation can be generalized further in a straightforward way to any other forms of lower bound functions .    given an objective function of @xmath10 variables , @xmath9 , or simply denoted as @xmath11 .",
    "assume that @xmath84 is a lower bound function of @xmath11 defined on the same set of variables .",
    "obviously the linear combination of the two functions , @xmath85 defines a new lower bound function of @xmath11 if the parameter @xmath86 satisfying @xmath87 .",
    "let us choose a simple form for the lower bound function as @xmath88 where @xmath89 is an unary component function defined on variable @xmath12 , for @xmath90 .",
    "its global minimum , denoted as @xmath91 , can be easily found by minimizing the unary component functions @xmath89 independently as @xmath92    assume that the objective function @xmath11 can be decomposed into @xmath10 sub - objective functions , @xmath93 the lower bound function @xmath84 can also be easily decomposed into @xmath10 sub - functions as follows @xmath94 based on the two decompositions , the new lower bound function  ( [ new_lbf ] ) can be rewritten as @xmath95 to put the above function in a simple form , let @xmath96 then it can be rewritten simply as @xmath97 in the above sum , let @xmath98 be the set of variables contained in the @xmath0-th component function @xmath99 .",
    "if we minimize the function with respect to all variables in @xmath98 except for @xmath12 , we obtain an unary function defined on @xmath12 , denoted as @xmath100 , i.e. , @xmath101 the sum of those unary functions defines another lower bound function of @xmath11 , denoted as @xmath102 , i.e. , @xmath103 this new lower bound function has exactly the same form as the original one @xmath104 .",
    "therefore , from a lower bound function @xmath105 of the form @xmath106 , we can compute another lower bound function @xmath102 of the same form .",
    "such a process can be repeated and we can have an iterative algorithm to compute new lower bound functions .    rewriting eq .",
    "( [ compute_psi_i ] ) in an iterative format , we have @xmath107 where @xmath26 is the iteration step , @xmath24 .",
    "the above @xmath10 difference equations define a basic cooperative optimization algorithm for minimizing an objective function @xmath11 of the form @xmath42 .",
    "the solution at iteration @xmath26 , denoted as @xmath46 , is defined as the global minimal solution of the lower bound function @xmath80 at the iteration , i.e. , @xmath108 which can be easily obtained as @xmath109    if @xmath110 at some iteration @xmath26 , then the solution @xmath46 must be the global minimum of the original objective function @xmath11 .    without loss of generality , we assume in the following discussions that all sub - objective functions @xmath15 are nonnegative ones",
    ". one may choose the initial condition as @xmath111 , for any value of @xmath12 and @xmath90 .",
    "the parameter @xmath32 can be varied from one iteration to another iteration .",
    "if it is of a constant value and the above initial condition has been chosen , cooperative optimization theory  @xcite tells us that the lower bound function @xmath80 is monotonically non - decreasing as shown in ( [ monotonically_nondecreasing ] ) .",
    "it has been shown that a basic cooperative optimization algorithm  ( [ cooperative_optimization ] ) has some important computational properties  @xcite . given a constant cooperation strength @xmath86 , i.e. , @xmath112 for all @xmath26s , the algorithm has one and only one equilibrium .",
    "it always converges to the unique equilibrium with an exponential rate regardless of initial conditions and perturbations .",
    "the two convergence theorems proved in @xcite are very important and so they are listed here again .",
    "one formally describes the existence and the uniqueness of the equilibrium of the algorithm , and the another reveals the convergence property of the algorithm .",
    "a basic cooperative optimization algorithm with a constant cooperation strength @xmath86 ( @xmath87 ) has one and only one equilibrium .",
    "that is , the difference equations  ( [ cooperative_optimization ] ) of the algorithm have one and only one solution ( equilibrium ) , denoted as a vector @xmath113 , or simply @xmath114 .",
    "[ theorem_7 ]    a basic cooperative optimization algorithm with a constant cooperation strength @xmath86 ( @xmath87 ) converges exponentially to its unique equilibrium @xmath114 with the rate @xmath86 with any choice of the initial condition @xmath115 .",
    "that is , @xmath116 where @xmath117 is the maximum norm of the vector @xmath118 defined as @xmath119 [ theorem_8 ]    the two theorems indicate that every basic cooperative optimization algorithm  ( [ cooperative_optimization ] ) is stable and has a unique attractor , @xmath114 .",
    "hence , the evolution of the algorithms is robust , insensitive to perturbations .",
    "the final solution of the algorithms is independent of their initial conditions .",
    "in contrast , the conventional algorithms based on iterative local improvement of solutions may have many local attractors due to the local minima problem .",
    "the evolution of those local optimization algorithms are sensitive to perturbations , and the final solution of those algorithms is dependent on their initial conditions .",
    "furthermore , the basic cooperative optimization algorithms  ( [ cooperative_optimization ] ) possess a number of global optimality conditions for identifying global optima .",
    "they know whether a solution they found is a global optimum so that they can terminate their search process efficiently .",
    "however , this statement does not imply that _ np = p _ because a basic cooperative optimization algorithm can only verify within a polynomial time whether a solution it found is a global optimum or not .",
    "it can not decide the global optimality for any given solution other than those it found .",
    "it is important to note that a basic canonical cooperative optimization algorithm  ( [ cooperative_optimization_canonical ] ) may no longer possess the unique equilibrium property when its cooperation strengths at some iterations are greater than one , i.e. , @xmath120 for some @xmath26s . in this case",
    ", the algorithm may have multiple equilibriums .",
    "it can evolve into any one of them depending on its initial settings of the assignment constraints @xmath23 @xmath121 .",
    "as described before , a basic cooperative optimization algorithm is defined by the @xmath10 difference equations  ( [ cooperative_optimization ] ) .",
    "the @xmath0-th equation defines the minimization of the @xmath0-th modified sub - objective function @xmath25 ( defined in ( [ modified_objective_function ] ) ) .",
    "given any variable , say @xmath12 , it may be contained in several modified sub - objective functions .",
    "at each iteration , @xmath12 has a value in the optimal solution for minimizing each of the modified sub - objective functions containing the variable",
    ". those values may not be the same .",
    "if all of them are of the same value at some iteration , we say that the cooperative optimization algorithm reach a consensus assignment for that variable .",
    "moreover , if a consensus assignment is reached for every variable of the problem at hand at some iteration , we call the minimization of the @xmath10 modified sub - objective functions reaches a solution consensus . that is",
    ", there is no conflict among the solutions in terms of variable assignments for minimizing those functions . in this case , those consensus assignments form a solution , called a consensus solution , and the algorithm is called reaching a consensus solution .    to be more specific , given @xmath10 modified sub - objective functions , @xmath99 , for @xmath90 ( to simplify notation , let us drop the superscript @xmath26 temporarily ) .",
    "let the optimal solution of the @xmath0-th modified sub - objective function be @xmath122 , i.e. , @xmath123 assume that variable @xmath12 is contained in both @xmath34-th and @xmath26-th modified sub - objective functions @xmath124 , @xmath125 .",
    "however , it is not necessary that @xmath126 given a variable @xmath12 , if the above equality holds for any @xmath34 and @xmath26 where @xmath124 and @xmath125 contain @xmath12 , then a consensus assignment is reached for that variable with the assignment value denoted as @xmath127 .",
    "moreover , if the above statement is true for any variable , we call the minimization for all @xmath99s reaches a solution consensus .",
    "the solution @xmath91 with @xmath127 as the value of variable @xmath12 is called a consensus solution .    as defined before , @xmath98 stands for the set of variables contained in the function @xmath99 .",
    "@xmath98 is a subset of variables , i.e. , @xmath128 . let @xmath129 stand for the restriction of a solution @xmath91 on @xmath98 . another way to recognize a consensus solution @xmath91 is to check if @xmath129 , for any @xmath0 , is the global minimum of @xmath99 , i.e. , @xmath130    simply put , a solution is a consensus one if it is the global minimum of every modified sub - objective function .",
    "consensus solution is an important concept of cooperative optimization .",
    "if a consensus solution is found at some iteration or iterations , then we can find out the closeness between the consensus solution and the global optimal solution in cost .",
    "the following theorem from @xcite makes these points clearer .",
    "let @xmath131 given any propagation matrix @xmath59 , and the general initial condition @xmath132 , for each @xmath0 , or @xmath133 .",
    "if a consensus solution @xmath91 is found at iteration @xmath134 and remains the same from iteration @xmath134 to iteration @xmath135 , then the closeness between the cost of @xmath91 , @xmath136 , and the optimal cost , @xmath137 , satisfies the following two inequalities , @xmath138 @xmath139 where @xmath140 is the difference between the optimal cost @xmath137 and the lower bound on the optimal cost @xmath141 obtained at iteration @xmath142 .    in particular , if @xmath143 , for @xmath144 , when @xmath145 , @xmath146 that is , the consensus solution @xmath91 must be global minimum of @xmath11 , i.e.,@xmath147 .",
    "[ theorem_2 ]    consensus solution is also an important concept of cooperative optimization for defining global optimality conditions .",
    "the cooperative optimization theory tells us that a consensus solution can be the global minimal solution . as mentioned in the previous subsection that a basic cooperative optimization algorithm has one and only one equilibrium given a constant cooperation strength .",
    "if a cooperative optimization algorithm reaches an equilibrium after some number of iterations and a consensus solution is found at the same time , then the consensus solution must be the global minimal solution , guaranteed by theory .",
    "the following theorem ( with its proof in the appendix ) establishes the connection between a consensus solution and a global optimal solution .",
    "[ sufficient_condition ] assume that a basic cooperative optimization  ( [ cooperative_optimization ] ) reaches its equilibrium at some iteration , denoted as @xmath114 .",
    "that is , @xmath114 is a solution to the difference equations  ( [ cooperative_optimization ] ) .",
    "if a consensus solution @xmath148 is found at the same iteration , then it must be the global minimum of @xmath11 , i.e.,@xmath147 .",
    "besides the basic global optimality condition given in the above theorem , a few more ones are offered in @xcite for identifying global optimal solutions .",
    "the capability of recognizing global optimums is a critical property for any optimization algorithm . without any global optimality condition",
    ", it will be hard for an optimization algorithm to know where to find global optimal solutions and whether a solution it found is a global optimum .",
    "finding ways of identifying global optimums for any optimization algorithm is of both practical interests as well as theoretical importance .",
    "the convergence theorem  [ theorem_2 ] can be generalized further to any initial conditions for @xmath115 and @xmath149 , and to any cooperation strength series @xmath150 .",
    "dropping the restriction on the initial conditions @xmath115 and @xmath149 in the theorem , from the difference equations  ( [ cooperative_optimization ] ) , we have @xmath151 it is obvious from the above equation that @xmath152 still approaches @xmath137 exponentially with the rate @xmath86 when the cooperation strength @xmath32 is of a constant value @xmath86 ( @xmath87 ) .    when the cooperation strength @xmath32 is not of a constant value @xmath86 , the convergence to the global optimum is still guaranteed as long as the cooperation strength series @xmath153 is divergent .",
    "let @xmath150 be a sequence of numbers of the interval @xmath154 .    1 .",
    "if @xmath155 , then @xmath156 2 .   if @xmath157 , then @xmath158    [ lemma_infinite_product ]    the proof of the lemma is offered in appendix .    from the above lemma and eq .",
    "( [ generalized_convergence ] ) , the convergence theorem  [ theorem_2 ] can be generalized further as follows .",
    "given any initial conditions , assume that a consensus solution @xmath91 is found by a basic cooperative optimization algorithm at some iteration @xmath26 and remains the same in the following iterations .",
    "if the series @xmath159 is divergent , then @xmath160 that is , the consensus solution @xmath91 must be the global minimal solution @xmath161 , @xmath162 .",
    "[ convergence_theorem_2 ]    if @xmath163 , for instance , the series  ( [ series_lambda ] ) is the harmonic series , @xmath164 the harmonic series is divergent . hence , with the choice of @xmath165 , if a consensus solution @xmath91 is found at some iteration and it remains the same in the following iterations , it must be the global minimal solution @xmath161 .",
    "if @xmath166 , as another example , is a convergent sequence of a positive limit , then @xmath167 is divergent . in this case , a consensus solution is also the global minimal solution .",
    "this statement can be generalized further to cauchy sequences .",
    "every convergent sequence is a cauchy sequence , and every cauchy sequence is bounded .",
    "thus , if @xmath168 is a cauchy sequence of a positive bound , a consensus solution is the global minimal solution .    to maximize the performance of a cooperative optimization algorithm",
    ", it is popular in the experiments to progressively increase the cooperation strength as the iteration of the algorithm proceeds .",
    "a weak cooperation level at the beginning leads to a fast convergence rate  ( see theorem  [ theorem_8 ] ) .",
    "a strong cooperation level at a later stage of the iterations increases the chance of finding a consensus solution .",
    "theorem  [ convergence_theorem_2 ] offers us some general guidance and justification for choosing a variable cooperation strength .",
    "it tells us that the increment of the cooperative strength should not be too fast if we want the guarantee of a consensus solution being the global optimal one .",
    "by combining different forms of lower bound functions and different ways of decomposing objective functions , we can design cooperative optimization algorithms of different complexities and powers for attacking different optimization problems .",
    "the basic canonical form of cooperative optimization  ( [ cooperative_optimization_canonical ] ) can be generalized further in a straightforward way to the general canonical one as follows .    given",
    "a multivariate objective function @xmath9 of @xmath10 variables , or simply denoted as @xmath11 , where each variable is of a finite domain .",
    "assume that @xmath11 can be decomposed into @xmath169 sub - objective functions @xmath15 which may satisfy the condition @xmath170    one may define another function @xmath105 , on the same set of variables as @xmath11 , as the composition of @xmath169 component functions as follows , @xmath171 where @xmath172 is a component function defined on a subset of variables @xmath173 , @xmath174 , for @xmath175 .",
    "@xmath176 is the restriction of @xmath118 on @xmath173 , denoted as @xmath177 .",
    "a cooperative optimization algorithm of the general canonical form is defined as minimizing the @xmath169 sub - objective functions @xmath15 in the following iterative and cooperative way , @xmath178 for @xmath179 . in the equations , @xmath180 is the iteration step ; @xmath21 is the set of variables contained in the functions at the right side of the @xmath0-th equation ; @xmath32 is a real value parameter at iteration @xmath26 satisfying @xmath181 ; and @xmath182 are also real value parameters satisfying @xmath183 .    the solution at iteration @xmath26 is defined as @xmath184 moreover ,",
    "@xmath46 is called a consensus solution if it is the conditional optimum of all the @xmath169 minimization problems defined in ( [ cooperative_optimization_general ] ) .",
    "that is , @xmath185 when @xmath186 and @xmath175 .",
    "one may choose the parameters @xmath32 and @xmath33 in such a way that they further satisfy the conditions of @xmath187 , for all @xmath34s , and all @xmath32s are less than one ( @xmath188 ) .",
    "with the settings , if the algorithm reaches its equilibrium at some iteration and the solution of the iteration is also a consensus one , then it must be the global minimal solution ( this global optimality condition can be proved in the exact same way as that of theorem  [ sufficient_condition ] )",
    ".    the general canonical form can be further generalized to variable propagation matrices , variable forms of lower bound functions , and variable ways of decomposing objective functions .",
    "a basic cooperative optimization algorithm  ( [ cooperative_optimization ] ) ( or a basic canonical one  ( [ cooperative_optimization_canonical ] ) ) is uniquely defined by the objective function decomposition @xmath16 , the cooperation strength series @xmath150 , and the propagation matrix @xmath189 .",
    "some general guideline for designing the cooperation strength series has discussed in the previous section .",
    "this section focuses on the rest two .        a large class of optimization problems",
    "have objective functions of the following form , @xmath190 the function @xmath191 is an unary function on variable @xmath12 , for @xmath90 .",
    "and the function @xmath192 is a binary function on two variables @xmath193 . to note the collection of all defined binary functions , the set @xmath194 is used which contains non - ordered pairs of variable indices where each pair @xmath195 corresponds to a defined binary function @xmath192 .    the above optimization problems",
    "are also referred to as binary constraint optimization problems ( binary cop ) in ai .",
    "the unary function @xmath196 is called an unary constraint on variable @xmath12 and the binary function @xmath192 is called a binary constraint on variables @xmath193 .",
    "binary constraint optimization problems are a very general formulation for many optimization problems arose from widely different fields .",
    "examples are the famous traveling salesman problems , weighted maximum satisfiability problems , quadratic variable assignment problems , stereo vision , image segmentation , and many more .",
    "solving a binary constraint optimization problem is np - hard in computation .",
    "an objective function in form of ( [ binary_cost_function ] ) can be represented with an undirected graph @xmath197 . in the graph , each variable @xmath12 is represented by a node , called a variable node , @xmath198 ; each binary constraint @xmath199 is represented by an undirected edge , connecting the variable nodes @xmath12 and @xmath35 , denoted by a non - ordered pair of variable nodes @xmath200 . by definition ,",
    "the set @xmath201 of the edges of the graph @xmath202 is @xmath203 .",
    "the simple example described in subsection  [ sec_simple_example ] is a binary constraint optimization problem .",
    "the objective function  ( [ obj_simple_example ] ) of the simple example has the form of ( [ binary_cost_function ] ) .",
    "it can be represented by an undirected graph as shown in figure  [ fig_constraint_graph ] .",
    "2.5 cm    if edge @xmath204 , then the variable nodes @xmath193 are called neighbors to each other . in graph theory , they are also called adjacent to each other . each variable node @xmath12 can have a number of neighboring variable nodes .",
    "let @xmath36 be the set of the indices of the neighboring variables of @xmath12 .",
    "by definition , @xmath205    using the notations , we can rewrite the objective function  ( [ binary_cost_function ] ) as @xmath206      the expression  ( [ binary_cost_function2 ] ) for an objective function of a binary constraint optimization problem also defines a straightforward way to decompose the energy function .",
    "that is , @xmath207 obviously , @xmath208 .",
    "this kind of decompositions is so called the straightforward decompositions .",
    "the sub - objective functions @xmath15 in the straightforward decompositions can be easily minimized as @xmath209    using the graphical representation of an objective function can help us to visualize the straightforward decomposition of an objective function .",
    "for example , the decomposition of the objective function of the simple example presented in subsection  [ sec_simple_example ] can be viewed an instance of this kind of decompositions .",
    "the original objective function has a graphical representation shown in figure  [ fig_constraint_graph ] .",
    "each sub - objective function @xmath15 of the decomposition can also be represented by a graph , which must be a subgraph of the original one .",
    "the graphical representation of the decomposition is illustrated in figure  [ fig_neighborhooddecomposition ] . in the figure",
    "we can see that the original loopy graph is decomposed into five loop - free subgraphs .",
    "8.0 cm    in general , in a graphical representation , the straightforward decompositions given in  ( [ sub_binary_cost_function ] ) can be understood as decomposing a loopy graph into @xmath10 loop - free subgraphs , one subgraph is associated with each variable node in the original graph . in the decomposition , each subgraph is of a star - like structure with its associated variable node as the star center .",
    "it consists of the variable node , the neighbors of the node , and the edges connecting the node with its neighbors .",
    "the graphical representation of an objective function may contain many loops .",
    "that is the major cause of the difficulty at minimizing the objective function .",
    "if the graph is loop - free , there exist algorithms with linear complexity ( e.g. , dynamic programming ) that can minimize the objective function efficiently . therefore",
    ", if we can decompose an objective function with a loopy graphical representation into a number of sub - objective functions with loop - free graphical representations , a hard optimization problem is , thus , broken into a number of sub - problems of lower complexities .",
    "cooperative optimization can then be applied to solve those sub - problems in a cooperative way .",
    "this kind of decompositions is called the graph - based decompositions .",
    "it is important to note that the modification given in ( [ modified_objective_function ] ) for a sub - objective function does not change its graphical structure .",
    "in other words , every modified sub - objective function defined in ( [ modified_objective_function ] ) has the exact same graphical structure as its original one .",
    "this is because only unary functions , the assignment constraints @xmath210 , are introduced in the definition .",
    "therefore , any optimization algorithm applicable to the original sub - objective functions should also be applicable to the modified ones .",
    "in other words , if a sub - objective function is of a tree - like structure , then its modified version defined by ( [ modified_objective_function ] ) must have the exact same tree - like structure .",
    "both of them can be minimized efficiently via dynamic programming .      in terms of the graph",
    "based decompositions , the straightforward decompositions are based on the direct neighbors of each variable node . another possible way of decomposing an objective function is based on the spanning trees of the graph representing the function . a tree is called a spanning tree of an undirected graph @xmath202 if it is a subgraph of @xmath202 and containing all vertices of @xmath202 .",
    "every finite connected graph @xmath202 contains at least one spanning tree @xmath211 .",
    "given an objective function @xmath11 of @xmath10 variables in form of ( [ binary_cost_function ] ) .",
    "let @xmath212 be its graphical representation with @xmath10 variable nodes .",
    "without loss of generality , we assume that @xmath202 is connected ( otherwise it implies that the original minimization problem can be broken into several independent sub - problems ) . for each variable node @xmath12 of @xmath202 , we can associate a spanning tree of @xmath202 , denoted as @xmath213 ( @xmath214 shares the same set of nodes as @xmath202 ) .",
    "there are @xmath10 such spanning trees in total , @xmath215 ( some trees may possibly be duplicated ) .",
    "we also choose those @xmath10 spanning trees in a way such that each edge of @xmath202 is covered at least by one of the @xmath10 trees .",
    "figure  [ fig_spanningtreedecomposition ] shows an example of decomposing the graph representing the objective function of the simple example into five spanning trees .",
    "after decomposing the graph @xmath202 into @xmath10 spanning trees , if we can define a sub - objective function @xmath15 for each spanning tree such that    1 .",
    "@xmath216 ; 2 .",
    "the graphical representation of @xmath15 is @xmath214 , for @xmath18 .",
    "then the set @xmath16 is a legitimate decomposition of the original objective function @xmath11 .",
    "this kind of decompositions is so called the spanning - tree based decompositions .    given an objective function @xmath11 of a binary constraint optimization problem with a graphical representation @xmath202 and @xmath10 spanning trees , each unary constraint @xmath217 of @xmath11 is associated with a variable node @xmath35 in @xmath202 covered by all the @xmath10 spanning trees .",
    "each binary constraint @xmath218 of @xmath11 is associated with an edge @xmath219 in @xmath202 covered at least by one of the @xmath10 spanning trees .",
    "assume that the edge @xmath220 is covered by @xmath221 spanning trees , where @xmath221 should be a positive integer .",
    "one way for defining sub - objective functions @xmath15 to satisfy the above two conditions is given as follows , @xmath222 for @xmath90 .",
    "we can apply algebraic graph theory to reveal some of properties of graphs and their spanning trees . let @xmath197 be a finite simple graph of @xmath10 nodes , with vertices @xmath223 . if @xmath202 is a graphical representation of an objective function , then the vertices @xmath224 is the variable node @xmath12 , i.e. , @xmath225 .",
    "the connectivity of a graph @xmath202 can be represented by the adjacency matrix @xmath226 of @xmath202 .",
    "the adjacency matrix @xmath226 of @xmath202 is defined as @xmath227 the adjacency matrix of an undirected graph is symmetric .",
    "let @xmath228 be the adjacency matrix of @xmath202 where the diagonal entries @xmath229 are replaced by the degrees of vertices @xmath230 .",
    "let @xmath231 be the matrix obtained by removing the first row and column of @xmath228 .",
    "then the number of spanning trees in g is equal to @xmath232 ( kirchoff s matrix - tree theorem ) .",
    "particularly , if @xmath202 is a complete graph , @xmath233 has the determinant @xmath234 .",
    "that is , every complete graph with @xmath10 vertices ( @xmath235 ) has exactly @xmath234 spanning trees ( theorem of cayley ) .      using factor graph  @xcite , any objective function containing @xmath26-ary constraints , where @xmath26 can be any integer number ,",
    "can be represented as a graph . with the representation",
    ", the two aforementioned kinds of decompositions can be easily generalized further to decompose the objective function .",
    "special decompositions can also be explored for graphs with special structures or constraints of some special properties to maximize the power of cooperative optimization algorithms .",
    "another way to apply the two kinds of decompositions for the @xmath26-ary constraints case is by converting the constraints of orders higher than two into binary constraints via variable clustering technique .",
    "as mentioned in the previous subsection , the objective function of a binary constraint optimization problem has a graphical representation @xmath212 . for the straightforward decompositions described in the previous subsection",
    ", we can design a propagation matrix based on the adjacency of the graph @xmath202 as follows @xmath236 where @xmath237 is the degree of the variable node @xmath35 .",
    "the propagation matrix  ( [ p2 ] ) of the simple example is designed in this way .",
    "another way to design a propagation matrix is given as follows , @xmath238 such a matrix has all of the diagonal elements of non - zero values .",
    "the design of cooperative optimization algorithms is not trivial even with the aforementioned guidelines . in the basic canonical form  ( [ cooperative_optimization_canonical ] ) , there are @xmath56 values for the propagation matrix @xmath189 and a series of values for the cooperation strength @xmath32 . to ease the design job for engineers and practitioners , the difference equations  ( [ cooperative_optimization_canonical ] ) of the basic canonical form can be simplified to @xmath239 where @xmath240 is the only parameter to be tuned in experiments to maximize performance .",
    "it plays the same role as the cooperation strength @xmath32 for controlling the cooperation level among the agents in a system .",
    "the above set of simplified difference equations defines the simple form of cooperative optimization .",
    "the simple form is derived from the basic canonical form  ( [ cooperative_optimization_canonical ] ) by setting @xmath33 be a positive constant @xmath241 , for any @xmath0 and @xmath34 , if @xmath35 is contained in @xmath15 ; and @xmath39 , otherwise .",
    "we also let the cooperation strength @xmath32 be of a constant value @xmath86 .",
    "let @xmath242 , we have ( [ cooperative_optimization_canonical ] ) simplified to ( [ simple_cooperative_optimization0 ] ) .",
    "if the parameter @xmath240 is of a large value , the difference equations  ( [ simple_cooperative_optimization0 ] ) of a simple cooperative optimization algorithm may have value overflow problems in computing the assignment constraints @xmath243 . to improve its convergence property , we can offset each @xmath243 by a value at each iteration .",
    "one choice is the minimal value of @xmath29 .",
    "that is we offset @xmath29 by its minimal value as follows , @xmath244 thus , the offsetting defines an operator on @xmath29 , denoted as @xmath245 . with the notation , the difference equations of a simple cooperative optimization algorithm become @xmath246",
    "just like many other problems in computer vision and image processing , stereo matching can be formulated as a binary constraint optimization problem with an objective function @xmath11 in form of ( [ binary_cost_function ] ) .",
    "for detail about the energy function definitions used for stereo matching , please see  @xcite .",
    "basically , an unary constraint @xmath191 in  ( [ binary_cost_function ] ) measures the difference of the intensities between site @xmath0 from one image and its corresponding site in another image given the depth of the site .",
    "a binary constraint @xmath247 measures the difference of the depths between site @xmath0 and site @xmath34 .",
    "this type of constraints is also referred to as the smoothness constraint in literature .",
    "it has also been widely used in solving image segmentation and other vision tasks .    in our experiments , we apply the simplified form of cooperative optimization  ( [ simple_cooperative_optimization ] ) for stereo matching with the parameter @xmath240 is set to @xmath248 .",
    "the maximum number of iterations is set to @xmath249 .",
    "the objective function associated with stereo - matching is decomposed based on the spanning - tree based decomposition .",
    "the detail of the decomposition and the minimization of the sub - objective functions are offered in the following subsection .",
    "often times , the graphical representation of the objective function of an image segmentation problem or a stereo matching problem is of a 2-d grid - like structure . because a 2-d grid - like graph is highly regular in structure , its spanning trees can be easily defined in a systematic way .",
    "given an objective function of a 2-d grid - like graphical representation @xmath197 , let @xmath250 be the height of the grid ( the number of rows ) and @xmath251 be the width of the grid ( the number of columns ) .",
    "let @xmath252 be the set of all horizontal edges of @xmath202 and @xmath253 be the set of all vertical edges .",
    "there are in total @xmath254 nodes , one for each variable .",
    "there are in total @xmath255 horizontal edges and @xmath256 vertical edges . with the notations ,",
    "the objective function can be expressed as @xmath257 or equivalently , @xmath258    the horizontal path @xmath259 through a variable node @xmath12 consists of all the nodes at the same horizontal line as @xmath260 , together with the edges connecting those nodes .",
    "the vertical path @xmath261 through a variable node @xmath12 consists of all the nodes at the same vertical line as @xmath260 , together with the edges connecting those nodes .    for each variable node @xmath12 ,",
    "let us define two spanning trees with the node as the root , called the horizontal spanning tree @xmath262 and the vertical spanning tree @xmath263 , respectively .",
    "the horizontal spanning tree @xmath262 consists of the horizontal path through the variable node @xmath12 and all the vertical paths through each node in the horizontal path .",
    "the vertical spanning tree @xmath263 consists of the vertical path through the variable node @xmath12 and all the horizontal paths through each node in the vertical path ( the illustrations are shown in fig .  [ fig_gridgraphhspan ] ) .     as their roots.,title=\"fig:\",width=147 ]   as their roots.,title=\"fig:\",width=147 ]    let the functions @xmath264 be the objective functions associated with the horizontal spanning tree @xmath262 and the vertical spanning tree @xmath263 , respectively . following the general design guideline described in the previous section for the spanning - tree based decompositions ( see eq .",
    "( [ tree_decomposition ] ) ) , we can define @xmath265 and @xmath266 as @xmath267 @xmath268 where @xmath269 , @xmath270 , and @xmath271 .",
    "the sub - objective function @xmath15 associated with variable @xmath12 is defined as @xmath272 clearly , we have @xmath273 .    as mentioned before , any objective function of a tree - like structure can be minimized efficiently using the dynamic programming technique .",
    "it is of a linear computational complexity and is simply based on local message passing from the leave nodes all the way back to the root node . the books  @xcite offer a detail explanation about message passing and the dynamic programming technique . when applying the technique for minimizing the objective function of a horizontal or vertical spanning tree , the message flows among the variable nodes",
    "are illustrated in figure  [ fig_gridgraphmessageflow ] .     at their roots.,width=359 ]      the middlebury college evaluation framework  @xcite for stereo matching is used in the experiments .",
    "the script used for evaluation is based on _",
    "exp6_gc.txt _ offered in the framework .",
    "the other settings come from the default values in the framework .",
    "the results of stereo matching algorithms together with the ground truths for the four test stereo image pairs from the evaluation framework are shown in figure  [ fig_cc ] .",
    "the quality of solutions of both algorithms are very close to each other from a visual inspection .",
    "the following four tables show the performance of the cooperative optimization algorithm ( upper rows in a table ) and the graph cut algorithm ( lower rows in a table ) over the four test image sets .",
    "the solution quality is measured in the overall area , no occluded areas , occluded areas , textured areas , texture - less areas , and near discontinuity areas ( see @xcite for the detail description of the evaluation framework ) .",
    "both algorithms does not handle occluded areas ( an occluded area is one that is visible in one image , but not the other ) .",
    "also , the runtimes of the two algorithms ( co = cooperative optimization algorithm , gc = graph cuts ) are listed . from the tables",
    "we can see that the cooperative optimization is very close to graph cuts in terms of solution quality and energy states .",
    "however , the cooperative optimization algorithm is around @xmath274 times faster than graph cuts in the software simulation .    ccccccc + & + & all & non occl & occl & textrd & textrls & d_discnt   + error & 4.04 & 0.85 & 16.18 & 0.85 & 0.36 & 2.84 + bad pixels & 5.35% & 0.18% & 86.78% & 0.18% & 0.00% & 2.46% + error & 3.91 & 1.07 & 15.45 & 1.07 & 0.38 & 3.65 + bad pixels & 5.63% & 0.36% & 88.76% & 0.36% & 0.00% & 4.52% +    0.3 in    ccccccc + & + & all & non occl & occl & textrd & textrls & d_discnt   + error & 1.46 & 0.61 & 7.92 & 0.63 & 0.33 & 1.56 + bad pixels & 3.93% & 1.35% & 93.06% & 1.48% & 0.14% & 5.96% + error & 1.49 & 0.70 & 7.88 & 0.73 & 0.40 & 1.60 + bad pixels & 3.99% & 1.38% & 94.02% & 1.49% & 0.31% & 6.39% +    0.3 in    ccccccc + & + & all & non occl & occl & textrd & textrls & d_discnt   + error & 1.30 & 0.99 & 5.41 & 1.00 & 0.97 & 2.01 + bad pixels & 4.77% & 2.59% & 87.38% & 2.57% & 2.61% & 10.63% + error & 1.25 & 0.92 & 5.35 & 1.04 & 0.73 & 2.02 + bad pixels & 4.24% & 2.04% & 87.60% & 2.77% & 1.05% & 10.00% +    0.3 in    ccccccc + & + & all & non occl & occl & textrd & textrls & d_discnt   + error & 1.58 & 1.11 & 8.29 & 0.93 & 1.42 & 1.49 + bad pixels & 3.29% & 1.65% & 90.72% & 1.38% & 2.20% & 7.28% + error & 1.47 & 0.95 & 8.33 & 0.81 & 1.18 & 1.31 + bad pixels & 3.58% & 1.93% & 91.55% & 1.56% & 2.68% & 6.84% +",
    "cooperative optimization offers us a general , distributed optimization method for attacking hard optimization problems . soft decision making ,",
    "message passing , and solution compromising are three important techniques for achieving cooperation among agents in a cooperative optimization system .",
    "the global optimality property of consensus solutions offers an appealing reason for agents in a system to compromise their solutions so that conflicts in their solutions can be resolved .",
    "the insights we gained at studying cooperative optimization might help us to apply the cooperation principle to understand or solve more generic decision optimization problems arose from fields like neurosciences , business management , political management , and social sciences .",
    "for any numbers @xmath286 in @xmath287 , the following inequality can be proved by the principle of mathematical induction , @xmath288 if @xmath289 converges , there exists @xmath251 such that for all @xmath290 , @xmath291 therefore , defining @xmath292 as @xmath293 we have that for all @xmath290 , @xmath294 therefore , the sequence @xmath295 is a non increasing sequence bounded from below by @xmath296 .",
    "it must have a positive limit @xmath297 so that @xmath298                  s.  geman and d.  geman , `` stochastic relaxation , gibbs distributions , and the bayesian restoration of images , '' _ ieee transactions on pattern analysis and machine intelligence _ , vol .",
    "pami-6 , no .  6 , pp .",
    "721741 , nov .",
    "1984 .",
    "y.  boykov and v.  kolmogorov , `` an experiemental comparison of min - cut / max - flow algorithms for energy minimization , '' _ ieee transactions on pattern analysis and machine intelligence _ , vol .",
    "26 , no .  9 ,",
    "december 2004 .",
    "r.  szeliski and r.  zabih , `` an experimental comparison of stereo algorithms , '' in _ vision algorithms : theory and practice _ , ser .",
    "lncs , b.  triggs , a.  zisserman , and r.  szeliski , eds .",
    "1883.1em plus 0.5em minus 0.4emcorfu , greece : springer - verlag , sept .",
    "1999 , pp . 119 .",
    "m.  f. tappen and w.  t. freeman , `` comparison of graph cuts with belief propagation for stereo , using identical mrf parameters , '' in _ 9th ieee international conference on computer vision ( iccv 2003 ) , 14 - 17 october 2003 , nice , france_.1em plus 0.5em minus 0.4emieee computer society , 2003 , pp .",
    "900907 .",
    "m.  j. wainwright , t.  s. jaakkola , and a.  s. willsky , `` map estimation via agreement on ( hyper)trees : message - passing and linear - programming approaches , '' _ ieee transactions on information theory _ ,",
    "51 , no .  11 , pp .",
    "36973717 , march 2005 .",
    "x.  huang , `` a cooperative search approach to global optimization , '' _ proceeding of the first international conference on optimiztion methods and software _ , vol .",
    "december , p. 140",
    ", hangzhou ,  p.r .",
    "china ,  2002 .",
    " , `` a general framework for constructing cooperative global optimization algorithms , '' in _ frontiers in global optimization _ , ser .",
    "nonconvex optimization and its applications.1em plus 0.5em minus 0.4em kluwer academic publishers , 2004 , pp .",
    "179221 .     , `` cooperative optimization for solving large scale combinatorial problems , '' in _ theory and algorithms for cooperative systems _ , ser",
    "series on computers and operations research.1em plus 0.5em minus 0.4emworld scientific , 2004 , pp .",
    "117156 .",
    " , `` cooperative optimization for energy minimization in computer vision : a case study of stereo matching , '' in _ pattern recognition , 26th dagm symposium_.1em plus 0.5em minus 0.4emspringer - verlag , lncs 3175 , 2004 , pp .",
    "302309 .                d.  j.  c. mackay , _ information theory , inference , and learning algorithms_.1em plus 0.5em minus 0.4emcambridge university press , 2003 , available from .",
    "[ online ] .",
    "available : http://www.cambridge.org/0521642981    j.  yedidia , w.  freeman , and y.  weiss , `` constructing free - energy approximations and generalized belief propagation algorithms , '' _ ieee transactions on information theory _",
    "51 , no .  7 , pp .",
    "22822312 , july 2005 .",
    "t.  j. richardson , m.  a. shokrollahi , and r.  l. urbanke , `` design of capacity - approaching irregular low - density parity - check codes , '' _ ieee transactions on information theory _ , vol .",
    "47 , no .  2 ,",
    "pp . 619637 , february 2001 .",
    "a.  blake , `` comparison of the efficiency of deterministic and stochestic algorithms for visual reconstruction , '' _ ieee transactions on pattern analysis and machine intelligence _ , vol .",
    "pami-11 , no .  1 ,",
    "pp . 212 , january 1989 ."
  ],
  "abstract_text": [
    "<S> often times , individuals working together as a team can solve hard problems beyond the capability of any individual in the team . </S>",
    "<S> cooperative optimization is a newly proposed general method for attacking hard optimization problems inspired by cooperation principles in team playing . </S>",
    "<S> it has an established theoretical foundation and has demonstrated outstanding performances in solving real - world optimization problems . with some general settings , </S>",
    "<S> a cooperative optimization algorithm has a unique equilibrium and converges to it with an exponential rate regardless initial conditions and insensitive to perturbations . </S>",
    "<S> it also possesses a number of global optimality conditions for identifying global optima so that it can terminate its search process efficiently . </S>",
    "<S> this paper offers a general description of cooperative optimization , addresses a number of design issues , and presents a case study to demonstrate its power .    </S>",
    "<S> [ section ] [ section ] [ section ] [ section ] </S>"
  ]
}