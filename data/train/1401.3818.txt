{
  "article_text": [
    "of the most important procedures in hsi is image classification , where the pixels are labeled to one of the classes based on their spectral characteristics . due to the numerous demands in mineralogy",
    ", agriculture and surveillance , the hsi classification task is developing very rapidly and a large number of techniques have been proposed to tackle this problem @xcite . comparing with previous approaches ,",
    "svm is found highly effective on both computational efficiency and classification results .",
    "a wide variety of svm s modifications have been proposed to improve its performance .",
    "some of them incorporate the contextual information in the classifiers @xcite .",
    "others design sparse svm in order to pursue a sparse decision rule by using @xmath0-norm as the regularizer @xcite .",
    "recently , src has been proposed to solve many computer vision tasks @xcite , where the use of sparsity as a prior often leads to state - of - the - art performance .",
    "src has also been applied to hsi classification @xcite , relying on the observation that hyperspectral pixels belonging to the same class approximately lie in the same low - dimensional subspace . in order to alleviate the problem introduced by the lack of sufficient training data , haq _",
    "et al_. @xcite proposed the homotopy - based src . another way to solve",
    "the problem of insufficient training data is to employ the contextual information of neighboring pixels in the classifier , such as spectral - spatial constraint classification @xcite .    in src ,",
    "a test sample @xmath1 , where @xmath2 is the number of spectral bands , can be written as a sparse linear combination of all the training pixels ( atoms in a dictionary ) as @xmath3 where @xmath4 , @xmath5 is @xmath0-norm . @xmath6 $ ] is a structured dictionary formed from concatenation of several class - wise sub - dictionaries , @xmath7 are the columns of @xmath8 and @xmath9 is the total number of training samples from all the @xmath10 classes , and @xmath11 is a scalar regularization parameter .",
    "the class label for the test pixel @xmath12 is determined by the minimum residual between @xmath12 and its approximation from each class - wise sub - dictionary : @xmath13 where @xmath14 is the group or class index , and @xmath15 is the indicator operation zeroing out all elements of @xmath16 that do not belong to the class @xmath17 .    in the case of hsi",
    ", src always suffers from the non - uniqueness or instability of the sparse coefficients due to the high mutual coherency of the dictionary @xcite .",
    "fortunately , a better reconstructed signal and a more robust representation can be obtained by either exploring the dependencies of neighboring pixels or exploiting the inherent dictionary structure .",
    "recently , structured priors have been incorporated into hsi classification @xcite , which can be sorted into three categories .",
    "( _ a _ ) priors that only exploit the correlations and dependencies among the neighboring spectral pixels or their sparse coefficient vectors , which includes joint sparsity @xcite , graph regularized lasso ( referred as the laplacian regularized lasso ) @xcite and the low - rank lasso @xcite .",
    "( _ b _ ) priors that only exploit the inherent structure of the dictionary , such as group lasso @xcite .",
    "( _ c _ ) priors that enforce structural information on both sparse coefficients and dictionary , such as collaborative group lasso @xcite and collaborative hierarchical lasso ( chilasso ) @xcite .",
    "besides src , structured sparsity prior can also be incorporated into other classifiers such as the logistic regression classifiers @xcite .",
    "the main contributions of this paper are _ ( a ) _ to assess the src performance using various structured sparsity priors for hsi classification , and _",
    "( b ) _ to propose a conceptually similar prior to chilasso , which is called the low - rank group prior .",
    "this prior is based on the assumption that pure or mixed pixels from the same classes are highly correlated and can be represented by a combination of sparse low - rank groups ( classes ) .",
    "the proposed prior takes advantage of both the group sparsity prior , which enforces sparsity across the groups , and the low rank prior , which encourages sparsity within the groups , by only using one regularizer . in the following sections",
    ", we investigate the roles of different structured priors imposed on the src optimization algorithm . starting with the classical sparsity @xmath0-norm prior",
    ", we then introduce several different priors with experimental results .",
    "the structured priors discussed are joint sparsity , laplacian sparsity , group sparsity , sparse group sparsity , low - rank and low - rank group prior .",
    "in hsi , pixels within a small neighborhood usually consist of similar materials .",
    "thus , their spectral characteristics are highly correlated . the spatial correlation between neighboring pixels",
    "can be indirectly incorporated through a joint sparsity model ( jsm ) @xcite by assuming that the underlying sparse vectors associated with these pixels share a common sparsity support .",
    "consider pixels in a small neighborhood of @xmath18 pixels .",
    "let @xmath19 represent a matrix whose columns correspond to pixels in a spatial neighborhood in a hyperspectral image .",
    "columns of @xmath20 $ ] can be represented as a linear combination of dictionary atoms @xmath21 , where @xmath22\\in \\mathbf{r}^ {   n \\times t}$ ] represents a sparse matrix . in jsm ,",
    "the sparse vectors of @xmath18 neighboring pixels , which are represented by the @xmath18 columns of @xmath23 , share the same support .",
    "therefore , @xmath23 is a sparse matrix with only few nonzero rows .",
    "the row - sparse matrix x can be recovered by solving the following lasso problem @xmath24 where @xmath25 is an @xmath26-norm and @xmath27 represents the @xmath28th row of @xmath23 .",
    "the label for the center pixel @xmath29 is then determined by the minimum total residual error @xmath30 where @xmath31 is the indicator operation zeroing out all the elements of @xmath23 that do not belong to the class @xmath17 .      in sparse representation , due to the high coherency of the dictionary atoms ,",
    "the recovered sparse coefficient vectors for multiple neighboring pixels could be partially different even when the neighboring pixels are highly correlated , and this may led to misclassification . as mentioned in the previous section ,",
    "joint sparsity is able to solve such a problem by enforcing multiple pixels to select exactly the same atoms .",
    "however , in many cases , when the neighboring pixels fall on the boundary between several homogeneous regions , the neighboring pixels will belong to several distinct classes ( groups ) and should use different sets of sub - dictionary atoms .",
    "laplacian sparsity enhances the differences between sparse coefficient vectors of the neighboring pixels that belong to different clusters .",
    "we introduce the weighting matrix @xmath32 , where @xmath33 characterizes the similarity between a pair of pixels @xmath34 and @xmath35 within a neighborhood .",
    "optimization with an additional laplacian sparsity prior can be expressed as @xmath36 where @xmath37 and @xmath38 are the regularization parameters .",
    "the matrix @xmath32 is used to characterize the similarity among neighboring pixels in the spectra space .",
    "similar pixels will possess larger weights , and therefore , enforcing the differences between the corresponding sparse coefficient vectors to become smaller , and similarly allowing the difference between sparse coefficient vectors of dissimilar pixels to become larger .",
    "therefore , the laplacian sparsity prior is more flexible than the joint sparsity prior in that it does not always force all the neighboring pixels to have the same common support . in this paper ,",
    "the weighting matrix is computed using the sparse subspace clustering method in @xcite .",
    "note that this grouping constraint is enforced on the testing pixels instead of the dictionary atoms , which is different from group sparsity .",
    "let @xmath39 be the normalized symmetric laplacian matrix @xcite , where @xmath40 is the degree matrix computed from @xmath32 .",
    "we can rewrite the equation ( [ eq : lap_1 ] ) as @xmath41 the above equation can be solved by a modified feature - sign search algorithm @xcite .",
    "the src dictionary has an inherent group - structured property since it is composed of several class sub - dictionaries , i.e. , the atoms belonging to the same class are grouped together to form a sub - dictionary . in sparse representation",
    ", we classify pixels by measuring how well the pixels are represented by each sub - dictionary .",
    "therefore , it would be reasonable to enforce the pixels to be represented by groups of atoms instead of individual ones . this could be accomplished by encouraging coefficients of only certain groups to be active and the remaining groups inactive .",
    "group lasso @xcite , for example , uses a sparsity prior that sums up the euclidean norm of every group coefficients .",
    "it will dominate the classification performance especially when the input pixels are inherently mixed pixels .",
    "group lasso optimization can be represented as @xmath42 where @xmath43 , @xmath44 represents the group sparse prior defined in terms of @xmath10 groups , @xmath45 is the weight and is usually set to the square root of the cardinality of the corresponding group to compensate for the different group sizes . here",
    ", @xmath46 refers to the coefficients of each group .",
    "the above group sparsity can be easily extended to the case of multiple neighboring pixels by extending problem ( [ eq : gs ] ) to collaborative group lasso , which is formulated as @xmath47 where @xmath48 represents a collaborative group lasso regularizer defined in terms of group and @xmath49 refers to each of the group coefficient matrix .",
    "when the group size is reduced to one , the group lasso degenerates into a joint sparsity lasso .      in the formulations ( [ eq : gs ] ) and",
    "( [ eq : cgs ] ) , the coefficients within each group are not sparse , and all the atoms in the selected groups could be active . if the sub - dictionary is overcomplete , then it is necessary to enforce sparsity within each group .",
    "to achieve sparsity within the groups , an @xmath0-norm regularizer can be added to the group lasso ( [ eq : gs ] ) , which can be written as @xmath50    similarly , eq .",
    "( [ eq : sgs ] ) can be easily extended to the multiple feature case , which can be written as    @xmath51    optimization problem ( [ eq : sgs ] ) is referred in the literature as the sparse group lasso and problem ( [ eq : chilasso ] ) as the collaborative hierarchical lasso ( chilasso ) @xcite .      based on the fact that spectra of neighboring pixels are highly correlated , it is reasonable to enforce the low rank sparsity prior on their coefficient matrix .",
    "the low rank prior is more flexible when compared with the joint sparsity prior which strictly enforces the row sparsity .",
    "therefore , when neighboring pixels are composed of small non - homogeneous regions , the low rank sparsity prior outperforms the joint sparsity prior .",
    "low rank sparse recovery problem has been well studied in @xcite and is stated as the following lasso problem @xmath52 where @xmath53 is the nuclear norm @xcite .    [",
    "fig : sp ]                             +                            to incorporate the structure of the dictionary , we now extend the low rank prior to group low rank prior , where the regularizer is obtained by summing up the rank of every group coefficient matrix , @xmath54 the low rank group prior is able to obtain the within - group sparsity by minimizing the nuclear norm of each group .",
    "furthermore , the summation of nuclear norms empowers the proposed prior to obtain a group sparsity pattern .",
    "hence , the low rank group prior is able to achieve sparsity both within and across groups by using only one regularization term .    [ cols=\"^,^,^\",options=\"header \" , ]     [ table : time ]      sparsity patterns of the toy example are shown in fig .",
    "[ fig : sp ] .",
    "the expected sparsity regions are shown in fig .",
    "[ fig : sp](a ) , where the y - axis labels the dictionary atom index and x - axis labels the test pixel index .",
    "the red and green regions correspond to the ideal locations of the active atoms for the class 2 and 14 , respectively .",
    "nonzero coefficients that belong to other classes are shown in blue dots .",
    "the joint sparsity , fig .",
    "[ fig : sp ] ( c ) , shows clear row sparsity pattern , but many rows are mistakenly activated .",
    "as expected , active atoms in fig .",
    "[ fig : sp ] ( d ) , ( e ) and ( g ) demonstrate group sparsity patterns . comparing the gs ( d ) and sgs ( e ) , it is observed that most of the atoms are deactivated within groups using sgs .",
    "the low rank group prior ( g ) demonstrates a similar sparsity pattern as that of sgs . for the laplacian sparsity ( h ) ,",
    "similarity of sparse coefficients that belong to the same classes is clearly visible .",
    "table and fig .",
    "[ fig : indianpine ] show the performance of srcs with different priors on the indian pine image .",
    "a spatial window of @xmath55 ( @xmath56 ) is used since this image consists of mostly large homogeneous regions . among srcs with different priors",
    ", the worst result occurs when we use simple @xmath0-admm .",
    "joint sparsity prior gives better result than the low rank prior .",
    "this is due to the large areas of homogeneous regions in this image , which favors the joint sparsity model .",
    "the highest oa is given by the laplacian sparsity prior via ffs , such a high performance is partly contributed to the accurate sparse recovery of the feature sign search method .",
    "both sgs and lrg outperform gs .",
    "we can see that among admm - based based methods , the low rank group prior yields the smoothest result .",
    "the computational time of various structured priors for indian pine image are shown in table [ table : time ] .",
    "among admm / sparsa - based methods , lrg , gs and sgs take roughly similar time ( @xmath572500s ) to process the image , while lr and js require longer time ( @xmath574000s ) .",
    "ls via ffs significantly impedes the computational efficiency .",
    "results for the university of pavia image are shown in table .",
    "the window size for this image is @xmath58 ( @xmath59 ) since many narrow regions are present in this image .",
    "the group sparsity prior gives the highest oa among the priors optimized by admm .",
    "the low rank sparsity prior gives a much better result than joint sparsity since this image contains many small homogeneous regions .",
    "the laplacian sparsity prior via ffs gives the highest oa performance .",
    "however , the difference between performance of various structured priors is quite small .",
    "this paper reviews five different structured sparse priors and proposes a low rank group sparsity prior . using these structured priors , classification results of srcs on hsi",
    "are generally improved when compared with the classical @xmath0 sparsity prior .",
    "the results have confirmed that the low rank prior is a more flexible constraint compared with the joint sparsity prior , while the latter works better on large homogeneous regions . imposing the group structured prior on the dictionary always gives higher overall accuracy compared with the @xmath0 prior .",
    "we have also observed that the performance is not only determined by the structured priors , but also depend on the corresponding optimization techniques .",
    "a. plaza , j. benediktsson , j. boardman , j. brazile , l. bruzzone , g. camps - valls , j. chanussot , m. fauvel , p. gamba , a. gualtieri , m. marconcini , j. tiltoni and g. trianni , `` recent advances in techniques for hyperspectral image processing , '' _ remote sens .",
    "_ , vol . 113 , no .",
    "s110-s122 , sept .",
    "g. camps - valls , l. gomez - chova , j. muoz - mar , j. vila - francs and j. calpe - maravilla , `` composite kernels for hyperspectral image classification , '' _ ieee geosci .",
    "remote sens .",
    "93 - 97 , jan . 2006 .",
    "l. gmez - chova , g. camps - valls , j. muoz - mar and j. calpe - maravilla , `` semi - supervised image classication with laplacian support vector machines , '' _ ieee geosci .",
    "remote sens .",
    "_ , vol . 5 , no .",
    "336 - 340 , jul . 2008 .",
    "q. haq , l. tao , f. sun and s. yang , `` a fast and robust sparse approach for hyperspectral data classification using a few labeled samples , '' _ ieee trans .",
    "remote sens .",
    "3973 - 3985 , june 2012 .",
    "g. liu , z. lin , s. yan , j. sun , y. yu and y. ma , `` robust recovery of subspace structures by low - rank representation , '' _ ieee trans .",
    "pattern anal .",
    "1 , pp . 171 - 184 , jan .",
    "2013 .",
    "y. qian , m. ye and j. zhou , `` hyperspectral image classification based on structured sparse logistic regression and three - dimensional wavelet texture features , '' _ ieee trans .",
    "remote sens .",
    "2276 - 2291 , apr . 2012 .",
    "s. boyd , n. parikh , e. chu , b. peleato and j. eckstein , `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ ftml .",
    "1 , pp . 1 - 122 , jan . 2010 ."
  ],
  "abstract_text": [
    "<S> pixel - wise classification , where each pixel is assigned to a predefined class , is one of the most important procedures in hyperspectral image ( hsi ) analysis . by representing a test pixel as a linear combination of a small subset of labeled pixels , a sparse representation classifier ( src ) gives rather plausible results compared with that of traditional classifiers such as the support vector machine ( svm ) . </S>",
    "<S> recently , by incorporating additional structured sparsity priors , the second generation srcs have appeared in the literature and are reported to further improve the performance of hsi . </S>",
    "<S> these priors are based on exploiting the spatial dependencies between the neighboring pixels , the inherent structure of the dictionary , or both . in this paper , we review and compare several structured priors for sparse - representation - based hsi classification . </S>",
    "<S> we also propose a new structured prior called the low rank group prior , which can be considered as a modification of the low rank prior . </S>",
    "<S> furthermore , we will investigate how different structured priors improve the result for the hsi classification .    </S>",
    "<S> hyperspectral image , sparse representation , structured priors , classification </S>"
  ]
}