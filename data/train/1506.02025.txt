{
  "article_text": [
    "over recent years , the landscape of computer vision has been drastically altered and pushed forward through the adoption of a fast , scalable , end - to - end learning framework , the convolutional neural network ( cnn )  @xcite .",
    "though not a recent invention , we now see a cornucopia of cnn - based models achieving state - of - the - art results in classification  @xcite , localisation  @xcite , semantic segmentation  @xcite , and action recognition  @xcite tasks , amongst others .",
    "a desirable property of a system which is able to reason about images is to disentangle object pose and part deformation from texture and shape .",
    "the introduction of local max - pooling layers in cnns has helped to satisfy this property by allowing a network to be somewhat spatially invariant to the position of features . however , due to the typically small spatial support for max - pooling (  @xmath0 pixels ) this spatial invariance is only realised over a deep hierarchy of max - pooling and convolutions , and the intermediate feature maps ( convolutional layer activations ) in a cnn are not actually invariant to large transformations of the input data  @xcite .",
    "this limitation of cnns is due to having only a limited , pre - defined pooling mechanism for dealing with variations in the spatial arrangement of data .    in this work",
    "we introduce a _ spatial transformer _",
    "module , that can be included into a standard neural network architecture to provide spatial transformation capabilities .",
    "the action of the spatial transformer is conditioned on individual data samples , with the appropriate behaviour learnt during training for the task in question ( without extra supervision ) . unlike pooling layers , where the receptive fields are fixed and local , the spatial transformer module is a dynamic mechanism that can actively spatially",
    "transform an image ( or a feature map ) by producing an appropriate transformation for each input sample .",
    "the transformation is then performed on the entire feature map ( non - locally ) and can include scaling , cropping , rotations , as well as non - rigid deformations .",
    "this allows networks which include spatial transformers to not only select regions of an image that are most relevant ( attention ) , but also to transform those regions to a canonical , expected pose to simplify recognition in the following layers .",
    "notably , spatial transformers can be trained with standard back - propagation , allowing for end - to - end training of the models they are injected in .",
    "spatial transformers can be incorporated into cnns to benefit multifarious tasks , for example : ( i )  _ image classification : _ suppose a cnn is trained to perform multi - way classification of images according to whether they contain a particular digit  where the position and size of the digit may vary significantly with each sample ( and are uncorrelated with the class ) ; a spatial transformer that crops out and scale - normalizes the appropriate region can simplify the subsequent classification task , and lead to superior classification performance , see fig .",
    "[ fig : teaser ] ; ( ii )  _ co - localisation : _ given a set of images containing different instances of the same ( but unknown ) class , a spatial transformer can be used to localise them in each image ; ( iii )  _ spatial attention : _ a spatial transformer can be used for tasks requiring an attention mechanism , such as in @xcite , but is more flexible and can be trained purely with backpropagation without reinforcement learning .",
    "a key benefit of using attention is that transformed ( and so attended ) , lower resolution inputs can be used in favour of higher resolution raw inputs , resulting in increased computational efficiency .    the rest of the paper is organised as follows : sect .",
    "[ sec : related ] discusses some work related to our own , we introduce the formulation and implementation of the spatial transformer in sect .",
    "[ sec : transformers ] , and finally give the results of experiments in sect .",
    "[ sec : experiments ] .",
    "additional experiments and implementation details are given in appendix  [ sec : morexp ] .",
    "in this section we discuss the prior work related to the paper , covering the central ideas of modelling transformations with neural networks  @xcite , learning and analysing transformation - invariant representations  @xcite , as well as attention and detection mechanisms for feature selection  @xcite .    early work by hinton  @xcite looked at assigning canonical frames of reference to object parts , a theme which recurred in  @xcite where 2d affine transformations were modeled to create a generative model composed of transformed parts .",
    "the targets of the generative training scheme are the transformed input images , with the transformations between input images and targets given as an additional input to the network .",
    "the result is a generative model which can learn to generate transformed images of objects by composing parts .",
    "the notion of a composition of transformed parts is taken further by tieleman  @xcite , where learnt parts are explicitly affine - transformed , with the transform predicted by the network .",
    "such generative capsule models are able to learn discriminative features for classification from transformation supervision .",
    "the invariance and equivariance of cnn representations to input image transformations are studied in @xcite by estimating the linear relationships between representations of the original and transformed images .",
    "cohen & welling  @xcite analyse this behaviour in relation to symmetry groups , which is also exploited in the architecture proposed by gens & domingos  @xcite , resulting in feature maps that are more invariant to symmetry groups .",
    "other attempts to design transformation invariant representations are scattering networks  @xcite , and cnns that construct filter banks of transformed filters  @xcite .",
    "stollenga  @xcite use a policy based on a network s activations to gate the responses of the network s filters for a subsequent forward pass of the same image and so can allow attention to specific features . in this work",
    ", we aim to achieve invariant representations by manipulating the data rather than the feature extractors , something that was done for clustering in  @xcite .",
    "neural networks with selective attention manipulate the data by taking crops , and so are able to learn translation invariance .",
    "work such as  @xcite are trained with reinforcement learning to avoid the need for a differentiable attention mechanism , while  @xcite use a differentiable attention mechansim by utilising gaussian kernels in a generative model .",
    "the work by girshick  @xcite uses a region proposal algorithm as a form of attention , and  @xcite show that it is possible to regress salient regions with a cnn .",
    "the framework we present in this paper can be seen as a generalisation of differentiable attention to any spatial transformation .",
    "[ cols=\"^,^,^,^ \" , ]     in this section we describe our fine - grained image classification architecture in more detail . for this task , we utilise the spatial transformers as a differentiable attention mechanism , where each transformer is expected to automatically learn to focus on discriminative object parts .",
    "namely , each transformer predicts the location ( x , y ) of the attention window , while the scale is fixed to @xmath1 of the image size . the transformers sample @xmath2 crops from the input image , each of which",
    "is then described each by its own cnn stream , thus forming a multi - stream architecture ( shown in  fig .",
    "[ fig : birdarch ] ) .",
    "the outputs of the streams are @xmath3-d crop descriptors , which are concatenated and classified with a @xmath4-way softmax classifier .    as the main building block of our network , we utilise the state - of - the - art inception architecture with batch normalisation  @xcite , pre - trained on the imagenet challenge ( ilsvrc ) dataset",
    ". our model achieves @xmath5 top-1 error on the ilsvrc validation set using a single image crop ( we only trained on single - scale images , resized so that the smallest side is @xmath6 ) .",
    "the crop description networks employ the inception architecture with the last layer ( @xmath7-way ilsvrc classifier ) removed , so that the output is a @xmath3-d descriptor .",
    "the localisation network is shared across _ all _ the transformers , and was derived from inception in the following way . apart from the ilsvrc classification layer , we also removed the last pooling layer to preserve the spatial information .",
    "the output of this truncated inception net has @xmath8 spatial resolution and @xmath3 feature channels . on top of it",
    ", we added three weight layers to predict the transformations : ( i ) @xmath9 convolutional layer to reduce the number of feature channels from @xmath3 to @xmath10 ; ( ii ) fully - connected layer with @xmath10-d output ; ( iii ) fully - connected layer with @xmath11-d output , where @xmath12 is the number of transformers ( we experimented with @xmath13 and @xmath14 ) .",
    "we note that we did not strive to optimise the architecture in terms of the number of parameters and the computation time .",
    "our aim was to investigate whether spatial transformer networks are able to automatically discover meaningful object parts when trained just on image labels , which we confirmed both quantitatively and qualitatively ( sect .",
    "[ sec : fgc ] ) .",
    "the model was trained for 30k iterations with sgd ( batch size 256 ) with an initial learning rate of 0.1 , reduced by a factor of 10 after 10k , 20k , and 25k iterations . for stability , the localisation network s learning rate is the base learning rate multiplied by @xmath15 .",
    "weight decay was set at @xmath16 and dropout of 0.7 was used before the 200-way classification layer .",
    "we evaluated two input images sizes for the spatial transformers : @xmath17 and @xmath18 . in the latter case",
    ", we added a fixed @xmath19 downscaling layer before the localisation net , so that its input is still @xmath17 .",
    "the difference between the two settings lies in the size of the image from which sampling is performed ( @xmath20 vs @xmath21 ) , with @xmath21 better suited for sampling small - scale crops .",
    "the output of the transformers are @xmath22 crops in both cases ( so that they are compatible with crop description inception nets ) . when training , we utilised conventional augmentation in the form of random sampling ( @xmath17 from @xmath23 and @xmath18 from @xmath24 where @xmath25 is the largest image side ) and horizontal flipping .",
    "the localisation net was initialised to tile the image plane with the spatial transformer crops .",
    "we also experimented with more complex transformations ( location and scale , as well as affine ) , but observed similar results .",
    "this can be attributed to the very small size of the training set ( 6k images , 200 classes ) , and we noticed severe over - fitting in all training scenarios .",
    "the hyper - parameters were estimated by cross - validation on the training set ."
  ],
  "abstract_text": [
    "<S> convolutional neural networks define an exceptionally powerful class of models , but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner . in this work </S>",
    "<S> we introduce a new learnable module , the _ spatial transformer _ , which explicitly allows the spatial manipulation of data within the network . </S>",
    "<S> this differentiable module can be inserted into existing convolutional architectures , giving neural networks the ability to actively spatially transform feature maps , conditional on the feature map itself , without any extra training supervision or modification to the optimisation process . </S>",
    "<S> we show that the use of spatial transformers results in models which learn invariance to translation , scale , rotation and more generic warping , resulting in state - of - the - art performance on several benchmarks , and for a number of classes of transformations .    </S>",
    "<S> google deepmind , london , uk + ` { jaderberg,simonyan,zisserman,korayk}@google.com ` </S>"
  ]
}