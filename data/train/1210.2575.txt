{
  "article_text": [
    "dimension reduction plays an important role in high dimensional data analysis .",
    "one then wishes to reduce the dimension of a @xmath0-variate random vector @xmath1 using a transformation @xmath2 where the transformation matrix @xmath3 is a @xmath4 matrix with linearly independent columns , @xmath5 .",
    "the column vectors of @xmath3 then span the @xmath6-dimensional subspace of interest .",
    "the transformation to the subspace can also be done using the corresponding @xmath7 orthogonal projector @xmath8 .",
    "the transformation @xmath9 projects the observations to a linear @xmath6-variate subspace .",
    "there are two main types of dimension reduction  unsupervised and supervised .",
    "principal component analysis ( pca ) is perhaps the best known unsupervised dimension reduction method .",
    "pca finds an orthogonal transformation matrix in such a way that the components in the new coordinate system are uncorrelated and ordered according to their variances . in dimension reduction ,",
    "only the @xmath6 components with highest variances are taken .",
    "independent component analysis ( ica ) is another example of an unsupervised dimension reduction method .",
    "the fourth - order blind identification ( fobi ) @xcite procedure then finds a transformation matrix in such a way that the new components are uncorrelated with respect to two distinct scatter matrices , the regular covariance matrix and a scatter matrix based on fourth moments , and ordered according to their kurtosis values .",
    "then the @xmath6 components with kurtosis values most deviating from that of a normal distribution are most interesting .    in dimension reduction , the goal is often to use the transformed ( reduced ) variables to predict the value of a known response variable @xmath10 . in supervised dimension reduction , the joint distribution of @xmath11 and @xmath10",
    "is then taken into consideration in the dimension reduction of @xmath11 , and it is hoped that @xmath12 . sliced inverse regression ( sir )",
    "@xcite is a well known supervised dimension reduction method .",
    "it is , along with ica , based on the comparison of two scatter matrices , where the second scatter matrix depends - unlike in unsupervised dimension reduction methods - on the joint distribution of @xmath11 and @xmath10 .",
    "other well known dimension reduction methods are for example the sliced average variance estimate ( save ) and the principal hessian directions ( phd ) ( see * ? ? ?",
    "* ; * ? ? ?",
    "* respectively ) .",
    "a recent contribution to the field of supervised dimension reduction methods is the supervised invariant coordinate selection ( sics ) @xcite .",
    "it is an extension of invariant coordinate selection ( ics ) @xcite .",
    "both ics and sics are based on the comparison of two scatter matrices @xmath13 and @xmath14 , and the transformation matrix is based on the eigenvalues and eigenvectors of @xmath15 . in sics however , the second scatter matrix depends on the joint distribution of @xmath11 and @xmath10 .",
    "individual dimension reduction methods are usually adequate to find only special types of subspaces or special relationships between @xmath11 and @xmath10 .",
    "for example , sir works well for linear relationships but may completely fail for other type of dependencies ( see for example * ? ? ? * ) .",
    "hence , there is a need for new approaches , which would use the good properties of individual dimension reduction methods and combine the information in an efficient way .",
    "the aim of this paper is to combine different dimension reduction methods in such a way that the  best qualities \" of each method are picked up .",
    "to combine different dimension reduction methods is to say that we combine the individual orthogonal projectors possibly with various ranks and find an  average orthogonal projector \" ( aop ) with an optimized rank .",
    "our approach is similar to the approach in @xcite .",
    "the idea is to find the aop which is , on the average , closest to individual orthogonal projectors with respect to some distance criterium .",
    "the paper is organised as follows . in section  [ sec : theory ] we discuss subspaces and propose a generalization of the crone and crosby distance . @xcite",
    "considered subspaces of equal dimensions , whereas our weighted distance allows subspaces of different dimensions .",
    "some natural choices of weights are given .",
    "furthermore , the concept of averages of subspaces is discussed . in section  [ sec : applications ] the performance of the weighted distance and the different aop s is evaluated in two unsupervised dimension reduction applications and one supervised dimension reduction simulation study . the paper ends with some final remarks .",
    "we first consider linear subspaces in @xmath16 with a fixed dimension @xmath6 , @xmath17 . a linear subspace and the distances between the subspaces can be defined in several ways .    1 .",
    "the subspace is defined as a linear subspace spanned by the linearly independent columns of a @xmath18 matrix @xmath3 , that is , @xmath19 this definition based on a matrix @xmath3 is a bit obscure in the sense that @xmath20 for all full - rank @xmath21 matrices @xmath22 . according to this definition , the same subspace can in fact",
    "be fixed by any member in a set of matrices equivalent to @xmath3 , @xmath23 the non - uniqueness of @xmath3 may cause technical problems in the estimation of a subspace . consider two @xmath4 matrices @xmath24 and @xmath25 with rank @xmath6 .",
    "then a measure of distance between subspaces spanned by @xmath24 and @xmath25 can be defined as @xmath26 where @xmath27 are the squared canonical correlations between @xmath24 and @xmath25 @xcite and @xmath28 .",
    "note that if @xmath24 and @xmath25 are equivalent then the squared canonical correlations are all 1 .",
    "the subspace is defined as a linear subspace spanned by the orthonormal columns of a @xmath18 matrix @xmath29 .",
    "note that , starting with @xmath3 , one can choose @xmath30 for this second definition .",
    "unfortunately , the definition is still obscure as @xmath31 for all orthonormal @xmath21 matrices @xmath32 , and the same subspace is given by any matrix in the class of equivalent orthonormal matrices @xmath33 the principal angles @xmath34 $ ] between the subspaces @xmath35 and @xmath36 with corresponding @xmath6-variate direction vectors @xmath37 and @xmath38 @xmath39 , are recursively defined by maximizing @xmath40 subject to the constraints @xmath41 , and @xmath42 , @xmath43 .",
    "the @xmath44th principal angle is then @xmath45 , @xmath46 , and a measure of distance between the subspaces may be obtained as @xmath47 .",
    "it is easy to see that it equals to @xmath48 .",
    "the subspace is defined as the linear subspace given by an orthogonal projector @xmath49 , that is , a @xmath50 transformation matrix @xmath49 such that @xmath51 matrix @xmath49 provides a unique way to fix the subspace @xmath52 .",
    "note that , starting from @xmath3 , one can define @xmath53 in a unique way .",
    "starting from @xmath54 gives similarly @xmath55 .",
    "the squared distance between the subspaces given by two orthogonal projectors @xmath56 and @xmath57 may then be defined as the matrix ( frobenius ) norm @xmath58 @xcite use @xmath59 as a distance between two @xmath6-dimensional subspaces of @xmath60 given by orthogonal projectors @xmath56 and @xmath57 .",
    "it is then easy to see that @xmath61 and that the distance obeys the triangular inequality @xmath62 for any orthogonal projectors @xmath56 , @xmath57 and @xmath63 .",
    "assume next that the ranks of the projection matrices @xmath56 and @xmath57 are @xmath64 and @xmath65 , respectively , where @xmath66 .",
    "for completeness of the theory , we also accept projection matrices @xmath67 with rank @xmath68 .",
    "as @xmath69 , one possible extension of the above distance is @xmath70^{1/2}. $ ] then @xmath71 but , unfortunately , the triangular inequality is not true for this distance .",
    "we therefore consider other extensions of the metric by @xcite .",
    "let @xmath72 be positive weights attached to dimensions @xmath73 .",
    "( we will later see that the choice of @xmath74 is irrelevant for the theory . )",
    "we then give the following definition .    a weighted distance between subspaces @xmath56 and",
    "@xmath57 with ranks @xmath64 and @xmath65 is given by @xmath75    the weights are used to make the orthogonal projectors @xmath56 and @xmath57 with different ranks more comparable in some sense . as the distance @xmath76 is based on the matrix ( frobenius ) norm , ( i ) @xmath77 , ( ii ) @xmath78 if and only if @xmath79 , ( iii ) @xmath80 , and ( iv ) @xmath81 , and we have the following result .",
    "[ metricwithweights ] for all weight functions @xmath82 , @xmath76 is a metric in the space of orthogonal projectors , and the strict lower and upper bounds of @xmath83 for the dimensions @xmath64 and @xmath65 are @xmath84 where @xmath85    some interesting choices of the weights are , for @xmath86 , @xmath87 weights in ( @xmath88 ) give the distance by @xcite .",
    "weights in ( @xmath89 ) and ( @xmath90 ) standardize the matrices so that @xmath91 and @xmath92 , respectively , if @xmath93 .",
    "it is remarkable that @xmath94 where @xmath95 is a correlation between vectorized @xmath56 and @xmath57 .",
    "proposition  [ metricwithweights ] implies that , for nonzero @xmath64 and @xmath65 , the distances @xmath83 get any values on the closed intervals @xmath96 ,   \\\\    & ( b ) : &   \\left[\\frac 12 \\big|k_1^{-1}-{k_2}^{-1}\\big| , \\frac 12\\left({k_1}^{-1}+{k_2}^{-1 } \\right)+{k_1^{-1}k_2^{-1 } } \\min\\ { p - k_1-k_2,0\\}\\right],\\ \\ \\mbox{and}\\\\    & ( c ) : & \\left [ 1-\\min\\{k_1^{1/2}k_2^{-1/2},k_1^{-1/2}k_2^{1/2 }   \\ } , 1 + k_1^{-1/2 } k_2^{-1/2}\\min\\ { p - k_1-k_2,0\\}\\right].\\end{aligned}\\ ] ] if @xmath97 , for example , then @xmath83 is simply @xmath98 .",
    "recall that , for all three choices of weights , the distance is zero only if @xmath79 ( and @xmath99 ) . for weights @xmath100 ,",
    "the largest possible value for @xmath101 is @xmath102 and it is obtained if and only if @xmath56 and @xmath57 are orthogonal and @xmath103 ( i.e. , @xmath104 ) . for weights",
    "@xmath105 , @xmath106 , and @xmath107 if and only if @xmath56 and @xmath57 are orthogonal and @xmath108 .",
    "finally , for weights @xmath109 , the maximum value @xmath110 for @xmath111 is attained as soon as @xmath56 and @xmath57 are orthogonal and @xmath112 .",
    "the following two special cases illustrate the differences between the three distances .    1 .",
    "first , consider the case when @xmath113 .",
    "then naturally @xmath114 and @xmath115 and therefore , for @xmath116 and with @xmath117 , @xmath118 one can see that @xmath119 depends only on the ratio between @xmath64 and @xmath65 , which can be seen as a nice feature . @xmath120 and",
    "@xmath121 however depend additionally on the actual values of @xmath64 and @xmath65 .",
    "2 .   second , consider the case when @xmath122 and @xmath123 are orthogonal , that is , when @xmath124 .",
    "then @xmath125 and therefore , for nonzero @xmath64 and @xmath65 , @xmath126 it is natural to think subspaces that are orthogonal to each other are furthest apart possible .",
    "this information is apparent in @xmath119 .",
    "however , interpreting both @xmath120 and @xmath121 is again more difficult since they depend on the actual values of @xmath64 and @xmath65 .",
    "consider orthogonal projectors @xmath127 with ranks @xmath128 . to combine the projectors we give the following    [ avedef ] the average orthogonal projector ( aop ) @xmath129",
    "based on weights @xmath72 is an orthogonal projector that minimizes the objective function @xmath130    to find the aop , we can use the following result .",
    "[ aoplemma ] the aop @xmath131 maximizes the function @xmath132 where @xmath133 is a regular average of weighted projectors , and @xmath6 is the rank of @xmath49 .    naturally , @xmath134 is symmetric and non - negative definite , but not a projector anymore . in the following derivations ,",
    "we need its eigenvector and eigenvalue decomposition @xmath135 where @xmath136 and @xmath37 is the eigenvector corresponding to the eigenvalue @xmath137 .",
    "recall that the eigenvectors are uniquely defined only for eigenvalues that are distinct from other eigenvalues . using the lemma  [ aoplemma ] and the eigenvector and eigenvalue decomposition @xmath134 , our main result easily follows .",
    "the rank @xmath6 of the aop @xmath129 maximizes the function @xmath138 where @xmath139 are the eigenvalues of @xmath134 .",
    "moreover , @xmath140 where @xmath141 are the eigenvalues corresponding to eigenvalues @xmath142 .",
    "note that the calculation of the aop @xmath131 is easy , only the eigenvalues and eigenvectors of @xmath134 are needed .",
    "the aop @xmath131 is not always unique .",
    "this happens for example if the rank of an aop is @xmath6 and @xmath143 .",
    "consider now the three weight functions @xmath144 the function @xmath145 for these three weight functions is , for @xmath86 , @xmath146    note that @xmath147 for all weights @xmath82 . to find local maxima for these functions",
    ", one can then use the results @xmath148 for @xmath149 .",
    "note that for @xmath150 , @xmath151 is a concave function and the global maximum is simply the largest @xmath6 with the eigenvalue @xmath152 .",
    "the functions in @xmath153 and @xmath154 are not concave , however , and the global maximum is found by computing all the values @xmath151 , @xmath155 .",
    "in this section we discuss the performance of the averages of orthogonal projectors ( aop ) for three different dimension reduction problems .",
    "the orthogonal projectors and their combinations aim for different targets in different applications .",
    "each problem with natural projectors will be first shortly introduced , and then the performance of aop is demonstrated using simulation studies . the computations in this section are done using r @xcite by mainly using the packages dr @xcite , mnm @xcite , pcapp @xcite and robustbase @xcite .",
    "classical principal component analysis ( pca ) may be based on the eigenvector and eigenvalue decomposition of the covariance matrix of a @xmath0-variate random vector @xmath11 , that is , on @xmath156 where @xmath157 are the ordered eigenvalues and @xmath158 are the corresponding eigenvectors .",
    "orthogonal projector @xmath159 then projects @xmath0-variate observations to the @xmath6-variate subset with maximum variation .",
    "it is unique if @xmath160 .",
    "let @xmath161 be the cumulative distribution function of @xmath11 .",
    "a @xmath50 matrix valued functional @xmath162 is a scatter matrix if @xmath162 is a non - negative definite and symmetric matrix with the affine equivariance property @xmath163 it is remarkable that , if @xmath11 has an elliptic distribution then the ordered eigenvectors of @xmath164 are those of @xmath165 . therefore ,",
    "in the elliptic case , any scatter matrix can be used to find @xmath166 and the matrix @xmath49 is a well - defined population quantity even if the second moments ( and the covariance matrix ) do not exist .",
    "naturally , the sample statistics corresponding to different scatter matrices then have different statistical ( efficiency and robustness ) properties . for a fixed value of @xmath6",
    ", one can then try to `` average '' these different pca approaches to get a compromise estimate .",
    "we next illustrate the performance of the aop in the following simple scenario .",
    "let first @xmath167 @xmath168 .",
    "we choose @xmath169 and wish to estimate @xmath170 .",
    "let then @xmath171 be a random sample from @xmath167 , and find an estimate @xmath172 , an orthogonal projector with rank @xmath169 obtained from the sample covariance matrix .",
    "this estimate is then combined with three robust estimates , namely ,    @xmath173 : :    that is based on tyler s shape matrix @xcite with the affine    equivariant version of spatial median as a multivariate location    estimate @xcite . @xmath174 : :    that is based on the minimum covariance determinant ( mcd ) estimator    @xcite . @xmath175 : :    that is based on projection pursuit ( pp ) approach for pca with the    median absolute deviation ( mad ) criterion as suggested in @xcite .    in the simulations",
    ", @xmath171 was a random sample from @xmath176 with @xmath177 , and the sampling was repeated 1000 times .",
    "as @xmath178 is fixed , we use only @xmath100 as the weight function .",
    "the average squared distances @xmath179 between the four projector estimates , their aop @xmath180 , and @xmath181 are shown in table  [ tab : distmatrix ] .",
    "a similar simulation study was conducted but with observations coming from a heavy - tailed elliptical @xmath182 distribution with @xmath183 as proportional eigenvalues .",
    "note that the regular scatter matrix does not exist in this case but the true projection matrix is still well defined .",
    ".average squared distances @xmath184 between the four projector estimates , their aop @xmath180 , and true @xmath181 .",
    "for all projectors , rank @xmath169 .",
    "the averages are based on 1000 random samples of size @xmath177 from @xmath176 . [ cols=\"<,<,<,<,<,<,<\",options=\"header \" , ]      in the previous section we used projection pursuit ( pp ) approach for principal component analysis .",
    "pp is a much more general technique , however , and there are many other types of indices than just measures of variation to define `` interesting '' one - dimensional directions .",
    "pp actually dates back to @xcite and usually one searches for nongaussian directions . for",
    "a recent review of the existing indices , see for example @xcite .",
    "a major challenge in pp is that it is computationally difficult to find the direction which globally maximizes the index and that there are usually several local maxima .",
    "however , since the local maxima may be also of interest , one possible strategy , as detailed in @xcite , is to run the algorithm many times using different initializations . with this strategy ,",
    "the user has many projectors of rank one but many of them are usually redundant .",
    "so , it is of particular interest to summarize all these projectors in order to extract the directions that are useful and unique .",
    "it means that , in that case , one is interested in an average projector of projectors with rank one that may have a higher rank .    to demonstrate the interest of aop in the context of pp",
    ", we choose the deflation - based fastica method @xcite as an example since it is well - understood and computationally quite simple .",
    "while deflation - based fastica is originally developed in the context of independent component analysis ( ica ) , it can be seen as a traditional pp approach when only one direction is extracted . for a random variable @xmath11 with the standardized version @xmath185",
    ", deflation - based fastica maximizes a measure of non - gaussianity of the form @xmath186 , under the constraint that @xmath187 , where @xmath188 is a selected twice differentiable nonlinear nonquadratic function with @xmath189 .",
    "the final pp direction is then @xmath190 , and the corresponding orthogonal projector is @xmath191 . in our simulations , we use four common choices of @xmath192 with derivative functions @xmath193 : ( i ) @xmath194 , ( ii ) @xmath195 , ( iii ) @xmath196 , and ( iv ) @xmath197 . if there are more than one non - gaussian direction in the data , the direction to be found depends heavily on the initial value of the algorithm , see e.g. @xcite .    in our simulation study , we choose a 10-variate @xmath198 where the first three variables are mixtures of gaussian distributions and @xmath199 , for @xmath200 . more precisely ,",
    "@xmath201 with @xmath202 , @xmath203 and @xmath204  ; @xmath205 with @xmath206 , @xmath207 and @xmath208 , and @xmath209 with @xmath210 , @xmath211 and @xmath212 .",
    "we generated 1000 random samples of sizes @xmath213 from the 10-variate distribution described above . for each sample , we found 100 one - dimensional pp directions ( 4 choices of @xmath188 , 25 random initial values for the algorithm for each choice of @xmath188 ) .",
    "for each sample , 100 pp projectors were then averaged using each of the three weight functions @xmath100 , @xmath105 and @xmath109 . the average projector should in this setting then be close to the projector @xmath214 with rank 3 that picks the three non - gaussian components of the data .",
    "figure  [ fig : rank_freq ] shows the relative frequencies of the ranks of the aops obtained with the three weight functions in 1000 repetitions .",
    "clearly the weight function @xmath100 is not appropriate in this application because @xmath215 implies that @xmath216 with @xmath217 , which means that there can not be more than one eigenvalue larger than 1/2 and , consequently , the rank @xmath6 equals zero or one . with weight functions @xmath105 and @xmath109 , the correct rank 3 is obtained in 82.6% and 68.3% of the runs , respectively .",
    "it is also hoped that the aops are close to the true projector @xmath218 . to evaluate that",
    ", we found that the the average distances @xmath179 between @xmath219 and @xmath218 , between @xmath220 and @xmath218 , and between @xmath221 and @xmath218 were 1.005 , 0.122 , and 0.199 , respectively .",
    "the same numbers for the distances based on @xmath105 and @xmath109 are 0.335 , 0.018 , and 0.035 , and 0.425 , 0.043 , and 0.0736 , respectively .",
    "notice that , for all distances , the aop @xmath220 is closest on average to the true value @xmath218 .      in the pca application",
    ", we used the same @xmath169 for orthogonal projectors and their aop . in the pp application ,",
    "the rank of the orthogonal projectors was taken as one while the rank of their aop was not fixed .",
    "however , for many dimension reduction methods , the ranks of the individual orthogonal projectors are not fixed but also estimated from the data , and the ranks may differ from one method to another .",
    "we now look at this scenario in the framework of supervised dimension reduction .    in supervised dimension reduction ,",
    "one often assumes that a response variable @xmath10 and the @xmath0-vector @xmath11 are related through @xmath222 with an unknown function @xmath223 and an unknown error variable @xmath224 .",
    "the goal of supervised dimension reduction is to estimate the value of @xmath6 and the matrix @xmath225 to obtain @xmath226 with rank @xmath6 .",
    "hence , for supervised dimension reduction , the joint distribution of @xmath10 and @xmath11 is of interest and , for the matrix @xmath3 , it holds that @xmath227 .",
    "many supervised dimension reductions are suggested in the literature and their performances often strongly depend on the unknown function @xmath223 .",
    "the well - known sliced inverse regression ( sir ) for example may not find directions with nonlinear dependencies while , on the other hand , principal hessian directions ( phd ) can not find linear relationships .",
    "hence , when using supervised dimension reduction methods in practice , the estimated rank @xmath6 and the corresponding projector might differ considerably depending on the method .",
    "we propose to use the aop in order to summarize in an efficient way the information brought by the complementary estimation strategies .",
    "in our example we generate data sets from the following three models .",
    "@xmath228 where @xmath229 , @xmath230 , @xmath231 and @xmath232 s are all 10-dimensional row vectors @xmath233 hence @xmath234 for model m1 and @xmath235 for models m2-m3 . in each case",
    ", we generated 100 samples of size 400 .",
    "in our illustration , we use supervised dimension reduction methods implemented in the dr package that provide both the estimate of @xmath6 and the orthogonal projector estimate with the same rank @xmath6 .",
    "the estimation strategies are then ( i ) sliced inverse regression ( sir ) , ( ii ) sliced variance estimation ( save ) , ( iii ) inverse regression estimation ( ire ) , and three types of principal hessian directions ( phd ) , namely , ( iv ) response based principal hessian directions ( phdy ) , ( v ) residual based principal hessian directions ( phdr ) , and ( vi ) the so called @xmath236-based principal hessian directions ( phdq ) . for details about these estimation methods , see @xcite and references therein .",
    "we also add here ( vii ) pca with @xmath6 chosen simply as the number of eigenvalues larger than 1 .",
    "naturally , pca ignores @xmath10 and is therefore not supervised .",
    "( its use could be motivated by the aim to avoid directions with small variation . in our case",
    "it just provides random projectors . )    in the following we want to compare the seven methods above and their aops based on the weight functions @xmath105 and @xmath109 .",
    "the use of @xmath100 is not reasonable with varying @xmath6 .",
    "we consider here the following four aops .    1 .",
    "the aop using @xmath105 with fixed and true @xmath6 .",
    "2 .   the aop using @xmath109 with fixed and true @xmath6 .",
    "the aop using @xmath105 with estimated @xmath6 .",
    "4 .   the aop using @xmath109 with estimated @xmath6 .",
    "some simulation results are collected in figures  [ fig : avproj_distances_m2]-",
    "[ fig : avproj_distances_m6 ] .",
    "the figures show the boxplots for the observed @xmath237 and @xmath238 distances between the true orthogonal projector and the projector estimates coming from different dimension reduction approaches .",
    "consider first the behavior of the estimates in the model m1 with @xmath234 ( see figure  [ fig : avproj_distances_m2 ] ) .",
    "the performances of sir , save and phd estimates seem to be very similar and they usually find only one direction .",
    "( for example , sir finds only the component with linear dependence , and save only the component of quadratic dependence . )",
    "the same seems to be true with ire but with more varying estimates .",
    "recall that , if @xmath113 and @xmath239 and @xmath240 , then @xmath241 , and the average distances of sir , save and phd estimates tend to be close to @xmath242 respectively .",
    "the aop estimates then nicely pick up the two dimensions and clearly outperform other estimates .",
    "note that there is no big difference between aop estimates with known @xmath6 and aop estimates with estimated @xmath6 .",
    "the aop estimate based on @xmath109 seems to be better .",
    "pca has a poor performance as expected .",
    "figure  [ fig : avproj_distances_m4 ] shows the results when the observations come from the model m2 .",
    "the model with linear dependence only is then of course the model where sir is the best one .",
    "ire also performs quite well but , for most samples , save and phd approaches do not find any solution at all . recall that , if @xmath97 and @xmath243 then @xmath244 .",
    "the aop estimates seem often to pick up the correct subspace , and there is no real difference between the @xmath105 and @xmath109 estimates .",
    "this time , the aop estimates with known dimension @xmath235 seem to perform better than the aom estimates with estimated @xmath6 .",
    "figure  [ fig : avproj_distances_m6 ] gives the results for the model m3 with @xmath235 and quadratic dependence .",
    "save and phd approaches work very well , and sir and ire completely fail in this case .",
    "again , all aop estimates neglect the bad estimates and pick up nicely the correct one direction . as in the other cases , pca provides a random reference method with a bad performance indeed .",
    "dimension reduction and subspace estimation is a topic with increasing relevance since modern datasets become larger and larger .",
    "different approaches have different shortcomings and combining the results coming from different approaches might give a better total overview . in this paper , we propose a generalization of the crone and crosby distance for the orthogonal projectors , a weighted distance that allows to combine subspaces of different dimensions .",
    "some natural choices of weights are considered in detail .",
    "the performance of three weighted distances and the combining approach is illustrated via simulations which show that each of them has its own justification depending on the problem at hand .",
    "similar to other areas of statistics , this kind of `` model averaging '' seems to be a way to combine information from competing estimates and to give a better idea of the true model at hand .",
    "99    natexlab#1#1    cardoso , j.  f. ( 1989 ) .",
    "source separation using higher order moments .",
    "_ proceedings of ieee international conference on acoustics , speech and signal processing _ , 4 , 21092112 .",
    "cook , r. d. and weisberg , s. ( 1991 ) . sliced inverse regression for dimension reduction : comment .",
    "_ journal of the american statistical association _ , 86 , 328332 .",
    "crone , l. j. and crosby , d. s. ( 1995 ) .",
    "statistical applications of a metric on subspaces to satellite meteorology . _ technometrics _ , 37 , 324328 .",
    "croux , c. , and ruiz - gazen , a. ( 2005 ) .",
    "high breakdown estimators for principal components : the projection - pursuit approach revisited .",
    "_ journal of multivariate analysis _ , 95 , 206226 .",
    "filzmoser , p. , fritz , h. , and kalcher , k. ( 2012 ) .",
    "pcapp : robust pca by projection pursuit .",
    "r package version 1.9 - 47 .",
    "friedman , j. h. and tukey , j.w .",
    "( 1974 ) . a projection pursuit algorithm for exploratory data analysis . _ ieee trans .",
    "computers c _ , 23 , 881889 .",
    "hettmansperger , t. p. , and randles , r. h. ( 2002 ) .",
    "a practical affine equivariant multivariate median .",
    "_ biometrika _ , 89 , 851860 .",
    "hotelling , h. ( 1936 ) .",
    "relations between two sets of variates .",
    "_ biometrika _ , 28 , 321377 .",
    "hyvrinen , a. ( 1999 ) .",
    "fast and robust fixed - point algorithms for independent component analysis . _ ieee transactions on neural networks _ , 10 , 626634 .",
    "li , k .- c .",
    "( 1991 ) . sliced inverse regression for dimension reduction .",
    "_ journal of the american statistical association _ , 86 , 316327 .",
    "li , k .- c .",
    "( 1992 ) . on principal hessian directions for data visualization and dimension reduction : another application of stein s lemma .",
    "_ journal of the american statistical association _ , 87 , 10251039 .",
    "liski , e. , nordhausen , k. and oja , h. ( 2012 ) .",
    "supervised invariant coordinate selection .",
    "submitted .",
    "nordhausen , k. , ilmonen , p. , mandal , a. , oja , h. and ollila , e. ( 2011 ) .",
    "deflation - based fastica reloaded .",
    "_ proceedings of 19th european signal processing conference 2011 ( eusipco 2011 ) _ , 18541858 .",
    "nordhausen , k. and oja , h. ( 2011 ) .",
    "multivariate l1 methods : the package mnm .",
    "_ journal of statistical software _ , 43 , 128 .",
    "r development core team ( 2012 ) .",
    "r : a language and environment for statistical computing .",
    "r foundation for statistical computing .",
    "vienna , austria .",
    "rodriguez - martinez , e. , goulermas , j.y . ,",
    "mu , t. , and ralph , j.f .",
    "automatic induction of projection pursuit indices .",
    "_ ieee transactions on neural networks _ , 21 , 12811295 .",
    "rousseeuw , p. ( 1986 ) .",
    "multivariate estimation with high breakdown point . in _ mathematical statistics and applications _",
    "grossman , w.,pflug , g. , vincze , i. , and wertz , w. ) , 283297 .",
    "dordrecht , reidel .",
    "rousseeuw , p. , croux , c. , todorov , v. , ruckstuhl , a. , salibian - barrera , m. , verbeke , t. , koller , m. , and maechler , m. ( 2012 ) .",
    "robustbase : basic robust statistics .",
    "r package version 0.9 - 2 .",
    "ruiz - gazen , a. , berro , a. and larabi marie - sainte , s. ( 2010 ) .",
    "detecting multivariate outliers using projection pursuit with particle swarm optimization .",
    "_ compstat2010 _ , 8998 .",
    "tyler , d. e. , ( 1987 ) .",
    "a distribution - free m - estimator of multivariate scatter . _ the annals of statistics _ , 15 , 234251 .",
    "tyler , d. e. , critchley , f. , dmbgen , l. , and oja , h. ( 2009 ) .",
    "invariant co - ordinate selection .",
    "_ journal of the royal statistical society , series b _ , 71 , 549592 .",
    "weisberg , s. ( 2002 ) .",
    "dimension reduction regression in r. _ journal of statistical software _",
    ", 7 , 122 .",
    "[ auxiliarylemma ] for two @xmath50 orthogonal projectors @xmath56 and @xmath57 with ranks @xmath64 and @xmath65 , respectively , @xmath245    * proof *   first note that @xmath246 and @xmath247 where @xmath35 has @xmath64 orthonormal columns and @xmath36 has @xmath65 orthonormal columns . then @xmath248 . as @xmath249 and",
    "@xmath250 one can conclude that @xmath251 .",
    "similarly , @xmath252 and therefore @xmath253 , and the result follows .",
    "note also that the lower and upper bounds in the above lemma are fixed .",
    "the upper bound is obtained with the choices @xmath254 and the lower bound with the choices @xmath255 where @xmath256 is a @xmath0-vector with the @xmath44th component one and other components zero .",
    "* proof of proposition 2.2 *  the aop @xmath131 maximizes @xmath262 where @xmath6 is the rank of @xmath49 .",
    "assume first that @xmath86 is fixed and @xmath263 where @xmath32 has @xmath6 orthonormal columns .",
    "then @xmath264 is maximized as soon as @xmath265 is maximized .",
    "then , as @xmath266 , @xmath267 is maximized if @xmath268 , and the maximum value is @xmath269 . for fixed @xmath86 , the maximum value of @xmath264 is then @xmath270 , and @xmath271 .",
    "the result follows ."
  ],
  "abstract_text": [
    "<S> dimensionality is a major concern in analyzing large data sets . </S>",
    "<S> some well known dimension reduction methods are for example principal component analysis ( pca ) , invariant coordinate selection ( ics ) , sliced inverse regression ( sir ) , sliced average variance estimate ( save ) , principal hessian directions ( phd ) and inverse regression estimator ( ire ) . </S>",
    "<S> however , these methods are usually adequate of finding only certain types of structures or dependencies within the data . </S>",
    "<S> this calls the need to combine information coming from several different dimension reduction methods . </S>",
    "<S> we propose a generalization of the crone and crosby distance , a weighted distance that allows to combine subspaces of different dimensions . </S>",
    "<S> some natural choices of weights are considered in detail . </S>",
    "<S> based on the weighted distance metric we discuss the concept of averages of subspaces as well to combine various dimension reduction methods . </S>",
    "<S> the performance of the weighted distances and the combining approach is illustrated via simulations .    </S>",
    "<S> keywords : dimension reduction ; distance ; metric ; principal component analysis ; projection pursuit ; subspace . </S>"
  ]
}