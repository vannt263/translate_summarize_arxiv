{
  "article_text": [
    "this paper is concerned with the question of reconstructing a vector @xmath0 in a finite - dimensional real or complex hilbert space @xmath1 when only the magnitudes of the coefficients of the vector under a redundant linear map are known .",
    "specifically our problem is to reconstruct @xmath2 up to a global phase factor from the magnitudes @xmath3 where @xmath4 is a frame ( complete system ) for @xmath1 .",
    "a previous paper @xcite described the importance of this problem to signal processing , in particular to the analysis of speech .",
    "of particular interest is the case when the coefficients are obtained from a windowed fourier transform ( also known as short - time fourier transform ) , or an undecimated wavelet transform ( in audio and image signal processing ) . while @xcite presents some necessary and sufficient conditions for reconstruction , the general problem of finding fast / efficient algorithms is still open . in @xcite",
    "we describe one solution in the case of stft coefficients .    for vectors in real hilbert spaces ,",
    "the reconstruction problem is easily shown to be equivalent to a combinatorial problem . in @xcite this problem",
    "is further proved to be equivalent to a ( nonconvex ) optimization problem .",
    "a different approach ( which we called the `` algebraic approach '' ) was proposed in @xcite .",
    "while it applies to both real and complex cases , noisless and noisy cases , the approach requires solving a linear system of size exponentially in space dimension .",
    "the algebraic approach mentioned earlier generalizes the approach in @xcite where reconstruction is performed with complexity @xmath5 ( plus computation of the principal eigenvector for a matrix of size @xmath6 ) .",
    "however this method requires @xmath7 frame vectors .",
    "recently the authors of @xcite developed a convex optimization algorithm ( phaselift ) and proved its ability to perform exact reconstruction in the absence of noise , as well as its stablity under noise conditions . in a separate paper , @xcite",
    ", the authors developed further a similar algorithm in the case of windowed dft transforms .    in this paper",
    "we analyze an iterative algorithm based on regularized least - square criterion .",
    "the organization of the paper is as follows . in section [ sec2 ]",
    "we define the problem explicitely . in section [ sec3 ]",
    "we describe our approach and prove some convergence results . in section [ sec4 ]",
    "we establish the cramer - rao lower bound for benchmarking its performance which is analyzed in section [ sec5 ] . section [ sec6 ] contains conclusions and is followed by references .",
    "let us denote by @xmath1 the n - dimensional hilbert space @xmath8 ( the real case ) or @xmath9 ( the complex case ) with scalar product @xmath10 .",
    "let @xmath11 be a spanning set of @xmath12 vectors in @xmath1 . in finite dimension",
    "( as it is the case here ) such a set forms a _",
    "frame_. in the infinite dimensional case , the concept of frame involves a stronger property than completness ( see for instance @xcite ) .",
    "we review additional terminology and properties which remain still true in the infinite dimensional setting .",
    "the set @xmath13 is frame if and only if there are two positive constants @xmath14 ( called frame bounds ) so that @xmath15 when we can choose @xmath16 the frame is said _",
    "tight_. for @xmath17 the frame is called _",
    "parseval_. a set of vectors @xmath13 of the @xmath6-dimensional hilbert space @xmath1 is said to have _ full spark _ if any subset of @xmath6 vectors is linearly independent .    for a vector @xmath2 ,",
    "the collection of coefficients @xmath18 represents the analysis of vector @xmath0 given by the frame @xmath13 . in @xmath1",
    "we consider the following equivalence relation : @xmath19 let @xmath20 be the set of classes of equivalence induced by this relation .",
    "thus @xmath21 in the real case ( when @xmath22 ) , and @xmath23 in the complex case ( when @xmath24 ) .",
    "the analysis map induces the following nonlinear map @xmath25 where @xmath26 is the set of nonnegative real numbers . in previous papers @xcite we studied when the nonlinear map @xmath27 is injective .",
    "we review these results below . in this paper",
    "we describe an algorithm to solve the equation @xmath28 and then we study its performance in the presence of additive white gaussian noise when the model becomes @xmath29 we shall derive the cramer - rao lower bound ( crlb ) for this model and compare its performance to this bound .      we revise now existing results on injectivity of the nonlinear map @xmath27 . in general a subset @xmath30 of a topological space",
    "is said _ generic _ if its open interior is dense .",
    "however in the following statements , the term _ generic _ refers to zarisky topology : a set @xmath31 is said _ generic _ if @xmath30 is dense in @xmath32 and its complement is a finite union of zero sets of polynomials in @xmath33 variables with coefficients in the field @xmath34 ( here @xmath35 or @xmath36 ) .",
    "[ th2.1 ] in the real case when @xmath22 the following are equivalent :    1 .",
    "the nonlinear map @xmath27 is injective ; 2 .   for any disjoint partition of the frame set @xmath37 , either @xmath38 spans @xmath1 or @xmath39 spans @xmath1 .",
    "[ th2.2 ] the following hold true in the real case @xmath22 :    1 .",
    "if @xmath27 is injective then @xmath40 ; 2 .   if @xmath41 then @xmath27 can not be injective ; 3 .   if @xmath42 then @xmath27 is injective if an only if @xmath13 is full spark ; 4 .",
    "if @xmath40 and @xmath13 is full spark then the map @xmath27 is injective ; 5 .   if @xmath40 then for a generic frame @xmath13 the map @xmath27 is injective .",
    "[ th2.3 ] in the complex case when @xmath24 the following hold true :    1 .",
    "if @xmath43 then for a generic frame @xmath13 the map @xmath27 is injective ; 2 .   if @xmath27 is injective then @xmath44 ; 3 .   if @xmath45 then the map @xmath27 can not be injective .",
    "we obtain equivalent conditions to ( 2 ) in theorem [ th2.1 ] . in the real case",
    "these new conditions are equivalent to @xmath27 being injective ; in the complex case they are only necessary condition for injectivity .",
    "[ th2.4 ] given a @xmath12-set of vectors @xmath46 the following conditions are equivalent :    1 .   for any disjoint partition of the frame set @xmath37 ,",
    "either @xmath38 spans @xmath1 or @xmath39 spans @xmath1 ; 2 .   for any two vectors",
    "@xmath47 if @xmath48 and @xmath49 then @xmath50 3 .",
    "there is a positive real constant @xmath51 so that for all @xmath47 , @xmath52 4 .",
    "there is a positive real constant @xmath51 so that for all @xmath2 , @xmath53 where the inequality is in the sense of quadratic forms .",
    "the constants in ( 3 ) and ( 4 ) above are the same ( hence the same notation ) .    _",
    "proof _    @xmath54 .",
    "we prove by contradiction : @xmath55 .",
    "assume there are @xmath47 , @xmath56 , @xmath49 so that @xmath57",
    ". then @xmath58 for all @xmath59 .",
    "let @xmath60 , and set @xmath61 .",
    "let @xmath62 , and set @xmath63 . since @xmath0 is orthogonal to @xmath38 it follows that @xmath38 can not span the whole @xmath1 ; similarly @xmath39 can not span @xmath1 because @xmath64 is orthogonal to all @xmath39 .",
    "this contradicts @xmath65 .",
    "the unit sphere @xmath67 is compact in @xmath1 and so is @xmath68 .",
    "since the map @xmath69 is continuous , it follows @xmath70 by homogeneity for any @xmath47 , @xmath71 we obtain : @xmath72 ) .",
    "if either @xmath73 or @xmath74 then ( [ eq : q2 ] ) holds true .",
    "@xmath75 . follows immediately by definition of quadratic forms .",
    "we prove by contradiction : @xmath77 . if there is a partition @xmath37 so that neither @xmath38 spans @xmath1 nor @xmath39 spans @xmath1 , then there are two non - zero vectors @xmath47 so that @xmath78 and @xmath79",
    ". thus @xmath80 for all @xmath59 . in turn",
    "this means @xmath81 which contradicts ( [ eq : q2 ] ) .",
    "@xmath82 .",
    "note the proof of this result produced the following condition equivalent to negating any of the statements of theorem [ th2.4 ] : there are two non - zero vectors @xmath47 and a subset @xmath83 so that @xmath84 for all @xmath85 , and @xmath86 for all @xmath87",
    ". then one can immediatly check that @xmath88 and @xmath89 are two non - equivalent vectors in @xmath1 with respect to the ( real or complex ) equivalence relation @xmath90 , and yet @xmath91 ; hence @xmath27 can not be injective .",
    "we thus obtained    [ th2.5 ]    1 .",
    "when @xmath22 , @xmath27 is injective if and only if any ( and hence all ) of the conditions of theorem [ th2.4 ] is satisfied ; 2 .",
    "when @xmath24 , if @xmath27 is injective then all conditions of theorem [ th2.4 ] must hold .",
    "consider the additive noise model in ( [ eq : model ] ) .",
    "our data is the vector @xmath92 .",
    "our goal is to find a @xmath2 that minimizes @xmath93 , where we use the euclidian norm .",
    "as discussed also in section [ sec4 ] , the least - square error minimizer represents the maximum likelihood estimator ( mle ) when the noise is gaussian . in this section",
    "we discuss an optimization algorithm for this criterion .",
    "consider the following function @xmath94 our goal is to minimize @xmath95 over @xmath96 , for some ( and hence any ) value @xmath97 .",
    "our strategy is the following iterative process : @xmath98 for some initialization @xmath99 and policy @xmath100 and @xmath101 .",
    "consider the regularized least - square problem : @xmath102 note the following relation @xmath103 where @xmath104 for @xmath105 the optimal solution is @xmath106 . note that if @xmath107 as a quadratic form then the optimal solution of @xmath108 is @xmath106 .",
    "consequently we assume the largest eigenvalue of @xmath109 is positive .",
    "as @xmath110 decreases the optimizer remains small .",
    "hence we can neglect the forth order term in @xmath96 in the expansion above and obtain : @xmath111 thus the critical value of @xmath110 for which we may get a nonzero solution is @xmath112 is the maximum eigenvalue of @xmath109 .",
    "let us denote by @xmath113 this ( positive ) eigenvalue and @xmath114 its associated normalized eigenvector .",
    "this suggests to initialize @xmath115 for some @xmath116 and @xmath117 , for some scalar @xmath118 .",
    "substituting into ( [ eq : j0 ] ) we obtain @xmath119 for fixed @xmath120 , the minimum over @xmath118 is achieved at @xmath121 the parameter @xmath122 controls the step size at each iteration . the larger the value the smaller the step . on the other hand",
    ", a small value of this parameter may produce an unstable behavior of the iterates .",
    "in our implementation we use the same initial value for both @xmath110 and @xmath122 : @xmath123      optimization problem ( [ eq : xt1 ] ) admits a closed form solution .",
    "specifically , expanding the quadratic in @xmath96 we obtain @xmath124 where @xmath125 and @xmath109 is defined in ( [ eq : q ] ) .",
    "we obtain that @xmath126 satisfies the following linear equation @xmath127 note the quadratic form on the left hand side is bounded below by @xmath128 where @xmath129 is given by ( [ eq : a0 ] ) .",
    "denote @xmath130 .",
    "we have the following general result :    [ th3.1 ] assume @xmath131 and @xmath132 .",
    "then for any initialization @xmath133 the sequence @xmath134 is monotonically decreasing and therefore convergent .",
    "this theorem follows immediately from the following lemma :    [ lem3.2 ] assume @xmath135 and @xmath136 , then @xmath137 .",
    "_ proof _ first remark the symmetry @xmath138 then we have : @xmath139 this concludes the proof of the lemma .",
    "we are now ready to state the first optimization algorithm :    * input data * : frame set @xmath140 , measurements @xmath141 , initialization parameter @xmath120 , stopping criterion @xmath142 , or maximum number of iterations @xmath143 .",
    "* initialization * : compute matrix @xmath109 in ( [ eq : q ] ) and its principal eigenvalue @xmath113 and eigenvector @xmath114 . compute @xmath144 in ( [ eq : beta0 ] ) . set @xmath145 and @xmath146    * iterate*.",
    "repeat :    1 .   compute @xmath147 given by ( [ eq : rt ] ) ; 2 .",
    "solve ( [ eq : xt+1 ] ) for @xmath126 ; 3 .",
    "update @xmath148 , @xmath149 using a preset or adaptive policy ( more details are provided in section [ sec5 ] ) ; 4 .",
    "compute @xmath150 and increment @xmath151 ;    until @xmath152 , or @xmath153 , or @xmath154 .    * outputs * : estimated signal @xmath155 , criterion @xmath156 , error @xmath157 .",
    "results of numerical simulations ( see section [ sec5 ] ) suggest the adaptation of @xmath110 and @xmath122 is too agressive . instead of running the algorithm until @xmath158 ( a small value ) , we implemented a second algorithm where we track the mean - square error : @xmath159 and return the minimum value .",
    "we thus obtain a second algorithm :    * input data * : frame set @xmath140 , measurements @xmath141 , initialization parameter @xmath120 , stopping criterion @xmath142 , or maximum number of iterations @xmath143 .",
    "* initialization * : compute matrix @xmath109 in ( [ eq : q ] ) and its principal eigenvalue @xmath113 and eigenvector @xmath114 . compute @xmath144 in ( [ eq : beta0 ] ) . set @xmath145 and @xmath160    * iterate*.",
    "repeat :    1 .   compute @xmath147 given by ( [ eq : rt ] ) ; 2 .",
    "solve ( [ eq : xt+1 ] ) for @xmath126 ; 3 .",
    "update @xmath148 , @xmath149 using a preset or adaptive policy ( more details are provided in section [ sec5 ] ) ; 4 .",
    "compute @xmath150 ; 5 .",
    "compute @xmath161 ; 6 .   if @xmath162 then @xmath163 and @xmath164 ; 7 .",
    "increment @xmath151 ;    until @xmath152 , or @xmath153 , or @xmath154 .    * outputs * : estimated signal @xmath165 , criterion @xmath156 , error @xmath166 .",
    "consider the noisy measurement model ( [ eq : model ] ) , @xmath167 , with @xmath168 . fix a direction in @xmath1 , say @xmath169 .",
    "we make the following two assumptions regarding the unknown signal @xmath0 : ( 1 ) we assume @xmath0 is not orthogonal to @xmath169 , that is @xmath170 ; ( 2 ) we assume we are given the sign of this scalar product ; say @xmath171 .",
    "these two assumptions allow us to resolve the global sign ambiguity .",
    "thus @xmath172 where @xmath173 is a half - space of @xmath1 .",
    "since it is a convex cone we can compute expectations of random variables defined in @xmath173 .",
    "the likelihood function is given by @xmath174 the fisher information matrix @xmath175 is given by @xmath176\\ ] ] where the expectation is over the noise process , for fixed @xmath0 .    in the following",
    "we perform the computations in the real case @xmath22 . for ease of notation",
    "we assume the canonical basis of @xmath8 and the lower index ( or second index ) denotes the coordinate with respect to this basis ; for instance @xmath177 and @xmath178 denote the @xmath179 coordinate of @xmath0 and @xmath180 , respectively .",
    "@xmath181 @xmath182 now use @xmath183 = |{{\\langlex , y_l\\rangle}}|^2 $ ] .",
    "we thus obtain @xmath184 where @xmath185 denotes the quadratic form introduced in ( [ eq : q2 ] ) .",
    "now we are ready to state the first lower bound result ( see e.g. @xcite theorem 3.2 ) .",
    "[ th4.1 ] in the real case @xmath22 , the fisher information matrix for model ( [ eq : model ] ) is given by @xmath175 in ( [ eq : fisher ] ) .",
    "consequently the covariance matrix of any uniabsed estimator @xmath186 for @xmath0 is bounded below by the cramer - rao lower bound as follows @xmath187 \\geq crlb(x ) : = ( { { \\mathbb i}}(x))^{-1 } = \\frac{\\sigma^2}{4 } ( r(x))^{-1}\\ ] ] furthermore the conditional mean - square error of any unbiased estimator @xmath186 is given by @xmath188 \\geq \\frac{\\sigma^2}{4 } trace\\{(r(x))^{-1}\\}.\\ ] ] when signal @xmath0 is random and drawn from @xmath189 , the mean - square error of the unbiased estimator @xmath186 is bounded below by @xmath190 \\geq \\frac{\\sigma^2}{4 } trace\\ { \\e[(r(x))^{-1 } ] \\}.\\ ] ]    corollary [ th2.5 ] implies that when @xmath27 is injective the fisher information matrix is invertible , hence a bounded crlb , and the signal is identifiable in @xmath173 ( up to a global phase factor ) .",
    "we derive now a different lower bound for a modified estimation problem .",
    "let us denote by @xmath191 and @xmath192 the rank-1 operators associated to vectors @xmath0 and @xmath193 respectively .",
    "note @xmath194 .",
    "hence @xmath195 we would like to obtain a lower bound on conditional mean - square error of an unbiased estimator of the rank-1 matrix @xmath196 .",
    "a naive computation of the fisher information associated to @xmath196 in the linear model above would produce a singular matrix whenever @xmath197 ( the reason being the fact that a general symmetric @xmath196 is not identifiable merely from @xmath198 measurements ) . instead",
    "the bound should be derived under the additional hypothesis that @xmath196 has rank one .",
    "we obtain such a bound using a modified crlb .",
    "let @xmath199 be the vector valued map @xmath200 of @xmath201 components .",
    "let @xmath202 denote any unbiased estimator of the rank-1 matrix @xmath196 .",
    "then ( see equation ( 3.30 ) in @xcite ) @xmath203 taking trace on both sides we get @xmath204 \\geq trace\\{{{\\mathbb i}}^{-1 }   \\left(\\frac{\\partial g}{\\partial x}\\right)^t \\left(\\frac{\\partial g}{\\partial x}\\right ) \\}\\ ] ] let @xmath205",
    ". then we have @xmath206 thus we obtained    [ th4.1b ] the conditional mean - square error of any unbiased estimator of the rank-1 matrix @xmath191 is bounded below by @xmath207 \\geq \\frac{\\sigma^2}{2}\\left ( { { \\|x\\|}}^2 trace\\{r^{-1}\\ } + x^t r^{-1}x \\right).\\ ] ]    consider now the case of the maximum likelihood estimator ( mle ) whose optimization problem was considered in the previous section . for model ( [ eq : model ] ) this takes the form of @xmath208 the mle computes the global minimum in the optimization problem above .",
    "assume that @xmath209 selects the closest global minimum to @xmath0 .",
    "we want to estimate lower bounds on the mle performance so that we can benchmark performance of any optimization algorithm against these bounds .",
    "first we estimate the bias of the mle estimator in the asymptotic limit @xmath210 .",
    "the estimator must satify the mle equation @xmath211 which turns into @xmath212 denote @xmath213 .",
    "the bias is given by @xmath214 $ ] .",
    "assymptotically we can assume @xmath215 is small with high probability .",
    "we shall expand the products in the above equation taking into account only the first terms in @xmath216 : @xmath217 expanding the products and neglecting higher order terms in @xmath216 we obtain : @xmath218 note @xmath219 .",
    "let us denote @xmath220 the equation that @xmath221 satisfies becomes @xmath222 .",
    "therefore @xmath223x.\\ ] ] for fixed @xmath56 , due to the lower bound in ( [ eq : q2 ] ) we obtain with high probability @xmath224 , where @xmath225 denotes the smallest eigenvalue of @xmath226 .",
    "note @xmath227 by ( [ eq : a0 ] ) .",
    "then using neumann s series expansion we get @xmath228 thus we obtain @xmath229 note also the similarity criterion in expansion above is related to @xmath230 which is of the order @xmath231 .",
    "since all odd moments of gaussian random variables vanish we obtain @xmath232 = 0 $ ] . hence @xmath233 = \\frac{1}{4 } \\e[r^{-1}nr^{-1}nx ] + \\e[o(n^4 ) ] =   \\nonumber \\\\    &   = &   \\frac{\\sigma^2}{4}\\sum_{k=1}^m { { \\langlex , f_k\\rangle } } { { \\langler^{-1}f_k , f_k\\rangle}}r^{-1}f_k + o((\\sigma{{\\|r^{-1}\\|}})^4 ) \\label{eq : bias}\\end{aligned}\\ ] ] the leading term in bias has the form @xmath234 note the dependence on @xmath0 is highly nonlinear .",
    "we would like next to obtain the modified crlb for mle taking into account its bias .",
    "we need to estimate the first derivatives of @xmath235 with respect to @xmath0 , @xmath236 .",
    "again we shall derive the asymptotic approximation of this matrix : @xmath237 the key relation to use is @xmath238 which comes from @xmath239 by differentiating with respect to @xmath240 , and from ( [ eq : q2 ] ) .",
    "after some straightforward but tedious algebra we obtain    @xmath241    now we can compute the modified cramer - rao lower bound for the mle estimator ( see e.g. @xcite equation ( 3.30 ) ) .",
    "[ th4.2 ] the mle estimator ( [ eq : mle ] ) is biased .",
    "its expectation admits the following asymptotic approximation @xmath242 = x + \\frac{\\sigma^2}{4}\\delta + o((\\sigma{{\\|r^{-1}\\|}})^4).\\ ] ] its covariance matrix is bounded below by @xmath243 \\geq ( i+\\frac{\\partial bias}{\\partial x}){{\\mathbb i}}^{-1}(i+(\\frac{\\partial bias}{\\partial x})^t ) = \\frac{\\sigma^2}{4}r^{-1 } + \\frac{\\sigma^4}{16}(r^{-1}\\delta^t+\\delta r^{-1 } ) + o((\\sigma{{\\|r^{-1}\\|}})^6)\\ ] ] where @xmath244 is the identity matrix .",
    "furthermore , the conditional mean - square error is bounded below by @xmath245 & = & { { \\|bias(x)\\|}}^2 + trace\\{{\\cal cov}[\\phi_{mle}(y)]\\ } \\nonumber \\\\",
    "\\geq \\frac{\\sigma^2}{4}trace\\{r^{-1}\\ } & + & \\frac{\\sigma^4}{16}\\left ( { { \\|\\delta\\|}}^2 + 2trace\\{\\delta r^{-1 } \\ } \\right ) + o((\\sigma{{\\|r^{-1}\\|}})^6 ) \\nonumber\\end{aligned}\\ ] ] here we used the notation @xmath246 , and @xmath247 given by ( [ eq : bias0 ] ) , ( [ eq : delta ] ) .",
    "in this section we present numerical simulations for the algorithms presented in this paper .",
    "we generated random frames or redundancy 3 , that is @xmath248 , as well as random signals @xmath0 .",
    "all these vectors ( frame and signal ) are drawn from @xmath249 .",
    "we set the first component of @xmath0 positive , and so we decided the global sign after reconstruction . to the magnitude square of signal coefficients",
    "@xmath250 we added gaussian noise of variance @xmath251 to achieve a fixed signal - to - noise - ratio defined as @xmath252}\\ ] ] note the similarity criterion used in asymptotic expansions ( [ eq : meanmle ] ) and ( [ eq : covmle ] ) is of the same order as @xmath253 ( up to multiplicative constants ) .",
    "we used 11 values of snrdb in 10db increments from -20db to + 80db .",
    "for the first algorithm , results are averaged over 100 noise realizations . in each instance of the algorithm we initialized @xmath254 as described in subsection [ subsec3.4 ] with @xmath255 . at each iteration @xmath256 .",
    "we run the algorithm for at least 100 steps , or until @xmath148 gets below @xmath257 .",
    "the parameter@xmath149 is adapted as follows : @xmath258 .",
    "we include results for @xmath259 , @xmath260 and @xmath261 .",
    "figure [ fig1 ] includes the conditional mean - square error averaged over 100 noise realizations , and the lower bounds : the unbiased crlb ( [ eq : crlb ] ) and the mle adapted crlb ( [ eq : msemle ] ) .",
    "note the two lower bounds are indistingueshable for @xmath262 . for low snr ,",
    "when the two bounds differ significantly , the approximation ( [ eq : v ] ) is no longer valid .",
    "hence the bound would be different as well .",
    "in general we can not obtaion a closed form solution for the new bound .",
    "( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) .",
    "[ fig1],title=\"fig:\",width=377,height=226 ]   ( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) .",
    "[ fig1],title=\"fig:\",width=377,height=226 ]   ( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) .",
    "[ fig1],title=\"fig:\",width=377,height=226 ]    in figure [ fig2 ] we plot the bias and variance components of the mean - square error for the same results in figure [ fig1 ] .",
    "note the bias is relatively small .",
    "the bulk of mean - square error is due to estimation variance .",
    "( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) .",
    "[ fig2],title=\"fig:\",width=377,height=226 ]   ( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) .",
    "[ fig2],title=\"fig:\",width=377,height=226 ]   ( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) .",
    "[ fig2],title=\"fig:\",width=377,height=226 ]    figure [ fig3 ] contains the average number of iterations for each of these cases .",
    "the algorithm runs for about 530 - 660 steps .",
    "( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) .",
    "[ fig3],title=\"fig:\",width=377,height=226 ]   ( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) .",
    "[ fig3],title=\"fig:\",width=377,height=226 ]   ( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) .",
    "[ fig3],title=\"fig:\",width=377,height=226 ]    for the second algorithm we repeated the same cases ( @xmath263 ) and same levels of snr , but we average over 1000 noise realizations .",
    "we present the mean - square error in two cases : in figure [ fig4 ] the case of fixed sign as discussed earlier ( first component of @xmath0 is positive ) ; in figure [ fig5 ] the case of a sign oracle , when the global sign is chosen as given by the minimum @xmath264 .",
    "( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) for fixed sign .",
    "[ fig4],title=\"fig:\",width=377,height=226 ]   ( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) for fixed sign .",
    "[ fig4],title=\"fig:\",width=377,height=226 ]   ( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) for fixed sign .",
    "[ fig4],title=\"fig:\",width=377,height=226 ]     ( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) for global sign oracle .",
    "[ fig5],title=\"fig:\",width=377,height=226 ]   ( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) for global sign oracle .",
    "[ fig5],title=\"fig:\",width=377,height=226 ]   ( top plot ) , @xmath260 ( middle plot ) , and @xmath261 ( bottom plot ) for global sign oracle .",
    "[ fig5],title=\"fig:\",width=377,height=226 ]",
    "novel necessary conditions for signal reconstruction from magnitudes of frame coefficients have been presented .",
    "these conditions are also sufficient in the real case",
    ". the least - square criterion has been analyzed , and two algorithms have been proposed to optimize this criterion .",
    "performance of the second algorithm presented in this paper is remarkably close to the theoretical lower bound given by the cramer - rao inequality .",
    "in fact for low snr its performance is better than the asymptotic approximation given by the modified crlb .",
    "r.  balan , p.  casazza , d.  edidin , _ equivalence of reconstruction from the absolute value of the frame coefficients to a sparse representation problem _ , ieee signal.proc.letters , * 14 * ( 5 ) ( 2007 ) , 341343 .",
    "s.  bandyopadhyay , p.  o. boykin , v.  roychowdhury , and f.  vatan , _ a new proof for the existence of mutually unbiased bases _ , algorithmica * 34 * ( 2002 ) , no .  4 , 512528 , quantum computation and quantum cryptography .",
    "p. o. boykin , m. sitharam , p. h. tiep , and p. wocjan _ mutually unbiased bases and orthogonal decompositions of lie algebras _ , e - print : arxiv.org/quant-ph/0506089 , to appear in quantum information and computation , 2007 .",
    "a.  r. calderbank , p.  j. cameron , w.  m. kantor , and j.  j. seidel , _",
    "@xmath265-kerdock codes , orthogonal spreads , and extremal euclidean line - sets _ , proc .",
    "london math .",
    "( 3 ) * 75 * ( 1997 ) , no .  2 , 436480 .",
    "e.  cands , t.  strohmer , v.  voroninski , _ phaselift : exact and stable signal recovery from magnitude measurements via convex programming _ , to appear in communications in pure and applied mathematics ( 2011 )      p.  casazza , _ the art of frame theory _ , taiwanese j. math . , ( 2 ) * 4 * ( 2000 ) , 129202 .",
    "p.  g. casazza and m.  fickus , _ fourier transforms of finite chirps _",
    ", eurasip j. appl . signal process .",
    "( 2006 ) , no .  frames and overcomplete representations in signal processing , communications , and information theory , art .",
    "i d 70204 , 1 - 7 .",
    "s.  d. howard , a.  r. calderbank , and w.  moran , _ the finite heisenberg - weyl groups in radar and communications _ , eurasip j. appl . signal process .",
    "( 2006 ) , no .  frames and overcomplete representations in signal processing , communications , and information theory , art .",
    "i d 85685 , 1 - 12 ."
  ],
  "abstract_text": [
    "<S> this paper is concerned with the question of reconstructing a vector in a finite - dimensional real or complex hilbert space when only the magnitudes of the coefficients of the vector under a redundant linear map are known . </S>",
    "<S> we present new invertibility results as well an iterative algorithm that finds the least - square solution and is robust in the presence of noise . </S>",
    "<S> we analyze its numerical performance by comparing it to two versions of the cramer - rao lower bound . </S>"
  ]
}