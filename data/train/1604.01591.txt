{
  "article_text": [
    "we deal with overdetermined system of linear equations @xmath3 , which is common in linear parameter estimation problem @xcite .",
    "if the data matrix @xmath2 and observation matrix @xmath4 are contaminated with errors , and all the errors are uncorrelated and have equal variances , then the total least squares ( tls ) technique is appropriate for solving this system @xcite .",
    "kukush and van huffel @xcite showed the statistical consistency of the tls estimator @xmath5 as the number  @xmath6 of rows in @xmath2 grows , provided that the errors in @xmath0 $ ] are row - wise i.i.d . with zero mean and covariance matrix proportional to a unit matrix ; the covariance matrix was assumed to be known up to a factor of proportionality ; the true input matrix @xmath7 was supposed to be nonrandom .",
    "in fact , in @xcite a more general , element - wise weighted tls estimator was studied , where the errors in @xmath0 $ ] were row - wise independent , but within each row , the entries could be observed without errors , and , additionally , the error covariance matrix could differ from row to row . in @xcite , an iterative numerical procedure was developed to compute the elementwise - weighted tls estimator , and the rate of convergence of the procedure was established .    in a univariate case where @xmath4 and @xmath1 are column vectors ,",
    "the asymptotic normality of @xmath5 was shown by gallo @xcite as @xmath6 grows . in @xcite , that result",
    "was extended to mixing error sequences .",
    "both @xcite and @xcite utilized an explicit form of the tls solution .    in the present paper , we extend the gallo s asymptotic normality result to a multivariate case , where @xmath2 , @xmath1 , and @xmath4 are matrices .",
    "now a closed - form solution is unavailable , and we work instead with the cost function",
    ". more precisely , we deal with the estimating function , which is a matrix derivative of the cost function .",
    "in fact , we show that under mild conditions , the normalized estimator converges in distribution to a gaussian random matrix with nonsingular covariance structure . for normal errors ,",
    "the latter structure can be estimated consistently based on the observed matrix @xmath0 $ ] .",
    "the results can be used to construct the asymptotic confidence ellipsoid for a vector @xmath8 , where @xmath9 is a column vector of the corresponding dimension .",
    "the paper is organized as follows . in section  [ s:2 ] , we describe the model , refer to the consistency result for the estimator , and present the objective function and corresponding matrix estimating function . in section  [ s:3 ] , we state the asymptotic normality of @xmath5 and provide a nonsingular covariance structure for a limit random matrix .",
    "the latter structure depends continuously on some nuisance parameters of the model , and we derive consistent estimators for those parameters . section  [ s:4 ] concludes .",
    "the proofs are given in appendix .",
    "there we work with the estimating function and derive an expansion for the normalized estimator using taylor s formula .",
    "the expansion holds with probability tending to @xmath10 .    throughout the paper ,",
    "all vectors are column ones , @xmath11 stands for the expectation and acts as an operator on the total product , @xmath12 denotes the covariance matrix of a random vector @xmath13 , and for a sequence of random matrices @xmath14 of the same size , the notation @xmath15 means that the sequence @xmath16 is stochastically bounded , and @xmath17 means that @xmath18 . by @xmath19",
    "we denote the unit matrix of size @xmath20 .",
    "consider the model @xmath3 . here",
    "@xmath21 and @xmath22 are observations , and @xmath23 is a parameter of interest . assume that @xmath24 and that there exists @xmath25 such that @xmath26 here @xmath7 is the nonrandom true input matrix , @xmath27 is the true output matrix , and @xmath28 , @xmath29 are error matrices .",
    "the matrix @xmath30 is the true value of the parameter .",
    "we can rewrite the model  as a classical functional errors - in - variables ( eiv ) model with vector regressor and vector response @xcite .",
    "denote by @xmath31 , @xmath32 , @xmath33 , @xmath34 , @xmath35 , and @xmath36 the rows of @xmath2 , @xmath7 , @xmath28 , @xmath4 , @xmath27 , and @xmath29 , respectively , @xmath37 .",
    "then the model considered is equivalent to the following eiv model : @xmath38 based on observations @xmath39 , @xmath40 , @xmath37 , we have to estimate @xmath30 .",
    "the vectors @xmath41 are nonrandom and unknown , and the vectors @xmath42 , @xmath43 are random errors .",
    "we state a global assumption of the paper .    1 .   [ i ]",
    "the vectors @xmath44 with @xmath45 $ ] , @xmath46 , are i.i.d .",
    ", with zero mean and variance  covariance matrix @xmath47 where the factor of proportionality @xmath48 is positive and unknown .",
    "the tls problem consists in finding the values of disturbances @xmath49 and @xmath50 minimizing the sum of squared corrections @xmath51 subject to the constraints @xmath52    here in , for a matrix @xmath53 , @xmath54 denotes the frobenius norm , @xmath55 . later on , we will also use the operator norm @xmath56 .",
    "it may happen that , for some random realization , problem  has no solution .",
    "in such a case , put @xmath57 .",
    "now , we give a formal definition of the tls estimator .",
    "the tls estimator @xmath5 of @xmath30 in the model  is a  measurable mapping of the underlying probability space into @xmath58 , which solves problem  if there exists a solution , and @xmath59 otherwise .",
    "we need the following conditions for the consistency of @xmath5 .    1 .   [ ii ] @xmath60 , where @xmath61 satisfies condition .",
    "[ iii ] @xmath62 as @xmath63 , where @xmath64 is a nonsingular matrix .",
    "the next consistency result is contained in theorem 4(a ) of @xcite .    [ thm:2 ] assume condition to .",
    "then @xmath5 is finite with probability tending to one , and @xmath5 tends to @xmath30 in probability as @xmath63",
    ".      denote @xmath65 the tls estimator is known to minimize the objective function ; see @xcite or formula ( 24 ) in @xcite .",
    "[ l:3 ] the tls estimator @xmath5 is finite iff there exists an unconstrained minimum of the function , and then @xmath66 is a minimum point of that function .",
    "introduce an estimating function related to the loss function : @xmath67    [ cor:4 ]    1 .",
    "[ 4.a ] under conditions to , with probability tending to one @xmath5 is a solution to the equation @xmath68 2 .",
    "[ 4.b ] under assumption , the function @xmath69 is unbiased estimating function , that is , for each @xmath70 , @xmath71 .",
    "expression as a function of @xmath1 is a mapping in @xmath72 .",
    "its derivative @xmath73 is a linear operator in this space .",
    "[ l:5 ] under condition , for each @xmath74 and @xmath70 , we have @xmath75=a_{0i}a_{0i}^{\\operatorname{\\mathsf t}}h.\\ ] ]    therefore , we can identify @xmath76 with the matrix @xmath77 .",
    "introduce further assumptions to state the asymptotic normality of @xmath66 .",
    "we need a bit higher moments compared with conditions and in order to use the lyapunov clt . recall that @xmath44 satisfies condition .    1 .   [ iv ] for some @xmath78 , @xmath79 .",
    "[ v ] for @xmath80 from condition , @xmath81 3 .",
    "[ vi ] @xmath82 as @xmath63 , where @xmath83 .",
    "[ vii ] the distribution of @xmath61 is symmetric around the origin .",
    "introduce a random element in the space of systems consisting of five matrices : @xmath84    hereafter @xmath85 stands for the convergence in distribution .",
    "[ l:6 ] assume conditions and . then @xmath86 where @xmath87 is a gaussian centered random element with matrix components .",
    "[ l:7 ] in assumptions of lemma  [ l:6 ] , replace condition with condition . then the convergence still holds with independent components @xmath88 .",
    "now , we state the asymptotic normality of @xmath5 .",
    "[ thm:8 ]    1 .",
    "[ 8.a ] assume conditions and . then @xmath89 @xmath90 where @xmath64 satisfies condition , and @xmath91 satisfy relation .",
    "[ 8.b ] in the assumption of part , replace condition with condition . then the convergence still holds , and , moreover , the limit random matrix @xmath92 has a nonsingular covariance structure , that is , for each nonzero vector @xmath93 , @xmath94 is a nonsingular matrix .",
    "conditions of theorem  [ thm:8 ] are similar to gallo s conditions @xcite for the asymptotic normality in the univariate case ; see also , @xcite , pp . 240243 .",
    "compared with theorems 2.3 and 2.4 of @xcite , stated for univariate case with mixing errors , we need not the requirement for entries of the true input @xmath7 to be totally bounded .    in @xcite , section  2 , we can find a discussion of importance of the asymptotic normality result for @xmath5 .",
    "it is claimed there that the formula for the asymptotic covariance structure of @xmath66 is computationally useless , but in case where the limit distribution is nonsingular , we can use the block - bootstrap techniques when constructing confidence intervals and testing hypotheses .",
    "however , in the case of normal errors @xmath44 , we can apply theorem  [ thm:8 ] to construct the asymptotic confidence ellipsoid , say , for @xmath95 , @xmath93 , @xmath96 .",
    "indeed , relations [ 3.2,3.3,3.4,3.5 ] show that the nonsingular matrix @xmath97 is a continuous function @xmath98 of unknown parameters @xmath30 , @xmath99 , and @xmath48 .",
    "( it is important here that now the components @xmath100 of @xmath87 are independent , and the covariance structure of each @xmath100 depends on @xmath48 and @xmath99 , not on some other limit characteristics of @xmath7 ; see lemma 6 . ) once we possess consistent estimators @xmath101 and @xmath102 of @xmath99 and @xmath48 , the matrix @xmath103 is a  consistent estimator for the covariance matrix @xmath104 .",
    "hereafter , a bar means averaging for rows @xmath37 , for example , @xmath105 .",
    "[ l:10 ] assume the conditions of theorem  [ thm:2 ] .",
    "define @xmath106,\\ ] ] @xmath107 then @xmath108    estimator is a multivariate analogue of the maximum likelihood estimator ( 1.53 ) in @xcite in the functional scalar eiv model .",
    "finally , for the case @xmath109 , based on lemma  [ l:10 ] and the relations @xmath110 we can construct the asymptotic confidence ellipsoid for the vector @xmath95 in a  standard way .    in a similar way",
    ", a confidence ellipsoid can be constructed for any finite set of linear combinations of @xmath30 entries with fixed known coefficients .",
    "we extended the result of gallo @xcite and proved the asymptotic normality of the tls estimator in a multivariate model @xmath111 .",
    "the normalized estimator converges in distribution to a random matrix with quite complicated covariance structure .",
    "if the error distribution is symmetric around the origin , then the latter covariance structure is nonsingular . for the case of normal errors , this makes it possible to construct the asymptotic confidence region for a vector @xmath95 , @xmath112 , where @xmath30 is the true value of @xmath1 .    in future papers",
    ", we will extend the result for the elementwise weighted tls estimator @xcite in the model @xmath111 , where some columns of the matrix @xmath0 $ ] may be observed without errors , and , in addition , the error covariance matrix may differ from row to row .",
    "\\(a ) for any @xmath113 and @xmath114 , the space @xmath115 is endowed with natural inner product @xmath116 and the frobenius norm . the matrix derivative @xmath117 of",
    "the functional is a linear functional on @xmath118 , which can be identified with certain matrix from @xmath72 based on the inner product .    using the rules of matrix calculus @xcite",
    ", we have for @xmath119 : @xmath120 collecting similar terms , we obtain : @xmath121 and @xmath122\\\\ & \\quad -\\mathrm{tr } \\bigl[x\\bigl(\\operatorname{\\mathrm i}_d+x^{\\operatorname{\\mathsf t}}x\\bigr)^{-1}\\bigl(x^{\\operatorname{\\mathsf t}}a - b\\bigr ) \\bigl(a^{\\operatorname{\\mathsf t}}x - b^{\\operatorname{\\mathsf t}}\\bigr ) \\bigl(\\operatorname{\\mathrm i}_d+x^{\\operatorname{\\mathsf t}}x\\bigr)^{-1}h^{\\operatorname{\\mathsf t}}\\bigr].\\end{aligned}\\ ] ]    using the inner product in @xmath115 , we get @xmath123 , where @xmath124 is the left - hand side of . in view of theorem  [ thm:2 ] and lemma [ l:3 ] , this implies the statement of corollary  [ cor:4 ] .",
    "\\(b ) now , we set @xmath125 where @xmath126 is a nonrandom vector , and , like in , @xmath127 { \\right})= \\sigma^2\\operatorname{\\mathrm i}_{n+d } , \\qquad \\operatorname{\\mathbf e}{\\left } [ \\begin{array}{l } \\tilde{a}\\\\ \\tilde{b } \\end{array }   { \\right}]=0.\\ ] ] then @xmath128 @xmath129 therefore ( see ) , @xmath130 this implies the statement of corollary  [ cor:4 ] .",
    "the derivative @xmath73 of the function is a linear operator in @xmath115 . for @xmath119",
    ", we have : @xmath131    as before , we set , and use relations , , and the relation@xmath132 .",
    "we obtain : @xmath133 this implies .",
    "the random elements @xmath134 , @xmath70 , in are independent and centered . we want to apply the lyapunov clt for the left - hand side of .",
    "\\(a ) all the second moments of @xmath135 converge to finite limits .",
    "for example , for the first component , we have @xmath136 and this has a finite limit due to assumption . here",
    "@xmath137 , and we use the inner product introduced in the proof of corollary  [ cor:4 ] .    for the fifth component , @xmath138 ^ 2 < \\infty,\\ ] ] because the fourth moments of @xmath43 are finite . here",
    "@xmath139 .    for mixed moments of the first and fifth components",
    ", we have @xmath140 and this , due to condition , converges toward @xmath141    other second moments can be considered in a similar way .",
    "\\(b ) the lyapunov condition holds for each component of .",
    "let @xmath80 be the quantity from assumptions , .",
    "then @xmath142 as @xmath63 by condition . for the fifth component , @xmath143 the latter expectation is finite by condition .",
    "the lyapunov condition for other components is considered similarly .",
    "\\(c ) parts ( a ) and ( b ) of the present proof imply by the lyapunov clt .",
    "under conditions and , all the five components of @xmath134 , which is given in , are uncorrelated ( e.g. , the cross - correlation like equals zero , and condition is not needed ) . as in proof of lemma  [ l:6 ] , the convergence still holds .",
    "the components @xmath88 of @xmath87 are independent because the components of @xmath134 are uncorrelated .",
    "our reasoning is typical for theory of generalized estimating equations , with specific feature that a matrix parameter rather than vector one is estimated .    by corollary  [ cor:4 ] , with probability tending to @xmath10",
    "we have @xmath144    now , we use taylor s formula around @xmath30 with the remainder in the lagrange form ; see @xcite , theorem 5.6.2 . denote @xmath145    then implies the relation @xmath146 here @xmath147 is a factor of the form @xmath148 relation holds with probability tending to @xmath10 because , due to theorem  [ thm:2 ] , @xmath149 ; expression is indeed @xmath147 because the derivative @xmath150 is quadratic in @xmath39 , @xmath40 ( cf . ) , and the averaged second moments of @xmath151 $ ] are assumed to be bounded .",
    "now , @xmath152 .",
    "next , by lemma  [ l:5 ] and condition , @xmath153 therefore , implies that @xmath154    now , we find the limit in distribution of @xmath155 .",
    "the summands in @xmath156 have zero expectation due to corollary  [ cor:4 ] . moreover ( see ) , @xmath157 @xmath158 here @xmath159 are the components of . by lemma  [ l:6 ] we have ( see )",
    "@xmath160    finally , relations , , and the nonsingularity of @xmath64 imply that @xmath161 , and by slutsky s lemma we get @xmath162 by condition the matrix @xmath64 is nonsingular .",
    "thus , the desired relation follows from .",
    "the convergence is justified as before , but using lemma  [ l:7 ] instead of lemma  [ l:6 ] .",
    "it suffices to show that @xmath163 is nonsingular for @xmath93 , @xmath96 .",
    "now , the components @xmath88 are independent",
    ". then ( see ) @xmath164",
    "by condition we have @xmath165 @xmath166 equality implies the first relation in because @xmath149 and @xmath167 , @xmath168 , @xmath169 ,"
  ],
  "abstract_text": [
    "<S> we consider a multivariate functional measurement error model . the errors in @xmath0 $ ] are uncorrelated , row - wise independent , and have equal ( unknown ) variances . </S>",
    "<S> we study the total least squares estimator of @xmath1 , which , in the case of normal errors , coincides with the maximum likelihood one . </S>",
    "<S> we give conditions for asymptotic normality of the estimator when the number of rows in @xmath2 is increasing . under mild assumptions , the covariance structure of the limit </S>",
    "<S> gaussian random matrix is nonsingular . for normal errors , </S>",
    "<S> the results can be used to construct an  asymptotic confidence interval for a linear functional of @xmath1 .    </S>",
    "<S> ./style / arxiv - vmsta.cfg    asymptotic normality , multivariate errors - in - variables model , total least squares 15a52,65f20,62e20,62s05,62f12,62h12 </S>"
  ]
}