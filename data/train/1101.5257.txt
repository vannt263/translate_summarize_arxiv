{
  "article_text": [
    "distributed storage system provides a scalable solution to the ever - increasing demand of reliable storage .",
    "the storage nodes are distributed in different geographical locations , and in case some disastrous event happened to one of them , the source data would remain intact .",
    "there are two common strategies for preventing data loss against storage node failures .",
    "the first one , employed by the current google file system  @xcite , is _",
    "data replication_. although replication - based scheme is easy to manage , it has the drawback of low storage efficiency .",
    "the second one is based on _ erasure coding _ , and is used in oceanstore  @xcite and total recall  @xcite for instance . with erasure coding , the storage network can be regarded as an erasure code which can correct any @xmath0 erasures ; a file is encoded into @xmath1 pieces of data , and from any @xmath2 of them the original file can be reconstructed .",
    "when a storage node fails , an obvious way to repair it is to rebuild the whole file from some other @xmath2 nodes , and then re - encode the data .",
    "the disadvantage of this method is that , when the file size is very large , excessive traffic is generated in the network .",
    "the bandwidth required in the repairing process seems to be wasted , because only a fraction of the downloaded data is stored in the new node after regeneration . by viewing the repair problem as a single - source multi - cast problem in network coding theory , dimakis _ et al .",
    "_ discovered a tradeoff between the amount of storage in each node and the bandwidth required in the repair process  @xcite .",
    "erasure codes for distributed storage system , aiming at minimizing the repair - bandwidth , is called _ regenerating code_.",
    "the construction of regenerating code is under active research .",
    "we refer the readers to  @xcite and the references therein for the application of network coding in distributed storage systems .",
    "most of the results in the literature on regenerating codes are for repairing a single storage node .",
    "however , there are several scenarios where _ multiple _ failures must be considered .",
    "firstly , in a system with high churn rate , the nodes may join and leave the system very frequently .",
    "when two or more nodes join the distributed storage system at the same time , the new nodes can exploit the opportunity of exchange data among themselves in the repair process .",
    "secondly , node repair may be done in batch . in systems like total recall ,",
    "a recovery is triggered when the fraction of available nodes is below a certain threshold , and the failed nodes are then repaired as a group . the new nodes which are going to be regenerated are called _ newcomers_. there are two ways in regenerating a group of newcomer : we may either repair them one by one , or repair them jointly with cooperation among the newcomers .",
    "it is shown in  @xcite that further reduction of repair - bandwidth is possible with cooperative repair .",
    "let the number of newcomers be  @xmath3 . in @xcite",
    "each newcomer is required to connect to all @xmath4 surviving storage nodes during the repair process , and in  @xcite , this requirement is relaxed such that different newcomers may have different number of connections .",
    "however , in both  @xcite and @xcite , only the storage systems which minimize storage per node are considered .    in this paper , an example of cooperatively regenerating multiple newcomers",
    "is described in section  [ sec : coop ] . in section  [ sec : mincut ] , we define the information flow graph for cooperative repair , and derive a lower bound on repair - bandwidth .",
    "this lower bound is applicable to _ functional repair _",
    ", where the content of a newcomer may not be the same as the failed node to be replaced , but the property that any @xmath2 nodes can reconstruct the original file is retained .",
    "the lower bound is function of the storage per node , and hence is an extension of the results in  @xcite",
    ". a more practical and easier - to - manage mode of operation is called _ exact repair _ , in which the regenerated node contains exactly the same encoded data as in the failed node . in section  [ sec : explicit ] , we give a family of explicit code constructions which meet the lower bound , and hence show that the construction is optimal .",
    "consider the following example taken from  @xcite .",
    "four data packets @xmath5 , @xmath6 , @xmath7 and @xmath8 , are distributed to four storage nodes .",
    "each of them stores two packets . the first one stores @xmath5 and @xmath6 , the second stores @xmath7 and @xmath8 .",
    "the third and fourth nodes are parity nodes .",
    "the third node contains two packets @xmath9 and @xmath10 , and the last node contains @xmath11 and @xmath12 . here",
    ", a packet is interpreted as an element in a finite field , and addition and multiplication are finite field operations .",
    "we can take @xmath13 as the underlying finite field in this example .",
    "any data collector , after downloading the packets from any two storage nodes , can reconstruct the four original packets by solving a system of linear equations .",
    "for example , if we download from the third and fourth nodes , we can recover @xmath5 and @xmath7 from packets @xmath9 and @xmath11 , and recover @xmath6 and @xmath8 from packets @xmath10 and @xmath12 .",
    "suppose that the first node fails .",
    "to repair the first node , we can download four packets from any other two nodes , from which we can recover the two required packets @xmath5 and  @xmath6 .",
    "for example , if we download the packets from the second and third nodes , we have @xmath7 , @xmath8 , @xmath9 and @xmath14",
    ". we can then recover @xmath5 by subtracting @xmath7 from @xmath9 , and @xmath6 by computing @xmath15 .",
    "it is illustrated in  @xcite that we can reduce the repair - bandwidth from four packets to three packets , by making three connections to the three remaining nodes , and downloading one packet from each of them ( fig .",
    "[ fig : ia ] ) .",
    "each of the three remaining nodes simply adds the two packets and sends the sum to the newcomer , who can then subtract off @xmath16 and obtain @xmath17 and @xmath18 , from which @xmath5 and @xmath6 can be solved .",
    "when two storage nodes fail simultaneously , the computational trick mentioned in the previous paragraph no longer works .",
    "suppose that the second and the fourth storage nodes fail at the same time .",
    "to repair both of them separately , each of the newcomers can download four packets from the remaining storage nodes , reconstruct packets @xmath5 , @xmath6 , @xmath7 and @xmath8 , and re - encode the desired packets ( fig .",
    "[ fig : sr ] ) .",
    "this is the best we can do with separate repair . using the result in  @xcite , it can be shown that any one - by - one repair process with repair - bandwidth strictly less than four packets per newcomer is infeasible .",
    "if the two newcomers can exchange data during the regeneration process , the total repair - bandwidth can indeed be reduced from eight packets to six packets ( fig .",
    "[ fig : cr ] ) .",
    "the two newcomers first make an agreement that one of them downloads the packets with subscript 1 , and the other one downloads the packets with subscript  2 .",
    "( they can compare , for instance , their serial numbers in order to determine who downloads the packets with smaller subscript . )",
    "the first newcomer gets @xmath5 and @xmath9 from node 1 and 3 respectively , while the second newcomer gets @xmath6 and @xmath10 from node 1 and 3 respectively .",
    "the first newcomer then computes @xmath7 and @xmath11 by taking the difference and the sum of the two inputs .",
    "the packet @xmath7 is stored in the first newcomer and @xmath11 is sent to the second newcomer .",
    "similarly , the second newcomer computes @xmath8 and @xmath12 , stores @xmath12 in memory and sends @xmath8 to the first newcomer .",
    "only six packet transmissions are required in this joint regeneration process .",
    "we formally define the cooperative repair problem as follows .",
    "there are two kinds of entities in a distributed storage system , _ storage nodes _ and _ data collectors _ , and two kinds of operations , _ file reconstruction _ and",
    "_ node repair_. a file of size @xmath19 units is encoded and distributed among the @xmath1 storage nodes , each of them stores @xmath20 units of data .",
    "the file can be reconstructed by a data collector connecting to any @xmath2 storage nodes . upon the failure of @xmath3 nodes ,",
    "a two - phase repair process is triggered . in the first phase ,",
    "each of the @xmath3 newcomers connects to @xmath21 remaining storage nodes , and download @xmath22 units of data from each of them . after processing the data they have downloaded , the @xmath3 newcomers exchange some data among themselves , by sending @xmath23 units of data to each of the other @xmath24 newcomers",
    "each newcomer downloads @xmath25 units of data in the first phase and @xmath26 units of data in the second phase .",
    "the repair - bandwidth per node is thus @xmath27 .    in the remaining of this paper",
    ", we will assume that @xmath28 .",
    "we construct an _ information flow graph _ as follows .",
    "there are three types of vertices in the information flow graph : one for the source data , one for the storage nodes and one for data collectors .",
    "the vertices are divided into stages .",
    "we proceed from one stage to the next stage after a repair process is completed .",
    "[ fig : flow1 ] ) .",
    "there is one single vertex , called the _ source vertex _ , in stage @xmath29 , representing the original data file .",
    "the @xmath1 storage nodes are represented by @xmath1 vertices in stage  0 , called @xmath30 , for @xmath31 . the source vertex is connected to each vertex in stage 0 by a directed edge with capacity  @xmath20 . for @xmath32 ,",
    "let @xmath33 be the set of @xmath3 storage nodes which fail in stage @xmath34 , and are regenerated in stage  @xmath35 .",
    "the set @xmath33 is a subset of @xmath36 with cardinality  @xmath3 . for each storage node @xmath37 in @xmath33",
    ", we construct three vertices in stage @xmath35 : @xmath38 , @xmath39 and @xmath40 .",
    "vertex @xmath38 has @xmath21 incoming edges with capacity @xmath22 , emanated from @xmath21 `` out '' nodes in previous stages .",
    "we join vertex @xmath38 and @xmath39 with a directed edge of infinite capacity . for @xmath41 , @xmath42",
    ", there is a directed edge from @xmath38 to @xmath43 with capacity @xmath23 .",
    "newcomer @xmath37 stores @xmath20 units of data , and this is represented by a directed edge from @xmath39 to @xmath40 with capacity  @xmath20 .    for each data collector , we add a vertex , called @xmath44 , in the information flow graph .",
    "it is connected to @xmath2 `` out '' nodes with distinct indices , but not necessarily from the same stage , by @xmath2 infinite - capacity edges .",
    "we call an information flow graph constructed in this way @xmath45 , or simply @xmath46 if the parameters are clear from the context . the number of stages is potentially unlimited .    a _ cut _ in an information flow graph is a partition of the set of vertices , @xmath47 , such that the source vertex is in @xmath48 and a designated data collector is in @xmath49 .",
    "we associate with each cut a value , called the _ capacity _ , defined as the sum of the capacities of the directed edges from vertices in @xmath48 to vertices in @xmath49 .",
    "an example is shown in fig .",
    "[ fig : cut ] . the max - flow - min - cut bound in network coding for single - source multi - cast network states that if the minimum cut capacities between data collectors and the source is at no larger than @xmath50 , then the amount of data we can send to each data collector is no more than @xmath50  @xcite .",
    "suppose that @xmath28 .",
    "the minimum cut of an information flow graph @xmath46 is less than or equal to @xmath51 where @xmath52 is any @xmath2-tuple of integers satisfying @xmath53 and @xmath54 for all  @xmath55 .",
    "[ thm : mincut ]    by relabeling the nodes if necessary , suppose that a data collector @xmath44 connects to storage node 1 to node  @xmath2 .",
    "let @xmath56 be the stages in which nodes 1 to @xmath2 are most recently repaired , where @xmath57 is an integer .",
    "we note that @xmath58 is contained in the union of @xmath59 , @xmath60 .",
    "for @xmath61 , let @xmath62 the physical meaning of @xmath63 is that the storage nodes with indices in @xmath63 are repaired in stage @xmath64 and remain intact until the data collector @xmath44 shows up .",
    "the index sets @xmath63 s are disjoint and their union is equal to @xmath58",
    ". we let @xmath65 to be the cardinality of @xmath63 .",
    "obviously we have @xmath66 , @xmath67 for all  @xmath55 , and @xmath68 .",
    "for @xmath61 , the @xmath65 `` out '' nodes in stage @xmath64 which are connected directly to @xmath44 must be in @xmath49 , otherwise , there would be an infinite - capacity edge from @xmath48 to @xmath49 . in stage @xmath64",
    ", we consider two different ways to construct a cut .",
    "we either put all `` in '' and `` mid '' nodes associated to the storage nodes in @xmath63 in @xmath49 , or put all of them in @xmath48 . in fig .",
    "[ fig : cut2 ] , we graphically illustrate the two different cuttings .",
    "the shaded vertices are in @xmath49 and the edges from @xmath48 to @xmath49 are shown .",
    "each `` in '' node in the first cut may connect to as small as @xmath69 `` out '' nodes in @xmath48 in previous stages .",
    "the sum of edge capacities from @xmath48 to @xmath49 can be as small as @xmath70 . in the second kind of cut ,",
    "the sum of edge capacities from @xmath48 to @xmath49 is @xmath71 . after taking the minimum of these two cut values",
    ", we get @xmath72 we obtain the expression in   by summing   over @xmath61 .    a cut described in the proof of theorem  [ thm : mincut ] is called a cut of type @xmath73 .",
    "we illustrate theorem  [ thm : mincut ] by the example in section  [ sec : coop ] .",
    "the parameters are @xmath74 , @xmath75 , @xmath76 , and @xmath77 .",
    "the are two pairs of integers @xmath78 , namely @xmath79 and @xmath80 , which satisfy the condition in theorem  [ thm : mincut ] .",
    "the capacity of minimum cut , by theorem  [ thm : mincut ] , is no more than @xmath81 and @xmath82 .",
    "the first cut imposes the upper bound @xmath83 on the file size  @xmath19 , which implies that @xmath84 .",
    "the second cut imposes another constraint on @xmath19 , @xmath85 from which we can deduce that @xmath86 . after summing @xmath84 and @xmath86",
    ", we obtain @xmath87 .",
    "the minimum possible repair - bandwidth @xmath88 matched by the regenerating code presented in section  [ sec : coop ] .",
    "the regenerating code in section  [ sec : coop ] is therefore optimal .",
    ", @xmath89 , @xmath90 , @xmath91),width=288 ]    we can formulate the repair - bandwidth minimization problem as follows . given the storage per node , @xmath20 ,",
    "we want to minimize the objective function @xmath92 over all non - negative @xmath22 and @xmath23 subject to the constraints that the file size @xmath19 is no more than the values in  , for all legitimate @xmath93 .",
    "it can be shown that the minimization problem can be reduced to a linear program , and hence can be effectively solved .",
    "we let the resulting optimal value be denoted by @xmath94 .",
    "this is a lower bound on repair - bandwidth for a given value of @xmath20 .    in fig .",
    "[ fig : tradeoff1 ] , we illustrate the lower bound @xmath95 for @xmath96 , @xmath89 , @xmath90 and @xmath91 . for comparison , we plot the storage - repair - bandwidth tradeoff for non - cooperative one - by - one repair in fig .",
    "[ fig : tradeoff1 ] . from",
    "* theorem 1 ) , the smallest repair - bandwidth of a non - cooperative minimum - storage regenerating code is given by the formula @xmath97 , which is equal to 84 in this example .",
    "it can be shown that @xmath98 . in the next section ,",
    "we give a construction of cooperative regenerating code which meets the lower bound @xmath99 when @xmath100 .",
    "exact repair has the advantage that the encoding vectors of the newcomers remain the same .",
    "this helps in reducing maintenance overhead . for non - cooperative and one - by - one repair ,",
    "there are several exact constructions of regenerating code available in the literature , for example the constructions in  @xcite and  @xcite . in this section",
    ", we construct a family of regenerating codes for cooperative repair with parameters @xmath101 , which contains the example given in section  [ sec : coop ] as special case .",
    "the recipe of this construction needs an maximal - distance separable ( mds ) code of length  @xmath1 and dimension  @xmath2 .",
    "given @xmath1 , let @xmath102 be the smallest prime power larger than or equal to  @xmath1 .",
    "we use the reed - solomon ( rs ) code over @xmath103 generated by the following generator matrix @xmath104 where @xmath105 , @xmath106 are @xmath1 distinct elements in @xmath103 .",
    "let @xmath107 be the @xmath55th column of  @xmath108 .",
    "given @xmath2 message symbols in @xmath103 , we put them in a row vector @xmath109 $ ] .",
    "( the superscript `` @xmath110 '' is the transpose operator . )",
    "we encode @xmath111 into the codeword @xmath112 .",
    "the mds property of rs code follows from the fact that every @xmath113 submatrix of @xmath108 is a non - singular vandermonde matrix .",
    "we apply the technique called `` striping '' from coding for disk arrays .",
    "the whole file of size @xmath19 is divided into many stripes , or chunks , and each chunk of data is encoded and treated in the same way . in the following , we will only describe the operations on each stripe of data .",
    "we divide a stripe of data into @xmath114 packets , each of them is considered as an element in @xmath103 .",
    "the @xmath114 packets are laid out in an @xmath115 matrix @xmath116 , called the _",
    "message matrix_. to set up the distributed storage system , we first encode the message matrix @xmath116 into @xmath117 , which is an @xmath118 matrix . for @xmath119 , node @xmath120 stores the @xmath3 packets in the @xmath120th column of @xmath117 .",
    "let the @xmath3 rows of @xmath116 be denoted by @xmath121 , @xmath122 .",
    "the packets stored in node @xmath120 are @xmath123 , for @xmath124 .",
    "a data collector downloads from @xmath2 storage nodes , say nodes @xmath125 , @xmath126 .",
    "the @xmath114 received packets are arranged in an @xmath127 matrix .",
    "the @xmath128-entry of this matrix is @xmath129 .",
    "this matrix can be factorized as @xmath130 $ ] .",
    "we can reconstruct the original file by inverting the vandermonde matrix @xmath131 $ ] .",
    "suppose that nodes @xmath132 , @xmath133 fail .",
    "the @xmath3 newcomers first coordinate among themselves , and agree upon an order of the newcomers , say by their serial numbers . for the ease of notation ,",
    "suppose that newcomer @xmath134 is the @xmath120th newcomer , for @xmath135 .",
    "the @xmath120th newcomer @xmath134 connects to any other @xmath2 remaining storage nodes , say @xmath136 , and downloads the packets encoded from @xmath137 , namely , @xmath138 , @xmath139 .",
    "( recall that we assume @xmath140 in this construction . )",
    "since @xmath141 $ ] is non - singular , newcomer @xmath134 can recover the message vector @xmath137 after the first phase . in the second phase , newcomer @xmath134 computes @xmath142 for @xmath143 , and sends the packet @xmath142 to newcomer  @xmath144 , @xmath145 .",
    "a total of @xmath24 packets are sent from each newcomer in the second phase .",
    "after the exchange of packets , newcomer @xmath134 then has the @xmath3 required packets @xmath146 , for @xmath124 .",
    "the repair - bandwidth per each newcomer is @xmath147 .    in this construction",
    ", we can pick the smallest prime power @xmath102 larger than or equal to @xmath1 as the size of the finite field .",
    "if the number of storage nodes @xmath1 increases , the finite field size increases linearly with  @xmath1 .",
    "the cooperative regenerating code described above is optimal , in the sense that if @xmath148 , @xmath140 , and each node stores @xmath149 packets , the minimal repair - bandwidth per each failed node is equal to @xmath150 .",
    "[ thm : mbsr ]    we use the notation as in theorem  [ thm : mincut ] .",
    "the capacity of a cut of type @xmath151 , as shown in  , is an upper bound on @xmath114 .",
    "if any summand @xmath152 in   is strictly less than @xmath153 for any @xmath55 , then the value in   is strictly less than @xmath154 .",
    "this would violate the fact that @xmath114 is upper bounded by  .",
    "hence we have @xmath155 for any cut associated with @xmath151 and any @xmath55 .",
    "_ case 1 : @xmath156_. from a cut of type @xmath157 , we have @xmath158 from . from another cut of type @xmath159 , from again , we obtain the condition @xmath160 which implies that @xmath84 .",
    "we then add @xmath161 to , and get @xmath162 .",
    "_ case 2 : @xmath163_. consider the two cuts associated with @xmath73 equal to @xmath164 and @xmath165 .",
    "we obtain the following two inequalities from  , @xmath166 we multiply both sides of   by @xmath167 , and multiply both sides of   by @xmath2 . after adding the two resulting inequalities , we get @xmath162 .      the regenerating code constructed in this section has the advantage that a storage node participating in a regeneration process is required to read and exactly the same amount of data to be sent out , without any arithmetical operations .",
    "this is called the _ uncoded repair _ property  @xcite .",
    "\\(ii ) one - by - one repair utilizing the newly regenerated node as a helper .",
    "the average repair - bandwidth per newcomer is @xmath171 the first term in the parenthesis is the repair - bandwidth of the first newcomer , which downloads from the four surviving nodes , the second term is the repair - bandwidth of the second newcomer , who connects to the four surviving nodes and the newly regenerated newcomer , and so on .",
    "\\(iii ) full cooperation among the three newcomers .",
    "the repair - bandwidth per newcomer can be reduced to  42 using the regenerating code given in this section .",
    "we thus see that newcomer cooperation is able to reduce the repair - bandwidth of a distributed storage system significantly .",
    ", `` oceanstore : an architecture for global - scale persistent storage , '' in _ proc .",
    "9th int . conf . on architectural support for programming languages and operating systems ( asplos )",
    "_ , cambridge , ma , nov .",
    "2000 , pp .",
    "190201 .",
    "r.  bhagwan , k.  tati , y.  cheng , s.  savage , and g.  voelker , `` total recall : system support for automated availability management , '' in _ proc . of the 1st conf . on networked systems",
    "design and implementation _ , san francisco , mar . 2004 .",
    "a.  g. dimakis , p.  b. godfrey , y.  wu , m.  j. wainwright , and k.  ramchandran , `` network coding for distributed storage system , '' in _ proc .",
    "conf . on computer commun .",
    "( infocom 07 ) _ , anchorage , alaska , may 2007 .      y.  hu , y.  xu , x.  wang , c.  zhan , and p.  li , `` cooperative recovery of distributed storage systems from multiple losses with network coding , '' _ ieee j. on selected areas in commun .",
    "_ , vol .  28 , no .  2 ,",
    "268275 , feb ."
  ],
  "abstract_text": [
    "<S> when there are multiple node failures in a distributed storage system , regenerating the failed storage nodes individually in a one - by - one manner is suboptimal as far as repair - bandwidth minimization is concerned . </S>",
    "<S> if data exchange among the newcomers is enabled , we can get a better tradeoff between repair bandwidth and the storage per node . </S>",
    "<S> an explicit and optimal construction of cooperative regenerating code is illustrated .    </S>",
    "<S> distributed storage , repair bandwidth , regenerating codes , erasure codes , network coding . </S>"
  ]
}