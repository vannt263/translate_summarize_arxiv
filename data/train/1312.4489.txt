{
  "article_text": [
    "optimization problems are widespread in real life decision making situations .",
    "however , data perturbations as well as uncertainty in at least part of the data are very difficult to avoid in practice . therefore , in most cases we have to deal with the reality that some aspects of the data of the optimization problem are uncertain .",
    "this uncertainty is caused by many sources such as forecasting , or approximations in models , or data approximation , or noise in measurements . in order to handle optimization problems under uncertainty",
    ", several techniques have been proposed .",
    "the most common , widely - known approaches are    * * sensitivity analysis : * typically , the influence of data uncertainty is initially ignored , and then the obtained solution is justified based on the data perturbations @xcite . * * chance constrained programming : * we use some stochastic models of uncertain data to replace the deterministic constraints by their probabilistic counterparts @xcite .",
    "it is a natural way of converting the uncertain optimization problem into a deterministic one .",
    "however , most of the time the result is a computationally intractable problem @xcite . * * stochastic programming : * the goal is to find a solution that is feasible for all ( or almost all ) possible instances of the data and to optimize the expectation of some function of the decisions and the random variables @xcite . * * robust optimization : * robust optimization is the method that is most closely related to our approach . generally speaking , robust optimization can be applied to any optimization problem where the uncertain data can be separated from the problem s structure .",
    "this method is applicable to convex optimization problems including semidefinite programming @xcite .",
    "our focus in this paper is on uncertain linear programming problems .",
    "uncertainty in the data means that the exact values of the data are not known , at the time when the solution has to be determined . in robust optimization framework ,",
    "uncertainty in the data is described through _",
    "uncertainty sets _ , which contain all ( or most of ) possible values that may be realized for the uncertain parameters .",
    "generally speaking , the distinction between robust optimization and stochastic programming is that robust optimization does not require the specification of the exact distribution .",
    "stochastic programming performs well when the distributions of the uncertainties are exactly known , and robust optimization is efficient when there is little information about those distributions .",
    "recently , a new method has been proposed , called _ distributionally robust _ optimization , that tries to cover the gap between robust optimization and stochastic programming @xcite . in this approach ,",
    "one seeks a solution that is feasible for the worst - case probability distribution in a set of possible distributions .",
    "since the interest in robust formulations was revived in the 1990s , many researchers have introduced new formulations for robust optimization framework in linear programming and general convex programming @xcite .",
    "ben - tal and nemirovski @xcite provided some of the first formulations for robust lp with detailed mathematical analysis .",
    "bertsimas and sim @xcite proposed an approach that offers control on the degree of conservatism for every constraint as well as the objective function .",
    "bertsimas et al .",
    "@xcite characterize the robust counterpart of an lp problem with uncertainty set described by an arbitrary norm . by choosing appropriate norms",
    ", they recover the formulations proposed in the above papers @xcite .",
    "the goal of classical robust optimization is to find a solution that is capable to cope best of all with @xmath0 realizations of the data from a given ( usually bounded ) uncertainty set @xcite . by the classical definition of robustness @xcite , a _ robust optimal solution _ is the solution of the following problem : @xmath1 where @xmath2 , @xmath3 , and @xmath4 are given uncertainty sets for @xmath5 , @xmath6 , and @xmath7 , respectively . throughout this paper",
    ", we refer to the formulation of as _ classical robust formulation_.      classical robust optimization is a powerful method to deal with optimization problems with uncertain data , however , we can raise some valid criticisms .",
    "one of the assumptions for robust optimization is that the uncertainty set must be precisely specified before solving the problem .",
    "even if the uncertainty is only in the rhs , expecting the decision maker ( dm ) to construct accurately an ellipsoid or even a hypercube for uncertainty set may not always be reasonable .",
    "another main criticism to classical robust optimization is that satisfying all of the constraints , if not make the problem infeasible , may lead to an objective value very far from the optimal value of the nominal problem .",
    "this problem is more critical for large deviations .",
    "as an example , @xcite considered some of the problems in the netlib library ( under reasonable assumptions on uncertainty of certain entries ) and showed that classical robust counterparts of most of the problems in netlib become infeasible for a small perturbation .",
    "moreover , in many other problems , objective value of the classical robust optimal solution is very low and may be unsatisfactory for the decision maker .",
    "several modifications of classical robust optimization have been introduced to deal with this issue .",
    "one , for example , is _",
    "globalized robust conterparts _ introduced in section @xmath8 of @xcite .",
    "the idea is to consider some constraints as  soft \" whose violation can be tolerated to some degree . in this method , we take care of what happens when data leaves the nominal uncertainty set . in other words , we have  controlled deterioration \" of the constraint .",
    "these modified approaches have more flexibility than the classical robust methodology , but we have the problem that the modified robust counterpart of uncertain problems may become computationally intractable .",
    "although the modified robust optimization framework rectifies this drawback to some extent , it intensifies the first criticism by putting more pressure on the dm to specify deterministic uncertainty sets before solving the problem .",
    "another criticism of the classical robust optimization is that it gives the same  weight \" to all the constraints . in practice",
    ", this is not the case as some constraints are more important for the dm .",
    "there are some options in classical robust optimization like changing the uncertainty set which again intensifies the first criticism .",
    "we will see that our approach can alleviate this difficulty .",
    "we present a framework which allows a fine - tuning of the classical tradeoff between robustness and conservativeness and engages dm continuously and in a more effective way throughout the optimization process . under a suitable theoretical modelling setup ,",
    "we prove that the classical robust optimization approach is a special case of our framework .",
    "we demonstrate that it is possible to efficiently perform optimization under this framework and finally , we illustrate the use of our methods numerically .",
    "one of the main contributions of this paper is the development of cutting - plane algorithms for robust optimization using the notion of weighted analytic centers in a small dimensional weight - space .",
    "we also design algorithms in the slack variable space as a theoretical stepping stone towards the more applicable weight - space cutting - plane algorithms .",
    "ultimately , we are proposing that our approach be used in practice with a small number ( say somewhere in the order of 1 to 20 ) of _ driving factors _ that really matter to the dm .",
    "these driving factors are independent of the number of variables and constraints , and determine the dimension of the weight space ( for interaction with the dm ) . working in a low dimensional weight - space not only simplifies the interaction for the dm , but also makes the cutting - plane algorithm more efficient .    the notion of moving across a weight space has been widely used in the area of multi - criteria decision making : when we have several competing objective values to optimize , a natural approach is to optimize a weighted sum of them @xcite , @xcite .",
    "authors in @xcite presented an algorithm for evaluating and ranking items with multiple attributes .",
    "@xcite is related to our work as the proposed algorithm is a cutting - plane one .",
    "however , our algorithm uses the concept of weighted analytic center which is completely different .",
    "authors in @xcite proposed a family of models ( denoted my mcrow ) for multi - expert multi - criteria decision making .",
    "their work is close to ours as they derived compact formulations of the mcrow model by assuming some structure for the weight region , such as polyhedral or conic descriptions .",
    "our work has fundamental differences with @xcite : cutting - plane algorithms in the weight - space find a weight vector @xmath9 in a fixed weight region ( the unit simplex ) such that the weighted analytic center of @xmath9 , say @xmath10 , is the desired solution for the dm .",
    "the algorithms we design in this paper make it possible to implement the ideas we mentioned above to help overcome some of the difficulties for robust optimization to reach a broader , practicing user base .",
    "for some further details and related discussion , also see moazeni @xcite and karimi @xcite .",
    "in section [ utility ] , we explain our approach , and also introduce the notion of weighted analytic centers . in section [ alg ] ,",
    "we design the cutting - plane algorithms , and explain some practical uses of our approach . in section [ convex ] , we prove that , under a theoretical framework due to bertsimas and sim @xcite , our approach is as least as strong as the classical robust optimization approach . some preliminary computational results are presented in section [ num ] . in section",
    "[ con ] , we briefly talk about the extension of the approach to semidefinite programming and quasi - concave utility functions , and then conclude the paper .      before introducing our approach in the next section ,",
    "let us first explain some of the assumptions and notations we are going to use .",
    "much of the prior work on robust linear programming addresses the uncertainty through the coefficient matrix .",
    "bertsimas and sim @xcite considered linear programming problems in which all data except the right - hand - side ( rhs ) vector is uncertain . in @xcite , it is assumed that the uncertainty affects the coefficient matrix and the rhs vector .",
    "some papers deal with uncertainty only in the coefficient matrix @xcite .",
    "optimization problems in which all of the data in the objective function , rhs vector and the coefficient matrix are subject to uncertainty , have been considered in @xcite .",
    "as we explain in section [ utility ] , the nominal data and a rough approximation of the uncertainty set are enough for our approach .",
    "however , the structure of uncertainty region is useful for the probability analysis . in this paper",
    ", we deal with the general setup that any part of the data @xmath11 may be subject to uncertainty ; however , we handle the uncertainty in @xmath12 by pushing it onto uncertainty in @xmath13 .",
    "moreover , in at least some applications , the amount of uncertainty in @xmath12 is limited whereas the uncertainty in the rhs and the objective function vectors may be very significant .",
    "some of the supporting arguments for this viewpoint are :    1 .",
    "instead of specifying uncertainty for each local variable , we can handle the whole uncertainty with some global variables .",
    "these global variables can be , for example , the whole budget , human resources , availability of certain critical raw materials , government quotas , etc .",
    "it may be easier for the dm to specify the uncertainty set for these global variables .",
    "then , we can approximate the uncertainty in the coefficient matrix with the uncertainty in the rhs and the objective function . in other words",
    ", we may fix the coefficient matrix on one of the samples from the uncertainty set and then handle the uncertainty by introducing uncertainty to the rhs vector as in @xcite .",
    "a certain coefficient matrix is typical for many real world problems . in many applications of planning and network design problems such as scheduling , manufacturing , electric utilities , telecommunications , inventory management and transportation",
    ", uncertainty might only affect costs ( coefficients of the objective function ) and demands ( the rhs vector)@xcite .",
    "* transportation systems : * in some problems , the nodes and the arcs are fixed .",
    "however , the cost associated to each arc is not known precisely .",
    "* traffic assignment problems : * we can assume that the drivers have perfect information about the arcs and nodes .",
    "however , their route choice behavior makes the travelling time uncertain .",
    "* distribution systems : * in some applications , the locations of warehouses and their capacities ( in inventory planning and distribution problems ) are well - known and fixed for the dm",
    ". however , the size of orders and the demand rate of an item could translate to an uncertain rhs vector .",
    "holding costs , set up costs and shortage costs , which affect the optimal inventory cost , are also typically uncertain .",
    "these affect at least the objective function .",
    "* medical / health applications : * in these applications ( see for instance , @xcite ) the dm may be a group of people ( including medical doctors and a patient who are more comfortable with a few , say 4 - 20 , driving factors which may be more easily handled by the mathematical model , if these factors could be represented as uncertain rhs values .",
    "+ in the aforementioned applications , well - understood existing resources , reliable structures ( well - established street and road networks , warehouses , and machines which are not going to change ) , and logical components of the formulation are translated into a certain coefficient matrix .",
    "the data in the objective function and the rhs vector are usually estimated by statistical techniques by the dm , or affected by uncertain elements such as institutional , social , or economical market conditions .",
    "therefore , determining these coefficients with precision is often difficult or practically impossible .",
    "hence , considering uncertainty in the objective function and the rhs vector seems to be very applicable , and motivates us to consider such formulation in lp problems separately .",
    "3 .   in our approach , we need the uncertainty sets for probabilistic analysis . uncertainty in the rhs and",
    "the objective value is easier to handle mathematically .    by the above explanation , for the rest of this paper",
    ", we assume that the uncertainty in @xmath12 has already been pushed into @xmath13 ; thus we may fix the coefficient matrix @xmath12 .",
    "it is clear that changing each entry of @xmath12 could change the geometry of the feasible region . on the other hand , neither we nor the dm know how each coefficient may affect the optimal solution before starting to solve the problem .",
    "therefore , to fix matrix @xmath12 , we rely on the nominal values ( expected values ) of the coefficients estimated by a method agreed by the dm .",
    "an uncertain linear programming problem with deterministic coefficient matrix @xmath14 , is of the form : @xmath15 where @xmath16 and @xmath17 are an @xmath18-vector and an @xmath19-vector respectively , whose entries are subject to uncertainty . @xmath2 and @xmath4 are called _ uncertainty sets_. in this paper , we deal with problem and suppose that the data uncertainty affects only the elements of the vectors @xmath7 and @xmath5 . in our probabilistic analysis ,",
    "we assume entries of @xmath5 and @xmath7 are random variables with unknown distributions , as it is impractical to assume that the exact distribution is explicitly known . by classical view of robust optimization , _ classical robust counterpart _ of problem is defined in with a certain @xmath12 . feasible / optimal solutions of problem are called _ classical robust feasible / classical robust optimal solutions _ of problem @xcite .",
    "let @xmath20 and @xmath21 be the expected value of @xmath22 and @xmath13 , respectively .",
    "the following lp program is the framework of our algorithms : @xmath23 here , without loss of generality , we impose the following restrictions on the problem ( for details , see @xcite ) : the matrix @xmath12 has full column rank , i.e. , @xmath24 .",
    "the set @xmath25 is bounded and has nonempty interior .    in this paper ,",
    "vectors and matrices are denoted , respectively , by lower and uppercase letters .",
    "the matrices @xmath26 and @xmath27 represent diagonal matrices , having the components of vectors @xmath28 and @xmath29 on their main diagonals , respectively .",
    "the letter @xmath30 and @xmath31 denote a vector of ones and a vector that is everywhere zero except at the @xmath32-th entry with the appropriate dimension , respectively .",
    "the rows of a matrix are shown by superscripts of the row , i.e. , @xmath33 is the @xmath32-th row of the matrix @xmath12 .",
    "the inner product of two vectors @xmath34 is shown both by @xmath35 and @xmath36 . for a matrix @xmath12",
    ", we show the range of @xmath12 with @xmath37 and the null space of @xmath12 with @xmath38 .",
    "in section [ intro ] , we introduced different methods for dealing with lp problems under uncertainty . for each method",
    ", we explained the drawbacks and practical difficulties . in this subsection",
    ", we introduce our new approach that helps us overcome some of these difficulties .",
    "let us focus on the robust optimization method that from many points of view is the strongest among the methods we introduced in section  [ intro ] .",
    "one of the main problems with robust optimization is that the uncertainty region must be specified before solving the problem .",
    "as we explained , in practice , even if the uncertainty is only in the rhs , expecting the dm to construct accurately an ellipsoid or a hypercube for uncertainty set may not be reasonable .",
    "the proposed method removes dm s anxiety about determining the uncertainty set precisely , and a nominal value of the data is enough .",
    "consider problem ; for any feasible point @xmath39 , we define the slack vector @xmath40 . in this paper",
    ", we prefer to work only with the slack variables .",
    "hence , we add a constraint that represents the objective function .",
    "this constraint is @xmath41 , where @xmath42 is a lower bound specified by the information from the dm .",
    "for example , if the dm decides that the objective value must not be below a certain value , we can put @xmath42 equal to that value . by the above definitions , lp problem is equivalent to the following problem : @xmath43 let us define @xmath44 as the set of all feasible slack vectors",
    ". then we can write as @xmath45 where @xmath46 .",
    "this @xmath47 , which we denote as utility function , is the simplest one that takes into account only maximizing the objective function .",
    "intuitively , we can cover a huge class of problems by using more complicated utility functions in . in this paper",
    ", we try to solve for a general utility function that models all the preferences of the dm .",
    "we do _ not _ have access to this utility function , however assume that , for a slack vector @xmath29 , we can ask the dm questions to extract some information about the function . in many applications ,",
    "robustness of a solution may be a monotone function of the slack variables ( this typically corresponds to quasi - concave utility function in our theoretical development ) ; however , this kind of property of the utility function is not as restrictive in our approach as it may seem since we can also have quasi - concave utility functions applied to any linear function of @xmath39 or @xmath29 , using our approach ( see subsection 3.4 ) .",
    "we can also use modelling techniques from goal programming ( see @xcite ) . assuming that @xmath47 is concave or quasi - concave",
    ", we retrieve the supergradient of @xmath47 at some points through a sequence of simple questions such as pairwise comparison questions ( see for instance @xcite ) .    in classical robust optimization approach , dm s preferences , expertise etc .",
    "are formulated / captured by the uncertainty regions ( this interaction takes place once ) . in our approach , we start with the axiom that the dm s preferences , expertise etc . can be approximated by a multivariate utility function ( however , we do not need to have this function , we do not need to reverse engineer it , we only assume its _ existence _ , which is quite typical in decision making situations and in interaction with the dm ) . instead of requiring the dm to provide the regions of uncertainty at once ,",
    "we interactively ask dm trade - off questions to extract enough information to construct approximate supergradients to the unknown utility function .",
    "this interactive approach has the additional benefit that in case the dm is inconsistent in his / her answers , since our approach is interactive and operates with very local information , we can provide the dm with a better chance of correcting mistakes as well as _ learning _ throughout the interactive process , what is possible within the given constraints and preferences .",
    "we can make a connection between the feasible slack vectors of an lp and the notion of weighted - analytic - centers .",
    "there are strong justifications for using weight space ( @xmath9-space ) instead of @xmath29-space that we will see when we design the algorithms . besides them , by using the notion of weighted center , we benefit from the differentiability . weight - space and",
    "weighted - analytic - centers approach embeds a ",
    "highly differentiable \" structure into the algorithms .",
    "such tools are extremely useful in both the theory and applications of optimization .",
    "in contrast , classical robust optimization and other competing techniques usually end up delivering a final solution where differentiability can not be expected ; this happens because their potential solutions located on the boundary of some of the structures defining the problem .      for every @xmath48 ,",
    "let @xmath49 be a closed convex subset of @xmath50 such that @xmath51 is bounded and has nonempty interior .",
    "+ let @xmath52 be a self - concordant barrier for @xmath49 , @xmath48 ( for a definition of self - concordant barrier functions see @xcite ) .",
    "for every @xmath53 , we define the @xmath9-center of @xmath54 as @xmath55 consider the special case when each @xmath49 is a closed half - space in @xmath50 .",
    "then the following result is well - known .",
    "[ w ] suppose for every @xmath48 , @xmath56 and @xmath57 are given such that : @xmath58 is bounded and @xmath59 is nonempty .",
    "also , for every @xmath48 define .",
    "then for every @xmath53 , there exists a unique @xmath9-center in the interior of @xmath54 , @xmath10 .",
    "conversely , for every @xmath60 , there exists some weight vector @xmath61 such that @xmath39 is the unique @xmath62-center of @xmath54 .",
    "define the following convex optimization problems : @xmath63 and @xmath64 where @xmath65 , @xmath66 , and @xmath67 . for every weight vector @xmath68 , the objective functions of the above problems are strictly convex on their domains .",
    "moreover , the objective function values tend to @xmath69 along any sequence of their interior points ( strictly feasible points ) , converging to a point on their respective boundary .",
    "the above problems have minimizers in the interior of their respective feasible regions . since the objective functions are strictly convex , the minimizers are unique .",
    "therefore , for every given @xmath68 , the above problems have unique solutions @xmath70 and @xmath71 .",
    "these solutions can be used to define many _ primal - dual weighted - central - paths _ as the solution set @xmath72 of the following system of equations and strict inequalities : @xmath73 where @xmath74 . when we set @xmath75 , @xmath76 , we obtain the usual primal - dual weighted - central - path .",
    "figure [ fig - pd ] illustrates some weighted central paths .",
    "as we mentioned above , we embed the objective function into matrix @xmath12 to work only with the slack variables . in view of ,",
    "let us define @xmath77 , \\ \\",
    "\\left [ \\begin{array}{c } b^{(0 ) } \\\\",
    "-v   \\end{array } \\right ] .\\ ] ] from now on , we may assume that @xmath65 also contains the last added constraint . as we embedded the objective function in @xmath12",
    ", we can put @xmath78 in , and the solutions of the following system form our desired weighted - analytic - center .",
    "@xmath79 for every given weight vector @xmath9 , @xmath80 is obtained uniquely and @xmath10 is called the _ weighted center _ of @xmath9 .",
    "we may also refer to @xmath80 as the weighted center of @xmath9 . for",
    "every given @xmath81 and @xmath82 , @xmath83 , that satisfy the above system , @xmath9 and @xmath84 are obtained uniquely .",
    "however , for a given @xmath81 , there are many weight vectors @xmath9 that give @xmath39 as the @xmath9-center of the corresponding polytope . without loss of generality ,",
    "we restrict ourselves to the weights on the unit simplex , i.e. , we consider weighted center @xmath85 corresponding to weight vectors @xmath86 such that @xmath87",
    ". a special case can be @xmath88 , where @xmath30 is the vector of all ones .",
    "we will show that this subset of weight vectors is enough to represent the feasible region .",
    "we call this simplex of weight vectors @xmath89 : @xmath90 we define the following notion for future reference :    [ centric ] a vector @xmath91 or @xmath82 is called _ centric _ if there exists @xmath39 such that @xmath92 satisfies for a weight vector @xmath93 where @xmath94 .",
    "let @xmath29 and @xmath28 be centric .",
    "first , we note that the simplex of the weight vectors can be divided into regions of constant @xmath28-vector ( @xmath95 ) and constant @xmath29-vector ( @xmath96 ) . by using lemma [ w1 ] ,",
    "if @xmath97 is the solution of system corresponding to the weight vector @xmath98 , and @xmath99 is any centric @xmath28-vector , then @xmath100 is the solution of system corresponding to the weight vector @xmath101 .",
    "this means that for every centric vector @xmath102 and any centric vector @xmath28 , @xmath103 is a weight vector in the simplex .    for every pair of centric vectors",
    "@xmath29 and @xmath28 , @xmath96 and @xmath95 are convex . to see this , let @xmath104 and @xmath92 be the weighted centers of @xmath105 and @xmath9 . then , it is easy to see that for every @xmath106 $ ] , @xmath107 is the weighted center of @xmath108 .",
    "different properties of @xmath96 and @xmath95 are studied in appendix [ appendix-1 ] , but the following simple examples make the geometry of @xmath96 and @xmath95 clearer .",
    "we present two examples with @xmath109 .",
    "[ wsy ] for the first example , let @xmath110^{\\top}$ ] and . by using ,",
    "the set of centric @xmath29-vectors is @xmath111^{\\top } : x\\in(0,1)\\}$ ] .",
    "the set of centric @xmath28-vectors is specified by solving @xmath112 and @xmath113 , while @xmath83 .",
    "we can see that in this example , as shown in figure [ fig2 ] , @xmath96s are parallel line segments while @xmath95s are line segments which all intersect at @xmath114^{\\top}$ ] .",
    "for the second example , let @xmath115^{\\top}$ ] and @xmath116^{\\top}$ ] .",
    "the set of @xmath96s and @xmath95s are shown in figure [ fig3 ] derived by solving . as can be seen ,",
    "this time @xmath95s are parallel line segments and @xmath96s are line segments which intersect at the point @xmath117^{\\top}$ ] .",
    "these examples show that the affine hulls of @xmath118 and @xmath119 might not intersect for two centric @xmath28-vectors @xmath120 and @xmath121 .",
    "this is also true for the affine hulls of @xmath122 and @xmath123 for two centric @xmath29-vectors @xmath124 and @xmath125 .",
    "[ wsy-2 ] for the second example , let @xmath126^{\\top}$ ] and @xmath127^{\\top}$ ] .",
    "the set of @xmath96s and @xmath95s are shown in figure [ fig - lastexample ] , derived by solving . in this",
    "example , none of @xmath95s , @xmath96s , or their affine hulls intersect in a single point .",
    "in this section , we develop some cutting - plane algorithms which find an optimal solution for the dm , using the facts we established in the previous sections . as we mentioned in section [ utility ] , we assume that the dm s preferences , knowledge , wisdom , expertise , etc . is modeled by a utility function ( as a function of the slack variables @xmath29 ) , i.e. , @xmath47 , and our problem is to maximize this utility function over the set of centric ( definition [ centric ] ) @xmath29-vectors @xmath128 .",
    "( of course , we do not assume to have access to this function @xmath129 , except through our limited interactions with the dm . )",
    "therefore , our problem becomes @xmath130 in the following , we denote an optimal solution of with @xmath131 . in many applications ,",
    "it is possible to capture choices with concave , quasi - concave , or nondecreasing utility functions .",
    "we are going to start with the assumption of concave @xmath47 .",
    "we see in subsection [ quasi - concave ] that the algorithm can easily be refined to be used for quasi - concave functions . here",
    ", we use the concept of supergradient we introduce shortly .",
    "supergradients ( subgradients for convex functions ) have been widely used before to design cutting - plane and ellipsoid algorithms .",
    "our goal is to use the concept to design cutting - plane algorithms .",
    "suppose we start the algorithm from a point @xmath132 with the corresponding @xmath29-vector @xmath133 .",
    "using the idea of supergradient , we can introduce cuts in the @xmath29-space or @xmath9-space to shrink the set of @xmath29-vectors or @xmath9-vectors , such that the shrunken space contains an optimal point . in the following subsections , we discuss these algorithms in @xmath29-space and @xmath9-space .",
    "our main algorithm is the one in the @xmath9-space , however , the @xmath29-space algorithm helps us understand the other better .",
    "as mentioned above , our algorithms are based on the notion of supergradient of a concave function .",
    "therefore , before stating the algorithms , we express a summary of the results we want to use .",
    "these properties are typically proven for convex functions in the literature @xcite , however we can translate all of them to concave functions .",
    "it is a well - known fact that for a concave function @xmath134 , any local maximizer is also a global maximizer .",
    "if a strictly concave function attains its global maximizer , it is unique .",
    "the following theorem is fundamental for developing our cutting - plane algorithms .",
    "[ t2 ] assume that @xmath134 is a concave function and let @xmath135",
    ". then there exists @xmath136 such that @xmath137 if @xmath138 is differentiable at @xmath139 , then @xmath140 is unique , and @xmath141 .",
    "the vector @xmath140 that satisfies is called the _ supergradient _ of @xmath138 at @xmath139 .",
    "the set of all supergradients of @xmath138 at @xmath142 is called the _ superdifferential _ of @xmath138 at @xmath139 , and is denoted @xmath143 . by theorem [ t2 ] , if @xmath138 is differentiable at @xmath139 , then @xmath144 .",
    "the following lemma about supergradient , which is a simple application of the chain rule , is also useful .",
    "[ t3 ] let @xmath134 be a concave function , and @xmath145 and @xmath146 be arbitrary matrices .",
    "then , @xmath147 is a concave function and we have : @xmath148      assume that we have a starting point @xmath149 and we can obtain a supergradient of @xmath129 at @xmath149 from the dm , e.g. @xmath150 , ( @xmath151 if @xmath129 is differentiable at @xmath149 ) . by using , for every @xmath29 ,",
    "@xmath152 this means that all optimal points are in the half - space @xmath153 .",
    "so , by adding this cut , we can shrink the @xmath29-space and guarantee that there exists an optimal solution in the shrunken part .",
    "we can translate this cut to a cut in the @xmath39-space by using : @xmath154 using this equation , we consider the cut as a new constraint of the original problem ; + .",
    "let us define @xmath155 and @xmath156 .",
    "we redefine @xmath54 by adding this new constraint and find the weighted center for a chosen weight vector @xmath157 .",
    "the step - by - step algorithm is as follows :    * @xmath27-space algorithm : *    * step 1 : set @xmath158 and find the @xmath159-centers @xmath160 with respect to @xmath161 . * step 2 : set @xmath162 , @xmath163 , @xmath164 , and @xmath165 .",
    "* step 3 : if @xmath166 satisfies the dm , return @xmath167 and * stop*. * step 4 : set @xmath168 .",
    "find @xmath169 , the supergradient of @xmath47 at @xmath170 .",
    "set @xmath171 ,   \\ \\ \\ \\ b^k=\\left[\\begin{array}{c } b^{k-1 } \\\\",
    "g_{k-1}^{\\top}a_{k-1}x^{k-1 } \\\\",
    "\\end{array}\\right ] ,   \\nonumber \\\\ & & \\mathcal f_k:=\\left\\ { x\\in\\mathbb r^{n } : \\ \\langle",
    "a_k^{(i)},x\\rangle \\leq b^{k}_i , \\forall i\\in\\{1,2,\\ldots , m+k\\}\\right\\}.\\end{aligned}\\ ] ] * step 5 : set @xmath172 for @xmath173 and @xmath174 for @xmath175 .",
    "find the @xmath176-center @xmath167 with respect to @xmath177 .",
    "return to step 3 .",
    "the logic behind step 5 is that we want to give smaller weights to the new constraints than the original ones ( however , our choices above are just examples ; implementers should make suitable , practical choices that are tailored to their specific application ) .",
    "a main problem with the algorithm is that the dimension of the weight - space is increased by one every time we add a constraint .",
    "we show that this problem is solved by our @xmath9-space algorithm in the following subsection .      in this subsection",
    ", we consider the cuts in the @xmath9-space .",
    "to do that , we first try a natural way of extending the algorithm in the @xmath29-space to the one in the @xmath9-space .",
    "we show that this extension only works for a limited subset of utility functions .",
    "then , we develop an algorithm applicable to all concave utility functions .    like the @xmath29-space",
    ", we try to use the supergradients of @xmath47 .",
    "let @xmath178 denote the utility function as a function of @xmath9 .",
    "from we have @xmath179 ; so , @xmath180 . if @xmath26 were constant for all weight vectors , @xmath181 would be a concave function , and we could use lemma [ t3 ] to find the supergradient at each point .",
    "the problem here is that @xmath26 is not necessarily the same for different weight vectors .",
    "assume that we confine ourselves to weight vectors in the simplex @xmath89 with the same @xmath28-vector ( @xmath95 ) .",
    "@xmath181 is a concave function on @xmath95 , so , we can define its supergradient . by lemma [ t3 ] , we conclude that @xmath182 for all @xmath183 .",
    "suppose we start at @xmath159 with the weighted center @xmath160 .",
    "let us define @xmath184 , where @xmath185 is a supergradient of @xmath47 at @xmath149 . then from we",
    "have , @xmath186    if we confine the weight - space to @xmath95 , by the same procedure used for @xmath29-space , we can introduce cuts in the @xmath9-space using .",
    "the problem is that we do not have a proper characterization of @xmath95 . on the other hand",
    ", @xmath178 may not be a concave function on the whole simplex .",
    "assume that @xmath131 is an optimal solution of , and @xmath187 is the set of weight vectors in the simplex with @xmath29-vector @xmath131 .",
    "it is easy to see that @xmath187 is convex .",
    "we also have the following lemma :    [ fl ] let @xmath188 be the weighted center corresponding to @xmath189 , @xmath131 be an optimal solution of , and @xmath190 be the supergradient of @xmath47 at @xmath191 .",
    "then , @xmath192 is in the half - space @xmath193 , where @xmath194 .",
    "we have @xmath195 the last inequality follows from the fact that @xmath131 is a maximizer and @xmath190 is a supergradient of @xmath47 at @xmath191 .",
    "the above lemma shows that using hyperplanes of the form @xmath196 , we can always keep a point from @xmath187 .",
    "now , using the fact that @xmath187 is convex and the above lemma , the question is : if we use a sequence of these hyperplanes , can we always keep a point from @xmath187 ?",
    "a simpler question is : we start with @xmath159 and shrink the simplex @xmath89 into the intersection of the half - space @xmath197 and the simplex , say @xmath198 .",
    "then we choose an arbitrary weight vector @xmath157 with weighted center @xmath199 from the shrunken space @xmath198 .",
    "if @xmath200 is a supergradient of @xmath47 at @xmath124 , then we shrink @xmath198 into the intersection of @xmath198 and the half - space @xmath201 , where @xmath202 , and call the last shrunken space @xmath203 .",
    "is it always true that a weight vector with @xmath29-vector @xmath131 exists in @xmath203 ? in the following , we show that this is true for some utility functions , but not true in general .",
    "we define a special set of functions that have good properties for cuts in the @xmath9-space , and the above algorithm works for them .",
    "[ ndas ] a function @xmath204 is called _ non - decreasing under affine scaling ( ndas ) _",
    "if for every @xmath205 we have :    1 .",
    "@xmath206 2 .   if for a single @xmath207 we have @xmath208 , then @xmath209 for all @xmath210 .    for every @xmath211",
    "the function @xmath212 is ndas .",
    "indeed , for every @xmath213 we have : @xmath214 and so we have @xmath215 .",
    "the second property is also easy to verify and the function is ndas .",
    "@xmath216 is also important due to its relation to a family of classical utility functions in mathematical economics : cobb - douglas production function which is defined as @xmath217 , where @xmath218 .",
    "usage of this function to simulate problems in economics goes back to 1920 s .",
    "maximization of @xmath219 is equivalent to the maximization of its logarithm which is equal to + @xmath220 .",
    "authors in @xcite considered the cobb - douglas utility function to present an algorithm for evaluating and ranking items with multiple attributes .",
    "@xcite is related to our work as the proposed algorithm is a cutting - plane one .",
    "@xcite also used the idea of weight - space as the utility function is the weighted sum of the attributes .",
    "however , our algorithm uses the concept of weighted analytic center which is different .",
    "now , we have the following proposition .    [ w - l3 ]",
    "assume that @xmath47 is a ndas concave function .",
    "let @xmath160 and @xmath199 be the weighted centers of @xmath159 and @xmath157 , and @xmath150 and @xmath200 be the supergradients of @xmath47 at @xmath149 and @xmath124 , respectively",
    ". then we have @xmath221 where @xmath222 and @xmath202 .",
    "see appendix [ appendix ] .    by proposition [ w - l3 ] ,",
    "using the first two hyperplanes , the intersection of the shrunken space and @xmath187 is not empty .",
    "now , we want to show that we can continue shrinking the space and have nonempty intersection with @xmath187 .",
    "[ ndas - thm ] assume that @xmath47 is a ndas concave function .",
    "let @xmath223 be the weighted centers of @xmath224 , @xmath225 , and @xmath226 be the supergradients of @xmath47 at @xmath227 .",
    "let us define @xmath228 where @xmath229 .",
    "assume we picked the points such that @xmath230 then we have @xmath231 where @xmath131 is an optimal solution of .",
    "see appendix [ appendix ] .",
    "proposition [ ndas - thm ] shows that the above - mentioned cutting - plane algorithm works for the ndas functions .",
    "it would be very helpful in designing a cutting - plane algorithm in the @xmath9-space if this proposition were true in general .",
    "however , this is not true for a general concave function . for a counter example , see example [ coe ] in appendix [ appendix ] . to be able to perform a cutting - plane algorithm in the @xmath9-space",
    ", we have to modify the definition of cutting hyperplanes . in the next two propositions ,",
    "we introduce a new set of cutting - planes .",
    "[ w - l5 ] for every point @xmath232 , there exists a hyperplane @xmath233 passing through it such that : 1- p contains all the points in @xmath234 , and 2- p cuts @xmath235 the same way as @xmath236 cuts it ; the intersections of p and @xmath236 with @xmath235 is the same , and the projections of their normals onto @xmath235 have the same direction",
    ".    see appendix [ appendix ] .",
    "[ w - l6 ] assume that we choose the points @xmath237 .",
    "the hyperplane @xmath233 passing through @xmath238 , with the normal vector @xmath239 , @xmath240 satisfies the following properties : 1- p contains all the points in @xmath122 , and 2- @xmath241 for every feasible maximizer of @xmath47 .",
    "see appendix [ appendix ] .    by proposition",
    "[ w - l6 ] , we can create a sequence of points and hyperplanes such that the corresponding half - spaces contain @xmath242 .",
    "the algorithm is as follows :    * @xmath89-space algorithm : *    * step 1 : set @xmath158 and find the @xmath159-centers @xmath160 with respect to @xmath161 . * step 2 : set @xmath162 , and @xmath243 . * step 3 : if @xmath166 satisfies the optimality condition , return @xmath167 and * stop*. * step 4 : find @xmath244 , the supergradient of @xmath47 at @xmath245 .",
    "find @xmath246 by solving the following equation @xmath247 * step 5 : set @xmath248 and @xmath249 . pick an arbitrary point @xmath250 from @xmath251 and find the @xmath250-center @xmath252 with respect to @xmath54 .",
    "set @xmath168 and return to step 3 .",
    "a clear advantage of this algorithm over the one in the @xmath29-space is that we do not have to increase the dimension of the @xmath9-space at each pass and subsequently we do not have to assign weights to the new added constraints .",
    "so , the above algorithm is straightforward to implement .",
    "we provided a discussion how to choose the next weight vector in the shrunken set in appendix [ tech ] .",
    "we can also use the properties of the weighted center we derived in appendix [ appendix-1 ] to improve the performance of the algorithm .",
    "we introduce two modified algorithms in appendix [ tech ] .",
    "introduction of cutting - plane algorithms goes back at least to 1960 s and one of the first appealing ones is the center of gravity version @xcite .",
    "the center of gravity algorithm has not been used in practice because computing the center of gravity , in general , is difficult .",
    "however , it is noteworthy due to its theoretical properties .",
    "for example , grnbaum @xcite proved that by using any cutting - plane through the center , more than 0.37 of the feasible set is cut out which guarantees a geometric convergence rate with a sizeable constant .",
    "many different types of centers have been proposed in the literature .",
    "a group of algorithms use the center of a specific localization set , which is updated at each step .",
    "one of them is the ellipsoid method @xcite where the localization set is represented by an ellipsoid containing an optimal solution .",
    "ellipsoid method can be related to our algorithm as we can use it to find the new weight vectors at each iteration .",
    "the cutting - plane method which is most relevant to our algorithm is the analytic center one , see @xcite for a survey . in this method ,",
    "the new point at each iteration is an approximate analytic center of the remaining polytope .",
    "the complexity of such algorithms has been widely studied in the literature .",
    "nesterov @xcite proved the @xmath253-accuracy bound of @xmath254 when the objective function is lipschitz continuous with constant @xmath255 , and the optimal set lies in a ball of diameter @xmath256 .",
    "goffin , luo , and ye @xcite considered the feasibility version of the problem and derived an upper bound of @xmath257 calls to the cutting - plane oracle .",
    "another family of cutting - plane algorithms are based on _ volumetric barriers _ or _ volumetric centers _",
    "vaidya used the volumetric center to design a new algorithm for minimizing a convex function over a convex set @xcite .",
    "more sophisticated algorithms have been developed based on vaidya s volumetric cutting plane method @xcite .      in the previous subsections",
    ", we introduced an algorithm that is highly cooperative with the dm and proved many interesting features about it . in this subsection",
    ", we set forth some implementation ideas .",
    "as we mentioned , one of our main criticisms of classical robust optimization is that it is not practical to ask the dm to specify an @xmath19-dimensional ellipsoid for the uncertainty set .",
    "our approach improves this situation by asking easier questions .",
    "the idea is similar to those used in the area of multi - criteria optimization .",
    "consider the system of inequalities @xmath258 and the corresponding slack vector @xmath259 representing the problem .",
    "the dm might prefer to directly consider only a few factors that really matter , we call them _ driving factors_. for example , the driving factors for a dm might be budget amount , profit , allocated human resources , etc .",
    "we can represent @xmath260 driving factors by @xmath261 , @xmath262 , and the problem for the dm is to maximize the utility function @xmath263 .",
    "similar to the way we added the objective of the linear program to the constraints , we can add @xmath260 constraints to problem and write as : @xmath264 as can be seen , the supergradient vector has only @xmath260 nonzero elements which makes it much easier for the dm to specify it for @xmath265 .",
    "@xmath260 is usually small and we can figure out approximate gradients by asking pair - wise comparison questions among the driving factors . however , it still may have the problem that the cutting plane algorithm is in a high - dimensional space and it might be slow .",
    "we can take one step further to resolve this difficulty .",
    "assume that the slack vector for our driving factors @xmath266 is a linear function of the slack vector of the original problem as @xmath267 for a matrix @xmath268 .",
    "our goal is to solve problem @xmath269 instead of , where @xmath270 is the polytope of slack variable for the driving factors . without loss of generality",
    ", we can assume that @xmath271 is a monotone non - decreasing function of @xmath272 ( it can be done by adjusting matrix @xmath268 ) .",
    "is a problem in 120 dimension and can be solve efficiently with cutting - plane algorithms .",
    "the dm deals only with problem , however , an optimizer / expert needs to translate the cuts in @xmath273-space into cuts in @xmath9-space and/or coordinate the search between the @xmath273-space and @xmath9-space .      in the previous subsection",
    ", we derived a cutting - plane algorithm in the @xmath9-space .",
    "as can be seen from propositions [ w - l5 ] and [ w - l6 ] , for the algorithm we need the supergradients of the utility function @xmath47 .",
    "however , we usually do not have an explicit formula for @xmath47 and our knowledge about it comes from the interaction with the dm .",
    "supplying supergradient information on preferences ( i.e. , the utility function ) might still be a difficult task for the dm .",
    "so , we have to simplify our questions for the dm and try to adapt our algorithm accordingly .",
    "we try to derive approximate supergradients based on simple questions from the dm .",
    "the idea is similar to the one used by arbel and oren in @xcite .",
    "assume that @xmath47 is differentiable which means the supergradient at each point is unique and equal to the gradient of the function at that point .",
    "assume that the algorithm is at the point @xmath29 .",
    "by taylor s theorem ( first order expansion ) for arbitrarily small scalars @xmath274 we have : @xmath275 assume that we have @xmath276 points @xmath29 and @xmath277 , @xmath278 . by the above equations ,",
    "if we have the value of @xmath47 at these points , we can find the approximate gradient .",
    "but in the absence of true utility function , we have to find these values through proper questions from the dm . here",
    ", we assume that we can ask the dm about the relative preference for the value of the function at these @xmath276 points .",
    "for example , dm can use a method called analytic hierarchy process ( ahp ) to assess relative preference .",
    "we use these relative preferences to find the approximate gradient .",
    "assume that the dm provides us with the priority vector @xmath279 , then we have the following relationship between @xmath279 and @xmath280 s @xmath281 now",
    ", we can substitute the values of @xmath282 from into and we have @xmath283^{\\top}.\\end{aligned}\\ ] ] the problem here is that we do not have the parameter @xmath284 .",
    "however , this parameter is not important in our algorithm because we are looking for normals to our proper hyperplanes and , as it can be seen in propositions [ w - l5 ] and [ w - l6 ] , a scaled gradient vector can also be used to calculate @xmath285 and @xmath286 .",
    "therefore , we can simply ignore @xmath284 in our algorithm .",
    "in previous sections , we introduced our new methodology to deal with lp problems with uncertainty .",
    "we explained in section [ utility ] that our approach has many good features in terms of interaction with the decision maker and usability , and its practical advantages over the classical robust optimization approach are clear . in this section , we prove , utilizing the approach by bertsimas and sim @xcite , that the robust optimal solutions generated by our algorithms are at least as desirable to the decision maker as any solution generated by many other robust optimization algorithms . we show that by choosing a suitable utility function @xmath47 we can model many of the classical robust formulations . in other words , we can find a solution of a classical robust optimization problem by solving @xmath287    many classical robust optimization models and their approximations can be written as follows @xmath288 where @xmath289 , @xmath290 , is a convex function such that @xmath291 for all feasible @xmath39 .",
    "if the convex uncertainty set @xmath292 is known for each @xmath290 and @xmath293 , then we have . by changing @xmath289 ,",
    "different formulations can be derived . in the following",
    "we bring some examples .",
    "assume that for each entry @xmath294 of matrix @xmath12 we have @xmath295 $ ] .",
    "it can easily be seen @xcite that the classical robust optimization problem is equivalent to for @xmath296 . for the second example , assume that @xmath297 for a given @xmath298 where @xmath299 is a general norm and @xmath300 is an invertible matrix .",
    "@xmath301 is a vector in @xmath302 created by stacking the columns of @xmath12 on top of one another .",
    "it is proved in @xcite that many approximate robust optimization models can be formulated by changing the norm .",
    "it is also proved in @xcite that this robust optimization model can be formulated as by @xmath303 , where @xmath304 is the dual norm and @xmath305 is a vector that contains @xmath39 in entries @xmath306 through @xmath307 , and 0 everywhere else .",
    "now , utilizing karush - kuhn - tucker ( kkt ) theorem , we prove that for every robust optimization problem that can be put into form , there exists a concave utility function @xmath129 for which has the same optimal solution as .",
    "assume that has slater points .",
    "then , there exists a concave function @xmath308 ( or equivalently @xmath47 ) such that optimization problems and have the same optimal solutions .    for the optimality condition of we",
    "have : there exists @xmath309 such that @xmath310 since the slater condition holds for , optimality conditions are necessary and sufficient .",
    "let @xmath311 be an optimal solution of , and let @xmath312 denote the set of indices for which @xmath313 .",
    "let us define @xmath308 as follows : @xmath314 where @xmath315 , are arbitrary numbers .",
    "we claim that @xmath308 is concave .",
    "@xmath316 is a concave function and @xmath317 is increasing concave , hence @xmath318 is a concave function for @xmath319 .",
    "@xmath308 is the summation of an affine function and some concave functions and so is concave .",
    "the gradient of @xmath308 is @xmath320 now define @xmath321 , as @xmath322.\\end{aligned}\\ ] ] using and comparing and , we conclude that @xmath311 is a solution of , as we wanted .",
    "the other direction can be proved similarly .",
    "the above argument proves the existence of a suitable utility function .",
    "a remaining question is that can we construct such a utility function without having a solution of ? in the following , we construct a function with objective value arbitrarily close to the objective value of .",
    "assume that strong duality holds for .",
    "let us define @xmath323 and assume that @xmath324 is the maximizer of @xmath308 .",
    "we have @xmath325 this means that @xmath324 is the maximizer of the lagrangian of the problem in , @xmath326 , for + , @xmath278 .",
    "so by strong duality , we have @xmath327 shows that by choosing @xmath328 small enough , we can construct @xmath308 such that the optimal objective value of is arbitrarily close to the optimal objective value of .",
    "note that many other approaches to robust optimization and decision making under uncertainty ( including the generalized robust counterpart introduced by ben - tal and nemirovski @xcite , and the approach of iancu and trichakis @xcite using the notion of pareto robust optimization ) can be included as a special case of our framework . a good starting point to prove the existence of a utility function is to start with indicator functions of sets encoding feasibility conditions .",
    "this approach first leads to utility functions that are not continuous ; however , as we did above , these functions can be smoothed by use of barriers which then lead to differentiable utility functions with desired properties .",
    "in this section , we present some numerical results to show the performance of the algorithms in the @xmath9-space designed in section [ alg ] .",
    "lp problems we use are chosen from the netlib library of lps .",
    "most of these lp problems are not in the format we have used throughout the paper which is the standard inequality form .",
    "hence , we convert each problem to the standard equality form and then use the dual problem . in this section ,",
    "the problem @xmath329 is the converted one . in the following ,",
    "we consider several numerical examples .",
    "* example 1 : * in this example , we consider a simple problem of maximizing a quadratic function .",
    "consider the adlittle problem ( in the converted form ) with 139 constraints and 56 variables .",
    "we apply the algorithm to function @xmath330 which makes two slack variables as close as possible .",
    "this function may not have any practical application , however , shows a simple example difficult to solve by classical robust optimization .",
    "the stopping criteria is @xmath331 . for @xmath332",
    "the algorithm takes 36 iterations and returns . for @xmath333",
    "the algorithm takes 35 iterations and returns @xmath334 .",
    "* example 2 : * consider the adlittle problem and assume that three constraints @xmath335 are important for the dm .",
    "assume that the dm estimates that there is @xmath336 percent uncertainty in the rhs of these inequalities .",
    "we have @xmath337 and so the desired slack variables are around @xmath338 . by using the classical robust optimization method that satisfies the worst case scenario ,",
    "the optimal objective value is @xmath339 .",
    "now assume that the following utility function represents dm s preferences : @xmath340 this function is a ndas function that we defined in definition [ ndas ] .",
    "assume that the dm set @xmath341 and . by using our algorithm",
    ", we get the objective value of with the slack variables @xmath342 . as we observe , the objective value is higher than the classical robust optimization method while two of the slack conditions are not satisfied . however , the slack variables are close to the desired ones .",
    "if the dm sets @xmath343 , we get the objective value of @xmath344 with the slack variables @xmath345 . however , all the iterates might be interesting for the dm .",
    "the following results are also returned by the algorithm before the optimal one : @xmath346    now assume that the dm wants to put more weight on constraints 68 and 71 and so set , @xmath347 and @xmath343 . in this case",
    ", the algorithm returns @xmath348 with the slack variables @xmath349 .",
    "* example 3 : * in this example , we consider the degen2 problem ( in the converted form ) with 757 constraints and 442 variables .",
    "the optimal solution of this lp is @xmath350 .",
    "assume that constraints 245 , 246 , and 247 are important for the dm who wants them as large as possible , however , at the optimal solution we have @xmath351 .",
    "the dm also wants the optimal objective value to be at least @xmath352 . as we stated before",
    ", we add the objective function as a constraint to the system . to have the objective value at least @xmath352",
    ", we can add this constraint as @xmath353 .",
    "for the utility function , the dm can use the ndas function @xmath354 by running the algorithm for the above utility function , we get + @xmath355 with objective value @xmath356 after 50 iterations and + @xmath357 with @xmath358 after 100 iterations .",
    "* example 4 : * we include a stopping criterion in the algorithm based on the norm of the supergradient .",
    "the dm should also have some control over the stopping criteria ( perhaps because of being satisfied or getting tired of the process ) . in this example",
    ", we consider the scorpion problem ( in the converted form ) with 466 constraints and 358 variables .",
    "the optimal objective value of this lp is @xmath359 .",
    "we consider the following two ndas utility functions : @xmath360 in this example , we apply the original and the 2nd modified algorithms to both @xmath361 and @xmath362 .",
    "the improvement in the utility function value after 200 iterations is shown in figure [ fig_numerical_1 ] .",
    "as can be seen , the rate of increase in the utility function is decreasing after each iteration .",
    "for this problem , the algorithm does not stop by itself and continues until the satisfaction of the dm .",
    "the dm can stop the algorithm , for example , when the rate of increase is less than a specified threshold .",
    "for this example , the rate of improvement for the 2nd modified algorithm is almost as good as the original one .",
    "however , in the modified algorithm , the weighted center is computed around 40 times during the 200 iterations which is much less computational work .",
    "* example 5 : * in this example , we consider utility functions introduced at the end of section [ prob ] . consider problem scorpion with optimal objective value of @xmath359 .",
    "assume that the uncertainty in constraints 211 to 215 are important for the dm and we have @xmath363 , @xmath364 , where @xmath365 was defined in .",
    "let @xmath324 be the solution of matlab s lp solver , then we have @xmath366 which is not satisfactory for the dm .",
    "besides , assume that the dm wants the objective value to be at least @xmath367 . to satisfy that , we add the @xmath368th constraint as @xmath369 which guarantees @xmath370 . for the utility function , first we define @xmath371 , @xmath364 similar to figure [ fig - utility ] with @xmath372 and @xmath373 .",
    "so we have for @xmath364 : @xmath374 now , we can define @xmath375 . by running the algorithm",
    ", the supergradient goes to zero after 65 iterations and the algorithm stops .",
    "denote the solution by @xmath311 , then the results are as follows : @xmath376 now , assume that the dm wants the objective value to be at least @xmath377 and the @xmath368th constraint becomes @xmath378 . in this case ,",
    "the norm of the supergradient reaches zero , after 104 iterations .",
    "the norm of supergradients versus the number of iterations are shown in figure [ fig_numerical_2 ] for these two cases .",
    "denote the solution after 100 iterations by @xmath379 , then we have : @xmath380    let @xmath381 be the returned value in the second case after 65 iterations .",
    "it is clearly not robust feasible ; however , we can use bound to find an upper bound on the probability of infeasibility .",
    "assume that @xmath382 and all the entries of @xmath365 are equal .",
    "then , bound reduces to @xmath383 , where @xmath384 .",
    "the probabilities of infeasibility of @xmath381 for constraints 211 to 215 are given in table [ table1 ] ( using bound ) .",
    "[ ht ] [ table1 ]    .the probability of infeasibility of @xmath381 for constraints 211 to 215 .",
    "[ cols=\">,>,>\",options=\"header \" , ]",
    "semidefinite programming is a special case of conic programming where the cone is a direct product of semidefinite cones .",
    "many convex optimization problems can be modeled by sdp . since our method",
    "is based on a barrier function for a polytope in @xmath50 , it can be generalized and used as an approximation method for robust semidefinite programming that is @xmath385-hard for ellipsoidal uncertainty sets .",
    "an sdp problem can be formulated as follows @xmath386 where @xmath387 and @xmath388 are symmetric matrices of appropriate size , and @xmath389 is the lwner order ; for two square , symmetric matrices @xmath390 and @xmath391 with the same size , we have @xmath392 iff @xmath393 is a semidefinite matrix . for every @xmath394 ,",
    "define @xmath395 assume that @xmath396 and let @xmath397 be a self - concordant barrier for @xmath398 .",
    "the typical self - concordant barrier for sdp is @xmath399 .",
    "assume @xmath400 is bounded and its interior is nonempty .",
    "now , as in the definition of the weighted center for lp , we can define a weighted center for sdp . for every @xmath401",
    ", we can define the weighted center as follows : @xmath402 the problem with this definition is that we do not have many of the interesting properties we proved for lp .",
    "the main one is that the weighted centers do not cover the relative interior of the whole feasible region and we can not sweep the whole feasible region by moving in the @xmath9-space .",
    "there are other notions of weighted centers that address this problem ; however , they are more difficult to work with algorithmically . extending the results we derived for lp to sdp can be a good future research direction to follow .",
    "the definition of the quasi - concave function is as follows :    a function @xmath403 is _ quasi - concave _ if its domain is convex , and for every @xmath404 , the set @xmath405 is also convex .",
    "all concave functions are quasi - concave , however , the converse is not true .",
    "quasi - concave functions are important in many fields such as game theory and economics . in microeconomics ,",
    "many utility functions are modeled as quasi - concave functions . for differentiable functions , we have the following useful proposition :    a differentiable function @xmath138 is quasi - concave if and only if the domain of @xmath138 is convex and for every @xmath39 and @xmath28 in @xmath406 we have : @xmath407    is similar to , which is the property of the supergradient we used to design our algorithms .",
    "the whole point is that for a differentiable quasi - concave function @xmath47 and any arbitrary point @xmath149 , the maximizers of @xmath47 are in the half - space + @xmath408 .",
    "this means that we can extend our algorithms to differentiable quasi - concave utility functions simply by replacing supergradient with gradient , and all the results for @xmath29-space and @xmath9-space stay valid .      in this paper",
    ", we presented new algorithms in a framework for robust optimization designed to mitigate some of the major drawbacks of robust optimization in practice .",
    "our algorithms have the potential of increasing the applicability of robust optimization .",
    "some of the advantages of our new algorithms are :    1 .   instead of a single , isolated , and very demanding interaction with the dm ,",
    "our algorithms interact continuously with the dm throughout the optimization process with more reasonable demands from the dm in each iteration .",
    "one of the benefits of our approach is that the dm  learns \" what is feasible to achieve throughout the process .",
    "another benefit is that the dm is more likely to be satisfied ( or at least be content ) with the final solution .",
    "moreover , being personally involved in the production of the final solution , the dm bears some responsibility for it and is more likely to adapt it in practice .",
    "our algorithms operate in the weight - space using only driving factors with the dm .",
    "this helps reduce the dimension of the problem , simplify the demands on the dm while computing the most important aspect of the problem at hand .",
    "weight - space and weighted - analytic - centers approach embeds a ",
    "highly differentiable \" structure into the algorithms .",
    "such tools are extremely useful in both the theory and applications of optimization .",
    "in contrast , classical robust optimization and other competing techniques usually end up delivering a final solution where differentiability can not be expected .",
    "note that many elements of our approach can be partly utilized in other approaches to robust optimization and decision making situations under uncertainty .",
    "moreover , our work creates natural connections between robust optimization and multi - attribute utility theory , elicitation methods used in multi - criteria decision making problems and goal programming theory ( see @xcite ) .",
    "developing similar algorithms for semidefinite programming is left as a future research topic . as we explained in subsection [ sdp ]",
    ", we can define a similar notion of weighted center for sdp . however , these weighted centers do not have some of the properties we used for lp , and we may have to switch to other notions of weighted centers that are more difficult to work with algorithmically , and have fewer desired properties compared to the lp setting .    100    k. m. anstreicher , on vaidya s volumetric cutting plane method for convex programming , _ mathematics of operations research _ , 22 ( 1997 )",
    ", 6389 .",
    "a. ardel , and s. oren , using approximate gradients in developing an interactive interior primal - dual multiobjective linear programming algorithm , _",
    "european journal of operational research _ , 89 ( 1996 ) , 202211 .",
    "a. ben - tal , s. boyd and a. nemirovski , extending scope of robust optimization : comprehensive robust counterparts of uncertain problems , _ mathematical programming _ , 107 ( 2006 ) 6389 .",
    "a. ben - tal , l. el ghaoui , and a. nemirovski , _ robust optimization _ , princeton series in applied mathematics , ( 2009 ) .",
    "a. ben - tal and a. goryashko and e. guslitzer and a. nemirovski , adjustable robust solutions of uncertain linear programs , _ mathematical programming _ , 99 ( 2004 ) 351376 .",
    "a. ben - tal and a. nemirovski , robust solutions of linear programming problems contaminated with uncertain data , _ math .",
    "_ 88 ( 2000 ) 411424    a. ben - tal and a. nemirovski , robust solutions of uncertain linear programs , _ operation research letters _ , 25 ( 1999 ) , 113 .",
    "a. ben - tal and a. nemirovski , robust convex optimization , _ mathematics of operations research _ , 23 ( 1998 ) , 769805 .",
    "d. bertsimas and d. pachamanova and m. sim , robust linear optimization under general norms , _ operations research letters _ , 32 ( 2004 ) , 510516 .",
    "d. bertsimas , and i. popescu , optimal inequalities in probability theory- a convex optimization approach , _ siam j. optim .",
    "_ , 15 ( 2005 ) , 780804 .",
    "d. bertsimas and m. sim , tractable approximations to robust conic optimization problems , _ mathematical programming _ , june 2005 .",
    "d. bertsimas and m. sim , the price of robustness , _ operations research _ , 52 ( 2004 ) , 3553 .",
    "d. bertsimas and m. sim , robust discrete optimization and network flows , _ math .",
    "_ , 98 ( 2003 ) , 4971 .    j.f",
    ". bonnans , and a. shapiro , _ perturbation analysis of optimization problems _ , springer , 2000 .",
    "t. bortfeld , t. c. y. chan , a. trofimov , and j. n. tsitsiklis , robust management of motion uncertainty in intensity - modulated radiation therapy , _ oper .",
    "res . _ 56 ( 2008 ) , 14611473 .",
    "s. boyd , and l. vanderberghe , _ convex optimization _ , cambridge university press , 2004 .",
    "chan and v. v. mii , adaptive and robust radiation therapy optimization for lung cancer , _",
    "european j. oper .",
    "res . _ 231 ( 2013 ) , 745756 .",
    "m. chu , y. zinchenko , s. g. henderson , and m. b. sharpe , robust optimization for intensity modulated radiation therapy treatment planning under uncertainty , _ physics in medicine and biology _ 50 ( 2006 ) , 54635477 .",
    "e. erdoan and g. iyengar , ambiguous chance constrained problems and robust optimization , _ mathematical programming _ , 107 ( 2006 ) , 3790 .",
    "j. h. gallier , _ geometric methods and applications : for computer science and engineering _ , springer , 2001 .",
    "l. el . ghaoui and f. oustry and h. lebret , robust solutions to uncertain semidefinite programs",
    "_ siam j. optim .",
    "_ 9 ( 1998 ) , 3352 .",
    "j. l. goffin , z. q. luo , and y. ye . on the complexity of a column generation algorithm for convex and quasiconvex feasibility problems , _ large scale optimization : state of the art , kluwer academic publishers _ , ( 1993 ) , 187196 .",
    "j. l. goffin and j. p. vial , convex non - differentiable optimization : a survey focused on the analytic center cutting - plane method , _ optimization methods @xmath409 software _ , 17 ( 2002 ) , 805867 .",
    "j. goh , and m. sim .",
    "distributionally robust optimization and its tractable approximations .",
    "_ operations research _ ,",
    "58.4-part-1 ( 2010 ) , 902917 .",
    "b. grnbaum , partitions of mass - distributions and convex bodies by hyperplanes , _ pacific j. math .",
    "_ , 10 ( 1960 ) , 12571261 .",
    "w. hoeffding , probability inequalities for sums of bounded random variables , _ journal of the american statistical association _ 58 ( 1963 ) , 1330 .",
    "j. hu , and s. mehrotra , robust and stochastically weighted multiobjective optimization models and reformulations , _ operations research _ , 60 ( 2012 ) , 936953 .    d. a. iancu , and n. trichakis . pareto efficiency in robust optimization , _ management science _ , 60.1 ( 2013 ) , 130147 .",
    "ignizio , _ goal programming and extensions _ , lexington books , lexington , ma , ( 1976 ) .",
    "v. s. iyengar , j. lee , and m. campbell , q - eval : evaluating multiple attribute items using queries , _ proceedings of the 3rd acm conference on electronic commerce _ , ( 2001 ) 144 - 153 .",
    "m. karimi , _ a quick - and - dirty approach to robustness in linear optimization _ , master s thesis , university of waterloo , 2012 .",
    "r. keeney , _ value - focused thinking _ , harvard university press , london , ( 1992 ) .",
    "r. keeney , and h. raiffa , _ decision with multiple objectives _ , wiley , new york , ( 1976 ) .",
    "m. kksalan , j. wallenius , and s. zionts , _ multiple criteria decision making : from early history to the 21st century _ ,",
    "singapore , world scientific , ( 2011 ) .",
    "l. b. miller , and h. wagner , chance - constrained programming with joint constraints , _ operations research _ , 13 ( 1965 ) , 930945 .",
    "s. moazeni , _ flexible robustness in linear optimization _ , master s thesis , university of waterloo , 2006 .",
    "m. g. morgan , and m. henrion , _ uncertainty - a guide to dealing with uncertainty in quantitative risk and policy analysis _ , cambridge university press , new york , ny , usa , ( 1990 ) .",
    "s. mudchanatongsuk , f. ordonez , and and j. liu , robust solutions for network design under transportation cost and demand uncertainty , usc ise working paper 200505 .",
    "j. m. mulvey , r. j. vanderbei , and s. a. zenios , robust optimization of large - scale systems , _ operations research _ , 43 ( 1995 ) , 264281 .",
    "a. nemirovski , and a. shapiro , convex approximations of chance constrained programs , _ siam journal of optimization _ , 4 ( 2006 ) 969996 .",
    "complexity estimates of some cutting - plane methods based on the analytic barrier .",
    "_ mathematical programming , series b _ , 69 ( 1995 ) , 149176 .",
    "nesterov , and a. nemirovskii , _ interior point polynomial algorithm in convex programming _",
    ", siam ; studies in applied and numerical mathematics , 1994 .",
    "d. j. newman , location of the maximum on unimodal surfaces , _ journal of the acm _",
    ", 12 ( 1965 ) , 395398 .",
    "f. ordonez and j. zhao , robust capacity expansion of network flows , usc - ise working paper 200401 .",
    "r. t. rockafellar , _ convex analysis _ , princeton university press , 1997 .",
    "t. santoso , s. ahmed , m. goetschalckx , and a. shapiro , a stochastic programming approach for supply chain network design under uncertainty , _",
    "european journal of operational research _ , 167 ( 2005 ) 96115 .",
    "m. y. sir , m. a. epelman , and s. m. pollock , stochastic programming for off - line adaptive radiotherapy , _ ann .",
    "_ 196 ( 2012 ) 767797 .",
    "a. l. soyster , convex programming with set - inclusive constraints and applications to inexact linear programming , _ operations research _ , 21 ( 1973 ) , 11541157 .",
    "p. m. vaidya , a new algorithm for minimizing convex functions over convex sets , _",
    "symposium on foundations of computer science _ , ( 1989 ) , 338343 .",
    "p. m. vaidya , and d. s. atkinson , a technique for bounding the number of iterations in path following algorithms , _ complexity in numerical optimization _ , world scientific , singapore , ( 1993 ) , 462489 .",
    "d. b. yudin and a.s .",
    "nemirovski , informational complexity and efficient methods for solving complex extremal problems , _ matekon _ , 13 ( 1977 ) , 2545 .",
    "in this appendix , we study the properties of weight space as well as @xmath96 and @xmath95 regions .",
    "let us start from the following well - known lemma :    [ first - l ] let @xmath92 and @xmath97 be the solutions of system corresponding to the weight vectors @xmath9 , @xmath410 , respectively .",
    "for every @xmath411 in the null space of @xmath412 we have : @xmath413    from , we have @xmath259 and @xmath414 , which results in @xmath415 .",
    "hence we have @xmath416 .",
    "as the null space of @xmath412 and the range of @xmath12 are orthogonal , for every @xmath417 we can write : @xmath418    let @xmath97 be the solution of system corresponding to the weight vector @xmath105 .",
    "moreover , assume that @xmath99 is such that @xmath419 .",
    "then , by using lemma [ first - l ] , we can show that @xmath100 is the solution of system corresponding to the weight vector @xmath101 .",
    "hence , there may be many weight vectors that give the same @xmath9-center .",
    "a stronger result is the following lemma which shows that in some cases , we can find the weighted center for a combination of weight vectors by using the combination of their weighted centers .",
    "[ w1 ] let @xmath420 , @xmath421 , be solutions of system , corresponding to the weights @xmath422 .",
    "then , for every set of @xmath423 $ ] , @xmath424 , such that @xmath425 , and for every @xmath426 , we have @xmath427 is the @xmath9-center of @xmath54 , where @xmath428 moreover , @xmath429    according to the assumptions , for every @xmath430 , we have @xmath431 now , it can be seen that @xmath427 satisfies the system : @xmath432 since the @xmath9-center of @xmath54 is unique , the proof for the first part is done .",
    "+ for the second part , from ( [ l1 ] ) we can write @xmath433 by lemma [ first - l ] , we have @xmath434 .",
    "therefore , we can continue the above series of equations as follows : @xmath435      in this subsection , we study the structure of the @xmath9-space , which is important for the design of the algorithms in section [ alg ] .",
    "let @xmath29 and @xmath28 be centric .",
    "first , we note that the simplex of the weight vectors can be divided into regions of constant @xmath28-vector ( @xmath95 ) and constant @xmath29-vector ( @xmath96 ) . by using lemma [ w1 ] ,",
    "if @xmath97 is the solution of system corresponding to the weight vector @xmath98 , and @xmath99 is any centric @xmath28-vector , then @xmath100 is the solution of system corresponding to the weight vector @xmath101 .",
    "this means that for every centric vector @xmath102 and any centric vector @xmath28 , @xmath103 is a weight vector in the simplex .    for every pair of centric vectors",
    "@xmath29 and @xmath28 , @xmath96 and @xmath95 are convex . to see this , let @xmath104 and @xmath92 be the weighted centers of @xmath105 and @xmath9 . then , it is easy to see that for every @xmath106 $ ] , @xmath107 is the weighted center of @xmath108 . with a similar reasoning ,",
    "@xmath95 is convex for every centric @xmath28 .    using",
    ", we can express @xmath96 and @xmath95 as follows : @xmath436 \\cap b_1(0,1),\\end{aligned}\\ ] ] @xmath437 \\cap b_1(0,1),\\end{aligned}\\ ] ] where @xmath438 is the unit ball in @xmath439-norm centered at zero vector .",
    "here , we want to find another formulation for @xmath95 that might work better in some cases .",
    "we use the following lemma .",
    "[ w - l4 ] assume that the rows of @xmath440 make a basis for the null space of @xmath441",
    ". then there exists @xmath442 such that @xmath443 if and only if @xmath444 .",
    "i.e. , @xmath445 iff @xmath446 .",
    "assume that there exists @xmath39 such that @xmath443 . by multiplying both sides with @xmath447 from the left and using the fact that @xmath448 we have the result .",
    "for the other direction , assume that @xmath444 .",
    "then @xmath449 which means @xmath450 is in the null space of @xmath447 .",
    "then , using the orthogonal decomposition theorem , we have @xmath451 .",
    "thus , there exists @xmath39 such that @xmath443 .",
    "assume that @xmath452 is such that its rows make a basis for the null space of @xmath412 .",
    "for every vector @xmath28 , we have @xmath453 , so if @xmath28 is in the null space of @xmath412 , @xmath454 is in the null space of @xmath441 .",
    "hence , if the rows of @xmath455 make a basis for the null space of @xmath412 , the rows of @xmath456 make a basis for the null space of @xmath441 and we can write @xmath457 . using lemma [ w - l4 ] , there exists @xmath39 such that @xmath443 if and only if @xmath458 , and we can write as : @xmath459 let us denote the affine hull with @xmath460",
    ". we can prove the following lemma about @xmath96 and @xmath95 .",
    "[ w - l1 ] assume that @xmath29 and @xmath28 are centric , we have @xmath461    we prove the first one and our proof for the second one is the same .",
    "clearly we have @xmath462 . to prove the other side ,",
    "assume by contradiction that there exist @xmath463 such that @xmath464 .",
    "pick an arbitrary @xmath465 and consider all the points @xmath466 for @xmath106 $ ] .",
    "both @xmath9 and @xmath105 are in @xmath467 , so all the points @xmath468 are also in @xmath467 . @xmath469 and @xmath470 ,",
    "so let @xmath471 be @xmath472 .",
    "note that all the points in @xmath96 has the same @xmath29-vector , so we have @xmath473 for @xmath474 . by using we must also have @xmath475 .",
    "we want to prove that @xmath476 .",
    "assume that @xmath477 .",
    "all the points on the line segment between @xmath478 and @xmath479 have the same @xmath29-vector and we can write them as @xmath480 for @xmath481 $ ] .",
    "but note that @xmath482 , so there is a small enough @xmath483 such that @xmath484 and hence @xmath485 is a weight vector in @xmath96 .",
    "however , it is also a vector on the line segment between @xmath479 and @xmath9 which is a contradiction to @xmath486 .",
    "so @xmath476 and @xmath487 which is a contradiction . hence @xmath488 and",
    "we are done .",
    "we conclude that @xmath89 is sliced in two ways by @xmath95 s and @xmath96 s for centric @xmath29 and @xmath28 vectors . for each centric @xmath29 and each centric @xmath28 , @xmath95 and @xmath96 intersect at a single point @xmath489 on the simplex .",
    "we want to prove that the smallest affine subspace containing @xmath96 and @xmath95 is . to that end , we prove some results on the intersection of affine subspaces .",
    "we start with the following definition :    [ lineality - d ] the _ recession cone _ of a convex set @xmath490 is denoted by @xmath491 and defined as : @xmath492 the _ lineality space _ of a convex set @xmath268 is denoted by @xmath493 and defined as : @xmath494    let @xmath129 be an affine subspace of @xmath495 . if @xmath496 , then @xmath497 , which means @xmath498 .",
    "therefore , by definition [ lineality - d ] , we have @xmath499 .",
    "then , by using the definition of the affine space we have : @xmath500 in other words , @xmath501 is a linear subspace such that @xmath502 for all @xmath503 where @xmath504 is the minkowski sum .",
    "the following two lemmas are standard , see , for instance , @xcite .",
    "[ aff-1 ] given a pair of nonempty affine subspaces @xmath129 and @xmath505 in @xmath506 , the following facts hold : ( 1 ) @xmath507 iff for every @xmath503 and @xmath508 , we have @xmath509 . ( 2 ) @xmath510 consists of a single point iff for every @xmath503 and @xmath508 , we have @xmath511 ( 3 ) for every @xmath503 and @xmath508 , we have @xmath512    [ aff-2 ] let @xmath129 and @xmath505 be nonempty affine subspaces in @xmath506",
    ". then we have the following properties :    \\(1 ) if @xmath513 , then @xmath514    \\(2 ) if @xmath507 , then @xmath515    using the above lemmas , we deduce the following proposition .",
    "[ aff-3 ] assume that @xmath29 and @xmath28 are centric @xmath29-vector and @xmath28-vector , respectively .",
    "then the smallest affine subspace containing @xmath96 and @xmath95 is @xmath516 .",
    "we assumed that @xmath517 has full column rank , i.e. , @xmath24 and the interior of @xmath518 is not empty .",
    "let @xmath128 denote the set of all centric @xmath29-vectors , i.e. , the set of @xmath29-vectors for which there exist @xmath92 satisfies all the equations in .",
    "we claim that @xmath519 . for every @xmath520 ,",
    "pick an arbitrary @xmath83 such that @xmath112 . for every scalar @xmath521 we have @xmath522 , so we can choose @xmath521 such that @xmath523 .",
    "hence @xmath524 satisfies and we conclude that @xmath519 .",
    "the range of @xmath12 has dimension @xmath18 and since @xmath128 is not empty ; it is easy to see that the dimension of @xmath128 is also @xmath18 .",
    "moreover , we have @xmath525 and since @xmath26 is non - singular , we have @xmath526",
    ".    now denote by @xmath447 the set of centric @xmath28-vectors . by",
    ", we have @xmath112 .",
    "the dimension of the null space of @xmath412 is @xmath527 . in addition",
    ", we have to consider the restriction @xmath94 ; we have @xmath528 so , we have @xmath113 for centric @xmath28-vectors which reduces the dimension by one ( since @xmath529 ) , and @xmath530 .",
    "we have @xmath531 and so by the same explanation @xmath532 .",
    "we proved that @xmath96 and @xmath95 intersect at only a single point @xmath533 , so @xmath534 . by using lemma [ aff-2]-(2 ) the dimension of the smallest affine subspace containing @xmath96 and @xmath95",
    "is @xmath535 the dimension of @xmath536 is also @xmath537 , so by lemma [ w - l1 ] @xmath536 is the least affine subspace containing @xmath96 and @xmath95 .",
    "in this appendix , we first have a discussion on choosing the next weight vector in implementing the weight space algorithm .",
    "after that we introduce two modified algorithms using the properties of the weight space we proved in appendix [ appendix-1 ] .      in the above algorithm , we did not explain about choosing the next weight vector in the shrunken space .",
    "in the case of little information about the function , different centers can be chosen to achieve better convergence , as we explain in subsection [ convergence ] .",
    "if we have enough information about the utility function , we might be able to choose a more appropriate weight vector .",
    "assume that @xmath538 where @xmath218 .",
    "by comparison of with the optimization problem for the weighted analytic center , we see that our problem is actually finding the weighted analytic center for the weight vector @xmath539 .",
    "hence , if we had @xmath539 , our problem would be finding the weighted center of @xmath539 .",
    "however , @xmath539 can be computed by using the gradient of the function .",
    "assume that we start with @xmath159 with the weighted center @xmath540 .",
    "defining @xmath541 , it is easy to see that @xmath542 .",
    "now we can choose @xmath543 where @xmath544 is the scaling factor such that @xmath545 , and the @xmath29-vector of @xmath157 is the solution of the problem .",
    "the same idea can be used if we know that the utility function is close to the sum of the logarithms .",
    "assume that @xmath47 is non - decreasing on each entry , i.e. , @xmath546 for all @xmath547 .",
    "consider @xmath89-space algorithm introduced above and the point @xmath548 from the simplex .",
    "the corresponding half - space is @xmath549 where @xmath550 , and @xmath551 is the solution of @xmath552 .",
    "it is easy to show that @xmath553 lies in that half space .",
    "we have : @xmath554 where the last inequality is from the fact that @xmath555 is positive semidefinite .",
    "the problem here is that @xmath556 may not be in the shrunken space , where @xmath557 is again the scaling factor .",
    "so , we can perform a line search to find a point on the line segment @xmath558 $ ] in the interior of the shrunken space .",
    "we designed a cutting - plane algorithm in the @xmath9-space for maximizing the utility function . in this subsection",
    ", we are going to use the properties of the weighted center we derived in section [ utility ] to improve the performance of the algorithm .",
    "we introduce two modified versions of the @xmath9-space algorithms in this subsection .      as we proved in section [ utility ] , for every centric @xmath28-vector @xmath559 and any centric @xmath29-vector @xmath102 ,",
    "@xmath560 is a weight vector in the simplex @xmath89 . as we are maximizing @xmath47 over @xmath29 , roughly speaking ,",
    "only the @xmath29-vector of the weighted center is important for us for each @xmath561 .",
    "this is somehow explicit in our algorithm as , for example , the normal to the cutting - plane at each step , given in proposition [ w - l6 ] , depends on @xmath29 and @xmath562 which is the @xmath28-vector of the starting point @xmath159 .",
    "the algorithm also guarantees to keep @xmath242 in the shrunken region at each step .",
    "hence , we lose nothing if we try to work with weight vectors with @xmath563 .",
    "consider lemma [ w1 ] which is about the convex combination of weight vectors .",
    "assume that we have weight vectors @xmath564 , @xmath565 , with weighted centers @xmath566 , which means they have the same @xmath28-vector .",
    "by lemma [ w1 ] , for every set of @xmath423 $ ] , @xmath565 , such that @xmath567 , we have @xmath568 is the @xmath9-center where @xmath569 . in other words , when the @xmath28-vectors are the same , @xmath29-vector ( equivalently @xmath39-vector ) of the convex combination of @xmath224 is equal to the convex combination of @xmath227 , @xmath565 .",
    "this is interesting because if we can update the weight vectors by using the convex combination , we do not need to compute the weighted center .",
    "we are going to use this to modify our algorithm .",
    "assume that the starting point is @xmath159 with weighted center @xmath160 .",
    "the modified algorithm is similar to the algorithm in subsection [ sec : algorithm w - space ] and the normal to the cutting - plane is derived by using .",
    "however , in the modified one , all @xmath224 have @xmath28-vector equal to @xmath562 .",
    "the modified algorithm has two modules :    _ module 1 : _ assume that at step @xmath32 , we have @xmath570 and @xmath571 with the corresponding normals of the cutting - planes @xmath572 and @xmath573 . by the choice of @xmath224 , we must have @xmath574 .",
    "in the modified algorithm , if we have @xmath575 , then we put @xmath576 ( it  is easy to see this weight vector is in the required cut simplex ) . in this case",
    ", we have @xmath577 and @xmath578 .",
    "if we have @xmath575 , then the line segment @xmath579 $ ] is no longer in the required cut simplex .",
    "however , there exists @xmath76 such that @xmath580 is in the required cut simplex",
    ". we can do a line search to find @xmath539 and then we set @xmath581 . in this case , we have @xmath577 and @xmath582 .",
    "_ module 2 : _ in module 1 , the algorithm always moves along a single line .",
    "when the weight vectors in module 1 get close to each other , we perform module 2 to get out of that line . to do that , we choose a constant @xmath483 and whenever in module 1 we have @xmath583 , we perform module 2 . in module 2 , like the algorithm in subsection",
    "[ sec : algorithm w - space ] , we pick an arbitrary weight vector @xmath105 in the remaining cut simplex and compute the weighted center @xmath97 .",
    "the problem now is that @xmath28-vector is not necessarily equal to @xmath562 .",
    "however , we said that @xmath102 is important for our algorithm ; hence , we consider the weight vector @xmath584 .",
    "this new weight vector is not necessarily in the required cut simplex . to solve this problem",
    ", we use the same technique as in module 1 .",
    "we consider the line containing the line segment @xmath585 $ ] and do a line search to find an appropriate weight vector on this line . to simplify the line search , we consider @xmath586 and @xmath587 separately .    at the end of module 2",
    ", we again come back to module 1 to continue the algorithm .",
    "as can be seen , we only have to find a weighted center in module 1 which makes the modified algorithm computationally more efficient than the original algorithm , in practice .",
    "consider the main algorithm and the proof of proposition [ w - l6 ] .",
    "we constructed normal vectors that satisfy . by using the supergradient inequality , @xmath588 results in @xmath589 .",
    "assume that a sequence of @xmath29-vectors @xmath590 has been created by the algorithm up to iteration @xmath591 .",
    "we may not have access to the value of @xmath592 , @xmath593 , however , we know that there exists @xmath594 such that @xmath595 for all @xmath593 . by the supergradient inequality we must have @xmath596 for all @xmath593 and from @xmath597 this means that @xmath598 is a weight vector in the desired cut simplex .",
    "let @xmath599 be the indices that @xmath600 for all @xmath601 . by the above explanation , we know that @xmath602 ( the @xmath29-vector with the largest value so far is in this set . ) .",
    "the idea of the modified algorithm is that when @xmath603 , we put a convex combination of these @xmath29-vectors as the new @xmath29-vector .",
    "we can divide the new algorithm into three modules .    _ module 1 : _",
    "@xmath603 : define @xmath604 and @xmath605 .    _ module 2 : _ @xmath606 .",
    "we only have one point @xmath607 that @xmath608 for all @xmath593 and by the above explanation we have @xmath595 for all @xmath593 .",
    "hence , @xmath609 is our best point so far and we use it to find the next one . to do that , we choose a direction @xmath610 such that @xmath611 .",
    "@xmath612 must be in @xmath37 and therefore a good choice is the projection of @xmath613 on @xmath37 .",
    "let us define @xmath614 as the projection matrix to @xmath37 , then we define @xmath615 and do a line search to find the appropriate @xmath521 such that @xmath616 is in the desired cut simplex .",
    "we also have @xmath605 .",
    "_ module 3 : _ in the first two modules , we do not have to calculate the weighted center . in this module , like the first modified algorithm ,",
    "when @xmath617 in module 1 or 2 is smaller than a specified value , we perform an iteration like the original algorithm ; pick an arbitrary point inside the cut simplex and compute the weighted center for that .",
    "probabilistic analysis is tied to robust optimization .",
    "one of the recent trends in robust optimization research is the attempt to try reducing conservatism to get better results , and at the same time keeping a good level of robustness . in other words",
    ", we have to show that our proposed answer has a low probability of infeasibility . in this section ,",
    "we derive some probability bounds for our algorithms based on weight and slack vectors .",
    "these bounds can be given to the dm with each answer and the dm can use them to improve the next feedback .      before starting the probabilistic analysis , want to relate the notion of weights to the parameters of the uncertainty set . as we explained in subsection [ notions ] , we consider our uncertainty sets as follows : @xmath618^{n_i } \\",
    "\\ \\tilde{b}_i = b_i^{(0)}+\\sum_{l=1}^{n_i}{\\delta b_i^l \\tilde{z}_i^l } \\right \\},\\end{aligned}\\ ] ] where @xmath619 , @xmath278 are independent random variables , and @xmath620 is the scaling factor of @xmath621 .",
    "we assume that the support of @xmath621 contains @xmath622 , i.e. , @xmath623 .",
    "let us define another set which is related to the weight vectors : @xmath624 where @xmath71 is the @xmath28-vector of @xmath9 .",
    "our goal is to explicitly specify a set of weights whose corresponding @xmath9-center makes the feasible solution of the robust counterpart .    [ 22 ] let @xmath39 satisfy @xmath625 for every @xmath626 .",
    "then there exists some @xmath627 , so that @xmath39 is the weighted analytic center with respect to the weight vector @xmath9 , i.e. , @xmath628 . in other words ,",
    "@xmath629    let @xmath630 be an arbitrary vector such that @xmath631 , and let @xmath97 be the weighted center corresponding to it .",
    "assume that @xmath39 is in the robust feasible region ; we must have @xmath632 for every @xmath633 with nonzero probability , particularly for @xmath634 where @xmath30 is all ones vector .",
    "so @xmath635 define @xmath636 .",
    "thus , from the above equation , for every @xmath290 we have @xmath637 and consequently @xmath638 using the fact that @xmath639 . for every @xmath290 , we set @xmath640 since @xmath641 satisfies the optimality conditions , we have @xmath628 .",
    "it remains to show that @xmath642 .",
    "first note that : @xmath643 where for the second equality we used lemma [ first - l ] .",
    "now , using the fact that @xmath644 for every @xmath290 , we have @xmath645 .",
    "we already proved that @xmath646 .",
    "these two inequalities prove that @xmath647 .",
    "the above proposition shows that when the robust counterpart problem with respect to the uncertainty set @xmath648 is feasible , the set @xmath649 is nonempty . in the next proposition",
    "we prove that the equality holds in the above inclusion .",
    "[ 33 ] ( a)we have @xmath650 + ( b ) assume that @xmath651 satisfies @xmath87 , and @xmath28 is its corresponding @xmath28-vector .",
    "for every @xmath278 , we have + @xmath652    \\(a ) @xmath653 part was proved in proposition [ 22 ] . for @xmath654 ,",
    "let @xmath627 and @xmath85 be its corresponding weighted center .",
    "by @xmath627 we have @xmath655 therefore , for all @xmath656 $ ] , @xmath657 which proves @xmath39 is a robust feasible solution with respect to the uncertainty set @xmath648 .",
    "+ ( b ) assume that @xmath651 satisfies @xmath87 , @xmath28 is its corresponding @xmath28-vector , and there exists @xmath278 such that @xmath658 .",
    "if there exists @xmath659 such that @xmath660 where @xmath661 , by using @xmath662 we have @xmath663 which is a contradiction .",
    "we conclude that @xmath664 for all @xmath659 .      without loss of generality , we make the following assumptions on @xmath7 and @xmath5 :    * for every @xmath48 , @xmath665 can be written as @xmath666 where @xmath619 are independent random variables for every @xmath278 . * for each @xmath667 , @xmath668 , we have @xmath669 where @xmath670 are independent random variables .    as can be seen above , each variable @xmath665 is the summation of a nominal value @xmath671 with scaled random variables @xmath619 . in practice ,",
    "the number of these random variables @xmath672 is small compared to the dimension of @xmath12 as we explained above : each random variable @xmath673 represents a major source of uncertainty in the system .",
    "suppose we wish to find a robust feasible solution with respect to the uncertainty set @xmath648 , where @xmath674 was defined in . by proposition",
    "[ 33 ] , it is equivalent to finding the weighted center for a @xmath627 , where @xmath649 is defined in . however , finding such a weight vector is not straight forward as we do not have an explicit formula for @xmath649 .",
    "assume that we pick an arbitrary weight vector @xmath68 such that @xmath87 , with the weighted center @xmath85 .",
    "let us define the vector @xmath675 for @xmath9 as @xmath676 where @xmath365 was defined in .",
    "for each @xmath278 , if @xmath677 , by proposition [ 33]-(b ) we have @xmath664 for all @xmath659 .",
    "so , the problem is with the constraints that @xmath678 . for every such constraint ,",
    "we can find a bound on the probability that @xmath679 .",
    "as in the proof of proposition [ 33]-(b ) , in general we can write : @xmath680 where the last inequality is derived by using hoeffding s inequality :    [ h ] ( _ _ hoeffding s inequality__@xcite ) let @xmath681 be independent random variables with finite first and second moments , and for every @xmath682 , @xmath683 . then for every @xmath684 @xmath685.\\ ] ]    bertsimas and sim @xcite derived the best possible bound , i.e. , a bound that is achievable .",
    "the corresponding lemma proved in @xcite is as follows :    [ price-3 ] ( a ) if @xmath621 , @xmath686 , are independent and symmetrically distributed random variables in @xmath687 $ ] , @xmath279 is a positive constant , and @xmath688 , @xmath686 , then @xmath689 where @xmath690,\\ ] ] where @xmath691 , and @xmath692 .",
    "+ ( b ) the bound in is tight for @xmath621 having a discrete probability distribution : + @xmath693 , @xmath694 , @xmath686 , an integral value of @xmath695 , and @xmath696 being even .",
    "we can use the bound for our relation as follows .",
    "assume that @xmath621 , @xmath686 , are independent and symmetrically distributed random variables in @xmath687 $ ]",
    ". also denote by @xmath697 , the maximum entry of @xmath365 .",
    "using , we can write @xmath698 to compare these two bounds , assume that all the entries of @xmath365 are equal . bound",
    "reduces to @xmath699 , and bound reduces to @xmath700 .",
    "we can prove that bound dominates bound .",
    "moreover , bound is somehow the best possible bound as it can be achieved by a special probability distribution as in lemma [ price-3 ] .",
    "the above probability bounds do not take part in our algorithm explicitly .",
    "however , for each solution , we can present these bounds to the dm and s / he can use them to improve the feedback to the algorithm . as an example of",
    "how these bounds may be used for the dm , we show how to construct a concave utility function @xmath47 based on these probability bounds .",
    "bounds and are functions of @xmath701 and as a result , functions of @xmath29 .",
    "now , assume that based on the probability bounds , the dm defines a function @xmath371 for each slack variable @xmath702 as shown in figure [ fig - utility ] .",
    "@xmath371 increases as @xmath702 increases , and then at the point @xmath703 becomes flat . at @xmath704",
    "it starts to decrease to reach zero .",
    "parameters @xmath703 and @xmath705 are specified by the dm s desired bounds .",
    "now , we can define the utility function as @xmath706 .",
    "this function is not concave , but maximization of it is equivalent to the maximization of @xmath707 which is concave .",
    "* proof of proposition [ w - l3 ] *    consider the weight vectors @xmath242 and @xmath708 .",
    "our two hyperplanes are @xmath709 by lemma [ fl ] , @xmath242 is in the half - space @xmath710 and @xmath708 is in the half - space @xmath711 .",
    "if one of these two points is also in the other half - space , then we are done .",
    "so , assume that @xmath712 ( we are seeking contradiction ) , which is equivalent to @xmath713 using and we conclude that @xmath714 however , note that @xmath715 and this is a contradiction to definition [ ndas ] .",
    "so is not true and at least one of @xmath242 and @xmath708 is in @xmath716      among the three representations of @xmath96 were given in , we use the second one in the following .",
    "if is not true , then the following system is infeasible : @xmath717 by farkas lemma , there exist @xmath718 , @xmath719 , and @xmath720 such that : @xmath721 now for each @xmath722 , we multiply both sides of the first inequality in with @xmath723 , then we have : @xmath724 where we used the facts that @xmath725 and @xmath726 .",
    "if we multiply the first set of inequalities in with @xmath727 and add it to the second one we have @xmath728 for all @xmath722 .",
    "@xmath720 and @xmath729 by supergradient inequality .",
    "hence , from , for each @xmath722 , there exists @xmath730 such that @xmath731 which , using , means @xmath732 .",
    "therefore , by the first property of ndas functions , we must have @xmath733 now , it is easy to see that there exists a sequence @xmath734 such that @xmath735 and @xmath736 . by using and the second property of ndas functions @xmath737 times we can write : @xmath738 however , we had @xmath739 which is a contradiction to .",
    "this means the system is feasible and we are done .",
    "consider the first example of example [ wsy ] .",
    "we have @xmath740 , @xmath741 , + @xmath742^{\\top}$ ] , and @xmath743^{\\top}$ ] .",
    "using , the set of centric @xmath29-vectors is @xmath744^{\\top } : z\\in(0,1)\\}.\\ ] ] the set of centric @xmath28-vectors , @xmath447 , is specified by solving @xmath112 and @xmath745 while @xmath83 and we can see that @xmath746^{\\top } : z\\in(0,1)\\}$ ] .",
    "as shown in figure [ fig2 ] , @xmath96s are parallel line segments while @xmath95s are line segments that all intersect at @xmath747^{\\top}$ ] .",
    "now , assume that the function @xmath47 is as follows ( does not depend on @xmath748 ) @xmath749 this function is piecewise linear and it is easy to see that it is concave .",
    "@xmath47 is also differentiable at all the points except the points @xmath750 . at any point that the function is differentiable , the supergradient is equal to the gradient of the function at that point .",
    "hence , we have @xmath751^{\\top}\\}$ ] for @xmath752 and @xmath753^{\\top}\\}$ ] for @xmath754 .",
    "if we consider @xmath47 on @xmath128 , we can see that the maximum of the function is attained at the point that @xmath750 , so .",
    "now assume that we start at . because we have @xmath755 for all centric @xmath28-vectors , @xmath756 , and we can easily find @xmath149 and @xmath562 as and @xmath757^{\\top}$ ] .",
    "the hyperplane passing through @xmath159 is + and since @xmath758 we have @xmath759(y^0)^{-1}=[3 , \\",
    "0],\\end{aligned}\\ ] ] and we can write the hyperplane as @xmath760 . in the next step , we have to choose a point @xmath157 such that @xmath761 .",
    "let us pick @xmath762^{\\top}$ ] for which we can easily find @xmath763^{\\top}$ ] and @xmath764^{\\top}$ ] .",
    "for this point we have @xmath765 , so @xmath766^{\\top}$ ] and the hyperplane passing through @xmath157 is .",
    "the intersection of two hyperplanes on the simplex can be found by solving the following system of equations : @xmath767.\\end{aligned}\\ ] ] the intersection of simplex and the hyperplanes @xmath768 and @xmath769 are shown in figure [ fig5 ] .",
    "the intersection of simplex with + is shown by hatching lines . as can be seen ,",
    "we have : @xmath770      assume that @xmath771 is the point that is chosen and let @xmath772 be the normal vector to the desired hyperplane @xmath233 .",
    "first , we want the hyperplane to contain @xmath234 .",
    "this means that for all centric @xmath559 , the vector @xmath773 is on @xmath233 , i.e. , we have @xmath774 . since @xmath775",
    ", we can put @xmath776 with an arbitrary @xmath285 and we have : @xmath777 now , we want to find @xmath285 such that @xmath778 cuts @xmath235 the same way as + @xmath779 cuts it .",
    "we actually want to find @xmath285 which satisfies the stronger property that @xmath780 for all @xmath781 .",
    "all the points in @xmath235 are of the form @xmath584 , so we must have @xmath782 . since @xmath783 is in the range of @xmath12 , this equation is true if and only if : @xmath784 this means that @xmath785 must be in the @xmath786 , which means @xmath787 .",
    "however , we had from above that @xmath776 and hence : @xmath788 so , the hyperplane with normal vector @xmath776 , where @xmath789 has the required properties .",
    "since this hyperplane cuts @xmath235 the same way as @xmath779 does , we conclude that @xmath790 . therefore , @xmath242 is in the half - space @xmath791 .",
    "the normal of the hyperplane derived in proposition [ w - l5 ] has a nice interpretation with respect to orthogonal projection and the primal - dual scaling @xmath792 .",
    "we have : @xmath793}_{\\pi } ( y^0)^{-1/2}(s^0)^{1/2}g_0 \\nonumber \\\\ & = & ( y^0)^{-1/2}(s^0)^{-1/2 } p ( y^0)^{-1/2}(s^0)^{1/2}g_0,\\end{aligned}\\ ] ] where @xmath794 is the orthogonal projection onto the range of @xmath795 .",
    "note that a main benefit of the hyperplane in proposition [ w - l5 ] is that when we choose a point , we can cut away all the points with the same @xmath29-vector .",
    "now , we prove the following proposition which shows we can cut the simplex with a sequence of hyperplanes such that the intersection of their corresponding half - spaces contain a point from @xmath187 .      as in the proof of proposition [ w - l5 ] , if we set @xmath796 , then the hyperplane contains all the points in @xmath122 . to satisfy the second property , we want to find @xmath286 with the stronger property that @xmath797 for all the centric @xmath102 .",
    "the reason is that we already have @xmath798 . by the choice of @xmath796 , for every centric @xmath28 we have @xmath799 so",
    ", we have @xmath800 and we can continue the above equation as follows : @xmath801 now we can continue in a similar way as in the proof of proposition [ w - l5 ] . since @xmath783 is in the range of @xmath12 , we must have : @xmath802 by the same reasoning , we have : @xmath803 so , the hyperplane with normal vector @xmath796 , where @xmath240 has the required properties ."
  ],
  "abstract_text": [
    "<S> we treat uncertain linear programming problems by utilizing the notion of weighted analytic centers and notions from the area of multi - criteria decision making . after introducing our approach , </S>",
    "<S> we develop interactive cutting - plane algorithms for robust optimization , based on concave and quasi - concave utility functions . </S>",
    "<S> in addition to practical advantages , due to the flexibility of our approach , we are able to prove that under a theoretical framework due to bertsimas and sim @xcite , which establishes the existence of certain convex formulation of robust optimization problems , the robust optimal solutions generated by our algorithms are at least as desirable to the decision maker as any solution generated by many other robust optimization algorithms in the theoretical framework . </S>",
    "<S> we present some probabilistic bounds for feasibility of robust solutions and evaluate our approach by means of computational experiments . </S>"
  ]
}