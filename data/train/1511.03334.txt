{
  "article_text": [
    "high - dimensional data , where the number of variables may greatly exceed the number of observations , has become increasingly more prevalent across a variety of disciplines .",
    "while such data pose many challenges to statisticians , we now have a variety of methods for fitting models to high - dimensional data , many of which are based on the lasso @xcite ; see @xcite for a review of some of the developments .",
    "more recently , huge strides have been made in quantifying uncertainty about parameter estimates .",
    "for the important special case of the high - dimensional linear model , frequentist @xmath0-values for individual parameters or groups of parameters can now be obtained through an array of different techniques @xcite  see @xcite for an overview of some of these methods .",
    "we note that some of these can be used for inference concerning generalised linear models and beyond .",
    "subsampling techniques such as stability selection @xcite and its variant complementary pairs stability selection ( cpss ) @xcite can also be used to select important variables whilst preserving error control in a wider variety of settings .    despite these advances ,",
    "something still lacking from the practitioners toolbox is a corresponding set of diagnostic checks to help assess the validity of , for example , the high - dimensional linear model .",
    "for instance , there are no well - established methods for detecting heteroscedasticity in high - dimensional models , or whether a nonlinear model may be more appropriate .    in this paper",
    ", we introduce an approach for creating diagnostic measures or goodness of fit tests that are sensitive to different sorts of departures from the ` standard ' high - dimensional linear model .",
    "as the measures are derived from examining the residuals following e.g.  a lasso fit to the data , we use the name residual prediction ( rp ) tests . to the best of our knowledge",
    ", it is the first methodology for deriving confirmatory statistical conclusions , in terms of @xmath0-values , to test for a broad range of deviations from a high - dimensional linear model . in section  [ sec : contrib ]",
    "we give a brief overview of the idea , but first we discuss what we mean by goodness of fit in a high - dimensional setting .",
    "consider the gaussian linear model @xmath1 where @xmath2 is a response vector , @xmath3 is the fixed design matrix , @xmath4 is the unknown vector of coefficients , @xmath5 is a vector of uncorrelated gaussian errors , and @xmath6 is the variance of the noise . in the low - dimensional situation where @xmath7 , we may speak of being misspecified such that @xmath8 .",
    "when @xmath9 has full row rank however , any vector in @xmath10 can be expressed as @xmath11 for some @xmath4 , leaving in general no room for nonlinear alternatives .",
    "when restricting to sparse linear models specified by , the situation is different though and misspecification can happen @xcite ; we will take a sparse gaussian linear model as our null hypothesis ( see also theorems  [ thm : maximise_pval ] and [ thm : single_pval_null ] ) .",
    "we discuss an approach to handle a relaxation of the gaussian error assumption in section  [ sec : non - gaussian ] in the supplementary material .",
    "when there is no good sparse approximation to @xmath11 , a high - dimensional linear model may not be an appropriate model for the data - generating process ; a sparse nonlinear model might be more interpretable and may generalise better , for example .",
    "moreover , the lasso and other sparse estimation procedures may have poor performance , undermining the various different high - dimensional inference methods mentioned above that make use of them .",
    "our proposed rp tests investigate whether the lasso is a good estimator of the signal .",
    "let @xmath12 be the residuals following a lasso fit to @xmath9 .",
    "if @xmath13 is such that it can be well - estimated by the lasso , then the residuals should contain very little signal and instead should behave roughly like the noise term @xmath14 . on the other hand ,",
    "if the signal is such that the lasso performs poorly and instead a nonlinear model were more appropriate , for example , some of the ( nonlinear ) signal should be present in the residuals , as the lasso would be incapable of fitting to it .",
    "now if we use a regression procedure that is well - suited to predicting the nonlinear signal ( an example may be random forest @xcite ) , applying this to the residuals and computing the resulting mean residual sum of squares ( rss ) or any other proxy for prediction error will give us a test statistic that under the null hypothesis of a sparse linear model we expect to be relatively large , and under the alternative we expect to be relatively small .",
    "different regression procedures applied to the residuals can be used to test for different sorts of departures from the gaussian linear model .",
    "thus rp tests consist of three components .    1 .   an initial procedure that regresses @xmath15 on @xmath9 to give a set of residuals ; this is typically the lasso if @xmath16 or could be ordinary least squares if @xmath9 is low - dimensional .",
    "2 .   a _ residual prediction method _ ( rp method ) that is suited to predicting the particular signal expected in the residuals under the alternative(s ) under consideration . 3 .",
    "some measure of the predictive capability of the rp method .",
    "typically this would be the residual sum of squares ( rss ) , but in certain situations a cross - validated estimate of prediction error may be more appropriate , for example",
    ".    we will refer to the composition of a residual prediction method and an estimator of prediction error as a _",
    "residual prediction function_. this must be a ( measurable ) function @xmath17 of the residuals and all available predictors , @xmath18 of them in total , to the reals @xmath19 .",
    "for example , if rather than testing for nonlinearity , we wanted to ascertain whether any additional variables were significant after accounting for those in @xmath9 , we could consider the mean rss after regressing the residuals on a matrix of predictors containing both @xmath9 and the additional variables , using the lasso .",
    "if the residuals can be predicted better than one would expect under the null hypothesis with a model as in , this provides evidence against the null .    clearly in order to use rp tests to perform formal hypothesis tests",
    ", one needs knowledge of the distribution of the test statistic under the null , in order to calculate @xmath0-values .",
    "closed form expressions are difficult if not impossible to come by , particularly when the residual prediction method is something as intractable as random forest .",
    "nevertheless , the parametric bootstrap @xcite can be used to give an indication of what to expect under the null .    in this work ,",
    "we show that under certain conditions , the bootstrap can be used , with some modifications , to calibrate any rp test .",
    "thus the rp method can be as exotic as needed in order to detect the particular departure from the null hypothesis that is of interest , and there are no restrictions requiring it to be a smooth function of the data , for example . in order to obtain such a general result ,",
    "the conditions are necessarily strong ; nevertheless , we demonstrate empirically that for a variety of interesting rp tests , bootstrap calibration tends to be rather accurate even when the conditions can not be expected to be met . as well as providing a way of calibrating rp tests",
    ", we also introduce a framework for combining several rp tests in order to have power against a diverse set of alternatives .",
    "some work related to ours here is that of @xcite , @xcite , @xcite and @xcite who study the use of the bootstrap with the ( adaptive ) lasso for constructing confidence sets for the regression coefficients .",
    "work that is more closely aligned to our aim of creating diagnostic measures for high - dimensional models is that of @xcite , though their approach is specifically geared towards variable selection and they do not provide theoretical guarantees within a hypothesis testing framework as we do .      simulating the residuals under the null is particularly simple when rather than using the lasso residuals , ordinary least squares residuals are used .",
    "we study this simple situation in section  [ sec : ols ] not only to help motivate our approach in the high - dimensional setting , but also to present what we believe is a useful method in its own right . in section  [ sec :",
    "combine ] we explain how several rp tests can be aggregated into a single test that combines the powers of each of the tests . in section  [ sec : lasso ] we describe the use of rp tests in the high - dimensional setting , and prove the validity of a calibration procedure based on the parametric bootstrap .",
    "we give several applications of rp tests in section  [ sec : apps ] along with the results of extensive numerical experiments , and conclude with a discussion in section  [ sec : discuss ] .",
    "the supplementary material contains further discussion of the power of rp tests ; a proposal for how to test null hypotheses of the form allowing for more general error distributions ; additional numerical results ; and all of the proofs .",
    "the ` r ` @xcite package ` rptests ` provides an implementation of the methodology .",
    "a simple but nevertheless important version of rp tests uses residuals from ordinary least squares ( ols ) in the first stage . for this",
    ", we require @xmath20 in the set - up of .",
    "let @xmath21 denote the orthogonal projection on to the column space of @xmath9 .",
    "then under the null hypothesis that the model is correct , the scaled residuals @xmath12 are @xmath22 and so their distribution does not depend on any unknown parameters : they form an ancillary statistic .",
    "note that the scaling of the residuals eliminates the dependence on @xmath23 .",
    "it is thus simple to simulate from the distribution of any function of the scaled residuals , and this allows critical values to be calculated for tests using any rp method .",
    "we note that by using ols applied to a larger set of variables as the rp method , and the rss from the resulting fit as the estimate of prediction error , the overall test is equivalent to a partial @xmath24-test for the significance of the additional group of variables . to see this",
    "let us write @xmath25 for an additional group of variables .",
    "let @xmath26 be the orthogonal projection on to all available predictors , that is projection on to @xmath27 , where @xmath28 .",
    "when the rp method is regression of the scaled residuals on to @xmath29 , the resulting rss is @xmath30 since @xmath31 .",
    "we reject for small values of the quantity above , or equivalently large values of @xmath32 which is precisely the @xmath24-statistic for testing the hypothesis in question .",
    "an alternative way to arrive at the @xmath24-test is to first residualise @xmath33 with respect to @xmath9 and define new variables @xmath34 .",
    "let us write @xmath35 for the orthogonal projection on to @xmath36 .",
    "now if our rp method is regression of the scaled residuals on to @xmath36 , we may write our rss as @xmath37 the final equality following from the fact that the column spaces of @xmath9 and @xmath36 and hence @xmath21 and @xmath35 are orthogonal .",
    "it is easy to see that @xmath38 , and so we arrive at the @xmath24-test once more .",
    "we can use each of the two versions of the @xmath24-test above as starting points for generalisation , where rather than using ols as a prediction method , we use other rp methods more tailored to specific alternatives of interest .",
    "the distribution of the output of an rp method under the null hypothesis of a linear model can be computed via simulation as follows . for a given @xmath39",
    "we generate independent @xmath40-vectors with i.i.d .",
    "standard normal components @xmath41 . from",
    "these we form scaled residuals @xmath42 let @xmath29 be the full matrix of predictors . writing the original scaled residuals as @xmath12 we apply our chosen rp function @xmath17 to all of the scaled residuals to obtain a @xmath0-value @xmath43 see also section [ sec : lasso ] for the extension to the case using lasso residuals .    even in situations",
    "where the usual @xmath24-test may seem the natural choice , an rp test with a carefully chosen rp method can often be more powerful against alternatives of interest .",
    "this is particularly true when we aggregate the results of various different rp methods to gain power over a diverse set of alternatives , as we describe in the next section .",
    "in many situations , we would like to try a variety of different rp methods , in order to have power against various different alternatives .",
    "a key example is when an rp method involves a tuning parameter such as the lasso .",
    "each different value of the tuning parameter effectively gives a different rp method .",
    "one could also aim to create a generic omnibus test to test for , say , nonlinearity , heteroscedasticity and correlation between the errors , simultaneously .",
    "to motivate our approach for combining the results of multiple rp tests , we consider the famous diabetes dataset of @xcite .",
    "this has @xmath44 predictors measured for @xmath45 diabetes patients and includes a response that is a quantitative measure of disease progression one year after baseline .",
    "given the null hypothesis of a gaussian linear model , we wish to test for the presence of interactions and quadratic effects . in order to have power against alternatives composed of sparse coefficients for these effects , we consider as rp methods the lasso applied to quadratic effects residualised with respect to the linear terms via ols .",
    "we regress the ols scaled residuals onto the transformed quadratic effects using the lasso with tuning parameters on a grid of @xmath46 values , giving a family of rp tests .",
    "we plot the residual sums of squares from the lasso fits to the scaled residuals in figure  [ fig : multi_lambda ] , as a function of @xmath46 .",
    "also shown are the residual sums of squares from lasso fits to scaled residuals simulated under the null hypothesis of a gaussian linear model , as simulation under the null hypothesis is the general principle which we use for deriving @xmath0-values .    at the point @xmath47 , the observed rss is not drastically smaller than those of the simulated residuals , as the top left density plot shows .",
    "indeed , were we to calculate a @xmath0-value just based on the @xmath47 results corresponding to the @xmath24-test , we would obtain roughly 10% .",
    "the output at @xmath48 , however , does provide compelling evidence against the null hypothesis , as the top right density plot shows . here",
    "the observed rss is far to the left of the support of the simulated residual sums of squares . in order to create a @xmath0-value for the presence of interactions based on all of the output ,",
    "we need a measure of how ` extreme ' the entire blue curve is , with respect to the red curves , in terms of carrying evidence against the null . forming a @xmath0-value based on such a test statistic",
    "is straightforward , as we now explain .",
    "suppose we have residual prediction functions @xmath49 , @xmath50 ( in our example these would be the rss when using the lasso with tuning parameter @xmath51 ) and their evaluations on the true scaled residuals @xmath52 and simulated scaled residuals @xmath53 . writing @xmath54 , let @xmath55 be the curve or vector of rp function evaluations at the @xmath56th scaled residuals , and denote by @xmath57 the entire collection of curves , potentially including the curve for the true scaled residuals @xmath58 , but excluding the @xmath56th curve .",
    "let @xmath59 be any measure of how extreme the curve @xmath60 is compared to the rest of the curves @xmath61 ( larger values indicating more extreme ) . here",
    "@xmath62 can be any function such that @xmath63 does not depend on the particular ordering of the curves in @xmath61 ; we will give a concrete example below .",
    "we can use the @xmath64 to calibrate our test statistic @xmath65 as detailed in the following proposition .",
    "[ prop : combine ] suppose the simulated scaled residuals are constructed as in . setting @xmath66",
    "we have that under the null hypothesis , @xmath67 for all @xmath68 $ ] , so @xmath69 constitutes a valid @xmath0-value .",
    "the result above is a straightforward consequence of the fact that under the null @xmath70 form an exchangeable sequence , and standard results on monte carlo testing ( see @xcite ch .  4 for example ) .",
    "under an alternative , we expect @xmath65 to be smaller and @xmath71 for @xmath72 to be larger on average , than under the null .",
    "thus this approach will have more power than directly comparing @xmath65 to a sample from its null distribution .",
    "we recommend constructing @xmath62 as follows .",
    "let @xmath73 and @xmath74 respectively be the empirical mean and standard deviation of @xmath75 .",
    "we then set @xmath76 the number of standard deviations by which the @xmath56th curve lies below the rest of the curves , maximised along the curve .",
    "the intuition is that were @xmath77 to have a gaussian distribution under the null for each @xmath78 , @xmath79 would be an approximate @xmath0-value based on the @xmath78th rp function , whence @xmath80 would be the minimum of these @xmath0-values . though it would be impossible to match the power of the most powerful test for the alternative in question ( perhaps that corresponding to @xmath48 in our diabetes example ) among the @xmath81 tests considered",
    ", one would hope to come close .",
    "we stress however that this choice of @xmath62 yields valid @xmath0-values regardless of the distribution of @xmath77 under the null .    using this approach with a grid of @xmath82 @xmath46 values ,",
    "we obtain a @xmath0-value of under 1% for the diabetes example ; clear evidence that a model including only main effects is inappropriate for the data .",
    "further simulations demonstrating the power of this approach are presented in section  [ sec : apps ] .",
    "when the null hypothesis is itself high - dimensional , we can use lasso residuals in the first stage of the rp testing procedure . although unlike scaled ols residuals , scaled lasso residuals are not ancillary , we will see that under certain conditions , the distribution of scaled lasso residuals are not wholly sensitive to the parameters @xmath83 and @xmath84 in .",
    "let us write @xmath85 for the scaled lasso residuals when the tuning parameter is @xmath46 ( in square - root parametrisation , see below ) : @xmath86 note that under @xmath87 and @xmath88 sometimes we will omit the first argument of @xmath89 for convenience in which case it will always be the true parameter value under the null , @xmath83 .",
    "here we are using the lasso in the square - root parametrisation @xcite rather than the conventional version where the term in the objective assessing the model fit would be @xmath90 .",
    "we note that the two versions of the lasso have identical solution paths but these will simply be parametrised differently . for this reason",
    ", we will simply refer to as the lasso solution .",
    "note that while the lasso solution may potentially be non - unique , the residuals are always uniquely defined as the fitted values from a lasso fit are unique ( see @xcite , for example ) . throughout",
    "we will assume that the columns of @xmath9 have been scaled to have @xmath91-norm @xmath92 .",
    "we set out our proposal for calibrating rp tests based on lasso residuals using the parametric bootstrap in algorithm  [ alg : lasso1 ] below .    1 .",
    "let @xmath93 be an estimate of @xmath83 , typically a lasso estimate selected by cross - validation .",
    "2 .   set @xmath94 .",
    "form @xmath95 scaled simulated residuals @xmath96 where the @xmath97 are i.i.d .",
    "draws from @xmath98 , and @xmath46 chosen according to the proposal of @xcite .",
    "4 .   based on the scaled simulated residuals",
    "@xmath96 , compute a @xmath0-value or use these to form an aggregated @xmath0-value as described in section  [ sec : combine ] .    in the following section we aim to justify the use of the parametric bootstrap from a theoretical perspective and also discuss the particular choices @xmath99 and @xmath46 used above .      given @xmath100 and a set @xmath101 , let @xmath102 be the subvector of @xmath103 with components consisting of those indexed by @xmath104 . also for a matrix @xmath105 ,",
    "let @xmath106 be the submatrix of @xmath105 containing those columns indexed by @xmath104 , and let @xmath107 , the @xmath108th column .",
    "the following result shows that if @xmath109 , with the sign function understood as being applied componentwise , we have partial ancillarity of the scaled residuals . in the following",
    "we let @xmath110 be the support set of @xmath83 .",
    "[ thm : lambda_fixed ] suppose @xmath93 is such that @xmath111 . for @xmath112 and @xmath113 , consider the deterministic set @xmath114 then we have that for all @xmath115 , @xmath116 provided @xmath117 .    in words ,",
    "provided the error @xmath118 is in the set @xmath119 and conditions for @xmath93 and @xmath120 are met , the scaled residuals from a lasso fit to @xmath121 are precisely equal to the scaled residuals from a lasso fit to @xmath122 .",
    "note that all of the quantities in the result are deterministic . under reasonable conditions and for a sensible choice of @xmath46 ( see theorem  [ thm : maximise_pval ] ) , when @xmath123 , we can expect the event @xmath115 to have large probability .",
    "thus theorem  [ thm : lambda_fixed ] shows that the scaled residuals are not very sensitive to the parameter @xmath84 or to the magnitudes of the components of @xmath83 , but instead depend largely on the signs of the latter .",
    "it is the square - root parametrisation that allows the result to hold for a large range of values of @xmath120 , and in particular for all @xmath120 sufficiently small .",
    "theorem  [ thm : lambda_fixed ] does not directly justify a way to simulate from the distribution of the scaled lasso residuals as in algorithm [ alg : lasso1 ] since the sign pattern of @xmath93 must equal that of @xmath83 .",
    "accurate estimation of the sign pattern of @xmath83 using the lasso requires a strong irrepresentable or neighbourhood stability condition @xcite .",
    "nevertheless , we now show that we can modify algorithm  [ alg : lasso1 ] to yield provable error control under more reasonable conditions . later though we argue heuristically that the same error control should hold for algorithm  [ alg : lasso1 ] in a wide range of settings .      under a so - called beta - min condition ( see theorem  [ thm : maximise_pval ] below ) , with high probability we can arrive at an initial estimate of @xmath83 , @xmath124 via the lasso for which @xmath125 , and where @xmath126 . with such a @xmath124 , we can aim to seek a threshold @xmath127 for which the lasso applied only on the subset of variables @xmath128 where @xmath129 yields an estimate that has the necessary sign agreement with @xmath83 .",
    "this then motivates algorithm  [ alg : lasso2 ] based on maximising over the candidate @xmath0-values obtained through different @xmath83 estimates derived from applying the lasso to different subsets of the initial active set ( see also @xcite which introduces a related scheme ) .    1",
    ".   let @xmath130 be the lasso estimate of @xmath83 .",
    "2 .   let @xmath131 and suppose @xmath132 are the non - zero components of @xmath124 arranged in order of non - decreasing magnitude .",
    "define @xmath133 .",
    "3 .   for @xmath134",
    "let @xmath135 be the lasso estimate from regressing @xmath15 on @xmath136 .",
    "further set @xmath137 .",
    "4 .   using each of the @xmath135 in turn and @xmath138 , generate sets of residuals @xmath139 where the @xmath97 are i.i.d",
    ".  draws from @xmath98 .",
    "use these to create corresponding @xmath0-values @xmath140 for rp tests based on or the method introduced in section  [ sec : combine ] .",
    "output @xmath141 as the final approximate @xmath0-value .",
    "note we do not recommend the use of algorithm  [ alg : lasso2 ] in practice ; we only introduce it to facilitate theoretical analysis which sheds light on our proposed procedure algorithm  [ alg : lasso1 ] .",
    "let @xmath142 and @xmath131 .",
    "the theorem below gives conditions under which with high probability , @xmath143 and residuals from responses generated around @xmath144 will equal the true residuals .",
    "this then shows that the maximum @xmath0-value @xmath69 will in general be a conservative @xmath0-value as it will always be at least as large as @xmath145 , on an event with high probability .    as well as a beta - min condition ,",
    "the result requires some relatively mild assumptions on the design matrix .",
    "let @xmath146 .",
    "the restricted eigenvalue @xcite is defined by @xmath147 for a matrix @xmath148 , @xmath149 and @xmath150 , the compatibility factor @xmath151 @xcite is given by @xmath152 when either of the final two arguments are omitted , we shall take them to be @xmath153 and @xmath9 respectively ; the more general form is required in section  [ sec : single ] .",
    "the sizes of @xmath154 of @xmath155 quantify the ill - posedness of the the design matrix @xmath9 ; we will require @xmath156 for some @xmath157 .",
    "note that in the random design setting where the rows of @xmath9 are i.i.d .",
    "multivariate normal with the minimum eigenvalue of the covariance matrix bounded away from zero , the factors and can be thought of as positive constants in asymptotic regimes where @xmath158 .",
    "we refer the reader to @xcite and @xcite for further details .",
    "[ thm : maximise_pval ] suppose the data follows the gaussian linear model .",
    "let @xmath159 with @xmath160 and @xmath161 .",
    "suppose for @xmath150 that @xmath162 assume a beta - min condition @xmath163 then for all @xmath68 $ ] , @xmath164 where @xmath165 as @xmath166 .",
    "we see that the beta - min condition needed to ensure @xmath167 has high probability is of the form @xmath168 .",
    "although the beta - min condition may be regarded as somewhat strong , the conclusion is correspondingly strong : any rp method or collection of rp methods with arbitrary @xmath62 for combining tests can be applied to the residuals and the result remains valid .",
    "however , given a particular rp method , exact equality of the residuals would not be needed to guarantee a result of the form .",
    "it is also worth noting that the distribution of @xmath169 does not depend directly on @xmath83 but rather on @xmath11 .",
    "thus in principle , only conditions on fitted values are required rather than the estimates of @xmath83 themselves .",
    "furthermore , the conditions are only required under the null .",
    "for example , if the alternative of interest was that an additional variable @xmath170 was related to the response after accounting for those in the original design matrix @xmath9 , no conditions on the relationship between @xmath9 and @xmath170 are required for the test to be valid .",
    "in fact , in this special case , we have a much stronger result ( see theorems  [ thm : single_pval_null ] and [ thm : single_pval_alt ] ) which shows that neither the beta - min condition nor the maximisation over candidate @xmath0-values of algorithm  [ alg : lasso2 ] is necessary for error control to hold . more generally , in our experiments we have found @xmath171 is usually equal to or close to the maximum @xmath69 for large @xmath95 across a variety of settings . thus selecting @xmath171 rather than performing the maximisation ( which amounts to algorithm  [ alg : lasso1 ] ) is able to deliver conservative error control as evidenced by the simulations in section  [ sec : apps ] .    a heuristic explanation for why the error is controlled is that typically the amount of signal remaining in @xmath172 increases with @xmath108 , simply because typically @xmath173 also increases with @xmath108 .",
    "this can result in the prediction error of a procedure applied to the various residuals decreasing with @xmath108 because the signal - to - noise ratios tend to be increasing ; thus the @xmath0-values tend to increase with @xmath108 .",
    "in addition , when the lasso performs well , we would expect residuals to contain very little signal , and any differences in the signals contained in @xmath174 and @xmath175 to be smaller still , particularly when @xmath176 and @xmath177 are close .",
    "typically the rp function will be insensitive to such small differences since they are unlikely to be too close to directions against which power is desired .",
    "we now discuss the choices of @xmath93 , @xmath46 and @xmath120 in algorithm  [ alg : lasso1 ] .",
    "[ [ choice - of - checkboldsymbolbeta . ] ] choice of @xmath93 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in view of the preceding discussion , it suffices for @xmath93 to satisfy a screening - type property : we would like the support of @xmath93 to contain that of @xmath83 .",
    "though theorem  [ thm : maximise_pval ] suggests a fixed @xmath46 , since @xmath93 only needs to be computed once , we can use cross - validation .",
    "this is the perhaps the most standard way of producing an estimate that performs well for screening ( see for example section 2.5.1 of @xcite ) .",
    "if the folds for cross - validation are chosen at random , the estimate will have undesirable randomness beyond that of the data .",
    "we thus suggest taking many random partitions into folds and using an estimate based on a @xmath46 that minimises the cross - validation error curve based on all of the folds used . in our simulations in section  [ sec : apps ] we partition the observations into 10 random folds a total of 8 times .    [",
    "[ choice - of - checksigma . ] ] choice of @xmath120 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the normalised rss is perhaps the most natural choice for @xmath178 ( see also @xcite ) , though as theorem  [ thm : lambda_fixed ] suggests , the results are essentially unchanged when this is doubled or halved , for example .    [",
    "[ choice - of - lambda - for - the - lasso - residuals . ] ] choice of @xmath46 for the lasso residuals .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the choice of @xmath46 should be such that with high probability , the resulting estimate contains the support of @xmath83 ( see theorem  [ thm : lambda_fixed ] ) .",
    "though theorem  [ thm : maximise_pval ] suggests taking @xmath179 for @xmath180 , the restriction on @xmath104 is an artefact of basing our result on oracle inequalities from @xcite , which place relatively simple conditions on the design .",
    "@xcite has a more involved theory which suggests a slightly smaller @xmath46 .",
    "we therefore use their method , the default in the ` r ` package @xcite , as a convenient fixed choice of @xmath46 .      here",
    "we consider the collection of null hypotheses @xmath181 and their corresponding alternatives that @xmath182 .",
    "note that for this setting there are many other approaches that can perform the required tests .",
    "our aim here is to show that rp tests can be valid under weaker assumptions than those laid out in theorems  [ thm : maximise_pval ] , and moreover that the simpler approach of algorithm  [ alg : lasso1 ] can control type i error .",
    "we begin with some notation . for @xmath183 and @xmath100 let @xmath184 and @xmath185 . for each variable @xmath108 ,",
    "our rp method will be a least squares regression onto a version of @xmath186 that has been residualised with respect to @xmath187 .",
    "since in the high - dimensional setting @xmath187 will typically have full row rank , an ols regression of @xmath186 on @xmath187 will return the 0-vector as residuals .",
    "hence we will residualise @xmath186 using the square - root lasso : @xmath188 this rp method is closely related to the pioneering idea by @xcite and similar to that of @xcite , who consider using the regular lasso ( without the square - root parametrisation ) at each stage . if @xmath186 were not residualised with respect to @xmath187 , and the regular lasso were used , the resulting rp method would be similar to that of @xcite .",
    "let @xmath189 be the residual @xmath190 .",
    "note for each @xmath108 we may write @xmath191 where @xmath192 . let @xmath193 be the square - root lasso regression of @xmath15 on to @xmath187 with tuning parameter @xmath46 .",
    "our rp function will be the rss from ols regression of the scaled lasso residuals @xmath194 on to @xmath189 .",
    "note this is an rp function even though it involves the residualised version of @xmath186 , @xmath189 ; the latter is simply a function of @xmath9 .",
    "equivalently , we can consider the test statistic @xmath195 with @xmath196 defined by @xmath197 note that @xmath196 is simply a regularised partial correlation between @xmath15 and @xmath186 given @xmath187 .",
    "the bootstrap version is @xmath198 where @xmath199 , @xmath200 and @xmath201 is the lasso regression of @xmath202 on @xmath187 . here",
    "we will consider taking @xmath203 where @xmath89 is the square - root lasso regression of @xmath15 on the full design matrix @xmath9 .",
    "as before , let @xmath153 be the support of @xmath83 , which without loss of generality we will take to be @xmath204 , and also let @xmath205 be the set of true nulls .",
    "assume @xmath5 .",
    "the following result shows that only a relatively mild compatibility condition is needed in order to ensure that the type i error is controlled .",
    "we consider an asymptotic regime with @xmath206 where @xmath207 and @xmath0 are all allowed to vary with @xmath40 though we suppress this in the notation . in the following we denote the cumulative distribution function of the standard normal by @xmath208 .",
    "[ thm : single_pval_null ] let @xmath209 for some constant @xmath210 and suppose that @xmath211 for some @xmath212 .",
    "let @xmath213 for some constant @xmath214 .",
    "define @xmath215 .",
    "then @xmath216    we see that a bootstrap approach can control the type i error uniformly across the noise variables and @xmath217 .",
    "we note that this result is for a fixed design @xmath9 and does not require any sparsity assumptions on the inverse covariance matrix of a distribution that could have generated the rows of @xmath9 @xcite , for example .",
    "[ thm : single_pval_alt ] let @xmath46 and @xmath218 be as in theorem  [ thm : single_pval_null ] .",
    "assume that for some @xmath219 with @xmath212 there is a sequence of sets @xmath220 such that @xmath221 and @xmath222 where @xmath223 .",
    "further assume that @xmath224 .",
    "define @xmath225 . then @xmath226    if @xmath227 and hence @xmath228 were sparse , we could take @xmath229 as the set of nonzeroes and the second condition involving @xmath230 would be vacuous",
    ". this would be the case with high probability in the random design setting where @xmath9 has i.i.d .",
    "gaussian rows with sparse inverse covariance matrix @xcite",
    ". however , @xmath231 can also have many small coefficients provided they have small @xmath232-norm .",
    "the result above shows that the power of our method is comparable to the proposals of @xcite and @xcite based on the debaised lasso .",
    "if @xmath233 as would typically be the case in the random design setting discussed above , we would have power tending to 1 if @xmath234 but @xmath235 .",
    "further results on power to detect nonlinearities are given in section  [ sec : power ] of the supplementary material .",
    "the theoretical results do not suggest any real benefit from using the bootstrap as to test hypotheses we can simply compare @xmath196 to a standard normal distribution .",
    "however our experience has been that this can be slightly anti - conservative in certain settings .",
    "instead , we propose to use the bootstrap to estimate the mean and standard deviation of the null distribution of the @xmath196 by computing the empirical mean @xmath236 and standard deviation @xmath237 of @xmath95 samples of @xmath238 .",
    "then we take as our @xmath0-values @xmath239 $ ] .",
    "this construction of @xmath0-values appears to yield tests that very rarely have size exceeding their nominal level . indeed in all our numerical experiments we found no evidence of this violation occurring .",
    "an additional advantage is that only a modest number of bootstrap samples is needed to yield the sort of low @xmath0-values that could fall below the threshold of a typical multiple testing procedure .",
    "we recommend choosing @xmath95 between 50 and 100 .",
    "using our bootstrap approach for calibration presents a significant computational burden when it is applied to test for the significance of each of a large number of variables in turn . some modifications to algorithm  [ alg : lasso1 ]",
    "can help to overcome this issue and allow this form of rp tests to be applied to typical high - dimensional data with large @xmath0 .",
    "firstly rather than using cross - validation to choose @xmath46 for computation of @xmath240 , we recommend using the fixed @xmath46 of @xcite ( see also section  [ sec : practice ] ) .",
    "the tuning parameter @xmath218 required to compute @xmath189 can be chosen in the same way , and we also note that these nodewise regressions only need to be done once rather than for each bootstrap sample .",
    "great computational savings can be realised by first regressing @xmath15 on @xmath9 to yield coefficients @xmath89 .",
    "writing @xmath241 , we know that for each @xmath242 , @xmath243 , so we only need to compute @xmath193 for those @xmath108 in @xmath244 . the same logic can be applied to computation of @xmath245 for the bootstrap replicates .",
    "we also remark that approaches for directly simulating lasso estimates @xcite may be used to produce simulated residuals .",
    "these have the potential to substantially reduce the computational burden ; not just in the case of testing significance of individual predictors but for rp tests in general .",
    "here we return to the problem of testing for quadratic effects in the diabetes dataset used in the example of figure  [ fig : multi_lambda ] . in order to further investigate the power of the aggregate rp test constructed through lasso regressions on a grid of 100 @xmath46 values as described in section  [ sec : combine ] , we created artificial signals from which we simulated responses .",
    "the signals ( mean responses ) were constructed by selecting at random @xmath246 of the quadratic terms and giving these coefficients generated using i.i.d .",
    "unif@xmath247 $ ] random variables .",
    "the remaining coefficients for the variables were set to 0 , so @xmath246 determined the sparsity level of the signal .",
    "responses were generated by adding i.i.d .",
    "gaussian noise to the signals , with variance chosen such that the @xmath24-test for the presence of quadratic effects has power 0.5 when the size is fixed at 0.05 .",
    "we created 25 artificial signals at each sparsity level @xmath248 .",
    "note that the total number of possible quadratic effects was 54 ( as one of the variables was binary ) , so the final sparsity level represents fully dense alternatives where we might expect the @xmath24-test to have good power .",
    "we note however that the average power of the @xmath24-test in the dense case rests critically on the form of the covariance between the generated quadratic coefficients , with optimality guarantees only in special circumstances ( see section 8 of @xcite ) .",
    "for the rp tests , we set the number of bootstrap samples @xmath95 to be 249 .",
    "we also compare the power of rp tests to the _ global test _ procedure of @xcite .",
    "the results , shown in figure  [ fig : f_test_diabetes ] , suggest that rp tests can outperform the @xmath24-test in a variety of settings , most notably when the alternative is sparse , but also in dense settings . when there are small effects spread out across many variables ( @xmath249 ) , the global test tends to do best ; indeed in such settings it is optimal . in the sparser settings , rp tests perform better .    ; the power of the @xmath24-test is fixed at 0.5 and shown as a dotted line.[fig : f_test_diabetes ] ]      in this section we report the results of using rp tests ( algorithm  [ alg : lasso1 ] ) tailored to detect particular alternatives on a variety of simulated examples where the null hypothesis is high - dimensional .",
    "we investigate both control of the type i error and the powers of the procedures .",
    "our examples are inspired by @xcite .",
    "we use @xmath250 simulated design matrices with @xmath251 and @xmath252 except for the setting where we test for heteroscedasticity in which we increase @xmath40 to 300 in order to have reasonable power against these alternatives .",
    "the rows of the matrices are distributed as @xmath253 with @xmath254 given by the three types described in table  [ tab : sim ] .",
    ".[tab : sim]generation of @xmath255 . [ cols= \"",
    "> , < \" , ]     in addition to the randomly generated design matrices , we also used a publicly available real design matrix from gene expression data of bacillus subtilis with @xmath256 observations and @xmath257 predictors @xcite . similarly to @xcite , in order to keep the computational burden of the simulations manageable , we reduced the number of variables to @xmath251 by selecting only those with the highest empirical variance . for each of the four design settings",
    ", we generated 25 design matrices ( those from the real data were all the same ) .",
    "the columns of the design matrices were mean - centred and scaled to have @xmath91-norm @xmath92 .    in order to create responses under the null hypothesis ,",
    "for each of these 100 design matrices , we randomly generated a vector of coefficients @xmath83 as follows .",
    "we selected a set @xmath153 of 12 variables from @xmath258 .",
    "we then assigned @xmath259 and each @xmath260 with @xmath261 was generated according to unif@xmath262 $ ] independently of other coefficients .",
    "this form of signal is similar to the most challenging signal settings considered in @xcite .",
    "other constructions for generating the non - zero regression coefficients are considered in section  [ sec : num ] in the supplementary material .",
    "given @xmath9 and @xmath83 , we generated @xmath263 responses according to the linear model with @xmath264 .",
    "thus in total we have evaluated the type i error control of our procedures on over 100 data - generating processes .",
    "the number of bootstrap samples @xmath95 used was 100 when testing for significance of individual predictors and fixed at 249 in all other settings .",
    "we now explain interpretation of the plots in figures  [ fig : groups][fig : hetero ] ; a description of figure  [ fig : single ] is given in section  [ sec : apps_single ] .",
    "the top and bottom rows of each of figures  [ fig : groups][fig : hetero ] concern settings under null and alternative hypotheses respectively .",
    "thin red curves trace the empirical cumulative distribution functions ( cdfs ) of the @xmath0-values obtained using rp tests , whilst thin blue curves , where shown , represent the same for debiased lasso - based approaches . in all plots , thickened coloured curves are averages of their respective thin coloured curves ; note these are averages over different simulation settings .",
    "the black dashed line is the cdf of the uniform distribution ; thus we would hope for the empirical cdfs to be close to this in the null settings ( top rows ) , and rise above it in the bottom rows indicating good power .",
    "of course , even if all of the @xmath0-value distributions were stochastically larger than uniform so the type i error was always controlled , we would not expect their estimated distributions i.e.  the empirical cdfs to always lie below the dashed line .",
    "the black dotted curve allows us to assess type i error control across the simulation settings more easily .",
    "it is constructed such that in each of the plots , were the type i error to be controlled exactly , we would expect on average 1 out of the 25 empirical cdfs for rp tests to escape above the region the line encloses .",
    "thus several curves not completely enclosed under the dotted line in a given plot would indicate poor control of type i error .",
    "more precisely , the line is computed as follows .",
    "let @xmath265 be the upper @xmath266 quantile of a @xmath267 distribution .",
    "note this is the marginal distribution of @xmath268 where @xmath269 is the empirical cdf of @xmath95 samples from the uniform distribution on @xmath270 .",
    "the curve then traces @xmath265 with @xmath266 chosen such that @xmath271 } ( \\hat{u}(x ) - q_\\alpha(x ) ) > 0\\big\\}=1/25.\\ ] ] we see that across all of the data - generating processes and for each of the three rp testing methods , it appears the size never exceeds the nominal level by a significant amount .",
    "moreover the same holds for the additional 100 data - generating processes whose results presented in the supplementary material : the type i error is controlled well uniformly across all settings considered .    we now describe the particular rp tests used in figures  [ fig : groups][fig : hetero ] , and the alternatives investigated , as well as the results shown in figure  [ fig : single ] concerning testing for the significance of individual predictors as detailed in section  [ sec : single ] .",
    "-values from rp tests ( red ) and the debiased lasso ( blue ) under the null ( top row ) and alternative ( bottom row ) respectively .",
    "the dashed line equals the 45 degree line corresponding to the unif@xmath272 $ ] distribution function , and the dotted curve is explained in the main text .",
    "[ fig : groups ] ]    -values from rp tests ( red ) and the debiased lasso ( blue ) under the null ( top row ) and alternative ( bottom row ) respectively .",
    "the dashed line equals the 45 degree line corresponding to the unif@xmath272 $ ] distribution function , and the dotted curve is explained in the main text .",
    "[ fig : groups ] ]    .[fig : nonlinear ] ]    .[fig : nonlinear ] ]    .[fig : hetero ] ]    .[fig : hetero ] ]     null ( top row ) and @xmath273 true variables ( bottom row ) selected at various threshold levels with rp tests ( red ) and the debiased lasso ( blue).[fig : single ] ]     null ( top row ) and @xmath273 true variables ( bottom row ) selected at various threshold levels with rp tests ( red ) and the debiased lasso ( blue).[fig : single ] ]      we consider the problem of testing the null hypothesis @xmath274 within linear model .",
    "one approach to is to regress each column of @xmath275 on to @xmath276 in turn using the square - root lasso ( c.f .",
    "section  [ sec : single ] ) , and consider a matrix of residuals @xmath277 .",
    "we may then use lasso regression on to @xmath278 as our family of rp methods and combine the resulting test statistics as in section  [ sec : combine ] .",
    "we use this approach on our simulated data and the results are displayed in red in figure  [ fig : groups ] . for the null settings ( top row ) we took @xmath279 to be a randomly selected set of size @xmath280 containing @xmath153 .",
    "thus under the null , @xmath281 had 12 non - zero components whilst @xmath282 .",
    "the alternatives , corresponding to the bottom row , also modify the signal such that @xmath283 is non - zero ( in addition to @xmath284 being non - zero as was the case under the null ) with coefficients generated in exactly the same way as for @xmath284 and @xmath104 being a randomly selected set of @xmath285 variables chosen from @xmath286 .",
    "the blue lines trace the empirical cdfs of @xmath0-values constructed using the debiased lasso proposal of @xcite and implemented in the ` hdi ` package @xcite for ` r ` . more specifically",
    ", we use the minimum of the @xmath0-values associated with each of the coefficients in @xmath286 ( see section 2.3 of @xcite ) as our test statistic , and calibrate this using the westfall  young procedure @xcite as explained in @xcite .",
    "this ensures that no power is lost due to correlations among the individual @xmath0-values , as would be the case with bonferroni correction , for example .",
    "remaining parameters were set to the defaults in the ` hdi ` package .    although the sizes of the debiased lasso - based tests averaged over the equal correlation design examples are very close to the nominal level , this is due to the several settings where the size exceeds the desired level being compensated for by other examples where the tests are more conservative . on the other hand ,",
    "rp tests have slightly conservative type i error control across all the examples , and greater power among the toeplitz and exponential decay settings .      in order to test for nonlinearity",
    ", we consider an rp method based on random forest @xcite .",
    "we used the default settings for random forest as implemented by @xcite , but rather than using a direct application to the residuals we apply it to the equicorrelation set : the set of variables with maximum absolute correlation with the residuals .",
    "this is invariably the set of variables selected in the initial lasso fit , though in situations where the lasso solution is not unique this will in general be a superset of the support of any lasso solution .",
    "using this smaller set of variables reduces the computational burden of a random forest fit , and also gives the test greater power in situations where the variables contributing to the nonlinear signal also feature in sparse linear approximations to the truth . applying a random forest to the entire set of variables",
    "may have slightly greater power when this is not the case , but would have greatly diminished power in the more natural situations where this this holds .",
    "rather than using the rss from the random forest fits as our proxy for prediction error , we use the out of bag error .",
    "this has the advantage of being more insensitive to the size of the equicorrelation set and tends to result in greater power .    to create the nonlinear signal for the alternative settings",
    ", we randomly divided @xmath153 into four groups of three .",
    "each variable @xmath287 was transformed via a sigmoid composed with a random affine mapping as below : @xmath288^{-1}.\\ ] ] here @xmath289 independently .",
    "the transformed variables in each group were multiplied together , and a linear combination of these resulting products with unif@xmath290 $ ] generated coefficients formed the nonlinear component of the signal .",
    "this nonlinear signal was then scaled such that the residuals from an ols fit to the variables in @xmath153 had an empirical variance of 2 , and finally added to the linear signal .",
    "the results displayed in figure  [ fig : nonlinear ] show that rp tests are able to deliver reasonable power in many of the settings considered , though the real design examples appear to be particularly challenging .      as testing for heteroscedasticity in a high - dimensional setting is rather challenging , here we increase the number of observations for the simulated design settings to @xmath291 in order to have reasonable power against the alternative .",
    "the data - generation procedure under the null was left unchanged . in order to generate vectors of variances for the alternative settings , we randomly selected 3 variables from @xmath153 and formed a linear combination of these variables with unif@xmath262 $ ] coefficients .",
    "a constant was then added , so the minimum component was 0.01 , and finally the vector was scaled so the average of its components was 1 .",
    "this vector then determined the variance of normal errors added to the signal .    to detect this heteroscedasticity",
    ", we used a family of rp methods given by lasso regression of the absolute values of the residuals onto the equicorrelation set .",
    "the results are shown in figure  [ fig : hetero ] .",
    "rp tests are able to deliver reasonable power in the the simulated design settings , but do struggle to detect the heteroscedasticity with the real design which has a lower number of observations ( @xmath256 ) .",
    "figure  [ fig : single ] shows the results of using rp tests as described in section  [ sec : single ] to test hypotheses @xmath292 .",
    "the red curves give the average proportions of false ( top row ) and true positives ( bottom row ) that would be selected given @xmath0-value thresholds varying along the @xmath287-axis .",
    "thus for example in order to obtain the expected number of false positives selected at a given threshold , the @xmath293 values should be multiplied by @xmath294 .",
    "the blue curves display the same results for the debaised lasso as implemented in the ` hdi ` package .",
    "the dashed 45 degree line gives the expected proportion of false positives that would be incurred by an exact test .",
    "we see that even at the low @xmath0-value thresholds particularly relevant for multiple testing correction , rp tests give consistent error control whilst also delivering superior or equal power .",
    "such error control effectively requires accurate knowledge of the extreme tails of the null distribution of the test statistics .",
    "we see here that the debiased lasso approach is not always able to achieve this in the toeplitz and exponential decay settings , and indeed error control for multiple testing is rare among the currently available methods @xcite .",
    "the rp testing methodology introduced in this work allows us to treat model checking as a prediction problem : that of fitting any ( prediction ) function to the scaled residuals from ols or lasso .",
    "this makes the problem of testing goodness of fit amenable to the entire range of prediction methods that have been developed across statistics and machine learning .",
    "we have investigated here rp tests for detecting significant single or groups of variables , heteroscedasticity , or deviations from linearity , and we expect that effective rp methods can also be found for testing for correlated errors , heterogeneity and other sorts of departures from the standard gaussian linear model .",
    "related ideas should be applicable to test for model misspecification in high - dimensional generalised linear models , for example .",
    "48 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    a.  belloni , v.  chernozhukov , and l.  wang .",
    "square - root lasso : pivotal recovery of sparse signals via conic programming .",
    "_ biometrika _ , 980 ( 4):0 791806 , 2011 .    p.  bickel , y.  ritov , and a.  tsybakov . .",
    "_ , 37:0 17051732 , 2009 .",
    "l.  breiman . .",
    "_ machine learning _ ,",
    "45:0 532 , 2001 .",
    "p.  bhlmann . .",
    "_ bernoulli _ , 19:0 12121242 , 2013 .",
    "p.  bhlmann and s.  van de geer .",
    "_ statistics for high - dimensional data : methods , theory and applications_. springer , 2011 .",
    "p.  bhlmann and s.  van  de geer .",
    "high - dimensional inference in misspecified linear models . _ electron .",
    "j.  statist .",
    "_ , 9:0 14491473 , 2015 .",
    "p.  bhlmann , m.  kalisch , and l.  meier .",
    "high - dimensional statistics with a view toward applications in biology .",
    "_ annual review of statistics and its application _",
    ", 1:0 255278 , 2014 .",
    "l.  camponovo . on the validity of the pairs bootstrap for lasso estimators .",
    "_ biometrika , to appear _ , 2014 .",
    "a.  chatterjee and s.  lahiri .",
    "asymptotic properties of the residual bootstrap for lasso estimators .",
    "am .  math .",
    "_ , 1380 ( 12):0 44974509 , 2010 .    a.  chatterjee and s.  n. lahiri .",
    "bootstrapping lasso estimators .",
    "_ j.  am .  statist .  ass .",
    "_ , 1060 ( 494):0 608625 , 2011 .",
    "a.  c. davison and d.  v. hinkley . _ bootstrap methods and their application _ , volume  1 .",
    "cambridge university press , 1997 .",
    "r.  dezeure , p.  bhlmann , l.  meier , and n.  meinshausen . .",
    "_ statistical science _ , 30:0 533558 , 2015",
    "b.  efron and r.  j. tibshirani . _ an introduction to the bootstrap_. crc press , 1994 .",
    "b.  efron , t.  hastie , i.  johnstone , and r.  tibshirani . .",
    "_ , 32:0 407451 , 2004 .",
    "j.  j. goeman , s.  a. van  de geer , and h.  c. van houwelingen . .",
    "j.  r.  statist .",
    "b _ , 68:0 477493 , 2006 .",
    "a.  javanmard and a.  montanari . .",
    "j.  mach .",
    "_ , 15:0 28692909 , 2014 .",
    "v.  koltchinskii .",
    "the dantzig selector and sparsity oracle inequalities .",
    "_ bernoulli _ , 150 ( 3):0 799828 , 2009 .",
    "a.  liaw and m.  wiener .",
    "classification and regression by randomforest .",
    "_ r news _ , 20 ( 3):0 1822 , 2002 .",
    "url http://cran.r - project.org / doc / rnews/.    r.  lockhart , j.  taylor , r.  j. tibshirani , and r.  tibshirani . .",
    "_ , 42:0 413468 , 2014 .",
    "n.  meinshausen .",
    "group bound : confidence intervals for groups of variables in sparse high dimensional regression without assumptions on the design .",
    "_ j.  r.  statist .",
    "b _ , 770 ( 5):0 923945 , 2015 .",
    "n.  meinshausen and p.  bhlmann . .",
    "_ , 34:0 14361462 , 2006 .",
    "n.  meinshausen and p.  bhlmann . .",
    "j.  r.  statist .",
    "b _ , 72:0 417473 , 2010 .",
    "n.  meinshausen , l.  meier , and p.  bhlmann . .",
    "j.  am .",
    "statist .  ass .",
    "_ , 104:0 16711681 , 2009 .",
    "y.  nan and y.  yang .",
    "variable selection diagnostics measures for high - dimensional regression . _ j.  computnl graph .",
    "_ , 230 ( 3):0 636656 , 2014",
    ".    y.  ning and h.  liu .",
    "a general theory of hypothesis tests and confidence regions for sparse high dimensional models .",
    "_ arxiv preprint arxiv:1412.8765 _ , 2014 .    .",
    "_ r : a language and environment for statistical computing_. r foundation for statistical computing , vienna , austria , 2005 .",
    "url http://www.r-project.org .",
    "s.  reid , r.  tibshirani , and j.  friedman .",
    "a study of error variance estimation in lasso regression .",
    "_ statistica sinica , to appear _ , 2016 .",
    "r.  d. shah and r.  j. samworth .",
    "variable selection with error control : another look at stability selection .",
    "_ j.  r.  statist .  soc .",
    "b _ , 750 ( 1):0 5580 , 2013 .",
    "_ scalreg : scaled sparse linear regression _ , 2013 .",
    "url https://cran.r-project.org/package=scalreg .",
    "r package version 1.0 .",
    "t.  sun and c .- h . zhang . scaled sparse linear regression .",
    "_ biometrika _ , 990 ( 4):0 879898 , 2012 .",
    "t.  sun and c .- h .",
    "sparse matrix inversion with scaled lasso .",
    "_ the j.  mach .",
    "_ , 140 ( 1):0 33853418 , 2013 .",
    "r.  tibshirani .",
    "regression shrinkage and selection via the lasso .",
    "_ j.  r.  statist .",
    ", 58:0 267288 , 1996 .",
    "r.  j. tibshirani .",
    "the lasso problem and uniqueness . _ electron .",
    "j.  statist .",
    "_ , 7:0 14561490 , 2013 .",
    "s.  van de geer and p.  bhlmann . .",
    "_ electron .",
    "j.  statist .",
    "_ , 3:0 13601392 , 2009 .    s.  van  de geer , p.  bhlmann , y.  ritov , and r.  dezeure .",
    ".  statist .",
    "_ , 42:0 11661202 , 2014 .",
    "a.  voorman , a.  shojaie , and d.  witten .",
    "inference in high dimensions with the penalized score test .",
    "_ arxiv preprint arxiv:1401.2678 _ , 2014 .",
    "l.  wasserman and k.  roeder . .",
    "_ , 37:0 2178 , 2009 .",
    "p.  westfall and s.  young .",
    "_ resampling - based multiple testing : examples and methods for p - value adjustment_. john wiley & sons , 1993 .",
    "zhang and s.  s. zhang . . _",
    "j.  r.  statist .",
    "b _ , 76:0 217242 , 2014 .",
    "zhang and t.  zhang . a general theory of concave regularization for high - dimensional sparse estimation problems .",
    "_ statistical science _ , 270 ( 4):0 576593 , 2012",
    ".    p.  zhao and b.  yu . .",
    "_ j.  mach .",
    "_ , 7:0 25412563 , 2006 .",
    "q.  zhou .",
    "monte carlo simulation for lasso - type problems by estimator augmentation .",
    "_ j.  am .",
    "statist .  ass .",
    "_ , 1090 ( 508):0 14951516 , 2014 .",
    "q.  zhou .",
    "uncertainty quantification under group sparsity .",
    "_ arxiv preprint arxiv:1507.01296 _ , 2015 .",
    "this supplementary material is organised as follows .",
    "section  [ sec : power ] contains results on the power of the rp tests approach for detecting nonlinearity . in section  [ sec : non - gaussian ] we discuss how the rp tests methodology can be extended to test for null hypotheses of linear models with non - gaussian errors , and present numerical results in support of our proposed scheme .",
    "additional numerical results to complement those of section  [ sec : apps ] in the main paper are presented in section  [ sec : num ] .",
    "finally the proofs of all of the results in the main paper , we well as those stated in section  [ sec : power ] , are collected in section  [ sec : proofs ] .",
    "note that all equations numbered 1 - 10 are in the main paper .",
    "in this section we briefly discuss the power of rp tests for detecting nonlinearity .",
    "suppose the response is generated according to @xmath295 where @xmath4 is a sparse vector with @xmath296 , @xmath142 and as before @xmath297 .",
    "the nonlinear term @xmath298 is to be thought of as a vector of function evaluations of some nonlinear function : @xmath299 where @xmath300 , though this is not assumed in the sequel .    as in section",
    "[ sec : lasso ] of the main paper , here we require that the columns of @xmath9 have been scaled to have @xmath91-norm @xmath92 . to facilitate theoretical analysis",
    ", we will assume @xmath93 is a lasso estimate with fixed @xmath301 and @xmath210 , rather than with the tuning parameter selected by cross - validation as in algorithm  [ alg : lasso1 ] .",
    "furthermore , we will also take this to be the tuning parameter used in the construction of the lasso scaled residuals @xmath302 . let the bootstrap scaled residuals be @xmath303 .",
    "we will also take this @xmath46 to be the tuning parameter used in the construction of the lasso scaled residuals @xmath302 .",
    "let the bootstrap scaled residuals derived from @xmath93 and @xmath304 be @xmath305 where @xmath306 .    to quantify the potential power of rp tests , we define @xmath307 and let @xmath308 be the nonlinear signal @xmath298 residualised with respect to @xmath9 . as in section",
    "[ sec : single ] we consider an asymptotic regime where @xmath0 , @xmath9 , @xmath83 , @xmath153 and @xmath298 can all change as @xmath206 , though we suppress this in the notation .",
    "also , as in theorem  [ thm : single_pval_null ] , let @xmath309 .",
    "the result , which follows from theorem  [ thm : single_pval_alt ] and its proof , shows that whilst the true residuals are positively correlated with the residualised signal @xmath310 , the bootstrap residuals are not .",
    "[ cor : power ] suppose @xmath311 and for some @xmath218 we have @xmath312 and @xmath313 .",
    "assume there exists @xmath212 with @xmath314 .",
    "we have @xmath315    an interesting application of the result above is quantification of the power to detect interactions , as we now discuss .",
    "consider a random design setting where @xmath9 is a scaled version of a matrix @xmath33 whose rows @xmath316 are independent with @xmath317 and @xmath318 for all @xmath319 .",
    "that is we have @xmath320 .",
    "let @xmath321 where without loss of generality , @xmath322 is a symmetric matrix .",
    "thus the nonlinear component of the signal is a quadratic function of the variables in @xmath153 .",
    "as before , we will consider asymptotics where @xmath323 and now also @xmath324 will change as @xmath325 , though we suppress this in the notation .",
    "we will assume a restricted eigenvalue - type condition on the sequence of covariance matrices @xmath254 : let @xmath326 and assume that for @xmath212 , we have @xmath327 as @xmath206 .",
    "note that this is weaker than assuming the minimum eigenvalue of @xmath254 is bounded away from zero , for example .",
    "[ thm : interactions ] suppose @xmath328 and @xmath329 .",
    "we have @xmath330    note that the theorem allows for @xmath331 , though we do need @xmath332 .",
    "we see that the nonlinear signal is positively correlated with the true lasso residuals , but not with the bootstrap residuals .",
    "thus the nonlinear signal is present in the true lasso residuals , and in principle can be detected by a suitable rp method .",
    "although the null hypothesis that the gaussian linear model is correct is often of interest , one may wish to consider a larger null hypothesis that allows allows for non - gaussian errors .",
    "theorem  [ thm : maximise_pval ] can not easily be extended to this setting as it allows for arbitrary ( collections of ) rp functions to be used , including those that might directly test for normality .",
    "we do not pursue this further here but note that knowing errors are non - gaussian can be helpful for designing a different objective function to use with @xmath232 penalisation that may be more efficient for estimation .",
    "we also note that one could in principle extend the results of theorems  [ thm : single_pval_null ] and [ thm : single_pval_alt ] to allow for non - gaussian error distributions under the null . the result for a single variable follows via the central limit theorem , but",
    "the uniformity of variables in @xmath333 requires the deep results of @xcite .    nevertheless , it is desirable that a test for e.g.  nonlinearity should not reject more often when a sparse linear model with non - normal errors holds .",
    "when non - gaussian errors must be included in the null hypothesis , we recommend taking the simulated errors @xmath97 to be a sample with replacement from the original scaled residuals @xmath12 .    figures  [ fig : groups_exp ] and [ fig : nonlinear_exp ] are identical to figures  [ fig : groups ] and [ fig : nonlinear ] but with exponential errors rather than gaussian errors used in all simulations .",
    "we use the nonparametric bootstrap approach described above .",
    "we see that type i error is very well controlled for rp tests across all the settings with the power also competitive .    .",
    "[ fig : groups_exp ] ]    .",
    "[ fig : groups_exp ] ]    .[fig : nonlinear_exp ] ]    .[fig : nonlinear_exp ] ]",
    "in this section we present additional numerical results of the same format as those in section  [ sec : apps ] in the main paper , but where the nonzero coefficients given active variables @xmath334 are chosen as follows : @xmath335 the coefficients are then scaled to have an @xmath232-norm of 12 .",
    "modulo these modifications , figures  [ fig : groups_decay ] and [ fig : nonlinear_decay ] are exactly analogous to figures  [ fig : groups ] and [ fig : nonlinear ] respectively .",
    "we see the results are very much in line with those of section  [ sec : apps ] .",
    "note that the reduced power of rp tests compared to the debiased lasso in the equal correlation and real design settings of figure  [ fig : groups_decay ] is due to the poor calibration of the debiased approach , which here tends to greatly exceeds its nominal level .    .",
    "[ fig : groups_decay ] ]    .",
    "[ fig : groups_decay ] ]    .[fig : nonlinear_decay ] ]    .[fig : nonlinear_decay ] ]",
    "in the proofs that follow , we will let @xmath336 denote constants , which may change from line to line .      in the following",
    "we suppress the dependence of @xmath337 on @xmath46 for notational simplicity . we know",
    "that every lasso solution @xmath338 is characterised by the kkt conditions @xmath339}{\\|{\\mathbf}x\\{{\\boldsymbol}\\beta - \\hat{{\\boldsymbol}\\beta } ( { \\boldsymbol}\\beta , \\sigma{\\boldsymbol}\\zeta)\\ } + \\sigma{\\boldsymbol}\\zeta\\|_2 } = \\lambda \\hat{{\\boldsymbol}\\nu},\\ ] ] where @xmath340 and @xmath341 with @xmath342 .    now picking a particular lasso solution @xmath343 in the case where it is not unique , let @xmath344 note that when @xmath345 , the upper bound on @xmath120 ensures that we have @xmath346 . next observe that @xmath347,\\ ] ] so @xmath348}{\\|{\\mathbf}x\\{\\check{{\\boldsymbol}\\beta } - \\tilde{{\\boldsymbol}\\beta } ( { \\boldsymbol}\\zeta)\\ } + \\check{\\sigma}{\\boldsymbol}\\zeta\\|_2 } = \\frac{1}{\\sqrt{n}}\\frac{{\\mathbf}x^t [ { \\mathbf}x\\{{\\boldsymbol}\\beta - \\hat{{\\boldsymbol}\\beta } ( { \\boldsymbol}\\beta , \\sigma{\\boldsymbol}\\zeta)\\ } + \\sigma{\\boldsymbol}\\zeta]}{\\|{\\mathbf}x\\{{\\boldsymbol}\\beta - \\hat{{\\boldsymbol}\\beta } ( { \\boldsymbol}\\beta , \\sigma{\\boldsymbol}\\zeta)\\ } + \\sigma{\\boldsymbol}\\zeta\\|_2}= \\lambda \\hat{{\\boldsymbol}\\nu}({\\boldsymbol}\\beta , { \\boldsymbol}\\zeta).\\ ] ] but this shows that @xmath349 satisfies the kkt conditions for @xmath350 .",
    "since lasso fitted values are unique , we must have @xmath351 . now substituting into",
    "finally shows that @xmath352 as required .",
    "the proofs of theorems  [ thm : maximise_pval][thm : interactions ] make use of theorem 2 and corollary 1 in @xcite .",
    "we re - state a subset of these results here for convenience .",
    "we have modified the notation in @xcite in order to avoid clashes with our own notation .",
    "furthermore , we have replaced the sign - restricted cone invertibility factor @xmath353 ( equation 21 in @xcite ) with its lower bound @xmath354 @xcite .",
    "consider the linear model setup of though without any assumptions on the distribution of @xmath355 initially .",
    "for @xmath150 and @xmath113 , define @xmath356,\\ ] ] where the minimum is over @xmath149 and @xmath357 .",
    "further let @xmath358 and @xmath359 .",
    "writing @xmath360 , define @xmath361 .",
    "we follow the proof of part ( ii ) of theorem 2 of @xcite .",
    "let @xmath373 .",
    "then @xmath374 follows a @xmath375-distribution with @xmath376 degrees of freedom . the only change we need to make in the aforementioned proof is to note that @xmath377 and modify ( a8 ) in @xcite appropriately .      without loss of generality",
    "assume @xmath378 .",
    "let @xmath379 and let @xmath21 denote the orthogonal projection on to @xmath380 . define the following subsets of @xmath10 : @xmath381 let @xmath382 and let @xmath383 .",
    "lemma  [ lem : event_prop ] shows that on the event @xmath384 we have the following properties .      in the following ,",
    "we suppress dependence on @xmath46 .",
    "let @xmath390 be @xmath144 with @xmath391 zeroes added : @xmath392 and let @xmath393 .",
    "note that by theorem  [ thm : lambda_fixed ] , on @xmath384 we know that @xmath394 for all @xmath395 .",
    "moreover , lemma  [ lem : indep ] shows that conditional on @xmath384 , @xmath396 and @xmath397 are independent .",
    "write @xmath398 for @xmath399 and let @xmath400 .",
    "we see that conditional on @xmath384 , @xmath401 are independent . also @xmath402",
    "are independent and identically distributed .",
    "let @xmath71 , @xmath403 be derived as in section  [ sec : combine ] by applying the function @xmath62 to appropriate functions of scaled residuals @xmath401 .",
    "recall that @xmath404 .",
    "let @xmath405 and note that letting @xmath406 we have @xmath407 .",
    "we have @xmath408 lemma  [ lem : event_prop ] gives the required bound on @xmath409 ; it only remains to show the first term on the rhs is at most @xmath287 . from the above , conditional on @xmath410 and the event @xmath384 , @xmath411 are exchangeable .",
    "now there can be at most @xmath412 values @xmath413 with @xmath414 .",
    "this entails that @xmath415 therefore @xmath416      the proofs of theorems  [ thm : single_pval_null ] and [ thm : single_pval_alt ] rest on the following decomposition of @xmath196 and an analogous one for @xmath238 : @xmath417 where @xmath418 the first term in is zero for @xmath419 and @xmath420 the main term we have to control is @xmath421 .",
    "the result of @xcite shows that @xmath422 is small with high probability .",
    "next appealing to the kkt conditions for the square - root lasso , we obtain @xmath423 hlder s inequality then gives @xmath424 proof of theorem  [ thm : single_pval_null ] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    in view of lemma  [ lem : convergence ] applied with @xmath425 simply a constant , for the first part we need only show that @xmath426 and @xmath427 .",
    "first note that @xmath428 in view of lemma  [ lem : mu_bd ] and consequently ( as clearly @xmath429 ) @xmath430 now writing @xmath431 for convenience , we see that for @xmath40 sufficiently large it must be the case that @xmath432 . by theorem",
    "[ thm : sun ] there is a sequence of events with probability tending to 1 on which we have @xmath433 where @xmath434 . from lemma  [ lem : chisq ]",
    "we know that @xmath435 , so in particular we have @xmath436 . thus also , on a sequence of events with probability tending to 1 , applying to we have @xmath437 which completes the proof of the first part .",
    "turning to the bootstrap results , we know that that on a sequence of events of the form @xmath438 with probability tending to 1 , we have @xmath439 here @xmath440 corresponds to the set of noise components of @xmath228 .",
    "thus on @xmath441 , by lemma  [ lem : mu_bd ] , we have @xmath442 for any fixed @xmath443 and some @xmath444 .",
    "let @xmath445 and @xmath446 be the bootstrap equivalents of @xmath421 and @xmath447 respectively . by applying lemma  [ lem : convergence ] now with @xmath448 , we see that it is enough to show that on @xmath441 , for all @xmath449 we have @xmath450 for this it is sufficient to exhibit a sequence @xmath451 whose probability tends to 1 such that on @xmath452 we have @xmath453 let @xmath454 .",
    "define @xmath455 provided @xmath456 we have @xmath457 for @xmath40 sufficiently large .",
    "this gives us the equivalent of and for @xmath40 sufficiently large : @xmath458 by lemma  [ lem : chisq ] , conditional on @xmath355 , @xmath459 .",
    "thus @xmath460 tending to 0 and therefore also and occur on a sequence of events with probability tending to 1 , @xmath461 , where @xmath462 is of the form @xmath463 . as on @xmath461 , @xmath464 , gives . applying to and then gives , which completes the proof .",
    "the proof proceeds similarly to that of theorem  [ thm : single_pval_null ] . for the first result",
    ", we use lemma  [ lem : convergence ] with @xmath425 simply constant .",
    "thus it suffices to show @xmath465 , @xmath466 and @xmath467 where @xmath468 . to this end",
    ", first note that by lemma  [ lem : mu_bd ] , for some @xmath469 we have @xmath470 .",
    "let @xmath471 then @xmath472 where @xmath473 is defined as in .",
    "since @xmath474 , we have that @xmath475 by lemma  [ lem : chisq ] . for later use we also note that @xmath476 by the central limit theorem and as @xmath477 .",
    "thus we have @xmath478 .",
    "note that by , @xmath479 therefore @xmath480 thus by theorem  [ thm : sun ] we have that on a sequence of events @xmath438 with probability tending to 1 , @xmath481 , @xmath482 , and @xmath483 .",
    "the latter in conjunction with shows @xmath466 .",
    "that @xmath484 follows from .",
    "now we derive the result concerning the bootstrap test statistic .",
    "note that on @xmath441 , @xmath485 and so also @xmath486 for any fixed @xmath443 and some @xmath444 .",
    "the rest of the proof then proceeds exactly as the proof for the bootstrap statistic in theorem  [ thm : single_pval_null ] .          by corollary 1 of @xcite , with probability tending to 1 we have @xmath489 . by lemma  [ lem : inter_bound ] and the fact that @xmath490 we see that @xmath491 .",
    "indeed , we have @xmath492 as @xmath206 .",
    "also , with probability tending to 1 , @xmath493 i.e.  @xmath494 .",
    "we now temporarily make the dependence of @xmath33 and @xmath0 on @xmath40 explicit by writing @xmath495 and @xmath496 respectively , in order to explain the structure of the argument to follow . from the above",
    ", we have a sequence of sets @xmath497 for which @xmath498 , and on the respective events @xmath499 and @xmath500 ( uniformly ) .",
    "let @xmath501 .",
    "in relation to corollary  [ cor : power ] , we will take @xmath502 sufficiently large such that the lasso regression of @xmath298 on @xmath9 produces the zero vector . by lemma  [ lem : convergence ] , it suffices to show that for each @xmath449 , @xmath503 here @xmath504 .",
    "the remainder of the argument to arrive at the first result is essentially identical to that in theorem  [ thm : single_pval_alt ] , with @xmath298 playing the role of @xmath505 , and with the probabilities being conditional on @xmath495 .",
    "the only difference is that in place of ( which leads to the equivalent of ) , we have @xmath506 where @xmath507 . since for all @xmath508 , @xmath499 uniformly , it is straightforward to show that @xmath509 for any @xmath449 , which then leads to .",
    "the bootstrap result is simpler .",
    "we know there is a sequence of events depending only on @xmath510 on which @xmath511 thus on the same sequence of events @xmath512 from which the result follows by arguing along the lines of the second part of the proof of theorem  [ thm : single_pval_null ] .",
    "first we bound @xmath515 . from lemma  [ lem : event_prop ] and the union bound we have @xmath516 .",
    "next note that @xmath517 where @xmath518 and @xmath519 denotes equality in distribution .",
    "thus lemma  [ lem : event_prop ] gives @xmath520 .",
    "finally , lemma  [ lem : chisq ] gives @xmath521 .    now turning to ( i )",
    "observe note that as @xmath522 and @xmath523 we have @xmath524 since @xmath525 ensures that @xmath526 . thus by theorem  [ thm : sun ] and the fact that @xmath527 we have @xmath528 where @xmath529 .",
    "this shows @xmath530 .",
    "next @xmath531 which shows ( i ) .",
    "now @xmath532 thus @xmath387 . by theorem",
    "[ thm : sun ] , the @xmath91 bound is also satisfied by @xmath144 , which then shows ( ii ) .",
    "also note that @xmath533 by theorem  [ thm : sun ] .",
    "from we have @xmath534 we see that the rhs is at least 1 when @xmath535 , which then shows ( iii ) as @xmath536 .",
    "write @xmath538 .",
    "first we claim that on @xmath384 , @xmath396 depends only on @xmath539 . to this end , we argue that for @xmath395 with @xmath540 , @xmath541 where @xmath542 .",
    "note the validity of the above inequalities would prove the initial claim for all @xmath395 with @xmath540 for each fixed @xmath543 .",
    "however , since @xmath544 the initial claim would then have to be true for all @xmath395 .",
    "we now set about proving these equalities .",
    "the first equality is clear by definition of @xmath12 . turning to the second ,",
    "let @xmath395 and denote by @xmath545 the operator norm . from lemma  [ lem : event_prop ]",
    "we have @xmath546 using the facts that @xmath157 and @xmath547 in the final line .",
    "note that by lemma  [ lem : event_prop ] , @xmath548 with the latter defined in theorem  [ thm : lambda_fixed ] and @xmath549 .",
    "now clearly if @xmath395 then also @xmath550 .",
    "an application of theorem  [ thm : lambda_fixed ] then gives the desired equality .",
    "equality then follows from a further application of theorem  [ thm : lambda_fixed ] noting that @xmath551 by assumption .",
    "next we examine @xmath144 and @xmath552 .",
    "note that on @xmath384 , the former is simply the lasso estimate from regressing on @xmath380 and the latter is the resulting normalised root - rss .",
    "write @xmath553 and @xmath554 .",
    "the least squares part of the lasso objective decomposes as @xmath555 thus it is clear that the fitted values @xmath556 do not depend on @xmath539 .",
    "this then implies that @xmath120 does not depend on @xmath539 since it is determined by @xmath557 and @xmath558 .",
    "now observe that @xmath559 so @xmath560 are jointly independent .",
    "let @xmath561 , @xmath562 , where @xmath563 and @xmath564 are arbitrary borel sets .",
    "let @xmath565 , @xmath566 . from the above @xmath567",
    "the conditional probabilities on the rhs remain unchanged if we modify the conditioning event to be @xmath568 since @xmath569 and @xmath570 .",
    "this completes the proof .",
    "[ lem : mu_bd ] given a sequence of collections of matrices @xmath571 , @xmath572 , @xmath573 , suppose there exists a sequence of collections of sets @xmath574 , tuning parameters @xmath575 and @xmath576 for constants @xmath577 such that the follow holds : @xmath578 moreover , suppose the collection of sequences @xmath579 is such that @xmath580 then there exists a @xmath581 such that @xmath582 for any @xmath583 ( where the function @xmath584 is defined in ) .",
    "[ lem : convergence ] let @xmath593 , @xmath572 , @xmath573 be a collection of random variables which we may decompose as @xmath594 where each @xmath595 is identically distributed with continuous distribution function @xmath24 .",
    "suppose further that each @xmath595 is independent of the random elements @xmath425 , and for all @xmath596 , @xmath597 for some random variables @xmath598 that are functions of @xmath425 , and @xmath599 . then @xmath600    first note that for all @xmath601 , there exists @xmath602 such that for all @xmath603 $ ] , @xmath604 indeed , the function @xmath605 is uniformly continuous on @xmath606 ^ 2 $ ] for @xmath607 sufficiently small ( since @xmath24 is uniformly continuous and compositions of uniformly continuous functions are continuous ) and the lhs of is @xmath608 .    hence given @xmath609 , let @xmath596 be such that the lhs of is at most @xmath610 . then @xmath611 } |{\\mathbb{p}}(z_{k , n } \\leq ( x + c_1)/(1+c_2)|\\mathcal{f}_n ) - f(x)|\\\\ & \\qquad + { \\mathbb{p}}(|b_{k , n}-d_{k , n}|>\\delta|\\mathcal{f}_n ) + { \\mathbb{p}}(|a_{k , n}-1|>\\delta|\\mathcal{f}_n ) \\\\ & \\leq \\epsilon/2 + { \\mathbb{p}}(|b_{k , n}-d_{k , n}|>\\delta|\\mathcal{f}_n ) + { \\mathbb{p}}(|a_{k , n}-1|>\\delta|\\mathcal{f}_n).\\end{aligned}\\ ] ] thus @xmath612      although in theorem  [ thm : interactions ] certain assumptions are placed on @xmath616 , since the probabilities above do not depend on @xmath616 here we may assume @xmath617 .",
    "fix @xmath618 . using the eigendecomposition @xmath619 ( where @xmath620 is diagonal and @xmath21 is orthogonal ) and writing @xmath519 for equality in distribution , we have @xmath621 where @xmath622 is multivariate gaussian with @xmath623 and @xmath624 .",
    "let the diagonal entries of @xmath620 be @xmath625 .",
    "now with a view to applying lemma  [ lem : tail_bd ] below , consider @xmath630 using the fact that @xmath631 .",
    "note that @xmath632 thus , by the am  gm inequality we have @xmath633 using .",
    "similarly , @xmath634 . putting things together , we see that @xmath635 lemma  [ lem : tail_bd ] then immediately gives .    to show , we first obtain a tail bound for @xmath636 .",
    "observe that if @xmath637 is multivariate gaussian with zero - mean , then since @xmath638 , we have @xmath639 then , by hlder s inequality @xmath640^{1/3 } \\bigg\\{{\\mathbb{e}}\\big(\\big|\\sum_j u_j^2\\theta_j/4\\big|^{r}\\big ) \\bigg\\}^{2/3}\\big/ r ! \\\\ & \\leq \\sum_{r=0}^\\infty { \\mathbb{e}}\\big(\\big|\\sum_j u_j^2\\theta_j/4\\big|^{r}\\big ) \\big/ r !",
    "+ \\sum_{r=0}^\\infty { \\mathbb{e}}\\{(v^2/4)^{r}\\ } /r ! \\\\",
    "& = { \\mathbb{e}}\\exp(|f_1|/4 ) + { \\mathbb{e}}(v^2/4 ) \\leq \\sqrt{2e } + \\sqrt{2},\\end{aligned}\\ ] ] using in the final line .",
    "note @xmath641 by , and @xmath642 by lemma  [ lem : chisq ] . thus by lemma  [ lem : tail_bd ] , @xmath643 for @xmath644 $ ] and some constants @xmath645 .",
    "thus for @xmath646 with @xmath647 , @xmath648 as @xmath325 and @xmath649 .",
    "[ lem : tail_bd ] let @xmath650 be independent random variables with zero mean such that @xmath651 for constants @xmath645 and @xmath652 $ ] . then there exist constants @xmath653 such that for @xmath654 $ ] , @xmath655"
  ],
  "abstract_text": [
    "<S> in this work we propose a framework for constructing goodness of fit tests in both low and high - dimensional linear models . </S>",
    "<S> we advocate applying regression methods to the scaled residuals following either an ordinary least squares or lasso fit to the data , and using some proxy for prediction error as the final test statistic . </S>",
    "<S> we call this family residual prediction ( rp ) tests . </S>",
    "<S> we show that simulation can be used to obtain the critical values for such tests in the low - dimensional setting , and demonstrate using both theoretical results and extensive numerical studies that some form of the parametric bootstrap can do the same when the high - dimensional linear model is under consideration . </S>",
    "<S> we show that rp tests can be used to test for significance of groups or individual variables as special cases , and here they compare favourably with state of the art methods , but we also argue that they can be designed to test for as diverse model misspecifications as heteroscedasticity and nonlinearity . </S>"
  ]
}