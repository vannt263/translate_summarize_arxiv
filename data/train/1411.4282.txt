{
  "article_text": [
    "the most acknowledged methods of measuring importance of nodes in graphs are based on random walk models .",
    "particularly , pagerank  [ 18 ] , hits  [ 11 ] , and their variants  [ 8 ,  9 ,  19 ] are originally based on a discrete - time markov random walk on a link graph . according to the pagerank algorithm",
    ", the score of a node equals to its probability in the stationary distribution of a markov process , which models a random walk on the graph . despite undeniable advantages of pagerank and its mentioned modifications , these algorithms miss important aspects of the graph that are not described by its structure .",
    "in contrast , a number of approaches allows to account for different properties of nodes and edges between them by encoding them in restart and transition probabilities ( see  [ 3 ,  4 ,  6 ,  10 ,  12 ,  20 ,  21 ] ) .",
    "these properties may include , e.g. , the statistics about users interactions with the nodes ( in web graphs  [ 12 ] or graphs of social networks  [ 2 ] ) , types of edges ( such as url redirecting in web graphs  [ 20 ] ) or histories of nodes and edges changes  [ 22 ] . particularly , the transition probabilities in browserank algorithm  [ 12 ] are proportional to weights of edges which are equal to numbers of users transitions . in the general ranking framework called supervised pagerank  [ 21 ] ,",
    "weights of nodes and edges in a graph are linear combinations of their features with coefficients as the model parameters .",
    "the authors consider an optimization problem for learning the parameters and solve it by a gradient - based optimization method .",
    "however , this method is based on computation of derivatives of stationary distribution vectors w.r.t .",
    "its parameters which include calculating the derivative for each element of a billion by billion matrix and , therefore , seems to be computationally very expensive .",
    "the same problem appears when using coordinate descent methods like  [ 15 ] does .",
    "another obstacle to the use of gradient or coordinate descent methods is that we ca nt calculate derivatives precisely , since we ca nt evaluate the exact stationary distribution .    in our paper",
    ", we consider the optimization problem from  [ 21 ] and propose a two - level method to solve it . on the lower level",
    ", we use the linearly convergent method from  [ 17 ] to calculate an approximation to the stationary distribution of the markov process .",
    "we show in section  [ learn ] that this method has the best among others  [ 5 ] complexity bound for the two - level method as a whole .",
    "however , it is not enough to calculate the stationary distribution itself , since we need also to optimize the parameters of the random walk with respect to an objective function , which is based on the stationary distribution . to overcome the above obstacles",
    ", we use a gradient - free optimization method on the upper level of our algorithm .",
    "the standard gradient - free optimization methods  [ 7 ,  16 ] require exact values of the objective function . our first contribution described in section  [ method ]",
    "consists in adapting the framework of  [ 16 ] to the case when the value of the function is calculated with some known accuracy .",
    "we prove a convergence theorem ( section  [ method ] ) for this method .",
    "our second contribution consists in investigating the trade - off between the accuracy of the lower level algorithm , which is controlled by the number of iterations , and the computational complexity of the two - level algorithm as a whole ( section  [ learn ] ) . for given accuracy",
    ", we estimate the number of arithmetic operations needed by our algorithm to find the values of parameters such that the difference between the respective value of the objective and its local minimum does not exceed this accuracy . in the experiments",
    ", we apply our algorithm to the problem of web pages ranking .",
    "we show in section  [ results ] that our two - level method outperforms an untuned gradient - free method in the ranking quality . the remainder of the paper is organized as follows . in section  [ model ] , we describe the random walk model . in section",
    "[ optimal ] , we define the learning problem and discuss its properties and possible methods for its solution . in section  [ method ]",
    "we describe the framework of random gradient - free optimization methods and generalize it to the case when the function values are inaccurate . in section",
    "[ learn ] we propose two - level algorithm for the stated learning problem .",
    "the experimental results are reported in section  [ experimental results ] . in section",
    "[ conclusion ] , we summarize the outcomes of our study , discuss its potential applications and directions of future work .",
    "let @xmath0 be a directed graph . denote by @xmath1 the number of vertices in @xmath2 .",
    "let @xmath3 be two classes of functions parameterized by @xmath4 respectively , where @xmath5 is the number of nodes features , @xmath6 is the number of edges features .",
    "we denote @xmath7 and @xmath8 .",
    "let us describe the random walk on the graph @xmath9 , which was considered in  [ 21 ] .",
    "the _ seed set _",
    "@xmath10 is defined as follows : @xmath11 if and only if @xmath12 for some @xmath13 .",
    "a surfer starts a random walk from a random page @xmath11 , the initial probability of being at vertex @xmath14 is called the _ restart probability _ and equals @xmath15_i=\\frac{f(\\vp_1 , i)}{\\sum_{\\tilde i\\in   v^1}f(\\vp_1 , \\tilde i ) } \\label{restart}\\ ] ] ( equals @xmath16 for @xmath17 ) . at each step , the surfer ( with a current position @xmath18 ) either chooses any vertex from @xmath19 in accordance with the distribution @xmath20 ( makes a _ restart _ ) with probability @xmath21 , which is called the _ damping factor _ , or chooses to traverse an outcoming edge ( makes a _ transition _ ) with probability @xmath22 .",
    "the probability @xmath23_{\\tilde i , i}=\\frac{g(\\vp_2 , \\tilde{i } \\to i)}{\\sum_{j : \\tilde{i } \\to   j}g(\\vp_2,\\tilde{i } \\to j ) } \\label{transition}\\ ] ] of traversing an edge @xmath24 is called the transition probability .",
    "finally , by equation  [ restart ] and equation  [ transition ] the total probability of choosing vertex @xmath11 conditioned by the surfer being at vertex @xmath25 equals @xmath26_i+(1-\\alpha)[p(\\vp)]_{\\tilde i , i}$ ] ( originally  [ 18 ] , @xmath27 ) .",
    "if @xmath17 , then this probability equals @xmath28_{\\tilde i , i}$ ] .",
    "denote by @xmath29 the stationary distribution of the described markov process .",
    "it can be found as a solution of the system of equations @xmath30_i",
    "= \\alpha [ \\pi^0(\\vp)]_i + ( 1-\\alpha)\\sum_{\\tilde{i } : \\tilde{i }",
    "\\to i \\in e}[p(\\vp)]_{\\tilde i , i}[\\pi]_{\\tilde i}. \\label{pi_general}\\ ] ] in this paper , we learn the ranking algorithm , which orders the vertices @xmath14 by their probabilities @xmath31_{i}$ ] in the stationary distribution @xmath32 .",
    "let @xmath33 be a set of search queries and weights of nodes and edges @xmath34 and @xmath35 depend on @xmath36 .",
    "let @xmath37 be a set of vertices which are relevant to @xmath38 .",
    "in other words , for any @xmath39 either @xmath40 for some @xmath13 or there exists a path @xmath41 in @xmath9 such that @xmath42 , @xmath43 for some @xmath44 and all @xmath45 . denote @xmath46 a set of all edges @xmath47 from @xmath48 such that @xmath49 and @xmath50 for some @xmath51 . for any @xmath36 ,",
    "denote @xmath52 . for fixed @xmath36 , the graph @xmath53 and functions @xmath54 , we consider the notations from the previous section and add the index @xmath38 : @xmath55 , @xmath56 , @xmath57 , @xmath58 , @xmath59 .",
    "the parameters @xmath60 and @xmath61 of the model do not depend on @xmath38 .",
    "our goal is to find the parameters vector @xmath61 which minimizes the discrepancy of the nodes ranking scores @xmath62_i$ ] , @xmath63 , calculated as the stationary distribution in the above markov process from the nodes ranking scores defined by assessors . for each @xmath36",
    ", there is a set of nodes in @xmath37 manually judged and grouped by relevance labels @xmath64 .",
    "we denote @xmath65 the set of documents annotated with label @xmath66 ( i.e. , @xmath67 is the set of all nodes with the highest relevance score ) . for",
    "any two nodes @xmath68 , let @xmath69_{i_2}-[\\pi_q]_{i_1})$ ] be the value of the loss function .",
    "if it is non - zero , then the position of the node @xmath70 according to our ranking algorithm is higher than the position of the node @xmath71 but @xmath72 .",
    "we consider square loss with margins @xmath73 , where @xmath74 : @xmath75 as it was done in previous studies  [ 12 ,  21 ,  22 ] .",
    "we minimize @xmath76_{i_2}-[\\pi_q]_{i_1 } )   \\label{eq : f_phi_def_1}\\ ] ] in order to learn our model using the data given by assessors . as it was said above",
    ", finding nodes ranking scores for the fixed query @xmath38 leads to the problem of finding the stationary distribution @xmath77 of the markov process as a solution of equation  [ pi_general ] or equivalently @xmath78 the solution @xmath79 of can be found as @xmath80^{-1 } \\pi^0_q(\\vp)$ ] , where @xmath81 is the identity matrix .",
    "it is easy to show  [ 17 ] that the vector @xmath82^i \\pi^0_q(\\vp ) } \\label{eq : tpi_def}\\ ] ] satisfies @xmath83 . as it also shown there to obtain vector @xmath84 satisfying @xmath85 one needs @xmath86 iterations of simple iteration method .",
    "each iteration of such method requires one multiplication of the matrix @xmath87 by the vector of dimension @xmath88 .",
    "this requires @xmath89 arithmetic operations . here",
    "@xmath90 is the maximum number of non - zero elements over columns of the matrix @xmath91 ( the _ sparsity parameter _ ) .",
    "so the total number of arithmetic operations for obtaining approximation satisfying is @xmath92 arithmetic operations .",
    "note that @xmath93 and that this algorithm for finding the vector @xmath84 can be fully paralleled .",
    "let us now turn to the problem of the minimization of the function @xmath94 .",
    "we can rewrite this function as @xmath95 where vector @xmath96 has components @xmath97_i=\\max\\{x_i,0\\}$ ] , the matrix @xmath98 represents assessor s view of the relevance of pages to the query @xmath38 , vector @xmath99 is the vector composed from thresholds @xmath100 in with fixed @xmath38 , @xmath101 is the number of summands in with fixed @xmath38 .",
    "due to huge hidden dimension @xmath88 , the calculation of the of @xmath94 includes calculating the derivative for each element of the @xmath102 matrix @xmath91 which is too expensive .",
    "so we are going to use gradient - free methods for minimization of the function @xmath94 .",
    "such methods were introduced rather long ago , see , e.g. ,  [ 13 ] .",
    "note that we have to work in the framework of non - exact zero - order oracle .",
    "note that each row of the matrix @xmath103 contains one @xmath104 and one @xmath105 , and all other elements of the row are equal to @xmath16 and hence @xmath106 .",
    "this leads to the following lemma which says how the error of the approximation of @xmath107 affects the error in the value of the function @xmath94 .",
    "assume that the vector @xmath84 satisfies equation  [ delta ] .",
    "denote @xmath108 , @xmath109",
    ". then @xmath110 satisfies @xmath111 .",
    "[ lm : delta_to_delta ]",
    "let us describe the well - known framework of random gradient - free methods  [ 1 ,  7 ,  16 ] .",
    "our main contribution , described in this section , consists in developing this framework for the situation of presence of error of unknown nature in the objective function value . apart from",
    "[ 16 ] we consider randomization on a euclidean ball which seems to give better large deviations bounds and does nt need the assumption that the function can be calculated at any point of the space @xmath112 .    in this section , we consider a general function @xmath113 and denote its argument by @xmath114 or @xmath115 to avoid confusion with other sections .",
    "assume that the function @xmath116 is convex and has lipschitz continuous gradient with constant @xmath117 ( we write @xmath118 ) : @xmath119 also we assume that the oracle returns the value @xmath120 , where @xmath121 is the oracle error satisfying @xmath122 .",
    "consider smoothed counterpart of the function @xmath123 : @xmath124 where @xmath125 is uniformly distributed over unit ball @xmath126 random vector , @xmath127 is the volume of the unit ball @xmath128 , @xmath129 is a smoothing parameter .",
    "it is easy to show that    * if @xmath130 is convex , then @xmath131 is also convex * if @xmath118 , then @xmath132 . * if @xmath118 , then @xmath133 for all @xmath134 .",
    "the random gradient - free oracle is defined as follows @xmath135 where @xmath136 is uniformly distributed vector over the unit sphere @xmath137 .",
    "it can be shown that @xmath138 .",
    "since we can use only zeroth - order oracle with error we also define the counterpart of the above random gradient - free oracle which can be really computed",
    ". we will call it the biased gradient - free oracle : @xmath139    the following estimates can be proved for the introduced inexact oracle ( the full proof is in the supplementary materials ) .",
    "let @xmath118 .",
    "then , for any @xmath140 , @xmath141 [ lm : gmud ]    we use gradient - type method with oracle @xmath142 instead of the real gradient in order to minimize @xmath143 .",
    "since it is uniformly close to @xmath123 we can obtain a good approximation to the minimum value of @xmath123 .",
    "algorithm  [ alg : gfpgm ] below is the variation of the gradient method . here",
    "@xmath144 denotes the euclidean projection of a point @xmath114 onto a set @xmath145 .    the point @xmath146 , radius @xmath147 , stepsize @xmath148 , number of steps @xmath149 .",
    "define @xmath150 .",
    "generate @xmath151 and corresponding @xmath152 .",
    "calculate @xmath153 .",
    "set @xmath154 .",
    "the point @xmath155 .",
    "next theorem gives the convergence rate of algorithm [ alg : gfpgm ] .",
    "denote by @xmath156 the history of realizations of the vectors @xmath157 , generated on each iteration of the method , @xmath158 , and @xmath159 , @xmath160 .",
    "we say that the smooth function is strongly convex with parameter @xmath161 if and only if for any @xmath140 it holds that @xmath162    let @xmath118 and the sequence @xmath163 be generated by algorithm [ alg : gfpgm ] with @xmath164 . then for any @xmath165 , we have @xmath166 where @xmath167 is the solution of the problem @xmath168 .",
    "if , moreover , @xmath130 is strongly convex with constant @xmath169 , then @xmath170 where @xmath171 .",
    "[ th_1 ]    the full of the theorem proof is in the supplementary materials .",
    "the estimate also holds for @xmath172 , where @xmath173 . to make the right hand side of the inequality less than a desired accuracy",
    "@xmath174 we need to choose @xmath175 let s note that we can also estimate the probability of large deviations from the obtained mean rate of convergence . if @xmath123 is strongly convex , then we have a geometric rate of convergence . consequently , from the markov",
    "s inequality we obtain that after @xmath176 iterations @xmath177 holds with probability greater than @xmath178 .",
    "if the function @xmath123 is not strongly convex , then we can introduce the regularization with parameter @xmath179 minimizing the function @xmath180 , which is strongly convex .",
    "this will give us that after @xmath181 iterations @xmath177 holds with probability greater than @xmath178 .",
    "our idea for minimizing the function @xmath182 is the following .",
    "we assume that we start from the small vicinity of the optimal value and hence the function @xmath182 is convex in this vicinity ( generally speaking , the function is nonconvex ) .",
    "we choose the desired accuracy @xmath174 for approximation of the optimal value of the function @xmath182 .",
    "this value gives us the number of steps of algorithm [ alg : gfpgm ] , the value of the parameter @xmath183 , the maximum value of the allowed error of the oracle @xmath184 .",
    "knowing the value @xmath184 , using lemma [ lm : delta_to_delta ] we choose the number of steps of the algorithm for an approximate solution of equation , i.e. the number @xmath185 in .",
    "this idea leads us to algorithm 2 . to the best of our knowledge ,",
    "this is the first time when the idea of random gradient - free optimization methods is combined with some efficient method for huge - scale optimization using the concept of zero - order oracle with error .",
    "the point @xmath186 , @xmath117  lipschitz constant for the function @xmath94 , radius @xmath147 , accuracy @xmath187 , numbers @xmath188 , @xmath189 defined in lemma [ lm : delta_to_delta ] .",
    "define @xmath190 , @xmath191 , @xmath192 , @xmath193 .",
    "set @xmath194 .",
    "generate random vector @xmath151 uniformly distributed over a unit euclidean sphere @xmath195 in @xmath196 .",
    "set @xmath197 . for every @xmath38 from 1 to @xmath198 calculate @xmath199 , @xmath200 defined in .",
    "calculate @xmath201 , where @xmath202 is defined in .",
    "calculate @xmath203 .",
    "set @xmath154 .",
    "the point @xmath204 .",
    "the most computationally consuming operation on each iteration of the main cycle of this method is the calculation of @xmath205 approximate solutions of the equation .",
    "hence , each iteration of algorithm [ alg : main_algo ] needs approximately @xmath206 arithmetic operations , where @xmath207 , @xmath208 .",
    "so , we obtain the following theorem , which gives the result for local convergence of algoritghm [ alg : main_algo ] .",
    "assume that the point @xmath186 lies in the vicinity of the local minimum point @xmath209 of the function @xmath94 and the function @xmath94 is convex in this vicinity .",
    "then the mean total number of arithmetic operations for the accuracy @xmath174 ( i.e. for inequality @xmath210 to hold ) is given by @xmath211 [ th_2 ]    let us make some remarks .",
    "note that each iteration of the main cycle of the algorithm above can be fully paralleled using @xmath198 processors .",
    "also it is important that the use of geometrically convergent method as the inner algorithm leads to the overall complexity bound which is the product of complexity bounds of the inner and outer algorithms .",
    "the direct calculation of the parameter @xmath117 has many obstacles and leads to the overestimation .",
    "another way is to use the restart method .",
    "since we know the exact required number of iterations for the fixed accuracy , confidence level and @xmath117 , we can use the following procedure .",
    "we start with some initial value of @xmath117 .",
    "calculate the approximation by algorithm [ alg : main_algo ] .",
    "then set @xmath212 and repeat , i.e. calculate the approximation by the algorithm [ alg : main_algo ] , working with new @xmath117 , etc .",
    "the stopping criterion here is stabilization ( with the same accuracy as before ) of this sequence of function values .",
    "the total number of such restarts will be of the order @xmath213 .",
    "the same can be done with the unknown parameter @xmath147 .",
    "here we have omitted the full description of the generalization of the fast - gradient - type scheme  [ 14 ,  16 ] for the case of inexact oracle and application of the obtained method for the minimization of the function @xmath182 .",
    "the fast - gradient - type scheme is faster but requires the oracle to be more precise .",
    "the resulting mean value of the number of arithmetic operations to achieve the accuracy @xmath174 for this method is @xmath214    the point @xmath186 , @xmath117  lipschitz constant for the function @xmath94 , @xmath169  the strong convexity parameter of the function @xmath94 ( note that @xmath215 if the function is convex ) , number @xmath147 such that @xmath216 , accuracy @xmath187 , numbers @xmath188 , @xmath189 defined in lemma [ lm : delta_to_delta ] . define @xmath217 , @xmath218 , @xmath219 , @xmath220 , @xmath221 , @xmath222 , @xmath164 .",
    "set @xmath194 .",
    "compute @xmath223 satisfying @xmath224 . set @xmath225 , @xmath226 , and @xmath227 .",
    "generate random vector @xmath151 uniformly distributed over a unit euclidean sphere @xmath195 in @xmath196 set @xmath228 .",
    "for every @xmath38 calculate @xmath229 , @xmath230 defined in .",
    "calculate @xmath231 , where @xmath232 is defined in .",
    "calculate @xmath233 , @xmath234 .",
    "set @xmath154 .",
    "the point @xmath235 .",
    "also we want to point that the algorithm for solving equation was chosen consciously from a set of modern methods for computing pagerank .",
    "we used review  [ 5 ] of such methods . since for our problem we need to estimate the error which is introduced to the function @xmath182 value by approximate solution of the ranking problem , we considered only three methods : markov chain monte carlo ( mcmc ) , spillman s and nemirovski - nesterov s ( nn ) .",
    "these three methods allow to make the difference @xmath236 , where @xmath237 is the approximation , small .",
    "this is crucial to prove results like lemma [ lm : delta_to_delta ] .",
    "spillman s alogoritm converges in infinity norm which is usually @xmath238 times larger than 2-norm .",
    "mcmc converges in 2-norm and",
    "nn converges in 1-norm .",
    "finally , the full complexity analysis of the two - level algorithm showed that for the dimensions @xmath239 and accuracy @xmath174 considered in our work the combination of gradient - free method with nn method is better than the combination with mcmc in terms of upper bound for arithmetic operations needed to achieve given accuracy .",
    "we compare the performances of different learning techniques , our gradient - free method , an untuned gradient - free method and classical pagerank . in the next section ,",
    "we describe the graph , which we exploit in our experiments ( the user browsing graph ) . in section",
    "[ data ] and section  [ results ] , we describe the dataset and the results of the experiments respectively .      in this section , we define the web user browsing graph ( which was first considered in  [ 12 ] ) .",
    "we choose the user browsing graph instead of a link graph with the purpose to make the model query - dependent .",
    "let @xmath38 be any query from the set @xmath33 .",
    "a user session @xmath240 ( see  [ 12 ] ) , which is started from @xmath38 , is a sequence of pages @xmath241 such that , for each @xmath242 , the element @xmath243 is a web page and there is a record @xmath244 which is made by toolbar .",
    "the session finishes if the user types a new query or if more than 30 minutes left from the time of the last user s activity .",
    "we call pages @xmath245 , @xmath246 , _ the neighboring elements _ of the session @xmath240 .",
    "we define the user browsing graph @xmath0 as follows .",
    "the set of vertices @xmath2 consists of all the distinct elements from all the sessions which are started from any query @xmath36 .",
    "the set of directed edges @xmath48 represents all the ordered pairs of neighboring elements @xmath247 from the sessions . for any @xmath36 , we set @xmath248 for all @xmath13 if there is no session which is started from @xmath38 and contains @xmath14 as its first element .",
    "moreover , we set @xmath249 for all @xmath51 if there is no session which is started from @xmath38 and contains the pair of neighboring elements @xmath250 .",
    "as in  [ 21 ] , we suppose that for any @xmath36 , any @xmath251 and any @xmath252 , a vector of node s features @xmath253 and a vector of edge s features @xmath254 are given .",
    "we set @xmath255 , @xmath256 .",
    "all experiments are performed with pages and links crawled by a popular commercial search engine .",
    "we utilize all the records from the toolbar that were made from 27 october 2014 to 18 january 2015 .",
    "we randomly choose the set of queries @xmath33 the user sessions start from , which contains @xmath257k queries .",
    "there are @xmath258 m vertices and @xmath259 m edges in graphs @xmath53 , @xmath36 , in total .",
    "for each query a set of pages was judged by professional assessors hired by the search engine .",
    "our data contains @xmath260k judged query ",
    "document pairs .",
    "the relevance score is selected from among 5 editorial labels .",
    "we divide our data into two parts . on the first part ( @xmath261 of the set of queries @xmath33 ) we train the parameters and on the second part we test the algorithms .",
    "to define weights of nodes and edges we consider a set of 26 query  document features . for any @xmath36 and @xmath251 , the vector @xmath262 contains values of all these feautures for query ",
    "document pair @xmath263 .",
    "we set @xmath264 and @xmath265_1,\\ldots,[\\mathbf{v}^q_{\\tilde i}]_{m_1},[\\mathbf{v}^q_i]_1,\\ldots,[\\mathbf{v}^q_i]_{m_1})$ ] .",
    "we find the optimal values of the parameters for all the methods by minimizing the objective @xmath130 defined by equation  [ eq : f_phi_def_1 ] by the common untuned gradient - free method gf1 ( algorithm  [ alg : gfpgm ] ) and our precise gradient - free method gf2 ( algorithm  [ alg : main_algo ] ) .",
    "besides , we use pagerank ( pr ) as the common baseline for the algorithms ( used as the only baseline for ssp in  [ 6 ] and one of the baselines for snp in  [ 21 ] ) .    the sets of parameters which are exploited by the optimization methods ( and not tuned by them ) are the following : the lipschitz constant @xmath266 , the accuracy @xmath267 ( in gf2 ) , the radius @xmath268 ( in both gf1 and gf2 ) , the parameter @xmath269  ( [ eq : tpi_def ] ) , which defines the approximation @xmath270 of the stationary distribution @xmath271 , of algorithms gf1 and pr is chosen in such a way that the accuracy @xmath272  ( [ delta ] ) equals @xmath273 . moreover , @xmath274 ( the number of iterations of the optimization method ) and @xmath275 ( the stepsize ) in the algorithms gf1 ( the number of iterations is less than the value of this parameter in gf2 ) .    in table",
    "[ table : ndcg ] , we present the ranking performances in terms of our loss function @xmath130 .    .performances of gf2 , gf and pr methods . [ cols=\"^,^\",options=\"header \" , ]     moreover , the ndcg@3 ( @5 ) gains of both gf1 and gf2 in comparison with pr exceeds @xmath276 for both metrics .",
    "we obtain the @xmath1-values of the paired @xmath277-tests for all the above differences in ranking qualities on the test set of queries .",
    "these values are less than 0.005 .",
    "thus , we conclude that the obtained values of the parameters by our optimization method are closer to optimal than in the case of gf1 .",
    "we consider a problem of learning parameters of supervised pagerank models , which are based on calculating the stationary distributions of the markov random walks with transition probabilities depending on the parameters .",
    "due to huge hidden dimension of the optimization problem and the impossibility of exact calculating derivatives of the stationary distributions w.r.t .",
    "its parameters , we propose a two - level method , based on random gradient - free method with inexact oracle to solve it instead of the previous gradient - based approach . we find the best settings of the gradient - free optimization method in terms of the number of arithmetic operations needed to achieve given accuracy of the objective . in particular , for the proposed method , we provide an estimate for the total number of arithmetic operations to obtain the given accuracy in terms of local convergence .",
    "we apply our algorithm to the web page ranking problem by considering a dicrete - time markov random walk on the user browsing graph .",
    "our experiments show that our two - level method outperforms both classical pagerank algorithm and the gradient - free algorithm with other settings ( which are , theoretically , not optimal ) . in the future",
    ", some globalization techniques can be considered ( e.g. , multi - start ) , because the objective function is nonconvex .",
    "the work was partially supported by russian foundation for basic research grants 14 - 01 - 00722-a , 15 - 31 - 20571-mol_a_ved .",
    "we have @xmath280 , where @xmath281 is the volume of the unit sphere which is the border of the ball in @xmath112 with radius @xmath188 . note that @xmath282 .",
    "let @xmath283 be the angle between @xmath284 and @xmath136 .",
    "then @xmath285 first changing the variable using equation @xmath286 , and then @xmath287 , we obtain @xmath288 where @xmath289 is the gamma - function .",
    "also we have @xmath290            we extend the proof in  [ 16 ] for the case of randomization on a sphere ( instead of randomization based on normal distribution ) and for the case when one can calculate the function value only with some error of unknown nature .      taking the expectation with respect to @xmath151",
    "we get @xmath302 taking expectation with respect to @xmath303 and defining @xmath304 we obtain @xmath305 summing up these inequalities and dividing by @xmath306 we obtain ."
  ],
  "abstract_text": [
    "<S> in this paper , we consider a problem of learning supervised pagerank models , which can account for some properties not considered by classical approaches such as the classical pagerank algorithm . due to huge hidden dimension of the optimization problem we use random gradient - free methods to solve it . </S>",
    "<S> we prove a convergence theorem and estimate the number of arithmetic operations needed to solve it with a given accuracy . </S>",
    "<S> we find the best settings of the gradient - free optimization method in terms of the number of arithmetic operations needed to achieve given accuracy of the objective . in the paper </S>",
    "<S> , we apply our algorithm to the web page ranking problem . </S>",
    "<S> we consider a parametric graph model of users behavior and evaluate web pages relevance to queries by our algorithm . </S>",
    "<S> the experiments show that our optimization method outperforms the untuned gradient - free method in the ranking quality . </S>"
  ]
}