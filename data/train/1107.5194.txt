{
  "article_text": [
    "nonnegative matrix factorization ( nmf ) consists in approximating a nonnegative matrix @xmath0 as a low - rank product of two nonnegative matrices @xmath1 and @xmath2 , i.e. , given a matrix @xmath3 and an integer @xmath4 , find two matrices @xmath5 and @xmath6 such that @xmath7 .    with a nonnegative input data matrix @xmath0 , nonnegativity constraints on the factors @xmath1 and @xmath2",
    "are well - known to lead to low - rank decompositions with better interpretation in many applications such as text mining @xcite , image processing @xcite , hyperspectral data analysis @xcite , computational biology @xcite , and clustering @xcite .",
    "unfortunately , imposing these constraints is also known to render the problem computationally difficult @xcite .",
    "since an exact low - rank representation of the input matrix does not exist in general , the quality of the approximation is measured by some criterion , typically the sum of the squares of the errors on the entries , which leads to the following minimization problem : @xmath8 where @xmath9 denotes the frobenius norm of matrix @xmath10 .",
    "most nmf algorithms are iterative , and exploit the fact that reduces to an efficiently solvable convex nonnegative least squares problem ( nnls ) when one of the factors @xmath1 or @xmath2 is fixed .",
    "actually , it seems that nearly all algorithms proposed for nmf adhere to the following general framework    1 .",
    "select initial matrices @xmath11 ( e.g. , randomly ) .",
    "then for @xmath12 , do 2 .",
    "fix @xmath13 and find @xmath14 such that @xmath15 .",
    "3 .   fix @xmath16 and find @xmath17 such that @xmath18 .",
    "more precisely , at each iteration , one of the two factors is fixed and the other is updated in such a way that the objective function is reduced , which amounts to a two - block coordinate descent method .",
    "notice that the role of matrices @xmath1 and @xmath2 is perfectly symmetric : if one transposes input matrix @xmath0 , the new matrix @xmath19 has to be approximated by a product @xmath20 , so that any formula designed to update the first factor in this product directly translates into an update for the second factor in the original problem . formally , if the update performed in step ( a ) is described by @xmath21 , an algorithm preserving symmetry will update the factor in step ( b ) according to @xmath22 . in this paper",
    ", we only consider such symmetrical algorithms , and focus on the update of matrix @xmath1 .",
    "this update can be carried out in many different ways : the most natural possibility is to compute an optimal solution for the nnls subproblem , which leads to a class of algorithms called alternating nonnegative least squares ( anls ) , see , e.g. , @xcite .",
    "however , this computation , which can be performed with active - set - like methods @xcite , is relatively costly",
    ". therefore , since an optimal solution for the nnls problem corresponding to one factor is not required before the update of the other factor is performed , several algorithms only compute an approximate solution of the nnls subproblem , sometimes very roughly , but with a cheaper computational cost , leading to an inexact two - block coordinate descent scheme .",
    "we now present two such procedures : the multiplicative updates of lee and seung and the hierarchical alternating least squares of cichocki et al .    in their seminal papers ,",
    "@xcite introduce the multiplicative updates : @xmath23}{[w^{(k ) } h^{(k ) } { h^{(k)}}^t ] } ,   \\nonumber\\ ] ] where @xmath24 ( resp . )",
    "denotes the component - wise product ( resp . division ) of matrices , and prove that each update monotonically decreases the frobenius norm of the error @xmath25 , i.e. , satisfies the description of steps ( a ) and ( b ) .",
    "this technique was actually originally proposed by @xcite to solve nnls problems .",
    "the popularity of this algorithm came along with the popularity of nmf and many authors have studied or used this algorithm or variants to compute nmf s , see , e.g. , @xcite and the references therein . in particular , the statistics toolbox implements this method .",
    "however , mu have been observed to converge relatively slowly , especially when dealing with dense matrices @xmath0 , see @xcite and the references therein , and many other algorithms have been subsequently introduced which perform better in most situations . for example , @xcite and , independently , several other authors @xcite proposed a technique called hierarchical alternating least squares ( hals ) , which successively updates each column of @xmath1 with an optimal and easy to compute closed - form solution .",
    "in fact , when fixing all variables but a single column @xmath26 of @xmath1 , the problem reduces to @xmath27 because each row of @xmath1 only affects the corresponding row of the product @xmath28 , this problem can be further decoupled into @xmath29 independent quadratic programs in one variable @xmath30 , corresponding to the @xmath31 row of @xmath0 .",
    "the optimal solution @xmath32 of these subproblems can be easily written in closed - form @xmath33 hence hals updates successively the columns of @xmath1 , so that @xmath34 can be computed in the following way : @xmath35 @xmath36 , where @xmath37 and @xmath38 .",
    "this amounts to approximately solving each nnls subproblem in @xmath1 with a single complete round of an exact block - coordinate descent method with @xmath39 blocks of @xmath29 variables corresponding to the columns of @xmath1 ( notice that any other ordering for the update of the columns of @xmath1 is also possible ) .",
    "other approaches based on iterative methods to solve the nnls subproblems include projected gradient descent @xcite or newton - like methods  @xcite ; see also @xcite and the references therein .",
    "+ we first analyze in section  [ ccost ] the computational cost needed to update the factors @xmath1 in mu and hals , then make several simple observations leading in section [ accelalgo ] to the design of accelerated versions of these algorithms .",
    "these improvements can in principle be applied to any two - block coordinate descent nmf algorithm , as demonstrated in section  [ appllin ] on the projected gradient method of @xcite .",
    "we mainly focus on mu , because it is by far the most popular nmf algorithm , and on hals , because it is very efficient in practice .",
    "section  [ conv ] studies convergence of the accelerated variants to stationary points , and shows that they preserve the properties of the original schemes . in section",
    "[ ne ] , we experimentally demonstrate a significant acceleration in convergence on several image and text datasets , with a comparison with the state - of - the - art anls algorithm of @xcite .",
    "in order to make our analysis valid for both dense and sparse input matrices , let us introduce a parameter @xmath40 denoting the number of nonzero entries in matrix @xmath0 ( @xmath41 when @xmath0 is dense ) .",
    "factors @xmath1 and @xmath2 are typically stored as dense matrices throughout the execution of the algorithms .",
    "we assume that nmf achieves compression , which is often a requirement in practice .",
    "this means that storing @xmath1 and @xmath2 must be cheaper than storing @xmath0 : roughly speaking , the number of entries in @xmath1 and @xmath2 must be smaller than the number of nonzero entries in @xmath0 , i.e. , @xmath42 .    descriptions of algorithms  [ mualgo ] and [ halsalgo ] below provide separate estimates for the number of floating point operations ( flops ) in each matrix product computation needed to update factor @xmath1 in mu and hals .",
    "one can check that the proposed organization of the different matrix computations ( and , in particular , the ordering of the matrix products ) minimizes the total computational cost ( for example , starting the computation of the mu denominator @xmath43 with the product @xmath44 is clearly worse than with @xmath45 ) .",
    "@xmath46 ; @xmath47 flops @xmath48 ; @xmath49 flops @xmath50 ; @xmath51 flops @xmath52}{[c]}$ ] ; @xmath53 flops + % total : @xmath54 flops    @xmath46 ; @xmath47 flops @xmath48 ; @xmath49 flops @xmath55 ; @xmath56 flops @xmath57 ; @xmath58 flops + % total : @xmath59 flops    mu and hals possess almost exactly the same computational cost ( the difference being a typically negligible @xmath60 flops ) .",
    "it is particularly interesting to observe that    1 .",
    "steps 1 . and 2 .",
    "in both algorithms are identical and do not depend on the matrix @xmath61 ; 2 .   recalling our assumption @xmath62 , computation of @xmath63 ( step 1 . )",
    "is the most expensive among all steps .",
    "therefore , this time - consuming step should be performed sparingly , and we should take full advantage of having computed the relatively expensive @xmath63 and @xmath64 matrix products .",
    "this can be done by updating @xmath61 several times before the next update of @xmath13 , i.e. , by repeating steps 3 . and 4 .",
    "in mu ( resp .",
    "steps 3 . to 6 . in hals ) several times after the computation of matrices @xmath63 and @xmath64 . in this fashion ,",
    "better solutions of the corresponding nnls subproblems will be obtained at a relatively cheap additional cost .",
    "the original mu and hals algorithms do not take advantage of this fact , and alternatively update matrices @xmath1 and @xmath2 only once per ( outer ) iteration .",
    "an important question for us is now : how many times should we update @xmath1 per outer iteration ?",
    "i.e. , how many inner iterations of mu and hals should we perform ?",
    "this is the topic of the next section .",
    "in this section , we discuss two different strategies for choosing the number of inner iterations : the first uses a fixed number of inner iterations determined by the flop counts , while the second is based on a dynamic stopping criterion that checks the difference between two consecutive iterates .",
    "the first approach is shown empirically to work better .",
    "we also describe a third hybrid strategy that provides a further small improvement in performance .",
    "let us focus on the mu algorithm ( a completely similar analysis holds for hals , as both methods differ only by a negligible number of flops ) .",
    "based on the flops counts , we estimate how expensive the first inner update of @xmath1 would be relatively to the next ones ( all performed while keeping @xmath2 fixed ) , which is given by the following factor @xmath65 ( the corresponding value for @xmath2 will be denoted by @xmath66 ) @xmath67 values of @xmath65 and @xmath66 for several datasets are given in section  [ ne ] , see tables  [ dip ] and [ dtm ] .    notice that for @xmath62 , we have @xmath68 so that the first inner update of @xmath1 is at least about twice as expensive as the subsequent ones . for a dense matrix",
    ", @xmath40 is equal to @xmath69 and we actually have that @xmath70 , which is typically quite large since @xmath71 is often much greater than @xmath39 .",
    "this means for example that , in our accelerated scheme , @xmath1 could be updated about @xmath72 times for the same computational cost as two independent updates of @xmath1 in the original mu .",
    "a simple and natural choice consists in performing inner updates of @xmath1 and @xmath2 a fixed number of times , depending on the values of @xmath65 and @xmath66 .",
    "let us introduce a parameter @xmath73 such that @xmath1 is updated @xmath74  times before the next update of @xmath2 , and @xmath2 is updated @xmath75  times before the next update of @xmath1 .",
    "let us also denote the corresponding algorithm mu@xmath76 ( mu@xmath77 reduces to the original mu ) .",
    "therefore , performing @xmath74 inner updates of @xmath1 in mu@xmath76 has approximately the same computational cost as performing @xmath78 updates of @xmath1 in mu@xmath77 .    in order to find an appropriate value for parameter @xmath79",
    ", we have performed some preliminary tests on image and text datasets .",
    "first , let us denote @xmath80 the frobenius norm of the error @xmath25 achieved by an algorithm within time @xmath81 , and define @xmath82 where @xmath83 is the error of the initial iterate @xmath11 , and @xmath84 is the smallest error observed among all algorithms across all initializations .",
    "quantity @xmath85 is therefore a normalized measure of the improvement of the objective function ( relative to the initial gap ) with respect to time ; we have @xmath86 for monotonically decreasing algorithms ( such as mu and hals ) .",
    "the advantage of @xmath85 over @xmath80 is that one can meaningfully take the average over several runs involving different initializations and datasets , and display the average behavior of a given algorithm .",
    "figure  [ mualpha ] displays the average of this function @xmath85 for dense ( on the left ) and sparse ( on the right ) matrices using the datasets described in section  [ ne ] for five values of @xmath87 .",
    "we observe that the original mu algorithm ( @xmath88 ) converges significantly less rapidly than all the other tested variants ( especially in the dense case ) .",
    "the best value for parameter @xmath79 seems to be @xmath89 .",
    "figure  [ halsalpha ] displays the same computational experiments for hals and rows of @xmath2 , we observed that an update of hals is noticeably slower than an update of mu when using ( especially for @xmath90 ) , despite the quasi - equivalent theoretical computational cost .",
    "therefore , to obtain fair results , we adjusted @xmath65 and @xmath66 by measuring directly the ratio between time spent for the first update and the next one , using the _ cputime _ function of . ] .",
    "hals with @xmath91 performs the best .",
    "for sparse matrices , the improvement is harder to discern ( but still present ) ; an explanation for that fact will be given at the end of section  [ hybrid ] .",
    "in the previous section , a fixed number of inner iterations is performed .",
    "one could instead consider switching dynamically from one factor to the other based on an appropriate criterion .",
    "for example , it is possible to use the norm of the projected gradient as proposed by @xcite . a simpler and cheaper possibility is to rely solely on the norm of the difference between two iterates .",
    "noting @xmath92 the iterate after @xmath93 updates of @xmath61 ( while @xmath13 is being kept fixed ) , we stop inner iterations as soon as @xmath94 shows the results for mu with different values of @xmath95 ( we also include the original mu and mu with @xmath96 presented in the previous section to serve as a comparison ) .",
    "figures  [ halseps ] displays the same experiment for hals .        in both cases , we observe that the dynamic stopping criterion is not able to outperform the approach based on a fixed number of inner iterations ( @xmath96 for mu , @xmath97 for hals ) . moreover , in the experiments for hals with sparse matrices , it is not even able to compete with the original algorithm .",
    "we have shown in the previous section that using a fixed number of inner iterations works better than a stopping criterion based solely on the difference between two iterates .",
    "however , in some circumstances , we have observed that inner iterations become ineffective before their maximal count is reached , so that it would in some cases be worth switching earlier to the other factor .",
    "this occurs in particular when the numbers of rows @xmath29 and columns @xmath71 of matrix @xmath0 have different orders of magnitude .",
    "for example , assume without loss of generality that @xmath98 , so that we have @xmath99 .",
    "hence , on the one hand , matrix @xmath1 has significantly less entries than @xmath2 ( @xmath100 ) , and the corresponding nnls subproblem features a much smaller number of variables ; on the other hand , @xmath99 so that the above choice will lead many more updates of @xmath1 performed . in other words , many more iterations are performed on the simpler problem , which might be unreasonable .",
    "for example , for the cbcl face database ( cf .",
    "section  [ ne ] ) with @xmath101 , @xmath102 and @xmath103 , we have @xmath104 and @xmath105 , and this large number of inner @xmath1-updates is typically not necessary to obtain an iterate close to an optimal solution of the corresponding nnls subproblem .",
    "+ therefore , to avoid unnecessary inner iterations , we propose to combine the fixed number of inner iterations proposed in section  [ fnit ] with the supplementary stopping criterion described in section  [ dcit ] .",
    "this safeguard procedure will stop the inner iterations before their maximum number @xmath106 is reached when they become ineffective ( depending on parameter @xmath95 , see equation  ) .",
    "algorithm  [ anmf ] displays the pseudocode for the corresponding accelerated mu , as well as a similar adaptation for hals .",
    "data matrix @xmath3 and initial iterates @xmath107 .",
    "compute @xmath46 and @xmath48 ; @xmath108 ; compute @xmath92 using either mu or hals ( cf .",
    "algorithms  [ mualgo ] and [ halsalgo ] ) ; break ; @xmath109 ; compute @xmath110 from @xmath13 and @xmath16 using a symmetrically adapted version of steps 2 - 9 ;    figures  [ mualphaeps ] and [ halsalphaeps ] displays the numerical experiments for mu and hals respectively .            in the dense case , this safeguard procedure slightly improves performance .",
    "we also note that the best values of parameter @xmath79 now seem to be higher than in the unsafeguarded case ( @xmath111 versus @xmath112 for mu , and @xmath112 versus @xmath91 for hals ) .",
    "worse performance of those higher values of @xmath79 in the unsafeguarded scheme can be explained by the fact that additional inner iterations , although sometimes useful , become too costly overall if they are not stopped when becoming ineffective .    in the sparse case ,",
    "the improvement is rather limited ( if not absent ) and most accelerated variants provide similar performances .",
    "in particular , as already observed in sections  [ fnit ] and [ dcit ] , the accelerated variant of hals does not perform very differently from the original hals on sparse matrices .",
    "we explain this by the fact that hals applied on sparse matrices is extremely efficient and one inner update already decreases the objective function significantly . to illustrate this ,",
    "figure  [ densevssparse ] shows the evolution of the relative error @xmath113 of the iterate @xmath92 for a sparse matrix @xmath0 , where @xmath114 . recall that @xmath115 denotes the solution obtained after @xmath116 outer iterations ( starting from randomly generated matrices ) .    for @xmath117 ( resp .",
    "@xmath118 ) , the relative error is reduced by a factor of more than 87% ( resp .",
    "97% ) after only one inner iteration .",
    "the accelerating procedure described in the previous sections can potentially be applied to many other nmf algorithms . to illustrate this ,",
    "we have modified lin s projected gradient algorithm ( pg ) @xcite by replacing the original dynamic stopping criterion ( based on the stationarity conditions ) by the hybrid strategy described in section  [ hybrid ] .",
    "it is in fact straightforward to see that our analysis is applicable in this case , since lin s algorithm also requires the computation of @xmath45 and @xmath119 when updating @xmath1 , because the gradient of the objective function in is given by @xmath120 .",
    "this is also a direct confirmation that our approach can be straightforwardly applied to many more nmf algorithms than those considered in this paper .",
    "figure  [ linalpha ] displays the corresponding computational results , comparing the original pg algorithm ( as available from @xcite ) with its dynamic stopping criterion ( based on the norm of the projected gradient ) and our variants , based on a ( safeguarded ) fixed number of inner iterations .",
    "it demonstrates that our accelerated schemes perform significantly better , both in the sparse and dense cases ( notice that in the sparse case , most accelerated variants perform similarly ) .",
    "the choice @xmath97 gives the best results , and        the safeguard procedure does not help much ; the reason being that pg converges relatively slowly ( we will see in section  [ ne ] that its accelerated variant converges slower than the accelerated mu ) .",
    "in this section , we briefly recall convergence properties of both mu and hals , and show that they are inherited by their accelerated variants .",
    "it was shown by @xcite and later by @xcite that a single multiplicative update of @xmath1 ( i.e. , replacing @xmath1 by @xmath121}{[whh^t]}$ ] while @xmath2 is kept fixed ) guarantees that the objective function @xmath122 does not increase . since our accelerated variant simply performs several updates of @xmath1 while @xmath2 is unchanged ( and vice versa )",
    ", we immediately obtain that the objective function @xmath122 is non - increasing under the iterations of algorithm  [ anmf ] .",
    "unfortunately , this property does not guarantee convergence to a stationary point of , and this question on the convergence of the mu seems to be still open , see @xcite .",
    "furthermore , in practice , rounding errors might set some entries in @xmath1 or @xmath2 to zero , and then multiplicative updates can not modify their values .",
    "hence , it was observed that despite their monotonicity , mu do not necessarily converge to a stationary point , see @xcite .",
    "however , @xcite proposed a slight modification of mu in order to obtain the convergence to a stationary point .",
    "roughly speaking , mu is recast as a rescaled gradient descent method and the step length is modified accordingly .",
    "another even simpler possibility is proposed by @xcite who proved the following theorem ( see also @xcite where the influence of parameter @xmath123 is discussed ) :    [ epsmu ] for any constant @xmath124 , @xmath125 and any means that @xmath1 and @xmath2 are component - wise larger than @xmath123 . ]",
    "@xmath126 , @xmath25 is nonincreasing under @xmath127}{[w h h^t]}\\big ) , \\qquad h \\leftarrow \\max\\big(\\delta , h \\circ \\frac{[w^t m]}{[w^t w h ] } \\big ) ,   \\label{lsupdateeps}\\ ] ] where the @xmath128 is taken component - wise .",
    "moreover , every limit point of the corresponding ( alternated ) algorithm is a stationary point of the following optimization problem @xmath129    the proof of theorem  [ epsmu ] only relies on the fact that the limit points of the updates are fixed points ( there always exists at least one limit point because the objective function is bounded below and non - increasing under updates ) .",
    "therefore , one can easily check that the proof still holds when a bounded number of inner iterations is performed , i.e. , the theorem applies to our accelerated variant ( cf .",
    "algorithm  [ anmf ] ) .",
    "it is important to realize that this is not merely a theoretical issue and that this observation can really play a crucial role in practice . to illustrate this ,",
    "figure  [ ceps ] shows the evolution of the normalized objective function ( cf .",
    "equation  ) using @xmath130 and @xmath131 starting from the same initial matrices @xmath132 and @xmath133 randomly generated ( each entry uniformly drawn between 0 and 1 ) .",
    "we observe that , after some number of iterations , the original mu ( i.e. , with @xmath130 ) get stuck while the variant with @xmath131 is still able to slightly improve @xmath1 and @xmath2 .",
    "notice that this is especially critical on sparse matrices ( see figure  [ ceps ] , right ) because many more entries of @xmath1 and @xmath2 are expected to be equal to zero at stationarity .",
    "for this reason , in this paper , all numerical experiments with mu use the updates from equation   with @xmath131 ( instead of the original version with @xmath130 ) .",
    "hals is an exact block - coordinate descent method where blocks of variables ( columns of @xmath1 and rows of @xmath2 ) are optimized in a cyclic way ( first the columns of @xmath1 , then the rows of @xmath2 , etc . ) .",
    "clearly , exact block - coordinate descent methods always guarantee the objective function to decrease .",
    "however , convergence to a stationary point requires additional assumptions .",
    "for example , @xcite ( proposition 2.7.1 ) showed that if the following three conditions hold :    * each block of variables belongs to a closed convex set ( which is the case here since the blocks of variables belong either to @xmath134 or @xmath135 ) , * the minimum computed at each iteration for a given block of variables is uniquely attained ; * the function is monotonically nonincreasing in the interval from one iterate to the next ;    then exact block - coordinate descent methods converge to a stationary point .",
    "the second and the third requirements are satisfied as long as no columns of @xmath1 and no rows of @xmath2 become completely equal to zero ( subproblems are then strictly convex quadratic programs , whose unique optimal solutions are given by the hals updates , see section  [ intro ] ) . in practice , if a column of @xmath1 or a row of @xmath2 becomes zero , we reinitialize it to a small positive constant ( we used @xmath136 ) .",
    "we refer the reader to @xcite and @xcite for more details on the convergence issues related to hals . because our accelerated variant of hals is just another type of exact block - coordinate descent method ( the only difference being that the variables are optimized in a different order : first several times the columns of @xmath1 , then several times the rows of @xmath2 , etc . ) , it inherits all the above properties .",
    "in fact , the statement of the above - mentioned theorem in @xcite mentions that ` the order of the blocks may be arbitrary as long as there is an integer @xmath40 such that each block - component is iterated at least once in every group of @xmath40 contiguous iterations ' , which clearly holds for our accelerated algorithm with a fixed number of inner iterations and its hybrid variant ) might not satisfy this requirement , because the number of inner iterations for each outer iteration can in principle grow indefinitely in the course of the algorithm . ] .",
    "in this section , we compare the following algorithms , choosing for our accelerated mu , hals and pg schemes the hybrid stopping criterion and the best compromise for values for parameters @xmath79 and @xmath95 according to tests performed in section  [ accelalgo ] .    1 .",
    "( * mu * ) the multiplicative updates algorithm of @xcite .",
    "( * a - mu * ) the accelerated mu with a safeguarded fixed number of inner iterations using @xmath137 and @xmath138 ( cf . algorithm  [ anmf ] ) .",
    "( * hals * ) the hierarchical alternating least squares algorithm of @xcite .",
    "( * a - hals * ) the accelerated hals with a safeguarded fixed number of inner iterations using @xmath97 and @xmath138 ( cf .",
    "algorithm  [ anmf ] ) .",
    "( * pg * ) the projected gradient method of @xcite .",
    "( * a - pg * ) the modified projected gradient method of @xcite using @xmath97 and @xmath139 ( cf",
    ". section  [ appllin ] ) . 7 .",
    "( * anls * ) the alternating nonnegative least squares algorithm of @xcite , which alternatively optimizes @xmath1 and @xmath2 exactly using a block - pivot active set method .",
    "kim and park showed that their method typically outperforms other tested algorithms ( in particular mu and pg ) on synthetic , images and text datasets .",
    "all tests were run using 7.1 ( r14 ) , on a 3ghz intel@xmath140 core2 dual core processor .",
    "we present numerical results on images datasets ( dense matrices , section  [ i m a ] ) and on text datasets ( sparse matrices , section  [ txt ] ) .",
    "code for all algorithms but anls is available at    http://sites.google.com / site / nicolasgillis/.",
    "table  [ dip ] summarizes the characteristics of the different datasets .    .image datasets . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     the factorization rank @xmath39 was set to 10 and 20 . for the comparison",
    ", we used the same settings as for the dense matrices .",
    "figure  [ sparsemat ] displays for each dataset the evolution of the average of functions @xmath85 over all runs .",
    "again the accelerated algorithms are much more efficient .",
    "in particular , a - mu and a - pg converge initially much faster than anls , and also obtain better final solutions .",
    "a - mu , hals and a - hals have the fastest initial convergence rates , and hals and a - hals generate the best solutions in all cases .",
    "notice that a - hals does not always obtain better final solutions than hals ( still this happens on half of the datasets ) , because hals already performs remarkably well ( see discussion at the end of section  [ hybrid ] ) .",
    "however , the initial convergence of a - hals is in all cases at least as fast as that of hals .",
    "in this paper , we considered the multiplicative updates of @xcite and the hierarchical alternating least squares algorithm of @xcite .",
    "we introduced accelerated variants of these two schemes , based on a careful analysis of the computational cost spent at each iteration , and preserve the convergence properties of the original algorithms .",
    "the idea behind our approach is based on taking better advantage of the most expensive part of the algorithms , by repeating a ( safeguarded ) fixed number of times the cheaper part of the iterations .",
    "this technique can in principle be applied to most nmf algorithms ; in particular , we showed how it can substantially improve the projected gradient method of @xcite .",
    "we then experimentally showed that these accelerated variants , despite the relative simplicity of the modification , significantly outperform the original ones , especially on dense matrices , and compete favorably with a state - of - the - art algorithm , namely the anls method of @xcite . a direction for future research would be to choose the number of inner iterations in a more sophisticated way , with the hope of further improving the efficiency of a - mu , a - pg and a - hals .",
    "finally , we observed that hals and its accelerated version are the most efficient variants for solving nmf problems , sometimes by far .",
    "berry , m.w . ,",
    "browne , m. , langville , a.n . ,",
    "pauca , p.v . & plemmons , r.j .",
    "algorithms and applications for approximate nonnegative matrix factorization . _ computational statistics and data analysis _ , _ 52 _ , 155173 .",
    "cichocki , a. , amari , s. , zdunek , r. & phan , a.h .",
    "non - negative matrix and tensor factorizations : applications to exploratory multi - way data analysis and blind source separation .",
    "_ wiley - blackwell_."
  ],
  "abstract_text": [
    "<S> nonnegative matrix factorization ( nmf ) is a data analysis technique used in a great variety of applications such as text mining , image processing , hyperspectral data analysis , computational biology , and clustering . in this paper , we consider two well - known algorithms designed to solve nmf problems , namely the multiplicative updates of lee and seung and the hierarchical alternating least squares of cichocki et al . </S>",
    "<S> we propose a simple way to significantly accelerate these schemes , based on a careful analysis of the computational cost needed at each iteration , while preserving their convergence properties . </S>",
    "<S> this acceleration technique can also be applied to other algorithms , which we illustrate on the projected gradient method of lin . </S>",
    "<S> the efficiency of the accelerated algorithms is empirically demonstrated on image and text datasets , and compares favorably with a state - of - the - art alternating nonnegative least squares algorithm .    </S>",
    "<S> * keywords : * nonnegative matrix factorization , algorithms , multiplicative updates , hierarchical alternating least squares . </S>"
  ]
}