{
  "article_text": [
    "there are several practical scenarios where it is inappropriate to assume that the distribution of the observations does not change .",
    "for example , financial datasets can exhibit alternate behaviours due to crisis periods . in this case",
    "it is sensible to assume changes in the underlying distribution .",
    "the change in the distribution can be either in the value of one or more of the parameters or , more in general , on the family of the distribution . in the latter case , one may deem appropriate to consider a normal density for the stagnation periods , while a student @xmath0 , with relatively heavy tails , may be more suitable to represents observations in the more turbulent data of a crisis .",
    "the task of identifying if , and when , one or more changes have occurred is not trivial and requires appropriate methods to avoid detection of a large number of changes or , at the opposite extreme , seeing no changes at all .",
    "whilst the literature covering change point analysis from a bayesian perspective is vast when prior distributions are elicited , the documentation referring to analysis under minimal prior information ( i.e. objective bayes ) , is very limited .",
    "in fact , to the best of our knowledge , only two papers tackled the problem from an objective point of view : @xcite and @xcite .",
    "the former discusses the single change point problem in a model selection setting , whilst the latter , which is an extension of the former , tackles the multivariate change point problem in the context of linear regression models .",
    "this work aims to contribute to the methodology for change point analysis under the assumption that the information about the number of change points and their location is minimal .",
    "first , we discuss the definition of an objective prior for change point location , both for single and multiple changes , assuming the number of changes is known a priori .",
    "then , we define a prior on the number of change points via a model selection approach . here",
    ", we assume that the change point coincides with one of the observations .",
    "as such , given @xmath1 data points , the change point location is discrete . to the best of our knowledge ,",
    "the sole objective approach to define prior distributions on discrete spaces is the one introduced by @xcite .",
    "to illustrate their idea , consider a probability distribution @xmath2 , where @xmath3 is a discrete parameter .",
    "then , the prior @xmath4 is obtained by objectively measuring what is lost if the value @xmath5 is removed from the parameter space , and it is the true value . according to @xcite , if a model is misspecified , the posterior distribution asymptomatically accumulates on the model which is the most similar to the true one , where the similarity is measured in terms of the kullback ",
    "leibler ( kl ) divergence .",
    "therefore , @xmath6 , where @xmath7 is the parameter characterising the nearest model to @xmath2 , represents the utility of keeping @xmath5 .",
    "the objective prior is then obtained by linking the aforementioned utility via the self - information loss : @xmath8 where the kullback ",
    "leibler divergence @xcite from the sampling distribution with density @xmath2 to the one with density @xmath9 is defined as : @xmath10{\\mathop{}\\!\\mathrm{d}}x .",
    "\\label{kldef}\\ ] ] throughout the paper , the objective prior defined in equation will be referenced as the loss - based prior .",
    "this approach is used to define an objective prior distribution when the number of change points is known a priori . to obtain a prior distribution for the number of change points",
    ", we adopt a model selection approach based on the results in @xcite , where a method to define an objective prior on the space of models is proposed . to illustrate ,",
    "let us consider @xmath11 bayesian models : @xmath12    where @xmath13 is the sampling density characterised by @xmath14 and @xmath15 represents the prior on the model parameter .",
    "assuming the prior on the model parameter , @xmath15 , is proper , the model prior probability @xmath16 is proportional to the expected minimum kullback  leibler divergence from @xmath17 , where the expectation is considered with respect to @xmath15 .",
    "that is : @xmath18\\right\\rbrace \\qquad     j=1 , \\ldots , k. \\label{villamodelcompactorg}\\end{aligned}\\ ] ]    the model prior probabilities defined in equation can be employed to derive the model posterior probabilities through : @xmath19^{-1 } , \\label{modelposterior}\\ ] ] where @xmath20 is the bayes factor between model @xmath21 and model @xmath22 , defined as @xmath23 with @xmath24 .",
    "the paper is structured as follows . in section [ sc_locations ]",
    "we establish the way we set objective priors on both single and multiple change point locations .",
    "section [ sc_numbercp ] shows how we define the model prior probabilities for the number of change point locations .",
    "illustrations of the model selection exercise are provided in sections [ sc_simulation ] and [ sc_realdata ] , where we work with simulated and real data , respectively .",
    "section [ conclusion ] is dedicated to final remarks .",
    "this section is devoted to the derivation of the loss - based prior when the number of change points is known a priori .",
    "specifically , let @xmath11 be the number of change points and @xmath25 their locations .",
    "we introduce the idea in the simple case where we assume that there is only one change point in the dataset ( see section [ sub_sc_onechgpoint ] ) .",
    "then , we extend the results to the more general case where multiple change points are assumed ( see section [ sub_sc_multiplechgpoints ] ) .      here , we show that the loss - based prior for the single change point case coincides with the discrete uniform over the set @xmath26 .",
    "let @xmath27 denote an _",
    "n_-dimensional vector of random variables , representing the random sample , and @xmath5 be our single change point location , that is @xmath28 , such that @xmath29{\\mbox{\\normalfont\\tiny\\sffamily i.i.d.}}}{\\sim}}}f_{1}(\\cdot|\\tilde\\theta_{1 } ) \\nonumber \\\\ x_{m+1 } , \\ldots , x_{n}| \\tilde\\theta_{2 } & { \\mathrel{\\overset{\\makebox[0pt]{\\mbox{\\normalfont\\tiny\\sffamily i.i.d.}}}{\\sim}}}f_{2}(\\cdot|\\tilde\\theta_{2 } ) .",
    "\\label{singlechangepointcase}\\end{aligned}\\ ] ] note that we assume that there is a change point in the series , as such the space of @xmath5 does not include the case @xmath30 .",
    "in addition , we assume that @xmath31 when @xmath32 . the sampling density for the vector of observations",
    "@xmath33 is : @xmath34 let @xmath35 .",
    "then , the kullback  leibler divergence between the model parametrised by @xmath5 and the one parametrised by @xmath7 is : @xmath36 without loss of generality , consider @xmath37 . in this case , note that @xmath38 leading to @xmath39 on the right hand side of equation , we can recognise the kullback  leibler divergence from density @xmath40 to density @xmath41 , thus getting : @xmath42 in a similar fashion , when @xmath43 , we have that : @xmath44    in this single change point scenario , we can consider @xmath7 as a perturbation of the change point location @xmath5 , that is @xmath45 where @xmath46 , such that @xmath47 .",
    "then , taking into account equations and , the kullback ",
    "leibler divergence becomes :    @xmath48    and @xmath49=\\nonumber \\\\&=\\min_{m^{\\prime}\\neq m}\\lbrace l \\cdot d_{kl}(f_{2}(\\cdot|\\tilde\\theta_{2})\\|f_{1}(\\cdot|\\tilde\\theta_{1})),l \\cdot d_{kl}(f_{1}(\\cdot|\\tilde\\theta_{1})\\|f_{2}(\\cdot|\\tilde\\theta_{2}))\\rbrace\\nonumber \\\\&= \\min_{m^{\\prime}\\neq m}\\lbrace d_{kl}(f_{2}(\\cdot|\\tilde\\theta_{2})\\|f_{1}(\\cdot|\\tilde\\theta_{1})),d_{kl}(f_{1}(\\cdot|\\tilde\\theta_{1})\\|f_{2}(\\cdot|\\tilde\\theta_{2}))\\rbrace   \\cdot \\underbrace{\\min_{m^{\\prime}\\neq m}\\{l\\}}_{1}. \\label{kl_no_m}\\end{aligned}\\ ] ] we observe that equation is only a function of @xmath50 and @xmath51 and does not depend on @xmath5 .",
    "thus , @xmath52 and , therefore , @xmath53 this prior was for example used in an econometric context by @xcite with the rationale of giving equal weight to every possible change point location .      in this section",
    ", we address the change point problem in its generality by assuming that there are @xmath54 change points .",
    "in particular , for the data @xmath33 , we consider the following sampling distribution @xmath55 where @xmath56 , @xmath57 , is the vector of the change point locations and @xmath58 is the vector of the parameters of the underlying probability distributions .",
    "schematically : @xmath59{\\mbox{\\normalfont\\tiny\\sffamily i.i.d.}}}{\\sim}}}f_{1}(\\cdot|\\tilde\\theta_{1 } ) \\\\",
    "x_{m_{1}+1 } & , \\ldots , & x_{m_2}|\\tilde\\theta_{2 } & { \\mathrel{\\overset{\\makebox[0pt]{\\mbox{\\normalfont\\tiny\\sffamily i.i.d.}}}{\\sim}}}f_{2}(\\cdot|\\tilde\\theta_{2})\\\\ \\vdots&,\\ldots , & \\vdots & \\vdots\\ldots\\vdots\\\\ x_{m_{k-1}+1 } & , \\ldots , & x_{m_k}|\\tilde\\theta_{k } & { \\mathrel{\\overset{\\makebox[0pt]{\\mbox{\\normalfont\\tiny\\sffamily i.i.d.}}}{\\sim}}}f_{k}(\\cdot|\\tilde\\theta_{k})\\\\ x_{m_{k}+1 } & , \\ldots , & x_{n}|\\tilde\\theta_{k+1 } & { \\mathrel{\\overset{\\makebox[0pt]{\\mbox{\\normalfont\\tiny\\sffamily i.i.d.}}}{\\sim}}}f_{k+1}(\\cdot|\\tilde\\theta_{k+1}).\\\\ \\end{array}\\ ] ] if @xmath60 , then it is reasonable to assume @xmath61 . in a similar fashion to the single change point case",
    ", we can not assume @xmath62 since we require exactly @xmath11 change points .",
    "in this case , due to the multivariate nature of the vector @xmath56 , the derivation of the loss - based prior is not as straightforward as in the one dimensional case .",
    "in fact , the derivation of the prior is based on heuristic considerations supported by the below theorem [ lemmaonechgpoint ] ( which proof is in the appendix ) . in particular , we are able to prove an analogous of equations and when only one component is arbitrarily perturbed .",
    "let us define the following functions : @xmath63 where @xmath64 .",
    "the following theorem is useful to understand the behaviour of the loss - based prior in the general case .",
    "+    let @xmath65 be the sampling distribution defined in equation and consider @xmath66 .",
    "let @xmath67 be such that @xmath68 for @xmath69 , and let the component @xmath70 be such that @xmath71 and @xmath72 .",
    "therefore , @xmath73 where @xmath74 .",
    "[ lemmaonechgpoint ]    note that , theorem [ lemmaonechgpoint ] states that the minimum kullback ",
    "leibler divergence is achieved when @xmath75 or @xmath76 .",
    "this result is not surprising since the kullback  leibler divergence measures the degree of similarity between two distributions .",
    "the smaller the perturbation caused by changes in one of the parameters is , the smaller the kullback  leibler divergence between the two distributions is . if now we consider the general case of having @xmath11 change points , it is straightforward to see that the kullback ",
    "leibler divergence is minimized when only one of the components of the vector @xmath77 is perturbed by ( plus or minus ) one unit . as such",
    ", the loss - based prior depends on the vector of parameters @xmath78 only , as in the one - dimensional case , yielding the uniform prior for @xmath77 .",
    "therefore , the loss - based prior on the multivariate change point location is @xmath79 where @xmath80 .",
    "the denominator in equation has the above form because , for every number of @xmath11 change points , we are interested in the number of @xmath11-subsets from a set of @xmath81 elements , which is @xmath82 . the same prior was also derived in a different way by @xcite .",
    "here , we approach the change point analysis as a model selection problem . in particular , we define an objective prior on the space of models , where each model represents a certain number of change points ( including the case of no change points ) .",
    "the method adopted to define an objective prior on the space of models is the one introduced in @xcite .",
    "we proceed as follows .",
    "assume we have to select from @xmath83 possible models .",
    "let @xmath84 be the model with no change points , @xmath85 the model with one change point and so on .",
    "generalising , model @xmath86 corresponds to the model with @xmath11 change points .",
    "the idea is that the current model encompasses the change point locations of the previous model . as an example , in model @xmath87 the first two change point locations will be the same as in the case of model @xmath88 . to illustrate the way we envision our models , we have provided figure [ modelconstruct ] . keeping in line with the notation used during the introductory section , for the model @xmath89 , with @xmath90 a non - negative integer , the model parameter @xmath91 is equivalent to the vector @xmath92 .",
    "here , @xmath93 represent the parameters of the underlying sampling distributions considered under model @xmath89 , and @xmath94 are the respective @xmath11 change point locations , as in figure [ modelconstruct ] .",
    "based on the way we have specified our models , which are in direct correspondence with the number of change points and their locations , we state theorem [ remark_model ] ( which proof is in the appendix ) .",
    "let @xmath95 for any @xmath96 integers , with @xmath90 , and the convention @xmath97 , we have the following : @xmath98,\\end{aligned}\\ ] ] and @xmath99.\\end{aligned}\\ ] ] [ remark_model ]    the result in theorem [ remark_model ] is useful when the model selection exercise is implemented .",
    "indeed , the @xcite approach requires the computation of the kullback ",
    "leibler divergences in theorem [ remark_model ] . recalling equation , the objective priors on the model prior probabilities are given by : @xmath100\\right\\rbrace\\qquad j=0 , 1 , \\ldots , k. \\label{modelpriors}\\end{aligned}\\ ] ]    for illustrative purposes , in the appendix we derive the model prior probabilities to perform model selection among @xmath101 , @xmath102 and @xmath103 .",
    "+ * remark . * in the case where the changes in the underlying sampling distribution are limited to the parameter values , the model prior probabilities defined in follow the uniform distribution .",
    "that is , @xmath104 . in the real data example",
    "illustrated in section [ sub_sc_british_coal_mine ] , we indeed consider a problem where the above case occurs .",
    "let us consider the case where we have to estimate whether there is or not a change point in a set of observations .",
    "this implies that we have to choose between model @xmath101 ( i.e. no change point ) and @xmath102 ( i.e. one change point ) .",
    "following our approach , we have :    @xmath105\\right\\ } ,   \\label{prior_m0_one_chg}\\end{aligned}\\ ] ]    and @xmath106\\right\\ }   \\label{prior_m1_one_chg}.\\end{aligned}\\ ] ]    now , let us assume independence between the prior on the change point location and the prior on the parameters of the underlying sampling distributions , that is @xmath107 .",
    "let us further recall that , as per equation , @xmath108 . as such , we observe that the model prior probability on @xmath102 becomes : @xmath109\\right\\rbrace .",
    "\\label{prior_m1_trans_one_chg}\\end{aligned}\\ ] ]    we notice that the model prior probability for model @xmath102 is increasing when the sample size increases .",
    "this behaviour occurs whether there is or not a change point in the data .",
    "we propose to address the above problem by using a non - uniform prior for @xmath110 .",
    "a reasonable alternative , which works quite well in practice , would be to use the following shifted binomial as prior : @xmath111 to argument the choice of , we note that as @xmath112 increases , the probability mass will be more and more concentrated towards the upper end of the support .",
    "therefore , from equations and follows : @xmath113\\right\\rbrace .",
    "\\label{prior_m1_pseudo_binom}\\end{aligned}\\ ] ] for the more general case where one considers more than two models , the problem highlighted in equation vanishes .",
    "in this section , we present the results of several simulation studies based on the methodologies discussed in sections [ sc_locations ] and [ sc_numbercp ] .",
    "we start with a scenario involving discrete distributions in the context of the one change point problem .",
    "we then show the results obtained when we consider continuous distributions for the case of two change points .",
    "the choice of the underlying sampling distribution is in line with @xcite .",
    "[ [ scenario-1 . ] ] scenario 1 .",
    "+ + + + + + + + + + +    the first scenario concerns the choice between models @xmath101 and @xmath102 .",
    "specifically , for @xmath101 we have : @xmath114{\\mbox{\\normalfont\\tiny\\sffamily i.i.d.}}}{\\sim}}}\\mbox{geometric}(p)\\ ] ] and for @xmath102 we have : @xmath115{\\mbox{\\normalfont\\tiny\\sffamily i.i.d.}}}{\\sim}}}\\mbox{geometric}(p ) \\\\",
    "x_{m_{1}+1 } , x_{m_{1}+2 } , \\ldots , x_{n}&|\\lambda { \\mathrel{\\overset{\\makebox[0pt]{\\mbox{\\normalfont\\tiny\\sffamily i.i.d.}}}{\\sim}}}\\mbox{poisson}(\\lambda)\\end{aligned}\\ ] ]    let us denote with @xmath116 and @xmath117 , the probability mass functions of the geometric and the poisson distributions , respectively .",
    "the priors for the parameters of @xmath118 and @xmath119 are @xmath120 and @xmath121 .    in the first simulation",
    ", we sample @xmath122 observations from model @xmath101 with @xmath123 . to perform the change point analysis ,",
    "we have chosen the following parameters for the priors on @xmath124 and @xmath125 : @xmath126 , @xmath127 , @xmath128 and @xmath129 . applying the approach introduced in section [ sc_numbercp ]",
    ", we obtain @xmath130 and @xmath131 .",
    "these model priors yield the posterior distribution probabilities ( refer to equation ) @xmath132 and @xmath133 .",
    "as expected , the selection process strongly indicates the true model as @xmath101 .",
    "table [ discrete_results_1 ] reports the above probabilities including other information , such as the appropriate bayes factors .",
    "the second simulation looked at the opposite set up , that is we sample @xmath122 observations from @xmath102 , with @xmath123 and @xmath134 .",
    "we have sampled 50 data points from the geometric distribution and the remaining 50 data points from the poisson distribution . in figure [ fig_gp ]",
    "we have plotted the simulated sample , where it is legitimate to assume a change in the underlying distribution .",
    "using the same prior parameters as above , we obtain @xmath135 and @xmath136 .",
    "again , the model selection process is assigning heavy posterior mass to the true model @xmath102 .",
    "these results are further detailed in table [ discrete_results_1 ] .     in scenario 1 . ]",
    ".model prior , bayes factor and model posterior probabilities for the change point analysis in scenario 1 .",
    "we considered samples from , respectively , model @xmath101 and model @xmath102 . [",
    "cols=\"^,^,^ \" , ]     from table [ finance_tableresults ] we note that the prior on model @xmath102 and @xmath103 assigned by the proposed method , are the same .",
    "this is not surprising as the only difference between the two models is an additional log - normal distribution with different parameter values .",
    "bayesian inference in change point problems under the assumption of minimal prior information has not been deeply explored in the past , as the limited literature on the matter shows .",
    "we contribute to the area by deriving an objective prior distribution to detect change point locations , when the number of change points is known a priori . as a change point location",
    "can be interpreted as a discrete parameter , we apply recent results in the literature @xcite to make inference . the resulting prior distribution , which is uniform , it is not new in the literature @xcite , and therefore can be considered as a validation of the proposed approach .",
    "a second major contribution is in defining an objective prior on the number of change points , which has been approached by considering the problem as a model selection exercise .",
    "the results of the proposed method on both simulated and real data , allow to show the strength of the approach in estimating the number of change points in a series of observations .",
    "a point to note is in the generality of the scenario considered .",
    "indeed , we consider situations where the change is in the value of the parameter(s ) of the underlying sampling distribution , or in the distribution itself",
    ". of particular interest , is the last real data analysis ( s&p 500 index ) , where we consider a scenario where we have both types of changes , that is the distribution for the first change point and on the parameters of the distribution for the second .",
    "xx    berk , r.  h. 1966 , ` limiting behavior of posterior distributions when the model is incorrect ' , _ the annals of mathematical statistics _ * 37*(1 ) ,  5158 .",
    "carlin , b.  p. , gelfand , a.  e.  smith , a.  f. 1992 , ` hierarchical bayesian analysis of changepoint problems ' , _ applied statistics _ * 41*(2 ) ,  389405 .",
    "chib , s. 1998 , ` estimation and comparison of multiple change - point models ' , _ journal of econometrics _ * 86*(2 ) ,  221  241 .",
    "girn , f.  j. , moreno , e.  casella , g. 2007 , objective bayesian analysis of multiple changepoints for linear models , _ in _",
    "j.  bernardo , m.  bayarri , j.  berger , a.  dawid , d.  heckerman , a.  smith  m.  west , eds , ` bayesian statistics 8 ' , oxford university press , pp .",
    "227252 .",
    "kass , r.  e.  raftery , a.  e. 1995 , ` bayes factors ' , _ journal of the american statistical association _ * 90*(430 ) ,  773795 .",
    "koop , g.  potter , s.  m. 2009 , ` prior elicitation in multiple change - point models ' , _ international economic review _ * 50*(3 ) ,  751772 .",
    "kullback , s.  leibler , r.  a. 1951 , ` on information and sufficiency ' , _ the annals of mathematical statistics _ * 22*(1 ) ,  7986 .",
    "moreno , e. , casella , g.  garcia - ferrer , a. 2005 , ` an objective bayesian analysis of the change point problem ' , _ stochastic environmental research and risk assessment _ * 19*(3 ) ,  191204 .",
    "villa , c.  walker , s. 2015 _ a _ , ` an objective approach to prior mass functions for discrete parameter spaces ' , _ journal of the american statistical association _ * 110*(511 ) ,  10721082 .",
    "villa , c.  walker , s. 2015 _ b _ , ` an objective bayesian criterion to determine model prior probabilities ' , _ scandinavian journal of statistics _ * 42*(4 ) ,  947966 .",
    "yu , j. 2001 , chapter 6 - testing for a finite variance in stock return distributions , _ in _ j.  knight ,  s.  satchell , eds , `",
    "return distributions in finance ' , quantitative finance , butterworth - heinemann , oxford , pp .  143  164 .",
    "here we show how model prior probabilities can be derived for the relatively simple case of selecting among scenarios with no change points ( @xmath101 ) , one change point ( @xmath102 ) or two change points ( @xmath103 ) .",
    "first , by applying the result in theorem [ remark_model ] , we derive the kullback ",
    "leibler divergences between any two models .",
    "that is :    * the prior probability for model @xmath84 depends on the following quantities : @xmath137 * the prior probability for model @xmath85 depends on the following quantities : @xmath138 * the prior probability for model @xmath88 depends on the following quantities : @xmath139      * for model @xmath84 : @xmath140}_{1}\\cdot \\left[\\inf_{\\tilde{\\theta}_2}d_{kl}(f_{1}(\\cdot|\\tilde{\\theta}_{1})\\|f_{2}(\\cdot|\\tilde{\\theta}_{2}))\\right]\\\\= & \\inf_{\\tilde{\\theta}_2}d_{kl}(f_{1}(\\cdot|\\tilde{\\theta}_{1})\\|f_{2}(\\cdot|\\tilde{\\theta}_{2}))\\\\     \\inf_{\\theta_2}d_{kl}(m_0\\|m_2)=&\\underbrace{\\left[\\inf_{m_{1}\\neq m_{2}}(m_{2}-m_{1})\\right]}_{1}\\cdot \\left[\\inf_{\\tilde{\\theta}_{2}}d_{kl}(f_{1}(\\cdot|\\tilde{\\theta}_{1})\\|f_{2}(\\cdot|\\tilde{\\theta}_{2}))\\right]\\\\ & + \\underbrace{\\left[\\inf_{m_{2 } \\neq n}(n - m_{2})\\right]}_{1}\\cdot \\left[\\inf_{\\tilde{\\theta}_3}d_{kl}(f_{1}(\\cdot|\\tilde{\\theta}_{1})\\|f_{3}(\\cdot|\\tilde{\\theta}_{3}))\\right]\\\\ = & \\inf_{\\tilde{\\theta}_{2}}d_{kl}(f_{1}(\\cdot|\\tilde{\\theta}_{1})\\|f_{2}(\\cdot|\\tilde{\\theta}_{2}))+\\inf_{\\tilde{\\theta}_3}d_{kl}(f_{1}(\\cdot|\\tilde{\\theta}_{1})\\|f_{3}(\\cdot|\\tilde{\\theta}_{3}))\\end{aligned}\\ ] ] * for model @xmath85 : @xmath141}_{1}\\cdot \\left[\\inf_{\\tilde{\\theta}_3}d_{kl}(f_{2}(\\cdot|\\tilde{\\theta}_{2})\\|f_{3}(\\cdot|\\tilde{\\theta}_{3}))\\right]\\\\=&\\inf_{\\tilde{\\theta}_3}d_{kl}(f_{2}(\\cdot|\\tilde{\\theta}_{2})\\|f_{3}(\\cdot|\\tilde{\\theta}_{3}))\\\\   \\inf_{\\theta_0=\\tilde{\\theta}_1}d_{kl}(m_1\\|m_0)=&(n - m_{1})\\cdot \\inf_{\\tilde{\\theta}_1}d_{kl}(f_{2}(\\cdot|\\tilde{\\theta}_{2})\\|f_{1}(\\cdot|\\tilde{\\theta}_{1}))\\end{aligned}\\ ] ] * for model @xmath88 : @xmath142      * the model prior probability @xmath143 is proportional to the exponential of the minimum between : @xmath144,\\mathbb{e}_{\\pi_{0}}\\left[\\inf_{\\tilde{\\theta}_{2}}d_{kl}(f_{1}(\\cdot|\\tilde{\\theta}_{1})\\|f_{2}(\\cdot|\\tilde{\\theta}_{2}))\\right.\\right . \\nonumber \\\\ & \\left.\\left .",
    "\\hspace*{0.3cm}+\\inf_{\\tilde{\\theta}_3}d_{kl}(f_{1}(\\cdot|\\tilde{\\theta}_{1})\\|f_{3}(\\cdot|\\tilde{\\theta}_{3}))\\right]\\right\\rbrace\\end{aligned}\\ ] ] * the model prior probability @xmath145 is proportional to the exponential of the minimum between : @xmath146 , \\right . \\nonumber \\\\ & \\left .",
    "\\hspace*{0.3 cm } \\mathbb{e}_{\\pi_1}\\left[(n - m_{1 } ) \\cdot \\inf_{\\tilde{\\theta}_1}d_{kl}(f_{2}(\\cdot|\\tilde{\\theta}_{2})\\|f_{1}(\\cdot|\\tilde{\\theta}_{1}))\\right]\\right\\rbrace\\end{aligned}\\ ] ] * the model prior probability @xmath147 is proportional to the exponential of the minimum between : @xmath148,\\right.\\nonumber\\\\&\\left.\\hspace*{0.3cm}\\mathbb{e}_{\\pi_{2}}\\left[(m_{2}-m_{1})\\cdot \\inf_{\\tilde{\\theta}_1}d_{kl}(f_{2}(\\cdot|\\tilde{\\theta}_{2})\\|f_{1}(\\cdot|\\tilde{\\theta}_{1}))+(n - m_{2})\\right.\\right.\\nonumber\\\\&\\left.\\left.\\hspace*{0.3cm}\\cdot\\inf_{\\tilde{\\theta}_1}d_{kl}(f_{3}(\\cdot|\\tilde{\\theta}_{3})\\|f_{1}(\\cdot|\\tilde{\\theta}_{1}))\\right]\\right\\rbrace\\end{aligned}\\ ] ]        we distinguish two cases : @xmath149 and @xmath150 . when @xmath149 , equivalent to @xmath151 : @xmath152\\,\\mathrm{d}\\mathbf{x}^{(n ) } \\nonumber \\\\ = & \\mathlarger{\\sum_{i = m_{j}+1}^{m_{j}^{\\prime } } } \\int f(\\mathbf{x}^{(n)}|\\bm{m},\\bm{\\tilde\\theta } ) \\cdot \\left[\\ln\\left(\\dfrac{f_{j+1}(x_{i}|\\tilde\\theta_{j+1})}{f_{j}(x_{i}|\\tilde\\theta_{j})}\\right)\\right]\\,\\mathrm{d}\\mathbf{x}^{(n ) } \\nonumber \\\\ = & \\mathlarger{\\sum_{i = m_{j}+1}^{m_{j}^{\\prime } } } \\left\\lbrace 1^{n-1 } \\cdot \\int f_{j+1}(x_{i}|\\tilde\\theta_{j+1 } ) \\cdot \\left[\\ln\\left(\\dfrac{f_{j+1}(x_{i}|\\tilde\\theta_{j+1})}{f_{j}(x_{i}|\\tilde\\theta_{j})}\\right)\\right]\\,\\mathrm{d}x_{i}\\right\\rbrace \\nonumber \\\\ = & \\sum_{i = m_{j}+1}^{m_{j}^{\\prime } } d_{kl}(f_{j+1}(x_{i}|\\tilde\\theta_{j+1})\\|f_{j}(x_{i}|\\tilde\\theta_{j } ) ) \\nonumber \\\\ = & ( m_j^{\\prime}-m_{j } ) \\cdot d_{kl}(f_{j+1}(\\cdot|\\tilde\\theta_{j+1})\\|f_{j}(\\cdot|\\tilde\\theta_{j } ) ) \\nonumber \\\\ = & ( m_j^{\\prime}-m_{j } ) \\cdot d_j^{+1}(\\bm{\\tilde\\theta } )",
    ". \\label{demopositive}\\end{aligned}\\ ] ] when @xmath150 , equivalent to @xmath153 , in a similar fashion , we get @xmath154 from equations and , we get the result in theorem [ lemmaonechgpoint ] .",
    "we recall that the model parameter @xmath155 is the vector @xmath156 , where @xmath157 . here ,",
    "@xmath158 represent the parameters of the underlying sampling distributions considered under model @xmath159 and @xmath160 are the respective @xmath161 change point locations . in this",
    "setting , @xmath162    we proceed to the computation of @xmath163 , that is the kullback ",
    "leibler divergence introduced in section [ sc_numbercp ] .",
    "similarly to the proof of theorem [ lemmaonechgpoint ] , we obtain the following result .",
    "@xmath164 given equation , if we integrate out the variables not involved in the logarithms , we obtain @xmath165 in a similar fashion , it can be shown that @xmath166"
  ],
  "abstract_text": [
    "<S> in this paper we present an objective approach to change point analysis . in particular , we look at the problem from two perspectives . the first focuses on the definition of an objective prior when the number of change points is known a priori . </S>",
    "<S> the second contribution aims to estimate the number of change points by using an objective approach , recently introduced in the literature , based on losses . </S>",
    "<S> the latter considers change point estimation as a model selection exercise . </S>",
    "<S> we show the performance of the proposed approach on simulated data and on real data sets .    </S>",
    "<S> * keywords : * change point ; discrete parameter space ; loss - based prior ; model selection ; objective bayes . </S>"
  ]
}