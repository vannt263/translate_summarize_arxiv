{
  "article_text": [
    "sparsity impacts the entire data analysis pipeline , touching algorithmic , mathematical modeling , as well as practical aspects .",
    "sparsity is frequently elicited through @xmath1-norm regularization  @xcite .",
    "but more refined , `` structured '' notions of sparsity are also greatly valuable : e.g. , groupwise - sparsity  @xcite , hierarchical sparsity  @xcite , gradient sparsity  @xcite , or sparsity over structured ` atoms '  @xcite .",
    "typical sparse models in machine learning involve problems of the form @xmath2 where @xmath3 is ( usually ) a convex and smooth loss function , while @xmath4 is ( usually ) a lower semicontinuous , convex , and nonsmooth regularizer that induces sparsity .",
    "for example , if @xmath5 and @xmath6 , we obtain the lasso problem  @xcite ; more complex choices of @xmath7 help model richer structure  for a longer exposition of several such choices of @xmath7 see e.g. ,  @xcite .",
    "we focus on instances of   where @xmath8 is a weighted _ anisotropic total - variation _ ( tv ) regularizer , which , for a vector @xmath9 and a fixed set of weights @xmath10 is defined as @xmath11 more generally , if @xmath12 is an order-@xmath13 tensor in @xmath14 with entries @xmath15 ( @xmath16 for @xmath17 ) ; we define the weighted _ @xmath13-dimensional anisotropic tv _",
    "regularizer @xmath18}_{j+1}-{\\mathsf{x}}^{[k]}_{j}|^{p_k}\\biggr)^{1/p_k},\\ ] ] where @xmath19}_j \\equiv { \\mathsf{x}}_{i_1,\\ldots , i_{k-1},j , i_{k+1},\\ldots , i_m}$ ] , @xmath20 are weights , and @xmath21 $ ] for @xmath22 .",
    "if @xmath12 is a matrix , expression reduces to ( note , @xmath23 ) @xmath24    these definitions look formidable , and are indeed nontrivial ; already 2d - tv   or even the simplest 1d - tv   are fairly complex , which can complicate the overall optimization problem  .",
    "fortunately , their complexity can be `` localized '' by wrapping into using the notion of _ prox - operators _",
    "@xcite , which are now widely used across machine learning  @xcite  see also the classic work on the proximal - point method  @xcite .",
    "the main idea of using prox - operators while solving   is as follows .",
    "suppose @xmath25 is a convex lsc function on a set @xmath26 .",
    "the _ prox - operator _ of @xmath25 is defined as the map @xmath27 this map is used in the _ proximal - point _ method to minimize @xmath28 over @xmath29 by iterating @xmath30 applying directly can be impractical : computing @xmath31 may be even harder than minimizing @xmath25 ! but for problems that have a _ composite objective _ @xmath32 , instead of iteration  , closely related _ proximal splitting _",
    "methods may be more effective .    here , one ` splits ' the optimization into two parts : a part that depends only on @xmath33 and another that computes @xmath34 , a step typically much easier than computing @xmath31 .",
    "this idea is realized by the _",
    "proximal gradient method _",
    "( also known as ` forward backward splitting ' ) , which performs a gradient ( forward ) step followed by a proximal ( backward ) step to iterate @xmath35 numerous other proximal splitting methods exist  please see  @xcite and the references therein for additional examples .",
    "iteration   suggests that to profit from proximal splitting we must implement the prox - operator @xmath34 efficiently .",
    "an additional concern is whether the overall proximal splitting algorithm requires _",
    "exact _ computation of @xmath34 , or moderately _ inexact _ computations are sufficient .",
    "this concern is very practical : rarely does @xmath8 admit an exact algorithm for computing @xmath34 .",
    "fortunately , proximal methods easily admit inexactness , e.g. ,  @xcite , which allows approximate prox - operators ( as long as the approximation is sufficiently accurate ) .      in light of the above background and motivation , we focus on computing prox - operators for tv regularizers , for which we develop ,",
    "analyze , and implement a variety of fast algorithms .",
    "the ensuing contributions of this paper are summarized below .    * a new algorithm for @xmath1-norm _ weighted _ 1d - tv proximity , which not only matches @xcite s remarkably fast method  @xcite for unweighted tv , but provides the fastest ( to our knowledge ) method for weighted tv ; moreover , it has an intuitive derivation . * efficient prox - operators for general @xmath0-norm ( @xmath36 ) 1d - tv .",
    "in particular , * * for @xmath37 we also derive a projected - newton method , which , though slower than our new algorithm , is of instructive value while also providing a practical framework for some subroutines needed for handling the difficult @xmath38 case",
    ". moreover , our derivations may be useful to readers seeking efficient prox - operators for other problems with structures similar to tv ( e.g. , @xmath1-trend filtering  @xcite ) ; in fact our projected - newton ideas provided a basis for the recent fast `` group fused - lasso '' algorithms of  @xcite . * * for @xmath39 , we present a specialized newton method based on root - finding , while for the general @xmath38 case we describe both `` projection - free '' and projection based first - order methods .",
    "* scalable algorithms based on proximal - splitting techniques for computing 2d   and higher - d tv   prox - operators ; our algorithms are designed to reuse our fast 1d - tv routines and to exploit the massive parallelization inherent in matrix and tensor tv problems . *",
    "the practically most important contribution of our paper is a well - tuned , multi - threaded open - source c++ implementation of our algorithms.]@xmath40    to complement our algorithmic contribution , we illustrate several applications of tv prox - operators : ( i ) to image and video denoising ; ( ii ) as subroutines in some image deconvolution solvers ; and ( iii ) as regularizers in four variants of fused - lasso .",
    "* note : * our paper strives to support reproducible research .",
    "given the vast attention that tv problems have received in the literature , we find it is valuable to both users of tv and other researchers to have access to our code , datasets , and scripts , to independently verify our claims , if desired .",
    "the literature on tv is dauntingly large , so we will not attempt any pretense at comprehensiveness . instead , we mention some of the most directly related work for placing our contributions in perspective .",
    "we focus on _ anisotropic_-tv ( in the sense of  @xcite ) , in contrast to _",
    "@xcite . like isotropic tv , anisotropic - tv is also amenable to efficient optimization , and proves quite useful in applications seeking parameters with sparse ( discrete ) gradients .",
    "the anisotropic tv regularizers @xmath41 and @xmath42 arise in image denoising and deconvolution  @xcite , in the fused - lasso  @xcite , in logistic fused - lasso  @xcite , in change - point detection  @xcite , in graph - cut based image segmentation  @xcite , in submodular optimization  @xcite ; see also the related work in  @xcite .",
    "this broad applicability and importance of anisotropic tv is the key motivation towards developing carefully tuned proximity operators .",
    "isotropic tv regularization arises frequently in image denoising and signal processing , and quite a few tv - based denoising algorithms exist  ( * ? ? ?",
    "* see e.g. ) .",
    "most tv - based methods , however , use the standard rof model  @xcite .",
    "there are also several methods tailored to anisotropic tv , e.g. , those developed in the context of fused - lasso  @xcite , graph - cuts  @xcite , admm - style approaches  @xcite , or fast methods based on dynamic programming  @xcite or kkt conditions analysis  @xcite .",
    "however , it seems that anisotropic tv norms other than @xmath1 has not been studied much in the literature , although recognized as a form of sobolev semi - norms  @xcite .",
    "previously , @xcite suggested a newton approach for tv ; they smoothed the objective , but noted that it leads to numerical difficulties .",
    "in contrast , we directly solve the nonsmooth problem .",
    "recently , @xcite presented tuned algorithms for @xmath41-proximity based on a careful `` restart '' heuristic ; their methods show strong empirical performance but do not extend easily to tv .",
    "our newton - type methods outperform the tuned methods of  @xcite , and fit nicely in a general algorithmic framework that allows tackling the harder two- and tv problems .",
    "the gains obtained are especially prominent for 2d fused - lasso , for which previously  @xcite presented a coordinate descent approach that turns out to be slower .",
    "recently , @xcite presented a so - called `` split - bregman '' ( sb ) method that applies directly to 2d - tv .",
    "it turns out that this method is essentially a variant of the well - known admm method .",
    "in contrast to our 2d approach , the sb strategy followed by @xcite is to rely on @xmath1-soft thresholding substeps instead of 1d - tv substeps . from an implementation viewpoint ,",
    "the sb approach is somewhat simpler , but not necessarily more accurate .",
    "incidentally , sometimes such direct admm approaches turn out to be less effective than admm methods that rely on more complex 1d - tv prox - operators  @xcite .    for 1d - tv ,",
    "there exist several direct methods that are exceptionally fast .",
    "we treat the 1d - tv problem in detail in section  [ sec:1d ] , hence refer the reader to that section for discussion of closely related work on fast 1d - tv solvers .",
    "we note here , however , that in contrast to all previous fast solvers , our 1d - tv solver allows weights , a capability that can be very important in applications  @xcite .",
    "it is tempting to assume that existing isotropic algorithms , such as the state - of - the - art pdhg  @xcite method , can be easily adapted to our problems .",
    "but this is not true .",
    "pdhg requires fine - tuning of its parameters , and to obtain fast performance its authors apply non - trivial adaptive rules that fail on our anisotropic model . in stark contrast , our solvers do not require _ any parameter tuning _ ; the default values work across a wide spectrum of inputs .",
    "it is worth highlighting that it is not just proximal solvers such as fista @xcite , sparsa  @xcite , salsa @xcite , twist @xcite , trip@xcite , that can benefit from our fast prox - operators .",
    "all other 2d and higher - d tv solvers , e.g. , @xcite , as well as the recent admm based trend - filtering solvers of  @xcite immediately benefit ( and in fact , even gain the ability to solve weighted versions of their problems , a feature that they currently lack ) .",
    "we begin with the 1d - tv problem  , for which we develop below carefully tuned algorithms .",
    "this tuning pays off : our weighted - tv algorithm offers a fast , robust , and low - memory ( in fact , in place ) algorithm , which is not only of independent value , but is also an ideal building block for scalably solving 2d- and higher - d tv problems .    to express   more compactly we introduce the usual _ differencing matrix _",
    "@xmath43 using which , becomes @xmath44 .",
    "the associated prox - operator solves @xmath45 the presence of @xmath46 and well as the @xmath0-norm makes   fairly challenging to solve ; even the simpler variants for @xmath47 are nontrivial .",
    "when solving   we may equivalently consider its dual form ( obtained using proposition  [ prop.pd ] and formula  , for instance ) : @xmath48 where @xmath49 and @xmath50 is the _ dual - norm _ of @xmath51 .",
    "if @xmath52 is the dual solution , then using kkt conditions we can recover the primal solution @xmath53 ; indeed , using @xmath54 , we obtain @xmath55 , from which we obtain the primal solution @xmath56 if we have a feasible dual variable @xmath57 , we can compute a candidate primal solution @xmath58 ; the associated _ duality gap _ is given by ( noting that we wrote   as a minimization ) : @xmath59 the duality gap provides a handy stopping criterion for many of our algorithms .    with these basic ingredients we are now ready to present specialized 1d - tv algorithms for different choices of @xmath60 .",
    "the choices @xmath61 are the most common , so we treat them in greater detail  especially the case @xmath37 , which is the most important of all the tv problems studied below .",
    "other choices of @xmath60 find fewer practical applications , but we nevertheless cover them for completeness .      in this section we present two efficient methods for computing the @xmath41 prox - operator .",
    "the first method employs a carefully tuned `` taut - string '' approach , while the second method adapts the classical projected - newton ( pn ) method  @xcite to tv .",
    "we first derive our methods for unweighted tv , before presenting details for the elementwise weighted tv problem  .",
    "the previous fastest methods handle only unweighted - tv .",
    "it is is nontrivial to extend them to handle weighted - tv , a problem that is crucial to several applications , e.g. , segmentation  @xcite and certain submodular optimization problems  @xcite .",
    "taut - string approaches are already known to provide extremely efficient solutions to unweighted tv - l1 , as seen in  @xcite .",
    "in addition to our optimized taut - string approach for weighted - tv , we also present details of a pn approach for its instructive value , and also because it provides key subroutines for solving @xmath38 problems . moreover , our derivation may be helpful to readers seeking to implement efficient prox - operators for other problems that have structure similar to tv , for instance @xmath1-trend filtering  @xcite .",
    "indeed , the pn approach proved foundational for the recent fast `` group fused - lasso '' algorithms of  @xcite .    before proceeding we note that other than  @xcite , other efficient methods to address unweighted @xmath41 proximity have been proposed .",
    "@xcite shows how solving @xmath62 proximity is equivalent to computing the data likelihood of an specific hidden markov model ( hmm ) , which suggests a dynamic programming approach based on the well - known viterbi algorithm for hmms .",
    "the resulting algorithm is very competitive , and guarantees an overall @xmath63 performance while requiring approximately @xmath64 storage",
    ". we will also consider this algorithm in our experimental comparison in  [ sec : tvproxexp ] .",
    "another , roughly similar approach to the proposed projected - newton method was already presented in  @xcite , which by means of an augmented langrangian method a dual formulation is obtained and solved following a newton  like strategy , though with the additional burden of requiring manual setting of a number of optimization parameters to ensure convergence .",
    "while taut - string methods seem to be largely unknown in machine learning , they have been widely applied in the field of statistics  see e.g.  @xcite .",
    "even the recent efficient method of  @xcite  though derived using kkt conditions of the dual problem  ( [ eq:3 ] ) ( for @xmath65)can be given a taut - string interpretation .",
    "below we introduce the general idea behind the taut - string approach and present our optimized version of it . surprisingly , the resulting algorithm is equivalent to the fast algorithm of  @xcite , though now with a clearer interpretation based on taut - strings , which proves key to obtaining a similarly fast method for weighted - tv .",
    "for tv - l1 the dual problem   becomes @xmath66 whose objective can be ( i.e. , without changing the argmin ) replaced by @xmath67 , which upon using the structure of matrix @xmath46 unfolds into @xmath68 introducing the fixed extreme points @xmath69 , we can thus replace the dual   by @xmath70 now we perform a change of variables by defining the new set of variables @xmath71 , where @xmath72 is the cumulative sum of input signal values .",
    "thus , becomes @xmath73 which upon simplification becomes @xmath74 now the key trick : problem   can be shown to share the same optimum as @xmath75 a proof of this relationship may be found in  @xcite ; for completeness and also because this proof will serve us for the weighted @xmath41 variant , we include an alternative proof in appendix [ app : tautstringproof ] .    the name `` taut - string '' is explained as follows .",
    "the objective in   can be interpreted as the euclidean length of a polyline through the points @xmath76 .",
    "thus , seeks the minimum length polyline ( the _ taut string _ ) crossing a tube of height @xmath77 with center the cumulative sum @xmath78 , and with the fixed endpoints ( @xmath79 ) .",
    "an example illustrating this is shown in figure [ fig : tautstringexample ] .",
    "once the taut string is found , the solution for the original proximity problem can be recovered by observing that @xmath80 where we used the primal - dual relation @xmath81 .",
    "intuitively , the above argument shows that the solution to the tv - l1 proximity problem is obtained as the discrete gradient of the taut string , or as the slope of its segments .",
    "it remains to describe how to find the taut string .",
    "the most widely used approach seems to be the one of  @xcite .",
    "this approach starts from the fixed point @xmath82 , and incrementally computes the _ greatest convex minorant _ of the upper bounds on the @xmath77 tube , as well as the _ smallest concave majorant _ of the lower bounds on the @xmath77 tube . when both curves intersect , a segment of the taut string can be identified and added to the solution .",
    "the procedure is then restarted at the end of the identified segment , and iterated until all taut string segments have been obtained .    in the worst - case",
    ", the identification of a single segment can require analyzing on the order of @xmath83 points in the signal , and the taut string may contain up to @xmath83 segments .",
    "thus , the worst - case complexity of the taut - string approach is @xmath84 .",
    "however , as also noted in  @xcite , in practice the performance is close to the best case @xmath63 .",
    "but in @xcite , it is noted that maintaining the minorant and majorant functions in memory is inefficient , whereby a taut - string approach is viewed as potentially inferior to their proposed method .",
    "surprisingly , the geometry - aware construction of taut - strings that we propose below , yields in a method analogous to  @xcite , both in space and time complexity . owing to its intuitive derivation ,",
    "our geometry - aware ideas extend naturally to weighted - tv  in our opinion , vastly more easily than the kkt - based derivations in  @xcite .    [",
    "[ details . ] ] details .",
    "+ + + + + + + +    our geometry - aware method requires only the following bookkeeping variables .    1 .",
    "@xmath85 : index of the current segment start 2 .",
    "@xmath86 : slope of the line joining segment start with majorant at the current point 3 .",
    "@xmath87 : slope of the line joining segment start with minorant at the current point 4 .",
    "@xmath88 : height of majorant w.r.t .",
    "the @xmath77-tube center 5 .",
    "@xmath89 : height of minorant w.r.t .",
    "@xmath77-tube center 6 .",
    "@xmath90 : index of last point where @xmath86 was updated ",
    "potential majorant break point 7 .",
    "@xmath91 : index of last point where @xmath87 was updated  potential minorant break point .",
    "figure  [ fig : tautstringalgvariables ] gives a geometric interpretation of these variables ; we use these variables to detect minorant - majorant intersections , without the need to compute or store them explicitly .",
    "algorithm [ algtv1tautstring ] presents full pseudocode of our optimized taut - string method .",
    "starting from the initial point , the method tries to build a single segment traversing the @xmath77-tube up to the end point @xmath92 . in this way , at each iteration the method steps one point further through the tube , updating the maximum / minimum slope of each such hypothetical segment ( @xmath93 ) as well as the minimum / maximum height that each segment could have at the current point ( @xmath94 ) .",
    "if at some point the tube has limits such that the maximum height @xmath88 falls below the tube bottom , or such that the minimum height @xmath89 grows above the tube ceiling , then it is not possible to continue building the segment . in this event ,",
    "the segment under construction must be broken at some point , and a new segment must be started after it . it can be shown that the segment must be broken at the point where it last touched the tube limits : either at the bottom limit if the segment has grown above the tube , or at the top limit if the segment has grown under the tube . with this criterion",
    "the segment is fixed from the starting point to such a breaking point and the procedure is restarted to build the next segment .",
    "figure [ fig : tautstringalgevolution ] visually shows an example of the algorithm s workings .",
    "initialize @xmath95 , @xmath96 , @xmath97 .",
    "find tube height : @xmath98 if @xmath99 , else @xmath100 update minorant height following current slope : @xmath101 .",
    "/ * check for ceiling violation : minorant is above tube ceiling * / build valid segment up to last minorant breaking point : @xmath102 .",
    "start new segment after break : @xmath103 , @xmath104 , @xmath105 , @xmath96 , @xmath97 * continue * update majorant height following current slope : @xmath106 . /",
    "* check for bottom violation : majorant is below tube bottom * / build valid segment up to last majorant breaking point : @xmath107 .",
    "start new segment after break : @xmath108 , @xmath109 , @xmath110 , @xmath96 , @xmath97 * continue * / * check if majorant height is above the ceiling * / correct slope : @xmath111 we are touching the majorant : @xmath112 this is a possible majorant breaking point : @xmath113 / * check if minorant height is below the bottom * / correct slope : @xmath114 we are touching the minorant : @xmath115 this is a possible minorant breaking point : @xmath116 continue building current segment : @xmath117 build last valid segment : @xmath118 .    [ [ height - variables . ] ] height variables .",
    "+ + + + + + + + + + + + + + + + +    it should be noted that in order to implement the method described above , the height variables @xmath94 are not strictly necessary as they can be obtained from the slopes @xmath93 .",
    "however , explicitly including them leads to efficient updating rules at each iteration , as we show below .",
    "suppose we are updating the heights and slopes from their estimates at step @xmath119 to step @xmath120 . updating the heights is immediate given the slopes , as we have that @xmath121 which is to say , since we are following a line with slope @xmath93 , the change in height from one step to the next is given by precisely such slope .",
    "note , however , that in our algorithm we do not compute absolute heights but instead relative heights with respect to the @xmath77tube center .",
    "therefore we need to account for the change in the tube center between steps @xmath119 and @xmath120 , which is given by @xmath122 .",
    "this completes the update , which is shown in algorithm [ algtv1tautstring ] as lines 4 and 11 .    of course it might well happen that the new height @xmath94 runs over or under the tube",
    "this would mean that we can not continue using the current slope in the majorant or minorant , and a recalculation is needed , which again can be done efficiently by now using the height information .",
    "let us assume , without loss of generality , that the starting index of the current segment is @xmath123 and the absolute height of the starting point of the segment is given by @xmath124 . then",
    ", for adjusting the majorant slope @xmath125 so that it touches the tube ceiling at the current point we note that @xmath126 where we have also added and subtracted the current value of @xmath127 .",
    "observe that this value was computed using the estimate of the slope so far @xmath128 , so we can rewrite it as the projection of the initial point in the segment following such slope , that is , as @xmath129 .",
    "doing so for one of the added heights @xmath130 produces @xmath131 which generates a simple updating rule .",
    "a similar derivation holds for the minorant .",
    "the resultant updates are included in the algorithm in lines 20 and 26 .",
    "after recomputing this slope we need to adjust the corresponding height back to the tube : since the heights are relative to the tube center we can just set @xmath132 , @xmath133 ; this is done in lines 21 and 27 .",
    "notice also that the special case of the last point in the tube where the taut - string must meet @xmath134 is handled by line 3 , where @xmath135 is set to @xmath123 at such a point to enforce this constraint . in practice , it is more efficient to add a separate step to the algorithm s main loop for handling this special case , as was already observed in  @xcite .",
    "overall , one iteration of the method is very efficient , as mostly just additions and subtractions are involved with the sole exception of the fraction required for the slope updates , which are not performed at every iteration .",
    "moreover , no additional memory is required beyond the constant number of bookkeeping variables , and in - place updates are also possible due to the fact that @xmath136 values for already fixed sections of the taut - string are not required again , so the output @xmath137 and the input @xmath138 can both refer to the same memory locations .",
    "several applications tv require penalizing the discrete gradients individually , which can be done by solving the _ weighted tv - l1 _ problem @xmath139 where the weights @xmath140 are all positive . to solve   using our optimized taut - string approach ,",
    "we again begin with its dual @xmath141 then , we repeat the derivation of the unweighted taut - string method with a few key modifications . more precisely",
    ", we transform   by introducing @xmath69 to obtain @xmath142 next , we perform the change of variable @xmath71 , where @xmath72 , and consider @xmath143 finally , applying theorem [ the : tautstringeq ] we obtain the equivalent _ weighted taut - string _",
    "problem @xmath144    problem   differs from its unweighted counterpart   in the constraints @xmath145 ( @xmath146 ) , which allow different weights for each component instead of using the same value @xmath77 .",
    "our geometric intuition also carries over to the weighted problem , albeit with a slight modification : the tube we are trying to traverse now has varying widths at each step ( instead of fixed @xmath77 width)figure [ fig : tautstringweightedexample ] illustrate this idea .    as a consequence of the above derivation and intuition , we ultimately obtain a weighted taut - string algorithm that can handle varying tube width by suitably modifying algorithm [ algtv1tautstring ] . in particular , when checking ceiling / floor violations as well as when checking slope recomputations and restarts , we must account for varying tube heights .",
    "algorithm [ algtv1tautstringweighted ] presents the precise modifications that we must make to algorithm  [ algtv1tautstring ] to handle weights .",
    "find tube height :",
    "@xmath147 if @xmath99 , else @xmath100 start new segment after break : @xmath103 , @xmath148 , @xmath149 , @xmath96 , @xmath97 start new segment after break : @xmath108 , @xmath109 , @xmath150 , @xmath96 , @xmath97      in this section we present details of a projected - newton ( pn ) approach to solving the weighted - tv problem  .",
    "although , our optimized taut - string approach is empirically superior to our pn approach , as mentioned before we still present its details .",
    "these details prove useful when developing subroutines for handling @xmath0-norm tv prox - operators , but perhaps their greatest use lies in presenting a general method that could be applied to other problems that have structures similar to tv , e.g. , group total - variation  @xcite and @xmath1-trend filtering  @xcite .",
    "the weighted - tv dual problem   is a bound - constrained qp , so it could be solved using a variety of methods such as tron  @xcite , l - bfgs - b  @xcite , or projected - newton ( pn )  @xcite .",
    "obviously , these methods will be inefficient if invoked off - the - shelf ; exploitation of problem structure is a must for solving   efficiently .",
    "pn lends itself well to such structure exploitation ; we describe the details below .",
    "pn runs iteratively in three key steps : first it identifies a special subset of _ active variables _ and uses these to compute a _ reduced _ hessian .",
    "then , it uses this hessian to scale the gradient and move in the direction opposite to it , damping with a stepsize , if needed .",
    "finally , the next iterate is obtained by projecting onto the constraints , and the cycle repeats .",
    "pn can be regarded as an extension of the gradient - projection method ( gp ,  @xcite ) , where the components of the gradient that make the updating direction infeasible are removed ; in pn both the gradient and the hessian are _ reduced _ to guarantee this feasibility .    at each iteration pn",
    "selects the active variables @xmath151_i > \\epsilon)\\quad\\text{or}\\quad ( u_i = w_i \\;\\text{and}\\ ; [ \\nabla\\phi({\\bm{u}})]_i < -\\epsilon)}\\right\\}},\\ ] ] where @xmath152 is small scalar .",
    "this corresponds to the set of variables at a bound , and for which the gradient points inside the feasible region ; that is , for these variables to further improve the objective function we would have to step out of bounds .",
    "it is thus clear that these variables are of no use for this iteration , so we define the complementary set @xmath153 of indices not in @xmath154 , which are the variables we are interested in updating . from the hessian @xmath155",
    "we extract the _ reduced hessian _ @xmath156 by selecting rows and columns indexed by @xmath157 , and in a similar way the _ reduce gradient _",
    "@xmath158_{\\bar{i}}$ ] . using these",
    "we perform a newton  like `` reduced '' update in the form @xmath159_{\\bar{i}}),\\ ] ] where @xmath124 is a stepsize , and @xmath160 denotes projection onto the constraints , which for box  constraints reduces to simple element  wise projection .",
    "note that only the variables in the set @xmath157 are updated in this iterate , leaving the rest unchanged .",
    "while such update requires computing the inverse of the reduced hessian @xmath156 , which in the general case can amount to computational costs in the @xmath161 order , we will see now how exploiting the structure of the problem allows us to perform all the steps above efficiently .",
    "first , observe that for  ( [ eq.15 ] ) the hessian is @xmath162    next , observe that whatever the active set @xmath154 , the corresponding reduced hessian @xmath156 remains symmetric tridiagonal .",
    "this observation is crucial because then we can quickly compute the updating direction @xmath163_{\\bar{i}}$ ] , which can be done by solving the linear system @xmath164_{\\bar{i}}$ ] as follows :    1 .",
    "compute the cholesky decomposition @xmath165 .",
    "2 .   solve the linear system @xmath166_{\\bar{i}}$ ] to obtain @xmath167 .",
    "3 .   solve the linear system @xmath168 to obtain @xmath169 .    because the reduced hessian is also tridiagonal ,",
    "its cholesky decomposition can be computed in _ linear time _ to yield a bidiagonal matrix @xmath170 , which in turn allows to solve the subsequent linear systems also in linear time .",
    "extremely efficient routines to perform all these tasks are available in the lapack libraries  @xcite .",
    "the next crucial ingredient is efficient selection of the stepsize @xmath124 .",
    "the original pn algorithm @xcite recommends armijo - search along projection arc .",
    "however , for our problem this search is inordinately expensive .",
    "so we resort to a backtracking strategy using quadratic interpolation  @xcite , which works admirably well .",
    "this strategy is as follows : start with an initial stepsize @xmath171 . if the current stepsize @xmath172 does not provide sufficient decrease in @xmath173 , build a quadratic model using @xmath174 , @xmath175 , and @xmath176",
    "then , the stepsize @xmath177 is set to the value that minimizes this quadratic model .",
    "in the event that at some point of the procedure the new @xmath177 is larger than or too similar to @xmath172 , its value is halved . in this fashion ,",
    "quadratic approximations of @xmath173 are iterated until a good enough @xmath124 is found .",
    "the goodness of a stepsize is measured using the following armijo - like sufficient descent rule @xmath178 ) \\geq \\sigma \\cdot \\alpha_k \\cdot \\left ( \\nabla\\phi({\\bm{u } } ) \\cdot { \\bm{d}}\\right),\\ ] ] where a tolerance @xmath179 works well practice .",
    "note that the gradient @xmath180 might be misleading in the condition above if @xmath57 has components at the boundary and @xmath181 points outside this boundary ( because then , due to the subsequent projection no real improvement would be obtained by stepping outside the feasible region ) . to address this concern , we modify the computation of the gradient @xmath180 , zeroing our the entries that relate to direction components pointing outside the feasible set .    the whole stepsize selection procedure is shown in algorithm [ algstep ] .",
    "the costliest operation in this procedure is the evaluation of @xmath173 , which , nevertheless can be done in linear time .",
    "furthermore , in practice a few iterations more than suffice to obtain a good stepsize .",
    "@xmath182 , @xmath183 , @xmath181 , tolerance parameter @xmath184 minimize quadratic model : @xmath185 . * if * @xmath186 * or * @xmath187 , * then * @xmath188 .",
    "@xmath189 @xmath172    overall , a full pn iteration as described above runs at @xmath63 cost .",
    "thus , by exploiting the structure of the problem , we manage to reduce the @xmath161 cost per iteration of a general pn algorithm to a linear - cost method .",
    "the pseudocode of the resulting method is shown as algorithm  [ algtv1 ] .",
    "note that in the special case when the weights @xmath190 are so large that the unconstrained optimum coincides with the constrained one , we can obtain @xmath52 directly via solving @xmath191 ( which can also be done at @xmath63 cost ) .",
    "the duality gap of the current solution ( see formula  [ eq.dgap ] ) is used as a stopping criterion , where we use a tolerance of @xmath192 in practice .    let @xmath193 ; solve @xmath191 . * if * @xmath194 , * return * @xmath52 . @xmath195 $ ] , @xmath196 .",
    "identify set of active constraints @xmath154 ; let @xmath197 .",
    "construct reduced hessian @xmath156 .",
    "solve @xmath164_{\\bar{i}}$ ] .",
    "compute stepsize @xmath124 using backtracking + interpolation ( alg .",
    "[ algstep ] ) .",
    "update @xmath198 $ ] .",
    "@xmath199 . @xmath200 .      as is apparent from the discussion above",
    ", one can follow a pn approach if the hessian @xmath201 is structured and can be factored efficiently . to exemplify this claim , we mention details for the _ second - order trend filtering _ tv problem  @xcite : @xmath202 which corresponds to using the second - order differencing matrix @xmath203 for this matrix , the hessian @xmath201 is the pentadiagonal @xmath204 matrix @xmath205 the cholesky decomposition of this matrix results in tridiagonal factors which can be computed in @xmath63 time .      for tv - l2 proximity ( @xmath39 ) the dual  ( [ eq:3 ] ) reduces to @xmath207 problem   is nothing but a version of the well - known trust - region subproblem ( trs ) , for which a variety of numerical approaches are known  @xcite .",
    "we derive a specialized algorithm based on the classic mor - sorensen newton ( msn ) method  @xcite .",
    "this method in general can be quite expensive , but again we can exploit the tridiagonal hessian to make it efficient .",
    "curiously , experiments show that for a limited range of @xmath77 values , even ordinary gradient - projection ( gp ) can be competitive .",
    "thus for overall best performance , one may prefer a hybrid msn - gp approach . towards solving  ,",
    "consider its kkt conditions : @xmath208 where @xmath124 is a lagrange multiplier .",
    "there are two possible cases regarding the @xmath209 : either @xmath210 , or @xmath211 .",
    "if @xmath210 , then the kkt condition @xmath212 , implies that @xmath213 must hold and @xmath57 can be obtained immediately by solving the linear system @xmath214 .",
    "this can be done in @xmath63 time owing to the bidiagonal structure of @xmath46 .",
    "conversely , if the solution to @xmath214 lies in the interior of the @xmath215 , then it solves ( [ eqcsptv2 ] ) .",
    "therefore , this case is trivial to solve , and we need to consider only the harder case @xmath216 .    for any given @xmath124 one can obtain the corresponding vector @xmath57 as @xmath217 . therefore , optimizing for @xmath57 reduces to the problem of finding the `` true '' value of @xmath124 .",
    "an obvious approach is to solve @xmath218 .",
    "less obvious is the _ msn equation _",
    "@xmath219 which has the benefit of being almost linear in the search interval , which results in fast convergence  @xcite .",
    "thus , the task is to find the root of the function @xmath220 , for which we use newton s method , which in this case leads to the iteration @xmath221 some calculation shows that the derivative @xmath222 can be computed as @xmath223    the key idea in msn is to eliminate the matrix inverse in   by using the cholesky decomposition @xmath224 and defining a vector @xmath225 , so that @xmath226",
    ". as a result , the newton iteration   becomes    @xmath227    and therefore @xmath228    as in tv - l@xmath229 , the tridiagonal structure of @xmath230 allows to compute both @xmath170 and @xmath231 in linear time , so the overall iteration runs in @xmath63 time .",
    "the above ideas are presented as pseudocode in algorithm [ algms ] . as a stopping criterion",
    "two conditions are checked : whether the duality gap is small enough , and whether @xmath57 is close enough to the boundary .",
    "this latter check is useful because intermediate solutions could be dual - infeasible , thus making the duality gap an inadequate optimality measure on its own . in practice",
    "we use tolerance values @xmath232 and @xmath233 .",
    "@xmath234 , @xmath235 , @xmath236 .",
    "compute cholesky decomp .",
    "obtain @xmath57 by solving @xmath238 .",
    "obtain @xmath231 by solving @xmath239 .",
    "@xmath200    even though algorithm  [ algms ] requires only linear time per iteration , it is fairly sophisticated , and in fact a much simpler method can be devised .",
    "this is illustrated here by a gradient - projection method with a _ fixed _",
    "stepsize @xmath241 , whose iteration is @xmath242    the theoretically ideal choice for the stepsize @xmath241 is given by the inverse of the lipschitz constant @xmath243 of the gradient @xmath180  @xcite . since @xmath174 is a convex quadratic , @xmath243 is simply the largest eigenvalue of the hessian @xmath201 . owing to its special structure , the eigenvalues of the hessian have closed - form expressions , namely @xmath244 ( for @xmath245 ) .",
    "the largest one is @xmath246 , which tends to @xmath247 as @xmath248 ; thus the choice @xmath249 is a good approximation .",
    "pseudocode showing the whole procedure is presented in algorithm [ alg : gp ] . combining this with the fact that the the projection @xmath250 is also trivial to compute ,",
    "the gp iteration   turns out to be very attractive . indeed ,",
    "sometimes it can even outperform the more sophisticated msn method , though only for a very limited range of @xmath77 values .",
    "therefore , in practice we recommend a hybrid of gp and msn , as suggested by our experiments .",
    "@xmath251 , @xmath235 .",
    "gradient update : @xmath252 .",
    "projection : @xmath253 . @xmath199 .",
    "@xmath200 .      for tv-@xmath255 proximity ( for @xmath256 ) the dual  ( [ eq:3 ] )",
    "problem becomes @xmath257 where @xmath258 .",
    "problem   is not particularly amenable to the newton - type approaches taken above , as neither pn , nor msn - type methods can be applied easily .",
    "it is somewhat amenable to a gradient - projection ( gp ) strategy , for which the same update rule as in   applies , but unlike the @xmath259 case , the projection step here is much more involved .",
    "thus , to complement gp , we also propose an alternative strategy using the projection - free frank - wolfe ( fw ) method .",
    "as expected , the overall best performing approach is actually a hybrid of gp and fw .",
    "let us thus present details of both below .",
    "the problem of projecting onto the @xmath260-norm ball is @xmath261 for this problem , it turns out to be more convenient to address its fenchel dual @xmath262 which is actually nothing but @xmath263 .",
    "the optimal solution , say @xmath264 , to   can be obtained by solving  , by using the moreau - decomposition   which yields @xmath265 projection   is computed many times within gp , so it is crucial to solve it rapidly and accurately . to this end , we first turn   into a differentiable problem and then derive a projected - newton method following our approach of  [ sec : tvl1 ] .",
    "assume therefore , without loss of generality that @xmath266 , so that @xmath10 also holds ( the signs can be restored after solving this problem ) .",
    "thus , instead of  , we solve @xmath267 the gradient of @xmath268 may be compactly written as @xmath269 where @xmath270 denotes elementwise exponentiation of @xmath271 .",
    "elementary calculation yields @xmath272 where @xmath273 , @xmath274 , @xmath275 , and @xmath276 is the dirac delta . in matrix notation , this hessian s diagonal plus rank-1 structure becomes apparent @xmath277    to develop an efficient newton method it is imperative to exploit this structure .",
    "it is not hard to see that for a set of non - active variables @xmath278 the reduced hessian takes the form @xmath279 with the shorthand @xmath280 , the matrix - inversion lemma yields @xmath281 furthermore , since in pn the inverse of the reduced hessian always operates on the reduced gradient , we can rearrange the terms in this operation for further efficiency ; that is , @xmath282 where @xmath283 , and @xmath284 denotes componentwise product .",
    "the relevant point of the above derivations is that the newton direction , and thus the overall pn iteration can be computed in @xmath63 time , which results in a highly effective solver",
    ".      the frank - wolfe ( fw ) algorithm ( see e.g. , @xcite for a recent overview ) , also known as the conditional gradient method  @xcite solves differentiable optimization problems over compact convex sets , and can be quite effective if we have access to a subroutine to solve linear problems over the constraint set .",
    "the generic fw iteration is illustrated in algorithm [ alg : fw ] .",
    "fw offers an attractive strategy for tv @xmath0 proximity because both the descent - direction as well as stepsizes can be computed very easily . specifically , to find the descent direction we need to solve @xmath285 this problem",
    "can be solved by observing that @xmath286 is attained by some vector @xmath287 proportional to @xmath288 , of the form @xmath289 .",
    "therefore , @xmath290 in   is found by taking @xmath291 , computing @xmath292 and then rescaling @xmath287 to meet @xmath293 .",
    "@xmath294 , compact convex set @xmath295 .",
    "initialize @xmath296 , @xmath235 .",
    "find descent direction : @xmath297 .",
    "determine stepsize : @xmath298 $ ] .",
    "update : @xmath299 @xmath300 .",
    "the stepsize can also be computed in closed form owing as the objective function is quadratic .",
    "note the update in fw takes the form @xmath301 , which can be rewritten as @xmath302 with @xmath303 . using this notation the optimal stepsize",
    "is obtained by solving @xmath304 } { \\tfrac{1}{2}}{{\\| { { \\bm{d}}^t ( { \\bm{u}}+ \\gamma { \\bm{d } } ) } \\|_{2}}}^2 - \\left ( { \\bm{u}}+ \\gamma { \\bm{d}}\\right)^t { \\bm{d}}{\\bm{y}}.\\ ] ] a brief calculation on the above problem yields @xmath305 where @xmath306 is the unconstrained optimal stepsize .",
    "we note that following @xcite we also check a `` surrogate duality - gap '' @xmath307 at the end of each iteration . if this gap is smaller than the desired tolerance , the real duality gap is computed and checked ; if it also meets the tolerance , the algorithm stops .",
    "the final case is @xmath309 .",
    "we mention this case only for completeness , but do not spend much time in developing tuned methods .. the dual   here becomes @xmath310 this problem can be again easily solved by invoking gp , where the only non - trivial step is projection onto the @xmath1-ball .",
    "but the latter is an extremely well - studied operation ( see e.g. ,  @xcite ) , and so @xmath63 time routines for this purpose are readily available . by integrating them in our gp framework an efficient prox solver",
    "is obtained .",
    "this section shows how to build on our efficient 1d - tv prox operators to handle proximity for the much harder multidimensional tv  ( [ eq.multid ] ) . to that end , we follow classical proximal optimization techniques  @xcite .",
    "the basic composite objective  ( [ eq.1 ] ) is a special case of the more general class of models where one may have several regularizers , so that we now solve @xmath311 where each @xmath312 ( for @xmath313 ) is lsc and convex .    just like the basic problem",
    ", the more complex problem   can also be tackled via proximal methods .",
    "the key to doing so is to use _ inexact proximal methods _ along with a technique that we call * proximity stacking*. inexact proximal methods allow one to use approximately computed prox operators without impeding overall convergence , while proximity stacking allows one to compute the prox operator for the entire sum @xmath314 by `` stacking '' the individual @xmath312 prox operators .",
    "this stacking leads to a highly modular design ; see figure [ fig : proximalmodulesstacking ] for a visualization .",
    "in other words , proximity stacking involves computing the prox operator @xmath315 by iteratively invoking the individual prox operators @xmath316 and then combining their outputs .",
    "this mixing is done by means of a combiner method , which guarantees convergence to the solution of the overall @xmath317 .    .",
    "proximal stacking makes the sum of regularizers appear as a single one to the proximal method , while retaining modularity in the design of each proximity step through the use of a combiner method . for non - smooth @xmath294",
    "the same schema applies by just replacing the @xmath294 gradient operator by its corresponding proximity operator .",
    "[ fig : proximalmodulesstacking ] ]    different proximal combiners can used for computing @xmath34  .",
    "for instance , for @xmath318 , the proximal dykstra ( pd )  @xcite method is a reasonable choice ; alternatively , as shown recently in  @xcite , the douglas - rachford ( dr ) scheme proves to be a more effective choice .",
    "the crux of both pd and dr , however lies in that they require only subroutines to compute the individual prox operators @xmath319 and @xmath320 , which allows us to reuse our 1d - tv subroutines .",
    "initialize @xmath322 , @xmath323 , @xmath235 .",
    "@xmath324 proximity operator : @xmath325 .",
    "@xmath324 step : @xmath326 .",
    "@xmath327 proximity operator : @xmath328 .",
    "@xmath327 step : @xmath329 . @xmath300 .",
    "@xmath330 .",
    "more generally , if more than two regularizers are present ( i.e. , @xmath331 ) , then a it is easier to use _ parallel - proximal dykstra _ ( ppd )",
    "@xcite or _ parallel douglas - rachford _",
    "( pdr ) as the proximal combiners  both methods being generalizations obtained via the `` product - space trick '' of @xcite .",
    "these parallel proximal methods are attractive because they not only combine an arbitrary number of regularizers , but also allows parallelizing the calls to the individual prox operators .",
    "this feature allows us to develop a highly parallel implementation for multidimensional tv proximity (  [ sec : multid ] ) .",
    "initialize @xmath322 , @xmath333 , for @xmath334 ; @xmath235 @xmath335 @xmath336 @xmath337 @xmath199 @xmath330    @xmath332 .",
    "initialize @xmath322 , @xmath333 , for @xmath334 ; @xmath235 @xmath335 @xmath336 @xmath337 @xmath199 @xmath330    thus , using proximal stacking and combination , any convex machine learning problem with multiple regularizers can be solved in a highly modular proximal framework .",
    "below we exemplify these ideas by applying them to two- and higher - dimensional tv proximity , which we then use within proximal solvers for addressing a wide array of applications .",
    "recall that for a matrix @xmath338 , our anisotropic 2d - tv regularizer takes the form @xmath339 this regularizer applies a @xmath62 regularization over each row of @xmath340 , and a @xmath341 regularization over each column . introducing differencing matrices @xmath342 and @xmath343 for the row and column dimensions , the regularizer   can be rewritten as @xmath344 where @xmath345 denotes the @xmath120-th row of @xmath340 , and @xmath346 its @xmath347-th column .",
    "the corresponding @xmath348-proximity problem is @xmath349 where we use the frobenius norm @xmath350 , where @xmath351 is the vectorization of @xmath340 . using",
    ", problem   becomes @xmath352 where the parentheses make explicit that @xmath353 is a combination of two regularizers : one acting over the rows and the other over the columns .",
    "formulation   fits the model solvable by pd or dr , though with an important difference : each of the two regularizers that make up @xmath353 is itself composed of a sum of several ( @xmath83 or @xmath13 ) 1d - tv regularizers .",
    "moreover , each of the 1d row ( column ) regularizers operates on a different row ( columns ) , and can thus be solved independently .",
    "going even beyond @xmath348 is the general multidimensional tv  , which we recall below .",
    "let @xmath12 be an order-@xmath13 tensor in @xmath14 , whose components are indexed as @xmath15 ( @xmath16 for @xmath17 ) ; we define tv for @xmath12 as @xmath354 where @xmath355 $ ] is a vector of scalars @xmath356 .",
    "this corresponds to applying a 1d - tv to each of the 1d fibers of @xmath12 along each of the dimensions . introducing the _ multi - index _",
    "@xmath357 , which iterates over every 1-dimensional fiber of @xmath12 along the @xmath358-th dimension , the regularizer   can be written more compactly as @xmath359 where @xmath360 denotes a row of @xmath12 along the @xmath358-th dimension , and @xmath361 is a differencing matrix of appropriate size for the 1d - fibers along dimension @xmath358 ( of size @xmath362 ) .",
    "the corresponding @xmath13-dimensional - tv proximity problem is @xmath363 where @xmath364 is a penalty parameter , and the frobenius norm for a tensor just denotes the ordinary sum - of - squares norm over the vectorization of such tensor .",
    "problem   looks very challenging , but it enjoys decomposability as suggested by   and made more explicit by writing it as a sum of @xmath365 terms @xmath366 the proximity task   can be regarded as the sum of @xmath13 proximity terms , each of which further decomposes into a number of inner @xmath365 terms .",
    "these inner terms are trivial to address since , as in the 2d - tv case , each of the @xmath365 terms operates on different entries of @xmath12 . regarding the @xmath13 major terms",
    ", we can handle them by applying the ppd combiner algorithm (  [ sec : proxmulti ] ) , which ultimately yields the prox operator for @xmath367 by just repeatedly calling @xmath365 prox operators .",
    "most importantly , both ppd and the natural decomposition of the problem provide a vast potential for parallel multithreaded computing , which is valuable when dealing with such complex and high - dimensional data .",
    "initialize @xmath369 , @xmath235 .",
    "choose gradient stepsize @xmath370 $ ] .",
    "gradient step : @xmath371 .",
    "choose proximity stepsize : @xmath372 $ ] .",
    "proximity step : @xmath373 . @xmath300 .",
    "@xmath374 .",
    "though less frequently , it is possible to find instances of machine learning problems where both loss and regularizer functions are non - smooth , such as in the @xmath1 regularized svm ( see table [ tab : modelsamples ] ) . in this case a different family of proximal methods are needed , which do not assume smoothness in any of the component terms of the problem .",
    "a good example of this kind of methods is douglas - rachford splitting ( drs ) @xcite , which solves the problem @xmath375 for @xmath294 , @xmath8 convex lower - semicontinuous ( not necessarily smooth ) functions .",
    "a pseudocode of this method is shown in algorithm [ alg : drs ] .",
    "initialize @xmath377 , @xmath235 .",
    "@xmath8 proximity step : @xmath378 .",
    "choose proximity stepsize : @xmath379 $ ] .",
    "@xmath294 proximity step : @xmath380 . @xmath300 .",
    "@xmath374 .",
    "as expected , the design for these kind of problems requires providing two routines computing the proximity steps of @xmath294 and @xmath8 , as illustrated in figure [ fig : proximalmodulesnonsmooth ] .",
    "once again , these routines are designed independently , and when plugged into the proximal method convergence to the solution of the overall problem is guaranteed .",
    "since the most important components of our methods are the efficient 1d - tv prox operators , let us begin by highlighting their empirical performance .",
    "in particular , we compare our methods against state - of - the - art algorithms , showing the advantages of our approach .",
    "our solvers are implemented in c++ , with calls to the lapack ( fortran ) library  @xcite .",
    "to permit reproducibility and to facilitate use of our tv prox - operators , we have made our complete toolbox available at : https://github.com/albarji/proxtv    we test solvers for 1d - tv regularization under two scenarios :    a.   increasing input size ranging from @xmath381 to @xmath382 .",
    "a penalty @xmath383 $ ] chosen at random for each run , and the data vector @xmath138 with uniformly random entries @xmath384 $ ] ( proportionally scaled to @xmath77 ) .",
    "b.   varying penalty parameter @xmath77 ranging from @xmath385 ( negligible regularization ) to @xmath386 ( the tv term dominates ) ; here @xmath83 is set to @xmath387 and @xmath136 is randomly generated in the range @xmath388 $ ] ( uniformly ) .",
    "starting with the proposed methods for @xmath41 proximity ( projected newton and optimized taut string ) , running times are compared against the following competing solvers :    * the * flsa * function ( c implementation ) of the slep library of @xcite for @xmath41-proximity  @xcite . *",
    "the state - of - the - art method of  @xcite , which by a different reasoning arrives at an algorithm essentially equivalent to the ( unweighted ) taut - string method .",
    "* the dynamic programming method of  @xcite , which guarantees linear running time .    for projected newton and slep a duality gap of @xmath389",
    "is used as the stopping criterion .",
    "the rest of algorithms are direct methods , and thus they are run until completion .",
    "timing results are presented in figure  [ fig : tv1run ] for both experimental scenarios .",
    "the following interesting facts are drawn from these results    * direct methods ( taut string , condat , johnson ) prove to be much faster than iterative methods ( projected newton , slep ) .",
    "this is true even for the case of the taut string and condat s methods , which have a theoretical worst - case performance of @xmath84 .",
    "this possibility was already noted at  @xcite . *",
    "even more , though johnson s method has a guaranteed running time of @xmath63 , it turns out to be slower than the taut string and condat s methods .",
    "this is explained by the need of johnson s method of an extra @xmath390 memory storage , which becomes a burden in contrast to the in - place strategies of taut string and condat s . *",
    "the best methods are taut string and condat s solvers ; both show equivalent performance for this unweighted tv problem .",
    "an advantage of the solvers proposed in this paper is their flexibility to easily deal with the more difficult , weighted version of the tv - l1 proximity problem . to illustrate this , figure  [",
    "fig : tv1wrun ] shows the running times of the projected newton and optimized taut string methods when solving both the standard and weighted tv - l1 prox operators .",
    "since for this set of experiments a whole vector of weights @xmath271 is needed , we have adjusted the experimental scenarios as follows :    a.   @xmath83 is generated as in the general setting , penalties @xmath391 $ ] are chosen at random for each run , and the data vector @xmath138 with uniformly random entries @xmath384 $ ] , with @xmath77 the mean of @xmath271 , using also this @xmath77 choice for the uniform ( unweighted ) case .",
    "b.   @xmath77 and @xmath83 are generated as in the general setting , and the weights vector @xmath271 is drawn randomly from the uniform distribution @xmath392 $ ] .",
    "as can be readily observed , performance for both versions of the problem is almost identical , even if the weighted problem is conceptually harder .",
    "conversely , adapting the other reviewed algorithms to address this problem while keeping up with performance is not a straightforward task .",
    "next we show results for @xmath206 proximity . to our knowledge , this version of tv has not been explicitly treated before , so there do not exist highly - tuned solvers for it .",
    "thus , we show running time results only for the msn and gp methods .",
    "we use a duality gap of @xmath389 as the stopping criterion ; we also add an extra boundary check for msn with tolerance @xmath393 to avoid early stopping due to potentially infeasible intermediate iterates .",
    "figure [ fig : tv2run ] shows results for the two experimental scenarios under test .",
    "the results indicate that the performance of msn and gp differs noticeably in the two experimental scenarios .",
    "while the results for the first scenario ( figure [ fig : tv2runn ] ) might suggest that gp converges faster than msn for large inputs , it actually does so depending on the size of @xmath77 relative to @xmath394 .",
    "indeed , the second scenario ( figure [ fig : tv2runlambda ] ) shows that although for small values of @xmath77 , gp runs faster than msn , as @xmath77 increases , gp s performance worsens dramatically , so much that for moderately large @xmath77 , it is unable to find an acceptable solution even after 10,000 iterations ( an upper limit imposed in our implementation ) .",
    "conversely , msn finds a solution satisfying the stopping criterion under every situation , thus showing a more robust behavior .",
    "these results suggest that it is preferable to employ a hybrid approach that combines the strengths of msn and gp .",
    "such a hybrid approach is guided using the following ( empirically determined ) rule of thumb : if @xmath395 use gp , otherwise use msn .",
    "further , as a safeguard , if gp is invoked but fails to find a solution within 50 iterations , the hybrid should switch to msn .",
    "this combination guarantees rapid convergence in practice .",
    "results for this hybrid approach are also included in the plots in figure [ fig : tv2run ] , and show how it successfully mimics the behavior of the better algorithm amongst msn and gp .",
    "now we show results for @xmath62 proximity . again , to our knowledge efficient solvers for this version of tv are not available ; still proposals for solving the @xmath260-ball projection problem do exist , such as the _ epp _ function in slep library @xcite , based on a zero finding approach .",
    "consequently , we present here a comparison between this reference projection subroutine and our pn  based projection when embedded in our proposed gradient projection solver of  [ sec : tvp ] . the alternative proposal given by the frank ",
    "wolfe algorithm of  [ sec : fw ] is also present in the comparison .",
    "we use a duality gap of @xmath389 as stopping criterion both for gp and fw .",
    "figure [ fig : tvpfixedrun ] shows results for the two experimental scenarios under test , for @xmath60 values of @xmath396 , @xmath397 and @xmath398 .",
    "+    a number of interesting conclusions can be drawn from the results .",
    "first , our projected newton @xmath260-ball subroutine is far more efficient than _ epp _ when in the context of the gp solver .",
    "two factors seem to be the cause of this : in the first place our projected newton approach proves to be faster than the zero finding method used by _",
    "epp_. secondly , in order for the gp solver to find a solution within the desired duality gap , the projection subroutine must provide very accurate results ( about @xmath399 in terms of duality gap ) .",
    "given its newton nature , our @xmath260-ball subroutine scales better in term of running times as a factor of the desired accuracy , which explains he observed differences in performance .",
    "it is also of relevance noting that frank ",
    "wolfe is significantly faster than projected newton .",
    "this should discourage the use of projected newton , but we find it to be extremely useful in the range of @xmath77 penalties where @xmath77 is large , but not enough to render the problem trivial ( @xmath400 solution ) . in this range the two variants of pn and also fw are unable to find a solution within the desired duality gap ( @xmath389 ) , getting stuck at suboptimal solutions .",
    "we solve this issue by means of a hybrid gp+fw algorithm , in which updates from both methods are interleaved at a ratio of 10 fw updates per 1 gp update , as fw updates are faster .",
    "as both algorithms guarantee improvement in each iteration but follow different procedures for doing so , they complement each other nicely , resulting a superior method attaining the objective duality gap and performing faster than gp .      for completeness",
    "we also include results for our @xmath309 solver based on gp + a standard @xmath1-projection subroutine .",
    "figure [ fig : tvinfrun ] presents running times for the two experimental scenarios under test . since @xmath1-projection is an easier problem than the general @xmath260-projection the resultant algorithm converges faster to the solution than the general gp @xmath402 prox solver , as expected .",
    "we now present a key application that benefits from our tv prox operators : * fused - lasso * ( fl )  @xcite , a model that takes the form @xmath403 the @xmath1-norm in   forces many @xmath404 to be zero , while @xmath41 favors nonzero components to appear in blocks of equal values @xmath405 .",
    "the fl model has been successfully applied in several bioinformatics applications @xcite , as it encodes prior knowledge about consecutive elements in microarrays becoming active at once .    following the ideas presented in sec .",
    "[ sec : proxmulti ] , since the fl model uses two regularizers , we can use proximal dykstra as the combiner to handle the prox operator . to illustrate the benefits of this framework in terms of reusability",
    ", we apply it to several variants of fl .    * * fused - lasso ( fl ) : * least - squares loss @xmath406 as in   * * @xmath0-variable fusion ( vf ) : * least - squares loss @xmath407 .",
    "though variable fusion was already studied by @xcite , their approach proposed an @xmath408-like regularizer in the sense that @xmath409 is used instead of the tv regularizer @xmath410 .",
    "using @xmath411 leads to a more conservative penalty that does not oversmooth the estimates .",
    "this fl variant seems to be new . *",
    "* logistic - fused lasso ( lfl ) : * logistic - loss @xmath406 , where the loss takes the form @xmath412 , and can be used in a fl formulation to obtain models more appropriate for classification on a dataset @xmath413 @xcite . *",
    "* logistic + @xmath0-fusion ( lvf ) : * logistic loss @xmath407 .    to solve these variants of fl ,",
    "all that remains is to compute the gradients of the loss functions , but this task is trivial . each of these four models can be then solved easily by invoking any proximal splitting method by appropriately plugging in gradient and prox operators .",
    "incidentally , the * slep * library  @xcite includes an implementation of fista  @xcite carefully tuned for fused lasso , which we base our experiments on .",
    "figure [ fig : flmodels ] shows a schematic of the algorithmic modules for solving each fl model .",
    "* remark : * a further algorithmic improvement can be obtained by realizing that for @xmath414 the prox operator @xmath415 . such a decomposition does not usually hold , but it can be shown to hold for this particular case  @xcite . therefore , for fl and lfl we can compute the proximal operator for the combined regularizer @xmath8 directly , thus removing the need for a combiner algorithm .",
    "this is also shown in figure  [ fig : flmodels ] .",
    "the standard fl model has been well - studied in the literature , so a number of practical algorithms addressing it have already been proposed . the aforementioned fused - lasso algorithm in the * slep *",
    "library can be regarded as the state of the art , making extensive use of an efficient proximity subroutine ( flsa ) .",
    "our experiments on @xmath41-proximity (  [ sec : tvproxexp ] ) have already shown superiority of our prox solvers over flsa ; what remains to be checked is whether this benefit has a significant impact on the overall fl solver .",
    "to do so , we compare running times with synthetic data .",
    "we generate random matrices @xmath416 with i.i.d .",
    "entries drawn from a zero mean , unit variance gaussian .",
    "we set the penalties to @xmath417 .",
    "we select the vector of responses @xmath138 using the formula @xmath418 , where @xmath330 , and @xmath167 are random vectors whose entries have variances @xmath419 and @xmath420 , respectively .",
    "the numerical results are summarized in figure [ fig : fl - obj ] , which compares out of the box slep ( version 4.0 ) @xcite against the very same algorithm employing our fast taut ",
    "string @xmath41 solver instead of the default flsa subroutine of slep .",
    "comparison is done by showing the relative distance to the problem s optimum versus time .",
    "the optimal values in each setting were estimated by running both algorithms for a very large number of iterations .    [ cols= \"",
    "< , < , < , < \" , ]     similarly to our previous image denoising experiments , we ran the algorithms under comparison for each video sequence and measured its isnr and relative distance to the optimal objective value of the current solution at each iteration through their execution . again",
    "the exception is the goldfarb - yin method , which is non  iterative and so we only report the time required for its termination . the optimal objective value was estimated by running all methods for a very large number of iterations and taking the minimum value of them all .",
    "this produced the plots shown in figures [ fig:3dtv - obj]-[fig:3dtv - isnr ] . from them",
    "the following observations are of relevance    * following the pattern observed in the image denoising experiments , yang s method is best suited for finding very accurate solutions . * the method by goldfarb and yin again provides suboptimal solutions , due to the discrete approximation it uses . * parallel proximal dykstra is the fastest to achieve a mid - quality solution . * intermediate solutions prior to convergence of the ppd run result in better isnr values for the _ coastguard _ and _ bicycle _ datasets .",
    "this hints that the denoising model used in this experiment may not be optimal for these kind of signals ; indeed , more advanced denoising models abound in the signal processing literature .",
    "hence we do not claim novel results in terms of isnr quality , but just in solving this classic denoising model more efficiently .",
    "the isnr plots in figure [ fig:3dtv - isnr ] also show how both parallel proximal dykstra and goldfarb - yin converge to equivalent solutions in practice .",
    "therefore , for the purpose of video denoising ppd seems to be the best choice , unless for some reason a high degree of accuracy is required , for which the method of goldfarb and yin should be preferred .",
    "b acknowledges partial financial support from spain s tin 2010 - 21575-c02 - 01 .",
    "we thank r. tibshirani for bringing  @xcite to our attention , and s. jegelka for alerting us to the importance of weighted total - variation problems .",
    "86 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    m.  v. afonso , j.  m. bioucas - dias , and m.  a.  t. figueiredo .",
    "fast image recovery using variable splitting and constrained optimization .",
    "_ ieee transactions on image processing _ , 190 ( 9 ) , september 2010 .    c.  m. alaiz , ",
    "barbero , and j.  r. dorronsoro .",
    "group fused lasso . _ artificial neural networks and machine learning ",
    "icann 2013 _ , page  66 , 2013 .",
    "e.  anderson et  al .",
    "_ lapack users guide_. society for industrial and applied mathematics , philadelphia , pa , third edition , 1999 .",
    "isbn 0 - 89871 - 447 - 8 ( paperback ) .",
    "f.  bach .",
    "structured sparsity - inducing norms through submodular functions . in _",
    "nips _ , 2010 .",
    "f.  bach , r.  jenatton , j.  mairal , and g.  obozinski .",
    "convex optimization with sparsity - inducing norms . in s.",
    "sra , s.  nowozin , and s.  j. wright , editors , _",
    "optimization for machine learning_. mit press , 2011 .    .",
    "barbero , j.  lpez , and j.  r. dorronsoro .",
    "finding optimal model parameters by discrete grid search . in _ advances in soft computing : innovations in hybrid intelligent systems 44 _ , pages 120127 .",
    "springer , 2008 .    .",
    "barbero , j.  lpez , and j.  r. dorronsoro .",
    "finding optimal model parameters by deterministic and annealed focused grid search .",
    "_ neurocomputing _ , 720 ( 13 - 15):0 28242832 , 2009 .",
    "issn 0925 - 2312 .",
    "doi : doi : 10.1016/j.neucom.2008.09.024 .",
    "h.  h. bauschke and p.  l. combettes .",
    "_ convex analysis and monotone operator theory in hilbert spaces_. cms books in mathematics .",
    "springer , 2011 .",
    "a.  beck and m.  teboulle . .",
    "_ siam journal of imgaging sciences _ , 20 ( 1):0 183202 , 2009 .    d.  p. bertsekas . projected newton methods for optimization problems with simple constraints .",
    "_ siam journal on control and optimization _",
    ", 200 ( 2 ) , march 1982 .",
    "d.  p. bertsekas .",
    "_ nonlinear programming_. athena scientific , 2nd edition ,",
    "september 1999 .",
    "j.  m. bioucas - dias and m.  a.  t. figueiredo . a new twist : two - step iterative shrinkage / thresholding algorithms for image restoration .",
    "_ ieee transactions on image processing _ , 160 ( 12):0 29923004 , december 2007 .",
    "j.  m. bioucas - dias , m.  a.  t. figueiredo , and j.  p. oliveira .",
    "total variation - based image deconvolution : a majorization - minimization approach . in _ icassp proceedings _ , 2006 .",
    "bm3d software and test sequences , 2013 .",
    "url http://www.cs.tut.fi/~foi / gcf - bm3d/.    r.  h. byrd , p.  lu , j.  nocedal , and c.  zhu .",
    "a limited memory algorithm for bound constrained optimization .",
    "technical report , northwestern university , 1994 .",
    "e.  j. cands and t.  tao .",
    "near - optimal signal recovery from random projections : universal encoding strategies .",
    "_ ieee trans .",
    "info . theory _ , 52:0 54065425 , 2004 .",
    "a.  chambolle and j.  darbon . on total variation minimization and surface evolution",
    "using parametric maximum flows . _ international journal of computer vision _ , 840 ( 3 ) , 2009 .",
    "a.  chambolle and t.  pock . a first - order primal - dual algorithm for convex problems with applications to imaging . _ journal of mathematical imaging and vision _ , 400 ( 1):0 120145 , 2011 .",
    "v.  chandrasekaran , b.  recht , p.  a. parrilo , and a.  s. willsky . .",
    "_ foundations of computational mathematics _ , 120 ( 6 ) , 2012 .",
    "r.  choksi , y.  van gennip , and a.  oberman . .",
    "technical report , mcgill university , july 2010 .",
    "p.  l. combettes .",
    "iterative construction of the resolvent of a sum of maximal monotone operators .",
    "_ journal of convex analysis _ , 16:0 727748 , 2009 .",
    "p.  l. combettes and j .- c .",
    "proximal splitting methods in signal processing .",
    "_ arxiv:0912.3522 _ , 2009 .",
    "l.  condat . a direct algorithm for 1d total variation denoising .",
    "technical report , greyc laboratory , cnrs - ensicaen - univ . of caen , 2012 .",
    "l.  condat . a generic proximal algorithm for convex optimization - application to total variation minimization .",
    "_ ieee signal proc . letters _ , 210 ( 8):0 985989 , 2014 .",
    "a.  r. conn , n.  i.  m. gould , and p.  l. toint .",
    "_ trust - region methods_. siam , 2000 .",
    "j.  dahl , p.  c. hansen , s.  h. jensen , and t.  l. jensen .",
    "algorithms and software for total variation image reconstruction via first - order methods .",
    "_ numer algor _ , 53:0 6792 , 2010 .",
    "p.  l. davies and a.  kovac . local extremes , runs , strings and multiresolution .",
    "_ the annals of statistics _ , 290 ( 1):0 165 , 2001 .    y.  duan and x .- c .",
    "domain decomposition methods with graph cuts algorithms for total variation minimization .",
    "_ adv comput math _ , 36:0 175199 , 2012 .",
    "doi : 10.1007/s10444 - 011 - 9213 - 4 .",
    "j.  friedman , t.  hastie , h.  hfling , and r.  tibshirani . .",
    "_ annals of applied statistics _ , 10 ( 2):0 302332 , aug . 2007 .",
    "d.  goldfarb and w.  yin .",
    "parametric maximum flow algorithms for fast total variation minimization .",
    "_ siam journal on scientific computing _ , 310 ( 5):0 37123743 , 2009 .    o.  s. goldstein t. .",
    "_ siam journal on imaging sciences _ , 20 ( 2):0 323343 , 2009 .",
    "t.  r. golub et  al .",
    "molecular classification of cancer . _ science _ , 2860 ( 5439):0 531537 , october 1999 .",
    "m.  grasmair .",
    "the equivalence of the taut string algorithm and bv - regularization .",
    "_ journal of mathematical imaging and vision _ , 270 ( 1):0 5966 , 2007 .",
    "issn 0924 - 9907 .",
    "doi : 10.1007/s10851 - 006 - 9796 - 4 .",
    "url http://dx.doi.org/10.1007/s10851-006-9796-4 .",
    "z.  harchaoui and c.  lvy - leduc . . _ journal of the american statistical association _ , 1050 ( 492):0 14801493 , 2010 .",
    "j.  hua , w.  d. tembe , and e.  r. dougherty .",
    "performance of feature - selection methods in the classification of high - dimension data .",
    "_ pattern recognition _ , 42:0 409424 , 2009",
    ".    k.  ito and k.  kunisch . an active set strategy based on the augmented lagrangian formulation for image restoration . _",
    "esaim : mathematical modelling and numerical analysis _ , 330 ( 1):0 121 , 1999 .",
    "url http://eudml.org/doc/193911 .",
    "m.  jaggi .",
    "revisiting frank - wolfe : projection - free sparse convex optimization . in _ proceedings of the 30th international conference on machine learning , _ , 2013 .",
    "s.  jegelka , f.  bach , and s.  sra .",
    "reflection methods for user - friendly submodular optimization . in _ advances in neural information processing systems _ , 2013 .",
    "to appear .",
    "n.  a. johnson . a dynamic programming algorithm for the fused lasso and @xmath421-segmentation . _ j. computational and graphical statistics _ , 2013 .",
    "d.  kim , s.  sra , and i.  dhillon . a scalable trust - region algorithm with application to mixed - norm regression . in _",
    "international conference on machine learning _ , 2010 .",
    "s.  kim , k.  koh , s.  boyd , and d.  gorinevsky .",
    "@xmath1 trend filtering .",
    "_ siam review _ , 510 ( 2):0 339360 , 2009 .",
    "doi : 10.1137/070690274 .",
    "k.  c. kiwiel .",
    "variable fixing algorithms for the continuous quadratic knapsack problem .",
    "_ j. optim .",
    "theory appl .",
    "_ , 136:0 445458 , 2008 .",
    "m.  kolar , l.  song , a.  ahmed , and e.  xing .",
    "estimaging time - varying networks . _",
    "the annals of applied statistics _ , 40 ( 1):0 94123 , 2010 .",
    "d.  krishnan and r.  fergus .",
    "fast image deconvolution using hyper - laplacian priors . in _ advances in neural information processing systems _ , 2009 .",
    "s.  r. land and j.  h. friedman .",
    "variable fusion : a new adaptive signal regression method .",
    "technical report 656 , department of statistics , carnegie mellon university pittsburgh , 1997 .",
    "y.  li and f.  santosa . a computational algorithm for minimizing total variation in image restoration .",
    "_ ieee transactions on image processing _ , 50 ( 6):0 987995 , 1996",
    "url http://dblp.uni-trier.de/db/journals/tip/tip5.html#lis96 .",
    "lin and j.  j. mor .",
    "newton s method for large bound - constrained optimization problems .",
    "_ siam journal on optimization _ , 90 ( 4):0 11001127 , 1999 .",
    "h.  liu and j.  zhang . .",
    "mach . learning ( icml ) _ , 2009 .",
    "j.  liu and j.  ye . .",
    "in _ icml _ , jun . 2009 .",
    "j.  liu , s.  ji , and j.  ye .",
    "_ slep : sparse learning with efficient projections_. arizona state university , 2009 . .",
    "j.  liu , l.  yuan , and j.  ye .",
    "an efficient algorithm for a class of fused lasso problems . in _",
    "acm sigkdd conference on knowledge discovery and data mining _ , 2010 .",
    "j.  mairal , r.  jenatton , g.  obozinski , and f.  bach . .",
    "in _ nips _ , 2010 .",
    "to appear .",
    "b.  martinet . .",
    "_ modlisation mathmatique et analyse numrique _ , 40 ( r3):0 154158 , 1970 .",
    "l.  meier , s.  van de geer , and p.  bhlmann .",
    "the group lasso for logistic regression .",
    "_ j. r. statist .",
    "_ , 70:0 5371 , 2008 .",
    "j.  j. mor and d.  c. sorensen . computing a trust region step .",
    "_ siam journal of scientific computing _ , 40 ( 3 ) , september 1983 .",
    "j.  j. moreau .",
    "fonctions convexes duales et points proximaux dans un espace hilbertien . _ c. r. acad .",
    "paris sr . a math .",
    "_ , 255:0 28972899 , 1962 .",
    "y.  nesterov .",
    "gradient methods for minimizing composite objective function .",
    "technical report  76 , catholic university of louvain , core , 2007 .",
    "j.  nocedal and s.  j. wright .",
    "_ numerical optimization_. springer verlag , 2000 .",
    "g.  pierra .",
    "decomposition through formalization in a product space .",
    "_ mathematical programming _ , 280 ( 1):0 96115 , 1984 .    c.  pontow and o.  scherzer . a derivative free approach for total variation regularization .",
    "_ arxiv:0911.1293 _ , 2009 .",
    "url http://arxiv.org/abs/0911.1293 .",
    "a.  ramdas and r.  j. tibshirani .",
    "fast and flexible admm algorithms for trend filtering .",
    "_ arxiv:1406.2082 _ , 2014 .",
    "f.  rapaport and e.  b. j .- p .",
    "_ bioinformatics _ , 240 ( 13):0 i375i382 , 2008 .",
    "a.  rinaldo .",
    "properties and refinements of the fused lasso . _",
    "annals of statistics _ , 370 ( 5b):0 29222952 , 2009 .    r.  t. rockafellar .",
    "monotone operators and hte proximal point algorithm .",
    "siam j. control and opt .",
    "_ , 140 ( 5):0 877898 , 1976 .",
    "s.  rogers , m.  girolami , c.  campbell , and r.  breitling . the latent process decomposition of cdna microarray data sets .",
    "_ ieee / acm trans . comp . bio . and bioinformatics _ , 20 ( 2 ) , april - june 2005 .",
    "l.  i. rudin , s.  osher , and e.  fatemi .",
    "nonlinear total variation based noise removal algorithms . _ physica d _ , 60:0 259268 , 1992 .",
    "s.  salzo and s.  villa .",
    "inexact and accelerated proximal point algorithms .",
    "_ j. convex analysis _ , 190 ( 4 ) , 2012 .",
    "m.  schmidt , n.  l. roux , and f.  bach . .",
    "in _ advances in neural information processing systems ( nips ) _ , 2011 .    s.  sra .",
    "scalable nonconvex inexact proximal splitting . in _ advances in neural information processing systems _ , 2012 .",
    "s.  sra , s.  nowozin , and s.  wright , editors .",
    "_ optimization for machine learning_. mit press , 2011 .",
    "g.  steidl , s.  didas , and j.  neumann .",
    "relations between higher order tv regularization and support vector regression . in _ scale - space _ , pages 515527 , 2005 .",
    "n.  stransky et  al . regional copy number - independent deregulation of transcription in cancer .",
    "_ nature genetics _ , 380 ( 12):0 13861396 , december 2006 .",
    "r.  tibshirani .",
    "regression shrinkage and selection via the lasso .",
    "_ j. r. statist .",
    "_ , 580 ( 1):0 267288 , 1996 .",
    "r.  tibshirani and p.  wang . .",
    "_ biostatistics _ , 90 ( 1):0 1829 , 2008 .    r.  tibshirani , m.  saunders , s.  rosset , j.  zhu , and k.  knight .",
    "sparsity and smoothness via the fused lasso .",
    "_ j. royal stat .",
    "series b _ , 670 ( 1):0 91108 , 2005 .    r.  j. tibshirani .",
    "adaptive piecewise polynomial estimation via trend filtering .",
    "_ the annals of statistics _ , 420 ( 1):0 285323 , 02 2014 .",
    "doi : 10.1214/13-aos1189 .",
    "broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays .",
    ", 96:0 67456750 , june 1999 .",
    "vert and k.  bleakley .",
    "fast detection of multiple change - points shared by many signals using group lars . in _ advances in neural information processing systems _ , 2010 .    c.  r. vogel and m.  e. oman .",
    "iterative methods for total variation denoising .",
    "_ siam journal on scientific computing _ , 170 ( 1):0 227238 , 1996 .",
    "b.  wahlberg , s.  boyd , m.  annergren , and y.  wang . .",
    "proceedings 16th ifac symposium on system identification _ , volume  16 , 2012 .",
    "s.  j. wright , r.  d. nowak , and m.  a.  t. figueiredo .",
    "sparse reconstruction by separable approximation .",
    "_ ieee trans .",
    "_ , 570 ( 7):0 24792493 , 2009 .",
    "m.  wytock , s.  sra , and j.  z. kolter . .",
    "in _ conference on uncertainty in artificial intelligence _ , 2014 .",
    "s.  yang , j.  wang , w.  fan , x.  zhang , p.  wonka , and j.  ye . .",
    "acm knowledge discovery and data mining ( kdd ) _ , chicago , illinois , usa , august 2013 .",
    "y.  yu . on decomposing the proximal map . in _ advances in neural information processing systems _ , 2013 .",
    "m.  yuan and y.  lin . .",
    "j. r. statist .",
    "b _ , 680 ( 1):0 4967 , 2006",
    ".    m.  zhu and t.  chan .",
    "an efficient primal - dual hybrid gradient algorithm for total variation image restoration .",
    "technical report , ucla cam , 2008 .",
    "we begin by recalling a few basic ideas from convex analysis ; we recommend the recent book  @xcite for more details .",
    "let @xmath26 be any set .",
    "a function @xmath422 is called _ lower semicontinuous _ if for every @xmath423 and a sequence @xmath424 that converges to @xmath137",
    ", it holds that @xmath425 the set of proper lsc convex functions on @xmath29 is denoted by @xmath426 ( such functions are also called _ closed convex functions _ ) .",
    "the _ indicator function _ of a set @xmath427 is defined as @xmath428 : { \\bm{x}}\\mapsto     \\begin{cases }      0 , & \\text{if}\\ { \\bm{x}}\\in c;\\\\      \\infty , & \\text{if}\\ { \\bm{x}}\\not\\in c ,    \\end{cases}\\ ] ] which is lsc if and only if @xmath427 is closed .",
    "the _ convex conjugate _ of @xmath8 is given by @xmath429 , and a particularly important example is the fenchel conjugate of a norm @xmath430 @xmath431 where the norm @xmath432 is dual to @xmath430 .",
    "let @xmath8 and @xmath94 be proper convex functions .",
    "the _ infimal convolution _ of @xmath8 with @xmath94 is the convex function given by @xmath433 . for our purposes ,",
    "the most important special case is infimal convolution of a convex function with the squared euclidean norm , which yields the _ moreau envelope _  @xcite .",
    "let @xmath434 and let @xmath435 .",
    "the _ moreau envelope _ of @xmath8 indexed by @xmath436 is @xmath437 the moreau envelope   is convex , real - valued , and continuous .",
    "see e.g.  ( * ? ? ?",
    "12.15 ) .    using the moreau envelope",
    ", we now formally introduce prox operators .",
    "let @xmath434 , and let @xmath438 .",
    "then @xmath439 is the unique point in @xmath29 that satisfies @xmath440 , i.e. , @xmath441 and the nonlinear map @xmath442 is called the _ prox operator _ of @xmath8 .",
    "sometimes the fenchel conjugate @xmath443 is easier to use than @xmath8 ; similarly , sometimes the operator @xmath444 is easier to compute than @xmath34 .",
    "the result below shows the connection .",
    "[ prop.decomp ] let @xmath434 , @xmath435 , and @xmath438 .",
    "then , @xmath445    a brief exercise ; see e.g. ,  ( * ? ? ?",
    "14.3 ) .",
    "this decomposition provides the necessary tools to exploit useful primal  dual relations . for the sake of clarity we also present a last result regarding a particular primal - dual relation that plays a key role in our algorithms .",
    "[ prop.pd ] let @xmath446 and @xmath447 .",
    "the problems below form a primal - dual pair .",
    "@xmath448    introduce an extra variable @xmath449 , so that the dual function is @xmath450 which upon rewriting using fenchel conjugates yields  .",
    "all the total  variation proximity solvers in this paper have been implemented as the * proxtv * toolbox , available at https://github.com/albarji/proxtv .",
    "the toolbox has been designed to be used out of the box in a user friendly way ; the top  level matlab function ` tv ` solves total  variation proximity for a given signal under a variety of settings . for instance    ....",
    "> > tv(x , lambda ) ....    solves @xmath451 proximity for a signal ` x ` of any dimension and a regularization value ` lambda ` .",
    "the weighted version of this problem is also seamlessly tackled by just providing a vector of weights of the appropriate length as the ` lambda ` parameter .    if a third parameter ` p ` is provided as    ....",
    "> > tv(x , lambda , p ) ....    the general @xmath411 proximity problem is addressed , whereupon an adequate solver is chosen by the library .",
    "more advanced uses of the library are possible , allowing to specify which norm ` p ` and regularizer ` lambda ` values to use for each dimension of the signal , and even applying combinations of several different @xmath411 regularizers along the same dimension .",
    "please refer to the documentation within the toolbox for further information .",
    "[ the : tautstringeq ] given the problems @xmath452 and @xmath453 for a non - zero vectors @xmath271 , both problems share the same minimum @xmath454 .",
    "the lagrangian of problem [ eq : tautstringeq1 ] takes the form @xmath455 and its karush - kuhn - tucker optimality conditions are given by @xmath456 @xmath457 , and where the first equation comes from the fact that @xmath458 at the minimum .    as the only difference between problems [ eq : tautstringeq1 ] and [ eq : tautstringeq2 ] is in the form of the objective",
    ", the kkt conditions for problem [ eq : tautstringeq2 ] take the same form , but for the first one , @xmath459 @xmath457 , and where we use hat notation for the dual coefficients to tell them apart from those of problem [ eq : tautstringeq1 ] .",
    "suppose @xmath290 minimizer to problem [ eq : tautstringeq1 ] , hence fulfilling the conditions [ eq : tautstringeq1kkt1]-[eq : tautstringeq1kkt5 ] .",
    "in particular this means that it is feasible to assign values to the dual coefficients @xmath460 in such a way that the conditions above are met . if we set @xmath461 in the conditions [ eq : tautstringeq2kkt1]-[eq : tautstringeq2kkt5 ] the following observations are of relevance    * condition [ eq : tautstringeq2kkt2 ] becomes the same as condition [ eq : tautstringeq1kkt2 ] , and so it is immediately met . *",
    "the operator @xmath462 is contractive and monotonous .",
    "* the couple @xmath463 can not be both zero at the same time , since @xmath464 enforces @xmath465 and @xmath466 enforces @xmath467 . * hence and because @xmath468 and condition [ eq : tautstringeq1kkt1 ] holds , when @xmath469 then @xmath464 , @xmath470 , and when @xmath471 then @xmath472 , @xmath466 .",
    "* @xmath473 has the same sign as @xmath474 , since @xmath294 is monotonous and as such preserves ordering . *",
    "since @xmath294 is contractive , condition [ eq : tautstringeq2kkt1 ] can be met by setting @xmath475 for some @xmath476 .",
    "note that this works because @xmath463 can not be both zero at the same time .",
    "* condition [ eq : tautstringeq2kkt3 ] is met for those choices of @xmath477 , as [ eq : tautstringeq1kkt3 ] was met for @xmath478 and @xmath476 . *",
    "conditions [ eq : tautstringeq2kkt4 ] and [ eq : tautstringeq2kkt5 ] are also met for those choices of @xmath477 , as @xmath479 and @xmath480 .",
    "therefore , all of the optimality conditions [ eq : tautstringeq2kkt1]-[eq : tautstringeq2kkt5 ] for problem [ eq : tautstringeq2 ] are met for @xmath290 solution of problem [ eq : tautstringeq1 ] , and so a minimum of problem [ eq : tautstringeq1 ] is also a minimum for problem [ eq : tautstringeq2 ] .",
    "the proof can be repeated the other way round by setting @xmath481 optimal for problem [ eq : tautstringeq2 ] , defining the operator @xmath482 , and observing that this operator is monotonous and expansive , so we can establish @xmath483 for some @xmath484 and the optimality conditions [ eq : tautstringeq1kkt1]-[eq : tautstringeq1kkt5 ] for problem [ eq : tautstringeq1 ] are met following a similar reasoning to the one presented above .",
    "thus , a minimum for problem [ eq : tautstringeq2 ] is also a minimum for problem [ eq : tautstringeq1 ] , which joined with the previous result completes the proof .",
    "the images used in the experiments are displayed in what follows , along with their noisy / denoised and convoluted / deconvoluted versions for each algorithm tested .",
    "qr barcode images were generated by encoding random text using google chart api .",
    "images _ shape _ and _ phantom _ are publicly available and frequently used in image processing .",
    "_ trollface _ and _ comic _ are also publicly available .",
    ", used in the multicore experiments , is a high resolution @xmath485 photograph of gaudi s casa batll .",
    "the rest of the images were originally created by the authors ."
  ],
  "abstract_text": [
    "<S> one of the most frequently used notions of `` structured sparsity '' is that of sparse ( discrete ) gradients , a structure typically elicited through _ total - variation ( tv ) _ regularizers . </S>",
    "<S> this paper focuses on anisotropic tv - regularizers , in particular on @xmath0-norm _ weighted tv regularizers _ for which it develops efficient algorithms to compute the corresponding proximity operators . </S>",
    "<S> our algorithms enable one to scalably incorporate tv regularization of vector , matrix , or tensor data into a proximal convex optimization solvers . </S>",
    "<S> for the special case of vectors , we derive and implement a highly efficient weighted 1d - tv solver . </S>",
    "<S> this solver provides a backbone for subsequently handling the more complex task of higher - dimensional ( two or more ) tv by means of a modular proximal optimization approach . </S>",
    "<S> we present numerical experiments that demonstrate how our 1d - tv solver matches or exceeds the best known 1d - tv solvers . </S>",
    "<S> thereafter , we illustrate the benefits of our modular design through extensive experiments on : ( i ) image denoising ; ( ii ) image deconvolution ; ( iii ) four variants of fused - lasso ; and ( iv ) video denoising . </S>",
    "<S> our results show the flexibility and speed our tv solvers offer over competing approaches . to underscore our claims , </S>",
    "<S> we provide our tv solvers in an easy to use multi - threaded c++ library ( which also aids reproducibility of our results ) . </S>"
  ]
}