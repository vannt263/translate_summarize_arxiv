{
  "article_text": [
    "principal component analysis ( pca ) is a powerful and widely used technique to analyze data by forming a custom set of `` principal component '' eigenvectors that are optimized to describe the most data variance with the fewest number of components @xcite . with the full set of eigenvectors",
    "the data may be reproduced exactly , _",
    "i.e. _ , pca is a transformation which can lend insight by identifying which variations in a complex dataset are most significant and how they are correlated .",
    "alternately , since the eigenvectors are optimized and sorted by their ability to describe variance in the data , pca may be used to simplify a complex dataset into a few eigenvectors plus coefficients , under the approximation that higher - order eigenvectors are predominantly describing fine tuned noise or otherwise less important features of the data .",
    "example applications within astronomy include classifying spectra by fitting them to pca templates @xcite , describing hubble space telescope point spread function variations @xcite , and reducing the dimensionality of cosmic microwave background map data prior to analysis @xcite .",
    "a limitation of classic pca is that it does not distinguish between variance due to measurement noise _",
    "vs_. variance due to genuine underlying signal variations .",
    "even when an estimate of the measurement variance is available , this information is not used when constructing the eigenvectors , _",
    "e.g. _ , by deweighting noisy data .",
    "a second limitation of classic pca is the case of missing data . in some applications , certain observations",
    "may be missing some variables and the standard formulas for constructing the eigenvectors do not apply .",
    "for example within astronomy , observed spectra do not cover the same rest - frame wavelengths of objects at different redshifts , and some wavelength bins may be masked due to bright sky lines or cosmic ray contamination .",
    "missing data is an extreme case of noisy data , where missing data is equivalent to data with infinite measurement variance .",
    "this work describes a pca framework which incorporates estimates of measurement variance while solving for the principal components .",
    "this optimizes the eigenvectors to describe the true underlying signal variations without being unduly affected by known measurement noise .",
    "code which implements this algorithm is available at https://github.com/sbailey/empca  .",
    "@xcite  13.6 and  14.2 review prior work on pca with missing data and incorporating weights into pca .",
    "most prior work focuses on the identification and removal of outlier data , interpolation over missing data , or special cases such as when the weights can be factorized into independent per - observation and per - variable weights .",
    "@xcite , @xcite , @xcite , and @xcite present iterative solutions for the case of general weights , though none of these find the true pca solution with orthogonal eigenvectors optimally ranked by their ability to describe the data variance . instead , they find an unsorted set of non - orthogonal vectors which as a set are optimized to describe the data variance , but individually they are not the optimal linear combinations to describe the most variance with the fewest vectors .",
    "their methods are sufficient for weighted lower - rank matrix approximation , but they lack the potential data insight from optimally combining and sorting the eigenvectors to see which components contribute the most variation .    within the astronomy literature , @xcite",
    "discuss how to interpolate over missing data and use pca eigenspectra to _ fit _",
    "noisy and/or missing data , but they do not address the case of how to _ generate _ eigenspectra from noisy but non - missing data .",
    "@xcite generate template spectra from noisy and missing data using non - negative matrix factorization ( nmf ) .",
    "this method is similar to pca with the constraint that the template spectra are strictly positive , while not requiring the templates to be orthonormal . @xcite",
    "present a more general `` heteroskedastic matrix factorization '' approach to study sloan digital sky survey spectra while properly accounting for measurement noise .",
    "their underlying goal is similar to this work , though with an algorithmically different implementation .",
    "the methods presented here directly solve for the pca eigenvectors with an iterative solution based upon expectation maximization pca ( empca ) .",
    "@xcite describes an unweighted version of empca , including a method for interpolating missing data , but he does not address the issue of deweighting noisy data .",
    "we also take advantage of the iterative nature of the solution to bring unique extensions to pca , such as noise - filtering the eigenvectors during the solution .",
    "the approach taken here is fundamentally pragmatic .",
    "for example , if one is interested in generating eigenvectors to describe 99% of the signal variance , it likely does nt matter if an iterative algorithm has `` only '' converged at the level of @xmath0 even if the machine precision is formally @xmath1 .",
    "we discuss some of the limitations of weighted empca in ",
    "[ sec : discussion ] , but ultimately we find that these issues are not limiting factors for practical applications .",
    "this work was originally developed for pca analysis of astronomical spectra and examples are given in that context .",
    "it should be noted , however , that these methods are generally applicable to any pca application with noisy and/or missing data  nothing in the underlying methodology is specific to astronomical spectra .",
    "this paper uses the following notation : vectors use arrows , @xmath2 , while @xmath3 represents the scalar element @xmath4 of vector @xmath2 . for sets of vectors ,",
    "@xmath5 represent vector number @xmath6 ( not element @xmath6 of vector @xmath2 ) .",
    "matrices are in boldface , @xmath7 .",
    "to denote vectors formed by selected columns or rows of a matrix , we use @xmath8 for the vector formed from column @xmath6 of matrix @xmath7 and @xmath9 for the vector formed from row @xmath4 of matrix @xmath7 . the scalar element at row @xmath4 column @xmath6 of matrix @xmath7 is @xmath10 .    for reference , we summarize the names of the primary variables here : @xmath7 is the data matrix with @xmath11 rows of variables and @xmath12 columns of observations .",
    "@xmath13 is the pca eigenvector matrix with @xmath11 rows of variables and @xmath14 columns of eigenvectors ; @xmath15 is a single eigenvector .",
    "these eigenvectors may fit the data using a matrix of coefficients @xmath16 , where @xmath17 is the contribution of eigenvector @xmath18 to observation @xmath6 .",
    "indices @xmath4 , @xmath6 , and @xmath18 index variables , observations , and eigenvectors respectively .",
    "@xmath19 is the vector formed by stacking the columns of matrix @xmath7 .",
    "the _ measurement _ covariance of dataset @xmath19 is @xmath20 , while @xmath21 is the weights matrix for dataset @xmath7 for the case of independent measurement noise such that @xmath21 has the same dimensions as @xmath7 .",
    "for an accessible tutorial on classic pca , see @xcite .",
    "a much more complete treatment is found in @xcite .",
    "algorithmically , the steps are simple : the principal components @xmath22 of a dataset are simply the eigenvectors of the covariance of that dataset , sorted by their descending eigenvalues .",
    "a new observation @xmath23 may be approximated as @xmath24 where @xmath25 is the mean of the initial dataset and @xmath26 is the reconstruction coefficient for eigenvector @xmath27 .",
    "for the rest of this paper , we will assume that @xmath25 has already been subtracted from the data , _",
    "i.e. _ , @xmath28 .    to find a particular coefficient @xmath29 ,",
    "take the dot product of both sides with @xmath30 , noting that because of the eigenvector orthogonality , @xmath31 ( kroeneker - delta ) : @xmath32    note that the neither the solution of @xmath22 nor @xmath33 makes use of any noise estimates or weights for the data .",
    "as such , classic pca solves the minimization problem @xmath34_{ij}^2\\ ] ] where @xmath7 is a dataset matrix whose columns are observations and rows are variables , @xmath13 is a matrix whose columns are the principal components @xmath35 to find , and @xmath16 is a matrix of coefficients to fit @xmath7 using @xmath13 . for clarity ,",
    "the dimensions of these matrices are : @xmath36 $ ] , @xmath37 $ ] , and @xmath38 $ ] , where @xmath39 , @xmath40 , and @xmath41 are the number of observations , variables , and eigenvectors respectively . for example , when performing pca on spectra , @xmath39 is the number of spectra , @xmath40 is the number of wavelength bins per spectrum , and @xmath41 is the number of eigenvectors used to describe the data , which may be smaller than the total number of possible eigenvectors .",
    "the goal of this work is to solve for the eigenvectors @xmath13 while incorporating a weights matrix @xmath21 on the dataset @xmath7 : @xmath42_{ij}^2\\ ] ] we also describe the more general cases of per - observation covariances @xmath43 : @xmath44 where we have used the notation that @xmath8 is the vector formed from the @xmath6th column of the matrix @xmath7 , and similarly for @xmath45 . in the most general case , there is covariance @xmath20 between all variables of all observations , _",
    "i.e. _ , we seek to minimize @xmath46 \\vec\\mathbf{c } \\right)^t      { \\mathbf{v}}^{-1 }      \\left ( \\vec{\\mathbf x } - [ \\mathbf{p } ] \\vec\\mathbf{c } \\right)\\ ] ] where @xmath47 and @xmath48 are the vectors formed by concatenating all columns of @xmath7 and @xmath16 , and @xmath49 $ ] is the matrix formed by stacking @xmath13 @xmath12 times .",
    "this allows one to incorporate error estimates on heteroskedastic data such that particularly noisy data does not unduly influence the solution . we will solve this problem using an iterative method known within the statistics community as `` expectation maximization ''",
    "expectation maximization ( em ) is an iterative technique for solving parameters to maximize a likelihood function for models with unknown hidden ( or latent ) variables @xcite .",
    "each iteration involves two steps : finding the expectation value of the hidden variables given the current model ( e - step ) , and then modifying the model parameters to maximize the fit likelihood given the estimates of the hidden variables ( m - step ) .    as applied to pca , the parameters to solve are the eigenvectors , the latent variables are the coefficients @xmath50 for fitting the data using those eigenvectors , and the likelihood is the ability of the eigenvectors to describe the data . to solve the single most significant eigenvector ,",
    "start with a random vector @xmath51 of length @xmath11 .",
    "for each observation @xmath5 , solve the coefficient @xmath52 which best fits that observation using @xmath15 . then using those coefficients , update @xmath15 to find the vector that best fits the data given those coefficients : @xmath53 . then normalize @xmath15 to unit length and iterate the solutions to @xmath50 and @xmath15 until converged . in summary :    0.5 in = 0.25 in = 0.25 in = 0.25 in = 1.0 in = @xmath54 random vector of length @xmath11 + repeat until converged : + for each observation @xmath5 : + @xmath55 + @xmath53 + @xmath56 _ renormalize + return @xmath51 _    0.0 in this generates a vector @xmath15 which is the dominant pca eigenvector of the dataset @xmath7 , where the observations @xmath5 are the columns of @xmath7 .",
    "the expectation step finds the coefficients @xmath57 which best fit @xmath7 using @xmath51 ( see equations [ eq : alphadot1 ] to [ eq : alphadotn ] ) .",
    "the likelihood maximization step then uses those coefficients to update @xmath15 to minimize @xmath58 in practice the normalization @xmath59 in the m - step is unnecessary since @xmath15 is renormalized to unit length after every iteration .    at first glance , it can be surprising that this algorithm works at all .",
    "its primary enabling feature is that at each iteration the coefficients @xmath60 and vector @xmath51 minimize the @xmath61 better than the previous iteration .",
    "the @xmath61 of equation  [ eq : unweighted_chi2 ] has a single minimum @xcite , thus when any minimum is found it is the true global minimum .",
    "it is possible , however , to also have saddle points to which the empca algorithm could converge from particular starting points .",
    "it is easy to test solutions for being at a saddle point and restart the iterations as needed .",
    "the specific convergence criteria are application specific .",
    "one pragmatic option is that the eigenvector itself is changing slowly , _",
    "i.e. _ , @xmath62 .",
    "alternately , one could require that the change in likelihood ( or @xmath63 ) from one iteration to the next is below some threshold .",
    "convergence and uniqueness will be discussed in @xmath64[sec : convergence ] and  [ sec : uniqueness ] .",
    "for now we simply note that many pca applications are interested in describing 95% or 99% of the data variance , and the above algorithm typically converges very quickly for this level of precision , even for cases when the formal computational machine convergence may require many iterations .    to find additional eigenvectors , subtract the projection of @xmath15 from @xmath65 and repeat the algorithm .",
    "continue this procedure until enough eigenvectors have been solved that the remaining variance is consistent with the expected noise of the data , or until enough eigenvectors exist to approximate the data with the desired fidelity .",
    "if only a few eigenvectors are needed for a large dataset , this algorithm can be much faster than classic pca which requires solving for all eigenvectors whether or not they are needed . scaling performance will be discussed further in @xmath64[sec : scaling ] .",
    "the above algorithm treats all data equally when solving for the eigenvectors and thus is equivalent to classic pca .",
    "if all data are of approximately equal quality then this is fine , but if some data have considerably larger measurement noise they can unduly influence the solution . in these cases , the high signal - to - noise data should receive greater weight than low signal - to - noise data .",
    "this is conceptually equivalent to the difference between a weighted and unweighted mean .    in some applications ,",
    "it is sufficient to have a single weight per observation so that all variables within an observation are equally weighted , but different observations are weighted more or less than others . in this case , empca can be extended with per - observation weights @xmath66 .",
    "the observations @xmath67 should have their weighted mean subtracted , and the likelihood maximization step ( m - step ) is replaced with : @xmath68 the normalization denominator has been dropped because we re - normalize @xmath15 to unit length every iteration .      if each variable for each observation has a different weight , the situation becomes more complicated since we can not use simple dot products to derive the coefficients @xmath57 .",
    "instead , one must solve a set of linear equations for @xmath57 .",
    "similarly , the likelihood maximization step must solve a set of linear equations to update @xmath51 instead of just performing a simple sum .",
    "the weighted empca algorithm now starts with a _ set _ of random orthonormal vectors @xmath69 and iterates over :    1 .   for each observation @xmath5 , solve coefficients @xmath70 : @xmath71 2 .   given @xmath72 ,",
    "solve each @xmath73 one - by - one for @xmath18 in @xmath74 : @xmath75    both of the above steps can be solved using weights on @xmath5 , thus achieving the goals of properly weighting the data while solving for the coefficients @xmath72 and eigenvectors @xmath73 .",
    "implementation details will be described in the following two sub - sections , where we will return to using matrix notation .      in equation [ eq : expectation ] ,",
    "the @xmath77 vectors are fixed and one solves the coefficients @xmath70 with a separate set of equations for each observation @xmath5 . written in matrix form",
    ", @xmath78 can be solved for each independent observation column @xmath6 of @xmath7 and @xmath16 : @xmath79 equation [ eq : solve_c ] is illustrated in figure  [ fig : solve_c ] .",
    "solving equation [ eq : solve_c ] for @xmath45 with noise - weighting by measurement covariance @xmath43 is a straight - forward linear least - squares problem which may be solved with singular value decomposition ( svd ) , qr factorization , conjugate gradients , or other methods .",
    "for example , using the method of `` normal equations '' and the shorthand @xmath80 and @xmath81 : @xmath82    if the noise is independent between variables , the inverse covariance @xmath83 is just a diagonal matrix of weights @xmath84 .",
    "note that the covariance here is the estimated _ measurement _ covariance , not the total dataset variance  the goal is to weight the observations by the estimated _ measurement _ variance so that noisy observations do not unduly affect the solution , while allowing pca to describe the remaining _ signal _ variance .    in the more general case of measurement covariance between different observations",
    ", one can not solve equation [ eq : solve_c ] for each column of @xmath7 independently .",
    "instead , solve @xmath85 \\vec { \\mathbf{c}}$ ] with the full covariance matrix @xmath86 of @xmath87 , where @xmath88 $ ] is the matrix formed by stacking @xmath13 @xmath12 times , and @xmath19 and @xmath89 are the vectors formed by stacking all the columns of the matrices @xmath7 and @xmath16 .",
    "this requires the solution of a single @xmath90 matrix rather than @xmath12 solutions of @xmath91 matrices .",
    "if the individual observations are uncorrelated , it is computationally advantageous to use this non - correlation to solve multiple smaller matrices rather than one large one .      in the second step of each iteration ( equation [ eq : maximization ] ) , we use the fixed coefficients @xmath16 ( dimensions @xmath93 ) and solve for the eigenvectors @xmath13 ( dimensions @xmath94 ) .",
    "we solve the eigenvectors one - by - one to maximize the power in each eigenvector before solving the next .",
    "selecting out just the @xmath18th eigenvector uses the @xmath18th column of @xmath13 and the @xmath18th row of @xmath16 : @xmath95 where @xmath96 signifies an outer product .",
    "if the variables ( rows ) of @xmath7 are independent , then we can solve for a single element of @xmath97 at a time : @xmath98 this is illustrated in figure  [ fig : solve_p ] . with independent weights @xmath99 on the data @xmath9 ,",
    "we solve variable @xmath4 of eigenvector @xmath18 with : @xmath100    as with section [ sec : solve_c ] , if there are measurement covariances between the data , equation [ eq : solve_p ] may be expanded to solve for all elements of @xmath97 simultaneously using the full measurement covariance matrix of @xmath7 .    after solving for @xmath97 ,",
    "subtract its projection from the data : @xmath101 this removes any variation of the data in the direction of @xmath97 so that additional eigenvectors will be orthogonal to the prior ones . then repeat the procedure to solve for the next eigenvector @xmath102",
    "the flexibility of the iterative empca solution allows for a number of powerful extensions to pca in addition to noise weighting .",
    "we describe a few of these here .",
    "if the length scale of the underlying signal eigenvectors is larger than that of the noise , it may be advantageous to smooth the eigenvectors to remove remaining noise effects .",
    "the iterative nature of empca allows smoothing of the eigenvectors at each step to remove the high frequency noise .",
    "this generates the optimal smooth eigenvectors by construction rather than smoothing noisy eigenvectors afterward .",
    "this will be shown in the examples in section [ sec : examples ] .",
    "alternately , one can include a smoothing prior or regularization term when solving for the principal components @xmath13 .",
    "that approach , however , requires solving equation [ eq : solve_p ] ( plus a regularization term ) for all elements of @xmath97 simultaneously instead of using the numerically much faster equation [ eq : solve_weighted_pik ] for the case of diagonal measurement covariance .      in some applications ,",
    "one has a few _ a priori _ template vectors to include in the fit , _",
    "e.g. _ , from some physical model .",
    "the goal is to find additional template vectors which are to be combined with the _ a priori _ vectors for the best fit of the data .",
    "due to noise weighting and the potential non - orthogonality of the _ a priori _ vectors , the best fit is a joint fit and one can not simply fit the _ a priori _ vectors and remove their components before proceeding with finding the other unknown vectors .",
    "this case can be incorporated into empca by including the _ a priori _ vectors in the starting vectors @xmath13 and simply keeping them fixed with each iteration rather than updating them . in each iteration ,",
    "the _ coefficients _ for the _ a priori _ vectors are updated , but not the vectors themselves .",
    "figure [ fig : toy_data ] shows example noisy data used to test weighted empca .",
    "100 data vectors were generated using 3 orthonormal sine functions as input , with random amplitudes drawn from gaussian distributions .",
    "the lower frequency sine waves were given larger gaussian sigmas such that they contribute more signal variance to the data .",
    "gaussian random noise was added , with 10% of the data vectors receiving 25 times more noise from [ 0 , @xmath103 and 5 times more noise from [ @xmath104 , @xmath105 . for weighted empca",
    ", weights were assigned as @xmath106 , where @xmath107 is the per - observation per - variable gaussian sigma of the added noise ( not the sigma of the underlying signal ) .",
    "a final dataset was created where a contiguous 10% of each observation was set to have weight=0 to create regions of missing data . as a crosscheck that the 0 weights are correctly applied , the data in these regions were set to a constant value of 1000  if these data are not correctly ignored by the algorithm they will have a large effect on the extracted eigenvectors .",
    "figure [ fig : toy_pca ] show the results of applying classic pca and weighted empca to these data .",
    "_ upper left _ : classic pca applied to the noiseless data recovers the input eigenvectors , slightly rotated to form the best ranked eigenvectors for describing the data variance . _",
    "upper right _ : empca applied to the same noiseless data recovers the same input eigenvectors . _",
    "middle left _ : when classic pca is applied to the noisy data , the highest order eigenvector is dominated by the noise , and the effects of the non - uniform noise are clearly evident as increased noise from [ 0 , @xmath103 .",
    "_ middle right _ :",
    "weighted empca is much more robust to the noisy data , extracting results close to the original eigenvectors .",
    "the highest order eigenvector is still affected by the noise , which is a reflection that the noise does contribute power to the data variance",
    ". however , the extra - noisy region from [ 0 , @xmath103 is not affected more than the region from [ @xmath104 , @xmath105 , due to the proper deweighting of the noisy data . _ lower left _ : smoothed weighted empca is almost completely effective at extracting the original eigenvectors with minimal impact from the noise .",
    "_ lower right _",
    ": even when 10% of every observation is missing , smoothed weighted empca is effective at extracting the underlying eigenvectors .",
    "all eigenvectors for all methods are orthogonal at the level of @xmath108 .",
    "figure  [ fig : qso_pca ] shows the results of applying classic pca and weighted empca to qso spectra from the sloan digital sky survey data release 7 @xcite , using the qso redshift catalog of @xcite .",
    "500  spectra of qsos with redshift @xmath109 were randomly selected and trimmed to @xmath110    to show the siiv and civ emission features .",
    "spectra with more than half of the pixels masked were discarded .",
    "each spectrum was normalized to median[flux(@xmath111   ) ]  @xmath112 and the weighted mean of all normalized spectra was subtracted .",
    "the left panel of fig .",
    "[ fig : qso_pca ] plots examples of high , median , and low signal - to - noise spectra and a broad absorption line ( bal ) qso from this sample .",
    "@xmath1132% of the spectral bins have been flagged with a bad - data mask , _",
    "e.g. _ , due to cosmic rays , poor sky subtraction , or the presence of non - qso narrow absorption features from the intergalactic medium .",
    "these are treated as missing data with weight=0 .",
    "the goal of weighted empca is to properly deweight the noisy spectra such that the resulting eigenvectors are predominantly describing the underlying signal variations and not just measurement noise .",
    "weights are @xmath114 where @xmath115 is the sdss pipeline estimated measurement noise for wavelength bin @xmath4 of spectrum @xmath6 .",
    "weighted empca can also properly ignore the masked data by using weight=0 without having to discard the entire spectrum , or artificially interpolate over the masked region .",
    "the right panels of fig .",
    "[ fig : qso_pca ] show the results for the first 3 eigenvectors of classic pca ( top right ) and weighted empca ( bottom right ) .",
    "eigenvectors 0 , 1 , and 2 are plotted in blue , green , and red respectively .",
    "the mean spectrum was subtracted prior to performing pca such that these eigenvectors represent the principal variations of the spectra with respect to that mean spectrum .",
    "eigenvectors are orthogonal at the level of @xmath108 .",
    "the empca eigenvectors are much less noisy than the classic pca eigenvectors . as such",
    ", they are more sensitive to genuine signal variations in the data . for example , the sharp features between @xmath116    in the empca eigenspectra arise from bal qsos , an example of which is shown in the bottom of the left panel of fig .",
    "[ fig : qso_pca ] . these features",
    "are used to study qso outflows , _ e.g. _",
    "@xcite , yet they are lost amidst the noise of the classic pca eigenspectra .",
    "similarly , the empca eigenspectra are more sensitive to the details of the variations in shape and location of the emission peaks used to study qso metallicity ( _ e.g. _  @xcite ) and black hole mass ( _ e.g. _  @xcite ) .",
    "@xcite discuss the convergence properties of the em algorithm in general .",
    "each iteration , by construction , finds a set of parameters that are as good or better a fit to the data than the previous step , thus guaranteeing convergence .",
    "the caveat is that the `` likelihood maximization step '' is typically implemented as solving for a stationary point of the likelihood surface rather than strictly a maximum .",
    "_ e.g. _ , @xmath117 is also true at saddle points and minima of the likelihood surface , thus it is possible that the em algorithm will not converge to the true global maximum .",
    "unweighted pca has a likelihood surface with a single global maximum , but in general this is not the case for weighted pca : the weights in equation  [ eq : weighted_chi2 ] can result in local false @xmath61 minima @xcite .",
    "3.6 also gives examples of this behavior ( taken from @xcite and @xcite ) for the closely related problem of factor analysis .",
    "the example datasets are somewhat contrived and the minimum or saddle point convergence only happens with particular starting conditions .",
    "we have encountered false minima with weighted empca when certain observations have @xmath11390% of their variables masked while giving large weight to their remaining unmasked variables . in this case",
    "the resultant eigenvectors can have artifacts tuned to the highly weighted but mostly masked input observations .",
    "when only a few ( @xmath11310% ) of the variables are masked per observation , we have not had a problem with false minima .",
    "the algorithm outlined in section [ sec : wempca ] solves for each eigenvector one at a time in order to maximize the power in the initial eigenvectors .",
    "this can result in a situation where a given iteration can improve the power described by the first few eigenvectors while degrading the total @xmath61 using all eigenvectors .",
    "we have not found a case where this significantly degrades the global @xmath61 , however .",
    "the speed of convergence is also not guaranteed .",
    "@xcite gives a toy example of fast convergence for gaussian - distributed data ( 3 iterations ) , and slow convergence for non - gaussian - distributed data ( 23 iterations ) . in practice",
    "we find that when empca is slow to converge , it is exploring a shallow likelihood surface between two nearly degenerate eigenvectors .",
    "this situation pertains to the uniqueness of the solution , described in the following section .",
    "weighted empca may produce unstable solutions if it is used to solve for more eigenvectors than are actually present in the data , or for eigenvectors that are nearly singular .",
    "since empca uses all eigenvectors while solving for the coefficients during each iteration , the singular eigenvectors can lead to delicately balanced meaningless values of the coefficients , which in turn degrades the solution of the updated eigenvectors in the next iteration .",
    "we recommend starting with solving for a small number of eigenvectors , and then increasing the number of eigenvectors if the resulting solution does not describe enough of the data variance .    for these reasons , one should use caution when analyzing data with empca , just as one should do with any problem which is susceptible to false minima or other convergence issues . in practice , we find that the benefits of proper noise - weighting outweigh the potential convergence problems",
    ".      given that empca is an iterative algorithm with a random starting point , the solution is not unique .",
    "in particular , if two eigenvalues are very close in magnitude , empca could return an admixture of the corresponding eigenvectors while still satisfying the convergence criteria . in practice , however , empca is pragmatic : if two eigenvectors have the same eigenvalue , they are also equivalently good at describing the variance in the data and could be used interchangeably .",
    "science applications , however , generally require strict algorithmic reproducibility and thus empca should be used with a fixed random number generator seed or fixed orthonormal starting vectors such as legendre polynomials .",
    "the convergence criteria define when a given vector is `` good enough '' to move on to the next iteration , but they do not guarantee uniqueness of that vector .",
    "figure [ fig : qso_convergence ] shows the first 3 eigenvectors for 5 different empca solutions of the qso spectra from section [ sec : qso ] with different random starting vectors . after 20 iterations",
    "the eigenvectors agree to @xmath118 on both large and small scales .",
    "although this agreement is worse than the machine precision of the computation , it is much smaller than the scale of differences between the eigenvectors and it represents a practical level of convergence for most pca applications .",
    "weighted empca improves the pca eigenvector solution by preventing noisy or missing data from unduly contributing noise instead of signal variation .",
    "however , the opposite case of high signal - to - noise data can also be problematic if just a few of the observations have significantly higher weight than the others .",
    "these will dominate the empca solution just as they would dominate a weighted mean calculation",
    ". this may not be the desired effect since the initial eigenvectors will describe the differences between the highly weighted data and subsequent eigenvectors will describe the lower weighted data .",
    "this may be prevented by purposefully down - weighting certain observations or applying an upper limit to weights so that the weighted dataset is nt dominated by just a few observations .",
    "the primary advantage of empca is the ability to incorporate weights on noisy data to improve the quality of the resulting eigenspectra .",
    "a secondary benefit over classic pca is algorithmic speed for the common case of needing only the first few eigenvectors from a dataset with @xmath119 .",
    "pca requires solving the eigenvectors of the data covariance matrix , an @xmath120 operation .",
    "the weighted empca algorithm described here involves iterating over multiple solutions of smaller matrices .",
    "each iteration requires @xmath12 solutions of @xmath121 to solve the coefficients and @xmath122 operations to solve the eigenvectors .",
    "thus weighted empca can be faster than classic pca when @xmath123 , ignoring the constant prefactors . if one has a few hundred spectra ( @xmath12 ) with a few thousand wavelengths each ( @xmath11 ) and wishes to solve for the first few eigenvectors ( @xmath14 ) , then empca can be much faster than classic pca .",
    "conversely , if one wishes to perform pca on all @xmath1131 million spectra from sdss , then @xmath124 and classic pca is faster , albeit with the limitations of not being able to properly weight noisy or missing data .",
    "if the problem involves off - diagonal covariances , then weighted empca involves a smaller number of larger matrix solutions for an overall slowdown , though note that classic pca is unable to properly solve the problem at all .    as a performance example , we used empca to study the variations in the simulated point spread function ( psf ) of a new spectrograph design .",
    "the psfs were simulated on a grid of 11 wavelengths and 6 slit locations , and were sampled over @xmath125  @xmath126 m spots on a 1  @xmath126 m grid , for a total of 40000 variables per spot .",
    "classic pca would require singular value decomposition of a @xmath127 matrix .",
    "while this is possible , it is beyond the scope of a typical laptop computer . on the other hand , using empca with constant weights we were able to recover the first 30 eigenvectors covering 99.7% of the psf variance in less than 6 minutes on a 2.13 ghz macbook air laptop .    for datasets where @xmath11 is particularly large , the memory needed to store the @xmath128 covariance matrix",
    "may be a limiting factor for classic pca .",
    "the iterative nature of empca allows one to scale to extremely large datasets since one never needs to keep the entire dataset ( nor its covariance ) in memory at one time .",
    "the multiple independent equations to solve in sections [ sec : solve_c ] and [ sec : solve_p ] are naturally computationally parallelizable .",
    "python code implementing the weighted empca algorithm described here is available at https://github.com/sbailey/empca  .",
    "the current version implements the case of independent weights but not the more generalized case of off - diagonal covariances .",
    "it also implements the smoothed eigenvectors described in  [ sec : smooth ] , but not _ a priori _ eigenvectors (  [ sec : apriori ] ) , nor distributed calculations (  [ sec : scaling ] ) . for comparison ,",
    "the empca module also includes implementations of classic pca and weighted lower - rank matrix approximation .",
    "examples for this paper were prepared with tagged version v0.2 of the code .",
    "when using the code , note that the orientation of the data and weights vectors is the transpose of the notation used here , _",
    "i.e. _ , data[j , i ] is variable i of observation j so that data[j ] is a single observation .",
    "to briefly summarize the algorithm : a data matrix @xmath7 can be approximated by a set of eigenvectors @xmath13 with coefficients",
    "@xmath16 : @xmath129 a covariance matrix @xmath20 describes the estimated measurement noise .",
    "the weighted empca algorithm seeks to find the optimal @xmath13 and @xmath16 given @xmath7 and @xmath20 .",
    "it starts with a random set of orthonormal vectors @xmath13 , and then iteratively alternates solutions for @xmath16 given @xmath130 and @xmath13 given @xmath131 .",
    "the problem is additionally constrained by the requirement to maximize the power in the fewest number of eigenvectors ( columns of @xmath13 ) . to accomplish this",
    ", the algorithm solves for each eigenvector individually , before removing its projection from the data and solving for the next eigenvector .",
    "if the measurement errors are independent , the covariance can be described by a weights matrix @xmath21 with the same dimensions as @xmath7 , and the problem can be factorized into independent solutions of small matrices .",
    "this algorithm produces a set of orthogonal principal component eigenvectors @xmath13 , which are optimized to describe the most signal variance with the fewest vectors while properly accounting for estimated measurement noise .",
    "we have described a method for performing pca on noisy data that properly incorporates measurement noise estimates when solving for the eigenvectors and coefficients .",
    "missing data is simply the limiting case of weight=0 .",
    "the method uses an iterative solution based upon expectation maximization .",
    "the resulting eigenvectors are less sensitive to measurement noise and more sensitive to true underlying signal variations .",
    "the algorithm has been demonstrated on toy data and qso spectra from sdss .",
    "code which implements this algorithm is available at https://github.com/sbailey/empca  .",
    "the author would like to thank rollin thomas and sbastien bongard for interesting and helpful conversations related to this work .",
    "the anonymous reviewer provided helpful comments and suggestions which improved this manuscript .",
    "the initial algorithm was developed during a workshop at the institut de fragny .",
    "this work was supported under the auspices of the office of science , u.s .",
    "doe , under contract no .",
    "de - ac02 - 05ch1123 .",
    "the example qso spectra were provided by the sloan digital sky survey .",
    "funding for the sdss and sdss - ii has been provided by the alfred p. sloan foundation , the participating institutions , the national science foundation , the u.s .",
    "department of energy , the national aeronautics and space administration , the japanese monbukagakusho , the max planck society , and the higher education funding council for england .",
    "the sdss web site is http://www.sdss.org/.    the sdss is managed by the astrophysical research consortium for the participating institutions .",
    "the participating institutions are the american museum of natural history , astrophysical institute potsdam , university of basel , university of cambridge , case western reserve university , university of chicago , drexel university , fermilab , the institute for advanced study , the japan participation group , johns hopkins university , the joint institute for nuclear astrophysics , the kavli institute for particle astrophysics and cosmology , the korean scientist group , the chinese academy of sciences ( lamost ) , los alamos national laboratory , the max - planck - institute for astronomy ( mpia ) , the max - planck - institute for astrophysics ( mpa ) , new mexico state university , ohio state university , university of pittsburgh , university of portsmouth , princeton university , the united states naval observatory , and the university of washington .",
    "turnshek , d.  a.  1988 , qso absorption lines : probing the universe .",
    "proceedings of the qso absorption line meeting held may 19 - 21 , 1987 , in baltimore , md usa . edited by j. chris blades , david a. turnshek , and colin a. norman .",
    "isbn 0 - 521 - 34561 8 ; cambridge university press , cambridge , england , 1988 , p.17"
  ],
  "abstract_text": [
    "<S> we present a method for performing principal component analysis ( pca ) on noisy datasets with missing values . </S>",
    "<S> estimates of the measurement error are used to weight the input data such that compared to classic pca , the resulting eigenvectors are more sensitive to the true underlying signal variations rather than being pulled by heteroskedastic measurement noise . </S>",
    "<S> missing data is simply the limiting case of weight=0 . </S>",
    "<S> the underlying algorithm is a noise weighted expectation maximization ( em ) pca , which has additional benefits of implementation speed and flexibility for smoothing eigenvectors to reduce the noise contribution . </S>",
    "<S> we present applications of this method on simulated data and qso spectra from the sloan digital sky survey . </S>"
  ]
}