{
  "article_text": [
    "consider the following functional linear regression model where the response @xmath0 is related to a square integrable random function @xmath1 through @xmath2 here @xmath3 is the intercept , @xmath4 is the domain of @xmath5 , @xmath6 is an unknown slope function and @xmath7 is a centered noise random variable .",
    "the domain @xmath4 is assumed to be a compact subset of an euclidean space .",
    "our goal is to estimate @xmath3 and @xmath6 as well as to retrieve @xmath8 based on a set of training data @xmath9 consisting of @xmath10 independent copies of @xmath11 .",
    "we shall assume that the slope function @xmath12 resides in a reproducing kernel hilbert space ( rkhs ) @xmath13 , a subspace of the collection of square integrable functions on @xmath4 .    in this paper",
    ", we investigate the method of regularization for estimating @xmath14 , as well as @xmath3 and @xmath12 .",
    "let @xmath15 be a data fit functional that measures how well @xmath16 fits the data and @xmath17 be a penalty functional that assesses the `` plausibility '' of @xmath16 .",
    "the method of regularization estimates @xmath14 by @xmath18,\\ ] ] where the minimization is taken over @xmath19 and @xmath20 is a tuning parameter that balances the fidelity to the data and the plausibility .",
    "equivalently , the minimization can be taken over @xmath21 instead of @xmath16 to obtain estimates for both the intercept and slope , denoted by @xmath22 and @xmath23 hereafter .",
    "the most common choice of the data fit functional is the squared error @xmath24 ^ 2.\\ ] ] in general , @xmath15 is chosen such that it is convex in @xmath16 and @xmath25 in uniquely minimized by @xmath14 .    in the context of functional linear regression , the penalty functional can be conveniently defined through the slope function @xmath26 as a squared norm or semi - norm associated with @xmath13 .",
    "the canonical example of @xmath13 is the sobolev spaces . without loss of generality , assume that @xmath27 $ ] , the sobolev space of order @xmath28 is then defined as @xmath29)&= & \\bigl\\{\\beta\\dvtx[0,1]\\to\\mathbb{r}| \\beta , \\beta^{(1)},\\ldots,\\beta^{(m-1 ) } \\mbox { are absolutely } \\\\ & & \\hspace*{104pt}{}\\mbox{continuous and } \\beta^{(m)}\\in\\mathcal{l}_2 \\bigr\\}.\\end{aligned}\\ ] ] there are many possible norms that can be equipped with @xmath30 to make it a reproducing kernel hilbert space .",
    "for example , it can be endowed with the norm @xmath31 the readers are referred to adams ( @xcite ) for a thorough treatment of this subject . in this case , a possible choice of the penalty functional is given  by=1 @xmath32 ^ 2\\,dt.\\]]=0 another setting of particular interest is @xmath27 ^ 2 $ ] which naturally occurs when @xmath33 represents an image .",
    "a popular choice in this setting is the thin plate spline where @xmath17 is given by @xmath34\\ , dx_1\\,dx_2,\\ ] ] and @xmath35 are the arguments of bivariate function @xmath26 .",
    "other examples of @xmath4 include @xmath36 for some positive integer @xmath37 , and unit sphere in an euclidean space among others .",
    "the readers are referred to wahba ( @xcite ) for common choices of @xmath13 and @xmath17 in these as well as other contexts .",
    "other than the methods of regularization , a number of alternative estimators have been introduced in recent years for the functional linear regression [ james ( @xcite ) ; cardot , ferraty and sarda ( @xcite ) ; ramsay and silverman ( @xcite ) ; yao , mller and wang ( @xcite ) ; ferraty and vieu ( @xcite ) ; cai and hall ( @xcite ) ; li and hsing ( @xcite ) ; hall and horowitz ( @xcite ) ; crambes , kneip and sarda ( @xcite ) ; johannes ( @xcite ) ] .",
    "most of the existing methods are based upon the functional principal component analysis ( fpca ) .",
    "the success of these approaches hinges on the availability of a good estimate of the functional principal components for @xmath5 .",
    "in contrast , the aforementioned smoothness regularized estimator avoids this task and therefore circumvents assumptions on the spacing of the eigenvalues of the covariance operator for @xmath5 as well as fourier coefficients of @xmath12 with respect to the eigenfunctions , which are required by the fpca - based approaches .",
    "furthermore , as we shall see in the subsequent theoretical analysis , because the regularized estimator does not rely on estimating the functional principle components , stronger results on the convergence rates can be obtained .    despite the generality of the method of regularization",
    ", we show that the estimators can be computed rather efficiently .",
    "we first derive a representer theorem in section  [ representer.sec ] which demonstrates that although the minimization with respect to @xmath16 in ( [ eq : mor ] ) is taken over an infinite - dimensional space , the solution can actually be found in a finite - dimensional subspace .",
    "this result makes our procedure easily implementable and enables us to take advantage of the existing techniques and algorithms for smoothing splines to compute @xmath38 , @xmath23 and @xmath39 .",
    "we then consider in section  [ diagonal.sec ] the relationship between the eigen structures of the covariance operator for @xmath5 and the reproducing kernel of the rkhs @xmath13 .",
    "these eigen structures play prominent roles in determining the difficulty of the prediction and estimation problems in functional linear regression .",
    "we prove in section  [ diagonal.sec ] a result on simultaneous diagonalization of the reproducing kernel of the rkhs @xmath13 and the covariance operator of @xmath5 which provides a powerful machinery for studying the minimax rates of convergence .",
    "section  [ rate.sec ] investigates the rates of convergence of the smoothness regularized estimators .",
    "both the minimax upper and lower bounds are established .",
    "the optimal convergence rates are derived in terms of a class of intermediate norms which provide a wide range of measures for the estimation accuracy . in particular",
    ", this approach gives a unified treatment for both the prediction of @xmath40 and the estimation of @xmath12 .",
    "the results show that the smoothness regularized estimators achieve the optimal rate of convergence for both prediction and estimation under conditions weaker than those for the functional principal components based methods developed in the literature .",
    "the representer theorem makes the regularized estimators easy to implement .",
    "several efficient algorithms are available in the literature that can be used for the numerical implementation of our procedure .",
    "section  [ numerical.sec ] presents numerical studies to illustrate the merits of the method as well as demonstrate the theoretical developments .",
    "all proofs are relegated to section  [ proof.sec ] .",
    "the smoothness regularized estimators @xmath38 and @xmath41 are defined as the solution to a minimization problem over an infinite - dimensional space . before studying the properties of the estimators , we first show that the minimization is indeed well defined and easily computable thanks to a version of the so - called representer theorem .",
    "let the penalty functional @xmath17 be a squared semi - norm on @xmath13 such that the null space @xmath42 is a finite - dimensional linear subspace of @xmath13 with orthonormal basis @xmath43 where @xmath44 .",
    "denote by @xmath45 its orthogonal complement in @xmath13 such that @xmath46 .",
    "similarly , for any function @xmath47 , there exists a unique decomposition @xmath48 such that @xmath49 and @xmath50 .",
    "note @xmath51 forms a reproducing kernel hilbert space with the inner product of @xmath13 restricted to @xmath45 .",
    "let @xmath52 be the corresponding reproducing kernel of @xmath45 such that @xmath53 for any @xmath50 .",
    "hereafter we use the subscript @xmath54 to emphasize the correspondence between the inner product and its reproducing kernel .    in what follows",
    ", we shall assume that @xmath54 is continuous and square integrable .",
    "note that @xmath54 is also a nonnegative definite operator on @xmath55 . with slight abuse of notation ,",
    "write @xmath56 it is known [ see , e.g. , cucker and smale ( @xcite ) ] that @xmath57 for any @xmath58 .",
    "furthermore , for any @xmath59 @xmath60 this observation allows us to prove the following result which is important to both numerical implementation of the procedure and our theoretical analysis .",
    "[ th : rep ] assume that @xmath15 depends on @xmath16 only through @xmath61 ; then there exist @xmath62 and @xmath63 such that @xmath64    theorem  [ th : rep ] is a generalization of the well - known representer lemma for smoothing splines ( wahba , @xcite ) .",
    "it demonstrates that although the minimization with respect to @xmath16 is taken over an infinite - dimensional space , the solution can actually be found in a finite - dimensional subspace , and it suffices to evaluate the coefficients @xmath65 and @xmath66 in ( [ eq : rep ] ) .",
    "its proof follows a similar argument as that of theorem 1.3.1 in wahba ( @xcite ) where @xmath15 is assumed to be squared error , and is therefore omitted here for brevity .",
    "consider , for example , the squared error loss .",
    "the regularized estimator is given by @xmath67 ^ 2+\\lambda j(\\beta ) \\biggr \\}.\\hspace*{-30pt}\\ ] ] it is not hard to see that @xmath68 where @xmath69 and @xmath70 are the sample average of @xmath71 and @xmath72 , respectively .",
    "consequently , ( [ eq : pls0 ] ) yields @xmath73 ^ 2+\\lambda j(\\beta ) \\biggr\\}.\\ ] ] for the purpose of illustration , assume that @xmath74 and @xmath75 . then @xmath76 is the linear space spanned by @xmath77 and @xmath78 . a popular reproducing kernel associated with @xmath45 is @xmath79 where @xmath80 is the @xmath28th bernoulli polynomial .",
    "the readers are referred to wahba ( @xcite ) for further details . following theorem  [ th : rep ]",
    ", it suffices to consider @xmath26 of the following form : @xmath81k(t , s)\\,ds \\label{beta.w2}\\ ] ] for some @xmath82 and @xmath83 .",
    "correspondingly , @xmath84\\beta(t)\\,dt \\\\ & & \\qquad = d_1 \\int_\\mathcal{t}[x(t)-\\bar{x}(t)]\\,dt + d_2\\int _ \\mathcal{t }",
    "[ x(t)-\\bar{x}(t)]t\\,dt \\\\ & & \\qquad\\quad{}+\\sum_{i=1}^n c_i \\int_\\mathcal{t}\\int_{\\mathcal{t } } [ x_i(s)-\\bar { x}(s)]k(t , s)[x(t)-\\bar{x}(t)]\\,ds\\,dt.\\end{aligned}\\ ] ] note also that for @xmath26 given in ( [ beta.w2 ] ) @xmath85 where @xmath86 is a @xmath87 matrix with @xmath88k(t , s)[x_j(t)-\\bar { x}(t)]\\,ds\\,dt.\\ ] ] denote by @xmath89 an @xmath90 matrix whose @xmath91 entry is @xmath92t^{j-1}\\,dt\\ ] ] for @xmath93 .",
    ". then @xmath95 which is quadratic in @xmath65 and @xmath66 , and the explicit form of the solution can be easily obtained for such a problem .",
    "this computational problem is similar to that behind the smoothing splines .",
    "write @xmath96 ; then the minimizer of ( [ eq : lsvec ] ) is given by @xmath97\\mathbf{y}.\\end{aligned}\\ ] ]",
    "before studying the asymptotic properties of the regularized estimators @xmath38 and @xmath41 , we first investigate the relationship between the eigen structures of the covariance operator for @xmath5 and the reproducing kernel of the functional space @xmath13 . as observed in earlier studies [ e.g. , cai and hall ( @xcite ) ; hall and horowitz ( @xcite ) ] , eigen structures play prominent roles in determining the nature of the estimation problem in functional linear regression .",
    "recall that @xmath54 is the reproducing kernel of @xmath45 .",
    "because @xmath54 is continuous and square integrable , it follows from mercer s theorem [ riesz and sz - nagy ( @xcite ) ] that @xmath54 admits the following spectral decomposition : @xmath98 here @xmath99 are the eigenvalues of @xmath54 , and @xmath100 are the corresponding eigenfunctions , that is , @xmath101 moreover , @xmath102 where @xmath103 is the kronecker s delta .",
    "consider , for example , the univariate sobolev space @xmath104)$ ] with norm ( [ eq : sobnorm ] ) and penalty ( [ eq : sobpen ] ) .",
    "observe that @xmath105 it is known that [ see , e.g. , wahba ( @xcite ) ] @xmath106 recall that @xmath107 is the @xmath28th bernoulli polynomial .",
    "it is known [ see , e.g. , micchelli and wahba ( @xcite ) ] that in this case , @xmath108 , where for two positive sequences @xmath109 and",
    "@xmath110 , @xmath111 means that @xmath112 is bounded away from @xmath113 and @xmath114 as @xmath115 .",
    "denote by @xmath116 the covariance operator for @xmath33 , that is , @xmath117[x(t)-e(x(t ) ) ] \\}.\\ ] ] there is a duality between reproducing kernel hilbert spaces and covariance operators [ stein ( @xcite ) ] .",
    "similarly to the reproducing kernel @xmath54 , assuming that the covariance operator @xmath116 is continuous and square integrable , we also have the following spectral decomposition @xmath118 where @xmath119 are the eigenvalues and @xmath120 are the eigenfunctions such that @xmath121    the decay rate of the eigenvalues @xmath122 can be determined by the smoothness of the covariance operator @xmath116 .",
    "more specifically , when @xmath116 satisfies the so - called sacks  ylvisaker conditions of order @xmath123 where @xmath123 is a nonnegative integer [ sacks and ylvisaker ( @xcite ) ] , then @xmath124 . the readers are referred to the original papers by sacks and ylvisaker or a more recent paper by ritter , wasilkowski and woniakwski ( @xcite ) for detailed discussions of the sacks  ylvisaker conditions .",
    "the conditions are also stated in the for .",
    "roughly speaking , a covariance operator @xmath116 is said to satisfy the sacks  ylvisaker conditions of order @xmath113 if it is twice differentiable when @xmath125 but not differentiable when @xmath126 . a  covariance operator @xmath116 satisfies the sacks  ylvisaker conditions of order @xmath127 for an integer @xmath128 if @xmath129 satisfies the sacks  ylvisaker conditions of order @xmath113 .",
    "in this paper , we say a covariance operator @xmath116 satisfies the sacks  ylvisaker conditions if @xmath116 satisfies the sacks  ylvisaker conditions of order @xmath127 for some @xmath130 .",
    "various examples of covariance functions are known to satisfy sacks  ylvisaker conditions .",
    "for example , the ornstein  uhlenbeck covariance function @xmath131 satisfies the sacks  ylvisaker conditions of order @xmath113 .",
    "ritter , wasilkowski and wo ' zniakowski ( @xcite ) recently showed that covariance functions satisfying the sacks  ylvisaker conditions are also intimately related to sobolev spaces , a fact that is useful for the purpose of simultaneously diagonalizing @xmath54 and @xmath116 as we shall see later .",
    "note that the two sets of eigenfunctions @xmath100 and @xmath132 may differ from each other .",
    "the two kernels @xmath54 and @xmath116 can , however , be simultaneously diagonalized . to avoid ambiguity",
    ", we shall assume in what follows that @xmath133 for any @xmath134 and @xmath135 .",
    "when using the squared error loss , this is also a necessary condition to ensure that @xmath25 is uniquely minimized even if @xmath26 is known to come from the finite - dimensional space @xmath76 . under this assumption",
    ", we can define a norm @xmath136 in @xmath13 by @xmath137 note that @xmath138 is a norm because @xmath139 defined above is a quadratic form and is zero if and only if @xmath140 .",
    "the following proposition shows that when this condition holds , @xmath138 is well defined on @xmath13 and equivalent to its original norm , @xmath141 , in that there exist constants @xmath142 such that @xmath143 for all @xmath47 .",
    "in particular , @xmath144 if and only if @xmath145 .",
    "[ prop : rnorm ] if @xmath133 for any @xmath146 and @xmath135 , then @xmath147 and @xmath141 are equivalent .    let @xmath148 be the reproducing kernel associated with @xmath138 . recall that @xmath148 can also be viewed as a positive operator .",
    "denote by @xmath149 the eigenvalues and eigenfunctions of @xmath148 .",
    "then @xmath148 is a linear map from @xmath55 to @xmath55 such that @xmath150 the square root of the positive definite operator can therefore be given as the linear map from @xmath55 to @xmath55 such that @xmath151 let @xmath152 be the eigenvalues of the bounded linear operator @xmath153 and @xmath154 be the corresponding orthogonal eigenfunctions in @xmath55 .",
    "write @xmath155 , @xmath156 also let @xmath157 be the inner product associated with @xmath138 , that is , for any @xmath158 , @xmath159 it is not hard to see that @xmath160 and @xmath161 the following theorem shows that quadratic forms @xmath162 and @xmath163 can be simultaneously diagonalized on the basis of @xmath164 .",
    "[ th : simdiag ] for any @xmath47 , @xmath165 in the absolute sense where @xmath166 . furthermore , if @xmath167 , then @xmath168 consequently , @xmath169    note that @xmath170 can be determined jointly by @xmath171 and @xmath172 .",
    "however , in general , neither @xmath173 nor @xmath174 can be given in explicit form of @xmath171 and @xmath172 .",
    "one notable exception is the case when the operators @xmath116 and @xmath54 are commutable .",
    "in particular , the setting @xmath175 , @xmath176 is commonly adopted when studying fpca - based approaches [ see , e.g. , cai and hall ( @xcite ) ; hall and horowitz ( @xcite ) ] .",
    "[ prop : simdiag ] assume that @xmath175 , @xmath176 then @xmath177 and @xmath178 .",
    "in general , when @xmath179 and @xmath180 differ , such a relationship no longer holds .",
    "the following theorem reveals that similar asymptotic behavior of @xmath173 can still be expected in many practical settings .",
    "[ th : eigen ] consider the one - dimensional case when @xmath27 $ ] . if @xmath13 is the sobolev space @xmath181)$ ] endowed with norm ( [ eq : sobnorm ] ) , and @xmath116 satisfies the sacks  ylvisaker conditions , then @xmath182 .",
    "theorem  [ th : eigen ] shows that under fairly general conditions @xmath183 . in this case",
    ", there is little difference between the general situation and the special case when @xmath54 and @xmath116 share a common set of eigenfunctions when working with the system @xmath184 .",
    "this observation is crucial for our theoretical development in the next section .",
    "we now turn to the asymptotic properties of the smoothness regularized estimators . to fix ideas , in what follows , we shall focus on the squared error loss . recall that in this case @xmath185 ^",
    "2+\\lambda j(\\beta ) \\biggr\\}.\\ ] ] as shown before , the slope function can be equivalently defined as @xmath186 ^ 2+\\lambda j(\\beta ) \\biggr\\},\\ ] ] and once @xmath23 is computed , @xmath187 is given by @xmath188 in light of this fact , we shall focus our attention on @xmath189 in the following discussion for brevity .",
    "we shall also assume that the eigenvalues of the reproducing kernel @xmath54 satisfies @xmath190 for some @xmath191 .",
    "let @xmath192 be the collection of the distributions @xmath193 of the process @xmath33 that satisfy the following conditions :    a.   the eigenvalues @xmath194 of its covariance operator @xmath195 satisfy @xmath196 for some @xmath197 .",
    "b.   for any function @xmath198 , @xmath199\\,dt \\biggr ) ^4\\nonumber \\\\[-8pt]\\\\[-8pt ] & & \\qquad\\le m \\biggl[e \\biggl(\\int_\\mathcal{t}f(t)[x(t)-e(x)(t)]\\,dt \\biggr)^2 \\biggr]^2.\\nonumber\\end{aligned}\\ ] ] c.   when simultaneously diagonalizing @xmath54 and @xmath116 , @xmath200 , where @xmath201 is the @xmath202th largest eigenvalue of @xmath153 where @xmath148 is the reproducing kernel associated with @xmath138 defined by ( [ eq : rrk ] ) .",
    "the first condition specifies the smoothness of the sample path of @xmath5 .",
    "the second condition concerns the fourth moment of a linear functional of @xmath5 .",
    "this condition is satisfied with @xmath203 for a gaussian process because @xmath204 is normally distributed . in the light of theorem",
    "[ th : eigen ] , the last condition is satisfied by any covariance function that satisfies the sacks  ylvisaker conditions if @xmath13 is taken to be @xmath205 with norm ( [ eq : sobnorm ] ) .",
    "it is also trivially satisfied if the eigenfunctions of the covariance operator @xmath116 coincide with those of @xmath54 .",
    "we are now ready to state our main results on the optimal rates of convergence , which are given in terms of a class of intermediate norms between @xmath206 and @xmath207 which enables a unified treatment of both the prediction and estimation problems . for @xmath208",
    "define the norm @xmath209 by @xmath210 where @xmath211 as shown in theorem  [ th : simdiag ] .",
    "clearly @xmath212 reduces to @xmath213 whereas @xmath214 .",
    "the convergence rate results given below are valid for all @xmath215 .",
    "they cover a range of interesting cases including the prediction error and estimation error .",
    "the following result gives the optimal rate of convergence for the regularized estimator @xmath23 with an appropriately chosen tuning parameter @xmath216 under the loss @xmath209 .",
    "[ th : main ] assume that @xmath217 and @xmath218 .",
    "suppose the eigenvalues @xmath219 of the reproducing kernel @xmath54 of the rkhs @xmath13 satisfy @xmath220 for some @xmath221 .",
    "then the regularized estimator @xmath41 with @xmath222 satisfies @xmath223\\\\[-8pt ] & & \\qquad=0.\\nonumber\\end{aligned}\\ ] ]    note that the rate of the optimal choice of @xmath216 does not depend on @xmath224 .",
    "theorem  [ th : main ] shows that the optimal rate of convergence for the regularized estimator @xmath23 is @xmath225 .",
    "the following lower bound result demonstrates that this rate of convergence is indeed optimal among all estimators , and consequently the upper bound in equation ( [ ubd.eq ] ) can not be improved .",
    "denote by @xmath226 the collection of all measurable functions of the observations @xmath227 .",
    "[ th : main1 ] under the assumptions of theorem  [ th : main ] , there exists a constant @xmath228 such that @xmath229 consequently , the regularized estimator @xmath23 with @xmath230 is rate optimal .",
    "the results , given in terms of @xmath209 , provide a wide range of measures of the quality of an estimate for @xmath12 .",
    "observe that @xmath231 where @xmath232 is an independent copy of @xmath33 , and the expectation on the right - hand side is taken over @xmath232 .",
    "the right - hand side is often referred to as the prediction error in regression .",
    "it measures the mean squared prediction error for a random future observation on @xmath33 . from theorems",
    "[ th : main ] and  [ th : main1 ] , we have the following corollary .",
    "[ co : me ] under the assumptions of theorem  [ th : main ] , the mean squared optimal prediction error of a slope function estimator over @xmath233 and @xmath234 is of the order @xmath235 and it can be achieved by the regularized estimator @xmath189 with @xmath216 satisfying ( [ eq : lamopt ] ) .",
    "the result shows that the faster the eigenvalues of the covariance operator @xmath116 for @xmath5 decay , the smaller the prediction error .    when @xmath175 , the prediction error of a slope function estimator @xmath236 can also be understood as the squared prediction error for a fixed predictor @xmath237 such that @xmath238 following the discussed from the last section .",
    "a similar prediction problem has also been considered by cai and hall ( @xcite ) for fpca - based approaches . in particular , they established a similar minimax lower bound and showed that the lower bound can be achieved by the fpca - based approach , but with additional assumptions that @xmath239 , and @xmath240 .",
    "our results here indicate that both restrictions are unnecessary for establishing the minimax rate for the prediction error .",
    "moreover , in contrast to the fpca - based approach , the regularized estimator @xmath23 can achieve the optimal rate without the extra requirements .    to illustrate the generality of our results , we consider an example where @xmath27 $ ] , @xmath241)$ ] and the stochastic process @xmath5 is a wiener process .",
    "it is not hard to see that the covariance operator of @xmath33 , @xmath242 , satisfies the sacks  ylvisaker conditions of order @xmath113 and therefore @xmath243 . by corollary  [ co : me ] ,",
    "the minimax rate of the prediction error in estimating @xmath12 is @xmath244 .",
    "note that the condition @xmath240 required by cai and hall ( @xcite ) does not hold here for @xmath245 .",
    "it is of interest to further look into the case when the operators @xmath116 and @xmath54 share a common set of eigenfunctions .",
    "as discussed in the last section , we have in this case @xmath246 and @xmath247 for all @xmath248 . in this context , theorems  [ th : main ] and  [ th : main1 ] provide bounds for more general prediction problems . consider estimating @xmath249 where @xmath250 satisfies @xmath251 .",
    "note that @xmath252 is needed to ensure that @xmath253 is square integrable .",
    "the squared prediction error @xmath254 is therefore equivalent to @xmath255 .",
    "the following result is a direct consequence of theorems  [ th : main ] and [ th : main1 ] .",
    "suppose @xmath250 is a function satisfying @xmath256 for some @xmath257 .",
    "then under the assumptions of theorem  [ th : main ] , @xmath258\\\\[-9pt ] & & \\hspace*{195pt}>dn^{-\\afrac{2(r+q)}{2(r+s)+1 } } \\biggr\\}>0\\nonumber\\end{aligned}\\ ] ] for some constant @xmath228 , and the regularized estimator @xmath189 with @xmath216 satisfying ( [ eq : lamopt ] ) achieves the optimal rate of convergence under the prediction error ( [ eq : prederr ] ) .",
    "it is also evident that when @xmath175 , @xmath259 is equivalent to @xmath260",
    ". therefore , theorems  [ th : main ] and  [ th : main1 ] imply the following result .    if @xmath246 for all @xmath248 , then under the assumptions of theorem  [ th : main ] @xmath261 for some constant @xmath228 , and the regularized estimate @xmath189 with @xmath216 satisfying ( [ eq : lamopt ] ) achieves the optimal rate .",
    "this result demonstrates that the faster the eigenvalues of the covariance operator for @xmath5 decay , the larger the estimation error .",
    "the behavior of the estimation error thus differs significantly from that of prediction error .",
    "similar results on the lower bound have recently been obtained by hall and horowitz ( @xcite ) who considered estimating @xmath12 under the assumption that @xmath262 decays in a polynomial order .",
    "note that this slightly differs from our setting where @xmath234 means that @xmath263 recall that @xmath190 .",
    "condition ( [ eq : hilcond ] ) is comparable to , and slightly stronger than , @xmath264 . when further assuming that @xmath265 , and @xmath266 for all @xmath248 , hall and horowitz ( @xcite ) obtain the same lower bound as ours .",
    "however , we do not require that @xmath265 which in essence states that @xmath12 is smoother than the sample path of @xmath33 .",
    "perhaps , more importantly , we do not require the spacing condition @xmath266 on the eigenvalues because we do not need to estimate the corresponding eigenfunctions .",
    "such a condition is impossible to verify even for a standard rkhs .",
    "theorems  [ th : main ] and  [ th : main1 ] can also be used for estimating the derivatives of @xmath12 .",
    "a natural estimator of the @xmath267th derivative of @xmath12 , @xmath268 , is @xmath269 , the @xmath267th derivative of @xmath23 .",
    "in addition to @xmath246 , assume that @xmath270 .",
    "this clearly holds when @xmath271 . in this case @xmath272 the following is then a direct consequence of theorems  [ th : main ] and  [ th : main1 ] .",
    "assume that @xmath246 and @xmath273 for all @xmath248 .",
    "then under the assumptions of theorem  [ th : main ] , for some constant @xmath228 , @xmath274\\\\[-8pt ] & & \\qquad>0,\\nonumber\\end{aligned}\\ ] ] and the regularized estimate @xmath23 with @xmath216 satisfying ( [ eq : lamopt ] ) achieves the optimal rate .",
    "finally , we note that although we have focused on the squared error loss here , the method of regularization can be easily extended to handle other goodness of fit measures as well as the generalized functional linear regression [ cardot and sarda ( @xcite ) and mller and stadtmller ( @xcite ) ] .",
    "we shall leave these extensions for future studies .",
    "the representer theorem given in section  [ representer.sec ] makes the regularized estimators easy to implement .",
    "similarly to smoothness regularized estimators in other contexts [ see , e.g. , wahba ( @xcite ) ] , @xmath38 and @xmath23 can be expressed as a linear combination of a finite number of known basis functions although the minimization in ( [ eq : mor ] ) is taken over an infinitely - dimensional space .",
    "existing algorithms for smoothing splines can thus be used to compute our regularized estimators @xmath38 , @xmath275 and @xmath39 .    to demonstrate the merits of the proposed estimators in finite sample settings",
    ", we carried out a set of simulation studies .",
    "we adopt the simulation setting of hall and horowitz ( @xcite ) where @xmath27 $ ] . the true slope function @xmath12 is given by @xmath276 where @xmath277 and @xmath278 for @xmath279 .",
    "the random function @xmath33 was generated as @xmath280 where @xmath281 are independently sampled from the uniform distribution on@xmath282 $ ] and @xmath283 are deterministic .",
    "it is not hard to see that @xmath284 are the eigenvalues of the covariance function of @xmath33 . following hall and horowitz ( @xcite ) , two sets of @xmath283 were used . in the first set",
    ", the eigenvalues are well spaced : @xmath285 with @xmath286 or @xmath287 . in the second",
    "set , @xmath288,&\\quad$k\\ge5$. } % \\ ] ]    as in hall and horowitz ( @xcite ) , regression models with @xmath289 where @xmath290 and @xmath291 were considered . to comprehend the effect of sample size , we consider @xmath292 and @xmath293 .",
    "we apply the regularization method to each simulated dataset and examine its estimation accuracy as measured by integrated squared error @xmath294 and prediction error @xmath295 . for the purpose of illustration , we take @xmath296 and @xmath75 , for which the detailed estimation procedure is given in section  [ representer.sec ] . for each setting , the experiment was repeated 1000 times .    as is common in most smoothing methods , the choice of the tuning parameter plays an important role in the performance of the regularized estimators .",
    "data - driven choice of the tuning parameter is a difficult problem . here",
    "we apply the commonly used practical strategy of empirically choosing the value of @xmath216 through the generalized cross validation .",
    "note that the regularized estimator is a linear estimator in that @xmath297 where @xmath298 and @xmath299 is the so - called hat matrix depending on @xmath216 .",
    "we then select the tuning parameter @xmath216 that minimizes @xmath300 denote by @xmath301 the resulting choice of the tuning parameter .    ):",
    "@xmath33 was simulated with a covariance function with well - spaced eigenvalues .",
    "the results are averaged over 1000 runs .",
    "black solid lines , red dashed lines , green dotted lines and blue dash - dotted lines correspond to @xmath302 and @xmath287 , respectively .",
    "both axes are in log scale . ]",
    "we begin with the setting of well - spaced eigenvalues .",
    "the left panel of figure  [ fig : sim - well - pred - small ] shows the prediction error , @xmath303 , for each combination of @xmath304 value and sample size when @xmath290 .",
    "the results were averaged over 1000 simulation runs in each setting .",
    "both axes are given in the log scale .",
    "the plot suggests that the estimation error converges at a polynomial rate as sample size @xmath10 increases , which agrees with our theoretical results from the previous section .",
    "furthermore , one can observe that with the same sample size , the prediction error tends to be smaller for larger @xmath304 .",
    "this also confirms our theoretical development which indicates that the faster the eigenvalues of the covariance operator for @xmath5 decay , the smaller the prediction error.=1    ): @xmath33 was simulated with a covariance function with well - spaced eigenvalues .",
    "the results are averaged over 1000 runs .",
    "black solid lines , red dashed lines , green dotted lines and blue dash - dotted lines correspond to @xmath305 and @xmath287 , respectively .",
    "both axes are in log scale . ]    to better understand the performance of the smoothness regularized estimator and the gcv choice of the tuning parameter , we also recorded the performance of an oracle estimator whose tuning parameter is chosen to minimize the prediction error .",
    "this choice of the tuning parameter ensures the optimal performance of the regularized estimator .",
    "it is , however , noteworthy that this is not a legitimate statistical estimator since it depends on the knowledge of unknown slope function @xmath12 .",
    "the right panel of figure  [ fig : sim - well - pred - small ] shows the prediction error associated with this choice of tuning parameter .",
    "it behaves similarly to the estimate with @xmath216 chosen by gcv .",
    "note that the comparison between the two panels suggest that gcv generally leads to near optimal performance .",
    "we now turn to the estimation error .",
    "figure [ fig : sim - well - est - small ] shows the estimation errors , averaged over 1000 simulation runs , with @xmath216 chosen by gcv or minimizing the estimation error for each combination of sample size and @xmath304 value .",
    "similarly to the prediction error , the plots suggest a polynomial rate of convergence of the estimation error when the sample size increases , and gcv again leads to near - optimal choice of the tuning parameter .    ):",
    "@xmath33 was simulated with a covariance function with well - spaced eigenvalues .",
    "the results are averaged over 1000 runs .",
    "black solid lines , red dashed lines , green dotted lines and blue dash - dotted lines correspond to @xmath305 and @xmath287 , respectively .",
    "both axes are in log scale . ]",
    "a comparison between figures  [ fig : sim - well - pred - small ] and [ fig : sim - well - est - small ] suggests that when @xmath33 is smoother ( larger @xmath304 ) , prediction ( as measured by the prediction error ) is easier , but estimation ( as measured by the estimation error ) tends to be harder , which highlights the difference between prediction and estimation in functional linear regression .",
    "we also note that this observation is in agreement with our theoretical results from the previous section where it is shown that the estimation error decreases at the rate of @xmath306 which decelerates as @xmath123 increases ; whereas the prediction error decreases at the rate of @xmath307 which accelerates as @xmath123 increases .    figure  [ fig : sim - well - large ] reports the prediction and estimation error when tuned with gcv for the large noise ( @xmath308 ) setting .",
    "observations similar to those for the small noise setting can also be made .",
    "furthermore , notice that the prediction errors are much smaller than the estimation error , which confirms our finding from the previous section that prediction is an easier problem in the context of functional linear regression .",
    "the numerical results in the setting with closely spaced eigenvalues are qualitatively similar to those in the setting with well - spaced eigenvalues",
    ". figure  [ fig : close ] summarizes the results obtained for the setting with closely spaced eigenvalues .",
    "was simulated with a covariance function with closely - spaced eigenvalues .",
    "the results are averaged over 1000 runs .",
    "both axes are in log scale .",
    "note that @xmath72-axes are of different scales across panels . ]",
    "we also note that the performance of the regularization estimate with @xmath216 tuned with gcv compares favorably with those from hall and horowitz ( @xcite ) using fpca - based methods even though their results are obtained with optimal rather than data - driven choice of the tuning parameters .",
    "observe that @xmath309 for some constant @xmath310 .",
    "together with the fact that @xmath311 , we conclude that @xmath312    recall that @xmath313 , @xmath314 are the orthonormal basis of @xmath315 . under the assumption of the proposition ,",
    "the matrix @xmath316 is a positive definite matrix .",
    "denote by @xmath317 its eigenvalues .",
    "it is clear that for any @xmath49 @xmath318 note also that for any @xmath50 , @xmath319    for any @xmath47 , we can write @xmath320 where @xmath321 and @xmath50",
    ". then @xmath322 recall that @xmath323 for brevity , assume that @xmath324 without loss of generality . by the cauchy ",
    "schwarz inequality , @xmath325 where we used the fact that @xmath326 in deriving the last inequality .",
    "therefore , @xmath327 together with the facts that @xmath328 and @xmath329 we conclude that @xmath330 the proof is now complete .",
    "first note that @xmath331 applying bounded positive definite operator @xmath332 to both sides leads to @xmath333    recall that @xmath334 .",
    "therefore , @xmath335    similarly , because @xmath336 , @xmath337      recall that for any @xmath146 , @xmath133 if and only if @xmath140 , which implies that @xmath338 .",
    "together with the fact that @xmath339 , we conclude that @xmath340 .",
    "it is not hard to see that for any @xmath158 , @xmath341 in particular , @xmath342 which implies that @xmath343 is also the eigen system of @xmath148 , that is , @xmath344 then @xmath345 therefore , @xmath346 which implies that @xmath347 , @xmath348 and @xmath349 .",
    "consequently , @xmath350      recall that @xmath351 , which implies that @xmath352 . by corollary 2 of ritter , wasilkowski and woniakowski ( @xcite ) , @xmath353 .",
    "it therefore suffices to show @xmath354 .",
    "the key idea of the proof is a result from ritter , wasilkowski and woniakowski ( @xcite ) indicating that the reproducing kernel hilbert space associated with @xmath116 differs from @xmath355)$ ] only by a finite - dimensional linear space of polynomials .",
    "denote by @xmath356 the reproducing kernel for @xmath357)$ ] .",
    "observe that @xmath358 [ e.g. , cucker and smale ( @xcite ) ] .",
    "we begin by quantifying the decay rate of @xmath359 . by sobolev s embedding theorem , @xmath360 .",
    "therefore , @xmath361 is equivalent to @xmath362 .",
    "denote by @xmath363 be the @xmath202th largest eigenvalue of a positive definite operator @xmath364 .",
    "let @xmath365 be the eigenfunctions of @xmath362 , that is , @xmath366 , @xmath156 denote by @xmath367 and @xmath368 the linear space spanned by @xmath369 and @xmath370 , respectively . by the courant  fischer  weyl min ",
    "max principle , @xmath371 for some constant @xmath372 . on the other hand , @xmath373 for some constant @xmath374 . in summary",
    ", we have @xmath375 .",
    "as shown by ritter , wasilkowski and woniakowski [ ( @xcite ) , theorem 1 , page  525 ] , there exist @xmath376 and @xmath377 such that @xmath378 , @xmath376 has at most @xmath379 nonzero eigenvalues and @xmath380 is equivalent to @xmath381 .",
    "moreover , the eigenfunctions of @xmath376 , denoted by @xmath382 ( @xmath383 ) are polynomials of order no greater than @xmath384 .",
    "denote @xmath385 the space spanned by @xmath386",
    ". clearly @xmath387 .",
    "denote @xmath388 the eigenfunctions of @xmath361 .",
    "let @xmath389 and @xmath390 be defined similarly as @xmath367 and @xmath391 . then by the courant  fischer  weyl min ",
    "max principle , @xmath392 for some constant @xmath372 . on the other hand , @xmath393 for some constant @xmath374 .",
    "hence @xmath394 .    because @xmath395 is equivalent to @xmath153 , following a similar argument as before , by the courant  fischer  weyl min  max principle",
    ", we complete the the proof .",
    "we now proceed to prove theorem  [ th : main ] .",
    "the analysis follows a similar spirit as the technique commonly used in the study of the rate of convergence of smoothing splines [ see , e.g. , silverman ( @xcite ) ; cox and osullivan ( @xcite ) ] . for brevity , we shall assume that @xmath396 in the rest of the proof .",
    "in this case , @xmath3 can be estimated by @xmath397 and @xmath12 by @xmath398.\\ ] ] the proof below also applies to the more general setting when @xmath399 but with considerable technical obscurity .",
    "recall that @xmath400 observe that @xmath401 ^ 2\\\\ & = & \\sigma^2+\\int_\\mathcal{t}\\int_\\mathcal{t}[\\beta(s)-\\beta_0(s ) ] c(s , t)[\\beta(t)-\\beta_0(t ) ] \\,ds\\,dt\\\\ & = & \\sigma^2 + \\vert\\beta-\\beta_0 \\vert^2_0.\\end{aligned}\\ ] ] write @xmath402 clearly @xmath403 we refer to the two terms on the right - hand side stochastic error and deterministic error , respectively .",
    "write @xmath404 and @xmath405",
    ". then theorem  [ th : simdiag ] implies that @xmath406 therefore , @xmath407    it can then be computed that for any @xmath408 , @xmath409 & = & \\sum_{k=1}^\\infty(1+\\gamma_k^{-a } ) \\biggl ( { \\lambda\\gamma_k^{-1}\\over 1+\\lambda\\gamma_k^{-1 } } \\biggr)^2a_k^2\\\\[-2pt ] & \\le&\\lambda^2 \\sup_k { ( 1+\\gamma^{-a})\\gamma_k^{-1}\\over ( 1+\\lambda \\gamma_k^{-1 } ) ^2}\\sum_{k=1}^\\infty\\gamma_k^{-1}a_k^2\\\\[-2pt ] & = & \\lambda^2j(\\beta_0 ) \\sup_k { ( 1+\\gamma^{-a})\\gamma_k^{-1}\\over ( 1+\\lambda\\gamma_k^{-1 } ) ^2}.\\end{aligned}\\ ] ] now note that @xmath410 & \\le&\\sup_{x > 0 } { x^{-1}\\over(1+\\lambda x^{-1 } ) ^2}+\\sup_{x > 0 } { x^{-(a+1)}\\over(1+\\lambda x^{-1 } ) ^2}\\\\[-2pt ] & = & { 1\\over\\inf_{x > 0 } ( x^{1/2}+\\lambda x^{-1/2 } ) ^2}+ { 1\\over\\inf _ { x > 0 } ( x^{(a+1)/2}+\\lambda x^{-(1-a)/2 } ) ^2}\\\\[-2pt ] & = & { 1\\over4\\lambda}+c_0\\lambda^{-(a+1)}.\\end{aligned}\\ ] ] hereafter , we use @xmath411 to denote a generic positive constant . in summary , we have    [ le : detererr ] if @xmath216 is bounded from above , then @xmath412      next , we consider the stochastic error @xmath413",
    ". denote @xmath414,\\\\[-2pt ] d\\ell_{\\infty } ( \\beta)f&=&-2e_x \\biggl(\\int_\\mathcal{t}x(t ) [ \\beta_0(t)-\\beta(t)]\\,dt\\int_\\mathcal{t}x(t)f(t)\\,dt \\biggr)\\\\[-2pt ] & = & -2\\int_\\mathcal{t}\\int_\\mathcal{t}[\\beta_0(s)-\\beta(s)]c(s , t)f(t)\\,ds\\,dt,\\\\[-2pt ] d^2\\ell_{n } ( \\beta)fg&=&{2\\over n}\\sum_{i=1}^n \\biggl[\\int_\\mathcal{t}x_i(t)f(t)\\,dt\\int_\\mathcal{t}x_i(t)g(t)\\,dt \\biggr],\\\\[-2pt ] d^2\\ell_{\\infty } ( \\beta)fg&=&2\\int_\\mathcal{t}\\int_\\mathcal{t}f(s)c(s , t)g(t)\\,ds\\,dt.\\end{aligned}\\ ] ] also write @xmath415 and @xmath416 .",
    "denote @xmath417 and @xmath418 it is clear that @xmath419 we now study the two terms on the right - hand side separately . for brevity , we shall abbreviate the subscripts of @xmath420 and @xmath421 in what follows .",
    "we begin with @xmath422 .",
    "hereafter we shall omit the subscript for brevity if no confusion occurs .",
    "[ le : stocherr1 ] for any @xmath208 , @xmath423    notice that @xmath424 . therefore @xmath425 ^ 2 & = & e [ d\\ell_{n}(\\bar{\\beta})f - d\\ell_{\\infty}(\\bar{\\beta})f ] ^2\\\\[-2pt ] & = & { 4\\over n } \\operatorname{var}\\biggl [ \\biggl(y-\\int_\\mathcal{t}x(t)\\bar{\\beta}(t)\\,dt \\biggr)\\int_\\mathcal{t}x(t)f(t)\\,dt \\biggr]\\\\[-2pt ] & \\le&{4\\over n } e \\biggl [ \\biggl(y-\\int_\\mathcal{t}x(t)\\bar{\\beta}(t)\\,dt \\biggr)\\int_\\mathcal{t}x(t)f(t)\\,dt \\biggr]^2\\\\[-2pt ] & = & { 4\\over n } e \\biggl(\\int_\\mathcal{t}x(t ) [ \\beta_0(t)-\\bar{\\beta}(t)]\\,dt\\int_\\mathcal{t}x(t)f(t)\\,dt \\biggr)^2\\\\[-2pt ] & & { } + { 4\\sigma^2\\over n}e \\biggl(\\int_\\mathcal{t}x(t)f(t)\\,dt \\biggr)^2,\\end{aligned}\\ ] ] where we used the fact that @xmath426 is uncorrelated with @xmath33 . to bound the first term , an application of the cauchy ",
    "schwarz inequality yields @xmath427\\,dt\\int_\\mathcal{t}x(t)f(t)\\,dt \\biggr)^2\\\\[-2pt ] & & \\qquad\\le \\biggl\\{e \\biggl(\\int_\\mathcal{t}x(t ) [ \\beta_0(t)-\\bar{\\beta}(t ) ] \\,dt \\biggr)^4e\\biggl(\\int_\\mathcal{t}x(t)f(t)\\,dt \\biggr)^4 \\biggr\\}^{1/2}\\\\[-2pt ] & & \\qquad\\le m \\|\\beta_0-\\bar{\\beta}\\|^2_0\\|f\\|_0 ^ 2,\\end{aligned}\\ ] ] where the second inequality holds by the second condition of @xmath428",
    ". therefore , @xmath429",
    "2\\le{4m\\over n } \\|\\beta _ 0-\\bar{\\beta } \\|^2_0\\|f\\|_0 ^ 2+{4\\sigma^2\\over n}\\|f\\|_0 ^ 2,\\vadjust{\\goodbreak}\\ ] ] which by lemma  [ le : detererr ] is further bounded by @xmath430 for some positive constant @xmath411 .",
    "recall that @xmath431 .",
    "we have @xmath432 ^ 2\\le c_0\\sigma^2/ n.\\ ] ] thus ,",
    "by the definition of @xmath236 , @xmath433\\\\ & \\le&{c_0\\sigma^2\\over4n}\\sum_{k=1}^\\infty(1+\\gamma_k^{-a})(1+\\lambda\\gamma_k^{-1})^{-2}\\\\ & \\le&{c_0\\sigma^2\\over4n } \\sum_{k=1}^\\infty\\bigl(1+k^{2a(r+s)}\\bigr)\\bigl(1+\\lambda k^{2(r+s)}\\bigr)^{-2}\\\\ & \\asymp&{c_0\\sigma^2\\over4n}\\int_1^\\infty x^{2a(r+s)}\\bigl(1+\\lambda x^{2(r+s)}\\bigr)^{-2}\\,dx\\\\ & \\asymp&{c_0\\sigma^2\\over4n}\\int_1^\\infty\\bigl(1+\\lambda x^{2(r+s)/(2a(r+s)+1 ) } \\bigr)^{-2}\\,dx\\\\ & = & { c_0\\sigma^2\\over4n}\\lambda^{- ( a+\\afrac{1}{2(r+s ) } ) } \\int_{\\lambda^{a+\\afrac{1}{2(r+s)}}}^\\infty\\bigl(1+x^{2(r+s)/(2a(r+s)+1 ) } \\bigr)^{-2}\\,dx\\\\ & \\asymp&n^{-1}\\lambda^{- ( a+\\afrac{1}{2(r+s ) } ) } .\\end{aligned}\\ ] ] the proof is now complete .",
    "now we are in position to bound @xmath434 . by definition",
    ", @xmath435 first - order condition implies that @xmath436 where we used the fact that @xmath437 is quadratic .",
    "together with the fact that @xmath438 we have @xmath439 therefore , @xmath440.\\ ] ] write @xmath441 then @xmath442 ^ 2\\\\ & & \\qquad\\le{1\\over4}\\sum_{k=1}^\\infty(1+\\lambda\\gamma_k^{-1})^{-2}(1+\\gamma_k^{-a } ) \\biggl [ \\sum_{j=1}^\\infty(\\hat{b}_j-\\bar{b}_j)^2(1+\\gamma_j^{-c})\\biggr]\\\\ & & \\qquad\\quad\\hspace*{23pt } { } \\times\\biggl(\\sum_{j=1}^\\infty(1+\\gamma_j^{-c})^{-1 } \\biggl[\\int_\\mathcal{t}\\int_\\mathcal{t}\\omega_j(s ) \\biggl({1\\over n}\\sum_{i=1}^nx_i(t)x_i(s)-c(s , t ) \\biggr ) \\\\ & & \\qquad\\quad\\hspace*{236pt}{}\\times\\omega_k(t)\\,ds\\,dt\\biggr]^2 \\biggr),\\end{aligned}\\ ] ] where the inequality is due to the cauchy ",
    "schwarz inequality .",
    "note that @xmath443 ^ 2 \\biggr)\\\\ \\hspace*{-5pt}&&\\qquad={1\\over n}\\sum_{j=1}^\\infty(1+\\gamma_j^{-c})^{-1}\\operatorname{var}\\biggl(\\int_\\mathcal{t}\\omega_j(t)x(t)\\,dt\\,\\int_\\mathcal{t}\\omega_k(t)x(t)\\,dt \\biggr)\\\\ \\hspace*{-5pt}&&\\qquad\\le{1\\over n}\\sum_{j=1}^\\infty(1+\\gamma_j^{-c})^{-1 } e \\biggl [ \\biggl(\\int_\\mathcal{t}\\omega_j(t)x(t)\\,dt \\biggr)^2 \\biggl(\\int_\\mathcal{t}\\omega_k(t)x(t)\\,dt \\biggr)^2 \\biggr]\\\\ \\hspace*{-5pt}&&\\qquad\\le{1\\over n}\\sum_{j=1}^\\infty(1+\\gamma_j^{-c})^{-1 } e \\biggl [ \\biggl(\\int_\\mathcal{t}\\omega_j(t)x(t)\\,dt \\biggr)^4\\biggr]^{1/2 } % & & \\qquad\\quad\\hspace*{20pt}{}\\times e \\biggl [ \\biggl(\\int_\\mathcal{t}\\omega_k(t)x(t)\\,dt \\biggr)^4\\biggr]^{1/2}\\\\ \\hspace*{-5pt}&&\\qquad\\le{m\\over n}\\sum_{j=1}^\\infty(1+\\gamma_j^{-c})^{-1 } e \\biggl [ \\biggl(\\int_\\mathcal{t}\\omega_j(t)x(t)\\,dt \\biggr)^2",
    "\\biggr]e \\biggl [ \\biggl(\\int_\\mathcal{t}\\omega_k(t)x(t)\\,dt \\biggr)^2\\biggr]\\\\ \\hspace*{-5pt}&&\\qquad={m\\over n}\\sum_{j=1}^\\infty(1+\\gamma_j^{-c})^{-1 } \\asymp n^{-1},\\end{aligned}\\ ] ] provided that @xmath444 .",
    "on the other hand , @xmath445    to sum up , @xmath446 in particular , taking @xmath447 yields @xmath448 if @xmath449 then @xmath450 together with the triangular inequality @xmath451 therefore , @xmath452 together with lemma  [ le : stocherr1 ] , we have @xmath453    putting it back to ( [ eq : hat - til ] ) , we now have :    [ le : stocherr2 ] if there also exists some @xmath454 such that @xmath455 , then @xmath456    combining lemmas  [ le : detererr][le : stocherr2 ] , we have @xmath457\\\\[-8pt ] & & \\qquad=0\\nonumber\\end{aligned}\\ ] ] by taking @xmath230 .",
    "we now set out to show that @xmath458 is the optimal rate .",
    "it follows from a similar argument as that of hall and horowitz ( @xcite ) .",
    "consider a setting where @xmath459 , @xmath460 clearly in this case we also have @xmath461 .",
    "it suffices to show that the rate is optimal in this special case .",
    "recall that @xmath462 .",
    "set @xmath463 where @xmath464 is the integer part of @xmath465 , and @xmath466 is either @xmath113 or @xmath291 .",
    "it is clear that @xmath467 therefore @xmath234 .",
    "now let @xmath33 admit the following expansion : @xmath468 where @xmath313s are independent random variables drawn from a uniform distribution on @xmath469 $ ] .",
    "simple algebraic manipulation shows that the distribution of @xmath33 belongs to @xmath470 .",
    "the observed data are @xmath471 where the noise @xmath472 is assumed to be independently sampled from @xmath473 .",
    "as shown in hall and horowitz ( @xcite ) , @xmath474 where @xmath475 denotes the supremum over all @xmath476 choices of @xmath477 , and @xmath478 is taken over all measurable functions @xmath479 of the data .",
    "therefore , for any estimate @xmath236 , @xmath480\\\\[-8pt ] & \\ge & mn^{-\\afrac{2(1-a)(r+s)}{2(r+s)+1}}\\nonumber\\end{aligned}\\ ] ] for some constant @xmath481 .    denote @xmath482 it is easy to see that @xmath483\\\\[-8pt ] & & \\qquad\\ge\\sum_{k = l_n+1}^{2l_n } l_n^{-1}k^{-2(1-a)(r+s)}(\\hspace*{2pt}\\tilde{\\hspace*{-2pt}\\tilde{\\theta}}_j-\\theta_j)^2.\\nonumber\\end{aligned}\\ ] ] hence , we can assume that @xmath484 without loss of generality in establishing the lower bound .",
    "subsequently , @xmath485\\\\[-8pt ] & \\le & l_n^{-2(1-a)(r+s)}.\\nonumber\\end{aligned}\\ ] ] together with ( [ eq : mseldb ] ) , this implies that @xmath486 for some constant @xmath228 .",
    "in section  [ diagonal.sec ] , we discussed the relationship between the smoothness of @xmath116 and the decay of its eigenvalues .",
    "more precisely , the smoothness can be quantified by the so - called sacks  ylvisaker conditions . following ritter ,",
    "wasilkowski and woniakowski ( @xcite ) , denote @xmath487\\\\[-8pt ] \\omega_-&=&\\{(s , t)\\in(0,1)^2\\dvtx   s < t\\}.\\nonumber\\end{aligned}\\ ] ] let @xmath488 be the closure of a set @xmath489 .",
    "suppose that @xmath490 is a continuous function on @xmath491 such that @xmath492 is continuously extendable to @xmath493 for @xmath494 .",
    "by @xmath495 we denote the extension of @xmath490 to @xmath496 ^ 2 $ ] , which is continuous on @xmath493 , and on @xmath496 ^ 2 \\setminus\\operatorname{cl}(\\omega_j)$ ] .",
    "furthermore write @xmath497 .",
    "we say that a covariance function @xmath498 on @xmath496 ^ 2 $ ] satisfies the sacks  ylvisaker conditions of order @xmath127 if the following three conditions hold :    a.   @xmath499 is continuous on @xmath496 ^ 2 $ ] , and its partial derivatives up to order 2 are continuous on @xmath491 , and they are continuously extendable to @xmath500 and @xmath501 .",
    "b.   @xmath502 c.   @xmath503 belongs to the reproducing kernel hilbert space spanned by @xmath490 and furthermore @xmath504                          johannes , j. ( 2009 ) .",
    "nonparametric estimation in functional linear models with second order stationary regressors",
    ". unpublished manuscript .",
    "available at http://arxiv.org/abs/0901.4266v1[http://arxiv.org/ ] http://arxiv.org/abs/0901.4266v1[abs/0901.4266v1 ] ."
  ],
  "abstract_text": [
    "<S> we study in this paper a smoothness regularization method for functional linear regression and provide a unified treatment for both the prediction and estimation problems . by developing a tool on simultaneous diagonalization of two positive definite kernels , </S>",
    "<S> we obtain shaper results on the minimax rates of convergence and show that smoothness regularized estimators achieve the optimal rates of convergence for both prediction and estimation under conditions weaker than those for the functional principal components based methods developed in the literature . despite the generality of the method of regularization , we show that the procedure is easily implementable . </S>",
    "<S> numerical results are obtained to illustrate the merits of the method and to demonstrate the theoretical developments .    and    .    . </S>"
  ]
}