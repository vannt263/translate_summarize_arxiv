{
  "article_text": [
    "the purpose of this paper is to present a novel approach for efficient estimation via the monte carlo ( mc ) method .",
    "the approach is very broadly applicable but here , to present the main ideas , we narrow the focus to ensemble monte carlo where estimation is based on stochastically independent trajectories of a system . to illustrate , we use simulation of time - dependent nonlinear processes",
    "for which monte carlo is a particularly general and powerful numerical method compared to available alternatives .",
    "time - dependent nonlinear processes are very general models used , among others , in statistical mechanics @xcite , data assimilation in climate , weather and ocean modeling @xcite , financial modeling @xcite , and quantitative biology @xcite . hence developing efficient mc methods",
    "may significantly impact a wide range of applications .",
    "a known weakness of mc is its slow rate of convergence .",
    "assume @xmath0 is a random quantity defined on paths of a process and let @xmath1 denote its standard deviation . the convergence rate of mc for estimating the expected value of @xmath0 is @xmath2 where @xmath3 is the number of independent paths of the process .",
    "in general the canonical @xmath4 rate of convergence can not be improved upon , hence , since the inception of the mc method , a number of variance reduction ( vr ) techniques have been devised to reduce @xmath1 ( see , @xcite for an early account and @xcite and @xcite for more recent discussions ) .",
    "most vr techniques lead to estimators of the form @xmath5 i.e. , a weighted average of the samples .",
    "these techniques prescribe ( i ) a recipe for selecting samples @xmath6 and ( ii ) a set of weights @xmath7 .",
    "to arrive at these prescriptions , one must rely on the existence of specific problem features and the ability of the user of the method to discover and effectively exploit such features .",
    "this lack of generality has significantly limited the applicability of vr techniques .",
    "the point of departure of a new strategy , called database monte carlo ( dbmc ) , is to address this shortcoming and to devise generic vr techniques that can be generically applied @xcite .",
    "all vr techniques bring additional information to bear on the estimation problem , however , as mentioned above , this information is problem specific and relies on exploiting special features of the problem at hand .",
    "by contrast , as will be clarified in this paper , dbmc adds a generic computational exploration phase to the estimation problem that relies on gathering information at one ( or more ) nominal model parameter(s ) to achieve estimation efficiency at neighboring parameters .",
    "the advantage of this approach is its generality and wide applicability : it is quite easy to implement and it can wrap existing ensemble mc codes . on the other hand , the computational exploration phase of the dbmc approach may require extensive simulations and can be computationally costly .",
    "therefore , the initial setup cost needs justification .",
    "the setup cost may be justified in projects that involve estimations at many model parameters and/or in projects where there is a real - time computational constraint . in the first type of project",
    ", the setup cost may lead to efficiency gain for each subsequent estimation , and for a large enough number of subsequent estimations it can be easily justified . in projects with a real - time constraint",
    "the setup cost is an off - line `` passive '' cost that can lead to estimates of significantly higher quality ( lower statistical error ) ; the higher quality in many such projects more than justifies the setup cost .    in this paper",
    "we limit ourselves to presenting the implementation of the vr technique of control variates ( cv ) in the dbmc setting ( see @xcite for discussion of other vr techniques ) .",
    "the cv technique , which compared to the vr technique of importance sampling is less utilized in computational physics , requires identifying a number of random variables called _ control variates _ , say @xmath8 , that are correlated with @xmath0 and have _ known _ means .",
    "the correlation with @xmath0 implies that @xmath9 s carry information about @xmath0 .",
    "the cv technique is a way of utilizing the information included in the controls ( their known means ) to help with the estimation of the mean of variable @xmath0 . in the dbmc setting we assume that @xmath10 depends on a model parameter @xmath11 and use @xmath12 where @xmath13 s are in a neighborhood of @xmath11 ( @xmath14 ) . in a departure from the classical cv technique ,",
    "we use `` high quality '' estimates of @xmath15 $ ] rather than precise values of @xmath15 $ ] to arrive at the controlled estimator of @xmath16 $ ] . as we argue in this paper ( and elsewhere @xcite ) this departure",
    "allows for substantially broader choices of control variates and makes the cv technique significantly more flexible and effective .",
    "the dbmc method shares a similar intent as the well - known histogram reweighing method @xcite from the markov chain monte carlo literature ( e.g. @xcite ) , but with a very different setting and implementation , and with broader applicability .",
    "for example , it does not rely on having a boltzmann distribution or @xmath17 structure .",
    "given its generality , it has potential applications , among others , in ensemble weather prediction , hydrological source location , climate and ocean , optimal control , and stochastic simulations of biological systems .",
    "the remainder of the paper is organized as follows . in section [ prel ]",
    "we discuss preliminaries , including the details of the example numerical study ",
    "the time - dependent ginzburg - landau ( tdgl ) equation  as well as the method of control variates .",
    "estimation of mean outcomes of the tdgl equation over a range of temperatures is of interest , especially considering the large difference in behavior below and above the coexistence curve . in section [ methods ]",
    "we describe the dbmc methodology and motivation in a general context .",
    "section [ numerical ] discusses the implementation and results of dbmc as applied to estimation of quantities generated by the tdgl equation , and the results of that numerical study .",
    "we conclude in section [ conclusions ] .",
    "we present aspects of our approach and numerical results in the context of the time - dependent ginzburg landau ( tdgl ) model .",
    "it is worth noting that this model is chosen for illustrative purposes only and we do not make use of any of its specific features .",
    "we use a canonical equation of phase - ordering kinetics @xcite the stochastic tdgl equation in two spatial dimensions .",
    "this is written as @xmath18 where @xmath19 represents a local order parameter , e.g. a magnetization at point @xmath20 and time @xmath21 ( @xmath22 denotes transpose ) .",
    "the noise has mean zero and covariance @xmath23 .",
    "we choose a double - well potential @xmath24 . as in @xcite @xmath25",
    "is a constant , and @xmath11 is a function of temperature such that a high @xmath11 corresponds to a low temperature .",
    "we use a discrete form of ( [ eq : tdgl ] ) using a forward euler - maruyama stochastic integrator and a 5-point stencil for the laplacian ( denoted @xmath26 ) for simulation : @xmath27\\\\   \\nonumber & + & \\sqrt{2 ( \\delta t/\\delta x ) } n({\\bf x},t)\\end{aligned}\\ ] ] with time step @xmath28 and lattice spacing @xmath29 , and where @xmath30 are independent and identically distributed standard normal random variables for each space - time point @xmath31 .",
    "what follows applies to other discretization schemes as well .      to cover a broad range of estimation problems , we consider the estimation of quantities related to a specific space - time point , quantities that are global ( entire lattice at a particular time ) and quantities that depend on the entire time evolution of the system . specifically , we consider the following representative quantities :    1 .   _ point magnetization _ : @xmath32 , 2 .",
    "_ total magnetization at a specific time @xmath21 _ : @xmath33 , and 3 .",
    "_ total space - time magnetization _ : @xmath34",
    ".    the problem of estimating the expected value of any one of the above quantities can be represented by : @xmath35\\ ] ] where @xmath36 is a vector of random numbers representing all the noise / uncertainty in a single complete path @xmath37 of the dynamics ; @xmath11 is the temperature related parameter ; @xmath38 is the random sample of a quantity of interest ( e.g. , the magnetization from a single sample path ) , and @xmath39 $ ] denotes expectation .",
    "note that knowing the noise @xmath36 and parameter @xmath11 completely determines the path @xmath37 and the sample quantity of interest @xmath38 .      here",
    "we give a brief review of the classical control variate ( cv ) technique for variance reduction ( see @xcite @xcite ) .",
    "let @xmath40 , @xmath41 $ ] .",
    "assume @xmath8 are random variables ( called control variates ) that are correlated with @xmath0 and assume their means @xmath15 $ ] are known .",
    "let @xmath42 , @xmath43=(e[x_1 ] , \\cdots , e[x_k])^\\top$ ] , and @xmath44 .",
    "then @xmath45 , defined below , is a controlled estimator of @xmath16 $ ] @xmath46)= y+ \\beta^\\top({\\bf x}-e[{\\bf x}])\\ ] ] the estimator @xmath45 uses information included in samples of the controls ( the degree of their deviation from their known means ) to `` correct / adjust '' the estimator @xmath0 and bring it closer to its unknown mean .",
    "this is the key idea of cv .",
    "( alternatively , @xmath45 can be viewed as the fitted value of @xmath0 when @xmath0 is linearly regressed on variables @xmath47 . in other words",
    ", @xmath45 includes the part of the variation in @xmath0 that can not be `` explained '' by @xmath9 s . )",
    "@xmath45 is an unbiased estimator of @xmath16 $ ] for all vectors @xmath48 .",
    "the coefficient vector that minimizes the variance of @xmath45 is : @xmath49 where @xmath50 is the @xmath51 covariance matrix of @xmath52 and @xmath53 is the @xmath54 vector of covariances of @xmath0 and @xmath9 s .",
    "when @xmath55 is used , the variance of @xmath45 is given by @xmath56 where @xmath57 therefore , @xmath58 and hence @xmath59 is precisely the theoretical degree of variance reduction if the controlled estimator @xmath60)$ ] is used to estimate @xmath61 as opposed to the crude mc estimator @xmath0 , and it is called the variance reduction ratio ( vrr ) statistic for control variates .",
    "note that there is no upper limit to the degree of achievable variance reduction since @xmath62 can potentially be very close to @xmath63 when the controls are highly correlated with the estimation variable @xmath0 .",
    "in other words , the cv technique can potentially be very effective leading to orders of magnitude of variance reduction .    in practice and in general , @xmath50 and @xmath53 ( i.e. @xmath55 )",
    "are not known exactly and need to be estimated from samples of @xmath9 s and @xmath0 .",
    "typically , @xmath55 is estimated from the same samples used to construct the controlled estimator @xmath45 . while this practice adds some bias for small sample sizes , and",
    "thus makes the effective decrease in estimator mean squared error not precisely equal to the variance reduction ratio @xmath59 , this bias converges to zero faster than the standard error of @xmath45 .",
    "thus , expending computational resources into generating separate pilot samples for estimating @xmath55 is not considered to be justifiable . for an insightful and detailed discussion of the cv technique ,",
    "see @xcite .      the critical task for using the cv technique",
    "is in finding effective controls . once the controls are selected , the rest of the procedure is fairly routine .",
    "an effective control , say @xmath64 , needs to satisfy two requirements ( to simplify the discussion we consider a scalar control ) :    1 .",
    "@xmath64 needs to be correlated with @xmath0 , and 2 .",
    "@xmath65 $ ] needs to be available to the user , i.e. , known .",
    "the main barrier to finding effective controls is the second requirement , namely the requirement of a known mean @xmath65 $ ] .",
    "a modification of the cv technique called biased control variate ( bcv ) reduces the burden of requirement ( r2 ) by allowing for a good approximation of @xmath65 $ ] when @xmath65 $ ] can not be evaluated analytically @xcite . while bcv lowers the requirement barrier and expands the range of available choices for effective controls , it nonetheless limits its potential scope by implicitly assuming an analytic path to arriving at the approximate value . as we describe in the next section , in the dbmc approach we turn the second requirement into a computational task",
    "; in other words , we use statistical estimation to obtain a good estimate of @xmath65 $ ] .",
    "therefore , barrier ( r2 ) is completely removed and the range of choices of controls is dramatically expanded .",
    "the relevant question now becomes whether the computational investment in estimating @xmath65 $ ] pays enough dividends to make the investment worthwhile .",
    "the starting point of the dbmc approach is the observation that in many parametric estimation settings , including in the example considered in this paper , quantities @xmath66 and @xmath67 are highly correlated when the same random input @xmath36 is used to generate them and when @xmath11 and @xmath68 are close .",
    "this suggests using control variates @xmath69 , @xmath70 , when estimating @xmath66 where @xmath13 s are `` close '' to @xmath11 .",
    "while we have identified potentially effective controls , we do not have sufficient information about them , i.e. , @xmath71 $ ] is not known and needs to be evaluated .",
    "this brings us to the second feature of the dbmc method that corresponds to its initial computational information gathering / setup stage .",
    "this stage corresponds to statistical estimation of @xmath72 .",
    "details are given below .",
    "the dbmc approach consists of a setup stage and an estimation stage .",
    "the dbmc setup phase involves generating a `` large '' number of input random vectors @xmath36 and obtaining `` high quality '' estimates of @xmath72 .",
    "let @xmath73 ( @xmath74 `` large '' ) denote a large set of random inputs .",
    "this set represents the _ database_. given the database , the averages of the controls are precisely calculated .",
    "a schematic of this stage is given in figure [ fig : dbmc setup ] .    ' '' ''    1 .",
    "for @xmath75 1 .   generate @xmath76 according to the distribution of the inputs ; 2 .",
    "for @xmath14 1 .   simulate the path @xmath77 2 .",
    "evaluate the value of the control @xmath78 + 2 .   for @xmath14 1 .",
    "find @xmath79 , the average of the @xmath80th control on the datebase , as @xmath81    ' '' ''      to estimate @xmath82 , at a @xmath11 close to @xmath13 s ( @xmath70 ) select a `` small '' sample ( say of size @xmath83 ) uniformly from the database .",
    "for each sample @xmath76 re - simulate the equation using @xmath76 and @xmath11 to obtain @xmath84 .",
    "for these samples the values of the controls @xmath85 are available in the database . using these",
    "evaluate a controlled estimate of @xmath82 . a schematic version of these steps",
    "is given in figure [ fig : dbmc estimation ] .    ' '' ''    1 .",
    "for @xmath86 1 .   select @xmath76 uniformly from the database ; 2 .   simulate the path @xmath87 ; 3 .",
    "evaluate the estimation variable @xmath84 .",
    "+ 2 .   find the controlled estimator of @xmath82 : @xmath88\\ ] ]    ' '' ''      there are two general schemes for implementation of our cv approach : ( i1 ) corresponding to what is described above , requires storing simulation inputs @xmath89 and outputs @xmath90 in a database for later resampling ; ( i2 ) does not utilize resampling , so there is no storage of data beyond recording the calculated control means .",
    "both implementations are feasible , the first is preferable in most cases ; the second may be preferred in some cases .",
    "we elaborate below .",
    "implementation ( i1 ) .",
    "* the database of random inputs , i.e. , @xmath76 s , are either directly stored or enough information about them ( e.g. input seeds of a pseudo - random number generator ) is stored to be able to regenerate @xmath76 s precisely . * the @xmath91 paths corresponding to @xmath13 , @xmath92 , i.e.",
    ", @xmath93 are generally simulated `` in parallel '' as elements of a random vector , @xmath94 , are progressively generated .",
    "* for each random input , say @xmath76 , the value of the controls , @xmath85 , @xmath95 , @xmath96 are stored .",
    "implementation ( i2 ) .",
    "* once the setup stage is completed , the only values stored are the `` high quality '' estimates of the means of @xmath72 s , i.e. , the @xmath91 values @xmath79 , @xmath14 .",
    "* at the estimation phase , @xmath3 random input vectors @xmath76 , @xmath97 , are generated _ anew _ ; paths at @xmath11 and @xmath13 , @xmath70 , are simulated using new random inputs and for each path @xmath85 and @xmath98 are calculated ; finally , using these values , the controlled estimator is evaluated .",
    "the promise of the approach is the following : by anchoring estimation via cv at a few high quality estimates ( at @xmath99 ) , it is possible to obtain high quality estimates at other locations in the parameter space ( at other @xmath11 ) with far fewer samples .",
    "the actual statistical properties of the resulting estimators , and the computational efficiency of generating them , reflect choices made in implementing each given problem .",
    "for example , how much computation should be `` invested '' in the exploration phase , and which points @xmath13 in the parameter space should be explored are two important questions that need further investigation .",
    "such choices generally involve problem dependent tradeoffs , and we leave them to future studies .    instead",
    ", the analysis that follows is meant to provide a general and qualitative understanding of the statistical properties , computational efficiency and the tradeoffs involved .",
    "the discussion is as general as possible , but consistent with the numerical study described in section [ numerical ] , where such implementation choices were made utilizing only a basic familiarity with the problem .",
    "for further discussion , see @xcite .",
    "we give the analysis for implementation ( i1 ) .",
    "in other words , assume we are re - sampling from the database .",
    "analysis of implementation ( i2 ) shows similar estimator statistical properties .    to simplify the discussion consider a single control , say @xmath100 .",
    "let @xmath101 $ ] , @xmath102 $ ] , @xmath103 , @xmath104 .",
    "assume a database of input variables are generated and let @xmath105 and @xmath106 be random variables corresponding to @xmath0 and @xmath100 that are generated by re - sampling ( uniformly , with replacement ) from the database .",
    "let @xmath107 , @xmath108 , @xmath109 , @xmath110 denote the means and variances of the re - sampled variables @xmath105 and @xmath111 .",
    "conditioned on the database , the controlled estimator is exactly the classical cv estimator and all results from classical cv apply .",
    "for example , for any scalar @xmath48 , @xmath112 is an unbiased estimator of @xmath107 , @xmath109 is known , and the optimal @xmath55 is what is prescribed by classical cv if we take all random variables as those defined on the database .",
    "a measure of variance reduction due to using a controlled estimator is @xmath113    we use the controlled estimator @xmath114 as an estimator for @xmath82 .",
    "assume optimal @xmath55 is used to define @xmath114 and assume @xmath115=j^*(\\theta)$ ] .",
    "e.g. @xcite , not to be confused with the resampling bias discussed in this section ] in general @xmath116 . therefore , @xmath114 is a biased estimator of @xmath82 where the bias is introduced by sampling from the database , i.e. , from @xmath105 , as opposed to from @xmath0 .",
    "we have some probabilistic assessment of this bias and we can reduce it by increasing the size of the database .",
    "specifically , for this bias we can obtain an approximate @xmath117 probability confidence interval : @xmath118 where @xmath119 is the @xmath120 quantile from the standard normal distribution .",
    "in other words , with high probability the bias is of the order of @xmath121 .",
    "we assume that for large @xmath74 the bias is sufficiently small to be disregarded and that we can focus on @xmath122 in ( [ eq : vrr_cv * ] ) as the key measure of computational gain in using the controlled estimator to estimate @xmath82 .      generating the above large database , as we pointed out earlier ,",
    "corresponds to an initial `` setup '' cost .",
    "let @xmath123 be the computational cost of generating a sample of @xmath66 .",
    "this cost involves generating an @xmath36 , simulating the path , and evaluating @xmath124 .",
    "a reasonable assumption for many problems is that this cost is about the same for all @xmath11 and @xmath36 .",
    "then the set - up cost of generating the database and obtaining averages of the controls is approximately @xmath125 .",
    "let @xmath126 denote the variance reduction ratio at @xmath11 , i.e. , the ratio of the variance of an uncontrolled sample and that of a controlled sample at @xmath11 .",
    "then , the statistical error of a controlled estimator based on @xmath3 samples is approximately the same as that of @xmath127 samples of an uncontrolled estimator .",
    "thus , the ratios of the computational costs of the two estimators ( to arrive at the same statistical accuracy ) is @xmath128 .",
    "therefore , @xmath129 can serve as a measure of benefit of the dbmc approach .",
    "the setup cost of the dbmc approach can be justified in two types of applications .",
    "the first type are those applications that require solving many instances of the estimation problem , at many @xmath11 s .",
    "if the total number of instances is sufficiently large , and some variance reduction is achieved on the average on those instances , then the large fixed set - up cost can be dwarfed by the total computational savings from the many estimations .",
    "the second type are real - time applications where the setup cost can be viewed as an off - line cost enabling significant efficiency gains in the critical task of real - time estimation .",
    "typically , the `` cost '' of delay in such real - time estimation is higher and not merely computational , justifying even a much larger computational effort off - line .",
    "the numerical results in this section are intended to give a qualitative illustration of the efficiency gains that can be achieved using the dbmc approach .",
    "specifically , we estimate the variance reduction that can be achieved over regular ( crude ) sampling , when estimating the three quantities of interest ( a point magnetization , total magnetization at a specific time @xmath21 and the total time - space magnetization ) at a range of the parameter @xmath11 .",
    "our choices of the size of the database , number of samples used for estimation , range of parameter values , and the controls are simply for illustration purposes .",
    "however , we expect that the numerical results are , qualitatively , quite representative .",
    "we simulate the tdgl dynamics on a @xmath130 lattice ( lattice spacing @xmath131 , @xmath132 ) with fixed @xmath133 . on each path , we evolve the system for a total of @xmath134 time steps ( @xmath135 ) which is sufficient for the system to exhibit behavior that is specific to its temperature region . the critical point for this system is @xmath136 @xcite , and our parameter range of interest ( @xmath137 to @xmath138 ) extends to both sides of that critical point .    to build a database ,",
    "we simulate @xmath139 paths and evaluate point magnetization , total magnetization at a specific time , and total space - time magnetization at two nominal values of @xmath11 , @xmath140 and @xmath141 .    for each quantity of interest , we consider three control variate estimators .",
    "the first two estimators , cv1.2 and cv1.35 , use single controls corresponding to @xmath142 and @xmath143 , respectively .",
    "we chose to anchor our estimators at those two nominal values for @xmath11 because they are located on opposite sides of the phase transition line @xmath144 .",
    "the third estimator , cv2c , uses both controls simultaneously .",
    "we use @xmath145 samples for crude and cv estimators . to estimate the variance of these estimators , following the micro - macro simulation approach ( see , e.g. , @xcite ) , we use @xmath146 independent macro simulations consisting of @xmath147 independent micro simulations .",
    "we obtain variance estimates from each macro simulation and average the resulting @xmath146 values to obtain an overall variance estimate .",
    "we report the ratios of the variance estimates ( crude / controlled , as in ( [ eq : vrr_cv * ] ) ) as @xmath122 . a sampling of vrr results for the total space - time magnetization ( problem p3 ) is given in table [ tab : table1 ] and the corresponding graph is given in figure [ fig : vrr_tim_log ] .",
    "the graph for point magnetization ( problem p1 ) are given in fig . [",
    "fig : vrr_point_log ] , and the results for the total magnetization at a time @xmath21 ( problem p2 ) are quite similar and are excluded .",
    ".[tab : table1]variance reduction ratios of the estimators applied to space - time integral of the magnetization , @xmath148 , at several values of @xmath11 . [ cols=\"<,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     , over a range of values of @xmath11 ( log scale ) .",
    "[ fig : vrr_tim_log ] ]    , in log scale .",
    "[ fig : vrr_point_log ] ]    based on these results , we draw the following conclusions :    * controlled estimators produce dramatic variance reduction for parameter values very close to the nominal parameters and substantial variance reduction at values moderately close to the nominal . * for all the estimation problems , adding the second control consistently improves performance , in some cases leading to substantial reduction in variance ( compared to single controls ) . of course , by incorporating information from points on both sides of the critical temperature",
    ", cv2c is expected to give better coverage than either of the single control estimators .",
    "however , cv2c does better than either of the single control estimators even in their own regions , which suggests that each control provides relevant information to the estimation problem in the opposite region .",
    "* vrr values for the total space - time magnetization are somewhat larger than those for the point and total magnetization at a specific time @xmath21  we expect this to be true more generally for path integrals when compared with values at specific time instances .",
    "in this paper we described a new strategy , database monte carlo ( dbmc ) , for improving computational efficiency of ensemble monte carlo . for a specific time - dependent nonlinear dynamics we showed that the approach can lead to significant efficiency gains for a range of estimation problems .",
    "our selection of the controls has been ad - hoc and for illustration purposes .",
    "further work is required to better understand the options available and the computational tradeoffs involved . to this end",
    ", our current research is focused on ( i ) derivation of more specific guidelines for the selection of effective control variates , ( ii ) implementation of the dbmc strategy in conjunction with other variance reduction techniques , for example , stratification and importance sampling , and ( iii ) application of the method in some specific domains , for example , estimation problems in geophysical fluids and biochemical systems ."
  ],
  "abstract_text": [
    "<S> in this paper we present a new approach to control variates for improving computational efficiency of ensemble monte carlo . </S>",
    "<S> we present the approach using simulation of paths of a time - dependent nonlinear stochastic equation . </S>",
    "<S> the core idea is to extract information at one or more nominal model parameters and use this information to gain estimation efficiency at neighboring parameters . </S>",
    "<S> this idea is the basis of a general strategy , called database monte carlo ( dbmc ) , for improving efficiency of monte carlo . in this paper </S>",
    "<S> we describe how this strategy can be implemented using the variance reduction technique of control variates ( cv ) . </S>",
    "<S> we show that , once an initial setup cost for extracting information is incurred , this approach can lead to significant gains in computational efficiency . </S>",
    "<S> the initial setup cost is justified in projects that require a large number of estimations or in those that are to be performed under real - time constraints .    </S>",
    "<S> monte carlo , variance reduction , control variates s05.10.ln , 02.70.uu , 02.70.tt </S>"
  ]
}