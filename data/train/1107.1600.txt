{
  "article_text": [
    "the use of biometric passwords , such as fingerprints , irises , etc .",
    ", has been an important issue in recent years , both because of the big advantages it may bring along and because of the clearly non negligible privacy concerns and implementation issues @xcite . in fact , as far as privacy is concerned , the storage of raw biometric data is not an acceptable solution , but , on the other hand , a secure storage can not be easily implemented , as for traditional passwords , by simply introducing an hash function .",
    "this is due to the fact that the binary strings derived from different acquisitions of the same biometric feature can slightly change from each other , and the biometric feature can slightly change itself . therefore a certain threshold of tolerance is needed to be able to identify legitimate from non legitimate users , but this prevents the standard use of collision resistant hash functions @xcite",
    ".    this problem has prompted researchers to devise other solutions for the secure storage and use of biometric passwords ( see @xcite for a selected survey of the literature ) .",
    "the idea behind most of these methods is a combined use of error correcting codes and hash functions , whose model is the fuzzy commitment scheme @xcite , which we revisit below .",
    "this has been later generalized to other types of metrics , such as the set difference metric @xcite and the edit distance metric @xcite .",
    "in particular , the fuzzy vault @xcite uses polynomial interpolation in order to allow authentication based on the matching of a sufficient number of features , while the fuzzy extractor @xcite is a further generalization which combines the previous constructions with particular objects called random extractors .",
    "these make the previous schemes stronger with respect to information leakage , though they can not prevent it @xcite .    briefly , privacy and implementation issues are still a concern and our aim is to give a further contribution , by showing that a syndrome based fuzzy hashing is actually feasible and even more convenient as far as information leakage is concerned .",
    "first of all , we address the design of codes to be used within this context , with focus on ldpc codes .",
    "secondly , we study the entropy of the fuzzy hashing output vectors and compare it with the corresponding quantity of its input vectors .",
    "the paper is structured as follows : in section 2 we briefly review the fuzzy commitment scheme and its main issues .",
    "section 3 is devoted to the syndrome based construction which we denote by fuzzy hashing .",
    "section 4 addresses the design of codes to be used in this scheme .",
    "section 5 is aimed at analyzing the entropy of the output vectors , in order to assess the performance of fuzzy hashing in terms of privacy .",
    "the fuzzy commitment scheme , proposed in @xcite , works as follows .",
    "suppose we want to securely store a length @xmath0 biometric vector @xmath1 , where @xmath2 is the galois field of order @xmath3 , and let @xmath4 be the maximum number of different symbols with respect to the reference vector @xmath5 that we can tolerate in any other acquisition of the same biometric feature .",
    "according to the fuzzy commitment scheme , we choose a hash function @xmath6 and an @xmath7$]-linear block code @xmath8 , able to correct @xmath4 errors , and then store @xmath9 , where @xmath10 is a random codeword associated to @xmath5 and @xmath11 .    given another biometric @xmath12 ,",
    "we compute the vector @xmath13 and apply the decoding algorithm of @xmath14 .",
    "if decoding succeeds , this results in a codeword @xmath15 , and we compute @xmath16 . if @xmath16 equals @xmath17 , i.e. , the value previously stored , we grant access , otherwise we deny it .",
    "in fact , if the hashes are the same , then @xmath18 ( apart from a negligible probability of a hash collision ) , so @xmath19 , where @xmath20 denotes the hamming distance . since @xmath21 and @xmath13 , it results @xmath22 .",
    "conversely , @xmath23 implies @xmath24 , so that decoding @xmath25 results in @xmath18 and @xmath26 .    in @xcite , we pointed out some of the main problems concerning the use of this scheme , namely implementation issues and security issues .",
    "in particular , privacy concerns may arise if the biometric templates are not uniformly distributed in the ambient space , that is their entropy is not maximal . in that case",
    ", it may be feasible to infer from @xmath27 some information about @xmath10 and , therefore , endanger the system security .",
    "starting from the fuzzy commitment principle , an alternative scheme ( here fuzzy hashing ) can be devised , in which syndromes are used in the place of codewords . under a coding theory viewpoint ,",
    "the two schemes are equivalent . despite this ,",
    "the use of syndromes has several advantages in the considered context .",
    "the idea of storing the syndrome of @xmath5 , instead of a shift vector from a codeword , already appeared in @xcite , where it is considered as an example of a sketch construction .",
    "we will show that the use of fuzzy hashing is advantageous with respect to the classical fuzzy commitment scheme , also by considering the characteristics of typical biometric data .    in the fuzzy hashing scheme , an @xmath7$]-linear block code @xmath8 ,",
    "able to correct @xmath4 errors , is selected , and it is described through its @xmath28 parity - check matrix @xmath29 , with @xmath30 . given a biometric vector @xmath5 to be stored , the pair @xmath31 is used to represent @xmath5 , were @xmath6 is a given hash function . when another biometric @xmath12 is acquired and is compared with @xmath5 , the value @xmath32 is computed , that coincides with the syndrome associated to the difference vector @xmath33 .",
    "then , syndrome decoding is applied on @xmath34 , according to the chosen code @xmath14 . if @xmath12 is taken from the same individual as @xmath5 , then @xmath35 has hamming weight equal to @xmath36 and it corresponds to a correctable error vector .",
    "so , syndrome decoding succeeds and correctly results in @xmath35 .",
    "then , starting from @xmath35 and @xmath12 , @xmath5 can be computed , as well as @xmath37 . the latter coincides with the stored value , so",
    "access is granted .",
    "otherwise , syndrome decoding fails or reports @xmath38 . in such case ,",
    "@xmath39 and @xmath40 is obtained , so access is denied .    in the fuzzy commitment ,",
    "the vector @xmath11 is stored .",
    "as some bits of the biometric @xmath5 might be known with high probability , this reveals some information on the secret codeword @xmath10 . the same may occur in fuzzy hashing , where the syndrome @xmath41 is stored , but only under the condition @xmath42 , where @xmath3 is a correctable error vector and",
    "@xmath43 is any codeword . in this case ,",
    "syndrome decoding results in @xmath3 ; so , some bits of @xmath43 can still be guessed , starting from the predictable bits of @xmath5 .",
    "however , especially for very low rate codes , the probability that @xmath5 is within the decoding radius of a codeword @xmath43 is very low , so fuzzy hashing provides better security with respect to the classical fuzzy commitment .",
    "in order to design suitable codes to be included in the fuzzy hashing scheme , we must consider the features of the biometric vectors we work with .",
    "if we refer to fingerprints or irises , a common acquisition will consist of a vector of several thousands of bits .",
    "however , it would be unpractical to apply fuzzy hashing directly on the plain acquisition , since a number of impairments could jeopardize the identification process .",
    "in fact , small changes in the acquisition conditions ( as ambient light or small movements of the subject ) could result in significant differences between two images of the same biometric feature .",
    "so , a common procedure is to extract a set of representative features from the biometric data through algorithms aimed at making them invariant to some frequent acquisition impairments .",
    "an example of this kind of algorithms will be considered in section [ sec : entropyanalysys ] .",
    "so , the code must be designed to work with the vectors produced as output by the feature extracting algorithm .",
    "typically , such vectors have length of the order of @xmath44k bits .",
    "another important aspect is the modeling of the errors affecting two vectors resulting from different biometric acquisitions from the same individual . in @xcite",
    ", the errors are modeled through a binary symmetric channel ( bsc ) with transition probability @xmath45 .",
    "we will adopt the same approach in this paper . as it will be shown in the next section , typical values of the percentage of different bits between the vectors representing two acquisitions of the same biometric range between @xmath44% and @xmath46% .",
    "so , we need codes with length about @xmath44k bits that are able to correct such high fractions of errors and , hence , have very low rate ( @xmath47 ) .",
    "just to give an idea , a bch code with ( @xmath48 , @xmath49 ) , that is , rate @xmath50 , is able to correct @xmath51 errors , which means it has a relative error correcting capability of about @xmath52% . a bch code with ( @xmath53 , @xmath54 ) ,",
    "that is , rate @xmath55 , is able to correct @xmath56 errors , that is almost the same percentage .",
    "a similar value is reached by the bch code having ( @xmath57 , @xmath58 ) , hence rate @xmath59 , able to correct @xmath60 errors .",
    "this evidences that , for classical algebraic codes , in order to maintain a given relative error correcting capability , the code rate must be decreased as the code length increases .",
    "furthermore , due to the long code length , decoding may also yield complexity issues , although recent algorithms can reduce the decoding complexity @xcite . a smarter choice is represented by modern iteratively decoded error correcting codes , like low - density parity - check ( ldpc ) codes @xcite .",
    "actually , the use of ldpc codes in this context has already been proposed in @xcite , but the code design was not addressed in those works . in summary , fuzzy hashing with ldpc codes brings the following advantages :    * fuzzy hashing reduces the amount of stored data , with respect to the fuzzy commitment , since @xmath61 . *",
    "fuzzy hashing reduces the predictability of the stored strings , as shown at the end of the previous section . *",
    "ldpc codes have greater error correction capabilities than classical algebraic codes . moreover , their relative error correcting capability , for a fixed rate , is almost constant as the code length increases .",
    "* ldpc codes allow to reduce the size of the code representation , by exploiting the sparse nature of @xmath29 .",
    "we are interested in almost regular codes , since they allow an easier implementation with respect to irregular codes ; so , we fix the column weight of the matrix @xmath29 to be equal to an integer @xmath62 . the row weight , for",
    "the code rate values here of interest , can not be constant as well .",
    "however , it will be minimally dispersed around its mean @xmath63 .",
    "if we suppose that ( as it occurs for all the codes we consider ) : @xmath64 the matrix @xmath29 can have rows with only the following two values of weight : @xmath62 and @xmath65 . in this case ,",
    "@xmath66 rows have weight @xmath62 and the other @xmath67 rows have weight @xmath68 .",
    "we can describe the column and row weight distributions of the matrix @xmath29 through the polynomials @xmath69 and @xmath70 representing , respectively , the variable node and check node degree distributions of the associated tanner graph @xcite .",
    "since we adopt the edge perspective , @xmath71 ( @xmath72 ) denotes the fraction of ones in the parity - check matrix @xmath29 which are in columns ( rows ) of weight @xmath73 .",
    "based on the hypotheses above , for the considered ensemble of codes it results : @xmath74x^{d_v-1 } + r(1+d_v)x^{d_v}. \\label{eq : lambda_rho}\\end{aligned}\\ ] ] starting from , we can estimate the asymptotic performance ( that is , for @xmath75 ) of ldpc codes in this ensemble by applying the density evolution method @xcite .",
    "gallager s a algorithm @xcite is an ldpc decoding algorithm for the bsc channel that permits an easy characterization through density evolution @xcite .",
    "so , we have estimated its convergence threshold ( that is the maximum channel error probability such that all the errors can be corrected using an infinite length code ) for the variable and check node degree polynomials given by .",
    "results are reported in table [ tab : thresholds ] , where the threshold values computed for @xmath76 are provided , for code rates ranging between @xmath77 and @xmath78 .",
    ".threshold values for the considered ldpc codes ensembles under gallager s a decoding .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]     as we observe from the table , the choice of a small value of @xmath62 ( like @xmath79 ) should be preferred . on the other hand ,",
    "the asymptotic performance under gallager s a decoding is not very good .",
    "for example , to reach a relative error correcting capability of @xmath52% , for @xmath80 , a code rate smaller than @xmath81 is required , that is similar to that needed by a bch code with @xmath57 .",
    "gallager s a algorithm allows an easy density evolution analysis , that is useful to verify that ldpc codes can asymptotically reach the error correcting performance we need and for which values of code rate . on the other hand ,",
    "when dealing with finite length codes , decoding algorithms with better performance can be used .",
    "in fact , gallager s a algorithm is a majority - based algorithm exploiting a fixed decision threshold @xmath82 , that is not the most effective choice .",
    "for example , adopting a variable @xmath82 ( as in gallager s b algorithm ) gives a first performance improvement .",
    "furthermore , several improved versions of these algorithms have been proposed in the literature @xcite , that are able to outperform gallager s original algorithms .",
    "finally , the classical sum product algorithm ( spa ) @xcite can also be applied on the bsc , even though , in absence of soft - information from the channel , the initial likelihood associated to each bit can assume only two opposite values . despite this , the spa is able to significantly improve the error correction performance with respect to that predicted in table [ tab : thresholds ] , as we will show in the next section , by providing some examples of practical codes .      in this subsection",
    "we provide examples of ldpc codes having parameters of interest in the fuzzy hashing context .",
    "the codes have been designed through the progressive edge growth ( peg ) algorithm @xcite , by imposing almost constant column and row weights for their parity - check matrices . for given column and row weights , the peg algorithm allows to design finite length ldpc codes with very good performance under belief propagation decoding .    in detail",
    ", we first fix the column weight @xmath62 .",
    "then , we impose the lower triangular form for the parity - check matrices , in such a way as to facilitate encoding , especially for very long codes . this introduces a last column having weight @xmath83 , and some columns having weight @xmath84 .",
    "however , their incidence with respect to the total number of columns is very small .",
    "then , the peg algorithm is used to optimize the length of the local cycles within the tanner graph associated to each code , while keeping the row weight distribution as much concentrated as possible .",
    "so , the characteristics of the codes we have designed are well overlaid with those fixed in the previous subsection . the codes mentioned above",
    "have been used to perform montecarlo simulations over the bsc , based on spa decoding .",
    "ldpc codes with @xmath85 and @xmath86 over the bsc with spa decoding .",
    "[ fig : rate01],width=226 ]    a first set of results is reported in fig .",
    "[ fig : rate01 ] , where ldpc codes having @xmath87 and @xmath88 ( hence , rate @xmath89 ) have been considered .",
    "we have designed two codes with different column weights : @xmath80 and @xmath90 .",
    "as we observe from the figure , the simulation confirms that the code with @xmath80 has better performance , in the waterfall region , with respect to the code having @xmath90 .",
    "this was expected on the basis of the results of density evolution .",
    "however , we also observe that the code with @xmath90 has a better performance in the error floor region , so its bit error rate ( ber ) and frame error rate ( fer ) curves tend to intersect with those of the first code .",
    "so , the choice of @xmath80 is suitable if a failure rate on the order of @xmath91 or more is acceptable ; otherwise , the choice of @xmath90 should be preferred .",
    "the performance improvement due to the spa is evident : both codes are able to achieve a rather low error rate for a percentage of bit errors around @xmath92% , or even more .",
    "and rate @xmath93 and @xmath81 over the bsc with spa decoding .",
    "[ fig : otherrates],width=226 ]    to further increase the error correcting capability of these codes , it is necessary to reduce their rate . to provide some examples in this sense ,",
    "we have considered @xmath94 and designed two other ldpc codes , having @xmath80 and rate @xmath93 and @xmath81 ( that is , @xmath95 and @xmath96 ) , respectively .    as we observe from their simulated performance ,",
    "reported in fig .",
    "[ fig : otherrates ] , by using the spa , these codes are able to reach very low error rates for a percentage of bit errors around @xmath46% and even more . also in this case , the performance improvement due to the spa with respect to the theoretical performance referred to gallager s a algorithm is evident .",
    "these results confirm that ldpc codes are well suited for the application in the considered context , in which a high correction capability is needed .",
    "furthermore , we can observe that , in this study , we have limited ourselves to consider almost regular codes , in order to keep their implementation complexity low .",
    "the adoption of irregular ldpc codes can result in a further performance improvement .",
    "in this section , we discuss the use of fuzzy hashing for iris recognition and we study how the adoption of syndromes affects some statistical properties of the biometric data . as a feature extractor , we use the algorithm described in @xcite and available in @xcite , together with its associated matching algorithm . in our simulations",
    ", we refer to the iris pattern database known as casia v.1 , provided by the institute of automation of the chinese academy of sciences @xcite .",
    "since the bits in iris templates are mutually dependent @xcite , we should compute the entropy of a source with memory , but this is computationally unfeasible for the sizes we are dealing with .",
    "so , according to @xcite , we evaluate the discrimination entropy over both the sets of iris templates and of their fuzzy hashes . for this purpose ,",
    "we first compute the distribution of the normalized hamming distances between all the couples of patterns within the set ( of images of the same iris or of images of different irises ) . then",
    ", we compute the mean @xmath97 and the standard deviation @xmath98 of the normalized hamming distance distribution .",
    "finally , the discrimination entropy ( also known as `` degrees of freedom '' or dof ) is obtained as @xmath99 .    applying fuzzy hashing to an iris recognition framework",
    "is not straightforward , due to the high variability in the iris acquisition phase .",
    "in fact , we must try to avoid all the differences given not only by the measure variability ( i.e. , scale and rotation ) , but also by the eye variability , that can significantly change the amount of visible iris and its shape .",
    "the standard way to take these issues into account is to compute a mask describing which bits in the iris template are free from such occlusions .",
    "the masks , in general , have a different number of set bits for each iris reading , resulting into information patterns having different lengths , both in the case they describe different irises and different readings of the same iris .",
    "this is not a problem in the case of the standard matching algorithms , since we can take , as inputs for the matching phase , the templates and masks of both the stored iris and the one we want to check .",
    "then , we can just compute the union of the two masks and obtain the number of different bits between the two templates , limited to the region excluded from the union of the masks .    instead , when we use syndromes , we can not access the reference template in clear ; so , we must cope with different lengths of the information patterns .",
    "one way is to treat the matching channel as an error - and - erasure channel @xcite , where erasures are given by the masks .",
    "however , in @xcite the authors use a different algorithm , while , in our case , the large number of bits erased by the masks makes this approach unusable . in order to obtain a fixed length of the information patterns ,",
    "we compute , for each template bit position @xmath73 , the probability @xmath100 that such bit is not erased by a mask . then , we compute a pseudomask selecting the bit positions corresponding to a value of @xmath100 lower than a threshold : @xmath101 . in our case , we fix @xmath102 .",
    "we are aware that , with this approach , we may neglect some bits that were not erased by their associated masks , but we have verified that this has a very limited effect for the considered algorithm . in fact , using all the selected bits in each template , we obtain , between two different readings of the same iris , an average hamming distance of @xmath103 , a standard deviation @xmath104 and a discrimination entropy equal to @xmath105 bits . instead , using only the bits selected by the pseudomask",
    ", we obtain an average hamming distance of @xmath106 , a standard deviation @xmath107 and a discrimination entropy equal to @xmath108 bit .",
    "the explanation for this moderate variation , in terms of discrimination entropy performance , is that the feature extraction algorithm does not compute the masks in the best possible way ( for example the two eyelids are approximated with straight lines and not with curves ) ; so , when we consider all the bits in each template , we introduce some errors that afflict the result , that is , we take some bits into account that we should actually erase .",
    "we model the channel as bsc both in the case of intra - class and inter - class comparisons .",
    "the use of a bsc model is further justified by the fact that , by exploiting the considered feature extraction algorithm and pseudomask , we have an experimental transition probability @xmath109 , @xmath110 for intra - class comparison and @xmath111 , @xmath112 for the inter - class case , thus confirming the ( almost ) symmetric nature of the channel .      in order to show that the use of fuzzy hashing is able to increase the discrimination entropy , that is to provide a better protection against information leakage , we estimate the dof on the set of plain templates , before and after the application of fuzzy hashing .",
    "the latter is performed through the ldpc code having @xmath87 , @xmath113 and @xmath86 , described in the previous section .",
    "we compute the normalized hamming distance between each pair of templates created from different irises and then estimate its probability density function .",
    "the results are reported in fig .",
    "[ fig : fighammingdistance ] .",
    "the set of plain template vectors has @xmath114 , @xmath115 , hence @xmath116 . after performing fuzzy hashing ,",
    "the values become @xmath117 , @xmath118 , @xmath119 , thus confirming the positive effect of fuzzy hashing .",
    "l.  bazzi , t.  j. richardson and r.  l. urbanke .",
    "exact thresholds and optimal codes for the binary - symmetric channel and gallager s decoding algorithm a. _ ieee trans .",
    "inform . theory _",
    "50 , no .  9 , pp . 20102021 , sep .",
    "2004 .",
    "l.  masek and p.  kovesi .",
    "matlab source code for a biometric identification system based on iris patterns .",
    "the school of computer science and software engineering , the university of western australia , 2003 .",
    "[ online ] .",
    "available : http://www.csse.uwa.edu.au/~pk/studentprojects/libor/sourcecode.html"
  ],
  "abstract_text": [
    "<S> the last decades have seen a growing interest in hash functions that allow some sort of tolerance , e.g. for the purpose of biometric authentication . among these , </S>",
    "<S> the syndrome fuzzy hashing construction allows to securely store biometric data and to perform user authentication without the need of sharing any secret key . </S>",
    "<S> this paper analyzes such a model , showing that it offers a suitable protection against information leakage and several advantages with respect to similar solutions , like the fuzzy commitment scheme . </S>",
    "<S> the design and characterization of ldpc codes to be used for this purpose is also addressed . </S>"
  ]
}