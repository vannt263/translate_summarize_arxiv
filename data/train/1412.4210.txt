{
  "article_text": [
    "in many animal sensory pathways , information about external stimuli is encoded in precise patterns of neuronal spikes @xcite .",
    "if the integrity of this form of information is to be preserved by downstream neurons , they have to respond to these precise patterns of input spikes with appropriate , precise patterns of output spikes .",
    "how networks of neurons can learn such spike train to spike train transformations has therefore been a question of significant interest .",
    "when the transformation is posited to map mean spike rates to mean spike rates , error backpropagation @xcite in multilayer feedforward networks of rate coding model of neurons has long served as the cardinal solution to this learning problem .",
    "our overarching objective in this article is to develop a counterpart for transformations that map precise patterns of input spikes to precise patterns of output spikes in multilayer feedforward networks of deterministic spiking neurons , in an online setting .",
    "in particular , we aim to devise a learning rule that is strictly spike timing based , that is , one that does not invoke concepts pertaining to spike rates or probabilistic models of spiking , and that seamlessly generalizes to multiple layers .    in an online setting",
    ", the spike train to spike train transformation learning problem can be described as follows . at one s disposal",
    "is a spiking neuron network with adjustable synaptic weights .",
    "the external stimulus is assumed to have been mapped  via a fixed mapping  to an input spike train .",
    "this input spike train is to be transformed into a desired output spike train using the spiking neuron network .",
    "the goal is to derive a synaptic weight update rule that when applied to the neurons in the network , incrementally brings the output spike train of the network into alignment with the desired spike train . with biological plausibility in mind",
    ", we also stipulate that the rule not appeal to computations that would be difficult to implement in neuronal hardware .",
    "we do not address the issue of what the desired output spike train in response to an input spike train is , and how it is generated .",
    "we assume that such a spike train exists , and that the network learning the transformation has access to it .",
    "finally , we do not address the question of whether the network has the intrinsic capacity to implement the input / output mapping ; we undertake to learn the mapping without regard to whether or not the network , for some settings of its synaptic weights , can instantiate the input / output transformation .",
    "there is , at the current time , little understanding of what transformations feedforward networks of a given depth / size and of a given spiking neuron model can implement , although some initial progress has been made in @xcite .",
    "the spike train to spike train transformation learning problem , as described above , has been a question of active interest for some time .",
    "variants of the problem have been analyzed and significant progress has been achieved over the years .",
    "one of the early results was that of the _ spikeprop _ supervised learning rule @xcite . here",
    "a feedforward network of spiking neurons was trained to generate a desired pattern of spikes in the output neurons , in response to an input spike pattern of bounded length .",
    "the caveat was that each output neuron was constrained to spike exactly once in the prescribed time window during which the network received the input .",
    "the network was trained using gradient descent on an error function that measured the difference between the actual and the desired firing time of each output neuron .",
    "although the rule was subsequently generalized in @xcite to accommodate multiple spikes emitted by the output neurons , the error function remained a measure of the difference between the desired and the first emitted spike of each output neuron .",
    "a subsequent advancement was achieved in the _ tempotron _ @xcite . here , the problem was posed in a supervised learning framework where a spiking neuron was tasked to discriminate between two sets of bounded length input spike trains , by generating an output spike in the first case and remaining quiescent in the second .",
    "the tempotron learning rule implemented a gradient descent on an error function that measured the amount by which the maximum postsynaptic potential generated in the neuron , during the time the neuron received the input spike train , deviated from its firing threshold .",
    "operating along similar lines and generalizing to multiple desired spike times , the fp learning algorithm @xcite set the error function to reflect the earliest absence ( presence ) of an emitted spike within ( outside ) a finite tolerance window of each desired spike .    elsewhere ,",
    "several authors have applied the widrow - hoff learning rule by first converting spike trains into continuous quantities , although the rule s implicit assumption of linearity of the neuron",
    "s response makes its application to the spiking neuron highly problematic , as explored at length in @xcite .",
    "for example , the _ resume _ learning rule for a single neuron was proposed in @xcite based on a linear - poisson probabilistic model of the spiking neuron , with the instantaneous output firing rate set as a linear combination of the synaptically weighted instantaneous input firing rates .",
    "the output spike train was modeled as a sample draw from a non - homogeneous poisson process with intensity equal to the variable output rate .",
    "the authors then replaced the rates with spike trains .",
    "although the rule was subsequently generalized to multilayer networks in @xcite , the linearity of the neuron model is once again at odds with the proposed generalization .",
    "likewise , the _ span _ learning rule proposed in @xcite convolved the spike trains with kernels ( essentially , turning them into pseudo - rates ) before applying the widrow - hoff update rule .",
    "a bird s eye view brings into focus the common thread that runs through these approaches . in all cases there",
    "are three quantities at play : the prevailing error @xmath0 , the output @xmath1 of the neuron , and the weight @xmath2 assigned to a synapse . in each case , the authors have found a _ scalar quantity _",
    "@xmath3 that stands - in for the real output spike train @xmath1 : the timing of the only / first spike in @xcite , the maximum postsynaptic potential / timing of the first erroneous spike in the prescribed window in @xcite , and the current instantaneous firing rate / pseudo - rate in @xcite .",
    "this has facilitated the computation of @xmath4 and @xmath5 , quantities that are essential to implementing a gradient descent on @xmath6 with respect to @xmath2 .",
    "viewed from this perspective , the immediate question becomes why not address @xmath1 directly instead of its surrogate @xmath3 ?",
    "after all , @xmath1 is merely a vector of output spike times .",
    "two major hurdles emerge upon reflection .",
    "firstly , @xmath1 , although a vector , can be potentially unbounded in length . secondly ,",
    "letting @xmath1 be a vector requires that @xmath0 compare the vector @xmath1 to the desired vector of spike times , and return a measure of disparity .",
    "this can potentially involve aligning the output to the desired spike train which not only makes differentiating @xmath0 difficult , but also strains biological plausibility @xcite .",
    "we overcome these issues in stages .",
    "we first turn to the neuron model and resolve the first problem .",
    "we then propose a closed form differentiable error functional @xmath0 that circumvents the need to align spikes .",
    "finally , virtual assignment of weights to spikes rather than synapses allows us to conduct a perturbation analysis of individual spike times and synaptic weights of the output as well as all intermediate neurons in the network .",
    "we derive the gradients of the error functional with respect to all output and intermediate layer neuron spike times and synaptic weights , and learning proceeds via a gradient descent mechanism that leverages these quantities .",
    "the perturbation analysis is of independent interest , in that it can be paired with other suitable differentiable error functionals to devise new learning rules .",
    "the overall focus on individual spike times , both in the error functional as well as in the perturbation analysis , has the added benefit that it sidesteps any assumptions of linearity in the neuron model or rate in the spike trains , thereby affording us a learning rule for multilayer networks that is theoretically concordant with the nonlinear dynamics of the spiking neuron .",
    "our approach applies to a general setup where the membrane potential of a neuron can be expressed as a sum of multiple weighted @xmath7-ary functions of spike times , for varying @xmath7 ( modeling the interactive effects of spikes ) , where gradients of the said functions can be computed .",
    "however , since the solution to the general setup involves the same conceptual underpinnings , for the sake of clarity we use a model of the neuron whose membrane potential function is additively separable ( i.e. , @xmath8 ) .",
    "the spike response model ( srm ) , introduced in @xcite , is one such model .",
    "although simple , the srm has been shown to be fairly versatile and accurate at modeling real biological neurons @xcite .",
    "the membrane potential , @xmath9 , of the neuron , at the present time is given by    @xmath10    where @xmath11 is the set of synapses , @xmath12 is the weight of synapse @xmath13 , @xmath14 is the prototypical postsynaptic potential ( psp ) elicited by a spike at synapse @xmath13 , @xmath15 is the axonal / synaptic delay , @xmath16 is the time elapsed since the arrival of the @xmath17 most recent afferent ( incoming ) spike at synapse @xmath13 , and @xmath18 is the potentially infinite set of past spikes at synapse @xmath13 .",
    "likewise , @xmath19 is the prototypical after - hyperpolarizing potential ( ahp ) elicited by an efferent ( outgoing ) spike of the neuron , @xmath20 is the time elapsed since the departure of the @xmath21 most recent efferent spike , and @xmath22 is the potentially infinite set of past efferent spikes of the neuron .",
    "the neuron generated a spike whenever @xmath9 crosses the threshold @xmath23 from below .",
    "we make two additional assumptions : ( i ) the neuron has an absolute refractory period that prohibits it from generating consecutive spikes closer than a given bound @xmath24 , and ( ii ) all input and output spikes that have aged past a given bound @xmath25 have no impact on the _ present _ membrane potential of the neuron",
    ".    the biological underpinnings of assumption ( i ) are well known .",
    "assumption ( ii ) is motivated by the following observations .",
    "it is generally accepted that all psps and ahps after an initial rise or fall , decay exponentially fast to the resting potential .",
    "this , in conjunction with the existence of an absolute refractory period , implies that for any given @xmath26 however small , there exists an @xmath25 such that the sum total effect of all spikes that have aged past @xmath25 can be bounded above by @xmath26 ( see @xcite ) . finally , observing that the biological neuron is a finite precision device , we arrive at assumption ( ii ) .",
    "the import of the assumptions is that the size of @xmath18 and @xmath22 can now be bounded above by @xmath27 .",
    "in essence , one has to merely look at a bounded past to compute the present membrane potential of the neuron , and moreover , there are only finitely many efferent and afferent spikes in this bounded past .",
    "it helps to conceptualize the state of a network of neurons as depicted in figure  [ fig : framework](a ) .",
    "the future spike trains generated by the neurons in the network depend only on the future input spikes and the spikes of all neurons in the bounded window @xmath28 $ ] .",
    "we make two other changes .",
    "first , we shift the function @xmath14 to the right so as to include the fixed axonal / synaptic delay . by so doing , we are relieved of making repeated reference to the delay in the analysis .",
    "more precisely , what was previously @xmath29 is now @xmath30 , with the new shifted @xmath14 satisfying @xmath31 for @xmath32 .",
    "the ahp @xmath19 remains as before satisfying @xmath33 for @xmath34 .",
    "second , and this has major consequences , since our objective is to update the synaptic weights in an online fashion , successive spikes on the same synapse can have potentially different weights ( assigned to the spike at its arrival at the synapse ) .",
    "we account for this by assigning weights to spikes rather than synapses ; we replace @xmath12 by @xmath35 . with these changes in place",
    ", we have    @xmath36",
    "having truncated the output spike train to a finite length vector of spike times , we turn to the error functional .",
    "the problem , stated formally , is : given two vectors of spike times , the output spike train @xmath37 and the desired spike train @xmath38 of potentially differing lengths , assign the pair a measure of disparity .",
    "there have been several such measures proposed in the literature ( see @xcite for details ) .",
    "however , for reasons that we delineate here , these measures do not fit our particular needs well . first and foremost",
    "comes the issue of temporal asymmetry . as described earlier , the effect of a spike on the potential of a neuron diminishes with age in the long run , until it ceases altogether at @xmath25 .",
    "we prefer a measure of disparity that focuses its attention more on the recent than the distant past .",
    "if the output and desired spike trains align well in the recent past , this is indicative of the synaptic weights being in the vicinity of their respective desired values .",
    "a measure that does not suppress disparity in the distant past will lead weight updates to overshoot .",
    "second comes the issue of the complex relationship between a spike train and its impact on the potential of a neuron , which is the quantity of real interest .",
    "we prefer a measure that makes this relationship explicit .",
    "finally comes the issue of the ease with which the measure can be manipulated .",
    "we prefer a measure that one can take the gradient of , in closed form .",
    "we present a measure that possesses these qualities .",
    "we begin with a parameterized class of non - negative valued functions with shape resembling psps .",
    "@xmath39    the functions are simplified versions of those in @xcite .",
    "figure  [ fig : framework](b ) displays these functions for various values of @xmath40 and @xmath41 .",
    "we set the putative impact of the vector of output spike times @xmath42 on a virtual postsynaptic neuron to be @xmath43 , and likewise for the vector of desired spike times @xmath44 .",
    "our goal is to assess the quantity    @xmath45    there are two paths we can pursue to eliminate the dependence on the parameters @xmath46 .",
    "the first is to set them to particular values .",
    "however , reasoning that it is unlikely for a presynaptic neuron to be aware of the shape of the psps of its postsynaptic neurons , of which there may be several with differing values of @xmath46 , we follow the second path ; we integrate over @xmath40 and @xmath41 .",
    "although @xmath40 can be integrated over the range @xmath47 , integrating @xmath41 over the same range results in spikes at @xmath25 having a fixed and finite impact on the membrane potential of the neuron . to regain control over the impact of a spike at @xmath25 , we integrate @xmath41 over the range @xmath48 $ ] , for a reasonably large @xmath49 . by setting @xmath25 to be substantially larger",
    "that @xmath49 , we can make the impact of a spike at @xmath25 be arbitrarily small .",
    "we therefore have :    @xmath50    following a series of algebraic manipulations and noting that    @xmath51    we get :    @xmath52    @xmath0 is a bounded from below and achieves its minimum value , 0 , at @xmath53 .",
    "computing the gradient of @xmath0 in eq  [ eq : e ] , we get :    @xmath54",
    "we now turn our attention to how perturbations in the weights and times of the input spikes of a neuron translate to perturbations in the times of its output spikes .",
    "the following analysis applies to any neuron in the network , be it an output or an intermediate layer neuron .",
    "however , we continue to refer to the input and output spike times as @xmath55 and @xmath20 to keep the nomenclature simple .",
    "consider the state of the neuron at the time of the generation of output spike @xmath56 .",
    "based on the present spike configuration , we can write    @xmath57    note that following definitions , @xmath14 returns the value 0 for all @xmath58 .",
    "likewise @xmath19 returns the value 0 for all @xmath59 .",
    "in other words , we do not have to explicitly exclude input / output spikes that were generated after @xmath56 .",
    "note also that we have replaced the threshold @xmath23 with @xmath60 .",
    "this reflects the fact that we are missing the effects of all spikes that at the time of the generation of @xmath56 had values less that @xmath25 but are currently aged beyond that bound .",
    "since these are not quantities that we propose to perturb , their effect on the potential can be considered a constant .    had the various quantities in eq  [ eq : base ] been perturbed in the past , we would have    @xmath61    combining eq  [ eq : base ] and eq  [ eq : perturb ] and using a first order taylor approximation , we get :    @xmath62    we can now derive the final set of quantities of interest from eq  [ eq : master ] :    @xmath63    and    @xmath64    the first term in the numerator of eq  [ eq : deltdelw ] and eq  [ eq : deltdelt ] corresponds to the direct effect of a perturbation .",
    "the second term corresponds to the indirect effect through perturbations in earlier output spikes .",
    "the equations are a natural fit for an online framework since the effects on earlier output spikes have previously been computed .",
    "we now have all the ingredients necessary to propose a gradient descent based learning mechanism .",
    "stated informally , neurons in all layers update their weights proportional to the negative of the gradient of the error functional . in what follows ,",
    "we specify the update for an output layer neuron and an intermediate layer neuron that lies one level below the output layer .",
    "the generalization to deeper intermediate layer neurons follows along similar lines .      in this case",
    "we would like to institute the gradient descent update @xmath65 , where @xmath66 is the learning rate .",
    "however , since the @xmath35 s belong to input spikes in the _ past _ , this would require us to reach back into the past to make the necessary change .",
    "instead , we institute a delayed update where the present weight at synapse @xmath13 is updated to reflect the combined contributions from the finitely many past input spikes in @xmath67 .",
    "formally ,    @xmath68    the updated weight is assigned to the subsequent spike at the time of its arrival at the synapse .",
    "@xmath69 is computed using the chain rule ( see figure  [ fig : framework](c ) ) , with the constituent parts drawn from eq  [ eq : deledelt ] and eq  [ eq : deltdelw ] summed over the finitely many output spikes in @xmath70 :    @xmath71      the update to a synaptic weight on an intermediate layer neuron follows along identical lines to eq  [ eq : delw ] and eq  [ eq : deledelwij ] with indices @xmath72 replaced by @xmath73 .",
    "the computation of @xmath74 , the partial derivative of the @xmath21 output spike time of the _ output _ layer neuron with respect to the weight on the @xmath75 input spike on synapse @xmath76 of the _ intermediate _ layer neuron , is as follows . to keep the nomenclature simple ,",
    "we assume that the @xmath17 output spike of the intermediate layer neuron , @xmath77 the @xmath17 input spike at the @xmath78 synapse of the output layer neuron . then , applying the chain rule ( see figure  [ fig : framework](c ) ) we have :    @xmath79    with the constituent parts drawn from eq  [ eq : deltdelt ] applied to the output layer neuron and eq  [ eq : deltdelw ] applied to the intermediate layer neuron , summed over the finitely many output spikes of the intermediate layer neuron which are identically the input spikes in @xmath67 of the output layer neuron .",
    "the earlier perturbation analysis is based on the assumption that infinitesimal changes in the synaptic weights or the timing of the afferent spikes of a neuron lead to infinitesimal changes in the timing of its efferent spikes .",
    "however , since the gradient descent mechanism described above takes finite , albeit small , steps , caution is warranted for situations where the step taken is inconsistent with the underlying assumption of the infinitesimality of the perturbations .",
    "there are two potential scenarios of concern .",
    "the first is when a spike is generated somewhere in the network due to the membrane potential just reaching threshold and then retreating .",
    "a finite perturbation in the synaptic weight or the timing of an afferent spike can lead to the disappearance of that efferent spike altogether .",
    "the perturbation analysis does account for this by causing the denominators in eq  [ eq : deltdelw ] and eq  [ eq : deltdelt ] to tend to zero ( hence , causing the gradients to tend to infinity ) . to avoid large updates",
    ", we set an additional parameter that capped the length of the gradient update vector .",
    "the second scenario is one where a finite perturbation leads to the appearance of an efferent spike .",
    "since there exists , in principle , an infinitesimal perturbation that does not lead to such an appearance , the perturbation analysis is unaware of this possibility .",
    "overall , these scenarios can cause @xmath0 to rise slightly at that timestep .",
    "however , since these scenarios are only encountered infrequently , the net scheme decreases @xmath0 in the long run .",
    "the efficacy of the learning rule derived in the previous section hinges on two factors : the ability of the spike timing based error to steer synaptic weights in the `` correct '' direction , and the qualitative nature of the nonlinear landscape of spike times as a function of synaptic weights , intrinsic to any multilayer network .",
    "we evaluate these in order .",
    "we begin with a brief description of the psp and ahp functions that were used in the simulation experiments .",
    "we chose the psp @xmath80 and the ahp @xmath19 to have the following forms ( see @xcite for details ) :    @xmath81    @xmath82    for the psp function , @xmath83 models the distance of the synapse from the soma , @xmath40 determines the rate of rise of the psp , and @xmath84 determines how quickly it decays .",
    "@xmath83 and @xmath40 are in dimensionless units . for the ahp function ,",
    "@xmath85 models the maximum drop in potential after a spike , and @xmath86 controls the rate at which the ahp decays .",
    "@xmath87 denotes the heaviside step function : @xmath88 for @xmath89 and @xmath90 otherwise .",
    "all model parameters other than the synaptic weights were held fixed through the experiments . in the vast majority of our experiments , we set @xmath91 for an excitatory synapse and @xmath92 for an inhibitory synapse , @xmath93 , @xmath94 for an excitatory synapse and @xmath95 for an inhibitory synapse . in all experiments , we set @xmath96 and @xmath97 . a synaptic delay @xmath98 was randomly assigned to each synapse in the range @xmath99\\,msec$ ] .",
    "the absolute refractory period @xmath24 was set to @xmath100 and @xmath49 was set to @xmath101 .",
    "@xmath25 was set to @xmath102 which made the impact of a spike at @xmath25 on the energy functional negligible .",
    "validating the learning rule would ideally involve presentations of pairs of input / desired output spike trains with the objective being that of learning the transformation in an unspecified feedforward network of spiking neurons .",
    "unfortunately , as observed earlier , the state of our current knowledge regarding what spike train to spike train transformations feedforward networks of particular architectures and neuron models can implement , is decidedly limited . to eliminate this confounding factor , we chose a witness based evaluation framework .",
    "specifically , we first generated a network , with synaptic weights chosen randomly and then fixed , from the class of architecture that we wished to investigate ( henceforth called the witness network ) .",
    "we drove the witness network with spike trains generated from a poisson process and recorded both the precise input spike train and the network s output spike train .",
    "we then asked whether a network of the same architecture , initialized with random synaptic weights , could learn this input / output spike train transformation using the proposed synaptic weight update rule .",
    "we chose a conservative criterion to evaluate the performance of the learning process ; we compared the evolving synaptic weights of the neurons of the learning network to the synaptic weights of the corresponding neurons of the witness network .",
    "specifically , the disparity between the synaptic weights of a neuron in the learning network and its corresponding neuron in the witness network was quantified using the mean absolute percentage error ( mape ) : the absolute value of the difference between a synaptic weight and the `` correct '' weight specified by the witness network , normalized by the `` correct '' weight , averaged over all synapses on that neuron .",
    "a mape of @xmath103 in the plots corresponds to @xmath104 . note that @xmath104 is the maximum achievable mape when the synaptic weights are lower than the `` correct '' weights .",
    "there are several reasons why this criterion is conservative .",
    "firstly , due to the finiteness of the length of the recorded input / output spike train of the witness network , it is conceivable that there exist other witness networks that map the input to the corresponding output .",
    "if the learning network were to tend toward one of these competing witness networks , one would erroneously deduce failure in the learning process .",
    "secondly , turning the problem of learning a spike train to spike train transformation into one of learning the synaptic weights of a network adds a degree of complexity ; the quality of the learning process now depends additionally on the characteristics of the input .",
    "it is conceivable that learning is slow or fails altogether for one input spike train while it succeeds for another .",
    "notably , the two extreme classes of spike train inputs , weak enough to leave the output neuron quiescent or strong enough to cause the output neuron to spike at its maximal rate , are both noninformative . in spite of these concerns",
    ", we found this the most objective and persuasive criterion .",
    "the synaptic weight update rule presented in the previous section does not specify a time of update .",
    "in fact , the synaptic weights of the neurons in the network can be updated at any arbitrary sequence of time points .",
    "however , as demonstrated here , the specific nature of one of the constituent parts of the rule makes the update insignificantly small outside a particular window of time .",
    "note that @xmath105 , the partial derivative of the error with respect to the timing of the @xmath106th efferent spike of the output neuron , appears in the update formulas of all synapses , be they on the output neuron or the intermediate neurons .",
    "we generated 10,000 random samples of pairs of vectors @xmath107 and @xmath108 with @xmath109 and @xmath110 chosen independently and randomly from the range @xmath111 $ ] and the individual spike times chosen randomly from the range @xmath28 $ ] .",
    "as noted earlier , @xmath25 and @xmath49 were set to @xmath112 and @xmath101 , respectively .",
    "we computed @xmath113 for the individual spikes in each @xmath114 according to eq  [ eq : deledelt ] .",
    "figure  [ fig : framework](d ) presents a scatter plot in log - scale of the absolute value of @xmath105 plotted against @xmath20 , for the entire dataset .",
    "as is clear from the plot , @xmath115 drops sharply with @xmath20 .",
    "hence , the only time period during which the gradient update formulas can have significant values is when at least one @xmath20 is small , that is , immediately after the generation of a spike by the output neuron . the symmetric nature of eq  [ eq : deledelt ] would indicate that this is also true for the timing of the desired spikes .",
    "we therefore chose to make synaptic updates to the entire network soon after the generation of a spike by the output neuron or the stipulation of a desired spike at the output neuron .",
    "it is clear from eq  [ eq : srm_new ] that the spike train output of a neuron , given spike train inputs at its various synapses , depends nonlinearly on its synaptic weights .",
    "the efficacy of the proposed error functional hinges on how reliably it can steer the synaptic weights of the learning network toward the synaptic weights of the witness network , operating solely on spike time disparities .",
    "this is best evaluated in a single layer network ( i.e. , a single neuron with multiple synapses ) since that eliminates the additional confounding nonlinearities introduced by multiple layers .",
    "consider an update to the synapses of a learning neuron at any point in time .",
    "observe that since the update is based on the pattern of spikes in the finite window @xmath28 $ ] , there are therefore uncountably many witness neurons that could have generated that pattern .",
    "this class of witness neurons is even larger if there are fewer desired spike times in @xmath28 $ ] .",
    "a gradient descent update that steers the synaptic weights of the learning neuron in the direction of any one of these potential witness neurons would constitute a `` correct '' update .",
    "it follows that when given a single witness neuron , correctness can only be evaluated over the span of multiple updates to the learning neuron .    to obtain a global assessment of the efficacy landscape in its entirety",
    ", we randomly generated @xmath116 witness - learning neuron pairs with @xmath117 excitatory synapses each ( the synaptic weights were chosen randomly from a range that made the neurons spike between @xmath118 and @xmath119 hz when driven by a @xmath117 hz input ) and presented each pair with a randomly generated @xmath117 hz poisson input spike train .",
    "each learning neuron was then subjected to @xmath120 gradient descent updates with the learning rate and cap set at small values .",
    "the initial versus change in ( that is , initial  final ) mape disparity between each learning and its corresponding witness neuron is displayed as a scatter plot in figure  [ fig : expt - single - layer-10-syn](a ) .",
    "across the @xmath116 pairs , @xmath121 ( @xmath122 ) showed improvement in their mape disparity .",
    "furthermore , we found a steady improvement of this percentage with increasing number of updates ( not shown here ) .",
    "note that since the input rate was set to be the same across all synapses , a rate based learning model would be expected to show improvement in approximately @xmath123 of the cases .",
    "a closer inspection of those learning neurons that did not show improvement indicated the lack of diversity in the input spike patterns to be the cause .",
    "we therefore ran a second set of experiments . once again , as before , we randomly generated @xmath116 witness - learning neuron pairs . only this time , input spike trains were drawn from an inhomogeneous poisson process with the rate set to modulate sinusoidally between @xmath90 and @xmath117 hz at a frequency of @xmath124 hz . the modulating rate was phase shifted uniformly for the @xmath117 synapses .",
    "surprisingly , after just @xmath116 gradient descent updates , @xmath125 ( @xmath126 ) neurons showed improvement , as displayed in figure  [ fig : expt - single - layer-10-syn](b ) , indicating that with sufficiently diverse input the error functional is _ globally _ convergent .    to verify the implications of the above finding with regard to the efficacy landscape , we chose @xmath119 random witness - learning neuron pairs spread uniformly over the range of initial mape disparities , and ran the gradient descent updates until convergence ( or divergence ) .",
    "input spike trains were drawn from the above described inhomogeneous poisson process .",
    "all learning neurons converged to their corresponding witness neurons as displayed in figure  [ fig : expt - single - layer-10-syn](c ) .",
    "the above experiments indicate that the error functional is globally convergent to the `` correct '' weights when the synapses on the learning neuron are driven by heterogeneous input .",
    "this finding can be related back to the nature of @xmath0 . as observed earlier",
    ", eq  [ eq : e ] makes @xmath0 nonnegative with the global minima at @xmath53 . for synapses on the learning and witness neuron pair to achieve this for all @xmath114 and @xmath127",
    ", they have to be identical .",
    "furthermore , it follows from eq  [ eq : deledelt ] that a local minima , if one exists , must satisfy @xmath109 independent constraints for all @xmath114 of length @xmath109 .",
    "this is highly unlikely for all @xmath114 and @xmath127 pairs generated by distinct learning and witness neurons , particularly so if the input spike train that drive these neurons is highly varied .",
    "although , this does not exclude the possibility of the sequence of updates resulting in a recurrent trajectory in the synaptic weight space , the experiments indicate otherwise .",
    "finally , we conducted additional experiments with neurons that had a mix of excitatory and inhibitory synapses with widely differing psps .",
    "in each of the @xmath119 learning - witness neuron pairs , @xmath128 of the @xmath117 synapses were set to be excitatory and the rest inhibitory .",
    "furthermore , half of the excitatory synapses were set to @xmath129 , and half of the inhibitory synapses were set to @xmath130 ( modeling slower nmda and @xmath131 synapses , respectively ) .",
    "the results were consistent with the findings of the previous experiments ; all learning neurons converged to their corresponding witness neurons as displayed in figure  [ fig : expt - single - layer-10-syn](d ) .",
    "having confirmed the efficacy of the learning rule for single layer networks , we proceed to the case of multilayer networks .",
    "the question before us is whether the spike time disparity based error at the output layer neuron , appropriately propagated back to intermediate layer neurons using the chain rule , has the capacity to steer the synaptic weights of the intermediate layer neurons in the `` correct '' direction .",
    "since the synaptic weights of any intermediate layer neuron are updated based not on the spike time disparity error computed at its output , but on the error at the output of the output layer neuron , the overall efficacy of the learning rule depends on the nonlinear relationship between synaptic weights of a neuron and output spike times at a downstream neuron .",
    "we ran a large suite of experiments to assess this relationship .",
    "all experiments were conducted on a two layer network architecture with five inputs that drove each of five intermediate neurons which in turn drove an output neuron .",
    "there were , accordingly , a sum total of @xmath132 synapses to train , @xmath133 on the intermediate neurons and @xmath118 on the output neuron .",
    "in the first set of experiments , as in earlier cases , we generated @xmath119 random witness networks with all synapses set to excitatory .",
    "for each such witness network we randomly initialized a learning network at various mape disparities and trained it using the update rule .",
    "input spike trains were drawn from an inhomogeneous poisson process with the rate set to modulate sinusoidally between @xmath90 and @xmath117 hz at a frequency of @xmath124 hz , with the modulating rate phase shifted uniformly for the @xmath118 inputs .",
    "the most significant insight yielded by the experiments was that the domain of convergence for the weights of the synapses , although fairly large , was not global as in the case of single layer networks .",
    "this is not surprising and is akin to what is observed in multilayer networks of sigmoidal neurons . of the @xmath119 witness - learning network pairs",
    ", @xmath134 learning networks converged to the correct synaptic weights , while @xmath135 did not .",
    "figure  [ fig : expt - two - layer-30-syn](a ) shows the average mape disparity ( averaged over the 5 intermediate and 1 output neuron ) of the @xmath134 networks that converged to the `` correct '' synaptic weights .",
    "figure  [ fig : expt - two - layer-30-syn](b ) shows the mape of the six constituent neurons of one of these @xmath134 networks ; each curve in figure  [ fig : expt - two - layer-30-syn](a ) corresponds to six such curves .    figure  [ fig : expt - two - layer-30-syn](c ) shows the average mape disparity ( averaged over the 5 intermediate and 1 output neuron ) of the @xmath135 networks that diverged",
    ". a closer inspection of the @xmath135 networks that failed to converge to the correct synaptic weights indicated a myriad of reasons , not all implying a definitive failure of the learning process . in many cases , all except a few of the @xmath132 synapses converged .",
    "figure  [ fig : expt - two - layer-30-syn](d ) shows one such example where all synapses on intermediate neurons as well as three synapses on the output neuron converged to their correct synaptic weights . for synapses on networks that did not converge to the correct weights , the reason was found to be excessively high or low pre / post synaptic spike rates , which as was noted earlier are noninformative for learning purposes ( incidentally , high rates accounted for the majority of the failures in the experiments ) .",
    "to elaborate , at high spike rates the tuple of synaptic weights that can generate a given spike train is not unique .",
    "gradient descent therefore can not identify a specific tuple of synaptic weights to converge to , and consequently the update rule can cause the synaptic weights to drift in an apparently aimless manner , shifting from one target tuple of synaptic weights to another at each update .",
    "not only do the synapses not converge , the error @xmath0 remains erratic and high through the process . at low spike rates , gradients of @xmath0 with respect to the synaptic weights",
    "drop to negligible values since the synapses in question are not instrumental in the generation of most spikes at the output neuron .",
    "learning at these synapses can then become exceedingly slow .    to corroborate these observations",
    ", we ran a second set of experiments on the @xmath135 witness - learning network pairs that did not converge .",
    "we reduced the maximum modulating input spike rate from @xmath117 to @xmath124 hz , i.e. , input spike trains were now drawn from an inhomogeneous poisson process with the rate set to modulate sinusoidally between @xmath90 and @xmath124 hz at a frequency of @xmath124 hz .",
    "figure  [ fig : expt - two - layer-30-syn - lr](a ) shows the average mape disparity of the @xmath135 networks with the color codes for the specific networks left identical to those in figure  [ fig : expt - two - layer-30-syn](c ) .",
    "only @xmath128 of the networks diverged this time .",
    "figure  [ fig : expt - two - layer-30-syn - lr](b ) shows the same network as in figure  [ fig : expt - two - layer-30-syn](d ) .",
    "this time all synapses converged with the exception of one at an intermediate neuron which displayed very slow convergence due to a low spike rate .",
    "we chose not to further redress the cases that diverged in this set of experiments with new , tailored , input spike trains to present a fair view of the learning landscape .    in our final set of experiments",
    ", we explored a network with a mix of excitatory and inhibitory synapses . specifically , two of the five inputs were set to inhibitory and two of the five intermediate neurons were set to inhibitory .",
    "the results of the experiments exhibited a recurring feature : the synapses on the inhibitory intermediate neurons , be they excitatory or inhibitory , converged substantially slower than the other synapses in the network .",
    "figure  [ fig : expt - two - layer-30-syn - exin](a ) displays an example of a network that converged to the `` correct '' weights .",
    "note , in particular , that the two inhibitory intermediate neurons were initialized at a lower mape disparity as compared to the other intermediate neurons , and that their convergence was slow . the slow convergence is clearer in the the close - up in figure  [ fig : expt - two - layer-30-syn - exin](b ) .",
    "the formal reason behind this asymmetric behavior has to do with the range of values @xmath136 takes for an inhibitory intermediate neuron as opposed to an excitatory intermediate neuron , and its consequent impact on eq  [ eq : deltokdelwgh ] . observe that @xmath137 , following the appropriately modified eq  [ eq : deltdelt ] , depends on the gradient of the psp elicited by spike @xmath138 at the instant of the generation of spike @xmath20 at the output neuron .",
    "the larger the gradient , the greater is the value of @xmath139 .",
    "typical excitatory ( inhibitory ) psps have a short and steep rising ( falling ) phase followed by a prolonged and gradual falling ( rising ) phase .",
    "since spikes are generated on the rising phase of inhibitory psps , the magnitude of @xmath139 for an inhibitory intermediate neuron is smaller than that of an excitatory intermediate neuron .",
    "a remedy to speed up convergence would be to compensate by scaling inhibitory psps to be large and excitatory psps to be small , which , incidentally , is consistent with what is found in nature .",
    "a synaptic weight update mechanism that learns precise spike train to spike train transformations is not only of importance to testing forward models in theoretical neurobiology , it can also one day play a crucial role in the construction of brain machine interfaces . in this article , we have presented such a mechanism formulated with a singular focus on the timing of spikes .",
    "the rule is composed of two constituent parts , ( a ) a differentiable error functional that computes the spike time disparity between the output spike train of a network and the desired spike train , and ( b ) a suite of perturbation rules that directs the network to make incremental changes to the synaptic weights aimed at reducing this disparity .",
    "we have already explored ( a ) , that is , @xmath105 as defined in eq  [ eq : deledelt ] , and presented its characteristic nature in figure  [ fig : framework](d ) .",
    "as regards ( b ) , when the learning network is driven by an input spike train that causes all neurons , intermediate as well as output , to spike at moderate rates , @xmath140 as defined in eq  [ eq : deltdelw ] and @xmath141 as defined in eq  [ eq : deltdelt ] can be simplified .",
    "observe that when a neuron spikes at a moderate rate , the past output spike times have a negligible ahp induced impact on the timing of the current spike .",
    "formally stated , @xmath142 in eq  [ eq : deltdelw ] and [ eq : deltdelt ] are negligibly small for any output spike train with well spaced spikes .",
    "therefore ,          the denominators in the equations above , as in eq  [ eq : deltdelw ] and [ eq : deltdelt ] , are normalizing constants that are strictly positive since they correspond to the rate of rise of the membrane potential at the threshold crossing corresponding to spike @xmath56 .",
    "the numerators relate an interesting story . although both are causal , the numerator in eq  [ eq:_deltdelt ] changes sign across the extrema of the psp",
    ". accumulated in a chain rule , these make the relationship between the pattern of input and output spikes and the resultant synaptic weight update rather complex .",
    "our experimental results have demonstrated that feedforward neuronal networks can learn precise spike train to spike train transformations guided by the weakest of supervisory signals , namely , the desired spike train at merely the output neuron .",
    "supervisory signals can of course be stronger , with the desired spike trains of a larger subset of neurons in the network being provided .",
    "the learning rule seamlessly generalizes to this scenario with the revised error functional @xmath0 set as the sum of the errors with respect to each of the supervising spike trains .",
    "what is far more intriguing is that the learning rule generalizes to _ recurrent _ networks as well .",
    "this follows from the observation that whereas neurons in a recurrent network can not be partially ordered , the spikes of the recurrent network in the bounded window @xmath28 $ ] can be partially ordered according to their causal structure ( see figure  [ fig : framework](c ) ) , which then permits the application of the chain rule . learning in this scenario",
    ", however , seems to be at odds with the sensitive dependence on initial conditions of the dynamics of a large class of recurrent networks @xcite , and therefore , the issue calls for careful analysis .",
    "jolivet , r. , lewis , t.j . &",
    "gerstner , w. ( 2004 ) generalized integrate - and - fire models of neuronal activity approximate spike trains of a detailed model to a high degree of accuracy .",
    "_ journal of neurophysiology _ * 92*(2):959 ."
  ],
  "abstract_text": [
    "<S> we derive a synaptic weight update rule for learning temporally precise spike train to spike train transformations in multilayer feedforward networks of spiking neurons . </S>",
    "<S> the framework , aimed at seamlessly generalizing error backpropagation to the deterministic spiking neuron setting , is based strictly on spike timing and avoids invoking concepts pertaining to spike rates or probabilistic models of spiking . </S>",
    "<S> the derivation is founded on two innovations . </S>",
    "<S> first , an error functional is proposed that compares the spike train emitted by the output neuron of the network to the desired spike train by way of their putative impact on a virtual postsynaptic neuron . </S>",
    "<S> this formulation sidesteps the need for spike alignment and leads to closed form solutions for all quantities of interest . </S>",
    "<S> second , virtual assignment of weights to spikes rather than synapses enables a perturbation analysis of individual spike times and synaptic weights of the output as well as all intermediate neurons in the network , which yields the gradients of the error functional with respect to the said entities . learning proceeds via a gradient descent mechanism that leverages these quantities . </S>",
    "<S> simulation experiments demonstrate the efficacy of the proposed learning framework </S>",
    "<S> . the experiments also highlight asymmetries between synapses on excitatory and inhibitory neurons . </S>"
  ]
}