{
  "article_text": [
    "seismic tomography is a geophysical imaging method that allows to estimate the three - dimensional structure of the earth s deep interior , using observations of seismic waves made at its surface .",
    "seismic waves generated by moderate or large earthquakes travel through the entire planet , from crust to core , and can be recorded by seismometers anywhere on earth .",
    "they are by far the most highly resolving wave type available for exploring the interior at depths to which direct measurement methods will never penetrate ( tens to thousands of kilometers ) .",
    "seismic tomography takes the shape of a large , linear(ized ) inverse problem , typically featuring thousands to millions of measurements and similar numbers of parameters to solve for .",
    "to first order , the earth s interior is layered under the overwhelming influence of gravity .",
    "its resulting , spherically symmetric structure had been robustly estimated by the 1980s [ @xcite ] and is characterized by @xmath0 parameters . since then seismologists",
    "have been mainly concerned with estimating lateral deviations from this spherically symmetric reference model [ @xcite ] .",
    "though composed of solid rock , the earth s mantle is in constant motion ( the mantle extends from roughly 30 km to 2900 km depth and is underlain by the fluid iron core ) .",
    "rock masses are rising and sinking at velocities of a few centimeters per year , the manifestation of advective heat transfer : the hot interior slowly loses its heat into space .",
    "this creates slight lateral variations in material properties , on the order of a few percent , relative to the statically layered reference model .",
    "the goal of seismic tomography is to map these three - dimensional variations , which embody the dynamic nature of the planet s interior .",
    "beneath well - instrumented regions  such as our chosen example , the united states ",
    "seismic waves are capable of resolving mantle heterogeneity on scales of a few tens to a few hundreds of kilometers . parameterizing the three - dimensional earth , or even just a small part of it , into blocks of that size results in the mentioned large number of unknowns , which mandate a linearization of the inverse problem .",
    "fortunately this is workable , thanks to the rather weak lateral material deviations of only a few percent ( larger differences can not arise in the very mobile mantle ) .",
    "seismic tomography is almost always treated as an optimization problem .",
    "most often a least squares approach is followed requiring general matrix inverses [ @xcite ] , while adjoint techniques are used when an explicit matrix formulation is computationally too expensive [ @xcite ] . while probabilistic seismic tomography using markov chain monte carlo ( mcmc ) methods has been given considerable attention by the geophysical ( seismological ) community , these applications have been restricted to linear or nonlinear problems of much lower dimensionality assuming gaussian errors [ @xcite ] .",
    "for example , @xcite compares the damped least - squares method ( lsqr ) , a genetic algorithm and the metropolis",
    " hastings ( mh ) algorithm in a low - dimensional linear tomography problem involving copper mining data .",
    "he finds that the mcmc sampling technique provides more robust estimates of velocity parameters compared to the other approaches .",
    "@xcite capture the uncertainty of the velocity parameters in a linear model by selecting the representation grid of the corresponding field , using a reversible jump mcmc ( rjmcmc ) approach .",
    "in bodin et  al .",
    "( @xcite ) again rjmcmc algorithms are developed to solve certain transdimensional nonlinear tomography problems with gaussian errors , assuming unknown variances . @xcite and @xcite study seismic and thermo - chemical structures of the lower mantle and solve a corresponding low - dimensional nonlinear problem using a standard mcmc algorithm .",
    "for exploring high - dimensional parameter space the mcmc sampling faces difficulties in evaluating the expensive nonlinear physical model while efficiently traversing the high - dimensional parameter space .",
    "we approach linearized tomographic problems ( physical forward model inexpensive to solve ) in a bayesian framework , for a fully dimensioned , continental - scale study that features @xmath153,000 data points and @xmath111,000 parameters . to our knowledge , this is by far the highest dimensional application of monte carlo sampling to a seismic tomographic problem so far . assuming gaussian distributions for the error and the prior",
    ", our mcmc sampling scheme allows for characterization of the posterior distribution of the parameters by incorporating flexible spatial priors using gaussian markov random field ( gmrf ) .",
    "spatial priors using gmrf arise in spatial statistics [ @xcite ] , where they are mainly used to model spatial correlation . in our geophysical context we apply a spatial prior to the parameters rather than to the error structure , since the parameters represent velocity anomalies in three - dimensional space .",
    "thanks to the sparsity of the linearized physical forward matrix as well as the spatial prior sampling from the posterior density , a high - dimensional multivariate gaussian can be achieved by a cholesky decomposition technique from @xcite or @xcite .",
    "their technique is improved by using a different permutation algorithm . to demonstrate the method",
    ", we estimate a three - dimensional model of mantle structure , that is , variations in seismic wave velocities , beneath the unites states down to 800 km depth .",
    "our approach is also applicable to other kinds of travel time tomography , such as cross - borehole tomography or mining - induced seismic tomography [ @xcite ] .",
    "other types of tomography , such as x - ray tomography in medical imaging , can also be recast as a linear matrix problem of large size with a very sparse forward matrix .",
    "however , the response is measured on pixel areas and , thus , the error structure is governed by a spatial markov random field , while the regression parameters are modeled nonspatially using , for example , laplace priors [ @xcite ] .",
    "some other inverse problems such as image deconvolution and computed tomography [ @xcite ] , electromagnetic source problems deriving from electric and magnetic encephalography , cardiography [ @xcite ] or convection - diffusion contamination transport problems [ @xcite ] can also be written as linear models . however , the physical forward matrix of those problems is dense in contrast to the situation we consider . for solutions to these problems , matrix - inversion or low - rank approximation to the posterior covariance matrix , as introduced in @xcite , are applied to high - dimensional linear problems . in image reconstruction problems",
    "@xcite demonstrates gibbs sampling on ( 1d and 2d- ) images using an intrinsic gmrf prior with the preconditioned conjugate gradient method in cases where efficient diagonalization or cholesky decomposition of the posterior covariance matrix is not available . in other tomography problems , such as electrical capacitance tomography , electrical impedance tomography or optical absorbtion and scattering tomography ,",
    "the physical forward model can not be linearized , so that the bayesian treatment of those problems is limited to low dimensions [ @xcite ] .",
    "the remainder of this paper is organized as follows : section [ sec2 ] describes the geophysical forward model and the seismic travel time data .",
    "section [ sec3 ] discusses flexible specifications for the spatial prior of the three - dimensional velocity model and the metropolis ",
    "gibbs sampling algorithm for estimating its posterior distribution .",
    "method performance under various model assumptions is examined in simulation studies in section [ sec4 ] .",
    "section [ sec5 ] applies the method to real travel time data , which have previously been used in conventional tomography [ @xcite ] , allowing for comparison .",
    "section [ sec6 ] discusses the advantages , limitations and possible extensions of our model .",
    "here we explain the physics and the data that enter seismic tomography and how they are formulated into a linear inverse problem , which will be treated by our markov chain monte carlo method in subsequent sections .",
    "every larger earthquake generates seismic waves of sufficient strength to be recorded by seismic stations around the globe .",
    "such seismograms are time series at discrete surface locations , that is , spatially sparse point samples of a continuous wavefield that exists        everywhere inside the earth and at its surface .",
    "figure  [ allevtsstats ] illustrates the spatial distribution of sources ( large earthquakes , blue ) and receivers ( seismic broadband stations , red ) that generated our data .",
    "each datum @xmath2 measures the difference between an observed arrival time @xmath3 of a seismic wave @xmath4 and its predicted arrival time @xmath5 : @xmath6 @xmath5 is evaluated using the spherically symmetric reference model iasp91 by @xcite . for the teleseismic p waves used in our application , this difference @xmath7 would typically be on the order of one second , whereas @xmath3 and @xmath5 are on the order 6001000 seconds .",
    "@xmath7 can be explained by slightly decreasing the modeled velocity in certain sub - volumes of the mantle .     at @xmath89000 grid nodes under north america , inside the subvolume marked by the red ellipse .",
    "red stars mark a few of the earthquake sources shown in figure [ allevtsstats ] .",
    "the densified point clouds , between the sources and a few stations in north america , map out the sensitivity kernels of the selected wave paths .",
    "each sensitivity kernel fills one row of matrix @xmath9 .",
    "left : schematic illustration of the components of an individual wave path . ]",
    "we adopt the parametrization and a subset of the data measured by @xcite .",
    "the earth is meshed as a sphere of irregular tetrahedra with 92,175 mesh nodes . at each mesh mode , the parameters of interest are the relative velocity variation of the mantle with respect to the reference velocity of spherically - symmetric model iasp91 [ @xcite ] . the parameter vector is denoted as @xmath10 , where the set of mesh node @xmath11 fills the entire interior of the earth . since both travel time deviations @xmath2 and the @xmath12 are small , the wave equation may be linearized around the layered reference model : @xmath13 where @xmath14 represents the frchet sensitivity kernel of the @xmath4th wavepath , that is , the partial derivatives of the chosen misfit measure or data @xmath2 with respect to the parameters @xmath12 .",
    "after numerical integration of kernel @xmath15 onto the mesh , ( [ phy1a ] )  takes the form @xmath16 geometrically speaking , row vector @xmath17 maps out the mantle subvolume that would influence the travel time @xmath2 if some velocity anomaly @xmath12 were located within it .",
    "this sensitivity region between an earthquake and a station essentially has ray - like character ( figure [ fwmat ] ) , though in physically more sophisticated approximations , the ray widens into a banana shape [ @xcite ] . over the past decade , intense research effort has gone into the computability of sensitivity kernels under more and more realistic approximations [ @xcite ] .",
    "since this issue is only tangential to our focus , we chose to keep the sensitivity calculations as simple as possible by modeling them as rays ( the @xmath17 are computed only once and stored ) .",
    "we note that the dependence of @xmath18 on @xmath19 can be neglected , as is common practice .",
    "this is justified by two facts : ( i ) velocity anomalies @xmath19 deviate from those of the ( spherically symmetric ) reference model by only a few percent , since the very mobile mantle does not support larger disequilibria , and ( ii ) , even though the ray path in the true earth differs ( slightly ) from that in the reference model , this variation affects the travel time observable only to second order , according to fermat s principle [ and analogous arguments for true finite - frequency sensitivities , @xcite ] .",
    "whatever the exact modeling is , it is very sparse , since every ray or banana visits only a small subvolume of the entire mantle  this sparsity is important for the computational efficiency of the mcmc sampling .",
    "gathering all @xmath20 observations , ( [ phy1b ] ) can be rewritten as @xmath21 , where sparse matrix @xmath22 contains in its rows the @xmath20 sensitivity kernels .",
    "the left panel of figure [ fwmat ] illustrates the sensitivity kernels between one station and several earthquakes ( i.e. , several matrix rows ) . in practice",
    ", the problem never attains full rank , so that regularization must be added to remove the remaining nonuniqueness .",
    "the linear system @xmath21 is usually solved by some sparse matrix solver  a popular choice is the sparse equations and least squares ( lsqr ) algorithm by @xcite , which minimizes @xmath23 , where @xmath24 is a regularization parameter that removes the underdeterminacy in @xmath9 [ @xcite ] .    in summary",
    ", we have formulated the seismic tomography problem as it is overwhelmingly practiced by the geophysical community today .",
    "we use travel time differences @xmath2 as the misfit criterion , that is , as input data to the inverse problem , and seek to estimate the three - dimensional distribution of seismic velocity deviations @xmath19 that have caused these travel time anomalies .",
    "the sensitivity kernels @xmath17 are modeled using ray theory , a high - frequency approximation to the full wave equation . in the conventional optimization approach",
    ", a regularization term is added , and the inverse problem is solved by minimizing the l2 norm misfit .      since all 92,175 velocity deviation parameters of the entire earth are currently not manageable for mcmc sampling , we regard as free parameters only 8977 of those parameters which are located beneath the western u.s . ,",
    "that is , between latitudes @xmath25n to @xmath26n , longitudes @xmath27w to @xmath28w , and 0800 km depth .",
    "tetrahedra nodes are spaced by 60150 km .",
    "we denote this subset of velocity parameters as  @xmath29 .",
    "besides velocity parameters , we also consider the uncertainty in the location and the origin time of each earthquake source , which contribute to the travel time measurement .",
    "government and research institutions routinely publish location estimates for every larger earthquake , but any event may easily be mistimed by a few seconds , and mislocated by ten or more kilometers ( corresponding to a travel duration of 1 s or more ) . this is a problem , since the structural heterogeneities themselves only generate travel time delays on the order of a few seconds .",
    "hence , the exact locations and timings of the earthquakes  or rather : their deviations from the published catalogue values  need to be treated as additional free parameters , to be estimated jointly with the structural parameters .",
    "these so - called `` source corrections '' are captured by three - dimensional shift corrections of the hypocenter ( @xmath30 ) and time corrections ( @xmath31 ) per earthquake .    using the lsqr method , @xcite",
    "jointly estimate all 92,175 parameters together with these `` source corrections . ''",
    "using those lsqr solutions , we have two modeling alternatives for the earth structural inversion with @xmath20 travel delay time observations @xmath32 : @xmath33 where @xmath34 denotes the ensemble of sensitivity kernels of the western usa .",
    "@xmath35 denotes the @xmath20-dimensional multivariate normal distribution with mean @xmath36 and covariance @xmath37 , and the @xmath20-dimensional unity matrix is denoted by @xmath38 . in model 1 , we only estimate the velocity parameters @xmath29 and keep the part of the travel delay time for the corrections parameters ( path ab in right panel of figure  [ fwmat ] ) fixed at the lsqr solutions of @xmath30 and @xmath31 estimated by @xcite . the extended model with joint estimation of source corrections is given by @xmath39\\\\[-8pt ] \\bolds{\\varepsilon}&\\sim&\\mathcal{n}_n\\biggl(\\mathbf{0 } , \\frac{1}{\\phi}i_n\\biggr).\\nonumber\\end{aligned}\\ ] ] here we apply the travel delay time @xmath40 assuming that the part of the travel time running through path ac is given .",
    "this given part of the travel times is again based on the lsqr solution estimated by @xcite .",
    "the number of travel time data from source - receiver pairs is @xmath41 , collected from 760 stations and 529 events .",
    "the number of hypocenter correction parameters is 1587 ( 529 earthquakes@xmath423 ) and there are 529 time correction parameters .",
    "@xcite found that in the uppermost mantle , between 0 km to 100 km depth , the velocity can deviate by more than @xmath43 from the spherically symmetric reference model .",
    "as depth increases , the mantle becomes more homogeneous and the velocity deviates less from the reference model .",
    "in both models ( [ stat1 ] ) and ( [ stat2 ] ) we have the spatial parameter @xmath29 , which we denote generically as @xmath19 in this section . in the bayesian approach",
    "we need a proper prior distribution for this high - dimensional parameter vector @xmath19 . to account for their spatially correlated structure ,",
    "we apply the conditional autoregressive model ( car ) and assume a markov random field structure for @xmath19 .",
    "this assumption says that the conditional distribution of the local characteristics @xmath44 , given all other parameters @xmath45 , @xmath46 , only depends on the neighbors , that is , @xmath47 , where @xmath48 and `` @xmath4 '' denotes the set of neighbors of site  @xmath4 .",
    "the car model and its application have been investigated in many studies , such as @xcite or @xcite .",
    "since the earth is heterogeneous and layered , lateral correlation length scales are larger than over depths , and so we propose an ellipsoidal neighborhood structure for the velocity parameters .",
    "let @xmath49 be the positions of the @xmath4th and the @xmath50th nodes in cartesian coordinates .",
    "the @xmath50th node is a neighbor of node @xmath4 if the ellipsoid equation is satisfied , that is , @xmath51 . to add a rotation of the ellipsoid to an arbitrary direction in the space",
    ", we could simply modify the vector @xmath52 to @xmath53 with a rotation matrix @xmath54 for given rotation matrices in the @xmath55 , @xmath56 and @xmath57 directions , respectively .",
    "the spherical neighborhood structure is a special case of the ellipsoidal structure with @xmath58 .",
    "let @xmath59 be the maximum distance of @xmath60 , @xmath61 and  @xmath62 . for weighting the neighbors we adopt either the exponential @xmath63 or reciprocal weight functions @xmath64 , that is , @xmath65 where @xmath66 is the euclidean distance between node @xmath4 and node @xmath50 .",
    "the exponential weight function is bounded while the reciprocal weight function is unbounded .",
    "those weighting functions have been studied by @xcite or @xcite .",
    "the left panel of figure [ weightsfunc ] illustrates the weight functions for @xmath67 km .",
    "km and @xmath67 km .",
    "right : the trade - off relationship between numbers of neighbors and the prior variance @xmath68 , @xmath69 km , @xmath70 weights . ]    let @xmath71 be either @xmath63 or @xmath64 in ( [ wfuncs ] ) . to model the spatial structure of @xmath29 in ( [ stat1 ] ) and ( [ stat2 ] ) , a car model is used .",
    "following @xcite , let @xmath72 with precision matrix @xmath73 they showed that @xmath74 is symmetric and positive definite , and that conditional correlations can be explicitly determined . for @xmath75 ,",
    "the precision matrix @xmath74 converges to the identity matrix , that is , @xmath76 corresponds to independent elements of @xmath19 .",
    "the precision matrix in ( [ qmat ] ) for both elliptical and spherical cases indicates anisotropic covariance structure and depends on the distance between nodes , the number of neighbors of each node and the weighting functions .",
    "the elliptical precision matrix additionally depends on the orientation .",
    "the right panel of figure [ weightsfunc ] shows the trade - off between numbers of neighbors and prior variance , which indicates that the more neighbors the @xmath4th node has , the smaller is its prior variance @xmath77 .",
    "posterior distribution of velocity parameters from regions with less neighborhood information can be rough , since they are not highly regularized due to the large prior covariances .",
    "this may produce sharp edges in the tomographic image .",
    "however , this is a more realistic modeling method since one is more sure about the optimization solution if a velocity parameter has more neighbors .",
    "moreover , this prior specification is adapted to the construction of the tetrahedral mesh : regions with many nodes have better ray coverage than regions with less nodes . in summary ,",
    "the prior incorporates diverse spatial knowledge about the velocity parameters .",
    "since a precision matrix is defined , which is sparse and positive definite , it provides a computational advantage in sampling from a high - dimensional gaussian distribution as required in our algorithm ( shown in the following sections ) .      to quantify uncertainty ,",
    "we adopt a bayesian approach .",
    "posterior inference for the model parameters is facilitated by a metropolis within gibbs sampler [ @xcite ] ] .",
    "recall the linear model in ( [ stat2 ] ) , @xmath78 where @xmath79 and @xmath80 .",
    "we now specify the prior distribution of @xmath19 as @xmath81 the prior covariance matrix @xmath82 is chosen as @xmath83 since we are interested in modeling positive spatial dependence , we impose that the spatial dependence parameter @xmath84 is the truncated normal distribution a priori , that is , @xmath85 . the priors for the precision scale parameters",
    "@xmath86 , @xmath87 , @xmath88 and @xmath89 are specified in terms of a gamma distribution @xmath90 with density @xmath91 , @xmath92 .",
    "the corresponding first two moments are @xmath93 and @xmath94 , respectively .",
    "the mcmc procedure is derived as follows : the full conditionals of @xmath19 are @xmath95\\\\[-8pt ] & & \\eqntext{\\mbox{with } \\omega_{\\beta}:=\\sigma_{\\beta } ^{-1 } + \\phi x'x , \\bolds{\\xi}_{\\beta}:=\\sigma_{\\beta}^{-1 } \\bolds{\\beta}_0 + \\phi x'\\mathbf{y}}\\end{aligned}\\ ] ] and @xmath96 . for @xmath86 , @xmath87 , @xmath88 and @xmath89 , the full conditionals are again gamma distributed .",
    "the estimation of @xmath84 requires a metropolis  hastings ( mh ) step .",
    "the logarithm of the full conditional of @xmath84 is proportional to @xmath97 for the mh step , we choose a truncated normal random walk proposal for @xmath84 to obtain a new sample , that is , @xmath98 .",
    "we use a cholesky decomposition with permutation to obtain a sample of @xmath19 in ( [ betafullcond ] ) ( section  [ sec3.4 ] ) . the method by @xcite , solving a sparse matrix equation , is not useful . here , computing the determinant of the cholesky factor of @xmath99 is much more efficient than calculating its eigenvalues , due to the size and sparseness of @xmath99 .      to show the relationship between our approach and ridge regression ( also called tikhonov regularization ) , we consider only model 1 . for simplicity",
    "we neglect the notation `` usa '' in ( [ stat1 ] ) .",
    "the analysis is also applicable to model 2 .",
    "let @xmath100 be the corresponding ordinary ridge regression ( orr ) estimate with shrinkage parameter @xmath24 [ @xcite ] .",
    "for given hyperparameters @xmath101 , @xmath89 and @xmath84 , the full conditional of @xmath19 is @xmath102 with @xmath103 and @xmath104 .",
    "the corresponding full conditional mean can therefore be expressed as @xmath105= \\biggl(x'x + \\frac{\\eta}{\\phi } q(\\psi ) \\biggr)^{-1 } \\biggl(x'y + \\frac{\\eta}{\\phi } q ( \\psi ) \\bolds{\\beta}_{0 } \\biggr).\\ ] ] this is close to the modified ridge regression estimator @xmath106 defined in @xcite .",
    "we can see that if @xmath75 , then @xmath107 , which is the equivalent to @xmath24 in the modified ridge regression .",
    "this shows that the prior precision matrix @xmath108 is a regularization matrix with parameter @xmath84 controlling the prior covariance . as discussed in section [ sec3.1 ]",
    ", the prior covariance @xmath109 also varies with the specified weights in ( [ wfuncs ] ) with maximum distance @xmath59 and with number of neighboring nodes . for large @xmath84 or large weights function values , as well as large number of neighbors , the prior variances are small , which well reflects the prior knowledge about the data coverage and parameter uncertainty .",
    "thus , the full conditional mean is close to the prior mean in this case .",
    "since the size of the travel time data requires high - dimensional parameters to be estimated , the traditional method of sampling the parameter vector @xmath19 from @xmath110 directly , as defined in ( [ betafullcond ] ) , is not efficient with respect to computing time .",
    "we instead use a cholesky decomposition of @xmath111 .",
    "since the sensitivity kernel @xmath9 is sparse , and the prior covariance matrix is sparse and positive definite , the matrix @xmath111 remains sparse and symmetric positive definite .",
    "therefore , we can reduce the cost of the cholesky decompositions . for this",
    "we apply an approximate minimum degree ordering algorithm ( amd algorithm ) to find a permutation @xmath112 of @xmath111 so that the number of nonzeros in its cholesky factor is reduced [ @xcite ] . in our case ,",
    "the number of nonzeros of the full conditional precision matrix @xmath111 in ( [ betafullcond ] ) is about @xmath113 of all elements . after this permutation",
    "the nonzeros of the cholesky factor are reduced by @xmath114 compared to the original number of nonzeros .    to sample a multivariate normal distributed vector after permutation",
    ", we follow @xcite .",
    "given the permutation matrix @xmath112 of @xmath115 , we sample a vector @xmath116 , where @xmath117 with @xmath118 a lower triangular matrix resulting from the cholesky decomposition of @xmath119 and @xmath120 a standard normal distributed vector , that is , @xmath121 . the original parameter vector of interest @xmath19 can be obtained after permuting vector @xmath122 again .",
    "@xcite suggested finding a permutation such that the matrix is banded .",
    "however , we found that in our case the amd algorithm is more efficient with regard to computing time .",
    "using matlab built - in functions , the cholesky decomposition with an approximate minimum degree ordering takes 8 seconds on a linux - cluster 8-way opteron with 32 cores , while the cholesky decomposition based on a banded matrix takes 15  seconds . the traditional method without permutation",
    "requires 118.5 seconds .",
    "in this section we examine the performance of our approach for model 1 .",
    "we want to investigate whether the method works correctly under the correct model assumptions and how much influence the prior has on the posterior estimation .",
    "we consider five different prior neighborhood structures of @xmath29 : ( 0 ) independent model of @xmath29 , @xmath123 fixed , that is , @xmath124 ,    \\(1 ) spherical neighborhood structure with reciprocal weight function ,    \\(2 ) ellipsoidal neighborhood structure with reciprocal weight function ,    \\(3 ) spherical neighborhood structure with exponential weight function ,    \\(4 ) ellipsoidal neighborhood structure with exponential weight function .",
    "note that the independent model of @xmath29 corresponds to the bayesian ridge estimator as described in section [ sec3.3 ] . for the weight functions in ( [ wfuncs ] )",
    ", we set @xmath125 km and @xmath126 km for modeling ellipsoidal neighborhood structures , and @xmath127 km for the spherical neighborhood distance .",
    "_ setup _ i : assume the solution by @xcite , denoted as @xmath128 , represents true mantle structure beneath north america .",
    "we use the forward model @xmath129 to compute noise - free , synthetic data .",
    "then , we generate two types of noisy data , that is , @xmath130 with :    \\(a ) gaussian noise ( @xmath131 , @xmath132 ) , ( b ) @xmath133-noise ( @xmath134 , @xmath135 , corresponds to @xmath136 ) .    although we add @xmath133-noise to our synthetic earth model @xmath128 , our posterior calculation is based on gaussian errors . additionally , we compare two priors for @xmath137 to examine the sensitivity of the posterior estimates to the prior choices :    \\(a ) @xmath138 , ( b ) @xmath139 ( spherically symmetric reference model ) .",
    "the priors for the hyperparameters are set as follows : @xmath140 , @xmath141 resulting in expectation and standard deviation of 10 , @xmath142 resulting in expectation of 5 and standard deviation of 1.6 .",
    "_ setup _ ii : in this case we examine the performance under known prior neighborhood structures .",
    "we construct a synthetic true mantle model with two types of known prior neighborhood structures : @xmath143 with @xmath144 and @xmath145 using :    \\(a ) a spherical neighborhood structure for @xmath146 with reciprocal weights ,    \\(b ) an ellipsoidal neighborhood structure for @xmath147 with reciprocal weights . again",
    ", gaussian noise is added to the forward model , that is , @xmath148 , @xmath149 , @xmath150 .",
    "posterior estimation is carried out assuming the five different prior structures .",
    "the number of mcmc iterations for scenarios in setups i and ii is 3000 , thinning is 15 , and burn - in after thinning is 100 . for convergence diagnostics we compute the trace , autocorrelation and estimated density plots as well as the effective sample size ( ess ) using ` coda ` package in r for those samples . according to @xcite ,",
    "the ess is defined by @xmath151 , with the original sample size @xmath152 and autocorrelation @xmath153 at lag @xmath154 .",
    "the infinite sum can be truncated at lag @xmath154 when @xmath155 becomes smaller than 0.05 [ @xcite ] .",
    "to evaluate the results , we use the standardized euclidean norm for both data and model misfits , @xmath156 and @xmath157 , respectively . the function @xmath158 of a vector @xmath159 of mean @xmath36 and covariance @xmath37",
    "is called the _ mahalanobis distance _ , defined by @xmath160 .",
    "to include model complexity , we calculate the deviance information criterion ( dic ) [ @xcite ] .",
    "let @xmath161 denote the parameter vector to be estimated .",
    "furthermore , the likelihood of the model is denoted by @xmath162 , where @xmath163 is the estimated posterior mean of @xmath161 , estimated by @xmath164 with @xmath165 number of independent mcmc samples . according to @xcite and @xcite ,",
    "the deviance is defined as @xmath166 .",
    "the term @xmath167 is a standardizing term which is a function of the data alone and does not need to be known .",
    "thus , for model comparison we take @xmath168 .",
    "the effective number of parameters in the model , denoted by @xmath169 , is defined by @xmath170 - d(\\bar{\\bolds{\\theta}})$ ] .",
    "the term @xmath171 $ ] is the posterior mean deviance and is estimated by @xmath172 .",
    "this term can be regarded as a bayesian measure of fit . in summary ,",
    "the dic is defined as @xmath173 + p_d = d(\\bar{\\bolds{\\theta } } ) + 2p_d$ ] .",
    "the model with the smallest dic is the preferred model under the trade - off of model fit and model complexity .",
    "= =    @lccccd3.2cc@ + & * prior * & & & & & & * mode * + * noises * & * struct * & @xmath174 & @xmath175 & @xmath176 & & * dic * & @xmath177 +   + [ 6pt ] ( a ) gaussian noise & ( 0 ) & 232.28&312.82&308.27&91.30 & 103,748&9.28 + @xmath178 & ( 1 ) & 231.71&258.32&255.74&349.70&103,467&2.89 + @xmath132 , @xmath86 , & ( 2 ) & 231.67&264.81&261.59&200.34&103,442&0.11 + @xmath84 unknown & ( 3 ) & 231.77&263.89&260.96&256.52&103,478&2.70 + & ( 4 ) & 231.70&267.81&264.44&185.04&103,456&0.20 + [ 4pt ] ( b ) @xmath133-noises & ( 0 ) & 227.74&602.35&604.21&46.83 & 112,436&0.57 + @xmath179 & ( 1 ) & 228.58&443.00&443.02&69.50 & 112,226&0.09 + @xmath180 , & ( 2 ) & 228.57&437.58&437.80&57.83 & 112,118&0.01 + @xmath135 , @xmath86 , & ( 3 ) & 228.51&450.67&450.27&62.03 & 112,175&0.11 + @xmath84 unknown & ( 4 ) & 228.52&445.22&445.42&55.87 & 112,126&0.01 + [ 6pt ] + [ 6pt ] ( a ) gaussian noise & ( 0 ) & 234.33&635.99&632.19&50.53&106,563&0.59 + @xmath178 & ( 1 ) & 233.46&458.55&454.42&40.53&105,365&0.09 + @xmath132 , @xmath86 , & ( 2 ) & 233.36&449.35&444.06&42.45&105,200&0.01 + @xmath84 unknown & ( 3 ) & 233.52&466.36&462.04&38.55&105,357&0.12 + & ( 4 ) & 233.40&458.05&452.60&42.71&105,256&0.01 + [ 4pt ] ( b ) @xmath133-noises & ( 0 ) & 226.53&831.85&832.84&40.10&113,023&0.19 + @xmath181 & ( 1 ) & 227.60&599.53&598.99&33.50&112,575&0.03 + @xmath180 , & ( 2 ) & 227.56&596.04&595.78&33.62&112,512&0.00 + @xmath135 , @xmath86 , & ( 3 ) &",
    "227.61&606.60&605.77&33.48&112,541&0.03 + @xmath84 unknown & ( 4 ) & 227.55&607.03&606.69&34.03&112,536&0.00 +    = =    @lccccccd2.2@ + & * prior * & & & & & & + * noises * & * struct * & @xmath174 & @xmath175 & @xmath176 & @xmath182 & * dic * & +   + [ 6pt ] gaussian noise & ( 0 ) & 279.38&575.55&520.63&40.06&129,433&1.09 + @xmath178 & ( 1 ) & 279.82&439.32&389.22&35.83&128,882&0.20 + @xmath132 , & ( 2 ) & 279.78&475.57&422.49&36.95&129,034&0.01 + @xmath183 , & ( 3 ) & 279.96&455.81&404.30&36.23&128,986&0.22 + @xmath184 & ( 4 ) & 279.79&481.30&427.90&37.10&129,066&0.02 + [ 6pt ] + [ 6pt ] gaussian noise & ( 0 ) & 234.71&305.10&292.47&27.71&104,662&11.84 + @xmath178 & ( 1 ) & 234.37&257.34&249.46&26.10&104,262&4.19 + @xmath132 , & ( 2 ) & 234.27&251.55&244.20&24.59&104,152&0.30 + @xmath183 , & ( 3 ) & 234.41&260.03&251.59&25.72&104,284&4.22 + @xmath184 & ( 4 ) & 234.30&253.50&245.76&24.50&104,173&0.53 +      the first two blocks in table [ chap3case1 ] illustrate posterior estimation results for setup i. it shows that the estimation method with ellipsoidal prior structures ( 2 ) and ( 4 ) turn out to be the most adequate , according to the dic criterion .",
    "the standardized data misfit criteria @xmath156 given the estimated posterior mode @xmath185 show similar results in all scenarios .",
    "however , this measure ignores the uncertainty of @xmath29 . the criteria @xmath186 and @xmath187 show the data misfit given the @xmath188 credible interval with lower and upper quantile posterior estimates @xmath189 and @xmath190 , respectively .",
    "these estimates give a range of the data misfit for all possible posterior solutions of @xmath29 and show that methods with independent prior generally yield larger ranges of misfit values than the ones with spatial structures .",
    "this indicates that the credible intervals of methods with spatial priors can fit the data better .",
    "further , methods with spatial priors in setup  i(b ) show smaller model misfit under @xmath157 than ones with independent prior , while in setup i(a ) results with independent priors are better",
    ". generally , estimated posterior modes of @xmath86 vary considerably due to the different prior assumptions .",
    "models with ellipsoidal neighborhood structures have a stronger prior ( in the sense of a smaller prior variance ) than models with spherical neighborhood structure .",
    "similarly , models with reciprocal weights have a stronger regularization toward the prior mean than models with exponential weights .",
    "this means that the posterior estimates of @xmath86 adapt to different prior settings .",
    "moreover , we notice that the estimate of the spatial dependence parameter @xmath84 depends strongly on its prior , as the prior mean is close to the posterior estimates of @xmath84 in all scenarios .",
    "the last two blocks in table [ chap3case1 ] illustrate results from setup ii assuming known spatial structure including hyperparameters .",
    "the dic values indicate that our approach correctly detects the underlying prior structures [ in ( a ) it is prior structure ( 1 ) , in ( b ) it is prior structure ( 2 ) ] .",
    "we can also observe that our approach estimates the hyperparameters correctly .",
    "estimated posterior modes of the parameters from the identified model are close to their true values.=-1    generally , tomographic images illustrate velocity parameters as deviation of the solution from the spherically symmetric reference model ( in % ) .",
    "blue colors represent zones that have faster seismic velocities than the reference earth model , while red colors denote slower velocities .",
    "physically , blue colors usually imply that those regions are colder than the default expectation for the corresponding mantle depth , while red regions are hotter than expected . in our simulation study",
    ", we assumed the true earth to be represented by the solution of @xcite , shown in the left column of figure [ chap3pic1 ] .",
    "the middle and right columns of figure [ chap3pic1 ] illustrate the estimated posterior modes @xmath191 from setup i with ellipsoidal neighborhood structure and reciprocal weight for both gaussian and @xmath133-noises , respectively .",
    "they show that the parameter estimates from gaussian noises are close to the true solution , while the solution from the @xmath133-noises tends to overestimate the parameters . the magnitude of mantle anomalies is overestimated but major structures are correctly recovered .",
    "the same effect can be seen in the last column of figure [ chap3pic1 ] which displays the estimated posterior modes of the tomographic solutions in setup i(b ) .",
    "we have overestimation since the noise is not adequate to the gaussian model assumption .",
    "moreover , we also observe that tomographic solutions with the prior mean @xmath192 are smoother than the ones with the prior mean @xmath193 .     in % from the spherically symmetric reference model .",
    "all other columns show the posterior mode of velocity deviation @xmath194 , estimated using ellipsoidal prior structure with reciprocal weights .",
    "middle columns show results for setup , which uses the prior mean @xmath195 .",
    "right columns show results for setup   assuming prior mean @xmath193 . ]    .",
    "the maps show velocity deviation in % from the reference earth model",
    ". left half shows the results under setup , which uses the prior mean @xmath192 ; right half describes setup , which uses prior mean @xmath196 . first and second rows map out the lower and upper quantiles of the 90% confidence interval .",
    "third row shows the posterior mode of velocity structure @xmath19 , but rendered only in regions that differ significantly from the reference model , according to the 90% confidence interval . ]",
    "figure [ chap3quantiles ] shows estimated credible intervals for the solutions of figure  [ chap3pic1 ] .",
    "credible intervals for solutions with @xmath133-noises are larger than those for the gaussian noises , as indicated by the darker shades of blue / red colors , which denote higher / lower quantile estimates .",
    "this implies that parameter uncertainty is greater if noise does not fit the model assumption .",
    "the same effect can be seen for results with the prior mean @xmath193 .",
    "the bottom row of figure [ chap3quantiles ] maps out how the regions differ from the reference model with @xmath188 posterior probability . for model - conform",
    "gaussian distributed noises and informative prior mean , more regions differ from the reference model with @xmath188 posterior probability than if we added @xmath133-noise or used the less informative prior . in the case of an informative prior and/or correctly modeled noise ,",
    "we achieve more certainty about the velocity deviations from the reference earth model .",
    "in this section we apply our mcmc approach to actually measured travel time data .",
    "the measurements are a subset of those generated by @xcite .",
    "we use the same wave paths , but only measurements made on the broadband waveforms , whereas they further bandpassed the data for finite - frequency measurements and also included amplitude data [ @xcite ] .",
    "most stations are located in the western u.s .",
    ", as part of the largest - ever seismological experiment ( usarray ) , which is still in the process of rolling across the continent from west to east .",
    "numerous tomographic studies have incorporated usarray data  the ones most similar to ours are @xcite , @xcite , @xcite , and @xcite .",
    "all prior studies obtained their solutions through least - squares minimization , which yields no uncertainty estimates .",
    "here we use 53,270 broadband travel time observations to estimate velocity structure under western north america ( over 11,000 parameters ) , plus source corrections for 529 events ( 2116  parameters ) .",
    "we conduct our bayesian inversion following two different scenarios : _ model _ 1 : we only invert for earth structural parameters . for the velocity parameters we assume @xmath197 as in ( [ stat1 ] ) with @xmath198 , @xmath199 and @xmath142 .",
    "_ model _ 2 : we invert for both earth structural parameters and the source corrections .",
    "the prior distributions are set to @xmath200 as in ( [ stat2 ] ) and @xmath82 as defined in ( [ sigmabeta ] ) . for @xmath84 , @xmath89 and @xmath86",
    ", we adopt the same distribution as in model 1 . for the parameters of the source corrections we adopt @xmath201 and @xmath202 .     at node",
    "@xmath203 , @xmath86 and @xmath204 . for 10,000 mcmc iterations",
    "the samples shown in plots are based on a burn - in of 200 and a thinning rate of 25 . ]",
    "we use the same five prior structures ( 0)(4 ) as in the simulation study and run the mcmc algorithm for 10,000 iterations .",
    "the high - dimensional @xmath19 vector can be sampled efficiently in terms of ess with low burn - in and thinning rates thanks to the efficient gibbs sampling scheme in ( [ betafullcond ] ) . however , the hyperparameters , for example , @xmath204 , are more difficult to sample . to achieve a good mixing",
    ", we applied a burn - in of 200 and a thinning rate of 25 ( 393 samples for each parameter ) in our analysis . on average ,",
    "the effective sample size ess values for @xmath205 , @xmath30 and @xmath31 are about @xmath206 , @xmath206 and @xmath207 , respectively , which indicate very low autocorrelations for most of the parameters .",
    "the ess of both @xmath86 and @xmath204 is about 103 , while both @xmath87 and @xmath89 have good mixing characteristics with ess values equal to the sample size , and @xmath88 has ess value equal to 165 .",
    "figure [ convdiag ] shows as examples the parameters @xmath208 at node @xmath203 , @xmath86 and @xmath204 .",
    "the computing cost of our algorithm is about @xmath209 .",
    "sampling model 1 with about 9000 parameters , our algorithm needs 12 hours in 10,000 runs on a 32-core cluster , while under the same condition it needs 38 hours for model 2 .",
    "@lccccccc@ + * prior * & & & & & * mode * & * mode * & * mode * + * struct * & @xmath210 & @xmath211 & @xmath212 & * dic * & @xmath213 & @xmath177 & @xmath214 + ( 0 ) & 228.46&490.92&490.46&102,928&0.40&1.40&@xmath215 + ( 1 ) & 229.14&389.73&390.21&104,096&0.39&0.20&9.63 + ( 2 ) & 228.72&464.73&465.67&103,466&0.40&0.01&9.98 + ( 3 ) & 228.90&430.78&431.34&103,749&0.39&0.17&9.63 + ( 4 ) & 228.74&471.50&472.37&103,408&0.40&0.01&9.98 +    @lccccccd2.2cc@ + * prior * & & & & & * mode * & * mode * & & * mode * & * mode * + * struct * & @xmath210 & @xmath211 & @xmath212 & * dic * & @xmath213 & @xmath177 & & @xmath216 & @xmath217 + ( 0 ) & 225.40&483.96&488.35&93,788&0.49&1.15 & & 0.01&5.01 + ( 1 ) & 225.76&515.29&524.61&94,993&0.48&0.10&9.63 & 0.01&4.53 + ( 2 ) & 225.48&498.96&501.45&94,374&0.48&0.00&9.55 & 0.01&4.70 + ( 3 ) & 225.61&503.20&512.01&94,669&0.48&0.11&9.63 & 0.01&4.53 + ( 4 ) & 225.44&496.69&498.97&94,312&0.49&0.01&10.00&0.01&4.70 +    table [ chap41 ] shows the results from model 1 ( estimation of earth structure ) and model  2 ( earth structure plus source corrections ) . for both models , results from the independent prior structures , corresponding to the bayesian ridge estimator , provide the best fit according to the dic criterion .",
    "we also run the model 1 with prior mean @xmath218 ( the spherically symmetric reference model ) and different covariance structures ( 0)(2 ) .",
    "the dic results for priors ( 0 ) , ( 1 ) and ( 2 ) are 103,100 , 103,700 and 103,370 , respectively .",
    "two reasons may explain the selection of prior  ( 0 ) : ( 1 )  the data has generally more correlation structure than the i.i.d .",
    "gaussian assumption , which can not be solely explained by the spatial prior structure of the @xmath19-fields .",
    "however , in our simulation study where different prior structures and the corrected data error are applied ( table [ chap3case1 ] ) , the dic was able to identify the correct models ; ( 2 ) since the data are noisy , fitting could be difficult without a shrinkage prior . the prior in ( 0 ) can be compared to shrinkage in the ridge regression , which is the limiting case of priors in ( 1 ) to ( 4 ) .",
    "priors in ( 1 ) to ( 4 ) do not shrink the solutions of @xmath19-fields as much as prior ( 0 ) .",
    "they better reflect the uncertainty since the prior covariances in ( 1)(4 ) are larger than variances in prior ( 0 ) in regions that have no data ( no neighboring nodes ) , and smaller in regions with lots of data ( lots of neighboring nodes ) .",
    "furthermore , the standardized data misfit criteria do not show much difference between models with different prior specifications . according to the estimated @xmath188 credible interval ,",
    "estimates using spherical prior structure show a smaller range of data misfit in model 1 , whereas in model 2 , the independence prior shows a better result . since our method assumes i.i.d .",
    "gaussian errors , the resulting residuals might not be optimally fitted as expected . with regard to computational time",
    ", the independent prior model has a definite advantage over other priors in both models 1 and 2 .",
    "the general advantage of our bayesian method is that the independent model yields an estimate given as the ratio between the variance of the data and the variance of the priors corresponding to ridge estimates with _ automatically chosen shrinkage _ described in section [ sec3.3 ] , whereas in @xcite , @xcite , @xcite and all other prior work , the shrinkage parameter ( strength of regularization ) had to be chosen by the user a priori .",
    "at a few selected model nodes , whose locations and depths are indicated on the map .",
    "unit on the x - axes is velocity deviation in % .",
    "dashed lines : prior density , the prior variance can be very small if number of neighbors is large .",
    "solid lines : posterior density with @xmath219 credible intervals . ]    ) . left columns : estimated posterior mode of velocity deviation , for the scenario of model 2 .",
    "right columns : same posterior mode , but rendered are only regions that differ from the reference model with @xmath188 posterior probability . ]",
    "figure [ postmodepos ] shows the estimated posterior and prior densities of parameters in model 2 , at four different locations of varying depth .",
    "we see that parameters at locations with good ray coverage , for example , node 5400 and node 3188 , have smaller credible intervals than parameters at locations with no ray coverage , for example , node 5564 and node 995 beneath the uninstrumented oceans .",
    "geologically , the regions between node 5400 and node 3188 are well known to represent the hot upper mantle , where seismic waves travel slower than the reference velocity .",
    "this is consistent with our results in figure [ postmodepos ] : the fact that @xmath220 does not fall inside the @xmath188 credible intervals indicates a velocity deviation from the spherically symmetric reference model with high posterior probability .",
    "figure [ postmodepos ] shows that the posterior is more diffuse than the prior .",
    "as mentioned in section [ sec3.1 ] , the spatial prior for @xmath19 depends on distance of neighboring nodes , number of neighbors and orientation .",
    "the variance can be very small if the number of neighbors is very large , as shown in figure [ weightsfunc ] .",
    "incorporating data , the information about @xmath19 is updated and thus may yield more diffuse posteriors than the priors , as we see here .",
    "the left half of figure [ chap4model2 ] shows the estimated posterior modes of mantle structure obtained by model 2 , for independent and for ellipsoidal priors with reciprocal weights .",
    "the right half of figure [ chap4model2 ] extracts only those regions that differ from the reference model according to the @xmath188 credible interval .",
    "the ellipsoidal prior results in higher certainty of velocity deviations at a depth of 200 km compared to the independence prior . at a depth of 400 km ,",
    "the credible regions resemble each other more strongly .",
    "this confirms geological arguments that deeper regions of the mantle are more homogeneous and do not differ as much from the spherically symmetric reference model as shallower regions .",
    "many lines of geoscientific investigations provide independent confirmation of the significantly anomalous regions of figure [ chap4model2 ] .",
    "the red areas map out the hot upper mantle under the volcanic , extensional basin and range province and yellowstone ; the blue anomalies map out the western edge of the old and cool north american craton .",
    "the overall comparison of our solutions to earlier least - squares inversions , for example , the model by @xcite shown in the left column of figure  [ chap3pic1 ] , confirms that bayesian inversion successfully retrieves the major features of mantle structure .",
    "the images are similar , but the major advantage and novelty of our approach is that it also quantifies uncertainties in the solution ( which we have chosen to visualize as credible intervals here ) .",
    "uncertainty quantification in underdetermined , large inverse problems is important , since a single solution is not sufficient for making conclusive judgements .",
    "two central difficulties for mcmc methods have always been the dimensionality of the problem ( number of parameters to sample ) or the evaluation of the complex physical forward model ( nonlinear problems ) in each mcmc iteration [ @xcite ] .",
    "consider the model @xmath221 with the physical forward model @xmath222 , high - dimensional parameter @xmath19 and error @xmath223 .",
    "in general , if the physical problem is linear @xmath224 and the full conditional of @xmath19 is gaussian , efficient sampling from the high - dimensional gaussian conditional distribution is essential for the exploration of model space . in this case",
    "the error @xmath223 need not necessarily be gaussian , but may be @xmath133 or skewed-@xmath133 distributed [ @xcite ] , or a gaussian error with a spatial correlation such as considered in @xcite . given a sparse posterior precision matrix [ e.g. , ( [ betafullcond ] ) ] , efficient sampling from a multivariate normal can be carried out by cholesky decomposition of a permuted precision matrix as discussed in @xcite or @xcite , by using an approximate minimum - degree ordering algorithm .",
    "a further improvement to the current sampling approach might be to apply the krylov subspace method from @xcite .",
    "this would require substantial implementation efforts and is the subject of further research .",
    "if the forward matrix or the prior precision matrix is not sparse , a dense posterior precision matrix for @xmath19 will result . in this case",
    "our sampling scheme is inefficient , but the model - space reduction method developed by @xcite might be used instead .",
    "they exploit the low - rank structure of the preconditioned hessian matrix of the data misfit , involving eigenvalue calculations .",
    "however , this approximation quantifies uncertainty of large - scale linear inverse problems only for known hyperparameters , thus ignoring uncertainty in those parameters .",
    "eigenvalue calculation in each mcmc step can be time consuming and prohibitive for hierarchical models with unknown hyperparameters when the posterior covariance matrix in every mcmc step changes . here",
    "additional research is needed .",
    "if the full conditionals can not be written as gaussian [ this case includes the cases of a nonlinear @xmath222 , a non - gaussian prior of @xmath19 or non - gaussian , nonelliptical distributed errors ] , using the standard mh algorithm to sample from the high - dimensional posterior distribution is often computationally infeasible . constructing proposal density that provides a good approximation of the stationary distribution while keeping the high - dimensional forward model @xmath222 inexpensive to evaluate has been the focus of the research over the past years : @xcite have drawn samples from an approximate posterior density on a reduced parameter space using a projection - based reduced - order model . in the adaptive rejection sampling technique by @xcite ,",
    "the exact posterior density is evaluated only if its approximation is accepted .",
    "the stochastic newton approach proposed by @xcite approximates the posterior density by local hessian information , thus resulting in an improvement of the langevin mcmc by @xcite .",
    "other random - walk - free , optimization - based mcmc techniques for improving the proposal and reducing correlation between parameters have been developed , such as hamiltonian monte carlo ( hmc ) [ @xcite ] , adaptive monte carlo ( am ) [ @xcite ] and several variations , for example , delay rejection am ( dram ) [ @xcite ] , differential evolution mc ( demc ) [ @xcite ] and differential evolution adaptive metropolis ( dream ) [ @xcite ] , just to mention a few .",
    "however , mcmc sampling of high - dimensional problems still requires a massive amount of computing time and resources .",
    "for example , the quasi three - dimensional nonlinear model of @xcite contains about 9000 parameters on a @xmath225 grid .",
    "we expect a long computing time since they use standard mcmc sampling methods .",
    "the example by @xcite shows that their algorithm achieves a significant improvement in both computing time and efficiency of parameter space sampling for a large nonlinear system of pdes that includes about 10,000 parameters .",
    "however , their algorithm gives 11,200 iterations in about 40 days , while our problem requires only 38 hours ( on a 32-core cluster ) for the same number of iterations for about 11,000 parameters .",
    "while the future may be in effective uncertainty quantification of nonlinear physical problems using model reduction and optimization techniques , the computing time and resources at the moment are too demanding to explore the large model space .",
    "this paper demonstrates effective bayesian analysis tailored to a realistically large seismic tomographic problem , featuring over 11,000 structural and source parameters .",
    "we deliver a precise uncertainty quantification of tomographic models in terms of posterior distribution and credible intervals using the mcmc samples , which allows us to detect regions that differ from the reference earth model with high posterior probability .",
    "our approach is the first to solve seismic tomographic problems in such high dimensions on a fine grid , and thus provides ground work in this important research area .",
    "the authors acknowledge two referees , the associate editor and the editor for helpful remarks and suggestions which led to a significant improved manuscript .",
    "the authors would like to thank the support of the leibniz - rechenzentrum in garching , germany ."
  ],
  "abstract_text": [
    "<S> we apply a linear bayesian model to seismic tomography , a high - dimensional inverse problem in geophysics . </S>",
    "<S> the objective is to estimate the three - dimensional structure of the earth s interior from data measured at its surface . since this typically involves estimating thousands of unknowns or more , it has always been treated as a linear(ized ) optimization problem . here </S>",
    "<S> we present a bayesian hierarchical model to estimate the joint distribution of earth structural and earthquake source parameters . </S>",
    "<S> an ellipsoidal spatial prior allows to accommodate the layered nature of the earth s mantle . with our efficient algorithm </S>",
    "<S> we can sample the posterior distributions for large - scale linear inverse problems and provide precise uncertainty quantification in terms of parameter distributions and credible intervals given the data . </S>",
    "<S> we apply the method to a full - fledged tomography problem , an inversion for upper - mantle structure under western north america that involves more than 11,000 parameters . in studies on simulated and real data , </S>",
    "<S> we show that our approach retrieves the major structures of the earth s interior as well as classical least - squares minimization , while additionally providing uncertainty assessments .    , </S>"
  ]
}