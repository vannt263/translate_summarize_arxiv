{
  "article_text": [
    "compressive sensing ( cs)@xcite is a promising technique that recovers a high - dimensional signal represented by a few non - zero elements using far - fewer measurements than the signal dimension .",
    "this technique has immense applications ranging from image compression to sensing systems requiring lower power consumption .",
    "the mathematical heart of cs is to solve a under - determined linear system of equations by harnessing an inherent sparse structure in the signal .",
    "let @xmath4 and @xmath5 be a real - valued spare signal vector and a compressive sensing matrix that linearly projects a high - dimensional signal in @xmath6 to a low - dimensional signal in @xmath7 where @xmath8 , respectively . formally , the noiseless cs problem is to reconstruct sparse signal vector @xmath9 by solving the following @xmath10-minimization problem : @xmath11 where the collection of non - zero elements positions in @xmath9 , @xmath12 , is defined as @xmath13\\}$ ] , with cardinality @xmath14 .",
    "unfortunately , computational complexity for solving this problem is np - hard , implying that , in practice , it is computationally infeasible to obtain the optimal solution when @xmath0 is very large .",
    "there exists many practical algorithms that perfectly reconstruct the sparse signal with polynomial time computational complexity , provided that a measurement matrix has a good incoherence property .",
    "greedy sparse signal recovery algorithms @xcite became popular due to the computational efficiency in implementing these algorithms .",
    "in practice , obtaining the measurement vector @xmath15 with infinite precision is infeasible .",
    "this is because , in many image sensors or communication systems , signal acquisition is performed by using analog - to - digital converters ( adcs ) that quantizes each measurement to a predefined value with a finite number of bits .",
    "this quantization process makes difficulty in recovering sparse signals , as it might give rise to significant measurement errors , especially when the number of quantization bits is small . numerous sparse signal recovery algorithms with quantized measurements in @xcite",
    "were proposed to overcome the impact of the quantization errors . in particular , under the premise that each measurement is quantized with just _",
    "one - bit _",
    "( i.e. , an extreme case of quantization errors ) , a compressive sensing problem was introduced in @xcite . for given @xmath16 and @xmath9 ,",
    "the measurements are obtained using their signs as @xmath17 where the measurement vector is in the boolean cube , i.e. , @xmath18 .",
    "it was shown that , with the one - bit measurements , sparse signal vectors with unit - norm can be recovered with high probability by convex optimization techniques @xcite or iterative greedy algorithms @xcite .    in this paper",
    ", we study a generalized compressive sensing problem in which each measurement is quantized with @xmath2 levels where @xmath2 ( @xmath19 ) is a prime number . we also consider a quantized source signal , i.e. , the non - zero elements of a sparse signal are chosen from a set of integer values , i.e. , @xmath20 .",
    "such setting can be found in many applications .",
    "for instance , in a random access wireless system , active users among all users in the system send quadrature amplitude modulated symbols ( i.e. , @xmath2-level quantized signals ) to a receiver , and it detects the active users signals using a @xmath2-level adc .",
    "a fundamental question we ask in this paper is : what is the sufficient condition for the perfect recovery of the integer sparse signal with @xmath2-level per measurement in the presence of gaussian noise ? to shed light on the answer to this question , we develop a new sparse signal recovery framework , which is referred to as `` _ _ coded compressive sensing__. '' the core idea of coded compressive sensing is to exploit both source and channel coding techniques in information theory .",
    "the proposed scheme consists of two cascade encoding and decoding phases .",
    "the first phase of encoding is the _ compression phase _ , in which a high dimensional sparse signal vector in @xmath21 is compressed to a low dimensional signal vector using a parity check matrix of a maximum distance separable ( mds ) linear code .",
    "the second phase is the _ dictionary coding phase_. in this phase , each dictionary vector ( each column vector of the parity check matrix ) is encoded to a coded dictionary vector by exploiting a ( near ) capacity - achieving lattice code for a gaussian channel .",
    "we propose a two - stage decoding method called `` _ _ compute - and - recover__. '' in the first stage of decoding , a linear combination of the encoded dictionary vectors corresponding the non - zero elements in @xmath22 is decoded .",
    "we call this as the dictionary equation decoding stage that produces a noise - free measurement vector .",
    "once the dictionary equation is perfectly decoded , in the second stage of decoding , we apply syndrome decoding to the equivalent finite field representation of the dictionary equation for the sparse signal recovery . using the proposed scheme ,",
    "we derive a lower bound of the number of measurements for the perfect recovery as a function of important system parameters : the quantization level @xmath2 , the sparsity level @xmath3 , the signal dimension @xmath0 , and the number of measurements @xmath1 . considering @xmath23 as a special case ,",
    "we compare the proposed scheme with existing algorithms developed for the one - bit compressive sensing problem @xcite .",
    "numerical results show that the proposed scheme outperforms than binary iterative hard thresholding ( bith ) @xcite in a low signal - to - noise ratio ( snr ) regime .",
    "in this section , we present a coded compressive sensing framework for an integer sparse signal recovery in the presence gaussian noise .",
    "we are interested in a sparse signal detection problem from a compressed measurement in the presence of noise .",
    "let @xmath24^{\\top } \\in \\mathbb{z}_p^{n\\times 1}$ ] be an unknown sparse signal vector whose sparsity level is equal to @xmath3 , i.e. , @xmath25 .",
    "the measurement equation of quantized compressed sensing is given by @xmath26 where @xmath27 denotes the @xmath2-level _ scalar _ quantizer that applied component - wise , and @xmath28^{\\top}$ ] and @xmath29^{\\top}$ ] denote the measurement and noise vector , respectively .",
    "all entries of the noise vector are assumed to be independent and identically distributed ( iid ) gaussian random variables with zero mean and variance @xmath30 , i.e. , @xmath31 for all @xmath32 .",
    "our objective is to reliably estimate the unknown sparse signal vector @xmath9 given @xmath15 in the presence of gaussian noise @xmath33 , by appropriately constructing a linear measurement matrix @xmath16 and @xmath2-level scalar quantizer @xmath27 .",
    "we define a sparse signal recovery decoder @xmath34 , which maps the measurement vector @xmath15 to an estimate @xmath35 of the original sparse signal vector @xmath9 .",
    "it is said that the average probability of error is at most @xmath36 if @xmath37 \\leq \\epsilon$ ] .      a linear encoding function",
    "is represented by a sensing matrix @xmath5 , which linearly maps the @xmath0-dimensional sparse vector to an @xmath1-dimensional output vector , where @xmath38 .",
    "we construct the sensing matrix @xmath16 using the proposed idea , which is referred to as _",
    "dictionary coding .",
    "_      let @xmath39 be a parity check matrix of a @xmath40-ary @xmath41 $ ] mds code , where @xmath40 is a prime power @xmath42 for any positive integer @xmath43 .",
    "in this paper we focus on a @xmath40-ary @xmath41 $ ] reed - solomon ( rs ) code with field size constraint @xmath44 .",
    "thus , the parity check matrix of the rs code has @xmath45 full - rank where @xmath46",
    ". the @xmath47th column vector of @xmath48 is denoted by @xmath49 where @xmath50 $ ] .",
    "we define the one - to - one mapping @xmath51 that maps each element of @xmath52 into an @xmath53-length word in @xmath54 . for instance , when @xmath23 , it is possible to express an element of @xmath55 as a binary vector of length @xmath53 . using this mapping",
    ", we can transform each dictionary vector @xmath56 into @xmath57 where @xmath58 .",
    "the transformed column vector @xmath59 is referred to as the @xmath47th dictionary basis vector .",
    "dictionary coding is to map a dictionary basis vector in @xmath60 into a lattice point in @xmath7 using lattice encoding where @xmath61 .",
    "we commence by providing a brief background for a lattice construction .",
    "let @xmath62 be the ring of gaussian integers and @xmath2 be a gaussian prime .",
    "let us denote the addition over @xmath54 by @xmath63 , and let @xmath64 be the natural mapping of @xmath65 onto @xmath66 .",
    "we recall the nested lattice code construction given in [ 14 ] .",
    "let @xmath67 be a lattice in @xmath68 , with full - rank generator matrix @xmath69 .",
    "let @xmath70 denote a linear code over @xmath54 with block length @xmath1 and dimension @xmath71 , with generator matrix @xmath72 where @xmath61 .",
    "the lattice @xmath73 is defined through `` _ _ construction a _ _ '' ( see @xcite and references therein ) as @xmath74 where @xmath75 is the image of @xmath76 under the mapping function @xmath77 .",
    "it follows that @xmath78 is a chain of nested lattices , such that @xmath79 and @xmath80 .    for a lattice @xmath81 and @xmath82",
    ", we define the lattice quantizer @xmath83 , the voronoi region @xmath84 and @xmath85\\mod \\lambda = { \\bf r}-{\\rm q}_\\lambda({\\bf r})$ ] . for @xmath81 and @xmath73",
    "given above , we define the lattice code @xmath86 with rate @xmath87 .",
    "construction a provides an encoding function that maps a dictionary basis vector @xmath88 into a codeword in @xmath89 .",
    "notice that the set @xmath90 is a system of coset representatives of the cosets of @xmath81 in @xmath73 .",
    "hence , the encoding function @xmath91 is defined by @xmath92 \\mod \\lambda,\\]]where @xmath93 consequently , the @xmath47th codeword vector @xmath94 is produced by the encoding function @xmath95 where each dictionary vector is chosen from lattice codewords in the nested lattice codebook @xmath89 , i.e. , @xmath96 . using this construction method",
    ", we have a linear sensing matrix consisting of @xmath0 column vectors as @xmath97.\\end{aligned}\\ ] ] the average power of each codeword is assumed to be @xmath98\\leq 1.\\ ] ] finally , in this paper , we choose the shaping lattice @xmath81 as a _",
    "cubic _ lattice , namely @xmath99 , which enables that a lattice decoding is implemented by a scalar quantizer ( see @xcite for more details ) . here",
    ", @xmath100 is chosen to satisfy the power constraint in ( [ eq : power_const ] ) as @xmath101 .",
    "then , the element - wise snr is defined as @xmath102      we propose a @xmath2-level scalar quantizer called _ sawtooth transform _ as depicted in fig .",
    "[ fig:1 ] , which can be implemented by the modulo operation followed by the scalar quantization as @xmath103 \\mod \\tau\\mathbb{z}.\\ ] ]",
    "in this section , we characterize the sufficient condition for the exact recovery of an integer sparse signal vector .",
    "the following theorem is the main result of this paper .",
    "[ th1 ] the proposed coded compressive sensing method perfectly reconstructs the sparse signal vector @xmath104 with @xmath105 , with vanishing error probability for large enough @xmath0 , provided that @xmath106where @xmath107 represents a @xmath2-ary entropy function and @xmath108 denotes an effective quantized noise obtained from the @xmath2-level quantizer as @xmath109 \\mod p\\mathbb{z } \\right)$ ] , where @xmath110 .",
    "the proof of this theorem is based on the proposed two - stage decoding method called `` _ _ compute - and - recover _ _ '' . in the first stage",
    ", we decode an integer linear combination of coded dictionary vectors by removing noise , which essentially yields a finite - field sparse signal recovery problem . in the second stage",
    ", we apply syndrome decoding over the finite - field to reconstruct the sparse signal vector .      in this stage , we decode a noise - free measurement vector @xmath111 from @xmath112 using the key property of a lattice code . recall that dictionary vector is a lattice code ; thereby , any integer - linear combination of lattice codewords is again a lattice codeword @xcite .",
    "thus we have that @xmath113 \\mod \\lambda \\in \\mathcal{l } $ ] due to @xmath114 .",
    "we will first exploit this fact to decode a noise - free measurement vector .",
    "letting @xmath115 be the support set of @xmath9 , the noisy measurement vector with the @xmath2-level quantizer is given by @xmath116 \\mod \\tau\\mathbb{z},\\end{aligned}\\ ] ] where the second equality follows from ( [ eq : quantizer ] ) .",
    "we transform this noisy and quantized measurement into a noiseless finite - field measurement as follows . from the quantized sequence @xmath15",
    ", we produce the sequence @xmath117 with components @xmath118 \\mod p\\mathbb{z}\\right),\\end{aligned}\\ ] ] for @xmath119 .",
    "since @xmath120 by construction , and using the obvious identity @xmath121 with @xmath122 and @xmath123 , we arrive at @xmath124 where the elements of the discrete additive noise vector @xmath125 are given by @xmath126 \\mod p\\mathbb{z}\\right),\\]]for @xmath119 .",
    "since , by linearity , @xmath127 is a codeword of @xmath76 , the above channel can be considered as a point - to - point channel with discrete additive noise over @xmath54 .",
    "then , we can reliably decode @xmath128 if @xmath129 this is an immediate consequence of the well - known fact that linear codes achieve the capacity of symmetric discrete memoryless channel@xcite . from this result",
    ", we can obtain that the sufficient condition for the perfect recovery of the noise - free measurement vector is @xmath130      recall that , in the first stage , the decoder has recovered the dictionary equation , i.e. , @xmath131 where @xmath132 . using the linearity of code @xmath76",
    ", we have : @xmath133 where @xmath134 represents the effective measurement vector in @xmath135 . as a result",
    ", the measurement equation can be equivalently rewritten in a matrix form over @xmath54 as @xmath136 where @xmath137 denotes the effective sensing matrix whose column vectors are selected from dictionary basis vectors @xmath138 .",
    "we would like to recover @xmath139 from the effective measurement vector @xmath140 in a noiseless setting and using one - to - one mapping @xmath141 .",
    "unlike the sparse recovery algorithm in a finite field in @xcite , we apply a syndrome decoding method @xcite .",
    "syndrome decoding harnesses the fact that there is a bijection mapping between a sparse signal ( error ) vector @xmath9 and the effective measurement ( syndrome ) vector @xmath134 , provided that @xmath9 contains at most @xmath142 non - zero entries , i.e , @xmath143 . recall that , in our construction , the @xmath47th dictionary vector @xmath144 in @xmath145 was generated from the mapping @xmath146 where @xmath147 , i.e. , @xmath148 .",
    "since @xmath149 is bijection , applying the inverse mapping function @xmath150 , we obtain the resultant measurement equation over @xmath52 as @xmath151 where the second equality follows from @xmath152 and the last equality is due to the one - to - one mapping between @xmath145 and @xmath48 by @xmath153 . since @xmath48 was selected from the parity - check matrix of the @xmath42-ary @xmath41$]-rs code whose minimum distance , @xmath154 achieves a singleton bound , i.e. , @xmath155 . as a result ,",
    "the syndrome decoding method allows us to recover the sparse signal perfectly , provided that @xmath156 putting two inequalities in and together and using the fact @xmath147 and @xmath157 , the number of required measurements for the sparse signal recover in the presence of gaussian noise boils down to @xmath158 which completes the proof .",
    "* remark 1 ( decoding complexity ) * : the proposed two - stage decoding method can be implemented with a polynomial time computational complexity . in the first stage",
    ", the lattice equation can be efficiently decoded with the @xmath2-level scalar quantizer in @xcite and the successive decoding algorithm of the polar code @xcite , which essentially uses @xmath159 operations .",
    "syndrome decoding used in the second stage can be implemented with polynomial time computational complexity algorithms such as berlekamp - massey algorithm , which requires @xmath160 operations in @xmath52 .",
    "considering @xmath52 is a vector space over @xmath54 , this amount corresponds to @xmath161 operations in @xmath54 . since @xmath162 and @xmath157 , the overall computational complexity of the proposed method is at most @xmath163 operations for recovery .",
    "* remark 2 ( universality of the measurement matrix ) * : the proposed coded compressive sensing method is universal , as it is possible to recover all @xmath3 sparse signals using a fixed sensing matrix @xmath16 .",
    "this universality is practically important , because one may needs to randomly construct a new measurement matrix @xmath16 for each signal .",
    "some existing one - bit compressive sensing algorithms @xcite do not hold the universality property .",
    "* remark 3 ( non - integer sparse signal case ) * : one potential concern for our integer sparse signal setting is that a sparse signal can have real value components in some applications .",
    "this concern can be resolved by exploiting an integer - forcing technique in which @xmath4 is quantized into an integer vector @xmath164 and interpreting the residual @xmath165 as additional noise .",
    "then , the effective measurements are obtained as @xmath166 where @xmath167 denotes effective noise .",
    "utilizing this modified equation , we are able to apply the proposed coded compressive sensing method to estimate the integer approximation @xmath168 . assuming the non - zero values in @xmath9 are bounded as @xmath169 for some @xmath170 , we conjecture that the proposed scheme guarantees to recover the sparse signal with a bounded estimation error @xmath171 with an increased number of measurements than that in theorem 1 . the rigorous proof of this conjecture will be provided in our journal version @xcite .    *",
    "remark 4 ( noiseless one - bit compressive sensing ) * : one interesting scenario is that when a one - bit quantizer and a binary signal are used . in the case of noise - free , the number of required measurements for the perfect recovery is lower bounded by @xmath172     with one - bit and noisy measurements.,width=326 ]",
    "in this section , we provide the signal recovery performance of the proposed coded compressive sensing method for @xmath23 , i.e. , one - bit compressive sensing , by numerical experiments .    to test the proposed algorithm ,",
    "@xmath3-sparse binary vector @xmath173 is generated in which the non - zero positions of @xmath9 is uniformly distributed between 1 and 511 . a fixed binary sensing matrix @xmath174",
    "is designed by the concatenation of compression matrix @xmath175 and the generator matrix @xmath176 of polar code ( which is completely determined by the rate - one arikan s kernal matrix and the information set @xcite ) , as illustrated in fig .",
    "[ fig:1 ] .",
    "in particular , the binary compression matrix @xmath145 is obtained from @xmath48 that is the parity check matrix of @xmath177-ary @xmath178 $ ] rs code with the minimum distance of @xmath179 .",
    "therefore , it is perfectly able to perform syndrome decoding up to the sparsity level of 5 in a noiseless case .",
    "in addition , we pick the binary polar generator matrix @xmath180 of code rate @xmath181 .",
    "we evaluate the perfect recovery probability , i.e. , @xmath182 $ ] of the sparse signal in the presence of noise with variance @xmath183 when the proposed algorithm is applied .",
    "we compare our coded compressive sensing algorithm with the following two well - known one - bit compressive sensing algorithms with some modification for a binary signal .",
    "* convex optimization : a variant of the @xmath184-minimization method proposed in @xcite for a binary sparse signal , which is summarized in table [ table1 ] ; * binary iterative hard thresholding ( biht ) : a heuristic algorithm in @xcite with some modifications for the binary signal recovery as in step 3 ) and 4 ) of table [ table1 ] .",
    "for the two modified reference algorithms , we use a gaussian sensing matrix @xmath185 whose elements are drawn from iid gaussian distribution @xmath186 . for each setting of @xmath1 , @xmath0 , @xmath3 , and @xmath183 , we perform the recovery experiment for 500 independent trials , and compute the average of perfect recovery rate .",
    ".a convex optimization algorithm for binary sparse signal [ cols= \" < \" , ]     [ table1 ]    .,width=297 ]    fig . [ fig:2 ] plots the perfect recovery probability versus snr for each algorithm , when @xmath187 , @xmath188 , and @xmath189 . as can be seen in fig .",
    "[ fig:2 ] , the proposed method outperforms biht significantly in terms of the perfect signal recovery performance .",
    "specifically , biht is not capable of recovering the signal with high probability until snr=12 db , because there are a lot of sign flips in the measurements due to noise .",
    "whereas the proposed algorithm is robust to noise ; thereby it recovers the signal with probability one when snr is 6 db above .",
    "the convex optimization approach provides a better performance than the other algorithms ; yet , it requires the computational complexity order of @xmath190 , which is much higher than that of the proposed one .",
    "in this paper , we proposed a novel compressive sensing framework with noisy and quantized measurements for integer sparse signals . with this framework we derived the sufficient condition of the perfect recovery as a function of important system parameters .",
    "considering one - bit compressive sensing as a special case , we demonstrated that the proposed algorithm empirically outperforms the existing greedy recovery algorithm .",
    "n. lee , `` map support detection for greedy sparse signal recovery algorithms in compressive sensing , '' aug .",
    "2015 ( available at : http://arxiv.org / abs/1508.00964 ) .",
    "e. j. cand@xmath191s , j. romberg , and and t. tao , `` stable signal recovery for incomplete and inaccurate measurements , '' vol .",
    "59 , pp . 1207 - 1223 ,"
  ],
  "abstract_text": [
    "<S> in this paper , we propose _ coded compressive sensing _ that recovers an @xmath0-dimensional integer sparse signal vector from a noisy and quantized measurement vector whose dimension @xmath1 is far - fewer than @xmath0 . </S>",
    "<S> the core idea of coded compressive sensing is to construct a linear sensing matrix whose columns consist of lattice codes . </S>",
    "<S> we present a two - stage decoding method named _ compute - and - recover _ to detect the sparse signal from the noisy and quantized measurements . in the first stage , we transform such measurements into noiseless finite - field measurements using the linearity of lattice codewords . in the second stage , </S>",
    "<S> syndrome decoding is applied over the finite - field to reconstruct the sparse signal vector . </S>",
    "<S> a sufficient condition of a perfect recovery is derived . </S>",
    "<S> our theoretical result demonstrates an interplay among the quantization level @xmath2 , the sparsity level @xmath3 , the signal dimension @xmath0 , and the number of measurements @xmath1 for the perfect recovery . considering 1-bit compressive sensing as a special case , </S>",
    "<S> we show that the proposed algorithm empirically outperforms an existing greedy recovery algorithm . </S>"
  ]
}