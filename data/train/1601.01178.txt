{
  "article_text": [
    "a mixture density is traditionally represented as a weighted average of densities from standard families , i.e. , @xmath0 each component of the mixture is characterised by a component - wise parameter @xmath1 and the weights @xmath2 of those components translate the importance of each of those components in the model .",
    "this particular representation gives a separate meaning to each component through its parameter @xmath1 , even though there is a well - known lack of identifiability in such models , due to the invariance of the sum by permutation of the indices .",
    "this issue relates to the equally well - known ",
    "label switching \" phenomenon in the bayesian approach to the model , which pertains both to inference and to simulation of the corresponding posterior @xcite . from this bayesian viewpoint ,",
    "the choice of the prior distribution on the component parameters is quite open , the only constraint being that the corresponding posterior is proper @xcite . @xcite and @xcite discussed the alternative approach of _ imposing _ proper posteriors on improper priors by banning almost empty components from the likelihood function . while consistent , this approach induces dependence between the observations , higher computational costs and is not handling overfitting very well .",
    "it has therefore seen little following .",
    "the prior distribution on the weights @xmath2 is equally open for choice , but a standard version is a dirichlet distribution with common hyperparameter @xmath3 , @xmath4 .",
    "recently , @xcite demonstrated that the choice of this hyperparameter @xmath3 relates to the inference on the total number of components , namely that a small enough value of @xmath3 manages to handle over - fitted mixtures in a convergent manner . in a bayesian non - parametric modelling , @xcite showed that the prior on the weights may have a higher impact when inferring about the number of components , relative to the prior on the component - specific parameters .",
    "as indicated above , the prior distribution on the @xmath1 s has received less attention and conjugate choices are most standard , since they facilitate simulation via gibbs samplers @xcite if not estimation , since posterior moments remain unavailable in closed form .",
    "in addition , @xcite among others proposed data - based priors that derive some hyperparameters as functions of the data , towards an automatic scaling of such priors .",
    "an r package , bayesm @xcite incorporates some of those ideas . in the case when @xmath5 is a location - scale parameter , @xcite proposed a reparameterisation of that express each component as a local perturbation of the previous one , namely @xmath6 @xmath7 with @xmath8 and @xmath9 being the reference values .",
    "based on this reparameterisation , @xcite established that a particular improper prior on @xmath10 still leads to a proper prior .",
    "we propose here to modify further this reparameterisation towards using the global mean and global variance of the mixture distribution as reference location and scale , respectively .",
    "this modification has foundational consequences in terms of using improper and non - informative priors over mixtures , in sharp contrast with the existing literature ( see , e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "bayesian computing for mixtures covers a wide variety of proposals , starting with the introduction of the gibbs sampler @xcite , some concerned with approximations @xcite and mcmc features @xcite , and others with asymptotic justifications , in particular when over - fitting mixtures @xcite , but most attempting to overcome the methodological hurdles in estimating mixture model @xcite .    in this paper , we introduce and study the global mean - variance reparameterisation ( section [ sec : mix ] ) , which main consequence is to constrain all other parameters to a compact space .",
    "we study several possible parameterisations of that kind and demonstrate that the improper jeffreys - like prior associated with them is proper . in section [ sec : tion ] , we propose some mcmc implementation to estimate the parameters of the mixture , discussing label switching ( section [ sec : witch ] ) and its resolution by tempering .",
    "extensions to non - gaussian mixtures are briefly discussed in section [ sec : con ] .",
    "let us first recall how both mean and variance of a mixture distribution can be represented in terms of the mean and variance parameters of the component of the mixture :    [ lem : mings ] if @xmath11 and @xmath12 denote the mean and variance of the distribution with density @xmath13 , respectively , the mean of the mixture distribution is given by @xmath14=\\sum_{i=1}^k p_i\\mu_i\\ ] ] and its variance by @xmath15 ^ 2)\\ ] ]    the population mean given by @xmath14=   \\sum_{i=1}^k p_i { \\mathbb{e}}_{f(\\cdot|\\theta_i ) } [ x ] = \\sum_{i=1}^k p_i\\mu_i\\ ] ] where @xmath16 $ ] is the expected value component @xmath17 .",
    "similarly , the population variance is given by @xmath18- { \\mathbb{e}}_{{\\boldsymbol{\\theta}},{\\mathbf{p}}}[x]^2 =   \\sum_{i=1}^k p_i ( \\sigma_i^2 + \\mu_i^2)-{\\mathbb{e}}_{{\\boldsymbol{\\theta}},{\\mathbf{p}}}[x]^2\\,,\\ ] ] which concludes the proof    for any location - scale mixture , we then propose a reparameterisation of the mixture model that starts by scaling all parameters in terms of its global mean @xmath19 and global variance @xmath20 .",
    "for instance , we can switch to the representation @xmath21 of the component - wise parameters , where @xmath22 and @xmath23 .",
    "this is formally equivalent to the reparameterisation of @xcite , except that they put no special meaning on the global mean and variance parameters .",
    "once the global mean and variance are set , this imposes natural constraints on the other parameters of the model .",
    "for instance , setting the global variance to @xmath20 implies that @xmath24 belongs to a specific ellipse conditional on the weights and @xmath20 , by virtue of lemma [ lem : mings ] .",
    "considering the @xmath25 s and the @xmath26 s in as the new parameters of the components , the following result states that the global mean and variance parameters are the sole freely varying parameters .",
    "in other words , once both the global mean and variance are set , there exists a parameterisation such that all remaining parameters of a mixture distribution are restricted to a compact set , which is most helpful in selecting a non - informative prior distribution .",
    "[ lem : const ] the parameters @xmath25 and @xmath26 in are constrained by @xmath27    the result is a trivial consequence of lemma [ lem : mings ] .",
    "the population mean is @xmath14=\\sum_{i=1}^k p_i\\mu_i=\\sum_{i=1}^k p_i ( \\mu+\\sigma\\alpha_i)=\\mu+\\sum_{i=1}^k p_i\\alpha_i = \\mu\\ ] ] and the first constraint follows .",
    "the population variance is @xmath28 ^ 2 ) \\\\ & = & \\displaystyle\\sum_{i=1}^k p_i\\sigma^2\\tau_i^2 + \\sum_{i=1}^k p_i p_i ( \\mu^2 + 2\\sigma\\mu\\alpha_i+\\sigma^2\\alpha_i^2-\\mu^2 ) \\\\ & = & \\displaystyle\\sum_{i=1}^k p_i\\sigma^2\\tau_i^2 + \\sum_{i=1}^k p_i\\sigma^2\\alpha_i^2=\\sigma^2 \\end{array}\\ ] ] the last equation simplifies to the second constraint above .      the constraints in lemma [ lem : const ] define a set of values of @xmath29 that is obviously compact . from a bayesian perspective",
    ", this allows for the call to uniform and other non - informative proper priors , conditional on @xmath30 .",
    "furthermore , since @xmath30 is a location - scale parameter , we may invoke @xcite to use the jeffreys prior @xmath31 on this parameter , even though this is not the genuine jeffreys prior for the mixture model @xcite . in the same spirit as @xcite who established properness of the posterior distribution derived by @xcite",
    ", we now establish that this choice of prior produces a proper posterior distribution for a minimal sample size of two .",
    "[ th : proper ] the posterior distribution associated with the prior @xmath31 and with the likelihood derived from is proper when the components @xmath32 are gaussian densities , provided ( a ) proper distributions are used on the other parameters and ( b ) there are at least two observations in the sample .    when @xmath33 , it is easy to show that the jeffreys posterior is not proper .",
    "the marginal likelihood is then @xmath34 the integral against @xmath35 is then not defined .    for two data - points , @xmath36 ,",
    "the associated marginal likelihood is @xmath37 if all those @xmath38 integrals are proper , the jeffrey posterior distribution is proper .",
    "an arbitrary integral @xmath39 in this sum leads to @xmath40 \\text{d}(\\mu,\\sigma ) \\bigg\\ } \\pi({\\mathbf{p}},{\\boldsymbol{\\alpha}},{\\boldsymbol{\\tau } } ) \\ , \\text{d}({\\mathbf{p}},{\\boldsymbol{\\alpha}},{\\boldsymbol{\\tau } } ) \\\\ = &   \\displaystyle\\int \\bigg\\{\\int_0^{\\infty } \\dfrac{p_i p_j}{\\sqrt{2\\pi } \\sigma^2 \\sqrt{\\tau^2_i + \\tau^2_j } } \\exp \\bigg [ \\dfrac{-1}{2(\\tau_i^2+\\tau_j^2 ) } \\bigg ( \\dfrac{1}{\\sigma^2}(x_{1}-x_{2})^2 + \\dfrac{2}{\\sigma}(x_{1}-x_{2})(\\alpha_i-\\alpha_j ) \\\\ & \\qquad + ( \\alpha_i-\\alpha_j)^2 \\bigg ) \\bigg ]   \\ , \\text{d}\\sigma \\bigg \\ } \\pi({\\mathbf{p}},{\\boldsymbol{\\alpha}},{\\boldsymbol{\\tau } } ) \\ , \\text{d}(\\sigma,{\\mathbf{p}},{\\boldsymbol{\\alpha}},{\\boldsymbol{\\tau}})\\ , .",
    "\\end{array}\\ ] ] substituting @xmath41 , the above is integrated with respect to @xmath42 , leading to @xmath43 where @xmath44 is the cumulative distribution function of the standardised normal distribution .",
    "given that the prior is proper on all remaining parameters of the mixture and that the integrand is bounded by @xmath45 , it integrates against the remaining components of @xmath46 .",
    "let us now consider the case @xmath47 . since the posterior @xmath48 is proper , it constitutes a proper prior when considering only the observations @xmath49 .",
    "therefore , the posterior is almost everywhere proper .      before proposing relevant priors ,",
    "let us note that the constraints in lemma [ lem : const ] suggest a new reparameterisation ( among many possible ones ) : this reparameterisations uses the weights @xmath2 in the definition of the component parameters , as to achieve a more generic constraint .",
    "the component location and scale parameters in ( [ eq : locavore ] ) can indeed be reparameterised as @xmath50 leading to the mixture representation @xmath51 given @xmath52 , these new parameters are constrained by @xmath53 which means that @xmath54 belongs to an hypersphere of @xmath55 intersected with an hyperplane of this space .    given these constraints , further simplifications via new reparameterisations",
    "can be contemplated , as for instance separating mean and variance parameters in by introducing a radius @xmath56 such that @xmath57 this choice naturally leads to a hierarchical prior where , e.g. , @xmath58 and @xmath59 are distributed from a @xmath60 and a @xmath61 distributions , respectively , while the vectors @xmath62 and @xmath63 are uniformly distributed on the spheres of radius @xmath56 and @xmath64 , respectively , under the additional linear constraint @xmath65 .",
    "we now describe how this reparameterisation leads to a practical construction of the constrained parameter space , for an arbitrary number of components @xmath66 .",
    "the vector @xmath62 belongs both to the hypersphere of radius @xmath56 and to the hyperplane orthogonal to @xmath68 .",
    "therefore , @xmath62 can be expressed in terms of spherical coordinates within that hyperplane .",
    "namely , if @xmath69 denotes an orthonormal basis of the hyperplane , @xmath62 can be written as @xmath70 with the angles @xmath71 in @xmath72 $ ] and @xmath73 in @xmath74 $ ] .",
    "the @xmath75-th orthonormal base @xmath76 can be derived from the @xmath66-dimensional orthogonal vectors @xmath77 where @xmath78 and the @xmath75-th vector is given by @xmath79    note the special case of @xmath80 since the angle @xmath81 is then missing . in this special case ,",
    "the mixture location parameter is defined by @xmath82 and @xmath56 takes both positive and negative values . in the general",
    "setting , the parameter vector @xmath83 is a transform of @xmath84 .",
    "a natural reference prior for @xmath85 is made of uniforms , @xmath86 $ ] and @xmath87 $ ] , although other choices are obviously possible and should be explored to test the sensitivity to the prior .      for the component variance parameters , the vector @xmath89 belongs to the @xmath66-dimension sphere of radius @xmath64 .",
    "a natural prior is then a dirichlet distribution with common hyperparameter @xmath3 , @xmath90 if @xmath66 is small enough , @xmath89 can then be simulated from the corresponding posterior with no computational challenge .",
    "however , as @xmath66 increases , sampling may become more delicate and benefits from a similar spherical reparameterisation .",
    "in this approach , the vector @xmath89 is rewritten through spherical coordinates with angle components @xmath91 , @xmath92 unlike @xmath85 , the support for all angles @xmath93 is limited to @xmath94 $ ] , due to the positivity requirement on the @xmath88 s . in this case ,",
    "a reference prior on the angles is @xmath95^{k-1})\\,,\\ ] ] while again other choices are possible .",
    "given the reparameterisations introduced in section [ sec : mix ] , different mcmc implementations are possible and we investigate in this section some of these . to this effect , we distinguish between two cases : ( i ) only @xmath96 is expressed in spherical coordinates ; and ( ii ) both the @xmath11 s and the @xmath97 s are associated with spherical coordinates .",
    "although the target density is similar to the target explored by early gibbs samplers in @xcite and @xcite , simulating directly the new parameters implies managing constrained parameter spaces .",
    "the hierarchical nature of the parameterisation also leads us to consider a block gibbs sampler that coincides with this hierarchy .",
    "since the corresponding full conditional posteriors are not in closed form , a metropolis - within - gibbs sampler is implemented here with random walk proposals . in this approach ,",
    "the scales of the proposal distributions are automatically calibrated towards optimal acceptance rates @xcite .",
    "convergence of a simulated chain is assessed based on the rudimentary convergence monitoring technique of @xcite .",
    "the description of the algorithm is provided by the pseudo - code version in figure [ fig : algobox ] .",
    "note that the metropolis - within - gibbs version does not rely on latent variables and complete likelihood as in @xcite and @xcite .",
    "following the adaptive mcmc method in section 3 of @xcite , we derive the optimal scales associated with proposal densities , based on 10 batches with size 50 .",
    "the scales @xmath98 are identified by a subscript with the corresponding parameter .    for the reparameterisation ( i ) ,",
    "all steps are the same except that steps 2.5 and 2.7 are combined together and that @xmath99 is updated in the same manner .",
    "one potential proposal density is a dirichlet distribution , @xmath100 alternative proposal densities will be discussed later along with simulation studies in section 4 .",
    "the standard parameterisation of mixture models contains weights @xmath101 and component - wise parameters @xmath102 as shown in .",
    "the likelihood function is invariant under permutations of the component indices .",
    "if an exchangeable prior is chosen on weights and component - wise parameters , the posterior density reproduces the likelihood invariance and component labels are not identifiable .",
    "this phenomenon is called _ label switching _ and is well - studied in the literature @xcite . this means that the posterior distribution consists of @xmath103 symmetric modes and a markov chain with such target distribution is expected to explore all of them .",
    "however , a chain often fails and rather ends up exploring a particular mode .    in our reparameterisation of gaussian mixture models ,",
    "each component mean and variance are functions of angular and radius parameters with weights .",
    "the mapping between both parameterisations is a one - to - one map conditional on the weights . in other words",
    ", there are unique component - wise means and variances given particular values for angular and radius parameters and weights .",
    "although the new parameterisation is not exchangeable , due to the choice of the orthogonal basis , adopting an exchangeable prior on the weights ( e.g. , a dirichlet distribution with a common parameter ) and uniform priors on all angular parameters leads to an exchangeable posterior on the natural parameters of the mixture . therefore , label switching should also occur with this prior modelling .",
    "when an mcmc chain manages to jump between modes , the inference on each of the mixture components becomes harder @xcite .",
    "to get component - specific inference and to give a meaning to each component , various relabelling methods have been proposed in the literature ( see , e.g. , @xcite ) .",
    "a first available alternative is to reorder labels so that the mixture weights are in increasing order @xcite . a second alternative method proposed by , e.g.",
    ", @xcite is that labels are reordered towards producing the shortest distance between the current posterior sample and the ( or a ) maximum posterior probability ( map ) estimate .",
    "let us denote by @xmath104 the map from our reparameterisation to the standard parameterisation of , i.e. , @xmath105 with its inverse @xmath106 available as well .",
    "we also denote by @xmath107 the set of permutations of @xmath108 .",
    "then , given an mcmc sample @xmath109 , the above relabelling technique procedure follows ;    1 .",
    "reparameterise the mcmc sample @xmath110 into component - wise means and standard deviations via the function @xmath104 , resulting in @xmath111 .",
    "2 .   find the map estimate by computing the posterior values of the sample ; denote the solution as @xmath112 .",
    "3 .   reorder @xmath113 as @xmath114 where @xmath115 .",
    "the resulting permutation is then denoted @xmath116 .",
    "label switching occurrences in an mcmc sequence can be monitored via the changes in the sequence @xmath117 .",
    "if the chain fails to switch modes , the sequence is likely to remain at the same permutation . on the opposite , if a chain moves between some of the @xmath103 symmetric posterior modes , the @xmath118 s are expected to vary .",
    "we proceed here by a simulation studies section and all algorithms used in this section are publicly available within the r package ultimixt @xcite .",
    "the package ultimixt contains functions that implement adaptive determination of optimal scales and convergence monitoring based on @xcite criterion .",
    "in addition , ultimixt includes functions that summarise the simulations and compute point estimates of each parameter , such as posterior mean and median .",
    "it also produces an estimated mixture density in numerical and graphical formats .",
    "the output further includes graphical representations of the generated parameter samples .",
    "for the potentially unimodal parameters @xmath19 , @xmath35 and @xmath56 , averaging and calculating the median over the generated chains directly returns valid point estimators , as those parameters are not subjected to label switching .",
    "for the other parameters ( component weights , means and variances ) , since label switching is a possible issue , we need to postprocess the mcmc draws as discussed earlier , by first relabelling these simulations .",
    "we then derive point estimates by clustering over the parameter space , using @xmath66-mean clustering @xcite .",
    "in this section , we examine the performances of the above metropolis - within - gibbs algorithm , when applied to both reparameterisations defined above .",
    "we also consider the special case @xmath80 in section [ sub : k=2 ] .",
    "all simulations were conducted using the package ultimixt @xcite .      in this specific case , we do not have to simulate any angle .",
    "two straightforward proposals are compared over simulation experiments .",
    "one is based on beta and dirichlet proposals : @xmath119 ( this will be called proposal 1 ) and another one is based on gaussian random walks : @xmath120 ( which will be called proposal 2 ) .",
    "the global parameters are proposed using normal and inverse - gamma proposals @xmath121 where @xmath122 and @xmath123 are sample mean and variance respectively .",
    "we present below some analyses and also explain how mcmc methods can be used to fit the reparameterised mixture distribution .",
    "[ ex : simulated_data ] in this experiment , a dataset of size @xmath124 is simulated from the mixture @xmath125 , which implies that while the true value of @xmath126 is @xmath127 .",
    "figure [ fig1 ] illustrates the performances of a metropolis - within - gibbs algorithm based on proposal 1 .",
    "it shows the outcomes of 10 parallel chains , each started randomly from different starting values .",
    "the estimated densities are almost indistinguishable among the different chains and they all converge to a neighbourhood of the true values .",
    "the chains are well - mixed and the sampler output covers the entire sample space in this case .    :* kernel estimates of the posterior densities of the parameters @xmath19 , @xmath35 , @xmath128 , @xmath56 , @xmath88 , based on @xmath129 parallel mcmc chains for proposal 1 and @xmath130 iterations , based on a single simulated sample of size @xmath124 .",
    "the true value of @xmath126 is @xmath127.,title=\"fig:\",height=151 ] : * kernel estimates of the posterior densities of the parameters @xmath19 , @xmath35 , @xmath128 , @xmath56 , @xmath88 , based on @xmath129 parallel mcmc chains for proposal 1 and @xmath130 iterations , based on a single simulated sample of size @xmath124 .",
    "the true value of @xmath126 is @xmath127.,title=\"fig:\",height=151 ] : * kernel estimates of the posterior densities of the parameters @xmath19 , @xmath35 , @xmath128 , @xmath56 , @xmath88 , based on @xmath129 parallel mcmc chains for proposal 1 and @xmath130 iterations , based on a single simulated sample of size @xmath124 .",
    "the true value of @xmath126 is @xmath127.,title=\"fig:\",height=151 ] :* kernel estimates of the posterior densities of the parameters @xmath19 , @xmath35 , @xmath128 , @xmath56 , @xmath88 , based on @xmath129 parallel mcmc chains for proposal 1 and @xmath130 iterations , based on a single simulated sample of size @xmath124 .",
    "the true value of @xmath126 is @xmath127.,title=\"fig:\",height=151 ] : * kernel estimates of the posterior densities of the parameters @xmath19 , @xmath35 , @xmath128 , @xmath56 , @xmath88 , based on @xmath129 parallel mcmc chains for proposal 1 and @xmath130 iterations , based on a single simulated sample of size @xmath124 .",
    "the true value of @xmath126 is @xmath127.,title=\"fig:\",height=151 ] : * kernel estimates of the posterior densities of the parameters @xmath19 , @xmath35 , @xmath128 , @xmath56 , @xmath88 , based on @xmath129 parallel mcmc chains for proposal 1 and @xmath130 iterations , based on a single simulated sample of size @xmath124 .",
    "the true value of @xmath126 is @xmath127.,title=\"fig:\",height=151 ]    we also run the metropolis - within - gibbs algorithm based on proposal 2 using the same simulated dataset for comparison purposes . as shown in figure",
    "[ fig2 ] , the outputs for both proposals are quite similar but proposal 1 produces more symmetric chains on @xmath131 , thus suggesting higher mixing abilities .    :* comparison between mcmc samples from our metropolis - within - gibbs algorithm using proposal 1 ( _ solid line _ ) or proposal 2 ( _ dashed line _ ) , with @xmath132 iterations and the same sample as in figure [ fig1 ] .",
    "the true value of @xmath126 is @xmath127 .",
    ", title=\"fig:\",height=151 ] : * comparison between mcmc samples from our metropolis - within - gibbs algorithm using proposal 1 ( _ solid line _ ) or proposal 2 ( _ dashed line _ ) , with @xmath132 iterations and the same sample as in figure [ fig1 ] . the true value of @xmath126 is @xmath127 . , title=\"fig:\",height=151 ] : * comparison between mcmc samples from our metropolis - within - gibbs algorithm using proposal 1 ( _ solid line _ ) or proposal 2 ( _ dashed line _ ) , with @xmath132 iterations and the same sample as in figure [ fig1 ] .",
    "the true value of @xmath126 is @xmath127 .",
    ", title=\"fig:\",height=151 ] :* comparison between mcmc samples from our metropolis - within - gibbs algorithm using proposal 1 ( _ solid line _ ) or proposal 2 ( _ dashed line _ ) , with @xmath132 iterations and the same sample as in figure [ fig1 ] .",
    "the true value of @xmath126 is @xmath127 .",
    ", title=\"fig:\",height=151 ] : * comparison between mcmc samples from our metropolis - within - gibbs algorithm using proposal 1 ( _ solid line _ ) or proposal 2 ( _ dashed line _ ) , with @xmath132 iterations and the same sample as in figure [ fig1 ] .",
    "the true value of @xmath126 is @xmath127 .",
    ", title=\"fig:\",height=151 ] : * comparison between mcmc samples from our metropolis - within - gibbs algorithm using proposal 1 ( _ solid line _ ) or proposal 2 ( _ dashed line _ ) , with @xmath132 iterations and the same sample as in figure [ fig1 ] .",
    "the true value of @xmath126 is @xmath127 .",
    ", title=\"fig:\",height=151 ]    the scales of the various proposals are determined by aiming at @xcite goal of an average acceptance rate of either @xmath133 or @xmath134 depending on the dimension of the simulated parameter . as shown in table [ tab1 ] , an adaptive metropolis - within - gibbs strategy manages to recover acceptance rates close to optimal values .",
    "@xmath135    .[tab1 ] * example [ ex : simulated_data ] * : acceptance rate ( @xmath136 ) and corresponding proposal scale ( @xmath98 ) when the adaptive metropolis - within - gibbs sampler is used . [ cols=\"^,^,^,^,^,^,^,^ \" , ]      components ( _ dark line _ ) ; ( _ right _ ) mixture density estimate based on @xmath137 mcmc iterations for @xmath138 components.,title=\"fig:\",height=170 ] components ( _ dark line _ ) ; ( _ right _ ) mixture density estimate based on @xmath137 mcmc iterations for @xmath138 components.,title=\"fig:\",height=170 ]     mcmc iterations.,title=\"fig:\",height=170 ] mcmc iterations.,title=\"fig:\",height=170 ]    it is seen that the proposed prior is more dispersed for @xmath8 and @xmath139 and is very skewed toward 0 for @xmath9 with long tail . when @xmath138 , bayesm yields a more concentrated prior for @xmath128 to accommodate all components and the proposed prior becomes dispersed to give flexible support on component - wise location and scale .",
    "samples for @xmath8 , @xmath9 and @xmath139 when ( _ top _ ) @xmath138 and ( _ bottom _ ) @xmath140 . for the proposed prior ( _ solid lines _ ) , the priors induced are @xmath141 and @xmath142 . for the prior by bayesm ( _ dashed lines _ ) , hyperparameters are @xmath143 for @xmath140 and @xmath144 for @xmath138 while @xmath145 and @xmath146.,title=\"fig : \" ] +   samples for @xmath8 , @xmath9 and @xmath139 when ( _ top _ ) @xmath138 and ( _ bottom _ ) @xmath140 . for the proposed prior ( _ solid lines _ ) , the priors induced are @xmath141 and @xmath142 . for the prior by bayesm ( _ dashed lines _ ) , hyperparameters are @xmath143 for @xmath140 and @xmath144 for @xmath138 while @xmath145 and @xmath146.,title=\"fig : \" ]",
    "in example [ ex : benchmark ] we have seen that for the old faithful dataset , the multimodality of the mixture model is not reproduced in the mcmc output , which means the adaptive metropolis - within - gibbs sampler can not escape one of the modes . in this case ,",
    "parallel tempering may be used @xcite .",
    "this method allows for better mixing in multimodal target distributions , when using straightforward metropolis - hastings algorithms fail @xcite .",
    "it is indeed designed to overcome low probability regions between modal areas . given the posterior density @xmath147 , we define tempered versions @xmath148 , where @xmath149 is the inverse temperature and @xmath150 corresponds to the original target distribution @xcite .",
    "the tempered mcmc algorithm then runs a basic mcmc algorithm on a range of tempered distribution and , at each iteration , the current samples are considered for potential exchanges between adjacent temperatures , with a metropolis ",
    "hastings acceptance probability @xmath151 as the chances of accepting a swap are higher for nearby temperatures .",
    "proposal scales are calibrated by adaptive mcmc method and is used for all tempered versions of the target .",
    "temperatures are chosen of the form @xmath152 @xmath153 and the sequence is determined according to the degree of symmetry in the distribution of the @xmath2 s or when the minimum acceptance rate for swaps between adjacent temperatures is larger than a default threshold .",
    "considering again the old faithful benchmark , we set this symmetry threshold to @xmath154 and this acceptance threshold to @xmath155 . using the same proposals as in example [ ex : benchmark ] and @xmath156 , the algorithm selects @xmath157 temperatures , thus equal to @xmath158 .",
    "figure [ fig4 ] demonstrates that the parallel tempering sampler visits all modes in the posterior distribution and that the mixing of the chains is greatly improved .",
    "@xmath135    :* posterior distribution of the mixture distribution parameters and comparison between the lowest and highest temperatures ( target distribution and @xmath159 ) of parallel tempering outputs based on @xmath160 iterations.,title=\"fig:\",height=132 ] : * posterior distribution of the mixture distribution parameters and comparison between the lowest and highest temperatures ( target distribution and @xmath159 ) of parallel tempering outputs based on @xmath160 iterations.,title=\"fig:\",height=132 ] : * posterior distribution of the mixture distribution parameters and comparison between the lowest and highest temperatures ( target distribution and @xmath159 ) of parallel tempering outputs based on @xmath160 iterations.,title=\"fig:\",height=132 ] :* posterior distribution of the mixture distribution parameters and comparison between the lowest and highest temperatures ( target distribution and @xmath159 ) of parallel tempering outputs based on @xmath160 iterations.,title=\"fig:\",height=132 ] : * posterior distribution of the mixture distribution parameters and comparison between the lowest and highest temperatures ( target distribution and @xmath159 ) of parallel tempering outputs based on @xmath160 iterations.,title=\"fig:\",height=132 ] : * posterior distribution of the mixture distribution parameters and comparison between the lowest and highest temperatures ( target distribution and @xmath159 ) of parallel tempering outputs based on @xmath160 iterations.,title=\"fig:\",height=132 ]    [ ex : licence_data_k3 ] we now implement parallel tempering for a mixture of @xmath161 components applied to a benchmark dataset from @xcite .",
    "this dataset is derived from an image of a car license plate , and made of 2625 observations . in @xcite , a lack of label switching",
    "is observed when using a gibbs sampler .",
    "once again , this means each component can be estimated by its mean and standard deviation .",
    "the sample size is larger here and more likely to mixing problems .",
    "this is clearly exhibited in the six top plots of figure [ figu5 ] where the estimates provided for the three components are quite distinct .",
    "when implementing parallel tempering , the temperature increase stops when when all acceptance rates of swaps are above @xmath162 , meaning for this dataset @xmath163 temperatures ranging from @xmath164 to @xmath165 .",
    "the six bottom plots of figure [ figu5 ] show that parallel tempering immensely improves the swaps between the posterior modes .",
    "the sample of @xmath85 s produced by parallel tempering visits a much larger region in @xmath166 , when compared with the highly peaked output of the original mcmc output .    ) :* comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ] ) : * comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ] ) : * comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ] ) :* comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ] ) : * comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ] ) : * comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ] ) :* comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ] ) : * comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ] ) : * comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ] ) :* comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ] ) : * comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ] ) : * comparison between metropolis - within - gibbs and parallel tempering outputs : the distributions of the samples of @xmath167 last points and corresponding @xmath168 plots.,title=\"fig:\",height=132 ]    the histograms in figure [ figu5 ] show that the posterior on @xmath128 and @xmath169 are now close to identical for each component .",
    "two - dimensional plots also highlight this correct label switching behaviour , which demonstrates better mixing and convergence of the produced chain . @xmath135",
    "this paper has introduced a new parametrisation for mixtures of location - scale models . by constraining the parameters in terms of the global mean and global variance of the mixture , i.e. , by recognising the location - scale nature of such mixtures",
    ", it has been shown that the remaining parameters can be expressed as varying within a compact set .",
    "therefore , it is possible to use a well - defined uniform prior on these parameters ( as well as any proper prior ) and we established that an improper prior of jeffreys type on the global mean and global variance returns a proper posterior distribution when handling at least two observations from the mixture . while the notion of _ non - informative _ or _ objective _ prior is open to interpretations and sometimes controversies , we believe we have defined in this paper what can be considered as the first reference prior for mixture models .    we have demonstrated that relatively standard simulation algorithms are able to handle this new parametrisation and that they can manage the computing issues connected with label switching . in case of poor switching , we also established that parallel tempering can be easily implemented . as exhibited in the associated ultimixt package ,",
    "relabelling techniques are readily available .    while the extension to non - gaussian cases with location - scale parameterisation ( and beyond ) is conceptually straightforward , considering this parameterisation in higher dimensions is delicate in terms of the covariance matrix . indeed ,",
    "even though we can easily set the global variance of the mixture as a parameter , reparameterising the component variances against this reference matrix remains an open question that we have not yet explored .",
    "lee , k. , j .-",
    "marin , k.  mengersen , and c.  robert ( 2009 ) .",
    "ayesian inference on mixtures of distributions . in n.",
    "n. sastry , m.  delampady , and b.  rajeev ( eds . ) , _ perspectives in mathematical sciences i : probability and statistics _ , pp . 165202 .",
    "singapore : world scientific .",
    "marin , j .-",
    ", k.  mengersen , and c.  robert ( 2005 ) .",
    "ayesian modelling and inference on mixtures of distributions . in c.  rao and d.  dey ( eds . ) ,",
    "_ handbook of statistics _ , volume  25 , pp .",
    "springer - verlag , new york .",
    "mengersen , k. and c.  robert ( 1996 ) .",
    "testing for mixtures : a bayesian entropic approach ( with discussion ) . in j.",
    "berger , j.  bernardo , a.  dawid , d.  lindley , and a.  smith ( eds . ) , _ bayesian statistics 5 _ , pp .",
    "oxford university press , oxford ."
  ],
  "abstract_text": [
    "<S> while mixtures of gaussian distributions have been studied for more than a century ( pearson , 1894 ) , the construction of a reference bayesian analysis of those models still remains unsolved , with a general prohibition of the usage of improper priors @xcite due to the ill - posed nature of such statistical objects . </S>",
    "<S> this difficulty is usually bypassed by an empirical bayes resolution @xcite . by creating a new parameterisation cantered on the mean and variance of the mixture distribution itself , we are able to develop here a genuine non - informative prior for gaussian mixtures with an arbitrary number of components . </S>",
    "<S> we demonstrate that the posterior distribution associated with this prior is almost surely proper and provide mcmc implementations that exhibit the expected exchangeability . while we only study here the gaussian case , extension to other classes of location - scale mixtures is straightforward . </S>"
  ]
}