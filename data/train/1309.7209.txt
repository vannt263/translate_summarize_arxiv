{
  "article_text": [
    "markov chain monte carlo ( mcmc ) algorithms have proved particularly successful in statistics for investigating posterior distributions in bayesian analysis of complex models ; see , for example , @xcite .",
    "almost all mcmc methods are based on the metropolis  hastings ( mh ) algorithm which owes much of its success to its tremendous flexibility .",
    "however , in order to use the classical mh algorithm , it must be possible to evaluate the target density up to a fixed constant of proportionality . while this is often possible , it is increasingly common for exact pointwise likelihood evaluation to be prohibitively expensive , perhaps due to the sheer size of the data set being analysed . in these situations ,",
    "classical mh is rendered inapplicable .    the _ pseudo - marginal metropolis ",
    "hastings algorithm _ ( psmmh ) @xcite provides a general recipe for circumventing the need for target density evaluation .",
    "instead it is required only to be able to unbiasedly _ estimate _ this density .",
    "the target densities in the numerator and denominator of the mh accept / reject ratio are then replaced by their unbiased estimates .",
    "remarkably , this yields an algorithm which still has the target as its invariant distribution .",
    "one possible choice of algorithm , the _ pseudo - marginal random walk metropolis _ ( psmrwm ) , is popular in practice ( e.g. , @xcite ) because it requires no further information about the target , such as the local gradient or hessian , which are generally more computationally expensive to approximate than the target itself @xcite .",
    "broadly speaking , the mixing rate of any psmmh algorithm decreases as the dispersion in the estimation of the target density increases @xcite .",
    "in particular , if the target density happens to be substantially over - estimated , then the chain will be overly reluctant to move from that state leading to a long run of successive rejections ( a _ sticky patch _ ) .",
    "now , in psmmh algorithms , the target estimate is usually computed using an average of some number , @xmath0 , of approximations ; see sections  [ sect.psrwm.intro ]  and  [ sect.optimising ] .",
    "this leads to a trade off , with increasing @xmath0 leading to better mixing of the chain , but also to larger computational expense . we shall consider the problem of optimising @xmath0 .",
    "it is well known ( e.g. , @xcite ) that the efficiency of the random - walk metropolis ( rwm ) algorithm varies enormously with the scale of the proposed jumps .",
    "small proposed jumps lead to high acceptance rates but little movement across the state space , whereas large proposed jumps lead to low acceptance rates and again to inefficient exploration of the state space .",
    "the problem of choosing the optimal scale of the rwm proposal has been tackled for various shapes of target ( e.g. , @xcite ) and has led to the following rule of thumb : choose the scale so that the acceptance rate is approximately @xmath1 .",
    "although nearly all of the theoretical results are based upon limiting arguments in high dimension , the rule of thumb appears to be applicable even in relatively low dimensions ( e.g. , @xcite ) .",
    "this article focusses on the efficiency of the psmrwm as the dimension of the target density diverges to infinity .",
    "for relatively general forms of the target distribution , under the assumption of additive independent noise in the log - target , we obtain ( theorem  [ thm.asymp.analysis ] ) expressions for the limiting expected squared jump distance ( esjd ) and asymptotic acceptance rate .",
    "esjd is now well established as a pragmatic and useful measure of mixing for mcmc algorithms in many contexts ( see , e.g. , @xcite ) , and is particularly relevant when diffusion limits can be established ; see , for example , the discussion in @xcite .",
    "we then prove a diffusion limit for a rescaling of the first component , in the case of a target with independent and identically distributed components ( theorem  [ thm.diff.lim ] ) , the efficiency of the algorithm is then given by the speed of this limiting diffusion , which is equivalent to the limiting esjd .",
    "we examine the relationship between efficiency , scaling , and the distributional form of the noise , and consider the _ joint _",
    "optimisation of the efficiency of the psmrwm algorithm ( taking computational time into account ) with respect to  @xmath0 , and the rwm scale parameter .",
    "exact analytical results are obtained ( corollary  [ cor.max.gauss ] ) under an assumption of gaussian noise in the estimate of the log - target , with a  variance that is inversely proportional to @xmath0 . in this case , we prove that the optimal noise variance is 3.283 , and the corresponding optimal asymptotic acceptance rate is 7.001% , thus extending the previous 23.4% result of @xcite . finally , we illustrate the use of these theoretical results in a simulation study ( section  [ sect.sim.study ] ) .      consider a state space @xmath2 , and let @xmath3 be a distribution on @xmath4",
    ", whose density ( with respect to lebesgue measure ) will be referred to as @xmath5 .",
    "the mh updating scheme provides a very general class of algorithms for obtaining an approximate dependent sample from a target distribution , @xmath3 , by constructing a markov chain with @xmath3 as its limiting distribution .",
    "given the current value @xmath6 , a new value @xmath7 is proposed from a pre - specified lebesgue density @xmath8 and is then accepted with probability @xmath9/[\\pi(\\mathbf{x } )   q ( \\mathbf{x},\\mathbf{x}^*)]$ ] .",
    "if the proposed value is accepted , then it becomes the next current value ; otherwise the current value is left unchanged .",
    "the psmmh algorithm @xcite presumes the computational infeasibility of evaluating @xmath5 and uses an approximation @xmath10 that depends on some auxiliary variable , @xmath11 .",
    "the auxiliary variable is sampled from some distribution @xmath12 , and the approximation @xmath10 is assumed to satisfy that @xmath13=c\\pi ( \\mathbf{x})$ ] , for some constant @xmath14 .",
    "the value of the constant is irrelevant to all that follows , and so , without loss of generality , we assume that @xmath15 .",
    "we also assume that @xmath16 .",
    "the psmmh algorithm creates a markov chain with a stationary density ( since @xmath15 ) of @xmath17 which has @xmath5 as its @xmath6 marginal .",
    "when a new value , @xmath18 , is proposed via the mh algorithm , a new auxiliary variable , @xmath19 , is proposed from the density @xmath20 .",
    "the pair @xmath21 are then jointly accepted or rejected .",
    "the acceptance probability for this mh algorithm on @xmath22 is @xmath23 we are thus able to substitute the estimated density for the true density , and still obtain the desired stationary distribution for @xmath6 .",
    "note that for symmetric proposals , this simplifies to @xmath24 $ ] .",
    "different strategies exist for producing unbiased estimators , for instance , using importance sampling or latent variable representations , as in @xcite , or using particle filters @xcite as in @xcite .",
    "we shall illustrate our theory in the context of bayesian analysis of a partially observed markov jump process .",
    "pitt et al .",
    "@xcite and doucet et al .",
    "@xcite examine the efficiency of pseudo - marginal algorithms using bounds on the integrated autocorrelation time ( @xmath25 ) and under the assumptions that the chain is stationary and the distribution of the additive noise in the log - target is independent of @xmath6 ( our assumption  [ ass.noise.diff.indep ] ) . under the further assumption that this additive noise is gaussian and the computing time inversely proportional to its variance ( our assumption  [ ass.standard.regime ] )",
    ", both articles then seek information on the optimal variance of this additive noise .",
    "pitt et al .",
    "@xcite consider the ( unrealistic ) case where the metropolis ",
    "hastings algorithm is an independence sampler which proposes from the desired target distribution for  @xmath26 , and obtain an optimal variance of @xmath27 .",
    "doucet et al .",
    "@xcite consider a general metropolis ",
    "hastings algorithm and define a parallel hypothetical kernel @xmath28 with the same proposal mechanism as the original kernel , @xmath29 , but where the acceptance rate separates into the product of that of the idealised marginal algorithm ( if the true target were known ) and that of an independence sampler which proposes from the assumed distribution for the noise .",
    "this kernel can never be more efficient than the true kernel .",
    "upper and lower bounds are obtained for the @xmath25 for @xmath28 in terms of the of @xmath25 of the exact chain and the @xmath25 and a particular lag-1 autocorrelation of the independence sampler on the noise .",
    "these bounds are examined under the assumption that the additive noise is gaussian and the optimal variance for the noise is estimated to lie between @xmath27 and @xmath30 .",
    "other theoretical properties of pseudo - marginal algorithms are considered in  @xcite , which gives qualitative ( geometric and polynomial ergodicity ) results for the method and some results concerning the loss in efficiency caused by having to estimate the target density .      in this paper , we follow the standard convention whereby capital letters denote random variables , and lower case letters denote their actual values .",
    "bold characters are used to denote vectors or matrices .",
    "we focus on the case where the proposal , @xmath7 , for an update to @xmath6 is assumed to arise from a random walk metropolis algorithm with an isotropic gaussian proposal @xmath31 and @xmath32 is the @xmath33 identity matrix , and @xmath34 is the scaling parameter for the proposal .",
    "the results presented in this article extend easily to a more general correlation matrix by simply considering the linear co - ordinate transformation which maps this correlation matrix to the identity matrix and examining the target in this transformed space . in proving the limiting results we consider a sequence of @xmath35-dimensional target probabilities @xmath36 . in dimension",
    "@xmath35 the proposal is @xmath37 .",
    "we will work throughout with the log - density of the target , and it will be convenient to consider the difference between the estimated log - target [ @xmath38 and the true log - target [ @xmath39 at both the proposed values ( @xmath40 ) and the current values ( @xmath41 ) , as well as the difference between these two differences , @xmath42 throughout this article we assume the following .",
    "[ ass.noise.diff.indep ] the markov chain @xmath43 is stationary , and the distribution of the additive noise in the estimated log - target at the proposal ,  @xmath44 , is independent of the proposal itself , @xmath18 .",
    "[ rem1 ] it is unrealistic to believe that the second part of assumption  [ ass.noise.diff.indep ] should hold in practice .",
    "pragmatically , this assumption is necessary in order to make progress with the theory presented herein ; however , in our simulation study in section  [ sect.sim.study ] we provide evidence that , in the scenarios considered , the variation in the noise distribution is relatively small .",
    "note that the noise term within the markov chain , @xmath45 , does not have the same distribution as the noise in the proposal , @xmath44 , since , for example , moves away from positive values of @xmath45 will be more likely to be rejected than moves away from negative values of @xmath45 . in the notation of section  [ sect.psrwm.intro ] , since @xmath44 is a function of @xmath46 , @xmath47 now gives rise to @xmath48 , the density of the noise in the estimate of the log - target , which is independent of @xmath7 . integrating ( [ eqn.joint.stat.v ] ) gives the joint stationary density of the markov chain @xmath49 as @xmath50 this is lemma @xmath51 of @xcite . under assumption  [ ass.noise.diff.indep ] , @xmath45 and @xmath52",
    "are therefore independent , and the stationary density of @xmath45 is @xmath53 .",
    "we describe in this section conditions on the sequence of target densities @xmath36 that ensure that the quantity @xmath54 $ ] behaves asymptotically as a gaussian distribution under an appropriate choice of jump scaling @xmath55 .",
    "the main assumption is that there exist sequences of scalings @xmath56 and @xmath57 for the gradient and the laplacian of the log - likelihood @xmath58 such that the following two limits hold in probability : @xmath59 for @xmath60 . in the rest of this article",
    "we assume that the sequence of densities @xmath36 is such that for each index @xmath61 , with all components of @xmath6 fixed except the @xmath62th , the @xmath62th component satisfies @xmath63 under this regularity condition , an integration by parts shows that @xmath64 = -\\mathbb{e } \\bigl [ \\delta\\log\\pi^{(d)}\\bigl(\\mathbf{x}^{({d})}\\bigr ) \\bigr].\\ ] ] equation ( [ eq.rescaled.grad.hess ] ) thus yields @xmath65 .",
    "we will suppose from now on , without loss of generality , that @xmath66 .",
    "we also require that no single component of the local hessian @xmath67_{0 \\leq i , j \\leq d}$ ] dominate the others in the sense that the limit @xmath68 } { ( s^{(d ) } ) ^4 } = 0\\ ] ] holds in probability .",
    "we also assume that the hessian matrix is sufficiently regular so that for any @xmath69 and @xmath70 @xmath71 \\mathbf{z}^{(d ) } \\rangle}{(s^{(d)})^2 } \\biggr| > \\varepsilon",
    "\\biggr ) \\nonumber \\\\[-8pt ] \\label{eqn.hessian.regularity } \\\\[-8pt ] \\nonumber & & \\qquad = 0.\\hspace*{-12pt}\\end{aligned}\\ ] ] these conditions are discussed in detail in @xcite where they are shown to hold , for example , when the target is the joint distribution of successive elements of a class of finite - order multivariate markov processes .",
    "the targets considered in @xcite and section  [ sec.diff.lim ] all satisfy the conditions with @xmath72 .",
    "we record the conditions formally as :    [ assump.lim.alpha ] the sequence of densities @xmath36 satisfies equations ( [ eq.rescaled.grad.hess ] ) , ( [ eqn.eigen.cip ] ) , ( [ eqn.hessian.regularity ] ) , and the regularity condition ( [ eqn.std.reg ] ) .",
    "we shall show in next section that under these assumptions the choice of jump size @xmath73 for a parameter @xmath74 leads to a gaussian asymptotic behaviour for@xmath75 $ ] .",
    "this ensures that for high dimensions , the mean acceptance probability @xmath76 of the mcmc algorithm , @xmath77,\\ ] ] stays bounded away from zero and one .",
    "a standard measure of efficiency for local algorithms is the euclidian expected squared jumping distance ( e.g. , @xcite ) usually defined as @xmath78 .",
    "consider , for example , a target with elliptical contours , or one which has components which are independent and identically distributed up to a scale parameter .",
    "in such situations the euclidean esjd is dominated by those components with a larger scale .",
    "we would prefer an efficiency criterion which weights components at least approximately equally , so that moves along each component are considered relative to the scale of variability of that component .",
    "a squared mahalanobis distance is the natural extension of euclidean esjd , and in the case of the two example targets mentioned above , it is exactly the correct generalisation of euclidean esjd .",
    "we therefore define a generalised potential squared jump distance for a single iteration with respect to some @xmath33 positive definite symmetric matrix @xmath79 , @xmath80 $ ] , where the markov chain @xmath81 is assumed to evolve at stationarity and @xmath82 .",
    "we will require that , in the limit as @xmath83 , no one principal component of @xmath79 dominates the others in the sense that @xmath84 / \\operatorname{trace } \\bigl[{\\mathbf{t}^{({d } ) } } \\bigr]^2 \\rightarrow0.\\ ] ] clearly , ( [ eqn.eccentric.distance ] ) is satisfied when @xmath85 ( i.e. , euclidian esjd ) .",
    "[ thm.asymp.analysis ] consider a psmrwm algorithm .",
    "assume that the additive noise satisfies assumption  [ ass.noise.diff.indep ] , the sequence of densities @xmath36 satisfy assumption  [ assump.lim.alpha ] , and the sequence of jump distance matrices @xmath86 satisfy  ( [ eqn.eccentric.distance ] ) . assume further that the jump size @xmath55 is given by  ( [ eq.jump.size ] ) for some fixed @xmath74 .    .",
    "the mean acceptance probabilities @xmath76 converge as @xmath87 to a nontrivial value @xmath88 , @xmath89 = : \\alpha(\\ell),\\ ] ] with @xmath90 as in  ( [ wbdef ] ) , where @xmath91 is the cumulative distribution of a standard gaussian distribution .    .",
    "a rescaled expected squared jump distance converges as @xmath87 to a related limit , @xmath92 } \\times\\mathbb{e } \\bigl\\| \\mathbf{x}^{(d)}_{k+1 } - \\mathbf{x}^{(d)}_k \\bigr\\|_{\\mathbf{t}^{(d)}}^2 = \\ell^2 \\times \\alpha(\\ell ) = : j ( \\ell).\\ ] ]    theorem  [ thm.asymp.analysis ] is proved in section  [ sec.proof.main.thm ] .",
    "it establishes limiting values for the acceptance probability and expected squared jump distance , and more importantly for the relationship between them , which is crucial to establishing optimality results as we shall see .",
    "further , ( [ eqn.gen.lim.esjd ] ) shows that , as is common in scaling problems for mcmc algorithms ( e.g. , in @xcite ) , the esjd decomposes into the product of the acceptance probability @xmath88 and the expected squared _ proposed _ jumping distance  @xmath93 , implying an asymptotic independence between the size of the proposed move and the acceptance event . as in the rwm case",
    ", we wish to be able to consider @xmath94 to be a function of the asymptotic acceptance rate @xmath88 .",
    "our next result , which is proved in section  [ sec.proof.bijection .",
    "] , shows that this is indeed possible .",
    "[ prop.func.of.alpha ] for a psmrwm algorithm with noise difference @xmath90 as in  ( [ wbdef ] ) , with jump size determined by @xmath74 as in  ( [ eq.jump.size ] ) , and with limiting asymptotic acceptance rate @xmath88 as in  ( [ eqn.gen.lim.alpha ] ) , the mapping @xmath95 is a continuous decreasing bijection from @xmath96 to @xmath97 $ ] , where @xmath98.\\ ] ]    proposition  [ prop.func.of.alpha ] yields that @xmath99 . when there is no noise in the estimate of the target , as already proved in @xcite , the acceptance rate simplifies to @xmath100 , and the associated expected squared jump distance reads @xmath101 .",
    "thus we may also consider the asymptotic efficiency of a pseudo - marginal algorithm relative to the idealised algorithm if the target were known precisely by defining @xmath102 , which also reads @xmath103.\\ ] ]    the following proposition , which is proved in section  [ proof.prop.bds ] , shows that the relative efficiency can never exceed unity and that it is bounded below by the acceptance rate in the limit as @xmath104 .",
    "[ prop.rel.eff.bds ] with @xmath88 and @xmath105 as defined in ( [ eqn.gen.lim.alpha ] ) and ( [ eqn.rel.eff.gen ] ) respectively , @xmath106     from ( [ eqn.gen.lim.esjd ] ) plotted as a function of the scaling parameter @xmath107 and of the standard deviation , @xmath108 , of the additive noise . in the left - hand panel the additive noise in the log - target is assumed to be gaussian , and in right - hand panel it is assumed to have a laplace distribution . ]",
    "the quantities @xmath88 , @xmath94 and @xmath109 depend upon the distribution of @xmath90 , and hence on the distribution of the additive noise @xmath45 from  ( [ wbdef ] ) .",
    "figure  [ fig.esjd.acc ] considers two particular cases : where the distribution of the additive noise is gaussian , that is , @xmath110 ( which we shall consider further in section  [ sect.optimising ] ) , and where the distribution of the additive noise is laplace ( i.e. , double - exponential ) , with mean @xmath111 and scale parameter @xmath112 .",
    "for each of these two cases , it shows a _ contour plot _ of @xmath94 as a function of the proposal scaling parameter @xmath107 and of the standard deviation of the additive noise , @xmath108 .",
    "figure  [ fig.esjd.rel.acc ] shows the equivalent plots for @xmath105 .     from ( [ eqn.rel.eff.gen ] ) ,",
    "the asymptotic expected squared jump distance relative to the idealised algorithm , plotted as a function of the scaling parameter @xmath107 and of the standard deviation , @xmath108 , of the additive noise . in the left - hand panel the additive noise in the log - target is assumed to be gaussian , and in the right - hand panel it is assumed to have a laplace distribution . ]    our ultimate goal is often to choose @xmath107 to _ maximise _ @xmath94 , and thus obtain an _",
    "optimal _ limiting diffusion ( and hence an approximately optimal algorithm for finite @xmath35 too ) .",
    "we shall use theorem  [ thm.asymp.analysis ] to establish an optimal acceptance rate in a particular limiting regime , in section  [ sect.opt.std ] below .",
    "figure  [ fig.esjd.rel.acc ] illustrates that , except for small values of the scaling , the relative efficiency for a given noise distribution is relatively insensitive to the scaling .",
    "related to this , from figure  [ fig.esjd.acc ] it appears that the optimal scaling [ i.e. , the value @xmath107 which maximises @xmath94 ] is relatively insensitive to the variance of the additive noise .",
    "when there is no noise , the optimum is @xmath114 as first noted in @xcite ; however , the optimum remains close to @xmath115 across a range of variances for both choices of noise distribution .",
    "for these two examples , as might be expected , for any given scaling of the random walk proposal , the efficiency relative to the idealised algorithm decreases as the standard deviation of the noise increases , a phenomenon that is investigated more generally in @xcite .",
    "thus there is an implicit _ cost _ of having to estimate the target density . as a result of this",
    ", we should not expect the optimal acceptance probability for rwm of @xmath1 to hold here .",
    "we next prove that psmrwm in high dimensions can be well - approximated by an appropriate diffusion limit ( obtained as @xmath116 ) .",
    "this provides further justification for measuring efficiency by the esjd , as discussed in detail in @xcite .",
    "briefly , the limiting esjd ( suitably scaled ) is equal to the square of the limiting process s diffusion coefficient , @xmath117 say . by a simple time change argument",
    ", the asymptotic variance of _ any _ monte carlo estimate of interest is inversely proportional to @xmath117 .",
    "minimising variance is thus equivalent to maximising @xmath117 ; that is , @xmath117 becomes ( at least in the limit ) unambiguously the right quantity to optimise . by constrast",
    ", mcmc algorithms which have nondiffusion limits can behave in very different ways , and esjd may not be an appropriate way to compare algorithms in such cases .",
    "we shall consider in this section the psmrwm algorithm applied to a sequence of simple i.i.d .",
    "target densities @xmath118 where @xmath119 is a one - dimensional probability density .",
    "we assume throughout this section that the following regularity assumptions hold .",
    "[ assump.f ] the first four moments of the distribution with density @xmath119 are finite .",
    "the log - likelihood mapping @xmath120 is smooth with second , third , and fourth derivatives globally bounded .",
    "one can verify that under assumption  [ assump.f ] , the target @xmath36 satisfies assumption  [ assump.lim.alpha ] .",
    "it is important to stress that the @xmath121 analysis of section  [ sect.setupesjd ] only relies on the weaker assumption  [ assump.lim.alpha ] , and as discussed at the end of the previous section , is valid for much more general target distributions than the ones with i.i.d . coordinates considered in this section .",
    "the stronger assumption  [ assump.f ] are standard in the diffusion - limit literature and are , perhaps , the simplest from which a diffusion limit is expected to result @xcite . however , these i.i.d .",
    "assumptions have been relaxed in various directions @xcite , and we believe that our diffusion limit theorem  [ thm.diff.lim ] could also be extended to similar settings at the cost of considerably less transparent proofs .    in the remainder of this article",
    "we consider the sequences of scaling functions @xmath122 , with @xmath123 = -\\mathbb{e } \\bigl [ \\bigl ( \\log f(x ) \\bigr)^{\\prime\\prime } \\bigr]\\ ] ] and @xmath124 . indeed , equation ( [ eq.rescaled.grad.hess ] ) is satisfied ; consequently , for a tuning parameter @xmath74 , we consider @xmath35-dimensional rwm proposals with scaling @xmath125 as in  ( [ eq.jump.size ] ) .",
    "the quantity @xmath126 , which quantifies the roughness and the scale of the marginal density @xmath127 , has been introduced in the definition of the rwm jump - size ( [ eq.jump.size.diff.lim ] ) so that all our limiting results on the _ optimal _ choice of parameter @xmath107 are independent of @xmath127 .",
    "the main result of this section is a diffusion limit for a rescaled version @xmath128 of the first coordinate process . for time @xmath129 we define the piecewise - constant continuous - time process @xmath130 with the notation @xmath131 so that @xmath132 is the first coordinate of @xmath133 .",
    "note that in general the process @xmath128 is not markovian .",
    "the next theorem shows that nevertheless , in the limit @xmath116 , the process @xmath128 converges weakly to an explicit langevin diffusion .",
    "this result thus generalises the original rwm diffusion limit proved in @xcite .",
    "[ thm.diff.lim ] let @xmath134 be a finite time horizon .",
    "for all @xmath135 let each markov chain and the additive noise satisfy assumption  [ ass.noise.diff.indep ] , let the sequence of product form densities @xmath36 satisfy the regularity assumption  [ assump.f ] and set the scale of the jump proposals as in equation ( [ eq.jump.size.diff.lim ] ) . then , as @xmath116 , @xmath136 in the skorokhod topology on @xmath137)$ ] , where @xmath138 satisfies the langevin sde @xmath139 with initial distribution @xmath140 and @xmath141 a standard brownian motion .",
    "the speed function @xmath117 is proportional to the asymptotic rescaled esjd function @xmath142 , @xmath143 with the constant of proportionality @xmath126 defined by equation ( [ e.i ] ) .",
    "the time change argument discussed before theorem  [ thm.diff.lim ] shows that the quantity @xmath144 exactly measure the loss of mixing efficiency ( computational time not taken into consideration ) when exact evaluations of the target density are replaced by unbiased estimates ; as already mentioned , the pseudo - marginal algorithm always has worse mixing properties than the idealised algorithm .",
    "we next consider the question of optimising the psmrwm . now , when examining the efficiency of a standard rwm , the expected computation ( cpu ) time is usually not taken into account since it is implicitly assumed to be independent of the choice of tuning parameter(s ) .",
    "this may indeed be approximately true for the rwm .",
    "however , for the psmrwm the expected cpu time for a single iteration of the algorithm is usually approximately inversely proportional to the variance of the estimator @xmath145 . for this reason",
    ", we measure the efficiency of the psmrwm through a rescaled version of the esjd , @xmath146 of course , for any increasing function @xmath147 , the quantity @xmath148 is a possible measure of efficiency .",
    "however , the discussion at the start of section  [ sec.diff.lim ] indicates that ( [ def.efficiency ] ) is the appropriate measure of efficiency in the high - dimensional asymptotic regime considered in this article .    in the remainder of this section",
    ", we implicitly assume that the target distributions satisfy assumption  [ assump.lim.alpha ]",
    ".      we shall restrict attention to the case in which the additive noise follows a gaussian distribution .",
    "more precisely , we shall assume the following , which we shall refer to for brevity as `` the standard asymptotic regime '' ( sar ) :    [ ass.standard.regime ] for each @xmath149 and @xmath150 , we have an unbiased estimator @xmath145 of @xmath151 , such that @xmath152 follows a gaussian distribution with variance  @xmath153 .",
    "furthermore , the expected one - step computing time is inversely proportional to  @xmath153 .",
    "intuitively , assumption  [ ass.standard.regime ] are designed to model the situation where @xmath151 is estimated as a product of @xmath154 averages of @xmath0 i.i.d .",
    "samples in the limit as @xmath155 and with @xmath156 . for a fixed large @xmath154 , approximate normality follows from the central limit theorem ; moreover @xmath157 for some @xmath14 , and the computational time is proportional to @xmath0 and hence to @xmath158 .",
    "assumption  [ ass.standard.regime ] have recently been shown to hold more generally , in the context of particle filtering for a hidden markov model ; see @xcite .",
    "there are other natural situations where multiplicative forms for the importance sampling estimator of the likelihood might make the estimator well - approximated as a log - gaussian , for example , in correcting for a pac likelihood approximation ; see @xcite .    under the sar of assumption  [ ass.standard.regime ]",
    ", we will prove an optimality result in section  [ sect.opt.std ] which specifies a particular optimal variance for the estimate of the log - target .",
    "in this section we consider a sequence @xmath36 of target distributions satisfying assumption  [ assump.lim.alpha ] and assume that each unbiased estimator satisfies the independence in assumption  [ ass.noise.diff.indep ] . under these assumptions ,",
    "the rescaled esjd of the psmrwm algorithm with jump size ( [ eq.jump.size ] ) is described by theorem  [ thm.asymp.analysis ] . under the sar , that is , assumption  [ ass.standard.regime ] , and with @xmath159 = \\sigma^2 $ ] , the noise difference is @xmath160 . since the mean one - step computing time",
    "is assumed to be inversely proportional to the variance , @xmath153 , the asymptotic efficiency , as @xmath116 , is proportional to @xmath161 where @xmath162 stands for the asymptotic rescaled @xmath163 identified in theorem  [ thm.asymp.analysis ] , that is , @xmath94 , in the special case where @xmath164 .",
    "figure  [ fig.gauss.eff.acc ] provides a contour plot of this efficiency @xmath165 , relative to the highest achievable efficiency , and of the logarithm of the asymptotic acceptance rate @xmath88 , both as functions of the scaling parameter @xmath107 and of the standard deviation , @xmath108 .",
    "it also provides a plot of the profile @xmath166 as a function of @xmath107 , again relative to the highest achievable value .",
    "as previously suggested by figure  [ fig.esjd.acc ] , we see that the conditional optimal value of @xmath107 is relatively insensitive to the value of @xmath108 .",
    "the point at which the maximal efficiency is achieved is detailed precisely in corollary  [ cor.max.gauss ] below .    , and of the base-10 logarithm of the asymptotic acceptance probability @xmath88 , and a plot of the profile relative efficiency @xmath167 , all for the scenario where the additive noise arises from the sar .",
    "]    [ cor.max.gauss ] the efficiency @xmath165 is maximised ( to three decimal places ) when the variance @xmath153 of the log - noise is @xmath168 and the scaling parameter @xmath107 is @xmath169 at which point the corresponding asymptotic acceptance rate is @xmath170 as @xmath171 the optimal scaling satisfies @xmath172 , and as @xmath173 the optimal variance satisfies @xmath174 .    for convenience , write @xmath175 , and introduce three independent standard gaussian random variables @xmath176 .",
    "notice that @xmath177 and @xmath178 \\nonumber\\\\ & = & \\tau^2 \\ell^2 \\mathbb{p } \\bigl[v < \\bigl(- \\tau^2/2 + \\tau u \\bigr)/\\ell-\\ell/2 \\bigr ] \\nonumber\\\\ \\label{eq.closed.form.eff}&= &   \\tau^2 \\ell^2 \\mathbb{p}\\bigl(\\ell v - \\tau u < - \\bigl ( \\tau ^2 + \\ell^2 \\bigr ) / 2 \\bigr ] \\\\",
    "\\nonumber & = &   \\tau^2 \\ell^2 \\mathbb{p } \\bigl [ \\sqrt{\\ell^2+\\tau^2 } z < - \\bigl(\\tau^2 + \\ell^2 \\bigr ) / 2 \\bigr ] \\\\ & = & \\tau^2 \\ell^2 \\phi \\bigl(-\\tfrac{1}{2}\\sqrt { \\tau^2+\\ell",
    "^2 } \\bigr).\\nonumber\\end{aligned}\\ ] ] for fixed @xmath179 , the quantity @xmath180 is maximised when @xmath181 , at which point the efficiency is @xmath182 .",
    "this is maximised numerically when @xmath183 ( to three decimal places ) , and at this point @xmath184 and @xmath185 with the corresponding numerical values as stated .    differentiating ( [ eq.closed.form.eff ] ) with respect to @xmath107",
    "we find that the optimal scaling satisfies @xmath186 the result for large @xmath187 follows from the relationship @xmath188 as @xmath189 .",
    "the symmetry of the function @xmath190 in @xmath191 and @xmath107 then provides the result for large @xmath107 .",
    "\\(1 ) this leads to a new optimal scaling for standard gaussian targets of @xmath192 with @xmath193 , and contrasts with the corresponding formula @xmath194 , with @xmath195 , for the usual random walk metropolis algorithm @xcite ; recall that @xmath196 satisfies @xmath197 .",
    "( 2 ) in the discussion of figure  [ fig.esjd.acc ] it was noted that for a gaussian or laplace noise regime the optimal scaling at a particular noise variance , @xmath153 , is insensitive to the value of @xmath153 . from figure  [ fig.gauss.eff.acc ] and from the symmetry of expression ( [ eq.closed.form.eff ] ) , the optimal variance at a particular scaling @xmath107 is also insensitive to the value of @xmath107 .",
    "moreover as @xmath198 the optimal variance is @xmath199 , which corresponds ( at least to 2 decimal places ) with the value obtained in @xcite .",
    "\\(3 ) in practice , @xmath153 might be a function of a _ discrete _ number @xmath0 of samples or particles and hence only take a discrete set of values .",
    "in particular , if the variance in the noise using @xmath200 is already lower than @xmath201 , then there can be little gain in increasing @xmath0 .",
    "\\(4 ) in many problems the computational cost of obtaining an unbiased estimate of the target is much larger than the cost of the remainder of the algorithm , but this is not always the case .",
    "consider therefore the more general problem where the cost of obtaining a single unbiased estimate is @xmath202 times the cost of the remainder of the algorithm .",
    "in this case the efficiency functional should be expressed as @xmath203 and the optimal acceptance rate is a function of @xmath202 which varies between @xmath204 ( as @xmath205 ) and @xmath206 ( as @xmath207 ) .",
    "figure  [ fig.gauss.eff.acc ] shows that in contrast to the insensitivity of the optimal scaling to the variance of the noise , the acceptance rate at this optimum could potentially vary by a factor of @xmath208 or more .",
    "thus if a particular scaling of the jump proposals maximises @xmath94 for some particular noise distribution and variance , then that scaling should be close to optimal across a wide range of noise distributions and variances .",
    "however , tuning to a particular acceptance rate , whilst more straightforward in practice , could lead to a sub - optimal scaling if the noise distributions encountered in the tuning runs are not entirely representative of the distributions that will be encountered during the main run .",
    "our theory applies in the limit when the dimension @xmath35 of the ( marginal ) target @xmath52 goes to infinity .",
    "however , using a similar argument to that in @xcite , when @xmath209 , it can be shown that under the sar with the proposal as in ( [ eqn.rwm.prop ] ) the @xmath163 and acceptance rate are @xmath210 \\quad\\mbox{and}\\\\   \\alpha(\\lambda , d ) & = &   \\mathbb{e } \\biggl [ \\phi \\biggl(-\\frac{\\lambda}{2}\\| \\mathbf{z}\\| + \\frac{b}{\\lambda\\|\\mathbf{z}\\| } \\biggr ) \\biggr],\\end{aligned}\\ ] ] where @xmath211 and @xmath212 .",
    "numerical optimisation of the efficiency function , @xmath213 for @xmath214 , and @xmath215 produces a steady decrease in @xmath216 from @xmath217 to @xmath218 and in @xmath219 from @xmath220 to @xmath221 , and a similarly steady increase in @xmath222 from @xmath223 to @xmath224 .",
    "thus , at least for gaussian targets and with efficiency measured by esjd , the asymptotic results for the optimal scaling and optimal variance are applicable in any dimension but there may be a small increase in the optimal acceptance rate , as is found for the nonpseudo - marginal rwm ( e.g. , @xcite ) .    in the simulation study of section  [ sect.sim.study ] below , we find that corollary  [ cor.max.gauss ] and its associated formulae provide a good description of the optimal settings for a particle filter with @xmath225 and @xmath226 .",
    "in this section we restrict attention to the sar of section  [ sec.standard.regime ] .",
    "corollary  [ cor.max.gauss ] suggests that the optimal efficiency should be obtained by choosing the number of unbiased estimates , @xmath0 , such that the variance in the log - target is approximately @xmath227 .",
    "the scale parameter , @xmath228 , should be set so that the acceptance rate is approximately @xmath229 .",
    "since the constant of proportionality relating @xmath228 and @xmath107 is unknown in practice , we can not simply set @xmath230 .    in practice",
    "the assumptions underlying this result may not hold : the dimension of the parameter space is finite , the distribution of the noise , @xmath44 , may not be gaussian , and it is likely to also vary with position , @xmath7 .",
    "we conduct a simulation study to provide an indication of both the extent of and the effect of such deviations .",
    "we use the particle marginal rwm algorithm ( pmrwm ) of @xcite to perform exact inference for the lotka ",
    "volterra predator - prey model ; see @xcite for a more detailed description of the pmrwm which focusses on this particular class of applications",
    ". starting from an initial value , which is , for simplicity , assumed known , the two - dimensional latent variable @xmath231 evolves according to a markov jump process ( mjp ) .",
    "each component is observed at regular intervals with gaussian error of an unknown variance .",
    "appendix  [ sec.lotka.details ] provides details of the observation regime and of the transitions of the mjp and their associated rates .",
    "it also provides the parameter values , the priors and the lengths of the mcmc runs .",
    "an initial run provided an estimate of a central value , @xmath232 ( the vector of posterior medians ) , and the posterior variance matrix , @xmath233 .",
    "since the shape of the target distribution , and hence the optimal shape of the proposal , is unknown , we follow the frequently used strategy for the rwm ( e.g. , @xcite ) of setting the proposal covariance matrix to be proportional to @xmath233 . from remark  [ rem1 ] following corollary  [ cor.max.gauss ] , we set @xmath234 with @xmath235 corresponding to an optimal tuning for a gaussian target .",
    "let @xmath236 define the set of choices for the number of particles , @xmath0 , and let @xmath237 define the set of choices for the relative scaling , @xmath238 .",
    "for each @xmath239 in @xmath240 an mcmc run of at least @xmath241 iterations was performed starting from @xmath232 . for diagnostic purposes runs of at least @xmath242 iterations",
    "were performed with @xmath243 and @xmath244 ( so @xmath245 throughout ) .",
    "we perform three checks on our assumptions .",
    "the diagnostic runs provide a sample from the distribution of @xmath44 , the estimate of the log - target at a proposed value ; this allows us to investigate the second part of assumption  [ ass.noise.diff.indep ] and both parts of assumption  [ ass.standard.regime ] .",
    "we first examine the sar assumption  [ ass.standard.regime ] .",
    "figure  [ fig.qqplots ] shows qqplots for @xmath246 , @xmath247 and @xmath248 against a gaussian distribution ; it is clear that at @xmath246 the right - hand tail is slightly too light and the left - hand tail is much heavier than that of a gaussian .",
    "similar but much smaller discrepancies are present at @xmath247 , whilst at @xmath248 the noise distribution is almost indistinguishable from that of a gaussian .",
    ", when @xmath246 ( left panel ) , @xmath247 ( centre ) , and @xmath248 ( right ) . ]",
    "the left - hand panel in figure  [ fig.morediags ] plots @xmath249 $ ] against @xmath250 and includes a line with the theoretical slope of @xmath251 and passing through an  additional point at @xmath252 .",
    "the heavy left - hand tail at @xmath246 leads to a considerably higher variance than that which would arise under the sar ; however , even by @xmath253 the fit is reasonably close .",
    "is plotted against the logarithm of the number of particles used ; the centre and right panels are plots of the logarithms of the empirical estimates of the moment generating functions of @xmath254 and @xmath255 ( @xmath256 and @xmath257 , resp . ) against @xmath258 .",
    "the additional lowest curve in the centre panel @xmath259 and in the right - hand panel is the logarithm of @xmath257 with @xmath252 , and constitutes our best estimate of `` truth . '' ]    we assess the degree of dependence of the distribution of @xmath44 on the position @xmath6 by considering the joint distribution of @xmath44 and @xmath260 , the true log - target evaluated at @xmath52 , where @xmath52 is distributed according to the target . for a particular @xmath0 ,",
    "all of the runs with @xmath261 provide a combined sample of size @xmath262 from the distribution of the estimate of the log - target at the current value , @xmath263 , whereas ( after scaling so that @xmath264 ) each run with @xmath244 provides a sample of size @xmath265 from the distribution of @xmath44 at @xmath245 . equation ( [ eqn.joint.stat.w ] ) shows that subject to assumption  [ ass.noise.diff.indep ] , @xmath45 and @xmath255 are independent and that the density of @xmath45 is an exponentially tilted version of the density of @xmath44 .",
    "these two properties lead directly to the following .",
    "[ prop.diagnostic ] if assumption  [ ass.noise.diff.indep ] hold , the identity @xmath266 / \\mathbb{e } \\bigl [ \\exp \\bigl\\ { { ( t+1)w^ * } \\bigr\\ } \\bigr ] = \\mathbb{e } \\bigl [ \\exp({tl } ) \\bigr]\\ ] ] holds for any @xmath267 such that all the above three expectations are well defined .",
    "the right - hand side of ( [ eqn.mgfa ] ) is independent of the noise distribution , or equivalently of the number of particles , @xmath0 . moreover ,",
    "if the noise is small enough then the ratio on the left - hand side should provide a good estimator of the true moment generating function ( mgf ) of @xmath255 even if there is dependence ( since the impact of any dependence will be small ) .    in our scenario , realisations of @xmath255",
    "are typically between @xmath268 and @xmath269 with a  mode at approximately @xmath270 , so the mgfs of @xmath255 and @xmath254 are dominated by the term @xmath271 , whatever the noise distribution . to be able to discern any differences we therefore consider for each value of @xmath0 , shifted estimators of the mgfs of @xmath254 and of @xmath255 @xmath272 \\quad\\mbox{and}\\\\ m_2(t ) & : = & m_1(t ) \\biggl(\\frac{1}{n_2}\\sum _ { i=1}^n\\exp \\bigl[(t+1)w^{*(i ) } \\bigr ] \\biggr)^{-1}.\\end{aligned}\\",
    "] ] the central panel of figure  [ fig.morediags ] shows @xmath256 with a separate curve for each value of  @xmath0 ; the lowest curve is our best estimate of the true mgf of @xmath255 ( @xmath257 from @xmath252 ) .",
    "the right - hand panel shows @xmath257 for each value of @xmath0 .",
    "clearly the curves in the right - hand panel do not coincide , and so the assumption of independence does not hold precisely .",
    "however , it is clear from the very different vertical scales of the two figures that _ most _ of the difference between the distribution of @xmath254 for any given @xmath0 and the distribution of @xmath255 _ can _ be explained by assumption  [ ass.noise.diff.indep ] .",
    "we now consider an empirical measure of efficiency @xmath273 , the quotient of the minimum ( over the parameters ) effective sample size and the cpu time .",
    "the left - hand panel of figure  [ fig.effsimstud ] shows @xmath273 plotted against @xmath238 for different values of @xmath0 , whilst the right - hand panel shows @xmath273 plotted against @xmath0 for different values of @xmath238 .",
    "the optimal ( over @xmath274 ) value for @xmath238 is either @xmath275 or @xmath276 whatever the value of @xmath0 , which is consistent with the expected insensitivity of the optimal scaling and suggests that the target is at least approximately gaussian .",
    "the optimal ( over @xmath277 ) value for @xmath0 is either @xmath278 , @xmath279 , or @xmath247 , corresponding to an optimal @xmath153 ( estimated from the sample for @xmath44 ) of either @xmath276 , @xmath280 or @xmath281 , again ( as far as can be discerned ) showing no strong sensitivity to @xmath238 . finally the overall optimum occurs at @xmath282 and @xmath283 with an acceptance rate of @xmath284 .",
    "the optimal @xmath153 is slightly lower than the theoretically optimal value of @xmath227 .",
    "further theoretical investigations ( using numerical integration ) for a true @xmath285-dimensional gaussian target corrupted by noise subject to the sar show that esjd per second is still optimised at @xmath286 ; however empirical investigations show that the ess@xmath287sec for this target is optimised at a value of @xmath288 .",
    "the discrepancy between the theory and our simulation study is therefore likely to be attributable to this discrepancy between ess and esjd in low - dimensional settings .",
    "the relatively high acceptance rate is a consequence of this lower variance and fits with our theory since from ( [ eq.closed.form.eff ] ) the acceptance rate should be @xmath289 .",
    ", measured in terms of minimum effective sample size per cpu second , plotted against ( left panel ) @xmath238 for different values of @xmath0 and ( right panel ) @xmath153 ( estimated from the sample of @xmath44 at the posterior median , @xmath232 ) for different values of @xmath238 . ]",
    "equation ( [ eqn.joint.stat.w ] ) yields that @xmath290 has density @xmath291 satisfying @xmath292 thus @xmath293 this fact will be used in the proofs of theorem  [ thm.asymp.analysis ] and proposition  [ prop.func.of.alpha ] .      for notational convenience",
    ", we drop the index @xmath294^{(d)}$ ] when the context is clear . as in section  [ sect.highdim ] , the hessian matrix of the log - likelihood @xmath295 at @xmath296",
    "is denoted by @xmath297_{1 \\leq i , j \\leq d}$ ] .",
    "* _ proof of equation _ ( [ eqn.gen.lim.alpha ] ) .",
    "the mean acceptance probability equals @xmath298 \\\\ & = & \\mathbb{e } \\bigl [ f \\bigl ( l\\bigl(\\mathbf{x}+\\lambda^{(d ) } \\mathbf{z } \\bigr)-l(\\mathbf{x } ) + b \\bigr ) \\bigr]\\end{aligned}\\ ] ] with @xmath299 , jump scale @xmath300 , random variable @xmath301 independent from @xmath52 , and accept - reject function @xmath302 .",
    "algebra shows that for any @xmath303 and @xmath304 , we have @xmath305 = \\phi(-\\ell/2 + b/\\ell ) + e^{b}\\phi(-\\ell/2 - b / \\ell)$ ] . by ( [ eqn.rho.symm ] ) @xmath306\\\\   & & \\qquad= \\int_{-\\infty}^\\infty h(b ) \\bigl ( e^{-b/2}\\phi(-\\ell/2 + b/\\ell ) + e^{b/2}\\phi(-\\ell/2 - b/\\ell ) \\bigr )",
    "\\,db \\\\ & & \\qquad= 2\\int_{-\\infty}^\\infty h(b ) e^{-b/2}\\phi ( - \\ell/2 + b/\\ell ) \\,db = 2 \\mathbb{e } \\bigl[\\phi ( -\\ell/2 + b/\\ell ) \\bigr].\\end{aligned}\\ ] ] since @xmath147 is continuous and bounded , in order to prove equation ( [ eqn.gen.lim.alpha ] ) , it therefore suffices to show that @xmath307 converges in law to a gaussian distribution with mean @xmath308 and variance @xmath93 .",
    "a second - order expansion yields @xmath309 with remainder @xmath310    \\mathbf{z } \\rangle    \\,dt$ ] .",
    "slutsky s lemma shows that to finish the proof of ( [ eqn.gen.lim.alpha ] ) it suffices to verify that @xmath311 converges in law to a centred gaussian distribution with variance @xmath312 and that @xmath313 in probability . *",
    "* note that conditionally upon @xmath314 the quantity @xmath315 has a centred gaussian distribution with variance @xmath316 .",
    "equation ( [ eq.rescaled.grad.hess ] ) shows that @xmath317 converges in law to a gaussian distribution with variance @xmath93 . * * conditionally upon @xmath318 the quantity @xmath319 has the same distribution as @xmath320 where @xmath321 is the spectrum of the hessian matrix @xmath322 .",
    "the conditional mean thus equals the rescaled laplacian @xmath323 , and the conditional variance is @xmath324 / \\bigl ( s_l^{(d ) } \\bigr)^2.\\ ] ] markov s inequality , equations ( [ eq.rescaled.grad.hess ] ) and ( [ eqn.eigen.cip ] ) , and the hypothesis @xmath325 yield that @xmath326 converges in probability to @xmath327 . * * equation ( [ eqn.hessian.regularity ] ) shows that the remainder @xmath328 converges to zero in probability . * _ proof of equation _ ( [ eqn.gen.lim.esjd ] ) .",
    "the proof of equation  ( [ eqn.gen.lim.esjd ] ) follows from equation  ( [ eqn.gen.lim.alpha ] ) .",
    "note that we have @xmath329 } \\times\\mathbb{e } \\bigl\\| \\mathbf{x}^{(d)}_{k+1 } - \\mathbf{x}^{(d)}_k \\bigr\\|_{\\mathbf{t}^{(d)}}^2 \\\\ & & \\qquad:= \\ell^2 \\mathbb{e } \\biggl [ \\frac { \\| \\mathbf{z } \\|_{\\mathbf { t}^{(d)}}^2}{\\operatorname{trace }   [ { \\mathbf{t}^{({d } ) } } ] } \\times f \\bigl ( l\\bigl ( \\mathbf{x}+\\lambda^{(d ) } \\mathbf{z}\\bigr)-l(\\mathbf{x } ) + b \\bigr ) \\biggr].\\end{aligned}\\ ] ] since @xmath330 = \\alpha(\\ell)$ ] , to prove equation ( [ eqn.gen.lim.esjd ] ) it suffices to verify that @xmath331 } - 1 \\biggr\\ } \\times f \\bigl ( l \\bigl(\\mathbf{x}+\\lambda^{(d ) } \\mathbf { z}\\bigr)-l(\\mathbf{x } ) + b \\bigr ) \\biggr]\\ ] ] converges to zero as @xmath116 .",
    "since the function @xmath147 is bounded , the conclusion follows once we have proved that @xmath332 } - 1   ) ^2 ] $ ] converges to zero .",
    "diagonalisation of the symmetric matrix @xmath79 in an orthonormal basis shows that this last quantity equals @xmath333 /\\break",
    "\\operatorname{trace }   [ { \\mathbf{t}^{({d } ) } } ] ^2 $ ] so that the conclusion directly follows from equation ( [ eqn.eccentric.distance ] ) .",
    "the dominated convergence theorem shows that @xmath334 $ ] is continuous and converges to zero as @xmath107 tends to infinity .",
    "since the limiting acceptance probability can also be expressed as @xmath335 for @xmath336 independent from all other sources of randomness , it also follows that the limiting acceptance probability @xmath88 converges to @xmath337 as @xmath107 converges to zero .",
    "to finish the proof of proposition  [ prop.func.of.alpha ] , it remains to verify that the function @xmath338 is strictly decreasing . to this end , we will establish that the derivative @xmath339 is strictly negative . applying ( [ eqn.rho.symm ] ) ,",
    "the derivative of @xmath340 is @xmath341 e^{-b/2 } h(b ) \\,db \\\\ & = &   -\\int_{b \\in\\mathbf{r } } \\varphi [ -\\ell/ 2 + b/ \\ell ] \\biggl\\{1 + \\frac{2b}{\\ell^2 } \\biggr\\ } e^{-b/2 } h(b ) \\,db\\end{aligned}\\ ] ] with @xmath342 the density of a standard gaussian distribution .",
    "algebra shows that the function @xmath343 $ ] is odd so that the derivative simplifies , @xmath344 e^{-b/2 } h(b ) \\,db.\\ ] ] this quantity is clearly strictly negative , completing the proof of proposition  [ prop.func.of.alpha ] .",
    "the upper bound follows from a similar argument to that in @xcite .",
    "let @xmath345 be an independent copy of @xmath44 , and let @xmath346 be independent from any other source of randomness .",
    "relating @xmath345 to @xmath45 through ( [ eqn.joint.stat.w ] ) yields @xmath347 & = & \\mathbb{e } \\bigl[{\\exp ( \\widetilde{w})\\wedge \\exp(v)\\exp \\bigl(w^*\\bigr ) } \\bigr]\\\\ & \\le & \\mathbb{e } \\bigl[{1 \\wedge\\exp(v ) } \\bigr]=2 \\times\\phi(-\\ell/2);\\end{aligned}\\ ] ] we have applied jensen s inequality twice to the function @xmath348 which is concave in both @xmath26 and @xmath349 . since @xmath350 $ ] , the upper bound follows .",
    "the lower bound follows from a similar argument to that used in @xcite .",
    "we note that @xmath351 .",
    "@xmath138 and @xmath90 are independent by assumption ; as @xmath352 $ ] , the result follows on taking expectations with respect to both of these variables .      in this section",
    "we use the following notation .",
    "we write @xmath353 when the absolute value of the quotient @xmath354 is bounded above by a constant which is independent of the index @xmath154 ; we write @xmath355 if @xmath356 and @xmath357 .",
    "for @xmath358 we write @xmath359 $ ] instead of @xmath360 $ ] . the metropolis ",
    "hastings accept - reject function is the globally lipschitz function @xmath361 .",
    "the log - likelihood function is denoted by @xmath362 in this section .",
    "we drop the index @xmath363 when the context is clear .",
    "the proof follows ideas from @xcite , which itself is an adaptation of the original paper @xcite .",
    "it is based on @xcite , theorem @xmath364 , chapter @xmath365 , which gives conditions under which the finite dimensional distributions of a sequence of processes converge weakly to those of some markov process .",
    "@xcite , corollary @xmath366 , chapter @xmath367 , provides further conditions for this sequence of processes to be relatively compact in the appropriate topology and thus establish weak convergence of the stochastic processes themselves .",
    "the situation is slightly more involved than the one presented in @xcite ; the proof needs a homogenisation argument since the processes @xmath368 and @xmath369 evolve on two different time scales .",
    "indeed , it will become apparent from the proof that the process @xmath368 takes @xmath370 steps to mix while the process @xmath369 takes @xmath371 steps to mix . in order to exploit this time - scales separation ,",
    "we introduce an intermediary time scale @xmath372 where @xmath373 is an exponent whose exact value is not important to the proof .",
    "the intuition is that after @xmath374 steps the process @xmath369 has mixed while each coordinate of @xmath375 has only moved by an infinitesimal quantity .",
    "we introduce the subsampled processes @xmath376 and @xmath377 defined by @xmath378 one step of the process @xmath376 ( resp . , @xmath377",
    ") corresponds to @xmath379 steps of the process @xmath368 ( resp . , @xmath369 ) .",
    "we then define an accelerated version @xmath380 of the subsampled first coordinate process @xmath381 . in order to prove a diffusion limit for the first coordinate of the process @xmath368",
    ", one needs to accelerate time by a factor of @xmath35 ; consequently , in order to prove a diffusion limit for the process @xmath376 , one needs to accelerate time by a factor @xmath382 , and thus define @xmath380 by @xmath383 the proof then consists of showing that the sequence @xmath384 converges weakly in the skorohod topology towards the limiting diffusion ( [ e.limiting.diffusion ] ) and verifying that @xmath385}$ ] converges to zero in probability ; this is enough to prove that the sequence @xmath128 converges weakly in the skorohod topology towards the limiting diffusion ( [ e.limiting.diffusion ] ) .",
    "the proof is divided into three main steps .",
    "first , we show that the finite dimensional marginals of the process @xmath384 converge to those of the limiting diffusion ( [ e.limiting.diffusion ] ) .",
    "second , we establish that the sequence @xmath380 is weakly relatively compact .",
    "these two steps prove that the sequence @xmath380 converges weakly in the skorohod topology towards the diffusion ( [ e.limiting.diffusion ] ) . as a final step ,",
    "we prove that the quantity @xmath386}$ ] converges to zero in probability , establishing the weak convergence of the sequence @xmath128 towards the diffusion ( [ e.limiting.diffusion ] ) . before embarking on the proof",
    "we define several quantities that will be needed in the sequel .",
    "we denote by @xmath387 the generator of the limiting diffusion @xmath388 .",
    "similarly , we define @xmath389 and @xmath390 the approximate generators of the first coordinate process @xmath391 and its accelerated version @xmath392 ; for any smooth and compactly supported test function @xmath393 , vector @xmath394 and scalar @xmath395 , we have @xmath396 , \\vspace*{5pt}\\cr { \\displaystyle}\\mathcal{l}^{(d ) } \\varphi(\\mathbf{x},w ) = \\mathbb{e}_{\\mathbf{x}^{(d)},w } \\bigl [ \\varphi\\bigl(x^{(d)}_{1,1}\\bigr ) - \\varphi(x_1 ) \\bigr ] / \\delta , \\vspace*{5pt}\\cr { \\displaystyle}\\widetilde{\\mathcal{l}}^{(d ) } \\varphi(\\mathbf{x},w ) = \\mathbb{e}_{\\mathbf{x}^{(d)},w } \\bigl [ \\varphi\\bigl(\\widetilde{x}^{(d)}_{1,1 } \\bigr ) - \\varphi(x_1 ) \\bigr ] / ( t_d \\times\\delta ) } \\ ] ] with @xmath397 .",
    "note that although @xmath398 is a scalar function , the functions @xmath399 and @xmath400 are defined on @xmath401 . in the sequel we sometimes write @xmath402 instead of @xmath403 .      in this section",
    "we prove that the finite dimensional distributions of the sequence of processes @xmath380 converge weakly to those of the diffusion ( [ e.limiting.diffusion ] ) .",
    "since the limiting process is a scalar diffusion , the set of smooth and compactly supported functions is a core for the generator of the limiting diffusion ( @xcite , theorem @xmath281 , chapter @xmath367 ) ; in the sequel , one can thus work with test functions belonging to this core only . to prove the convergence of the finite dimensional marginals",
    ", one can apply @xcite , chapter  @xmath365 , theorem @xmath364 , corollary  @xmath404 , to the pair @xmath405 defined by @xmath406 \\,ds\\quad \\mbox{and } \\nonumber \\\\[-8pt ] \\label{eq.xi.phi}\\\\[-8pt ] \\nonumber \\varphi^{(d)}(t ) & = &   \\widetilde{\\mathcal{l}}^{(d ) } \\varphi \\bigl ( \\widetilde{\\mathbf{x}}^{(d)}_{\\lfloor{td / t_d } \\rfloor } , \\widetilde { w}^{(d)}_{\\lfloor{td / t_d } \\rfloor } \\bigr).\\end{aligned}\\ ] ] to establish that this result applies , we will concentrate on proving that for any smooth and compactly supported function @xmath393 the following limit holds : @xmath407 for @xmath408 an i.i.d .",
    "sequence of random variables distributed according to @xmath409 and @xmath410 .",
    "equation ( [ e.finite.dim.conv ] ) implies equation ( 8.11 ) of @xcite , chapter  4 , and the stationarity assumption implies equations ( 8.8 ) and ( 8.9 ) of @xcite , chapter  4 . to verify that equation ( 8.10 ) of @xcite , chapter  4 , holds , one can notice that for any index @xmath411 we have @xmath412 \\lesssim k \\delta^{1/2}$ ] , which is a direct consequence of the triangle inequality and the fact that @xmath398 is a lipschitz function .",
    "the proof of ( [ e.finite.dim.conv ] ) is based on an averaging argument that exploits the following relationship between the generators @xmath389 and @xmath390 , @xmath413.\\ ] ] equation  ( [ e.telescopin ] ) follows from the telescoping expansion @xmath414 and the law of iterated conditional expectations .",
    "the following lemma is crucial :    [ lem.asymp.gen ] let assumptions [ ass.noise.diff.indep ] and [ assump.f ] be satisfied",
    ". there exist two bounded and continuous functions @xmath415 satisfying the following properties :    let @xmath45 be a random variable distributed as the stationary distribution of the log - noise , @xmath416 , and @xmath88 be the asymptotic mean acceptance probability identified in theorem  [ thm.asymp.analysis ] .",
    "the following identity holds : @xmath417 = \\mathbb{e}\\bigl[b(w)\\bigr]= \\tfrac{1}{2 } \\alpha(\\ell).\\ ] ]    for any smooth and compactly supported function @xmath418 the averaged generator @xmath419 defined for any @xmath420 by @xmath421\\ ] ] satisfies @xmath422 for an i.i.d .",
    "sequence @xmath408 marginally distributed as @xmath409 and constant @xmath126 defined by ( [ e.i ] ) .",
    "@xmath423the above lemma thus shows that the approximate generator @xmath424 / \\delta$ ] asymptotically only depends on the first coordinate @xmath425 and the log - noise @xmath426 .",
    "the proof is an averaging argument for the @xmath427 coordinates @xmath428 ; this is mainly technical and details can be found in appendix  [ sec.asymp.gen.lem ] .",
    "the next step consists in exploiting the separation of scales between the processes @xmath81 and @xmath429 .",
    "[ lem.separation.scale ] let @xmath430 be a bounded measurable function .",
    "suppose that for any @xmath431 the markov chain @xmath432 is started at stationarity .",
    "the following limit holds : @xmath433 \\biggr| = 0,\\ ] ] with @xmath45 distributed according to the stationary distribution @xmath410 .",
    "the above lemma thus shows that @xmath434 steps , with @xmath373 , are enough for the process @xmath369 to mix .",
    "the proof relies on a coupling argument and the ergodic theorem for markov chains .",
    "details can be found appendix  [ sec.sep.sc.lem ] .",
    "we now have all the tools in hands to prove equation ( [ e.finite.dim.conv ] ) .",
    "first , with the notation @xmath435 , the telescoping expansion ( [ e.telescopin ] ) and jensen s conditional inequality yields @xmath436 \\biggr| \\\\ & & \\qquad\\leq   \\mathbb{e } \\biggl| \\frac{1}{t_d } \\sum_{k=0}^{t_d-1 } \\mathcal { l}^{(d ) } \\varphi\\bigl(\\mathbf{x}^{(d)}_{k } , w^{(d)}_{k}\\bigr ) - \\mathcal { l}\\varphi\\bigl(x^{(d)}_{0,1 } \\bigr ) \\biggr|.\\end{aligned}\\ ] ] one can then use the triangle inequality to obtain the bound @xmath437 to complete the proof of the convergence of the finite dimensional distributions of @xmath380 towards those of the limiting diffusion ( [ e.limiting.diffusion ] ) , it remains to prove that @xmath438 as @xmath87 for @xmath439 :    * since the markov chain @xmath440 is assumed to be stationary , the quantity @xmath441 also equals @xmath442 .",
    "lemma  [ lem.asymp.gen ] shows that @xmath443 as @xmath116 .",
    "* the formula for the quantity @xmath444 shows that the expectation @xmath445 also reads @xmath446 \\label{eq.e2.expanded}\\\\[-8pt ] \\nonumber & & \\hspace*{64pt}\\quad\\qquad{}+ \\frac{1}{t_d } \\sum_{k=0}^{t_d-1 } b \\bigl(w^{(d)}_{k}\\bigr ) \\bigl\\ { \\varphi ^{\\prime\\prime } \\bigl(x^{(d)}_{k,1}\\bigr ) - \\varphi^{\\prime\\prime } \\bigl(x^{(d)}_{0,1}\\bigr ) \\bigr\\ } \\biggr|.\\end{aligned}\\ ] ] under assumption [ assump.f ] the function @xmath447 is globally lipschitz ; since @xmath398 is smooth with compact support , the functions @xmath448 and @xmath449 are both globally lipschitz . using the boundedness of the functions @xmath450 and @xmath451 , this yields that the quantity in equation ( [ eq.e2.expanded ] ) is bounded by a constant multiple of @xmath452 . for any index @xmath453",
    "we have @xmath454 so that @xmath455 .",
    "since @xmath456 , the conclusion follows .",
    "* lemma  [ lem.asymp.gen ] shows that one can express the generator of the limiting diffusion  ( [ e.limiting.diffusion ] ) as @xmath457 a'(x )    \\varphi'(x ) + \\frac{\\ell^2}{i } \\mathbb{e}[b(w ) ]    \\varphi ^{\\prime\\prime}(x)$ ] .",
    "the expectation @xmath458 thus also reads @xmath459 \\biggr\\ } a'\\bigl(x^{(d)}_{0,1 } \\bigr ) \\varphi'\\bigl(x^{(d)}_{0,1}\\bigr ) \\\\ & & \\hspace*{28pt}\\qquad\\quad{}+ \\biggl\\ { \\frac{1}{t_d } \\sum_{k=0}^{t_d-1 } b\\bigl(w^{(d)}_{k}\\bigr ) - \\mathbb { e}\\bigl[b(w)\\bigr ] \\biggr\\ } \\varphi^{\\prime\\prime}\\bigl(x^{(d)}_{0,1}\\bigr ) \\biggr|.\\end{aligned}\\ ] ] because the function @xmath398 is smooth with compact support , it follows(cauchy  schwarz ) that this quantity is less than a constant multiple of @xmath460 \\biggr\\}^2 \\biggr]^{1/2 } \\times \\mathbb{e } \\bigl [ a'(x)^2 \\bigr]^{1/2 } \\\\ & & \\qquad{}+ \\mathbb{e } \\biggl| \\frac{1}{t_d } \\sum_{k=0}^{t_d-1 } b\\bigl(w^{(d)}_{k}\\bigr ) - \\mathbb{e}\\bigl[b(w)\\bigr ] \\biggr|.\\end{aligned}\\ ] ] lemma  [ lem.separation.scale ] shows that @xmath461   | \\to0 $ ] , and under assumption [ assump.f ] the expectation @xmath462 $ ] is finite .",
    "therefore , to finishthe proof of the limit @xmath463 , one needs to verify that@xmath464   \\}^2 ] \\to0 $ ] . according to lemma  [ lem.separation.scale ]",
    ", the sequence @xmath465   ) $ ] converges in @xmath466 to zero .",
    "the sequence is also bounded in @xmath467 since the function @xmath450 is bounded .",
    "a sequence bounded in @xmath467 that converges to zero in  @xmath466 also converges to zero in any @xmath468 for @xmath469 .",
    "the conclusion follows .",
    "the process @xmath380 is started at stationarity and the space of smooth functions with compact support is an algebra that strongly separates points .",
    "ethier and kurtz ( @xcite , chapter  4 , corollary  8.6 ) show that in order to prove that the sequence @xmath380 is relatively weakly compact in the skorohod topology it suffices to verify that equations ( 8.33 ) and ( 8.34 ) of @xcite , chapter  4 , hold .    * to prove ( 8.34 ) it suffices to show that for any smooth and compactly supported test function @xmath398 the sequence @xmath470 is bounded .",
    "one can use the telescoping expansion ( [ e.telescopin ] ) , lemma  [ lem.asymp.gen ] and the stationarity of the markov chain @xmath471 and obtain that @xmath472 & & { } + \\mathbb{e } \\biggl| \\frac{1}{t_d}\\sum_{k=0}^{t_d-1 } \\mathcal{g}\\varphi \\bigl(x^{(d)}_{k,1 } , w^{(d)}_{k } \\bigr ) \\biggr|^2 \\\\[2pt ] & \\leq & \\frac{1}{t_d}\\sum_{k=0}^{t_d-1 } \\mathbb{e } \\bigl| \\mathcal{l}^{(d ) } \\varphi\\bigl(\\mathbf{x}^{(d)},w^{(d)}_{k } \\bigr ) - \\mathcal{g}\\varphi \\bigl(x^{(d)}_{k,1 } , w^{(d)}_{k}\\bigr ) \\bigr|^2 \\\\[2pt ] & & { } + \\frac{1}{t_d}\\sum _ { k=0}^{t_d-1 } \\mathbb{e } \\bigl| \\mathcal{g}\\varphi \\bigl(x^{(d)}_{k,1 } , w^{(d)}_{k}\\bigr ) \\bigr|^2 \\\\[2pt ] & = & \\mathbb{e } \\bigl| \\mathcal{l}^{(d ) } \\varphi\\bigl(\\mathbf{x}^{(d)},w \\bigr ) - \\mathcal{g}\\varphi(x_1 , w ) \\bigr|^2 + \\mathbb{e } \\bigl| \\mathcal { g}\\varphi(x_1 , w ) \\bigr|^2 \\\\ & = & o(1 ) + \\mathcal{o}(1).\\end{aligned}\\ ] ] this proves equation  ( 8.34 ) .",
    "* to prove ( 8.33 ) one needs to show that the expectation of @xmath473 \\}$ ] converges to zero as @xmath116 , where the process @xmath474 is defined in equation ( [ eq.xi.phi ] ) .",
    "note that the supremum is less than @xmath475 is the lipschitz constant of @xmath398 .",
    "therefore , since @xmath476 are i.i.d .",
    "standard gaussian random variables such that @xmath477 , the following lemma gives the conclusion .",
    "[ lem.discrepancy ] let @xmath478 an i.i.d .",
    "sequence of standard gaussian random variables @xmath479 .",
    "we have @xmath480 = 0.\\ ] ]    indeed , it suffices to prove that @xmath481 times the expectation of the supremum @xmath482 , with @xmath483 , converges to zero ; this follows from markov s inequality and standard gaussian computations .",
    "this completes the proof of the relative weak compactness in the skorohod topology . the sequence of processes @xmath380 is weakly compact in the skorohod topology , and the finite dimensional distributions of @xmath380 converge to the finite dimensional distribution of the diffusion ( [ e.limiting.diffusion ] ) .",
    "consequently , the sequence of processes @xmath380 converges weakly in the skorohod space @xmath137)$ ] to the diffusion  ( [ e.limiting.diffusion ] ) .",
    "the next section shows that the discrepancy between @xmath128 and @xmath384 is small and thus proves that the sequence of processes @xmath128 also converges to the diffusion ( [ e.limiting.diffusion ] ) .",
    "since @xmath484 is less than the supremum of equation ( [ eq.discrepancy ] ) , lemma  [ lem.discrepancy ] yields that @xmath485}$ ] converges to zero in probability .",
    "this ends the proof of theorem  [ thm.diff.lim ] .",
    "we have examined the behaviour of the pseudo - marginal random walk metropolis algorithm in the limit as the dimension of the target infinity , under the assumption that the noise in the estimate of the log - target at a proposed new value , @xmath6 , is additive and independent of @xmath6 .",
    "subject to relatively general conditions on the target , limiting forms for the acceptance rate and for the efficiency , in terms of expected squared jump distance ( esjd ) , have been obtained .",
    "we examined two different noise distributions ( gaussian and laplace ) , and found that the optimal scaling of the proposal is insensitive to the variance of the noise and to whether the noise has a gaussian or a laplace distribution .",
    "we then examined the behaviour of the markov chain on the target , @xmath6 , and the noise , obtaining a limiting diffusion for the first component of a target with independent and identically distributed components .",
    "the efficiency function in this case is proportional to the speed of the diffusion , thus further justifying the use of esjd in this context .",
    "we identified a `` standard asymptotic regime '' under which the additive noise is gaussian with variance inversely proportional to the number of unbiased estimates that are used . in this regime",
    "the efficiency function is especially tractable , and we showed that it is maximised when the acceptance rate is approximately 7.0% and the variance of the gaussian noise is approximately 3.3 .",
    "we noted that in this regime the optimal noise variance is also insensitive to the choice of scaling .    a detailed simulation study on a lotka  volterra markov jump process using a particle filter suggested that in the scenario considered the assumptions of the standard asymptotic regime are reasonable provided the number of particles is not too low .",
    "furthermore , whilst the assumption that the distribution of the noise does not depend on the current position is not true , variations in the distribution have a small effect on the distribution of the estimates of the log - target compared with the effect of the noise itself .",
    "the optimal scaling was found to be insensitive to the noise variance ( or equivalently the number of particles ) , and the optimal noise variance was relatively insensitive to the choice of scaling .",
    "the overall optimal scaling was consistent with the theoretical value obtained ; however the optimal variance was a little lower than the theoretically optimal value .",
    "investigations showed that this discrepancy can be explained by the differences between our theoretical measure of efficiency ( esjd ) and empirical measures used in the simulation study ( ess ) .",
    "the results from the simulation study suggest that in low dimension a safer option than tuning to a particular variance and acceptance rate might be to take advantage of the insensitivity of the optimal scaling to the variance and vice versa and optimise scaling and variance independently .",
    "the diffusion limit provides strong support for the optimisation strategies suggested by the esjd criterion .",
    "however , in an ideal world it would be good to show that the sequence of algorithms which achieves the minimal optimal integrated autocorrelation time for a given functional might converge to the optimal diffusion .",
    "this is a generic question which is relevant to all diffusion limits for mcmc algorithms , and there are still important open questions regarding the relationships between esjd , diffusion limits , and limiting optimal integrated autocorrelation . in this direction , a recent paper @xcite has shown that diffusion limits can be translated into _ complexity _ results , thus demonstrating that at least the order of magnitude of the number of iterations to `` converge '' can be read off from the diffusion limit .",
    "the optimal variance of 3.28 under the standard asymptotic regime is similar to the value of 2.83 obtained in @xcite under the same noise assumptions and for a scenario where the component of the markov chain on @xmath4 mixes infinitely more slowly than the noise component . indeed ,",
    "as noted in a remark following corollary  [ cor.max.gauss ] , 2.83 is ( to two decimal places ) the optimal variance that we obtain when @xmath486 .",
    "there are many differences between the approaches in @xcite and this article .",
    "for example , we optimise a limiting efficiency for the random walk metropolis with respect to both the scaling and the variance whereas doucet et al . @xcite consider the univariate optimisation of a bound on the efficiency of metropolis ",
    "hastings kernels which satisfy a positivity condition .",
    "that a similar conclusion may be drawn from two very different approaches is encouraging .",
    "let @xmath487 be an i.i.d .",
    "sequence of random variables distributed as @xmath127 , @xmath410 , @xmath488 an i.i.d",
    ". sequence of @xmath479 random variables , @xmath489 an i.i.d .",
    "sequence of random variables uniformly distributed on @xmath490 , and @xmath491 an i.i.d . sequence",
    "distributed as @xmath492 .",
    "all these random variables are assumed to be independent from one another .",
    "for all integers @xmath493 we set @xmath494 and @xmath495 .",
    "we introduce the proposals @xmath496 and define @xmath497 if @xmath498 and @xmath499 otherwise .",
    "we define @xmath500 . for any dimension",
    "@xmath431 the process @xmath501 is a metropolis ",
    "hastings markov chain started at stationarity , that is , @xmath502 , targeting the distribution @xmath36 .",
    "in this section , for notational convenience , we write @xmath503 instead of @xmath504 and @xmath44 instead of @xmath505 .",
    "we set @xmath506 \\quad\\mbox{and}\\quad b(w ) : = \\tfrac{1}{2 } \\mathbb{e } \\bigl [ f \\bigl(\\omega+ w^ * - w\\bigr ) \\bigr]\\ ] ] with @xmath507 and @xmath508 independent from all other sources of randomness .",
    "to prove lemma  [ lem.asymp.gen ] , it suffices to show that the function @xmath450 and @xmath451 are continuous , bounded , satisfy identity ( [ a.averaging.identity ] ) , and that the following two limits hold : @xmath509 - \\ell^2 i^{-1 } a(w)a'(x_1 ) \\bigr|^2 = 0 , \\vspace*{3pt}\\cr { \\displaystyle}\\lim_{d \\to\\infty } \\mathbb{e } \\bigl| \\tfrac{1}2 \\mathbb { e}_d \\bigl [ \\bigl(x^{1,d}_1-x_1 \\bigr)^2 / \\delta \\bigr ] - \\ell^2 i^{-1 } b(w ) \\bigr|^2 = 0.}\\ ] ] we have used the notation @xmath510 $ ] for @xmath511 $ ] .",
    "the fact that the functions @xmath450 and @xmath451 are bounded and continuous follows from the dominated convergence theorem .",
    "* _ proof of equation _ ( [ a.averaging.identity ] ) .",
    "note that @xmath512 = \\frac{1}2 \\mathbb{e}[1 \\wedge\\exp ( \\omega+ b)]$ ] with @xmath513 .",
    "a standard computation show that for any @xmath514 , we have @xmath515 = 2 \\phi(-\\ell/2 + \\beta/ \\ell)$ ] , so that the identity @xmath512 = \\frac{1}{2 } \\alpha(\\ell)$ ] directly follows from the definition of @xmath516 in theorem  [ thm.asymp.analysis ] .",
    "+ for proving the identity @xmath517 = \\frac{1}2 \\alpha(\\ell)$ ] , note that the expectation @xmath517 $ ] equals @xmath518.\\end{aligned}\\ ] ] we have used the change of variable @xmath519 to go from the second line to the third .",
    "this computation shows that @xmath517 : = \\mathbb{e } [ e^{\\omega+ w^*-w }    \\mathrm { i}_{\\{\\omega+w^*-w < 0\\}}]= \\mathbb{e } [ \\mathrm{i}_{\\{\\omega+w^*-w > 0\\}}]$ ] . since @xmath520",
    ", it follows that @xmath521 = \\mathbb{e } \\bigl [ e^{\\omega+ w^*-w } \\mathrm{i}_{\\{\\omega+w^*-w < 0\\}}\\bigr ] + \\mathbb{e } [ \\mathrm{i}_{\\{\\omega+w^*-w > 0\\}}],\\ ] ] and therefore @xmath517 = \\alpha(\\ell)/2 $ ] . * _ proof of equation _ ( [ e.reduction ] ) .",
    "we will only verify that the first limit in equation ( [ e.reduction ] ) holds .",
    "the proof of the second limit is similar but easier . in other words ,",
    "we will focus on proving that the sequence @xmath522 $ ] converges in @xmath523 to @xmath524 .",
    "an integration by parts shows that for any continuous function @xmath525 such that @xmath526 has a finite number of discontinuities , if @xmath527 and @xmath528 have a finite first moment for @xmath529 , the identity @xmath530 = \\mathbb{e}[g'(z)]$ ] holds .",
    "it follows that @xmath531\\\\ & & \\qquad = \\ell i^{-1/2 } \\delta^{1/2 } \\mathbb{e}_d\\bigl [ z_1 \\times f \\bigl(\\omega^{(d ) } + w^ * - w\\bigr ) \\bigr ] \\\\ & & \\qquad= \\ell^2 i^{-1 } \\mathbb{e}_d \\bigl [ f'\\bigl(\\omega^{(d ) } + w^ * - w\\bigr ) \\times a'\\bigl(x_1 + \\ell i^{-1/2 } \\delta^{1/2 } z_1\\bigr ) \\bigr]\\end{aligned}\\ ] ] with @xmath532 . under assumption  [ assump.f ]",
    "the function @xmath533 is globally lipschitz so that , since the function @xmath534 is bounded , one can focus on proving that @xmath535 \\times a'(x_1)\\ ] ] converges in @xmath523 to @xmath536 . by the cauchy ",
    "schwarz inequality , this reduces to proving that @xmath537 - \\mathbb{e}_d\\bigl [ f'\\bigl(\\omega+ w^ * - w\\bigr ) \\bigr ] \\bigr|^4 \\bigr ] = 0.\\ ] ] by the portmanteau s theorem , the dominated convergence theorem , and the definition of @xmath538 , this reduces to proving that for almost every realisation @xmath539 of the i.i.d .",
    "sequence @xmath540 the following limit holds in distribution : @xmath541 under assumption  [ assump.f ] the third derivative of @xmath542 is bounded so that a second order taylor expansion yields that the difference @xmath543 equals @xmath544 ; consequently , @xmath545 for @xmath546 independent from all other sources of randomness .",
    "the law of large numbers shows that for almost every realisation @xmath539 the right - hand side of the above equation converges in distribution towards @xmath547 .      for convenience ,",
    "we first give a high - level description of the reasoning .",
    "we construct processes @xmath548 , @xmath549 , and @xmath550 satisfying the following :    * with high probability @xmath551 for all @xmath552 .",
    "* the process @xmath553 has the same law as the process @xmath548 . * with high probability @xmath554 for all @xmath555 . *",
    "the process @xmath556 is a markov chain that is ergodic with invariant distribution @xmath557 .",
    "one can thus use an approximation of the type @xmath558 \\biggr| \\approx \\mathbb{e } \\biggl| \\frac{1}{t_d } \\sum_{k=0}^{t_d-1 } h(y_{k } ) - \\mathbb { e}\\bigl[h(w)\\bigr ] \\biggr|\\ ] ] and the usual ergodic theorem gives the conclusion .",
    "we use at several places the following elementary lemma .",
    "[ lem.coupling.traj ] let @xmath559 with @xmath560 .",
    "let @xmath561 and @xmath562 be two arrays of @xmath490-valued random variables .",
    "let @xmath563 be a sequence of random variables uniformly distributed on the interval @xmath490 .",
    "we suppose that for all dimension @xmath431 the random variable @xmath564 is independent from @xmath565 and @xmath566 .",
    "consider the event @xmath567 under the assumption that @xmath568 \\lesssim k / \\sqrt{d}$ ] , we have @xmath569    note that @xmath570 $ ] .",
    "since @xmath571 is supposed to be independent from the event @xmath572 , it follows that @xmath573 = 1- \\mathbb { e } [ |p^{(d)}_{j } - q^{(d)}_{j}|     | e^{(d)}_{j-1 } ] $ ] .",
    "the conclusion then directly follows from the bound @xmath568 \\lesssim k / \\sqrt{d}$ ] and @xmath574 .",
    "we now describe the construction of the processes @xmath575 , @xmath553 and @xmath556 . to this end",
    ", we need an i.i.d .",
    "sequence @xmath576 of standard @xmath479 gaussian random variables independent from all other sources of randomness .",
    "all the processes start at the same position @xmath577 .",
    "we define @xmath578 if @xmath579\\ ] ] and @xmath580 otherwise .",
    "we define @xmath581 if @xmath582\\ ] ] and @xmath583 otherwise .",
    "we define @xmath584 if @xmath585\\ ] ] and @xmath586 otherwise .",
    "* @xmath587 _ with high probability_. we prove that @xmath588 = 1 $ ] . because the metropolis ",
    "hastings function @xmath147 is globally lipschitz , lemma  [ lem.coupling.traj ] shows that it suffices to verify that @xmath589 under assumption [ assump.f ] the second and third derivatives of @xmath542 are bounded so that bound ( [ e.boun.taylor ] ) follows from a second - order taylor expansion , @xmath590 we have used the bound @xmath591 . * _ @xmath592 and @xmath593 have same law .",
    "_ it is straightforward to verify that the processes @xmath575 and @xmath553 have the same law . *",
    "_ @xmath594 with high probability .",
    "_ we prove that @xmath595 = 1 $ ] .",
    "lemma  [ lem.coupling.traj ] shows that this follows from the elementary bound @xmath596 .",
    "we now show that the markov chain @xmath556 is a markov chain that is reversible with respect to the distribution @xmath597 , @xmath598 \\bigr ] = e^y g^*(y ) g^*(x ) \\mathbb{e } \\bigl [ \\mathbb{e } \\bigl[f(\\omega + x - y)\\bigr ] \\bigr]\\ ] ] for all @xmath599 .",
    "this boils down to verifying that the function @xmath600 $ ] is symmetric ; proposition @xmath601 of @xcite shows that this quantity can be expressed as @xmath602 which is indeed symmetric .",
    "note that this markov chain corresponds to the _ penalty method _ of @xcite ; see also @xcite for a discussion of this algorithm .",
    "the ergodic theorem for markov chains applies ; for any bounded and measurable function @xmath430 we have @xmath603 \\biggr|=0.\\ ] ] one can thus use the triangle inequality several times to see that for any bounded and measurable function @xmath430 , we have @xmath604 \\biggr| \\\\ & & \\qquad\\leq    \\frac{1}{t_d } \\sum_{k=0}^{t_d-1 } \\mathbb{e } \\bigl| h\\bigl(w^{(d)}_{k}\\bigr ) - h\\bigl ( \\widehat{w}^{(d)}_{k}\\bigr ) \\bigr| + \\mathbb{e } \\biggl| \\frac{1}{t_d } \\sum_{k=0}^{t_d-1 } h\\bigl ( \\widehat { w}^{(d)}_{k}\\bigr ) - \\mathbb{e}\\bigl[h(w)\\bigr ] \\biggr| \\\\ & & \\qquad\\lesssim   \\bigl(1-\\mathbb{p}\\bigl[w^{(d)}_{k } = \\widehat{w}^{(d)}_{k } \\dvtx   k=1 , \\ldots , t_d\\bigr ] \\bigr ) + \\mathbb{e } \\biggl| \\frac{1}{t_d } \\sum_{k=0}^{t_d-1 } h\\bigl(\\widehat{y}^{(d)}_{k}\\bigr ) - \\mathbb{e}\\bigl[h(w ) \\bigr ] \\biggr| \\\\ & & \\qquad\\lesssim   o(1 ) + \\frac{1}{t_d } \\sum_{k=0}^{t_d-1 } \\mathbb{e } \\bigl| h\\bigl(\\widehat{y}^{(d)}_{k}\\bigr ) - h(y_{k } ) \\bigr| + \\mathbb{e } \\biggl| \\frac{1}{t_d } \\sum _ { k=0}^{t_d-1 } h(y_{k } ) - \\mathbb{e } \\bigl[h(w)\\bigr ] \\biggr| \\\\ & & \\qquad= o(1 ) + o(1 ) + o(1),\\end{aligned}\\ ] ] which completes the proof of lemma  [ lem.separation.scale ] .",
    "in this appendix , we present details of the lotka  volterra model used in the simulation study of section  [ sect.sim.study ] . the lotka",
    " volterra model is a continuous - time markov chain on @xmath605 .",
    "the transitions and associated rates for this model are @xmath606 the rate for any other transition is zero .",
    "observations of the markov chain , when they occur , are subject to gaussian error , @xmath607 , { \\left } [ \\matrix { x_4&0 \\cr 0&x_5 }   { \\right } ] { \\right}).\\ ] ] using @xmath608 , a realisation of the stochastic process was simulated from initial value @xmath609 for @xmath225 time units .",
    "the state , perturbed with gaussian noise , @xmath610 , was recorded at @xmath611 . for inference , @xmath612 were assumed to be independent , _ a priori _ with @xmath613 $ ] , ( @xmath614 ) .",
    "the initial value for each chain was a vector of estimates of the posterior median for each parameter , obtained from the initial run ; hence no `` burn - in '' was required .",
    "each algorithm was run for @xmath241 iterations , except with @xmath246 and @xmath253 , where @xmath615 iterations were used .",
    "output was thinned by a factor of @xmath215 for storage .",
    "we are grateful to the associate editor and three referees for their comments , which helped improve both the presentation and the content of this article .",
    "gareth roberts and jeffrey rosenthal are grateful for financial support in carrying out this research from , respectively , epsrc of the uk , through the crism ( ep / d002060/1 ) and ilike ( ep / k014463/1 ) projects , and nserc of canada ."
  ],
  "abstract_text": [
    "<S> we examine the behaviour of the pseudo - marginal random walk metropolis algorithm , where evaluations of the target density for the accept / reject probability are estimated rather than computed precisely . under relatively general conditions on the target distribution </S>",
    "<S> , we obtain limiting formulae for the acceptance rate and for the expected squared jump distance , as the dimension of the target approaches infinity , under the assumption that the noise in the estimate of the log - target is additive and is independent of the position . for targets with independent and identically distributed components </S>",
    "<S> , we also obtain a limiting diffusion for the first component .    </S>",
    "<S> we then consider the overall efficiency of the algorithm , in terms of both speed of mixing and computational time . </S>",
    "<S> assuming the additive noise is gaussian and is inversely proportional to the number of unbiased estimates that are used , we prove that the algorithm is optimally efficient when the variance of the noise is approximately 3.283 and the acceptance rate is approximately 7.001% . we also find that the optimal scaling is insensitive to the noise and that the optimal variance of the noise is insensitive to the scaling . </S>",
    "<S> the theory is illustrated with a simulation study using the particle marginal random walk metropolis .    </S>",
    "<S> ,    , </S>"
  ]
}