{
  "article_text": [
    "recently , several deep multi - modal learning tasks have emerged . there are image captioning @xcite , text conditioned image generation @xcite , object tagging @xcite , text to image search @xcite , and so on .",
    "for all these works , how to achieve semantic multi - modal representation is the most crucial part .",
    "therefore , there were several works for multi - modal representation learning @xcite . and",
    "all of these works require image - text pair information .",
    "their assumption is , image - text pair has similar meaning , so if we can embed image - text pair to similar points of multi - modal space , we can achieve semantic multi - modal representation . but",
    "pair information is not always available in several situations .",
    "image and text data usually not exist in pair and if they are not paired , manually pairing them is an impossible task .",
    "but tag or category information can exist separately for image and text . and",
    "also , does not require paired state and can be manually labeled separately . and",
    "learning multi - modal representation from image - text pair information can be a narrow approach . because , their training objective focuses on adhering image and text in same image - text pair and does nt care about adhering image and text , that are semantically similar , but in different pair .",
    "so some image and text can have not similar multi - modal feature even though they are semantically similar . in addtion , resolving every pair relations can be a bottleneck with large training dataset . to deal with above problems , for multi - modal representation learning , we bring concept from ganin s work@xcite which does unsupervised image to image domain adaptation by adversarial backpropagation .",
    "they use adversarial learning concept which is inspired by gan ( generative adversarial network)@xcite to achieve category discriminative and domain invariant feature .",
    "we extend this concept to image - text multi - modal representation learning .",
    "we think image and text data are in covariate shift relation .",
    "it means , image and text data has same semantic information or labelling function in high level perspective but they have different distribution shape .",
    "so we regard , multi - modal representation learning process is adapting image and text distribution to same distribution and retain semantic information at the same time.in contrast with previous multi - modal representation learning works , we do nt exploit image - text pair information and only use category information .",
    "our focus is on achieving category discriminative , domain ( image , text ) invariant and semantically universal multi - modal representation from image and text . with above points of view , we did multi - modal embedding with category predictor and domain classifier with gradient reversal layer .",
    "we use category predictor for achieving discriminative power of multi - modal feature . and using domain classifier with grdient reversal layer , which makes adversarial relationship with embedding network and domain classifier , for achieving domain ( image , text ) invariant multi - modal feature .",
    "domain invariant means image and text have same distribution in multi - modal space .",
    "we show that our multi - modal feature distribution is well mixed about domain , which means image and text multi - modal feature s distributions in multi - modal space are similar , and also well distributed by t - sne@xcite embedding visualization . and comparison classification performance of multi - modal feature and uni - modal ( image only , text only ) feature shows , there exists small information loss within multi - modal embedding process and still multi - modal feature has category discriminative power even though it is domain invariant feature after multi - modal embedding .",
    "and our sentence to image search result ( figure [ fig : intro_search_ex ] ) with multi - modal feature shows our multi - modal feature has universal semantic information , which is more than category information .",
    "it means , within multi - modal - embedding process , extracted universal information from word2vec@xcite and vgg - verydeep-16 @xcite is not removed . in this paper",
    ", we make the following contributions .",
    "first , we design novel image - text multi - modal representation learning method which use adversarial learning concept .",
    "second , in our knowledge , this is the first work that does nt exploit image - text pair information for multi - modal representation learning .",
    "third , we verify image - text multi - modal feature s quality in various perspectives and various methods.our approach is much generic as it can be easily used for any different domain ( e.g. sound - image , video - text ) multi - modal representation learning works with backpropagation only .",
    "several works about image - text multi - modal representation learning have been proposed over the recent years .",
    "specific tasks are little bit different for each work , but these works crucial common part is achieving semantic image - text multi - modal representation from image and text .",
    "image feature extraction and text feature extraction method are different with each work .",
    "but almost they commonly use image - text pair information to learn image - text semantic relation .",
    "many previous approaches use ranking loss ( the training objective is minimizing distance of same image - text pair and maximizing distance of different image - text pair in multi - modal space ) for multi - modal embedding .",
    "work@xcite use r - cnn@xcite for image feature and brnn@xcite for text feature and apply ranking loss .",
    "and some approaches ( @xcite,@xcite ) use vgg - net for image feature extracting and use neural - language - model for text feature extracting and apply ranking loss or triplet ranking loss . some other approaches",
    "use deep generative model@xcite or dbm ( deep boltzmann machine)@xcite for multi - modal representation learning . in these methods",
    ", they intentionally miss one modality feature and generate missed feature from other modality feature to learn relation of different modalities .",
    "therefore they also use image - text pair information and the process is complicate and not intuitive .      adversarial network concept has started from gan ( generative adversarial network ) @xcite .",
    "this concept showed great results for several different tasks .",
    "for example , dcgan ( deep convolutional generative adversarial network ) @xcite drastically improve generated image quality . and",
    "text - conditioned dcgan @xcite generate related image from text .",
    "besides image generation , some approach @xcite apply adversarial learning concept to domain adaptation field with gradient reversal layer . they did domain adaptation from pre - trained image classification network to semantically similar but visually different domain ( e.g. edge image , low - resolution image ) image target . for this , they set category predictor and domain classifier , which do adversarial learning , so network s feature trained for category discriminative and domain invariant property .",
    "covariate shift is a primary assumption for domain adaptation field , which assumes that source domain and target domain have same labelling function ( same semantic feature or information ) but mathematically different distribution form .",
    "there was theoretical work about domain adaptation within covariate shift relation source and target domain@xcite .",
    "and we assume that image and text are also in covariate shift relation .",
    "we assume image and text have same semantic information ( labeling function ) but have different distribution form .",
    "so our multi - modal embedding process is adapting those distributions as same and retain semantic information at the same time .",
    "now , we detail our proposed novel method and model ( figure [ fig : model ] ) , multi - modal representation learning by adversarial backpropagation .",
    "we use ms coco dataset @xcite .      our network structure ( figure [ fig : model ] )",
    "is divided into two parts : feature extraction and multi - modal representation learning .",
    "the former part aims at transforming each modality signal into feature .",
    "the latter part is devised to embed each feature representation into single ( multi - modal ) space.for the representation of visual features , we use vgg16 @xcite which is pre - trained on imagenet@xcite . to extract image features , we re - size an image to the size @xmath0 and crop @xmath1 patches from the four corners and the center .",
    "then the 5 cropped area are flipped to get total 10 patches .",
    "we extract fully - connected features ( fc7 ) from each patch , and average them to get a single feature .",
    "to represent sentences , we use word2vec @xcite , which embeds word into 300-dimensional semantic space . in feature extraction process",
    ", words in a sentence are converted into word2vec vectors , each of which is a 300-dimensional vector .",
    "if a sentence contains @xmath2 words , we get a feature whose size is @xmath3 .",
    "we add zero padding to the bottom row of the feature to fix its size .",
    "since the maximum length of a sentence in ms coco dataset @xcite is 59 , we set the feature size to @xmath4 . after extracting features from each modality , the multi - modal representation learning process follows . for an image feature @xmath5 and a sentence feature @xmath6",
    ", we apply two transformations @xmath7 and @xmath8 for images and sentences respectively , to embed two features into a single @xmath9-dimensional space .",
    "that is , @xmath10 and @xmath11 are satisfied.for embedding image feature @xmath5 , we use two fully connected layers with relu@xcite activation .",
    "since sentence feature @xmath6 is 2-dimensional , we apply textcnn @xcite to @xmath12 , to make it possible for @xmath12 to be embedded in the space . at the end of each feature embedding network",
    ", we use batch - normalization@xcite and l2-normalization respectively .",
    "and we apply dropout@xcite for all fully - connected layers.the embedding process is regulated by two components  category predictor and domain classifier with gradient reversal layer , which is a similar concept to that of @xcite .",
    "the category predictor regulates the features on the multi - modal space , in such a way that multi - modal features are discriminative enough to be classified into the valid categories .",
    "meanwhile , the domain classifier with gradient reversal layer makes the multi - modal features being invariant to their domain .",
    "we adopt the concept from @xcite.grl ( gradient reversal layer ) is a layer in which backward pass is reversing gradient values . for a layer s input @xmath13 , the output @xmath12 , and the identity matrix @xmath14 ,",
    "forward pass is shown on equation [ eq : grl_forward ] .",
    "in backward pass , for the loss of the network @xmath15 , the gradient subject to @xmath13 is shown on equation [ eq : grl_backward ] .",
    "@xmath16 is adaptation factor which is the amount of domain invariance we want to achieve at a point of training .",
    "@xmath17 @xmath18    the domain classifier is a simple neural network that has two fully - connected layers , with the last sigmoid layer that determines the domain of features in the multi - modal embedding space .",
    "that is , it is trained in a way that it discriminates the difference between features from two domains.however , since the grl reverses the gradient , feature embedding networks are trained to generate features whose domains are difficult to be determined by the domain classifier .",
    "this makes adversarial relationship between the embedding network and the domain classifier .",
    "consequently , domain - invariant features can be generated by the multi - modal embedding networks .",
    "for the calculation of network loss , we sum the losses of two ends  category predictor and domain classifier .",
    "we use sigmoid cross entropy loss for the two ends . for calculating joint gradient of the category predictor and the domain classifier , the two gradients are added , which is shown in the equation below . @xmath19 where @xmath20 and @xmath21 is the error of category predictor and domain classifier respectively , @xmath12 is the output of the last feature embedding layer , and @xmath16 is the adaptation factor.we use adam optimizer @xcite for training with relatively small learning rate @xmath22 . that s because of the empirical difficulty of generating domain - invariant features with regular learning rate .",
    "to achieve domain invariant feature , we use domain classifier and gradient reversal layer @xcite . and we should properly schedule @xmath16 ( adaptation factor ) value from 0 to some positive value . because , at the first stage of training , domain classifier should become smart in advance for adversarial learning process . and with @xmath16 value increasing , domain classifying with multi - modal feature become difficult and domain classifier become smarter to classify it correctly . in our experiment , it turns out that proper @xmath16 scheduling is important to achieve domain invariant feature . after exploring many scheduling methods",
    ", we find below schedule scheme is optimal , which is exactly the same scheduling as @xcite . in the equation , @xmath23 is the fraction of current step in max training steps .",
    "we used batch normalization@xcite and l2-normalization for normalizing image and text feature distribution just before the multi - modal feature layer . in our experiment , without proper normalization , it seems to be trained well ( loss value decreases gently and classification accuracy is fine ) but when checking the t - sne@xcite embedding and search result , we can recognize that image and text feature distribution is collapsed just for achieving domain invariant feature ( collapsed means distance between features going to zero ) .",
    "so proper normalization process is important to achieve domain invariant and also well distributed multi - modal feature .",
    "above figure [ fig : tsne ] is a t - sne@xcite embedding result of computed multi - modal features from ms coco test set s 5000 images and sentences .",
    "( a ) is result of trained with triplet ranking loss ( the training objective is minimizing distance of same image - text pair and maximizing distance of different image - text pair in multi - modal space ) which exploits image - text pair relation . for implementing ,",
    "we consult wangs@xcite work .",
    "we use fv - hglmm@xcite for sentence representation , pre - trained vgg16 for image representation and two - branch fully - connected layers for multi - modal embedding , which is same as wang s did .",
    "difference is they use complex data sampling scheme and we use random data sampling at training stage .",
    "( b ) is result of trained with category predictor and domain classifier which uses our model . in ( a ) , image and text feature distributions are not well mixed , which means image and text multi - modal feature s distributions in multi - modal space are not similar .",
    "image and text multi - modal features are not overlapped in multi - modal space .",
    "it means , semantically similar image and text are not embedded to near points of multi - modal space .",
    "but in ( b ) , our result , we can get well mixed with domains image - text multi - modal feature distribution .",
    "image and text are overlapped in multi - modal space and also distributed enough for being discriminated .",
    "it means , image and text has similar distribution in multi - modal space . we think difference of ( a ) and ( b ) comes from difference of training objective . because our model(b ) trained for hard to classify domain ( image , text ) of the multi - modal feature and triplet ranking loss(a ) is trained for adhering same image - text pair and pushing different image - text pair .",
    "and result means triplet ranking loss not adapt image and text to same distribution in multi - modal space .",
    "so this result shows our model s training objective is more suitable for learning well mixed with domains and also well distributed multi - modal feature than other methods .",
    ".category classification result of ms coco val set with various modes .",
    "`` image only '' means just use vgg - net and category predictor , `` text only '' means just use word2vec , textcnn and category predictor .",
    "so , `` image only '' and `` text only '' modes do nt include domain classifier and gradient reversal layer . ``",
    "image+text(m ) '' is our multi - modal network model ( figure [ fig : model ] ) . [",
    "cols=\"^,^,^,^\",options=\"header \" , ]     we build our sentence to image search system with 40504 number of ms coco validation set which is never seen at training stage .",
    "we train with 82783 images ( train set ) and test for 40504 images ( val set ) . and simply do k - nearest - neighbor search in multi - modal space with computed multi - modal feature .",
    "figure [ fig : search_compare ] shows comparison of our search result and category based search result .",
    "you can see the sentence query has more semantic information than its category label . and",
    "category based search can not exploit that semantic information but our search system can exploit that semantic information of sentence query . in figure",
    "[ fig : search_compare ] , we can see our search system finds several objects which is not contained in category information but exists in sentence query . in 1st row of figure",
    "[ fig : search_compare ] , our search system rightly catches information ",
    "woman standing under trees by a field \" from sentence even though it was just trained to predict [ person , tie ] from sentence and image at training time .",
    "it means our multi - modal embedding process did nt remove universal information extracted from word2vec and vgg16 . and also match image and text semantically relevant feature during multi - modal embedding process . in 2nd row of figure",
    "[ fig : search_compare ] , our search system thinks that most similar image with the query is food image which that category is [ spoon , broccoli ] , which is not overlapped with query s category .",
    "but interestingly , in human s semantic perspective , we can recognize they have similar semantic information .",
    "( `` covered with different vegetables and cheese . '' ) in figure [ fig : search_result ] ( next page ) , you can see more various search results from our multi - modal search system .",
    "for benchmark of search system , we did recall@k evaluation ( table [ table : recallk ] , next page ) with sentence - to - image and image - to - sentence retrieval .",
    "for this , we used karpathy s data split scheme@xcite . compare to state - of - the - art results , our model s performance is relatively low .",
    "we think , the major reason is , previous models trained for adhering image - text pair and pushing different image - text pair in multi - modal space and recall@k evaluate query s pair appeared or not in retrieval result .",
    "so , even if search result is semantically reasonable ( figure [ fig : search_result ] ) , if query s pair not appear in retrieval result , recall@k can be low .",
    "so we think , this metric is not fully appropriate to assess search quality .",
    "but for comparison , we also did recall@k experiment .",
    "we have proposed a novel approach for multi - modal representation learning which uses adversarial backpropagation concept .",
    "our method does not require image - text pair information for multi - modal embedding but only uses category label .",
    "in contrast , until now almost all other methods exploit image - text pair information to learn semantic relation between image and text feature.our work can be easily extended to other multi - modal representation learning ( e.g. sound - image , sound - text , video - text ) .",
    "so our method s future work will be extending this method to other multi - modal case .",
    "deng , jia , dong , wei , socher , richard , li , li - jia , li , kai , and fei - fei , li .",
    "imagenet : a large - scale hierarchical image database . in _",
    "computer vision and pattern recognition , 2009 .",
    "cvpr 2009 .",
    "ieee conference on _ , pp .   248255 .",
    "ieee , 2009 .",
    "frome , andrea , corrado , greg  s , shlens , jon , bengio , samy , dean , jeff , mikolov , tomas , et  al .",
    "devise : a deep visual - semantic embedding model . in _ advances in neural information processing systems",
    "_ , pp . 21212129 , 2013 .",
    "ganin , yaroslav and lempitsky , victor .",
    "unsupervised domain adaptation by backpropagation . in blei ,",
    "david and bach , francis ( eds . ) , _ proceedings of the 32nd international conference on machine learning ( icml-15 ) _ , pp . 11801189 .",
    "jmlr workshop and conference proceedings , 2015 .",
    "url http://jmlr.org/proceedings/papers/v37/ganin15.pdf .",
    "goodfellow , ian , pouget - abadie , jean , mirza , mehdi , xu , bing , warde - farley , david , ozair , sherjil , courville , aaron , and bengio , yoshua .",
    "generative adversarial nets . in _ advances in neural information processing systems",
    "_ , pp . 26722680 , 2014 .",
    "karpathy , andrej and fei - fei , li .",
    "deep visual - semantic alignments for generating image descriptions . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pp .   31283137 , 2015 .",
    "kim , yoon .",
    "convolutional neural networks for sentence classification . in moschitti , alessandro , pang , bo , and daelemans , walter",
    "( eds . ) , _ proceedings of the 2014 conference on empirical methods in natural language processing , emnlp 2014 , october 25 - 29 , 2014 , doha , qatar , a meeting of sigdat , a special interest group of the acl _ , pp .",
    "acl , 2014 .",
    "isbn 978 - 1 - 937284 - 96 - 1 .",
    "url http://aclweb.org/anthology/d/d14/d14-1181.pdf .",
    "klein , benjamin , lev , guy , sadeh , gil , and wolf , lior .",
    "associating neural word embeddings with deep image representations using fisher vectors . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pp .   44374446 , 2015 .",
    "lin , tsung - yi , maire , michael , belongie , serge , hays , james , perona , pietro , ramanan , deva , dollr , piotr , and zitnick , c  lawrence .",
    "microsoft coco : common objects in context . in",
    "european conference on computer vision _ , pp .",
    "springer , 2014 .",
    "ma , lin , lu , zhengdong , shang , lifeng , and li , hang .",
    "multimodal convolutional neural networks for matching image and sentence . in _ proceedings of the ieee international conference on computer vision _ , pp .   26232631 , 2015 .",
    "mikolov , tomas , sutskever , ilya , chen , kai , corrado , greg  s , and dean , jeff . distributed representations of words and phrases and their compositionality . in _ advances in neural information processing systems",
    "_ , pp . 31113119 , 2013 .",
    "nair , vinod and hinton , geoffrey  e. rectified linear units improve restricted boltzmann machines . in _ proceedings of the 27th international conference on machine learning ( icml-10 ) _ , pp .   807814 , 2010 .",
    "radford , alec , metz , luke , and chintala , soumith .",
    "unsupervised representation learning with deep convolutional generative adversarial networks . _",
    "corr _ , abs/1511.06434 , 2015 .",
    "url http://arxiv.org/abs/1511.06434 .",
    "reed , scott  e. , akata , zeynep , yan , xinchen , logeswaran , lajanugen , schiele , bernt , and lee , honglak .",
    "generative adversarial text to image synthesis . in balcan , maria - florina and weinberger , kilian  q. ( eds . ) , _ proceedings of the 33nd international conference on machine learning , icml 2016 , new york city , ny , usa , june 19 - 24 , 2016 _ , volume  48 of _ jmlr workshop and conference proceedings _ , pp .   10601069 .",
    "jmlr.org , 2016 .",
    "url http://jmlr.org/proceedings/papers/v48/reed16.html .",
    "srivastava , nitish , hinton , geoffrey  e , krizhevsky , alex , sutskever , ilya , and salakhutdinov , ruslan .",
    "dropout : a simple way to prevent neural networks from overfitting . _",
    "journal of machine learning research _ , 150 ( 1):0 19291958 , 2014 .",
    "vinyals , oriol , toshev , alexander , bengio , samy , and erhan , dumitru .",
    "show and tell : a neural image caption generator . in _ proceedings of the ieee conference on computer vision and pattern recognition _",
    ", pp .   31563164 , 2015 ."
  ],
  "abstract_text": [
    "<S> we present novel method for image - text multi - modal representation learning . in our knowledge , this work is the first approach of applying adversarial learning concept to multi - modal learning and not exploiting image - text pair information to learn multi - modal feature . </S>",
    "<S> we only use category information in contrast with most previous methods using image - text pair information for multi - modal embedding .    in this paper , we show that multi - modal feature can be achieved without image - text pair information and our method makes more similar distribution with image and text in multi - modal feature space than other methods which use image - text pair information . </S>",
    "<S> and we show our multi - modal feature has universal semantic information , even though it was trained for category prediction . </S>",
    "<S> our model is end - to - end backpropagation , intuitive and easily extended to other multi - modal learning work .    </S>",
    "<S> = 1 </S>"
  ]
}