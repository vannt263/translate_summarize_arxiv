{
  "article_text": [
    "recently , much effort has been made to uncover the dynamical process underlying a given time series of scale and time dependent complex systems@xcite . in many cases it is possible to describe such systems by a langevin equation , extracted directly from the data , which separates the deterministic and stochastic processes inherent to the system@xcite .",
    "such an approach has already been carried out successfully for instance for data from turbulent fluid dynamics@xcite , financial data@xcite , climate indices@xcite and for electroencephalographic recordings from epilepsy patients@xcite and additional improvements were proposed to address the case of low sampling rates@xcite .",
    "however , typically the signal is subject to noise , due to experimental constraints or due to the measurement or discretization procedure leading to the data set to be studied .",
    "such noise is not intrinsic to the system , differing from what is known as dynamical noise , and therefore one is interested to separate it from the stochastic process .",
    "we call such non - intrinsic noise measurement noise . to separate the measurement noise from the dynamics of the measured variable different predictor models or schemes for noise reduction may be used@xcite . in this context , an alternative procedure has been proposed@xcite to extract the intrinsic dynamics associated with langevin processes strongly contaminated by measurement noise , based solely on the two conditional moments directly calculated from the data@xcite .    in this manuscript",
    "we will revisit this nonparametric procedure , describing it in detail and explaining the main steps for its implementation , with the aim of applying it to empirical data sets .",
    "let us consider a one - dimensional langevin process @xmath0 ( an extension to more dimensions is straightforward ) defined as @xmath1 where @xmath2 represents a gaussian @xmath3-correlated white noise @xmath4 and @xmath5 .",
    "functions @xmath6 and @xmath7 are the drift and diffusion coefficients defined as @xmath8 for @xmath9 , where @xmath10 denotes the @xmath11-th order conditional moment of the data , as explained below .",
    "further , we consider that @xmath0 is ` contaminated ' by a gaussian @xmath3-correlated measurement white noise , which leads to the series of observations @xmath12 where @xmath13 denotes the amplitude of the measurement noise .    when there is no measurement noise ( @xmath14 ) , eq .",
    "( [ yy ] ) yields the particular case @xmath15 , and the evolution equation underlying the signal can be extracted directly from the two conditional moments ( @xmath9 ) @xmath16 as described in refs .",
    "@xcite .    in the presence of measurement noise ( @xmath17 ) the conditional moments depend on @xmath18 , @xmath19 and @xmath13 . since generally the limit @xmath20 does not exist , eq .",
    "( 3 ) can not be applied .",
    "the aim of this paper , however , is to explicitly derive a procedure which can transform the functional form of the noisy conditional moments @xmath21 and @xmath22 at small @xmath19 into the true coefficients @xmath6 and @xmath7 and simultaneously retrieve the amplitude @xmath13 of the associated measurement noise .",
    "for that , we show that @xmath23 for fixed @xmath24 is typically linear in @xmath19 for a certain range @xmath25 $ ] of values ( see fig .  [ fig4 ] below ) . therefore , even when @xmath26 one can estimate the quantities @xmath27    we start in sec .  [",
    "sec : processes ] by briefly describing the procedure to extract langevin equations from data sets and show how the drift and diffusion coefficients depend on the measurement noise strength @xmath13 . in particular , we will see that the proposed estimate@xcite does not yield the correct value when the measurement noise is too strong . in sec .",
    "[ sec : algor ] we then proceed to minimize a proper least square function using the levenberg - marquardt procedure@xcite . by applying this algorithm to synthetic data",
    "we show that indeed this approach is able to reliably extract the noise amplitude even in cases where it is of the same order as the synthetic signal without noise .",
    "furthermore , the procedure yields simultaneously more accurate estimates for the clean signal @xmath0 . finally , in sec .",
    "[ sec : applications ] , we apply this framework to an empirical data set , namely the north atlantic oscillation daily index@xcite , giving some insight from the obtained results to the underlying system . discussion and",
    "conclusions are given in sec .",
    "[ sec : conc ] , where further possible applications are proposed .",
    "all details concerning the implementation of the minimization procedure to extract strong measurement noise are given as appendices .",
    "we consider a time series generated by integrating eq .",
    "( [ xlangevin ] ) with drift and diffusion coefficient assumed to be linear and quadratic forms respectively    @xmath28    [ dd ]    and by adding separately to each data point the measurement term @xmath29 in eq .",
    "( [ yy ] ) .",
    "though we concentrate on the particular expressions for @xmath30 and @xmath31 given above , it should be stressed that they comprehend a large collection of different processes , such as ornstein - uhlenbeck processes@xcite .",
    "further , some generalizations may be carried out as will be discussed in sec .",
    "[ sec : conc ] . using eqs .",
    "( [ dd]a ) and ( [ dd]b ) , one has six parameters : five coefficients @xmath32 defining the evolution equation of the clean signal and a sixth parameter @xmath13 for the amplitude of the measurement noise .     of the series with noise ( see eq .",
    "( [ yy ] ) ) , with the corresponding mean value @xmath33 and standard deviation @xmath34 in the inset , and the corresponding functions * ( b ) * @xmath35 and * ( c ) * @xmath36 , see eq .",
    "( [ dprime ] ) . in all cases ,",
    "the assumed time series @xmath0 without measurement noise uses the coefficients @xmath37 and @xmath38.,width=302 ]     and @xmath39 ( see text and eq .",
    "( [ dprime ] ) ) the underlying langevin time series @xmath0 without noise is the same as in fig .",
    "[ fig1].,width=302 ]    figure [ fig1 ] illustrates this influence of noise for a particular choice of @xmath40 . as shown in fig .",
    "[ fig1]a , for increasing @xmath13 one obtains broader probability density functions @xmath41 as one intuitively expects .",
    "quantitatively , the standard deviation @xmath34 of @xmath41 varies quadratically with the measurement noise @xmath13 , while the mean value @xmath33 of @xmath41 remains constant , as shown in the inset of fig .",
    "[ fig1]a . the estimated functions @xmath42 and @xmath39 change significantly , as shown in fig .  [ fig1]b and [ fig1]c respectively .",
    "assuming @xmath43 and @xmath44 , fig .",
    "[ fig2 ] shows how the estimated parameters @xmath45 deviate from the ` true ' uncontaminated values @xmath32 in eq .",
    "( [ dd ] ) when measurement noise increases .",
    "notice that for @xmath14  see left vertical axis in the plots of fig .",
    "[ fig2 ]  the estimated parameter values are approximately correct .     and @xmath46 as a function of bin @xmath47 , for @xmath48 and different measurement noise strengths .",
    "the asymmetry of @xmath49 is due to @xmath50 ( see eqs .",
    "( [ dd ] ) ) .",
    "the same @xmath0 as in fig .",
    "[ fig1 ] was used.,width=321 ]     and * ( b ) * @xmath46 as a function of @xmath19 , for bin @xmath51 and different measurement noise strengths . in * ( c )",
    "* one compares the true measurement noise with the approximation @xmath52 given in eq .",
    "( [ approxmeasnoise ] ) . in the inset",
    "the corresponding absolute and relative erros are given by @xmath53 and @xmath54 respectively .",
    "errors for @xmath55 are negligible .",
    "the same @xmath0 as in fig .",
    "[ fig1 ] was used.,width=321 ]    , @xmath56 , @xmath57 and @xmath58 ( symbols ) defining the conditional moments in eqs .",
    "( [ ym_2 ] ) . the underlying langevin time series @xmath0 without noise",
    "is characterized by a drift coefficient @xmath37 and a diffusion coefficient @xmath38 .",
    "the measurement noise was fixed at @xmath59 . each hat - function",
    "is compared with the corresponding integral form in eqs .",
    "( [ gamma - m ] ) using the first estimate of parameters values ( dashed lines ) and the true values ( solid lines).,width=321 ]    to correctly derive the drift and diffusion coefficients @xmath6 and @xmath7 when @xmath13 is strong , we consider the measured conditional moments @xmath60 and @xmath61 , as in eq .",
    "( [ gencondmom ] ) , the hat indicating that they are calculated from the measured data @xmath62 directly . since this conditional moments depend in a non trivial way on both time @xmath19 and amplitude @xmath47 , we approximate them up to first order on @xmath19 :    @xmath63    [ ym_2 ]    where @xmath62 is taken in the range @xmath64 for each bin @xmath65 , and @xmath66 depends on the binning considered .",
    "appendix [ app : km ] gives the full derivation of eqs .",
    "( [ ym_2 ] ) .",
    "figure [ fig3 ] shows both conditional moments for @xmath48 and with different measurement noise strengths .",
    "conversely , in fig .",
    "[ fig4]a and [ fig4]b one sees that the conditional moments depend linearly on @xmath19 for a fixed amplitude @xmath24 , which justifies the approximation assumed in eqs .",
    "( [ ym_2 ] ) . therefore , to study the dependence of the conditional moments on @xmath24 we will consider the linear decompositions in eqs .",
    "( [ ym_2 ] ) , as done in fig .",
    "our simulations with synthetic data have shown that using a to large range of @xmath19 values yields results for @xmath30 and @xmath31 deviated from their true values . the best estimation for both kramers - moyal coefficients",
    "are obtained using the range @xmath67 .",
    "notice that for sufficiently small measurement noise a good estimate of it is given by@xcite @xmath68 where @xmath33 is the average value of @xmath62 data points in the time series . for details see append .",
    "[ app : km ] . however , as shown in fig .",
    "[ fig4]c , this approximation is no longer valid for sufficiently high measurement noise , namely when @xmath69 ( see inset of fig .  [",
    "fig4]c ) and even otherwise coefficients @xmath30 and @xmath31 are not correctly estimated ( see fig .  [ fig2 ] ) .",
    "therefore , a better algorithm to estimate such parameters is necessary .",
    "the heart of our procedure to correctly estimate measurement noise lies in the fact that while the functions @xmath70 and @xmath71 ( i=1,2 ) are obtained explicitly for each bin value @xmath47 , functions @xmath72 and @xmath73 depend generally on the drift and diffusion coefficients as follows :    @xmath74 \\bar{f}_{\\sigma}(x\\vert y)dx , \\cr    & & \\label{m2}\\end{aligned}\\ ] ]    [ gamma - m ]    where @xmath75 is the probability for the system to adopt the value @xmath18 when a measured value @xmath24 is observed . for details about the derivation of functions in eqs .",
    "( [ gamma - m ] ) see append .",
    "[ app : km ] and for the explicit expression of @xmath75 see append .",
    "[ app : condprob ] .    in fig .",
    "[ fig5 ] we illustrate both the hat - functions in eqs .",
    "( [ ym_2 ] ) and their integral form in eqs .",
    "( [ gamma - m ] ) . due to the measurement noise fixed in this example at @xmath59 the hat - functions ( symbols )",
    "are not properly fitted by the integral form in eqs .",
    "( [ gamma - m ] ) using the first estimate ( dashed lines ) of the parameters @xmath32 , taken from fig .",
    "[ fig2 ] , and @xmath13 , computed from eq .",
    "( [ approxmeasnoise ] ) . if instead we use the true parameter values in the integral forms of our @xmath72 and @xmath73 functions a proper fit is obtained ( solid lines ) .     in eq .",
    "( [ f ] ) as a function of * ( a ) * @xmath76 , * ( b ) * @xmath77 , * ( c ) * @xmath78 , * ( d ) * @xmath79 , * ( e ) * @xmath80 and * ( f ) * @xmath13 . the same situation as in fig .",
    "[ fig2 ] is here chosen : @xmath37 , @xmath38 and @xmath59 .",
    "dashed lines indicate the true values used for generating the data series , while the bullet indicates the estimated values of the kramers - moyal coefficients for @xmath14 . in each plot while varying one parameter , the remaining ones are fixed at their true values ( see text).,width=313 ]    therefore , the problem we want to solve is to determine the parameters that minimize the function : @xmath81 , \\label{f}\\end{aligned}\\ ] ] where the summation extends over all @xmath82 bins , @xmath83 is the error associated to function @xmath57 at the value @xmath47 and similarly for @xmath84 , @xmath85 and @xmath86 .",
    "notice that the values of such @xmath87 and @xmath88 are taken directly from the data only .",
    "see appendix [ app : km ] for details .",
    "taking again the example illustrated in fig .",
    "( [ fig2 ] ) with @xmath59 we plot in fig .",
    "[ fig6 ] function @xmath89 in eq .",
    "( [ f ] ) as function of each one of the parameters keeping all others fixed at their true values .",
    "evidently , the estimated values are near the minimum of @xmath89 in each case .",
    "further , the one - dimensional cuts of function @xmath89 show only one minimum .",
    "one should note however that , for the entire @xmath90-dimensional parameter space , several local minima of @xmath89 may appear .",
    "in fact , after minimizing @xmath89 by varying one parameter , function @xmath89 also changes as a function of the other parameters , i.e.  its minimum as a function of the other parameter changes .",
    "in the next section we will see how to minimize function @xmath89 , in order to find good estimates for the correct values for each parameter .",
    "after computing the functions @xmath57 , @xmath58 , @xmath91 and @xmath56 as well as the corresponding errors @xmath92 , etc , directly from the measured time series @xmath62 and estimating the coefficients @xmath30 and @xmath31 given by the functional forms in eqs .",
    "( [ dd ] ) there are several ways to minimize @xmath89 .",
    "all of them start from the initially estimated set of values for the parameters and iteratively improve the solution , by finding lower values of @xmath89 , till convergence is attained .    to proceed the following remark should be considered .",
    "parameter @xmath76 can be always eliminated with a simple transformation @xmath93 . alternatively , and since we do not know beforehand the true values of @xmath76 and @xmath77 we can consider also the fact that averaging eq .",
    "( [ xlangevin ] ) yields @xmath94 and consider the transformation @xmath95 . with these arguments ,",
    "we henceforth disregard @xmath76 , which reduces the dimension of parameter space by one .",
    "parameter @xmath76 is computed from the relations above , only after minimizing @xmath89 . for simplicity the primes in @xmath96",
    "will be omitted .",
    "the simplest way is to minimize each term in @xmath89 and repeat that a large number of times starting from different initial conditions for the parameters , in a sort of a monte carlo procedure of random walks  @xcite or lvy - walks@xcite .",
    "the monte carlo procedure assures that a substantial number of local minimal for @xmath89 will be visited , and in the end we take the minimum of all @xmath89 values found .",
    "simulations have shown however that a monte carlo procedure is too expensive in this case , since there are different local minima and the choice of the minimum is strongly path dependent .",
    "we will therefore consider the levenberg - marquardt method@xcite .",
    "for the levenberg - marquardt procedure one computes the first and second derivative of @xmath89 .",
    "symbolizing the parameters @xmath97 and @xmath80 by @xmath98 with @xmath99 respectively , these derivatives read    @xmath100 , \\label{derf}\\\\     & & \\cr     & & \\cr \\frac{\\partial^2 f}{\\partial p_k\\partial p_{\\ell } } & = &            \\frac{2}{m } \\sum_{i=1}^m \\big [                  \\frac{1}{\\sigma^2_{\\hat{\\gamma}_1}(i ) }                 \\frac{\\partial \\gamma_1}{\\partial p_k }                  \\frac{\\partial \\gamma_1}{\\partial p_{\\ell}}-                  \\frac{\\hat{\\gamma}_1-\\gamma_1 }                      { \\sigma^2_{\\hat{\\gamma}_1}(i ) }                  \\frac{\\partial^2 \\gamma_1}{\\partial p_k\\partial p_{\\ell } } + \\cr    & & \\cr    & & \\cr    & & \\phantom{\\frac{2}{m } \\sum_{i=1}^m \\big [ }                 \\frac{1}{\\sigma^2_{\\hat{\\gamma}_2}(i ) }                 \\frac{\\partial ( \\gamma_2+\\sigma^2)}{\\partial p_k }                  \\frac{\\partial ( \\gamma_2+\\sigma^2)}{\\partial p_{\\ell}}-                  \\frac{\\hat{\\gamma}_2-\\gamma_2-\\sigma^2 }                      { \\sigma^2_{\\hat{\\gamma}_2}(i ) }                  \\frac{\\partial^2 ( \\gamma_2+\\sigma^2 ) }                      { \\partial p_k\\partial p_{\\ell } } + \\cr    & & \\cr    & & \\cr    & & \\phantom{\\frac{2}{m } \\sum_{i=1}^m \\big [ }                 \\frac{1}{\\sigma^2_{\\hat{m}_1}(i ) }                 \\frac{\\partial m_1}{\\partial p_k }                  \\frac{\\partial m_1}{\\partial p_{\\ell}}-                  \\frac{\\hat{m}_1-m_1 }                      { \\sigma^2_{\\hat{m}_1}(i ) }                  \\frac{\\partial^2 m_1 }                      { \\partial p_k\\partial p_{\\ell } } +                 \\frac{1}{\\sigma^2_{\\hat{m}_2}(i ) }                 \\frac{\\partial m_2}{\\partial p_k }                  \\frac{\\partial m_2}{\\partial p_{\\ell}}-                  \\frac{\\hat{m}_2-m_2 }                      { \\sigma^2_{\\hat{m}_2}(i ) }                  \\frac{\\partial^2 m_2 }                      { \\partial p_k\\partial p_{\\ell } } \\big ] \\cr    & & \\cr    & & \\cr    & \\sim &            \\frac{2}{m } \\sum_{i=1}^m \\big [                  \\frac{1}{\\sigma^2_{\\hat{\\gamma}_1}(i ) }                 \\frac{\\partial \\gamma_1}{\\partial p_k }                  \\frac{\\partial \\gamma_1}{\\partial p_{\\ell } }                 +                 \\frac{1}{\\sigma^2_{\\hat{\\gamma}_2}(i ) }                 \\frac{\\partial ( \\gamma_2+\\sigma^2)}{\\partial p_k }                  \\frac{\\partial ( \\gamma_2+\\sigma^2)}{\\partial p_{\\ell } }                 + \\cr    & & \\cr    & & \\cr    & & \\phantom{\\frac{2}{m } \\sum_{i=1}^m \\big [ }                 \\frac{1}{\\sigma^2_{\\hat{m}_1}(i ) }                 \\frac{\\partial m_1}{\\partial p_k }                  \\frac{\\partial m_1}{\\partial p_{\\ell } }                 +                 \\frac{1}{\\sigma^2_{\\hat{m}_2}(i ) }                 \\frac{\\partial m_2}{\\partial p_k }                  \\frac{\\partial m_2}{\\partial p_{\\ell } }                  -2\\delta_{\\sigma p_k}\\delta_{\\sigma p_{\\ell } }                 \\frac{\\hat{\\gamma}_2-\\gamma_2-\\sigma^2 }                      { \\sigma^2_{\\hat{\\gamma}_2}(i ) }                   \\big ] .\\label{derderf}\\end{aligned}\\ ] ]    in the right - hand side of eq .",
    "( [ derderf ] ) we neglect the terms containing second derivatives of @xmath101 and @xmath102 functions .",
    "this last approximation of neglecting second derivatives is acceptable as far as the model is successful@xcite .    by symbolizing first and second derivatives as @xmath103 and @xmath104",
    "respectively the iterative procedure computes the increments @xmath105 for each parameter @xmath98 ( @xmath99 ) , which are the solutions of @xmath106 furthermore , one assumes that @xmath107 , which considering dimensional analysis@xcite can be written as : @xmath108 where typically @xmath109 . for a given @xmath110 value , instead of the second derivatives",
    "@xmath111 one assumes @xmath112 for @xmath113 and @xmath114 otherwise and solves eq .",
    "( [ betaalpha ] ) for @xmath115 @xcite .",
    "if @xmath116 , the parameter values are updated , @xmath117 , and @xmath110 is typically decreased by @xmath118 . otherwise , if @xmath119 one increases @xmath110 by @xmath118 and determines new increments @xmath105 .",
    "the procedure stops after attaining the required convergence .    ,",
    "@xmath120 , @xmath121 and @xmath122 for the langevin process with @xmath37 , @xmath38 and @xmath59 .",
    "symbols indicate the functions obtained for the data , dashed line corresponds to the first estimate of the parameters and solid line corresponds to the parameter values obtained from the levenberg - marquardt procedure ( see text ) . in this case , for the first estimate one has @xmath123 while the final estimate retrieves @xmath124 .",
    "the true minimum is @xmath125.,width=302 ]    : * ( a ) * @xmath126 , * ( b ) * @xmath76 , * ( c ) * @xmath77 , * ( d ) * @xmath78 , * ( e ) * @xmath79 , * ( f ) * @xmath80 .",
    "the measurement noise is correctly extracted as well as the parameters defining the drift coefficient @xmath6 which controls the deterministic part of the underlying evolution equation ( see text).,width=321 ]    using the same data as generated in fig .",
    "[ fig5 ] with @xmath59 , we now plot in fig .",
    "[ fig7 ] the functions @xmath70 and @xmath71 for the data ( symbols ) and compare them with the integral forms of those functions for the first estimate of parameter values ( dashed lines ) and the optimized solution obtained with the levenberg - marquardt procedure ( solid lines ) .",
    "clearly , the optimized functions fit better the data and the minimum of @xmath89 found is very close to its true value ( see caption of fig .",
    "[ fig7 ] ) .",
    "notice that the optimized values @xmath127 are obtained for the transformed data ( @xmath128 ) , assuming @xmath129 . in practice one obtains @xmath130 , typically two orders of magnitude smaller than the other coefficients . using @xmath131 ,",
    "one obtains the true coefficients according to @xmath132 , @xmath133 , @xmath134 , @xmath135 and @xmath136 .    to show the power of the present procedure we next generate several synthetic data sets from eq .",
    "( [ xlangevin ] ) with different measurement noise amplitudes @xmath137 in the range @xmath138 $ ] . the same @xmath6 and @xmath7 as in fig .",
    "[ fig2 ] is used .",
    "results are shown in fig .",
    "the circles indicate the obtained parameter values for the first estimate , as in fig .",
    "the solid lines indicate the true values used to generate the data , while bullets indicate the value after optimization .    from fig .",
    "[ fig8]a one sees that after optimization the value of @xmath137 is always correctly determined .",
    "such finding is of major importance and shows the relevance of our approach for practical applications even for strong measurement noise , since the uncontaminated series @xmath18 typically lies within the range @xmath139 $ ] , having therefore values close to the amplitude @xmath137 of the measurement noise .",
    "figures [ fig8]b and [ fig8]c also show a very reliable estimate for the two parameters @xmath76 and @xmath77 respectively , defining the drift coefficient @xmath6 .",
    "since this coefficient characterizes the deterministic part of the evolution equation for @xmath18 , this accurate estimate should provide valuable insight into the dynamics of the underlying system .",
    "as for the diffusion coefficient @xmath7 , figs .",
    "[ fig8]d - f show that the estimate of @xmath80 is no longer as good as for the other parameters .",
    "parameter @xmath78 is reasonably estimated but the optimized estimate is as good as the first one .    for stronger measurement noise , namely for @xmath140 , one faces the problem that the optimization procedure is sometimes stucked in a local minimum of the function @xmath89 leading to unreliable coefficients @xmath141 .",
    "this is in principle a shortcoming of the presently used minimization algorithm .",
    "in addition , the function @xmath89 itself is based on estimated functions @xmath102 and @xmath101 and therefore itself subject to errors .",
    "a forthcoming study will address the observed issues in the context of global optimization .",
    "in this section we apply our framework to the north atlantic oscillation daily index , which presents data with strong measurement noise .",
    "table [ tab1 ] summarizes the optimized values for all parameter describing the data set , comparing it with simulations .     and",
    "@xmath142 of the daily north atlantic index @xmath143@xcite ( @xmath144 datapoints ) , together with the corresponding * ( c ) * @xmath145 , * ( d ) * @xmath146 , * ( e ) * @xmath147 and * ( f ) * @xmath148 . results for the empirical nao index are represented with bullets whereas the synthetic data also with @xmath144 datapoints and parameter values given by tab .",
    "[ tab1 ] is shown with circles for comparison .",
    "the corresponding fits are given with solid and dashed lines , respectively.,width=321 ]    the north atlantic oscillation ( nao ) is a source of variability in the global atmosphere , describing a large - scale vacillation in atmospheric mass between the anticyclone near the azores and the cyclone near iceland  @xcite .",
    "the state of the nao is usually measured by an index @xmath143 , defined as the normalized pressure difference between the high and the low poles , where the pressures are averaged over each , day , month or year  @xcite .",
    "the nao index and climate indices in general are receiving much attention due to their important role in climate change .",
    "lately , evidences for the stochasticity of this index have been shown@xcite . in this section",
    "we address the problem of estimating its measurement noise amplitude .",
    "figures [ fig9]a and [ fig9]b show the drift and diffusion coefficients respectively for the nao daily index ( bullets ) and the corresponding fit ( solid line ) .",
    "the parameters @xmath32 for both @xmath30 and @xmath31 are given in tab .",
    "[ tab1 ] together with the amplitude of the measurement noise .",
    "probably due to the small amount of data points ( @xmath149 values ) one observes large scattering of the data , particularly away from the average value @xmath150 .    to evaluate the reliability of considering the nao index a markov process described by eq .",
    "( [ xlangevin ] ) we also plot in figs .",
    "[ fig9 ] the results obtained when integrating such equation ( circles ) using the coefficient values in tab .",
    "[ tab1 ] including the amplitude of the measurement noise . the same sample size was considered .",
    "the corresponding fit is represented with a dashed line . while the drift coefficient @xmath30 resembles the one observed for the nao index",
    ", there is a significant shift of the diffusion coefficient , that only for a very narrow range around the average value is well reproduced .",
    "indeed , as one sees from tab .",
    "[ tab1 ] , the coefficient values for @xmath31 in our simulation significantly deviate from the ones found for the nao series .    further , functions @xmath72 and @xmath73 , plotted in figs .",
    "[ fig9]c - f , show also large scattering , particularly for @xmath122 .",
    "this feature raises difficulties in a proper minimum search for @xmath89 .",
    ".[tab1 ] optimized parameter values for the daily north atlantic oscillation daily index@xcite compared with the average values for @xmath151 sets of synthetic data ( `` with noise '' ) using the same number of points and parameter values .",
    "in order to evaluate the reliability of our synthetic data we also run the optimization procedure for @xmath151 sets of synthetic data with the same @xmath30 and @xmath31 found in nao series and @xmath14 ( `` no noise '' ) . in the last column",
    "we plot the results returned from the optimization procedure for synthetic data of pure measurement noise with amplitude @xmath152 , the one obtained for nao series . [ cols=\"^,^,^,^,^ \" , ]     in order to check the reliability of the calculations we reproduce the synthetic data @xmath151 times and present in tab .",
    "[ tab1 ] ( column `` with noise '' ) the average values for each parameter , where the error is taken as the largest deviation from the average over the sample of data sets . the measurement noise , which dominates all parameters , is well reproduced .",
    "for the drift and diffusion coefficient the order of magnitude of each parameter is also correct , but for @xmath153 and and @xmath78 one observes significant deviations from the estimated values obtained for the nao series .",
    "this mismatch between the empirical and synthetic series could raise the question if the nao index is indeed suitably described by a markovian stochastic process with a perceivable deterministic part . in fact , since one observes @xmath154 the series is approximately a pure white noise ( i.e.  @xmath155 in eq .",
    "( [ yy ] ) ) , which in fact also yields a linear drift and quadratic diffusion coefficients .    to address this problem we rerun our optimization procedure for synthetic data , for two additional situations , one where @xmath14 and drift and diffusion coefficients are given by the nao index , and another one which simulates a pure white noise ( @xmath156 ) with @xmath13 equal to the value found for the nao series .",
    "the results are also given in tab .",
    "[ tab1 ] , columns `` no noise '' and `` only noise '' respectively .    for the pure white noise process one obtains @xmath13 as the only non - zero parameter , apart fluctuations , but with an amplitude different from the one used to generate the synthetic data , namely @xmath157 , which corresponds to @xmath158 of the inserted measurement noise ( @xmath159 ) . for the synthetic process with no noise , the order of magnitude for the parameters of @xmath30 and @xmath31 is correctly computed , whereas a non - zero measurement noise is retrieved covering the remaining @xmath160 of the inserted measurement noise .",
    "in other words , one can argue that in this situation our procedure retrieves @xmath158 of the total amount of measurement noise . in this scope ,",
    "our results point in the direction of previous arguments given by some authors@xcite : differently from other climate indices such as the enso index , the nao index seems to be an almost pure white noise process with only a minor contribution from a stochastic process governed by a langevin - like equation .",
    "alternative indices should be therefore considered and studied as recently suggested@xcite .",
    "we described in detail a nonparametric procedure to extract measurement noise in empirical stochastic series with strong measurement noise .",
    "the algorithm is able to accurately extract the strength of measurement noise and the values of the parameters defining the drift coefficient and to estimate with good accuracy the diffusion coefficient that fully describe the evolution equation for the measured quantity in the time series .",
    "this has been shown by synthetically generated data sets contaminated by increasing measurement noise .",
    "additionally , the algorithm was applied to a set of measured data providing new insight in the underlying systems .",
    "the data for the climate index shows a large scattering , probably due to the small amount of data points .",
    "larger data sets for climate indices are not available up to our knowledge .",
    "it should be noticed that the nonparametric reconstruction of the langevin eq .",
    "( [ xlangevin ] ) from measured stationary data sets generally requires that the process exhibits markovian properties and fulfils the pawula theorem@xcite .",
    "while the second constraint can be relaxed extending the analysis to a broader class of langevin - like systems in which the gaussian @xmath3-correlated white noise langevin force is replaced by a more general lvy noise@xcite , in general the markov condition remains a crucial constraint .",
    "recently , it has been shown that processes corrupted from measurement noise may loose their markov properties@xcite .",
    "for this reason the proper analysis of data suffering from strong measurement noise in general is a complicated task .",
    "we , however , would like to point out , that the method presented here solely relies on markov properties of the underlying , undisturbed process @xmath0 . in case of @xmath3-correlated measurement noise the method",
    "presents a general approach to access the process @xmath18 and the noise amplitude @xmath13 at the same time .",
    "therefore , since the algorithm is general for a broad class of stochastic systems other applications can be proposed . particularly in cases",
    "where the measurement procedure is subject to large measurement noise due to the distance between the location where the measure is taken and the location where the phenomena occurs .",
    "two important applications in this context are seismographic data@xcite , where the epicenter can not be predicted before - hand , and data from surface eeg@xcite , which , though having stronger measurement noise , are much recommended instead of _ insitu _ measurements for the sake and comfort of the patient",
    ". a further application would be the analysis of sensors to which one has no access , for example sensors being installed in remote systems showing more and more measurement noise due to aging effects . here",
    "it should even be possible to know quite precisely the functional structure of the underlying process , an assumption of our analysis here .",
    "such applications however appeal for the extension of the present procedures to higher dimensions , i.e.  more than one time - series , which implies the consideration of different measurement noise sources and consequently noise mixing . to ascertain in which conditions and up to",
    "which point can we separate different measurement noise sources is an open question which we will address elsewhere .    in all simulations",
    "a linear function was assumed for the drift coefficient and a quadratic one for diffusion .",
    "although such assumptions comprehend already a broad class of systems@xcite our approach and all expressions may easily be extended to higher order polynomials for @xmath6 and @xmath7 , as long as the number of parameters for modelling @xmath6 and @xmath7 is not too high . in this case",
    "the calculations presented in the appendices are valid if one considers proper higher powers in the integrand of integrals @xmath161 and @xmath162 ( see eqs .",
    "( [ hmore ] ) in append .",
    "[ app : deriv ] ) .",
    "furthermore , other possibilities for optimization are possible .",
    "for instance , though in this case we have shown that random monte carlo procedures are computationally expensive consuming , one could think of a non - local search procedure using for example bigger jumps such as the ones of a lvy flight process@xcite .",
    "alternatively one may also study how good would be an optimization procedure that considers the minimization of a splitted cost function @xmath89 .",
    "preliminary results have shown that for a proper decomposition of @xmath89 our optimization problem may be reduced to a cubic equation and a lower dimensional system of linear equations .",
    "another possibility would be to use genetic algorithms@xcite .",
    "these points will be addressed elsewhere .",
    "the authors thank wilhelm and else heraeus foundation for supporting the meeting hold in bad honnef , where very usefull discussions happened and also the project drebm / daad/03/2009 for the bilateral cooperation between portugal and germany .",
    "pgl thanks reza m.  baram and bibhu biswal for usefull discussions .",
    "taking a series of measurements @xmath62 as defined in eq .",
    "( [ yy ] ) , its @xmath11-th order conditional moment reads @xmath163 where @xmath164 is the probability to measure @xmath24 in the presence of a measurement noise with variance @xmath165 , when the system ( without noise ) has the value @xmath18 , @xmath166 is the probability for the system to evolve from a value @xmath167 to a value @xmath18 within a time interval @xmath19 and @xmath168 has the inverse meaning of @xmath169 : it is the probability for the system to adopt the value @xmath167 when a measured value @xmath170 is observed . while @xmath171 is unknown , @xmath169 and @xmath172",
    "are related with each other according to bayes theorem ( see app .",
    "[ app : condprob ] ) .    from such assumptions",
    "one easily arrives to the identities    @xmath173    [ eq ]    and using these identities the general expression ( [ gencondmom_app ] ) can be approximated up to first order assuming @xmath174 . more precisely , the first two moments @xmath175 and @xmath55 yield @xmath176    from eq .",
    "( [ ym2_2_app ] ) one has @xmath177 where @xmath178 .",
    "such observations justify the first estimate for the measurement noise stated in eq .",
    "( [ approxmeasnoise ] ) , since when @xmath13 is small enough , probability density function @xmath168 is similar to @xmath179 ( see eq .",
    "( [ eq1 ] ) ) and therefore , one can take as a first approximation @xmath180 .    notice that the last equalities in @xmath175 and @xmath55 yield first order approximations under the assumption that @xmath181 . in ref .",
    "@xcite another approach is proposed for the estimation of drift and diffusion coefficients in the case of low sampling rates .",
    "the errors for @xmath182 , @xmath183 , @xmath184 and @xmath185 are just given from the linear fit of @xmath175 and @xmath55 for each fixed @xmath170 , given in eqs .",
    "( [ ym1_2 ] ) and ( [ ym2_2 ] ) .",
    "the errors of @xmath186 and @xmath187 can be also directly computed from the data as    @xmath188 ^ 2   \\rangle_{t\\in \\ { t_1,\\dots , t_n\\}}\\cr   & & \\cr   & = & \\langle ( y(t+\\tau)-y(t))^2 + \\hat{m}_1 ^ 2(y_0,\\tau ) - 2\\hat{m}_1(y_0,\\tau )       ( y(t+\\tau)-y(t))\\rangle \\cr   & & \\cr   & = & \\tfrac{1}{n_y }       \\left (       \\hat{m}_2(y_0,\\tau ) + \\hat{m}_1 ^ 2(y_0,\\tau ) -2\\hat{m}_1 ^ 2(y_0,\\tau )       \\right ) \\cr   & & \\cr   & = & \\frac{\\hat{m}_2(y,\\tau)-\\hat{m}_1 ^ 2(y,\\tau ) }                             { n_y}\\label{errorm1_app}\\\\    & & \\cr \\sigma_{\\hat{m}_2}^2(y,\\tau)&= &          \\langle \\left [ ( y(t+\\tau)-y(t))^2-                          \\langle ( y(t+\\tau)-y(t))^2\\rangle          \\right ] ^2",
    "\\rangle_{t\\in \\ { t_1,\\dots , t_n\\}}\\cr   & & \\cr   & = & \\langle ( y(t+\\tau)-y(t))^4 + \\hat{m}_2 ^ 2(y_0,\\tau ) - 2\\hat{m}_2(y_0,\\tau )       ( y(t+\\tau)-y(t))^2\\rangle \\cr   & & \\cr   & = & \\tfrac{1}{n_y }       \\left (       \\hat{m}_4(y_0,\\tau ) + \\hat{m}_2 ^ 2(y_0,\\tau ) -2\\hat{m}_2 ^ 2(y_0,\\tau )       \\right ) \\cr   & & \\cr   & = & \\frac{\\hat{m}_4(y,\\tau)-\\hat{m}_2 ^ 2(y,\\tau)}{n_y}\\label{errorm2_app } , \\end{aligned}\\ ] ]    [ errors_app ]    where @xmath189 is the number of data points in bin @xmath24 .    for the optimization procedure it is convenient to simplify the expressions for functions @xmath72 and @xmath73 ( @xmath190 ) .",
    "namely , @xmath191 and @xmath120 can be written as expressions of @xmath121 and @xmath122 .",
    "in fact , substituting eqs .",
    "( [ d1 ] ) and ( [ d2 ] ) into eqs .",
    "( [ m1 ] ) and ( [ m2 ] ) , and adding and subtracting properly @xmath24 , yields    @xmath192              \\bar{f}_{\\sigma}(x\\vert y)dx \\cr         & & \\cr         & = & d_{10}\\int_{-\\infty}^{+\\infty }              \\bar{f}_{\\sigma}(x\\vert y)dx +              d_{11}\\int_{-\\infty}^{+\\infty } ( x - y)\\bar{f}_{\\sigma}(x\\vert y)dx +              d_{11}y\\int_{-\\infty}^{+\\infty } \\bar{f}_{\\sigma}(x\\vert y)dx \\cr         & & \\cr         & = & d_{10 } + d_{11}(y+\\gamma_1(y ) ) \\label{newm1}\\\\         & & \\cr m_2(y ) & = & 2\\int_{-\\infty}^{+\\infty } [ ( x - y)d_1(x)+d_2(x ) ]              \\bar{f}_{\\sigma}(x\\vert y)dx \\cr         & & \\cr         & = & 2\\int_{-\\infty}^{+\\infty }              [ ( x - y)(d_{10}+d_{11}x)+d_{20}+d_{21}x+d_{22}x^2 ]             \\bar{f}_{\\sigma}(x\\vert y)dx \\cr         & & \\cr         & = & 2\\int_{-\\infty}^{+\\infty } \\big [                  ( x - y)\\left ( d_{10}+d_{11}(x - y+y)\\right ) +                  d_{20}+ \\cr         & & \\cr         & & \\hspace{2.0 cm } d_{21}(x - y+y)+d_{22}(x - y+y)^2 \\big ]                  \\bar{f}_{\\sigma}(x\\vert y)dx \\cr         & & \\cr         & = & 2 \\big [ \\gamma_1(y ) d_{10 } +             ( \\gamma_2(y)+y\\gamma_1(y ) ) d_{11 } +             d_{20 } +             ( \\gamma_1(y ) + y ) d_{21 } +             ( 2y\\gamma_1(y ) + \\gamma_2(y ) + y^2 ) d_{22 }              \\big ] .\\label{newm2}\\end{aligned}\\ ] ]    [ newm ]    substituting eqs .  ( [ newm1 ] ) and ( [ newm2 ] ) into eq .  ( [ f ] ) yields @xmath89 as a functional depending only on the integrals @xmath193 and @xmath194 defined in eqs .  ( [ gamma1 ] ) and ( [ gamma2 ] ) , apart the six parameters , @xmath13 and @xmath195 , we want to optimize .",
    "to solve the minimization problem we will need to explicitly write expressions for @xmath75 .",
    "this conditional probability density function appears in eqs .",
    "( [ gamma1 ] ) and ( [ gamma2 ] ) and according to the bayes theorem is given by : @xmath196 where @xmath164 is the probability density function of the measurement noise @xmath197 , i.e.  a gaussian function centered at @xmath24 with variance @xmath165 , @xmath198 and @xmath199 can be written , assuming that the process is stationary , as @xmath200 where @xmath201 is some normalized function such that @xmath202 and @xmath203    for an ornstein - uhlenbeck process @xmath204 and @xmath205 one finds @xmath206 from which one easily sees that @xmath207 is a necessary condition to have a well - defined probability density function @xmath199 .    for the general case given by eqs .",
    "( [ dd ] ) one has typically @xmath208 with @xmath209 , which yields @xmath210 . in these situations ,",
    "@xmath199 can also be integrated , yielding @xmath211 with @xmath212 .",
    "\\label{h0}\\ ] ]",
    "the minimization problem needs also the expression of the derivatives for the @xmath101 s and @xmath102 s . to compute them",
    "one needs first to write the derivatives of function @xmath75 defined in eq .",
    "( [ fx|y ] ) .    defining @xmath213",
    "one has in general @xmath214 where @xmath215 is some variable on which @xmath172 depends .",
    "since @xmath199 depends only on parameters @xmath32 and @xmath164 depends only on @xmath13 , we have                      @xmath226 \\bar{f}_{\\sigma}(x\\vert y )            \\label{der - f - sigma}\\\\       & & \\cr \\frac{\\partial \\bar{f}_{\\sigma}(x\\vert y)}{\\partial d_{ij } } & = &        \\frac{\\hbox{\\large{e}}^{-\\frac{(x - y)^2}{2\\sigma^2 } }              \\frac{\\partial p(x)}{\\partial d_{ij } }              -\\bar{f}_{\\sigma}(x\\vert y )              \\int_{-\\infty}^{+\\infty }               \\hbox{\\large{e}}^{-\\frac{(x^{\\prime}-y)^2}{2\\sigma^2 } }              \\frac{\\partial p(x^{\\prime})}{\\partial d_{ij}}dx^{\\prime } }             { \\int_{-\\infty}^{+\\infty } g(x^{\\prime},y)dx^{\\prime } } \\label{der - f - dij}\\end{aligned}\\ ] ]        @xmath229 \\label{der - gamma1-sigma}\\\\       & & \\cr \\frac{\\partial \\gamma_2(y)}{\\partial \\sigma } & = &        \\frac{1}{\\sigma^3 } \\left [ h_2(y ) -",
    "\\gamma_2 ^ 2(y )                           \\right ] \\label{der - gamma2-sigma}\\\\       & & \\cr \\frac{\\partial m_1(y)}{\\partial \\sigma } & = & d_{11 }              \\frac{\\partial \\gamma_1(y)}{\\partial \\sigma }      \\label{der - m1-sigma}\\\\     & & \\cr \\frac{\\partial m_2(y)}{\\partial \\sigma } & = &        2\\left (          ( d_{21}+y(d_{11}+2d_{22 } ) )       \\frac{\\partial \\gamma_1(y)}{\\partial \\sigma }           + ( d_{11}+d_{22 } )       \\frac{\\partial \\gamma_2(y)}{\\partial \\sigma }         \\right )     \\label{der - m2-sigma}\\\\     & & \\cr \\frac{\\partial \\gamma_1(y)}{\\partial d_{ij } } & = &        \\int_{-\\infty}^{+\\infty } ( x^{\\prime}-y )           \\frac{\\partial \\bar{f}_{\\sigma}(x^{\\prime}\\vert y)}{\\partial d_{ij } } dx^{\\prime }       \\label{der_gamma1_dij}\\\\      & & \\cr \\frac{\\partial \\gamma_2(y)}{\\partial d_{ij } } & = &        \\int_{-\\infty}^{+\\infty } ( x^{\\prime}-y)^2           \\frac{\\partial \\bar{f}_{\\sigma}(x^{\\prime}\\vert y)}{\\partial d_{ij } } dx^{\\prime }       \\label{der_gamma2_dij}\\\\      & & \\cr \\frac{\\partial m_1(y)}{\\partial d_{11 } } & = &              y+\\gamma_1(y ) + d_{11 }              \\frac{\\partial \\gamma_1(y)}{\\partial d_{11 } }      \\label{der - m1-d11}\\\\     & & \\cr \\frac{\\partial m_1(y)}{\\partial d_{2j } } & = &              d_{11 } \\frac{\\partial \\gamma_1(y)}{\\partial d_{2j } }      \\label{der - m1-d2j}\\\\     & & \\cr \\frac{\\partial m_2(y)}{\\partial d_{11 } } & = &              2\\left (               ( d_{21}+y(d_{11}+2d_{22 } ) )               \\frac{\\partial \\gamma_1(y)}{\\partial d_{11 } } +               ( d_{11}+d_{22 } )               \\frac{\\partial \\gamma_2(y)}{\\partial d_{11 } } +               \\gamma_2(y)+y\\gamma_1(y )             \\right )     \\label{der - m2-d11}\\\\     & & \\cr \\frac{\\partial m_2(y)}{\\partial d_{20 } } & = &              2\\left (               ( d_{21}+y(d_{11}+2d_{22 } ) )               \\frac{\\partial \\gamma_1(y)}{\\partial d_{20 } } +               ( d_{11}+d_{22 } )               \\frac{\\partial \\gamma_2(y)}{\\partial d_{20 } } +               1             \\right )     \\label{der - m2-d20}\\\\     & & \\cr \\frac{\\partial m_2(y)}{\\partial d_{21 } } & = &              2\\left (               ( d_{21}+y(d_{11}+2d_{22 } ) )               \\frac{\\partial \\gamma_1(y)}{\\partial d_{21 } } +               ( d_{11}+d_{22 } )               \\frac{\\partial \\gamma_2(y)}{\\partial d_{21 } } +               \\gamma_1(y)+y             \\right )     \\label{der - m2-d21}\\\\     & & \\cr \\frac{\\partial m_2(y)}{\\partial d_{22 } } & = &              2\\left (               ( d_{21}+y(d_{11}+2d_{22 } ) )               \\frac{\\partial \\gamma_1(y)}{\\partial d_{22 } } +               ( d_{11}+d_{22 } )               \\frac{\\partial \\gamma_2(y)}{\\partial d_{22 } } +               2y\\gamma_1(y)+\\gamma_2(y)+y^2             \\right )     \\label{der - m2-d22}\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> it is a big challenge in the analysis of experimental data to disentangle the unavoidable measurement noise from the intrinsic dynamical noise . here </S>",
    "<S> we present a general operational method to extract measurement noise from stochastic time series , even in the case when the amplitudes of measurement noise and uncontaminated signal are of the same order of magnitude . </S>",
    "<S> our approach is based on a recently developed method for a nonparametric reconstruction of langevin processes . minimizing a proper non - negative function </S>",
    "<S> the procedure is able to correctly extract strong measurement noise and to estimate drift and diffusion coefficients in the langevin equation describing the evolution of the original uncorrupted signal . as input , </S>",
    "<S> the algorithm uses only the two first conditional moments extracted directly from the stochastic series and is therefore suitable for a broad panoply of different signals . to demonstrate the power of the method we apply the algorithm to synthetic as well as climatological measurement data , namely the daily north atlantic oscillation index , shedding new light on the discussion of the nature of its underlying physical processes . </S>"
  ]
}