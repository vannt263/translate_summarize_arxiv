{
  "article_text": [
    "physicists develop mathematical models from physical considerations , but the process of solving a model is nt always related to its physics . intermediate steps of a long calculation may not have physical meaning nor grant physical insight .",
    "an important example of this is sparse linear system solving , which is used to solve discretized approximations of physical problems described by time - independent partial differential equations .",
    "the most general - purpose sparse linear solvers involve either direct factorization @xcite , where intermediate steps contain partially factored matrices , or iterative solvers @xcite , where intermediate steps contain approximate solutions of increasing accuracy . for some well - understood problems , most notably the poisson equation , known physical properties can be incorporated into a linear solver , either via multigrid methods @xcite or multilevel preconditioning @xcite , leading to algorithms that are both more physical and of optimal complexity .",
    "these methods operate by approximately transforming away local details of the physical system , leaving successively smaller but continually sparse  coarsened \" matrix equations that each represent the physical system on a different length scale . the multigrid framework @xcite and coarsening procedures @xcite have been generalized into a more algebraic formalism , but their success is still tied to certain spectral properties of the underlying physical system . in this paper",
    "we construct a more general sparse matrix solver based on a high accuracy limit of algebraic multigrid that does not require as input a physical intuition for the problem and is not restricted by spectral properties .",
    "the test system we use in this paper is the 2d helmholtz equation discretized on a uniform grid using finite differences and defined by the five point matrix stencil @xmath0,\\ ] ] where @xmath1 is proportional to the frequency squared .",
    "the finite difference approximation loses accuracy as @xmath1 gets larger and breaks down entirely for @xmath2 due to inadequate sampling of oscillations , but we are interested in the matrix problem and not necessarily its accuracy in reproducing the continuum problem .",
    "this is perhaps the simplest example of a problem for which optimal linear solvers exist but as yet require some analytic knowledge of the solution to construct . for rectangular domains ,",
    "the eigenfunctions are composed of sinusoidal oscillations , and thus fast fourier transforms @xcite can diagonalize the matrix .",
    "given the exact analytic inverse of the helmholtz equation , one can hierarchically compress and apply it in an optimal manner using the fast multipole method @xcite . in a more algebraic manner , using just the knowledge that solutions have a characteristic frequency of oscillation , it is possible to construct an optimal ray - based multigrid scheme @xcite .",
    "algebraic methods that do nt take specific account of the oscillatory nature of solutions currently fail to solve the problem in an optimal manner . for direct factorizations ,",
    "the cost has been proven to be non - optimal for problems on a 2d grid @xcite . for the parameter range of oscillatory behavior , @xmath3 ,",
    "preconditioners based on multigrid principles fail to be optimal due to a loss of smoothness on coarse grids @xcite , and structured direct methods fail due to the loss of low off - diagonal rank @xcite .    in section [ formtrans ]",
    ", we describe the form of the linear solver as a succession of local transformations and some of their properties and governing equations . in section [ linmethod ]",
    ", we derive an efficient method for constructing local transformations and apply it to our test problem . in section [ blocksection ] , we further generalize the local transformations by changing sparsity patterns to improve accuracy .",
    "the basic operation of our linear solver is to start from @xmath4 , an @xmath5 real symmetric sparse matrix at some stage of factorization , and apply a real symmetric transformation , @xmath6 that leaves us with an @xmath7 that has one more diagonalized row / column than @xmath4 and a small amount of error , @xmath8 .",
    "transformations of this form are found in direct ldl factorization @xcite , where @xmath9 is the identity plus a rank one matrix and @xmath8 is just floating point roundoff error .",
    "this form can also be related to multigrid solvers , if coarsening and relaxation are combined into a single invertible transformation @xcite and if coarsening is performed only on one single small subdomain at a time .",
    "the @xmath9 will be a dense matrix on this small subdomain and identity outside of it , and @xmath8 on the subdomain will be substantially larger than roundoff errors .",
    "the benefit of multigrid , despite the large error , is that the sparsity pattern of @xmath7 can be more controlled and the critical filling in of @xmath4 during factorization that prevents ldl factorization from being optimal can be avoided .",
    "the large error @xmath8 incurred at each factorization step can be negated by including a multilevel refinement scheme in the linear solving procedure , but the success of refinement is based on details of the spectral properties of the problem @xcite .",
    "the current prescription for reducing multigrid coarsening error is simply to increase fill - in of the sparsity pattern of @xmath7 @xcite , but this relates error to matrix fill and reduces our ability to control the sparsity pattern .",
    "our more general approach is to hold the sparsity pattern of @xmath7 fixed while allowing more freedom in the choice of @xmath9 , enough to enable @xmath10 to be arbitrarily reduced , bounded only by machine precision .",
    "we denote this fixed sparsity , high accuracy limit of algebraic coarsening as _ perfect algebraic coarsening _ and denote the @xmath9 matrices as _ local sparsity - preserving transformations_.    repeated transformations take us from our initial matrix @xmath11 to a final diagonal form @xmath12 , @xmath13 which leads to a compact representation of the inverse of @xmath11 , @xmath14 if @xmath10 can be reduced sufficiently , then this multigrid - based factorization can be made as accurate as a direct factorization , foregoing the need for the iterative steps of multigrid .",
    "if we can restrict each transformation @xmath15 to differ from identity only on an @xmath16-independent sized subdomain of the problem , then each of these @xmath16 transformations can be calculated with an @xmath16-independent cost , and the resulting linear solver will be of optimal @xmath17 complexity .    the restriction on each @xmath9 is a  local \" one , which in terms of the underlying grid means that a transformation that decouples a node should only act on neighbors of that node up to at most some @xmath18 nearest neighbor .",
    "the restricted transformation takes the form @xmath19 \\left [ \\begin{array}{cc } a_{ll } & a_{le } \\\\ a_{le}^t & a_{ee } \\end{array } \\right ] \\left [ \\begin{array}{cc } x_l & 0 \\\\ 0 & i \\end{array } \\right ] = \\left [ \\begin{array}{cc } \\tilde{a}_{ll } & a_{le } \\\\ a_{le}^t & a_{ee } \\end{array } \\right ] + e , \\label{localtransform}\\ ] ] where the subscript ` @xmath20 ' refers to a local partition and ` @xmath8 ' to the remaining external partition . in order for eq . ( [ localtransform ] ) to be satisfied with a small error , we have to enforce the condition , @xmath21 , either approximately with some least squares approach or exactly by finding the null space of @xmath22 or more simply by further partitioning the local region into an interior ` @xmath23 ' and boundary ` @xmath24 ' , @xmath25,\\ ] ] and further restricting @xmath26 to @xmath27 \\left [ \\begin{array}{cc } a_{ii } & a_{ib } \\\\",
    "a_{ib}^t & a_{bb } \\end{array } \\right ] \\left [ \\begin{array}{cc } x_i & x_b \\\\ 0 & i \\end{array } \\right ] = \\left [ \\begin{array}{cc } \\tilde{a}_{ii } & \\tilde{a}_{ib } \\\\ \\tilde{a}_{ib}^t & \\tilde{a}_{bb } \\end{array } \\right ] + e. \\label{interiortransform}\\ ] ] our calculation of @xmath9 and @xmath7 may now proceed independently of the external partition , with some @xmath16-independent cost dependent only on @xmath28 , @xmath29 , and @xmath30  the sizes of the local , boundary , and internal partitions  and @xmath31 , the number of independent nonzero elements in the symmetric @xmath32 .",
    "we must next define an error norm to be minimized by our choice of transformation . a convenient choice of norms when dealing with variable matrices is the frobenius norm , @xmath33 .",
    "minimizing @xmath34 directly leads to the expression @xmath35 with @xmath7 restricted to a given sparsity pattern and @xmath9 restricted to the form in eq .",
    "( [ interiortransform ] ) .",
    "this error norm is problematic because it is dependent on a choice of normalization for @xmath9 to prevent such spurious solutions as @xmath36 and to prevent @xmath9 from becoming singular .",
    "an error norm that does nt rely on normalizing @xmath9 is @xmath37 , with the corresponding minimization @xmath38 where @xmath39 is given the same local form as @xmath9 .",
    "this expression is less appealing because it is more nonlinear than eq .",
    "( [ errmin ] ) in that it contains @xmath40 order variable terms rather than just quartic terms .",
    "however , it is a more direct minimization of the error perturbation that takes us from our approximate inverse to the exact inverse , @xmath41 this error norm will be used for the remainder of this paper .",
    "using a local transformation to remove matrix elements is only a specific application of a general ability to alter the values of matrix elements while preserving the sparsity pattern of a matrix . we can consider a transformation to be part of a continuous family of transformations , @xmath42 and @xmath43 , that begins at @xmath44 as the error free identity , @xmath45 and @xmath46 , and ends at @xmath47 .",
    "we evolve from the error free transformation by following the minimum error transformations as we continuously turn on a non - negative constraint that enforces the final , restricted sparsity pattern at @xmath47 , @xmath48 + t \\cdot f_{constraint}[x(t),\\tilde{a}(t ) ] \\right ) .",
    "\\label{error_constrained}\\ ] ] following this defined path of transformations , the constrained error norm in eq .",
    "( [ error_constrained ] ) is non - decreasing with increasing @xmath49 . in order for the final transformation at @xmath47 to have a small error ,",
    "the error must be small throughout the path and the jacobian of error with respect to changes of @xmath50 must have an equally small near - null @xmath51 component tangent to the path .",
    "correspondingly , we expect the condition number of the error minimization process to be inversely proportional to the minimum error attainable by the transformation .    to illustrate the ill - conditioned nature of eq .",
    "( [ errmin2 ] ) , we attempt to minimize it by following the negative gradient for a single transformation on our helmholtz test problem at @xmath52",
    ". we decouple one node on the interior of the grid without adding or subtracting any other terms from the sparsity pattern of @xmath7 and the local region consists of all nodes within @xmath53 hops of the decoupled node .",
    "we start from an initial guess of @xmath54 and the nonzero terms of @xmath7 set to the corresponding values of @xmath4 . at each iteration ,",
    "the gradient is calculated and the error norm is minimized in the direction of the gradient . the error norm for the first 1000 iterations is plotted in fig .",
    "[ fig_sd ] for several values of @xmath53 .",
    "only the @xmath55 case converges within 1000 iterations , but the expected trend of decreasing error and increasing condition number with increasing @xmath53 is readily apparent .",
    "a tractable calculation of @xmath56 and @xmath7 requires a more careful treatment of the ill - conditioned jacobian .    .",
    "calculating and inverting the exact jacobian of eq .",
    "( [ errmin2 ] ) is impractical due to its size , ill - conditioning , and large null space .",
    "the symmetric error matrix , @xmath57 , contains @xmath58 elements to be minimized and the @xmath56 and @xmath7 variables contain @xmath59 independent unknowns . for some local partitions , such as @xmath60 in our test problem , there are more unknowns than matrix elements to be minimized , but the minimization is not underdetermined due to a large null space . to alleviate these difficulties we separate the minimizations of @xmath56 and @xmath7 in an approximate way that leaves us with a well - conditioned problem in @xmath56",
    "whose null space can be analytically removed and a much smaller , ill - conditioned problem in @xmath7 .",
    "we approximately linearize eq .",
    "( [ errmin2 ] ) by expanding @xmath56 and @xmath7 in small changes , @xmath61 and @xmath62 , and keeping only terms to first order in @xmath63 and @xmath64 within the norm , @xmath65 because of the restricted form of @xmath56 , @xmath63 is an @xmath66 matrix and @xmath67 is an @xmath66 submatrix of identity .",
    "this is not the correct way to linearize eq .",
    "( [ errmin2 ] ) - there are additional linear terms proportional to @xmath10 whose neglect leads to a linear convergence of the minimization - but this approximation leads to a greatly simplified solution .",
    "the frobenius norm is invariant with respect to orthogonal rotations of its operand , and we choose a particularly useful rotation consisting of the null space @xmath68 and the spanned space @xmath69 of @xmath70 . due to the @xmath67",
    ", the spanned space usually contains @xmath30 vectors , but it can contain less if @xmath7 is rank deficient .",
    "if we apply the rotation to the error norm , we can write the norm squared as @xmath71 the first two terms of eq .",
    "( [ roterr ] ) can be canceled with a proper choice of @xmath63 , @xmath72 where the inverse is a pseudo - inverse .",
    "this leaves the third term to be minimized by @xmath64 , @xmath73 which is overdetermined and ill - conditioned .    the approximately linearized eq .",
    "( [ roterr ] ) has a significant null space , corresponding to additions to @xmath63 of the form @xmath74 for any antisymmetric matrix @xmath75 .",
    "this null space has a size of @xmath76 for full rank @xmath7 , which is large enough to account for eq .",
    "( [ errmin2 ] ) being overdetermined .",
    "solving eq .",
    "( [ errmin3 ] ) is the most difficult and expensive step of the error minimization .",
    "the cost of an unstructured qr factorization of the problem is @xmath77 .",
    "however , the system s matrix has some structure , it is a sum of two submatrices of the kronecker product @xmath78 .",
    "there are no existing structured qr factorization algorithms for this kind of matrix , but the structure allows for an efficient construction of the normal equations , which is a sum of two @xmath79 submatrices of @xmath80 .",
    "the cost of constructing and solving the normal equations is @xmath81 , which is an improvement over unstructured qr if @xmath82 . in our 2d example",
    "@xmath83 making the order of complexity equal in both methods , but the normal equations are still faster due to a smaller prefactor .",
    "the disadvantage of using the normal equations is the squaring of the condition number , which has a noticeable effect in the ill - conditioned , small error limit .",
    "we return to the test problem at @xmath52 , now using the linearized solution approach rather than following the gradient . the same local region , @xmath7 sparsity pattern , and initial @xmath56 and @xmath7",
    "are used as in section [ illcond ] .",
    "the @xmath7 minimization is performed using the normal equations which are solved using singular value decomposition ( svd ) for testing purposes .",
    "after each iteration the solution is updated , @xmath84 and @xmath85 , with @xmath86 chosen to minimize the error norm .",
    "the svd of eq .",
    "( [ errmin3 ] ) , which is performed numerically on its normal equations , is shown in fig .",
    "[ fig_svd ] for the initial @xmath56 and @xmath7",
    ". an interesting feature of each svd spectra is the null space of size @xmath30 , resolved in this calculation to single precision , @xmath87 , relative to the largest singular value .",
    "the null space corresponds to the set of local transformations that exactly preserve sparsity and in this case diagonal scaling of the interior block , @xmath88 .",
    "a change in diagonal scaling does nt effect the chosen error norm from eq .",
    "( [ errmin2 ] ) and correspondingly the error term in eq .",
    "( [ errmin3 ] ) is orthogonal to the null space within machine precision .",
    "the contribution to @xmath64 from the null space should be zero , and since it can be clearly distinguished from the gap in the spectrum , we can simply ignore the null space component .",
    "the smallest singular value of the rest of the spectrum shows an exponential decay with respect to @xmath53 , which suggests an exponential decay of the minimum error according to the argument in section [ illcond ] .",
    "the limiting effects of finite precision are clearly visible in the vanishing of the gap between the null and spanned space for @xmath89 .    ) for @xmath52 ( calculated from the normal equations ) . ]    the convergence of the linearized solution approach is shown in fig .",
    "[ fig_conv ] .",
    "each calculation takes approximately @xmath53 steps to converge , which signifies the success of our approximate inverse jacobian in capturing the ill - conditioned aspects of the problem .",
    "the error exponentially decays with @xmath53 as expected from the spectrum of eq .",
    "( [ errmin3 ] ) . this spatial decay of error can be related to a spatial decay of @xmath7 to @xmath4 and @xmath56 to @xmath23 by taking the error to be caused by the truncation of some dense exact @xmath56 to @xmath23 outside a local region .",
    "the relation of the decays can be seen in fig .",
    "[ fig_space ] , where the error norm as a function of @xmath53 is plotted against the deviations of @xmath56 and @xmath7 from @xmath23 and @xmath4 measured by column and plotted by the geometric distance on the 2d grid of the associated node from the central , decoupled node . since @xmath56 and @xmath7 are only defined up to a diagonal scaling of the interior block , the rows of @xmath56 are normalized to a 2-norm of one to make them unique .    . ]     values and column norms from the @xmath90 minimum error solution . ]     using linearized solutions . ]    ) at convergence for @xmath91 . ]",
    "we next try the method on the more interesting @xmath3 case , though only the @xmath92 range needs to be tested as the matrix for @xmath93 can be mapped to @xmath94 with a diagonal scaling . the converged error norm for multiple values of @xmath1 and @xmath53 are plotted in fig .",
    "[ fig_lambda ] . the condition number of the row normalized @xmath56 is less than twelve for all calculations performed .",
    "the decay of @xmath95 off the diagonal is exponential in geometric distance for @xmath96 , but this qualitative change in behavior from @xmath97 does nt cause any kinks in the error at @xmath52 .",
    "we observe that the exponential decay rate of error with @xmath53 is approximately proportional to @xmath98 . near the @xmath99 point",
    ", the exponential error decay appears to break down leaving an error with an @xmath53-dependence proportional to the logarithmic decay of off - diagonal elements of @xmath95 .",
    "the most obvious matrix property to attribute to the loss of decay near @xmath99 is the vanishing of the diagonal elements of @xmath4 .    the inverse proportionality between the condition number of the non - null subspace of eq .",
    "( [ errmin3 ] ) and the minimum error norm continues to hold as a function of @xmath1 as shown in fig .",
    "[ fig_errcond ] . the condition number plotted",
    "is calculated at the converged @xmath100 value , but the condition number varies very little between iterations and it is within a factor of two of the condition number calculated from the initial @xmath100 guess .    the loss of exponential error decay as @xmath101 signifies the disappearance of locally removable degrees of freedom from a model restricted in form by the restriction on the sparsity pattern . as @xmath101 the wavelength of oscillations in the helmholtz equation approaches four times the grid spacing , a high frequency limit where multigrid also fails . for the multigrid approach to continue into this limit , the solution",
    "must be decomposed into a sum of envelope functions times oscillatory solutions with wavevectors in various directions @xcite .",
    "this is a transformation from a scalar differential equation to a vector differential equation , which ca nt be represented by @xmath102 unless the sparsity pattern of @xmath7 is allowed to fill in somewhat .",
    "error decay is restored for @xmath2 only because the discretization of the helmholtz equation breaks down and the correct high frequency oscillations are no longer present in the matrix problem .",
    "for all the tests performed in section [ linmethod ] we strictly prevented fill - in in transforming from @xmath4 to @xmath7 , but it is only really necessary to control fill - in enough to preserve the scalability of the factorization . it can be beneficial to add nonzero matrix elements to @xmath7 because that increases the number of degrees of freedom in the error minimization , eq .",
    "( [ errmin2 ] ) , and can reduce the minimum error norm .    an important reason for filling in @xmath7",
    "is to prevent the removal of a node from breaking the global connectivity of a problem .",
    "the simplest case of this is a tridiagonal matrix , which can be associated with a problem on a 1d grid . if a node is removed from the grid without filling in the matrix , then the grid will be split in half .",
    "the associated transformation would have to contain all the response of each half of the grid on the other and can not in general be accurately made local .",
    "one simple way to avoid changing the connectivity of a problem is to aggregate nodes together into supernodes where all the member nodes share all the connections of other member nodes .",
    "once a supernode is formed , the decoupling of a node in the supernode wo nt break any connectivity as long as one node remains within the supernode .",
    "the supernode concept has been used before in sparse gaussian elimination for efficiency reasons @xcite to allow for the use of dense matrix operations in inner loops , but here it serves a more fundamental purpose .",
    "the larger the supernodes are made , the more filled in the matrix will be , and the error norm will have a decreasing minimum with fixed local region size .",
    "if the supernodes are made large enough , then gaussian elimination steps can be performed without additional matrix filling before the more expensive algebraic coarsening procedure in a possibly more efficient hybrid approach . for our example on a 2d grid",
    ", the grid of nodes can be made a grid of supernodes , which can be interpreted as a discretization of a vector differential equation where the number of vector components is the size of the supernode .",
    "we again return to the test problem , now with a 2d grid of supernodes constructed by merging @xmath103 rectangles of neighboring nodes .",
    "a local transformation is performed to remove one node from one supernode and the local region is chosen to include all supernodes within @xmath53 hops of the removed node . the converged error norm for @xmath104 , @xmath105 is plotted in fig .",
    "[ fig_block ] and the important difference with fig .",
    "[ fig_lambda ] is the error seems to continue to decay exponentially in @xmath53 near @xmath99 rather than stagnate at @xmath106 . a comparison between three different supernode sizes is plotted in fig .",
    "[ fig_multiblock ] . for similar @xmath28",
    "all errors are roughly the same in the non - oscillatory regime , @xmath107 , and at the maximally oscillatory point , @xmath99 , while the larger supernodes errors are smaller in the intermediate oscillatory regime , @xmath108 .",
    "this result suggests that supernodes are useful for increasing the rate of error decay , but if the @xmath99 case can indeed be improved , larger supernodes are required .    , @xmath105 supernodes . ]    . ]",
    "the arguments made in ref .",
    "@xcite suggest that at least an eight wave expansion is required for an efficient solution of the @xmath99 case , which might be properly captured by @xmath109 or @xmath110 . however , both cost and conditioning are a barrier to the current approach to calculating these transformations . the cost of solving the normal equation version of eq .",
    "( [ errmin3 ] ) for fixed local region size scales as @xmath111 .",
    "the conditioning of eq .",
    "( [ errmin3 ] ) remains inversely proportional to the minimum error norm , but the constant of proportionality is observed to change substantially with @xmath112 and @xmath113 , causing calculations to be more ill - conditioned with the same minimum error norms .",
    "we have studied the possibility of factoring sparse matrices by means of local sparsity - preserving transformations with numerical tests of a single transformation .",
    "intermediate stages of such a factorization require transformations to be performed on matrices of similar sparsity but with different values of their matrix elements , which was examined here in a simple , artificial manner by varying the frequency of the helmholtz equation .",
    "qualitatively , we expect the intermediate ,  coarsened \" matrices to still represent the helmholtz equation with the frequency scaled to represent a change of length scale .",
    "the absence of local degrees of freedom for the @xmath99 case in section [ numerical ] suggests that this interpretation fails when the wavelength becomes proportional to the grid spacing . to continue to remove local degrees of freedom beyond this frequency , it becomes necessary to allow the coarsened matrices to take a more general form .",
    "the sparse linear solver methodology presented in this paper has demonstrated a behavior distinct from both direct and iterative solvers .",
    "the success of direct solvers is dependent on the filling in of the matrix in the intermediate stages of factorization , which is a graph theoretic property and is controlled by the order in which nodes are factored .",
    "the success of iterative solvers is dependent on the condition number of the matrix and is controlled by preconditioning a problem to reduce the condition number . here",
    "the determining characteristic of how costly it is to solve a matrix is the decay of error of a transformation with respect to local region size and can be controlled by changing the local region or sparsity pattern .",
    "matrix fill is no longer a problem as it is strictly controlled , and the error decay is a local property completely independent of the global spectrum and conditioning of the matrix .",
    "there remain technical difficulties with calculating local sparsity - preserving transformations that must be resolved before a practical linear solver can be implemented with them .",
    "the most important problem is determining whether or not a well - conditioned process exists for calculating local transformations .",
    "the ill - conditioning is associated with minimizing an error norm , and a method based on additional criteria might precondition the process .",
    "another important problem is understanding what properties of a matrix and sparsity pattern determine the rate of decay of error .",
    "this is needed to determine precisely when sparsity patterns should be changed and how they should be changed to make calculations most efficient .",
    "once the issues associated with single transformations are resolved , there is the additional problem of choosing the ordering of transformations .",
    "the ordering can determine error decay rates of successive transformations , error propagation during factorization , and the amount to which the process can be parallelized .",
    "the simple answer at least for the purpose of parallelization is to choose as many transformations as possible on disjoint local regions to maximize the number of concurrent calculations of local transformations .",
    "this work was supported by national science foundation grant no .",
    "dmr04 - 39768 and by the director , office of science , office of basic energy sciences , division of materials sciences and engineering , u.s .",
    "department of energy under contract no .",
    "de - ac03 - 76sf00098 .",
    "g. h. golub and c. f. van loan , _ matrix computations _ , 3rd ed .",
    "( john hopkins university press , baltimore , 1996 ) .",
    "r. barrett , m. berry , t. f. chan , j. demmel , j. donato , j. dongarra , v. eijkhout , r. pozo , c. romine , and h. van der vorst , _",
    "templates for the solution of linear systems : building blocks for iterative methods _ ,",
    "( siam , philadelphia , pa , 1994 ) . w. l. briggs , v. e. henson , and s. f. mccormick , _ a multigrid tutorial _",
    "( siam , philadelphia , pa , 2000 ) .",
    "w. dahmen and a. kunoth , multilevel preconditioning , _ numer .",
    "_ * 63 * , 315 ( 1992 ) .",
    "r. d. falgout and p. s. vassilevski , on generalizing the amg framework , _ siam j. numer .",
    "* 42 * , 1669 ( 2004 ) .",
    "a. brandt , general highly accurate algebraic coarsening , _ electron .",
    "trans . numer .",
    "* 10 * , 1 ( 2000 ) .",
    "j. w. cooley and o. w. tukey , an algorithm for the machine calculation of complex fourier series , _ math .",
    "_ * 19 * , 297 ( 1965 )",
    ". r. coifman , v. rokhlin , and s. wandzura , the fast multipole method for the wave equation : a pedestrian prescription , _ ieee antennas propag . _",
    "* 35 * , 7 ( 1993 ) .",
    "a. brandt and i. livshits , wave - ray multigrid method for standing wave equations , _ electron .",
    "anal . _ * 6 * , 162 ( 1997 ) .",
    "a. george , nested dissection of a regular finite element mesh , _",
    "siam j. numer .",
    "* 10 * , 345 ( 1973 ) .",
    "h. c. elman , o. g. ernst , and d. p. oleary , a multigrid method enhanced by krylov supspace iteration for discrete helmholtz equations , _",
    "siam j. sci .",
    "* 23 * , 1291 ( 2001 ) .",
    "m. bebendorf and w. hackbusch , existence of @xmath114-matrix approximants to the inverse fe - matrix of elliptic operators with @xmath115-coefficients , _ numer .",
    "_ * 95 * , 1 ( 2003 ) .",
    "j. w. demmel , s. c. eisenstat , j. r. gilbert , x. s. li , and j. w. h. liu , a supernodal approach to sparse partial pivoting , _ siam j. matrix anal .",
    "a. _ * 20 * , 720 ( 1999 ) ."
  ],
  "abstract_text": [
    "<S> presented in this paper is a new sparse linear solver methodology motivated by multigrid principles and based around general local transformations that diagonalize a matrix while maintaining its sparsity . </S>",
    "<S> these transformations are approximate , but the error they introduce can be systematically reduced . </S>",
    "<S> the cost of each transformation is independent of matrix size but dependent on the desired accuracy and a spatial error decay rate governed by local properties of the matrix . </S>",
    "<S> we test our method by applying a single transformation to the 2d helmholtz equation at various frequencies , which illustrates the success of this approach . </S>"
  ]
}