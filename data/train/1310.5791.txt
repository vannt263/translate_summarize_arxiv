{
  "article_text": [
    "accurate recovery of low - rank matrices has a wide range of applications , including quantum state tomography @xcite , face recognition , recommender systems @xcite and linear system identification and control @xcite .",
    "for example , a key step in reconstructing the quantum states in low - rank quantum tomography is the estimation of a low - rank matrix based on pauli measurements @xcite . and phase retrieval ,",
    "a problem which arises in a range of signal and image processing applications including x - ray crystallography , astronomical imaging and diffraction imaging , can be reformulated as a low - rank matrix recovery problem @xcite .",
    "see recht et  al .",
    "@xcite and cands and plan @xcite for further references and discussions .    motivated by these applications ,",
    "low - rank matrix estimation based on a small number of measurements has drawn much recent attention in several fields , including statistics , electrical engineering , applied mathematics and computer science .",
    "for example , cands and recht @xcite , cands and tao @xcite and recht @xcite considered the exact recovery of a low - rank matrix based on a subset of uniformly sampled entries .",
    "negahban and wainwright @xcite investigated matrix completion under a row / column weighted random sampling scheme .",
    "recht et  al .",
    "@xcite , cands and plan @xcite and cai and zhang @xcite studied matrix recovery based on a small number of linear measurements in the framework of restricted isometry property ( rip ) , and koltchinskii et  al .",
    "@xcite proposed the penalized nuclear norm minimization method and derived a general sharp oracle inequality under the condition of restrict isometry in expectation .",
    "the basic model for low - rank matrix recovery can be written as @xmath0 where @xmath1 is a linear map , @xmath2 is an unknown low - rank matrix and @xmath3 is a noise vector .",
    "the goal is to recover the low - rank matrix @xmath4 based on the measurements @xmath5 .",
    "the linear map @xmath6 can be equivalently specified by @xmath7 @xmath8 measurement matrices @xmath9 with @xmath10 where the inner product of two matrices of the same dimensions is defined as @xmath11 . since @xmath12 , ( [ eqmodel ] ) is also known as trace regression .",
    "a common approach to low - rank matrix recovery is the constrained nuclear norm minimization method which estimates @xmath4 by @xmath13 here , @xmath14 is the nuclear norm of the matrix @xmath15 which is defined to be the sum of its singular values , and @xmath16 is a bounded set determined by the noise structure .",
    "for example , @xmath17 in the noiseless case and @xmath16 is the feasible set of the error vector @xmath3 in the case of bounded noise .",
    "this constrained nuclear norm minimization method has been well studied .",
    "see , for example , @xcite .",
    "two random design models for low - rank matrix recovery have been particularly well studied in the literature .",
    "one is the so - called `` gaussian ensemble '' @xcite , where the measurement matrices @xmath18 are random matrices with i.i.d .",
    "gaussian entries . by exploiting the low - dimensional structure",
    ", the number of linear measurements can be far smaller than the number of entries in the matrix to ensure stable recovery .",
    "it has been shown that a matrix @xmath4 of rank @xmath19 can be stably recovered by nuclear norm minimization with high probability , provided that @xmath20 @xcite .",
    "one major disadvantage of the gaussian ensemble design is that it requires @xmath21 bytes of storage space for @xmath6 , which can be excessively large for the recovery of large matrices .",
    "for example , at least @xmath22 tb of space is need to store the measurement matrices @xmath23 in order to ensure accurate reconstruction of 10,000@xmath2410,000 matrices of rank 10 .",
    "( see more discussion in section  [ simulationssec ] . )",
    "another popular design is the `` matrix completion '' model @xcite , under which the individual entries of the matrix @xmath4 are observed at randomly selected positions . in terms of the measurement matrices @xmath25 in ( [ eqm ] ) , this can be interpreted as @xmath26 where @xmath27 is the @xmath28th standard basis vector , and @xmath29 and @xmath30 are randomly and uniformly drawn with replacement from @xmath31 and @xmath32 , respectively .",
    "however , as pointed out in @xcite , additional structural assumptions , which are not intuitive and difficult to check , on the unknown matrix @xmath4 are needed in order to ensure stable recovery under the matrix completion model .",
    "for example , it is impossible to recover spiked matrices under the matrix completion model .",
    "this can be easily seen from a simple example where the matrix @xmath4 has only one nonzero row . in this case , although the matrix is only of rank one , it is not recoverable under the matrix completion model unless all the elements on the nonzero row are observed .    in this paper",
    ", we introduce a `` _ _ rank - one projection _ _ '' ( _ rop _ ) model for low - rank matrix recovery and propose a constrained nuclear norm minimization method for this model . under the rop model , we observe @xmath33 where @xmath34 and @xmath35 are random vectors with entries independently drawn from some distribution @xmath36 , and @xmath37 are random errors . in terms of the linear map @xmath38 in ( [ eqmodel ] ) , it can be defined as @xmath39_i = \\bigl ( \\beta^{(i)}\\bigr)^{\\intercal } a \\gamma^{(i ) } , \\qquad i=1 , \\ldots , n.\\ ] ] since the measurement matrices @xmath40 are of rank - one , we call the model ( [ eqrop - model ] ) a `` _ _ rank - one projection _ _ '' ( _ rop _ ) model .",
    "it is easy to see that the storage for the measurement vectors in the rop model ( [ eqrop - model ] ) is @xmath41 bytes which is significantly smaller than @xmath21 bytes required for the gaussian ensemble .",
    "we first establish a sufficient identifiability condition in section  [ gaussiansec ] by considering the problem of exact recovery of low - rank matrices in the noiseless case .",
    "it is shown that , with high probability , rop with @xmath42 random projections is sufficient to ensure exact recovery of all rank-@xmath19 matrices through the constrained nuclear norm minimization .",
    "the required number of measurements @xmath43 is rate optimal for any linear measurement model since a rank-@xmath19 matrix @xmath44 has the degree of freedom @xmath45 .",
    "the gaussian noise case is of particular interest in statistics .",
    "we propose a new constrained nuclear norm minimization estimator and investigate its theoretical and numerical properties in the gaussian noise case .",
    "both upper and lower bounds for the estimation accuracy under the frobenius norm loss are obtained .",
    "the estimator is shown to be rate - optimal when the number of rank - one projections satisfies either @xmath46 or @xmath47 .",
    "the lower bound also shows that if the number of measurements @xmath48 , then no estimator can recover rank-@xmath19 matrices consistently . the general case where the matrix @xmath4 is only approximately low - rank",
    "is also considered .",
    "the results show that the proposed estimator is adaptive to the rank @xmath19 and robust against small perturbations .",
    "extensions to the sub - gaussian design and sub - gaussian noise distribution are also considered .",
    "the rop model can be further simplified by taking @xmath49 if the low - rank matrix @xmath4 is known to be symmetric .",
    "this is the case in many applications , including low - dimensional euclidean embedding @xcite , phase retrieval @xcite and covariance matrix estimation @xcite .",
    "in such a setting , the rop design can be simplified to symmetric rank - one projections ( srop ) @xmath50_i = \\bigl(\\beta^{(i ) } \\bigr)^\\intercal a \\beta^{(i)}.\\ ] ] we will show that the results for the general rop model continue to hold for the srop model when @xmath4 is known to be symmetric .",
    "recovery of symmetric positive definite matrices in the noiseless and @xmath51-bounded noise settings has also been considered in a recent paper by chen et  al .",
    "@xcite which was posted on arxiv at the time of the writing of the present paper . their results and techniques for symmetric positive definite matrices are not applicable to the recovery of general low - rank matrices .",
    "see section  [ discussionssec ] for more discussions .",
    "the techniques and main results developed in the paper also have implications to other related statistical problems .",
    "in particular , the results imply that it is possible to accurately estimate a spiked covariance matrix based only on one - dimensional projections .",
    "spiked covariance matrix model has been well studied in the context of principal component analysis ( pca ) based on i.i.d .",
    "data where one observes @xmath52-dimensional vectors @xmath53 with @xmath54 and @xmath55 being low - rank @xcite . this covariance structure and its variations have been used in many applications including signal processing , financial econometrics , chemometrics and population genetics .",
    "see , for example , @xcite .",
    "suppose that the random vectors @xmath56 are not directly observable .",
    "instead , we observe only one - dimensional random projections of @xmath57 , @xmath58 where @xmath59 .",
    "it is somewhat surprising that it is still possible to accurately estimate the spiked covariance matrix @xmath60 based only on the one - dimensional projections @xmath61 .",
    "this covariance matrix recovery problem is also related to the recent literature on covariance sketching @xcite , which aims to recover a symmetric matrix @xmath4 ( or a general rectangular matrix @xmath62 ) from low - dimensional projections of the form @xmath63 ( or @xmath64 ) .",
    "see section  [ applicationssec ] for further discussions .",
    "the proposed methods can be efficiently implemented via convex programming .",
    "a simulation study is carried out to investigate the numerical performance of the proposed nuclear norm minimization estimators .",
    "the numerical results indicate that rop with @xmath65 random projections is sufficient to ensure the exact recovery of rank-@xmath19 matrices through constrained nuclear norm minimization and show that the procedure is robust against small perturbations , which confirm the theoretical results developed in the paper .",
    "the proposed estimator outperforms two other alternative procedures numerically in the noisy case .",
    "in addition , the proposed method is illustrated through an image compression example .",
    "the rest of the paper is organized as follows . in section",
    "[ gaussiansec ] , after introducing basic notation and definitions , we consider exact recovery of low - rank matrices in the noiseless case and establish a sufficient identifiability condition .",
    "a constrained nuclear norm minimization estimator is introduced for the gaussian noise case .",
    "both upper and lower bounds are obtained for estimation under the frobenius norm loss .",
    "section  [ extensionssec ] considers extensions to sub - gaussian design and sub - gaussian noise distributions .",
    "an application to estimation of spiked covariance matrices based on one - dimensional projections is discussed in detail in section  [ applicationssec ] .",
    "section  [ simulationssec ] investigates the numerical performance of the proposed procedure through a simulation study and an image compression example . a brief discussion is given in section  [ discussionssec ] .",
    "the main results are proved in section  [ proofssec ] and the proofs of some technical lemmas are given in the supplementary material @xcite .",
    "in this section , we first establish an identifiability condition for the rop model by considering exact recovery in the noiseless case , and then focus on low - rank matrix recovery in the gaussian noise case .",
    "we begin with the basic notation and definitions . for a vector @xmath66 , we use @xmath67{\\sum_{i=1}^n{\\vert}\\beta_i { \\vert}^q}$ ] to define its vector @xmath68-norm . for a matrix @xmath69 ,",
    "the frobenius norm is @xmath70 and the spectral norm @xmath71 is @xmath72 . for a linear map @xmath73 from @xmath74 to @xmath75 given by  ( [ eqm ] ) , its dual operator @xmath76 is defined as @xmath77 . for a matrix @xmath69 , let @xmath78 be the singular value decomposition of @xmath15 with the singular values @xmath79 .",
    "we define @xmath80 and @xmath81 . for any two sequences @xmath82 and @xmath83 of positive numbers , denote by @xmath84 when @xmath85 for some uniform constant @xmath86 and denote by @xmath87 if @xmath88 and @xmath89 .",
    "we use the phrase `` rank-@xmath19 matrices '' to refer to matrices of rank at most @xmath19 and denote by @xmath90 the set of all @xmath91 symmetric matrices .",
    "a linear map @xmath92 is called rop from distribution @xmath36 if @xmath6 is defined as in ( [ rop2 ] ) with all the entries of @xmath34 and @xmath35 independently drawn from the distribution @xmath36 .      an important step toward understanding the constrained nuclear norm",
    "minimization is the study of exact recovery of low - rank matrices in the noiseless case which also leads to a sufficient identifiability condition .",
    "a widely used framework in the low - rank matrix recovery literature is the restricted isometry property ( rip ) in the matrix setting .",
    "see @xcite . however , the rip framework is not well suited for the rop model and would lead to suboptimal results .",
    "see section  [ otherconditionssec ] for more discussions on the rip and other conditions used in the literature .",
    "see also @xcite . in this section ,",
    "we introduce a restricted uniform boundedness ( rub ) condition which will be shown to guarantee the exact recovery of low - rank matrices in the noiseless case and stable recovery in the noisy case through the constrained nuclear norm minimization .",
    "it will also be shown that the rub condition are satisfied by a range of random linear maps with high probability .",
    "[ dfl1bound ] for a linear map @xmath93 , if there exist uniform constants @xmath94 and @xmath95 such that for all nonzero rank-@xmath19 matrices @xmath2 @xmath96 where @xmath97 means the vector @xmath98 norm , then we say that @xmath6 satisfies the restricted uniform boundedness ( rub ) condition of order @xmath19 and constants @xmath94 and @xmath95 .",
    "in the noiseless case , we observe @xmath99 and estimate the matrix @xmath4 through the constrained nuclear norm minimization @xmath100 the following theorem shows that the rub condition guarantees the exact recovery of all rank-@xmath19 matrices .",
    "[ thl1boundrecovery ] let @xmath101 be an integer .",
    "suppose @xmath6 satisfies rub of order @xmath102 with @xmath103 , then the nuclear norm minimization method recovers all matrices .",
    "that is , for all rank-@xmath19 matrices @xmath4 and @xmath104 , we have @xmath105 , where @xmath106 is given by ( [ eqnuclearnormminimizationnoiseless ] ) .    theorem [ thl1boundrecovery ] shows that rub of order @xmath102 with @xmath103 is a sufficient identifiability condition for the low - rank matrix recovery model ( [ eqmodel ] ) in the noisy case .",
    "the following result shows that the rub condition is satisfied with high probability under the rop model with a sufficient number of measurements .",
    "[ thgaussianl1bound ] suppose @xmath107 is rop from the standard normal distribution . for integer @xmath108 ,",
    "positive numbers @xmath109 and @xmath110 , there exist constants @xmath86 and @xmath111 , not depending on @xmath112 and @xmath19 , such that if @xmath113 then with probability at least @xmath114 , @xmath6 satisfies rub of order @xmath102 and constants @xmath94 and @xmath115 .",
    "the condition @xmath116 on the number of measurements is indeed necessary for @xmath6 to satisfy nontrivial rub with @xmath117 .",
    "note that the degree of freedom of all rank-@xmath19 matrices of @xmath118 is @xmath119 .",
    "if @xmath120 , there must exist a nonzero rank-@xmath19 matrix @xmath2 such that @xmath121 , which leads to the failure of any nontrivial rub for @xmath6 .",
    "as a direct consequence of theorems [ thl1boundrecovery ] and [ thgaussianl1bound ] , rop with the number of measurements @xmath122 guarantees the exact recovery of all rank-@xmath19 matrices with high probability .    [ rmgaussiannoiseless ]",
    "suppose @xmath123 is rop from the standard normal distribution .",
    "there exist uniform constants @xmath86 and @xmath111 such that , whenever @xmath124 , the nuclear norm minimization estimator @xmath125 given in ( [ eqnuclearnormminimizationnoiseless ] ) recovers all rank-@xmath19 matrices @xmath2 exactly with probability at least @xmath114 .",
    "note that the required number of measurements @xmath126 above is rate optimal , since the degree of freedom for a matrix @xmath44 of rank @xmath19 is @xmath45 , and thus at least @xmath45 measurements are needed in order to recover @xmath4 exactly using any method .      we have shown that rub implies exact recovery in the noiseless and proved that the random rank - one projections satisfy rub with high probability whenever the number of measurements @xmath124 . as mentioned earlier , other conditions , including the restricted isometry property ( rip ) , rip in expectation and spherical section property ( ssp ) , have been introduced for low - rank matrix recovery based on linear measurements . among them , rip is perhaps the most widely used . a linear map @xmath127 is said to satisfy rip of order @xmath19 with positive constants @xmath94 and @xmath115 if @xmath128 for all rank-@xmath19 matrices @xmath4 .",
    "many results have been given for low - rank matrices under the rip framework .",
    "for example , recht et  al .",
    "@xcite showed that gaussian ensembles satisfy rip with high probability under certain conditions on the dimensions .",
    "cands and plan @xcite provided a lower bound and oracle inequality under the rip condition .",
    "cai and zhang @xcite established the sharp bounds for the rip conditions that guarantee accurate recovery of low - rank matrices .",
    "however , the rip framework is not suitable for the rop model considered in the present paper .",
    "the following lemma is proved in the supplementary material  @xcite .",
    "[ lmripfail ] suppose @xmath129 is rop from the standard normal distribution .",
    "let @xmath130 then for all @xmath131 , @xmath132 with probability at least @xmath133 .",
    "lemma [ lmripfail ] implies that at least @xmath134 number of measurements are needed in order to ensure that @xmath6 satisfies the rip condition that guarantees the recovery of only rank - one matrices .",
    "since @xmath134 is the degree of freedom for all matrices @xmath2 and it is the number of measurements needed to recover all @xmath8 matrices ( not just the low - rank matrices ) , lemma [ lmripfail ] shows that the rip framework is not suitable for the rop model . in comparison ,",
    "theorem [ thgaussianl1bound ] shows that if @xmath135 , then with high probability @xmath6 satisfies the rub condition of order @xmath19 with bounded @xmath136 , which ensures the exact recovery of all rank-@xmath19 matrices .",
    "the main technical reason for the failure of rip under the rop model is that rip requires an upper bound for @xmath137 where @xmath138 is a set containing low - rank matrices .",
    "the right - hand side of ( [ eqripfail ] ) involves the 4th power of the gaussian ( or sub - gaussian ) variables @xmath139 and @xmath140 .",
    "a much larger @xmath7 than the bound given in ( [ nlowerbd ] ) is needed in order for the linear map @xmath6 to satisfy the required rip condition , which would lead to suboptimal result .",
    "koltchinskii et  al .",
    "@xcite uses rip in expectation , which is a weaker condition than rip . a random linear map @xmath141 is said to satisfy rip in expectation of order @xmath19 with parameters @xmath142 and @xmath143 if @xmath144 for all rank-@xmath19 matrices @xmath2 .",
    "this condition was originally introduced by koltchinskii et  al .",
    "@xcite to prove an oracle inequality for the estimator they proposed and a minimax lower bound .",
    "the condition is not sufficiently strong to guarantee the exact recovery of rank-@xmath19 matrices in the noiseless case . to be more specific ,",
    "the bounds in theorems 1  and  2 in @xcite depend on @xmath145 , which might be nonzero even in the noiseless case .",
    "in fact , in the rop model considered in the present paper , we have @xmath146 which means rip in expectation is met for @xmath147 and @xmath148 for any number of measurements @xmath7 . however , as we discussed earlier in this section that at least @xmath126 measurements are needed to guarantee the model identifiability for recovery of all rank-@xmath19 matrices , we can see that rip in expectation can not ensure recovery .",
    "dvijotham and fazel @xcite and oymak et al .",
    "@xcite used a condition called the spherical section property ( ssp ) which focuses on the null space of @xmath6 .",
    "@xmath149 is said to satisfy @xmath150-ssp if for all @xmath151 , @xmath152 .",
    "dvijotham and fazel @xcite showed that if @xmath6 satisfies @xmath150-ssp , @xmath153 and @xmath154 , the nuclear norm minimization ( [ eqnuclearnormminimizationnoiseless ] ) recovers @xmath4 exactly in the noiseless case .",
    "however , the ssp condition is difficult to utilize in the rop framework since it is hard to characterize the matrices @xmath155 when @xmath156 is rank - one projections .",
    "we now turn to the gaussian noise case where @xmath157 in ( [ eqrop - model ] ) .",
    "we begin by introducing a constrained nuclear norm minimization estimator .",
    "define two sets @xmath158 where @xmath159 , and let @xmath160 note that both @xmath161 and @xmath162 are convex sets and so is @xmath163 .",
    "our estimator of @xmath4 is given  by @xmath164    the following theorem gives the rate of convergence for the estimator @xmath165 under the squared frobenius norm loss .",
    "[ thmixedestimator ] let @xmath6 be rop from the standard normal distribution and let @xmath166 .",
    "then there exist uniform constants @xmath86 , @xmath167 and @xmath111 such that , whenever @xmath168 , the estimator @xmath169 given in ( [ eqnuclearnormminimization2 ] ) satisfies @xmath170 for all rank-@xmath19 matrices @xmath4 , with probability at least @xmath171",
    ".    moreover , we have the following lower bound result for rop .    [ thlowerbound ]",
    "assume that @xmath6 is rop from the standard normal distribution and that @xmath166 .",
    "there exists a uniform constant @xmath86 such that , when @xmath172 , with probability at least @xmath173 , @xmath174\\label{eqlowerboundp }   \\\\[-8pt]\\nonumber & & \\qquad   \\geq1 - e^{-(p_1+p_2)r/64 } , \\\\",
    "\\label{eqlowerbounde } & & \\inf_{\\hat a}\\sup_{a\\in\\mathbb{r}^{p_1 \\times p_2}\\dvtx \\operatorname { rank}(a ) = r}e_z { \\vert}\\hat a- a{\\vert}_f^2 \\geq\\frac{\\sigma^2r(p_1 + p_2)}{4n},\\end{aligned}\\ ] ] where @xmath175 , and @xmath176 are the expectation and probability with respect to the distribution of @xmath3 .    when @xmath177 , then @xmath178    comparing theorems [ thmixedestimator ]  and  [ thlowerbound ] , our proposed estimator is rate optimal in the gaussian noise case when @xmath179 [ which is equivalent to @xmath180 or @xmath181 . since @xmath182 , this condition",
    "is also implied by @xmath183 .",
    "theorem [ thlowerbound ] also shows that no method can recover matrices of rank @xmath19 consistently if the number of measurements @xmath7 is smaller than @xmath184 .    the result in theorem [ thmixedestimator ]",
    "can also be extended to the more general case where the matrix of interest @xmath4 is only approximately low - rank .",
    "let @xmath185 .",
    "[ approx - low - rankprop ] under the assumptions of theorem [ thmixedestimator ] , there exist uniform constants @xmath86 , @xmath186 , @xmath187 and @xmath111 such that , whenever @xmath124 , the estimator @xmath169 given in ( [ eqnuclearnormminimization2 ] ) satisfies @xmath188\\\\[-8pt]\\nonumber & & { } + w_2\\frac{{\\vert}a_{-\\max(r)}{\\vert}_\\ast^2}{r}\\end{aligned}\\ ] ] for all matrices @xmath189 , with probability at least @xmath190 .    if the matrix @xmath4 is approximately of rank @xmath19 , then @xmath191 is small , and the estimator @xmath169 continues to perform well .",
    "this result shows that the constrained nuclear norm minimization estimator is adaptive to the rank @xmath19 and robust against perturbations of small amplitude .",
    "[ rmrademacher ] all the results remain true if the gaussian design is replaced by the rademacher design where entries of @xmath34 and @xmath35 are i.i.d .",
    "@xmath192 with probability @xmath193",
    ". more general sub - gaussian design case will be discussed in section  [ extensionssec ] .",
    "[ rmcomparsion ] the estimator @xmath169 we propose here is the minimizer of the nuclear norm under the constraint of the intersection of two convex sets @xmath194  and  @xmath195 .",
    "nuclear norm minimization under either one of the two constraints , called `` @xmath98  constraint nuclear norm minimization '' ( @xmath196 ) and `` matrix dantzig selector '' ( @xmath197 ) , has been studied before in various settings @xcite .",
    "our analysis indicates the following :    the @xmath98 constraint minimization performs better than the matrix dantzig selector for small @xmath7 ( @xmath198 ) when @xmath199 .",
    "the matrix dantzig selector outperforms the @xmath98 constraint minimization for large @xmath7 as the loss of the matrix dantzig selector decays at the rate @xmath200 .",
    "the proposed estimator @xmath169 combines the advantages of the two estimators .",
    "see section  [ simulationssec ] for a comparison of numerical performances of the three methods .      for applications such as low - dimensional euclidean embedding @xcite , phase retrieval @xcite and covariance matrix estimation @xcite",
    ", the low - rank matrix @xmath4 of interest is known to be symmetric .",
    "examples of such matrices include distance matrices , gram matrices , and covariance matrices .",
    "when the matrix @xmath4 is known to be symmetric , the rop design can be further simplified by taking @xmath201 .",
    "denote by @xmath202 the set of all @xmath91 symmetric matrices in @xmath203 .",
    "let @xmath204 , @xmath205 be independent @xmath52-dimensional random vectors with i.i.d .",
    "entries generated from some distribution @xmath36 . define a linear map @xmath206 by @xmath50_i = \\bigl(\\beta^{(i ) } \\bigr)^\\intercal a\\beta^{(i ) } , \\qquad i=1 , \\ldots , n.\\ ] ] we call such a linear map @xmath6 `` symmetric rank - one projections '' ( srop ) from the distribution @xmath36 .",
    "suppose we observe @xmath207 and wish to recover the symmetric matrix @xmath4 .",
    "as for the rop model , in the noiseless case we estimate @xmath4 under the srop model by @xmath208    [ prsymmetricnoiseless ] let @xmath6 be srop from the standard normal distribution .",
    "similar to corollary [ approx - low - rankprop ] , there exist uniform constants @xmath86 and @xmath111 such that , whenever @xmath209 , the nuclear norm minimization estimator @xmath106 given by ( [ eqnuclearminisymnoiseless ] ) recovers exactly all rank-@xmath19 symmetric matrices @xmath210 with probability at least @xmath114 .    for the noisy case , we propose a constraint nuclear norm minimization estimator similar to ( [ eqnuclearnormminimization2 ] ) .",
    "define the linear map @xmath211 by @xmath212_i = \\bigl [ \\mathcal{x}(a)\\bigr]_{2i-1 } - \\bigl[\\mathcal { x}(a)\\bigr]_{2i } , \\qquad i=1,\\ldots , \\biggl\\lfloor\\frac{n}{2}\\biggr\\rfloor\\ ] ] and define @xmath213 by @xmath214 based on the definition of @xmath215 , the dual map @xmath216 is @xmath217 let @xmath218 .",
    "the estimator @xmath169 of the matrix @xmath4 is given by @xmath219    an important property in the rop model considered in section  [ gaussianropsec ] is that @xmath220 , that is , @xmath221 for all the measurement matrices @xmath25 .",
    "however , under the srop model @xmath222 and so @xmath223 .",
    "the step of taking the pairwise differences in ( [ eqtildex ] ) and ( [ eqtildey ] ) is to ensure that @xmath224 .",
    "the following result is similar to the upper bound given in proposition [ approx - low - rankprop ] for  rop .",
    "[ prsymmetricmixedestimator ] let @xmath6 be srop from the standard normal distribution and let @xmath225 .",
    "there exist constants @xmath226 and @xmath111 such that , whenever @xmath227 , the estimator @xmath169 given in ( [ eqhatasymmetric ] ) satisfies @xmath228 for all matrices @xmath210 , with probability at least @xmath229 .",
    "in addition , we also have lower bounds for srop , which show that the proposed estimator is rate - optimal when @xmath230 or @xmath231 , and no estimator can recover a rank-@xmath19 matrix consistently if the number of measurements @xmath232 .    [ prsymmetriclowbound ]",
    "assume that @xmath6 is srop from the standard normal distribution and that @xmath233 .",
    "then there exists a uniform constant @xmath86 such that , when @xmath234 and @xmath235 , with probability at least , @xmath236 where @xmath169 is any estimator of @xmath4 , @xmath237 are the expectation and probability with respect to @xmath3 .    when @xmath238 and @xmath235 , then @xmath239",
    "we have focused on the gaussian design and gaussian noise distribution in section  [ gaussiansec ] .",
    "these results can be further extended to more general distributions . in this section",
    ", we consider the case where the rop design is from a symmetric sub - gaussian distribution @xmath36 and the errors @xmath37 are also from a sub - gaussian distribution .",
    "we say the distribution of a random variable @xmath240 is sub - gaussian with parameter @xmath241 if @xmath242 the following lemma provides a necessary and sufficient condition for symmetric sub - gaussian distributions .",
    "[ prsub - gaussian ] let @xmath36 be a symmetric distribution and let the random variable @xmath243 .",
    "define @xmath244 then the distribution @xmath36 is sub - gaussian if and only if @xmath245 is finite .    for the sub - gaussian rop design and sub - gaussian noise",
    ", we estimate the low - rank matrix @xmath4 by the estimator @xmath169 given in ( [ eqnuclearnormminimization ] ) with @xmath246\\\\[-8pt]\\nonumber & & { } \\cap \\bigl\\{z\\dvtx \\bigl{\\vert}\\mathcal{x}^\\ast(z)\\bigr{\\vert}\\leq6 \\alpha_{\\mathcal { p}}^2\\tau \\bigl(\\sqrt{6n(p_1+p_2 ) } + 2\\sqrt{\\log n}(p_1+p_2 ) \\bigr ) \\bigr\\},\\end{aligned}\\ ] ] where @xmath245 is given in ( [ eqalphap ] ) .",
    "[ thsubgaussiannoise ] suppose @xmath123 is rop from a symmetric and variance 1 sub - gaussian distribution @xmath247 .",
    "assume that @xmath37 are i.i.d .",
    "sub - gaussian with parameter @xmath241 and @xmath169 is given by ( [ eqnuclearnormminimization ] ) with @xmath248 defined in ( [ eqbgsub - gaussian ] ) .",
    "then there exist constants @xmath249 which only depend on @xmath36 , such that if @xmath250 , we have @xmath251\\\\[-8pt]\\nonumber & & { } + w_2\\frac{{\\vert}a_{-\\max(r)}{\\vert}_\\ast^2}{r}\\end{aligned}\\ ] ] with probability at least @xmath252",
    ".    an exact recovery result in the noiseless case for the sub - gaussian design follows directly from theorem [ thsubgaussiannoise ] . if @xmath253 , then , with high probability , all rank-@xmath19 matrices @xmath4 can be recovered exactly via the constrained nuclear minimization ( [ eqnuclearnormminimizationnoiseless ] ) whenever @xmath254 for some constant @xmath255 .    for the srop model considered in section  [ gaussiansropsec ] , we can similarly extend the results to the case of sub - gaussian design and sub - gaussian noise .",
    "suppose @xmath6 is srop from a symmetric variance 1 sub - gaussian distribution @xmath247 ( other than the rademacher @xmath2561 distribution ) and @xmath3 satisfies ( [ eqsub - guassiannoise ] ) .",
    "define the estimator of the low - rank matrix @xmath4 by @xmath257 where @xmath258 with @xmath259 some constant depending on @xmath36 .",
    "[ prexceptrademacher ] suppose @xmath260 is srop from a symmetric sub - gaussian distribution @xmath36 with variance 1 .",
    "also , assume that @xmath261 [ i.e. ,  @xmath262 where @xmath263 .",
    "let @xmath169 be given by ( [ eqhatasropsubgaussian ] ) . then there exist constants @xmath264 and @xmath111 which only depend on @xmath36 , such that for @xmath265 , @xmath266 with probability at least @xmath267 .    by restricting @xmath268 ,",
    "rademacher @xmath192 is the only symmetric and variance 1 distribution that has been excluded .",
    "the reason why therademacher @xmath192 distribution is an exception for the srop design is as follows . if @xmath34 are i.i.d .",
    "rademacher @xmath269 distributed , then @xmath50_i = \\bigl(\\beta^{(i ) } \\bigr)^\\intercal a\\beta^{(i ) } = \\sum_{j=1}^p a_{jj } + \\sum_{j\\neq k } \\beta_j^{(i ) } \\beta_k^{(i ) } a_{jk},\\qquad i = 1,\\ldots , n.\\ ] ] so the only information contained in @xmath270 about @xmath271 is @xmath272 , which makes it impossible to recover the whole matrix @xmath4 .",
    "in this section , we consider an interesting application of the methods and results developed in the previous sections to estimation of a spiked covariance matrix based on one - dimensional projections .",
    "as mentioned in the , spiked covariance matrix model has been used in a wide range of applications and it has been well studied in the context of pca based on i.i.d .",
    "data where one observes i.i.d .",
    "dimensional random vectors @xmath56 with mean 0 and covariance matrix @xmath60 , where @xmath54 and @xmath273 being low - rank .",
    "see , for example , @xcite . here , we consider estimation of @xmath273 ( or equivalently @xmath60 ) based only on one - dimensional random projections of @xmath57 . more specifically , suppose that the random vectors @xmath56 are not directly observable and instead we observe @xmath274 where @xmath59 .",
    "the goal is to recover @xmath273 from the projections @xmath275 .",
    "let @xmath276 with @xmath277 .",
    "note that @xmath278 and so @xmath279 .",
    "define a linear map @xmath280 by @xmath281_i = \\beta^{(i)\\intercal}a \\beta^{(i)}.\\ ] ] then @xmath282 can be formally written as @xmath283 where @xmath284 .",
    "we define the corresponding @xmath215 and  @xmath285 as in ( [ eqtildex ] ) and  ( [ eqtildey ] ) , respectively , and apply the constraint nuclear norm minimization to recover the low - rank matrix @xmath273 by @xmath286 the tuning parameters @xmath287 and @xmath288 are chosen as @xmath289 where @xmath290 , @xmath291 are constants .",
    "we have the following result on the estimator ( [ eqminimizationcovariance ] ) for spiked covariance matrix estimation .    [ thcovariance ]",
    "suppose @xmath292 , we observe @xmath293 , as in ( [ eqcovarianceobs ] ) , where @xmath294 and @xmath295 with @xmath54 and @xmath273 positive semidefinite and @xmath296 .",
    "let @xmath297 be given by ( [ eqminimizationcovariance ] ) .",
    "then there exist uniform constants @xmath86 , @xmath298 , @xmath111 such that when @xmath299 , @xmath300\\\\[-8pt]\\nonumber & & \\qquad \\leq c\\min \\biggl(\\frac{rp}{n}{\\vert}\\sigma { \\vert}_\\ast^2 + \\frac{r p^2\\log^4 n}{n^2 } \\bigl({\\vert}\\sigma { \\vert}_\\ast^2 + \\log^2 n { \\vert}\\sigma{\\vert}^2\\bigr ) , { \\vert}\\sigma{\\vert}_\\ast^2 \\biggr)\\end{aligned}\\ ] ] with probability at least @xmath301",
    ".    we have focused estimation of spiked covariance matrices on the setting where the random vectors @xmath57 are gaussian .",
    "similar to the discussion in section  [ extensionssec ] , the results given here can be extended to more general distributions under certain moment conditions .",
    "the problem considered in this section is related to the so - called covariance sketching problem considered in dasarathy et  al .",
    "@xcite . in covariance",
    "sketching , the goal is to estimate the covariance matrix of high - dimensional random vectors @xmath302 based on the low - dimensional projections @xmath303 where @xmath304 is a fixed @xmath305 projection matrix with @xmath306 .",
    "the main differences between the two settings are that the projection matrix in covariance sketch is the same for all @xmath57 and the dimension @xmath307 is still relatively large with @xmath308 for some @xmath309 . in our setting ,",
    "@xmath310 and @xmath304 is random and varies with @xmath28 . the techniques for solving the two problems are very different .",
    "comparing to @xcite , the results in this section indicate that there is a significant advantage to have different random projections for different random vectors @xmath57 as opposed to having the same projection for all @xmath57 .",
    "the constrained nuclear norm minimization methods can be efficiently implemented .",
    "the estimator @xmath169 proposed in section  [ gaussianropsec ] can be implemented by the following convex programming : @xmath311\\succeq0,\\qquad\\bigl{\\vert}y-\\mathcal{x}(a ) \\bigr{\\vert}_1 \\leq\\lambda _ 1 , \\\\ & \\qquad & \\bigl{\\vert}\\mathcal{x}^\\ast\\bigl(y-\\mathcal{x}(a)\\bigr)\\bigr{\\vert}\\leq \\lambda_2,\\nonumber\\end{aligned}\\ ] ] with optimization variables @xmath312 , @xmath2 .",
    "we use the cvx package @xcite to implement the proposed procedures . in this section",
    ", a simulation study is carried out to investigate the numerical performance of the proposed procedures for low - rank matrix recovery in various settings .",
    "we begin with the noiseless case . in this",
    "setting , theorem [ thgaussianl1bound ] and corollary  [ rmgaussiannoiseless ] show that the nuclear norm minimization recovers a rank @xmath19 matrix exactly whenever @xmath313 a similar result holds for the gaussian ensemble @xcite .",
    "however , the minimum constant @xmath86 that guarantees the exact recovery with high probability is not specified in either case .",
    "it is of practical interest to find the minimum constant @xmath86 . for this purpose ,",
    "we randomly generate @xmath8 rank-@xmath19 matrices @xmath4 as @xmath314 , where @xmath315 , @xmath316 are i.i.d .",
    "gaussian matrices .",
    "we compare rop from the standard gaussian distribution and the gaussian ensemble , with the number of measurements @xmath317 from a range of values of @xmath86 using the constrained nuclear norm minimization ( [ eqnuclearnormminimizationnoiseless ] ) .",
    "a recovery is considered successful if @xmath318 .",
    "figure  [ figrorm100 ] shows the rate of successful recovery when @xmath319 and @xmath320 .    , @xmath320 , and @xmath317 for @xmath86 ranging from 3 to 6 . ]",
    "the numerical results show that for rop from the gaussian distribution , the minimum constant @xmath86 to ensure exact recovery with high probability is slightly less than 5 in the small scale problems ( @xmath321 ) we tested .",
    "the corresponding minimum constant @xmath86 for the gaussian ensemble is about @xmath322 .",
    "matrix completion requires much larger number of measurements .",
    "based on the theoretical analyses given in @xcite , the required number of measurements for matrix completion is @xmath323 , where @xmath324 is some coherence constant describing the `` spikedness '' of the matrix @xmath4 . hence , for matrix completion , the factor @xmath86 in ( [ c ] ) needs to grow with the dimensions @xmath325 and @xmath326 and it requires @xmath327 , which is much larger than what is needed for the rop or gaussian ensemble .",
    "the required storage space for the gaussian ensemble is much greater than that for the rop . in order to ensure accurate recovery of @xmath91 matrices of rank @xmath19 ,",
    "one needs at least @xmath328 bytes of space to store the measurement matrices , which could be prohibitively large for the recovery of high - dimensional matrices .",
    "in contrast , the storage space for the projection vectors in rop is only @xmath329 bytes , which is far smaller than what is required by the gaussian ensemble in the high - dimensional case .",
    "we then consider the recovery of approximately low - rank matrices to investigate the robustness of the method against small perturbations . to this end",
    ", we randomly draw @xmath330 matrix @xmath4 as @xmath331 , where @xmath332 and @xmath333 are random matrices with orthonormal columns .",
    "we then observe @xmath334 random rank - one projections with the measurement vectors being i.i.d .",
    "gaussian . based on the observations ,",
    "the nuclear minimization procedure ( [ eqnuclearnormminimizationnoiseless ] ) is applied to estimate @xmath4 .",
    "the results for different values of @xmath19 are shown in figure  [ figapproxlowrank ] .",
    "it can be seen from the plot that in this setting one can exactly recover a matrix of rank at most 4 with 2000 measurements . however , when the rank @xmath19 of the true matrix @xmath4 exceeds 4 , the estimate is still stable . the theoretical result in proposition",
    "[ approx - low - rankprop ] bounds the loss ( solid line ) at @xmath335 ( shown in the dashed line ) with high probability , which corresponds to figure  [ figapproxlowrank ] .    , where @xmath336 , @xmath337 , @xmath338 .",
    "the dashed line is the theoretical upper bound . ]     and @xmath339 for @xmath340 , @xmath341 , @xmath342 , and @xmath7 ranging from 850 to 1200 .",
    "right panel : ratio of the squared frobenius norm loss of @xmath343 to that of the proposed estimator for @xmath340 , @xmath341 , and @xmath7 varying from 2000 to  15,000 . ]",
    "we now turn to the noisy case .",
    "the low - rank matrices @xmath4 are generated by @xmath344 , where @xmath315 and @xmath345 are i.i.d .",
    "gaussian matrices .",
    "the rop @xmath6 is from the standard gaussian distribution and the noise vector @xmath346 .",
    "based on @xmath5 with @xmath347 , we compare our proposed estimator @xmath169 with the @xmath98 constraint minimization estimator @xmath348 @xcite and the matrix dantzig selector @xmath339  @xcite , where @xmath349 with @xmath350 and @xmath351 .",
    "note that @xmath348 is similar to the estimator proposed in chen et  al .",
    "@xcite , except their estimator is for symmetric matrices under the srop but ours is for general low - rank matrices under the rop .",
    "figure  [ fig50comparison ] compares the performance of the three estimators .",
    "it can be seen from the left panel that for small @xmath7 , @xmath98  constrained minimization outperforms the matrix dantzig selector , while our estimator outperforms both @xmath348 and @xmath339 .",
    "when @xmath7 is large , our estimator and @xmath339 are essentially the same and both outperforms @xmath348 .",
    "the right panel of figure  [ fig50comparison ] plots the ratio of the squared frobenius norm loss of @xmath348 to that of our estimator",
    ". the ratio increases with @xmath7 .",
    "these numerical results are consistent with the observations made in remark [ rmcomparsion ] .",
    "we now turn to the recovery of symmetric low - rank matrices under the srop model ( [ eqsrop - model ] ) .",
    "let @xmath352 be srop from the standard normal distribution .",
    "we consider the setting where @xmath353 , @xmath7 varies from 50 to 600 , @xmath354 $ ] with @xmath3550.1 , 0.01 , 0.001 or 0.0001 , and @xmath4 is randomly generated as rank-5 matrix by the same procedure discussed above .",
    "the setting is identical to the one considered in section  5.1 of @xcite .",
    "although we can not exactly repeat the simulation study in @xcite as they did not specify the choice of the tuning parameter , we can implement both our procedure @xmath356 and the estimator @xmath348 with only the @xmath98 constraint which was proposed by chen et  al .",
    "@xcite @xmath357 the results are given in figure  [ fig40comparisonchen ] .",
    "it can be seen that our estimator @xmath169 outperforms the estimator @xmath348 .     with the @xmath348 .",
    "here @xmath353 , @xmath320 , @xmath358 and @xmath7 ranges from 50 to 800 . ]",
    "we have so far considered the estimators @xmath359 for the rop and srop , respectively .",
    "the theoretical choice of the tuning parameters @xmath360 and @xmath361 depends on the knowledge of the error distribution such as the variance . in real applications ,",
    "such information may not be available and/or the theoretical choice may not be the best .",
    "it is thus desirable to have a data driven choice of the tuning parameters .",
    "we now introduce a practical method for selecting the tuning parameters using @xmath362-fold cross - validation .",
    "let @xmath363 be the observed sample and let @xmath364 be a grid of positive real values",
    ". for each @xmath365 , set @xmath366\\\\[-12pt]\\nonumber & = & \\cases { \\bigl(t , t \\bigl(\\sqrt{\\log n}(p_1 + p_2 ) + \\sqrt{n(p_1 + p_2 ) } \\bigr ) \\bigr ) , & \\quad for rop ; \\vspace*{5pt}\\cr \\bigl(t , t ( \\sqrt{\\log n}p + \\sqrt{np } ) \\bigr ) , & \\quad for srop.}\\end{aligned}\\ ] ] randomly split the @xmath7 samples @xmath367 into two groups of sizes @xmath368 and @xmath369 for @xmath370 times .",
    "denote by @xmath371 the index sets for groups 1  and  2 , respectively , for the @xmath28th split .",
    "apply our procedure [ ( [ eqmethodrop ] ) for rop and ( [ eqmethodsrop ] ) for srop , resp . ] to the sub - samples in group  1 with the tuning parameters @xmath372 and denote the estimators by @xmath373 , @xmath374 .",
    "evaluate the prediction error of @xmath373 over the subsample in group  2 and set @xmath375 we select @xmath376 and choose the tuning parameters @xmath377 as in ( [ eqlambdaeta ] ) with @xmath378 and the final estimator @xmath169 based on ( [ eqmethodrop ] ) or ( [ eqmethodsrop ] ) with the chosen tuning parameters .    , @xmath379 , @xmath7 varies from 750 to 1400 .",
    "right panel : srop , @xmath380 , @xmath320 , @xmath7 varies from 50 to 800 . ]    we compare the numerical result by 5-fold cross - validation with the result based on the known @xmath381 by simulation in figure  [ figcv ] .",
    "both the rop and srop are considered .",
    "it can be seen that the estimator with the tuning parameters chosen through fold cross - validation has the same performance as or outperforms the one with the theoretical choice of the tuning parameters .      since a two - dimensional image can be considered as a matrix , one approach",
    "to image compression is by using low - rank matrix approximation via the singular value decomposition .",
    "see , for example , @xcite . here",
    ", we use an image recovery example to further illustrate the nuclear norm minimization method under the rop model .    for a grayscale image",
    ", let @xmath382 be the intensity matrix associated with the image , where @xmath383 is the grayscale intensity of the @xmath384 pixel .",
    "when the matrix @xmath4 is approximately low - rank , the rop model and nuclear norm minimization method can be used for image compression and recovery . to illustrate this point ,",
    "let us consider the following grayscale mit logo image ( figure  [ figmit - gray ] ) .",
    "the matrix associated with mit logo is of the size @xmath385 and of rank 6 .",
    "we take rank - one random projections @xmath270 as the observed sample , with various sample sizes .",
    "then the constrained nuclear norm minimization method is applied to reconstruct the original low - rank matrix .",
    "the recovery results are shown in figure  [ figmit - recovered ] .",
    "the results show that the original image can be compressed and recovered well via the rop model and the nuclear norm minimization .",
    "this paper introduces the rop model for the recovery of general low - rank matrices .",
    "a constrained nuclear norm minimization method is proposed and its theoretical and numerical properties are studied .",
    "the proposed estimator is shown to be rate - optimal when the number of rank - one projections @xmath179 or @xmath181 .",
    "it is also shown that the procedure is adaptive to the rank and robust against small perturbations . the method and results",
    "are applied to estimation of a spiked covariance matrix .",
    "it is somewhat unexpected that it is possible to accurately recover a spiked covariance matrix from only one - dimensional projections .",
    "an interesting open problem is to estimate the principal components / subspace based on the one - dimensional random projections .",
    "we leave this as future work .    in a recent paper , chen et  al .",
    "@xcite considered quadratic measurements for the recovery of symmetric positive definite matrices , which is similar to the special case of srop that we studied here .",
    "the paper was posted on arxiv as we finish writing the present paper .",
    "they considered the noiseless and @xmath98 bounded noise cases and introduced the so - called `` rip-@xmath386/@xmath98 '' condition .",
    "the `` rip-@xmath386/@xmath98 '' condition is similar to rub in our work .",
    "but these two conditions are not identical as the rip-@xmath387/@xmath98 condition can only be applied to symmetric low - rank matrices as only symmetric operators are considered in the paper .",
    "in contrast , rub applies to all low - rank matrices .",
    "chen et  al .",
    "( @xcite version 4 ) considered @xmath98-bounded noise case under the srop model and gave an upper bound in their theorem  3 ( after a slight change of notation ) @xmath388 this result for @xmath98 bounded noise case is not applicable to the i.i.d .",
    "random noise setting . when the entries of the noise term @xmath389 are of constant order , which is the typical case for i.i.d .",
    "noise with constant variance , one has @xmath390 with high probability .",
    "in such a case , the term @xmath391 on the right - hand side of ( [ ineqhatsigma - sigma ] ) does not even converge to 0 as the sample size @xmath392 .    in comparison , the bound ( [ eqapproxsyslowrankinequality ] ) in proposition [ prexceptrademacher ] can be equivalently rewritten  as @xmath393 where the first term @xmath394 is of the same order as @xmath395 in ( [ ineqhatsigma - sigma ] ) while the second term decays to 0 as @xmath392 .",
    "hence , for the recovery of rank-@xmath19 matrices , as the sample size @xmath7 increases our bound decays to 0 but the bound ( [ ineqhatsigma - sigma ] ) given in chen et  al .",
    "@xcite does not .",
    "the main reason of this phenomenon lies in the difference in the two methods : we use nuclear norm minimization under two convex constraints ( see remark [ rmcomparsion ] ) , but chen et  al .",
    "@xcite used only the @xmath98 constraint . both theoretical results ( see remark [ rmcomparsion ] ) and numerical results ( figure  [ fig50comparison ] in section  [ simulationssec ] ) show that the additional constraint @xmath162 improves the performance of the estimator",
    ".    moreover , the results and techniques in @xcite for symmetric positive definite matrices are not applicable to the recovery of general nonsymmetric matrices .",
    "this is due to the fact that for a nonsymmetric square matrix @xmath396 , the quadratic measurements @xmath397 satisfy @xmath398 where @xmath399 .",
    "hence , for a nonsymmetric matrix @xmath4 , only its symmetrized version @xmath400 can be possibly identified and estimated based on the quadratic measurements , the matrix @xmath4 itself is neither identifiable nor estimable .",
    "we prove the main results in this section .",
    "we begin by collecting a few important technical lemmas that will be used in the proofs of the main results .",
    "the proofs of some of these technical lemmas are involved and are postponed to the supplementary material @xcite .",
    "lemmas [ lmmgf ] and [ lmmean ] below are used for deriving the rub condition ( see definition [ dfl1bound ] ) from the rop design .",
    "[ lmmgf ] suppose @xmath189 is a fixed matrix and @xmath6 is rop from a symmetric sub - gaussian distribution @xmath36 , that is ,",
    "@xmath50_j = \\beta^{(j)t } a \\gamma^{(j)},\\qquad j=1,\\ldots , n,\\ ] ] where @xmath401 are random vectors with entries i.i.d . generated from @xmath36 .",
    "then for @xmath402 , we have @xmath403 with probability at least @xmath404 . here",
    ", @xmath405 is defined by ( [ eqalphap ] ) .",
    "[ lmmean ] suppose @xmath2 is a fixed matrix .",
    "@xmath406 are random vectors such that @xmath407 , where @xmath36 is some symmetric variance 1 sub - gaussian distribution , then we have @xmath408 where @xmath409 is given by ( [ eqalphap ] ) .",
    "let @xmath410 be i.i.d .",
    "sub - gaussian distributed . by measure concentration theory , @xmath411 , @xmath412 , are essentially bounded ; specifically , we have the following lemma .    [ lml1norm ]",
    "suppose @xmath413 and @xmath414 , we have @xmath415 more general , when @xmath37 are i.i.d .",
    "sub - gaussian distributed such that ( [ eqsub - guassiannoise ] ) holds , then @xmath416    lemma [ lmdsbound ] below presents an upper bound for the spectral norm of @xmath417 for a fixed vector @xmath3 .",
    "[ lmdsbound ] suppose @xmath6 is rop from some symmetric sub - gaussian distribution @xmath36 and @xmath413 is some fixed vector , then for @xmath418 , we have @xmath419 with probability at least @xmath420 . here , @xmath409 is defined by  ( [ eqalphap ] ) .",
    "we are now ready to prove the main results of the paper .",
    "we introduce the following two technical lemmas that will be used in the proof of theorem .",
    "the _ null space property _ below is a well - known result in affine rank minimization problem ( see @xcite ) .",
    "it provides a necessary , sufficient and easier - to - check condition for exact recovery in the noiseless setting .",
    "[ lmnullspace ] using ( [ eqnuclearnormminimizationnoiseless ] ) , one can recover all matrices @xmath4 of rank at most @xmath19 if and only if for all @xmath421 , @xmath422    the following lemma is given in @xcite , which provides a way to decompose the general vectors to sparse ones .",
    "[ lmsparsemean ] suppose @xmath423 is a nonnegative integer , @xmath424 and @xmath425",
    ". then @xmath426 , if and only if @xmath427 can be expressed as a weighted mean , @xmath428 where @xmath429 satisfies@xmath430\\\\[-8pt]\\nonumber & & { \\vert}u_i{\\vert}_1={\\vert}v{\\vert}_1 , \\qquad{\\vert}u_i{\\vert}_\\infty\\leq\\theta.\\end{aligned}\\ ] ]    for the proof of theorem [ thl1boundrecovery ] , by null space property ( lemma [ lmnullspace ] ) , we only need to show for all nonzero @xmath431 with @xmath432 , we must have @xmath433 .",
    "if this does not hold , suppose there exists nonzero @xmath431 with @xmath434 and @xmath435 .",
    "we denote @xmath436 and assume the singular value decomposition of @xmath431 is @xmath437 where @xmath429 , @xmath438 are orthogonal basis in @xmath439 , @xmath440 , respectively , and @xmath441 is the singular value vector such that @xmath442 . without loss of generality , we can assume @xmath443 , otherwise we can set the undefined entries of @xmath381 as @xmath444 .",
    "consider the singular value vector @xmath445 , we note that@xmath446 satisfies @xmath447 denote @xmath448 , by the two inequalities above we have @xmath449 and @xmath450 . now apply lemma  [ lmsparsemean ] , we can get @xmath451 such that @xmath452 , @xmath453 and @xmath454\\\\[-8pt]\\nonumber \\bigl{\\vert}b^{(i)}\\bigr{\\vert}_1&= & { \\vert}\\vec \\sigma_{-\\max(kr)}{\\vert}_1,\\qquad\\bigl{\\vert}b^{(i ) } \\bigr{\\vert}_\\infty\\leq\\theta,\\end{aligned}\\ ] ] which leads to @xmath455 if @xmath456 , we have @xmath457 if @xmath458 , we have @xmath459 since @xmath101 , we always have @xmath460 .",
    "finally , we define @xmath461 , then the rank of @xmath462 are all at most @xmath102 and @xmath463 and @xmath464 hence , @xmath465 here , we used the rub condition .",
    "the last inequality is due to @xmath466 and @xmath467 ( so @xmath468 ) .",
    "this is a contradiction , which completes the proof of the theorem .",
    "notice that for @xmath36 as standard gaussian distribution , the constant @xmath245 [ defined as ( [ eqalphap ] ) ] equals @xmath469 .",
    "we will prove the following more general result than theorem [ thgaussianl1bound ] instead .",
    "the proof is provided in the supplementary material @xcite .",
    "[ prsub - gaussianl1bound ] suppose @xmath107 is rop from some variance 1 symmetric sub - gaussian distribution @xmath247 . for integer @xmath101 , positive @xmath470",
    "[ @xmath409  is defined as ( [ eqalphap ] ) ] and @xmath110 , there exists constants @xmath86 and @xmath111 , only depending on @xmath471 but not on @xmath472 , such that if @xmath473 , then with probability at least @xmath114 , @xmath6 satisfies rub of order @xmath102 and constants @xmath94 and  @xmath115 .      in order to prove the result",
    ", we introduce the following technical lemma as an extension of null space property ( lemma [ lmnullspace ] ) from exact low - rank into the approximate low - rank setting .",
    "[ lmnullspaceapproximatelowrank ] suppose @xmath474 , @xmath475 . if @xmath476 , we have @xmath477    the following two lemmas described the separate effect of constraint @xmath478 and @xmath479 on the estimator .",
    "[ lmnoisyl1 ] suppose @xmath6 satisfies rub condition of order @xmath102 with constants @xmath480 such that @xmath481 .",
    "assume that @xmath482 satisfy @xmath476 , @xmath483 .",
    "then we have @xmath484    [ lmnoisyds ] suppose @xmath6 satisfies rub condition of order @xmath102 with constants @xmath480 such that @xmath481 .",
    "assume that @xmath485 satisfies @xmath486 .",
    "then we have @xmath487    the proof of lemmas [ lmnullspaceapproximatelowrank ] , [ lmnoisyl1 ] and [ lmnoisyds ] are listed in the supplementary material @xcite .",
    "now we prove theorem [ thmixedestimator ] and proposition [ approx - low - rankprop ] .",
    "we only need to prove proposition [ approx - low - rankprop ] since theorem [ thmixedestimator ] is a special case of proposition [ approx - low - rankprop ] . by lemmas [ lml1norm ]  and  [ lmdsbound ] ,",
    "we have @xmath488 here , @xmath489 ( @xmath176 or @xmath490 ) means the probability with respect to @xmath6 [ @xmath3 or @xmath491 .",
    "hence , we have @xmath492 under the event that @xmath493 , @xmath4 is in the feasible set of the programming ( [ eqnuclearnormminimization2 ] ) , which implies @xmath494 by the definition of @xmath169 .",
    "moreover , we have @xmath495 on the other hand , suppose @xmath496 , by theorem [ thgaussianl1bound ] , we can have find a uniform constant @xmath86 and @xmath111 such that if @xmath497 , @xmath6 satisfies rub of order @xmath498 and constants @xmath499 with probability at least @xmath500 .",
    "hence , we have @xmath501 and @xmath502 such that if @xmath503 , @xmath6 satisfies rub of order @xmath498 and constants @xmath480 satisfying @xmath504 with probability at least @xmath500 .    now under the event that :    @xmath6 satisfies rub of order @xmath498 and constants @xmath505 satisfying @xmath506 ,    @xmath493 ,    apply lemmas [ lmnoisyl1 ]  and  [ lmnoisyds ] with @xmath507 , we can get ( [ eqapproxlowrankinequality ] ) .",
    "the probability that these two events both happen is at least @xmath508 .",
    "set @xmath509 , we finished the proof of proposition [ approx - low - rankprop ] .    for theorem [ thsubgaussiannoise ] ,",
    "the proof is similar .",
    "we apply the latter part of lemmas  [ lml1norm ]  and  [ lmdsbound ] and get @xmath510 besides , we choose @xmath511 , then we can find @xmath512 and @xmath513 such that @xmath514 .",
    "apply proposition [ prsub - gaussianl1bound ] , there exists @xmath515 only depending on @xmath36 , @xmath480 such that if @xmath516 , @xmath6 satisfies rub of order @xmath102 with constants @xmath94 and @xmath115 with probability at least @xmath517 .",
    "note that @xmath480 only depends on @xmath36 , we can conclude that there exist constants @xmath518 only depending on @xmath36 such that if @xmath519 , @xmath6 satisfies rub of order @xmath102 with constants @xmath505 satisfying @xmath520 .",
    "similarly , to the proof of proposition [ approx - low - rankprop ] , under the event that :    @xmath6 satisfies rub of order @xmath102 and constants @xmath505 satisfying @xmath466 ,    @xmath493 ,    we can get ( [ eqsubgaussiannoiseinequality ] ) ( we shall note that @xmath186 depends on @xmath36 , so its value can also depend on @xmath245 ) .",
    "the probability that those events happen is at least @xmath521 for @xmath522 .      without loss of generality",
    ", we assume that .",
    "we consider the class of rank-@xmath19 matrices @xmath523 namely the matrices with all nonzero entries in the first @xmath19 rows .",
    "the model ( [ eqmodel ] ) become @xmath524 where @xmath525 is the vector of the first to the @xmath19th entries of @xmath34 .",
    "note that this is a linear regression model with variable @xmath526 , by lemma 3.11 in @xcite , we have @xmath527 , \\\\",
    "\\inf_{\\hat a}\\sup_{a\\in\\mathcal{f}_c } e\\bigl{\\vert}\\hat a(y ) - a\\bigr{\\vert}_f^2 & = & \\infty\\qquad\\mbox{when $ \\mathcal{x}_r^\\ast\\mathcal{x}_r$ is singular,}\\end{aligned}\\ ] ] where @xmath528 is the @xmath6 constrained on @xmath529 , then @xmath530 sends @xmath531 to @xmath532 . when @xmath533 , @xmath534 is singular",
    ", hence we have  ( [ eqlowerboundinfty ] ) .",
    "when @xmath535 , we can see in order to show ( [ eqlowerbounde ] ) , we only need to show @xmath536 with probability at least @xmath537 .",
    "suppose the singular value of @xmath534 are @xmath538 , @xmath539 , then @xmath540 .",
    "suppose @xmath6 is rop while @xmath541 is i.i.d .",
    "standard gaussian random matrix ( both @xmath6 and @xmath542 are random ) . then by some calculation",
    ", we can see @xmath543 note ( 0.20 ) in the proof of lemma [ lmmgf ] in the supplementary material @xcite , we know @xmath544 .",
    "hence , @xmath545 besides , @xmath546 hence , @xmath547 then by chebyshev s inequality , we have @xmath548 with probability at least @xmath549 . by cauchy  schwarz s inequality , we have @xmath550 therefore , we have @xmath551 with probability at least @xmath552 , which shows ( [ eqlowerbounde ] ) .",
    "finally , we consider ( [ eqlowerboundp ] ) .",
    "suppose inequality ( [ eqineqsigmai^2x ] ) holds , then @xmath553 by lemma 3.12 in @xcite , we know @xmath554 where @xmath555 is the indicator function .",
    "note that when @xmath556 , @xmath557 is identical distributed as @xmath558 , where @xmath559 , hence , @xmath560 the last inequality is due to the tail bound of @xmath561 distribution given by lemma 1 in  @xcite ; the second last inequality is due to ( [ eqlowerboundnumber ] ) . in summary ,",
    "when ( [ eqineqsigmai^2x ] ) holds , we have @xmath562 finally , since @xmath563 , we showed that with probability at least @xmath537 , @xmath6 satisfies ( [ eqlowerboundp ] ) .",
    "we first introduce the following lemma about the upper bound of @xmath564 .",
    "[ lmcovarianceerrorbound ] suppose @xmath3 is defined as ( [ eqyandz ] ) , then for constants @xmath565 , @xmath566 , we have @xmath567\\\\[-8pt]\\nonumber p \\biggl(\\frac{c_1}{n}\\sum_{i=1}^n \\xi_{i}^2 \\leq m_1c_1{\\vert}\\sigma{\\vert}_\\ast \\biggr)&\\geq & 1 - \\frac{9}{n(m_1 - 1)^2};\\end{aligned}\\ ] ] for constants @xmath568 , @xmath569 , @xmath570\\\\[-8pt]\\nonumber p \\biggl(\\frac{c_2 ^ 2\\sum_{i=1}^n\\xi_{i}^4}{n } \\leq m_2c_2 ^ 2 { \\vert}\\sigma",
    "{ \\vert}_\\ast^2 \\biggr)&\\geq & 1 - \\frac{105 ^ 2}{n(m_1 - 9)^2};\\end{aligned}\\ ] ] for constants",
    "@xmath571 , @xmath572 , @xmath573    the proof of lemma [ lmcovarianceerrorbound ] is listed in the supplementary material @xcite .",
    "the rest of the proof is basically the same as proposition [ prsymmetricmixedestimator ] .",
    "suppose @xmath574 and @xmath575 are given by ( 0.36 ) , ( 0.37 ) and ( 0.39 ) in the supplementary material @xcite , then @xmath576 , @xmath577 are rop . by lemma [ lmdsbound ] , @xmath578 with probability at least @xmath579 .",
    "hence , there exists @xmath402 such that @xmath580 here , we used the fact that @xmath581 , @xmath582 similarly to the proof of proposition [ prsymmetricmixedestimator ] , since @xmath576 is rop , there exists constants @xmath298 and @xmath502 such that if @xmath299 , @xmath583 satisfies rub of order @xmath584 with constants @xmath480 satisfying @xmath506 with probability at least @xmath585",
    ".    now under the event that :    @xmath4 is feasible in ( [ eqminimizationcovariance ] ) ,    @xmath576 satisfies rub of order @xmath584 with constants @xmath480 satisfying @xmath506 ,    the latter part of ( [ eqcovarianceinequalityl1 ] ) , ( [ eqcovarianceinequalityl2 ] ) and ( [ eqcovarianceinequalityinfty ] ) hold for some @xmath566 , @xmath569 , @xmath586 ,    we can prove ( [ eqcovarianceineq ] ) similarly as the proof of proposition [ prsymmetricmixedestimator ] , which we omit the proof here .",
    "we thank the associate editor and the referees for their thorough and useful comments which have helped to improve the presentation of the paper ."
  ],
  "abstract_text": [
    "<S> estimation of low - rank matrices is of significant interest in a range of contemporary applications . in this paper </S>",
    "<S> , we introduce a rank - one projection model for low - rank matrix recovery and propose a constrained nuclear norm minimization method for stable recovery of low - rank matrices in the noisy case . </S>",
    "<S> the procedure is adaptive to the rank and robust against small perturbations . both upper and lower bounds for the estimation accuracy under the frobenius norm loss </S>",
    "<S> are obtained . </S>",
    "<S> the proposed estimator is shown to be rate - optimal under certain conditions . </S>",
    "<S> the estimator is easy to implement via convex programming and performs well numerically .    </S>",
    "<S> the techniques and main results developed in the paper also have implications to other related statistical problems . </S>",
    "<S> an application to estimation of spiked covariance matrices from one - dimensional random projections is considered . </S>",
    "<S> the results demonstrate that it is still possible to accurately estimate the covariance matrix of a high - dimensional distribution based only on one - dimensional projections . </S>"
  ]
}