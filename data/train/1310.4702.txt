{
  "article_text": [
    "the reliability of storage systems is usually a foremost concern to implementers and users .",
    "data loss events can be extremely costly , consider for example the value of data in systems storing financial or medical records .",
    "for this reason , storage systems must be engineered such that the chance of an irrecoverable data loss is extremely low , perhaps on the order of one chance in million per year of operation .",
    "these immensely high reliabilities , however , prohibit real - world testing due to the fact that it would require a huge number of these systems to be evaluated for an extremely long time to empirically measure with any degree of accuracy .",
    "therefore , implementers of storage systems must rely on mathematical models for gauging the reliability of their designs .",
    "it is important that the model used neither over- nor under - estimate reliability .",
    "if the model overestimates reliability , then the system will be more prone to data loss than expected .",
    "if the model underestimates reliability , the system will be designed with an excessive level of fault - tolerance and therefore be overly expensive .",
    "we evaluated two models used in reliability analysis , one presented by chen et al.@xcite and another given by angus@xcite .",
    "we found that while the chen and angus methods agree for systems with a fault - tolerance of zero or one , beyond that , the models diverge by a factorial of the system s fault - tolerance .",
    "this paper is organized as follows : in the _ background _ section , we introduce basic concepts in reliability analysis and define the meaning of notation used throughout this paper .",
    "next we introduce two models used in reliability analysis , one presented by chen et al . which is commonly used in the analysis of data storage systems and a more general model presented by angus for analyzing the reliability of @xmath1-of-@xmath2 systems . to judge the applicability of these models we show simulations of how long systems take to fail and discuss the results",
    "following that , we derive a new model which the simulation shows to have superior accuracy to both the chen and angus models .",
    "lastly , we mention some areas for further improvement to the model we give .",
    "the term _ reliability _ , as used in this paper , refers to the probability of correct operation over a given period of time , where correct operation is defined as the absence of an irrecoverable data loss .",
    "gibson@xcite showed that for systems with a constant failure rate , the following exponential function may be used to estimate reliability over time @xmath3 given the _",
    "mean - time - to - data - loss _ ( @xmath0 ) of the system : @xmath4 . knowing this function , the main difficulty in estimating reliability becomes accurately estimating the system s @xmath0 .    in this analysis , it is important to distinguish between a data loss and an _ irrecoverable _ data loss .",
    "hard drives and tapes inevitably fail and these are instances of data loss . however ,",
    "storage systems are capable of recovering from such failures so long as the number of failures does not exceed the system s fault - tolerance . when the number of failures exceeds a system s fault - tolerance , data can no longer be read and therefore lost data can no longer be recovered .",
    "the rate at which individual components ( hard drives , tapes , etc . ) fail is denoted as @xmath5 while the rate at which those components are repaired is @xmath6 .",
    "alternatively , these two rates might instead be expressed as times : _ mean - time - to - failure _ ( @xmath7 ) and _ mean - time - to - repair _ ( @xmath8 ) respectively .",
    "when @xmath5 and @xmath6 are constant over time , meaning exponentially distributed , @xmath9 and @xmath10 . throughout this paper @xmath7 and @xmath8 will only be used to refer to the @xmath7 and @xmath8 of components , never the system .    to increase a system s",
    "@xmath0 the @xmath7 of components should be as long as possible while the @xmath8 should be as short as possible .",
    "while implementers of storage systems can choose the components to use , once selected , they have little control over the @xmath7 or @xmath8 .",
    "@xmath7 is usually defined by the manufacturer of the component and little to nothing can be done to increase it .",
    "@xmath8 , on the other hand , may be improved to an extent .",
    "@xmath8 is the sum of the service time and rebuild time . by using hot spares , the service time",
    "may be reduced to near zero and by prioritizing rebuild i / o over normal i / o requests , rebuild time may be minimized . for storage devices , however , rebuild time has an ultimate floor defined by the i / o rate of the device .",
    "consider that if a 1 tb disk fails , rebuilding it requires writing 1 tb of data . if the i / o rate of the disk is 100 mb / s it will take a minimum of 2.9 hours to recover .",
    "given the limited control implementers have over @xmath7 and @xmath8 the most important consideration for reaching a target @xmath0 is choosing an appropriate level of fault - tolerance for the system .",
    "the simplest method for achieving fault - tolerance in a storage system is by using replication .",
    "that is , create some number ( @xmath2 ) of copies of the data and store each copy to a different component .",
    "a replicated data storage system can tolerate the failure of @xmath11 components , so long as one copy remains , the failed copies can be remade . in this respect , implementers have complete control over the fault - tolerance , and by extension , the reliability of the systems they create .",
    "achieving fault - tolerance by making replicated copies , however , is very inefficient .",
    "other more advanced methods are known for achieving fault - tolerance , such as raid 5 , raid 6 , and erasure codes . unlike copy - based systems which require 1 of the @xmath2 components to remain operational ,",
    "these systems require @xmath1 of @xmath2 components to remain operational , where @xmath12 with @xmath13 being fault - tolerance .    with an accurate model for estimating @xmath0 , implementers may engineer systems to meet the reliability requirements using a minimum level of fault - tolerance . by minimizing fault - tolerance",
    "the storage system will be more efficient as less redundant information need be stored or calculated .",
    "all reliability models presented in this paper are capable of estimating @xmath0 from the four metrics : @xmath1 , @xmath2 , @xmath7 and @xmath8 .",
    "one might ask , what level of fault - tolerance is sufficient for any practical purpose ? unfortunately , no answer remains true indefinitely .",
    "consider that as disk capacities have grown , their performance has not kept pace . this has resulted in a very large increase in disk repair times . whereas it took 57 seconds to read an entire 40 mb disk in 1991",
    ", it takes 3.3 hours on a modern 750 gb drive@xcite . to cope with these longer repair times ,",
    "systems have had to increase their level of fault - tolerance .",
    "raid 5 was sufficient when disks could be rebuilt in minutes ; now raid 6 is required to keep an acceptable reliability as rebuilds take hours or days .",
    "another factor causing the fault - tolerance of storage systems to increase is the sheer size of storage systems being built today .",
    "the resulting decrease in reliability as storage capacity increases is linear ; with all else being equal , a system storing 2 pb of data on 2,000 disks has twice the chance of experiencing data loss as a system storing 1 pb on 1,000 disks@xcite . storing more data",
    "inherently carries a higher risk of loss , and with storage requirements doubling every 24 months@xcite the reliability of all that data is halved every 24 months .",
    "lastly , disk failures do not always manifest as complete operational failures .",
    "elerath@xcite showed that a more common path to data lass is via latent failures , caused by improper writes or degradation of the media over time .",
    "latent failures , more commonly known as _",
    "unrecoverable read errors _ ( ures ) result when a drive is unable to correctly read some sector .",
    "the rate at which ures manifest is generally reported to be between @xmath14 and @xmath15 per bit read .",
    "this means that even if the ure rate remains constant , as disk sizes grow the likelihood of encountering a ure will increase .",
    "consider a raid 6 ( 8 + 2 ) array composed of 1 tb disks .",
    "after two disk failures all eight of the remaining disks must be read perfectly without error . with a ure rate of @xmath14 , the chance of being able to read this amount of data without error is given by : @xmath16    this means that about half the time , the system will encounter a ure during rebuild and therefore experience data loss .",
    "even though raid 6 can supposedly recover from double disk failures , factoring in ures one finds that half the time it can not .",
    "therefore the true reliability of this raid 6 array is only marginally better than a system with a fault - tolerance of one .",
    "the net result is that when considering increasing disk capacities , larger storage systems , and the growing risk of ures , one may conclude that increasing levels of fault - tolerance will be required in the future to simply maintain the same level of reliability .",
    "therefore , it is important that the reliability model used by storage system implementers be able to accurately model highly - fault tolerant systems .",
    "chen et al . presented models for estimating the mttdl for various raid configurations@xcite , including raid 0 ( no parity ) , raid 5 ( single parity ) and",
    "raid 6 ( dual parity ) . as founders in the field of raid ,",
    "their model has seen wide adoption by those in the storage industry . in the paper ,",
    "zero redundancy raid 0 systems are said to have a mttdl equal to the mttf of individual disks divided by the number of disks :    @xmath17    they further presented models for raid 5 ,    @xmath18    and raid 6 arrays :    @xmath19    a clear pattern emerges in the progression of increased fault - tolerance .",
    "looking at the above formulas , one sees that the mttr term is taken to the power of the fault - tolerance ( @xmath13 ) while the mttf is taken to the power of @xmath20 . to account for the multiplication of @xmath21 ... @xmath22 we may use the factorial operator to find : @xmath23 , recalling that @xmath12 . therefore we may obtain a generalization of the chen model which works for any arbitrary @xmath1 and @xmath2 :    @xmath24    it is straightforward to see that when @xmath25 , one obtains the raid 0 formula . if one sets @xmath26 or @xmath27 , one derives the raid 5 or raid 6 formulas respectively .",
    "j.  e.  angus published a paper titled `` on computing mtbf for a k - out - of - n : g repairable system''@xcite .",
    "his model is more general than those given by chen et al .",
    "but nonetheless each may be used to estimate time to failure for data storage system .",
    "there is , however , a difference between what the angus and chen models attempt to calculate .",
    "the chen model calculates mttdl which if we stated more generally , is the _ mean - time - to - first - failure _ ( mttff ) .",
    "this is an important distinction because after the first failure in a data storage system , the system can not be repaired because data is lost .",
    "the angus model does not assume this , and therefore allows repair from cases where more than ( @xmath28 ) devices have failed .",
    "this is why angus defines the result as _ mean - time - between - failures _ ( mtbf ) rather than mttff , his model finds the average amount of time between failures over an infinite amount of time . in many situations , the mttff will be very close to the mtbf , and in those cases the angus model may be used to accurately model mttdl . later in this paper , we will explore the conditions under which this assumption is not valid .",
    "below is the formula angus gave in his paper .",
    "note that this is as it appeared in the original notation , where he used @xmath5 and @xmath6 instead of mttf and mttr :    @xmath29    if we substitue @xmath5 and @xmath6 with the notation used by chen , the angus formula becomes :    @xmath30    note that for cases when @xmath31 , as is normally the case for disk drives , the summation component of the formula rapidly converges to zero .",
    "given that for most cases , mttf will be a time in years and the mttr a time in hours , the ratio of @xmath32 will be in the thousands for typical cases and therefore , if iterations beyond @xmath33 are ignored , the result will only deviate by a few thousandths .",
    "this level of accuracy is acceptable for most purposes , and therefore when @xmath31 one may simplify the angus formula as :    @xmath34    this formula looks very similar to the one given by chen .",
    "each has @xmath35 in the numerator , and @xmath36 in the denominator .",
    "where they differ is in their treatment of the @xmath1 and @xmath2 terms .",
    "chen gives :    @xmath37    while angus gives :    @xmath38    by decomposing the @xmath39 term used in the angus model one obtains :    @xmath40    the only difference between the simplified angus model and the chen model is that there is an extra term in the numerator of @xmath41 . therefore the mttdl predicted by chen s model will be a factorial of the fault - tolerance times less than the mttdl predicted by angus s model . for raid 0 , and raid 5 systems , @xmath42 and @xmath43 are both @xmath44 , so no difference is observed between their predictions . however , for raid 6 systems with a fault tolerance of 2 , chen s model will yield a mttdl one half of what angus will give .",
    "this difference is arguably minor , but what about for highly fault - tolerant systems that are now becoming possible via erasure codes ?",
    "it was shown in the introduction that increased levels of fault - tolerance will be required for very large storage systems .",
    "systems have already been developed@xcite which can support much higher levels of fault - tolerance .",
    "one such system has a standard configuration of 10-of-16 . with a fault tolerance of 6",
    ", the chen and angus predictions will differ by a factor of @xmath45 ( 720 ) when estimating this system s mttdl .",
    "why is this so , and which prediction is right ?    to see why these models give different predictions , it helps to look at how the models were derived .    _ _    _ _    the angus model explicitly states the assumption that there are unlimited repairmen .",
    "this means that whether 1 device or 100 fail simultaneously , each failed device will be repaired at a constant rate .",
    "the chen model , on the other hand , appears to assume the per - device repair rate is inversely proportional to the number of failed disks",
    ". there is , however , no fundamental reason why this should be the case , as each drive has its own independent i / o resources . if two disks are simultaneously being rebuilt , the rebuild process may write to both of the disks at twice the rate it could write to a single failed disk .",
    "another possible consideration made in the chen model is that because each disk has a fixed repair time , the first disk to be repaired will necessarily be the first disk to have failed .",
    "the speed at which the first disk is repaired does not increase according to the number of failed disks , so why should the repair rate increase ? if anything is clear , it is that these models can not both be correct . to attempt to verify the validity of one of these two models , a monte carlo simulation of system failure time was created",
    "the goal of the simulation is to model the failures and repairs of @xmath2 independent devices until such time that more than @xmath28 devices are simultaneously in a failed state .",
    "failures are random and exponentially distributed over time according to the mttf . repairs for each device take a constant amount of time equal to mttr , such that mttr time after a device s failure it will be operational .      .... random_ttf ( )    return mttf * -ln(random(0,1 ) )    initialize ( )    fail_times : = fail_time[n ]    for each fail_time in fail_times :      fail_time : = random_ttf ( )    count_failures(start , end )    count : = 0    for each fail_time in fail_times :      if ( start < = fail_time < = end ) :        count : = count + 1    return count    simulate_time_to_data_loss ( )    while true :      nf : = min(fail_times )      count : = count_failures(nf , nf+mttr )      if count > n - k :        dl : = time of ( n - k+1)-th failure        break      nf : = nf + mttr + random_ttf ( )    return dl ....    the first method , _ random_ttf ( ) _ generates a random failure time for a device given the device mttf . because the simulation assumes failures are exponentially distributed , the negative of the natural logarithm of a random value between 0 and 1 produces a multiplier for the device mttf .",
    "this function is used by the _ initialize ( ) _ method to assign random failure times to each of the @xmath2 devices .",
    "what the main simulation loop does is first check the time of the next failure , then it counts the number of failures occurring in the range from that failure time until the time that device is repaired .",
    "if the number of failures exceeds the fault - tolerance of the system , the loop breaks , and the time of the failure that pushes the system above its fault - tolerance is returned .",
    "otherwise , the failure time for the device is advanced by adding the mttr and another random time to failure .",
    "when the loop continues , the same process will be run for the next failing device .",
    "it is important to note that a full run of this simulation returns only one random time to data loss . to derive an accurate estimation of the _ mean _ time to data loss",
    ", this simulation must be run over many thousands of iterations to find the arithmetic average of all the results .",
    "the average of the results should give a close approximation of the system s true mttdl .",
    "we conducted various runs of the simulation , using different values of @xmath2 , @xmath1 , @xmath7 , and @xmath8 .",
    "it was found that the magnitude of @xmath8 was immaterial to the result of the simulation , only the ratio of @xmath7 to @xmath8 is important .",
    "therefore , for each of the results reported in the tables below , @xmath8 is assumed to have a value of 1 . each observed result is the average of at least 2,000 iterations of the above simulation code .",
    "note that for the highly fault - tolerant configurations , the mttf had to be reduced for the simulation to complete within a reasonable period of time .    in this table , _ predicted _ refers to the chen model :    [ cols=\">,>,>,>,>,>\",options=\"header \" , ]     therefore we find that this model yields results which are much closer to those of the simulation . in the instances where the angus model was off by a factor of 3.77 ,",
    "this model was within 1% .",
    "one may wonder how common it is for the @xmath32 ratio to be so low .",
    "pris and long presented a method for predicting reliability in the face of batch - correlated failures@xcite . in it , they suggest that to model the manifestation of a batch - correlated failure , one should reduce the @xmath7 of disks in the system to something much lower than it would be normally , suggesting a time between one week and one month might be reasonable .",
    "in such a case , the @xmath32 ratio could be as low as 3.5 .",
    "another reason to expect the ratio to drop is that in the past 15 years , the time it takes to read an entire hard drive has increased by over 200 times .",
    "this means the lower bound on rebuild time , and therefore the minimum mttr has likewise increased by this amount .",
    "a similar decrease over the next 15 years would see the ratio drop from the thousands to around 10 to 20 .",
    "with @xmath31 this new formula may be simplified by keeping only the biggest contributors to the summation .",
    "the biggest contributor occurs for @xmath46 . evaluating only this case",
    ", the formula reduces to :    @xmath47    through some simple transformations this formula may be reweritten as :    @xmath48    which is identical to the simplified angus model .",
    "therefore when these two models are expected to produce very similar results when @xmath31 , because in that case the less significant contributors to the summation converge rapidly .",
    "we have demonstrated that a common model for estimating @xmath0 due to disk failures grossly underestimates the true @xmath0 for systems that have a high degree of fault - tolerance .",
    "furthermore , we showed that a model presented by angus provides accurate estimations of mttdl for systems with a high degree of fault - tolerance so long as @xmath31 .    while the angus model is more complicated than the one presented by chen et al .",
    ", a simplified version of angus can be used which deviates by only a few percent for reasonable @xmath32 ratios . for",
    "more less constrained situations , we presented a model derived through markov theory which exhibits a high degree of accuracy for cases of small @xmath32 ratios and showed the common relationship holds with the angus model .",
    "our main result is that while the chen model is adequate for systems with a fault tolerance of 0 or 1 , it should not be used for systems with a fault - tolerance beyond that .",
    "therefore to accurately model raid 6 , triple- ( or higher ) replication , or erasure code systems , the angus formula , or its simplified version ought to be used . when modeling a system whose @xmath32 ratio is less than a few hundred , one should use the model presented in this paper over the angus method .",
    "there is much room for further investigating and improvement to the method presented in this paper .",
    "some candidates for further research include : modeling of correlated failures , using non - exponentially distributed failures for system components , and investigating the true likelihood of unrecoverable read errors in light of our findings .",
    "the models presented in this paper all assume failures are statistically independent events , but much research has been done to refute this assumption in practice@xcite . in the paper by chen et al . , a simple method for modeling correlated disk failures was presented@xcite . what their model prescribed was to assume the mttf for the second disk failure was 1/10th what it was for the first failure , and further assume that every subsequent disk failure is 10 times more likely than the last .",
    "this provides reasonable results for a raid 5 or raid 6 system which can only tolerate one or two failures .",
    "for raid 6 , at worst the third disk failure will only be 100 times more likely to fail than the first .",
    "however , consider a system that could tolerate 5 disk failures .",
    "the 6th disk failure would be modeled to have a mttf 1/10,000th what it would be normally . if the @xmath32 ratio is less than 10,000 , adding increasing levels of fault - tolerance actually decreases the mttdl predicted through this method .",
    "while it is usually better to underestimate mttdl than overestimate it , clearly this method reaches a breaking point if adding additional fault - tolerance causes the estimated mttdl to decrease rather than increase . to blindly follow this method s predictions , one would design a system that is less reliable than what he or she might otherwise choose .",
    "therefore , developing a method for modeling correlated failures in highly fault - tolerant systems would be quite beneficial .",
    "the models in this paper assume constant failure rates over time for the underlying components .",
    "gibson and schroeder showed that in practice , disk failure rates follow a bathtub curve@xcite with higher levels of failure initially , stabilization during normal operating life , and slowly increasing with age .",
    "they further found that the weibull distribution could be used to provide a reasonable approximation of failure rates of hard drives over their useful lives .",
    "it remains an open question how our reliability model might be amended to accommodate for hard drives with a non - exponentially distributed failure rate .      our conjecture for why the chen method was off for higher levels of fault - tolerance was that it fails to account for progress made in the rebuild of the first disk to have failed .",
    "this consideration should also alter the expectation of encountering a ure .",
    "since on average , the first @xmath49 portion of the disks will have already been read in rebuilding the first failed disk by the time the system experiences @xmath13 failures .",
    "a ure causing irrecoverable data loss , however , must happen on the part of the disks that remains to be read to rebuild the first failed disk .",
    "therefore , this consideration should reduce the expected likelihood of a ure causing irecoverable data loss . exploring",
    "exactly how the estimated mttdl is affected remains to be explored , but we expected it to have a non - negligible effect for systems with a high degree of fault - tolerance .",
    "we owe a special thanks to yura volvovskiy who offered methodology for the calculation of _ mean - time - to - first - failure _ and solving it in general case",
    ". this result would not be possible without his contribution .",
    "we would like to take this opportunity to thank our fellow colleagues at cleversafe for their insightful feedback and advice regarding this paper , and in particular andrew baptist who offered invaluable advice regarding the simulation methodology and sanjaya kumar for his insightful advice and feedback regarding this paper .",
    "pris , j. and long , d. e. , `` using device diversity to protect data against batch - correlated disk failures '' .",
    "proceedings of the second acm workshop on storage security and survivability , alexandria , virgina ( 2006 ) , p. 47",
    "- 52 .",
    "plank , j.s .",
    ", xu , l. , `` optimizing cauchy reed - solomon codes for fault - tolerant network storage applications '' , _ nca-06 : 5th ieee international symposium on network computing applications _ cambridge , ma , ( 2006 ) .",
    "b.  schroeder , and g.  a.  gibson , `` disk failures in the real world : what does",
    "an mttf of 1,000,000 hours mean to you ? '' .",
    "proceedings of the 2007 usenix technical conference , san jose , ca , feb 14 - 16 , 2007 ."
  ],
  "abstract_text": [
    "<S> we found that a reliability model commonly used to estimate _ mean - time - to - data - loss _ </S>",
    "<S> ( @xmath0 ) , while suitable for modeling raid 0 and raid 5 , fails to accurately model systems having a fault - tolerance greater than 1 . </S>",
    "<S> therefore , to model the reliability of raid 6 , triple - replication , or @xmath1-of-@xmath2 systems requires an alternate technique . in this paper </S>",
    "<S> , we explore some alternatives , and evaluate their efficacy by comparing their predictions to simulations . </S>",
    "<S> our main result is a new formula which more accurately models storage system reliability . </S>"
  ]
}