{
  "article_text": [
    "in decision theory an agent faces an environment @xmath0 from the environment set @xmath1 and has to decide which action @xmath2 to take from the action set @xmath3 .",
    "the desirability of the action in that particular environment is quantified by the utility function @xmath4 .",
    "the objective of the decision - maker is to maximize the utility depending on the environment @xmath0 , that is @xmath5 when the number of possible actions is vast , the maximization problem for any environment becomes computationally infeasible because the decision - maker must evaluate the utility of every possible action and choose the best one .",
    "this raises the need of extending classic decision - theory to take into account the available computational resources .",
    "here we consider decision - makers with a prior distribution @xmath6 over the search space and with limited computational resources .",
    "we measure computational resources in terms of an `` information distance '' , namely the relative entropy @xmath7 between the prior behaviour @xmath8 and a new behaviour @xmath9 after deliberation .",
    "this information - processing cost can be related to physical inefficiencies in thermodynamics  @xcite .",
    "for every environment @xmath0 the decision problem can be phrased as a constrained maximization problem with @xmath10 , where the decision - maker can only afford to spend a maximum number of bits @xmath11 to change the prior behavior @xmath8 to the new behavior @xmath9 , such that @xmath12 where @xmath13 is the lagrange multiplier and serves as a resource parameter that trades off utility and computational cost .",
    "for @xmath14 we recover classic decision - theory , and for @xmath15 the decision - maker has no resources and acts according to the prior",
    ". we can extend the maximization problem in eq .   to also maximize over the prior distribution in average over all environments @xmath16\\ ] ] this objective function",
    "is commonly known as the rate - distortion objective first stated by c. shannon .",
    "the solution to this problem that gives the optimal prior @xmath17 is the following set of two analytic self - consistent equations @xmath18 where @xmath19 is the partition sum . in practice",
    ", the solution can be computed using the blahut - arimoto algorithm that basically iterates through both equations until convergence .",
    "however , every iteration includes the computation of the partition sum @xmath20 that can potentially be infeasible because it involves a function evaluation over all possible actions in the set @xmath21 which could be very large .",
    "another inconvenience is that the blahut - arimoto algorithm can only be applied directly when the action space is discrete .",
    "here we propose a sampling - based approach that avoids the computation of partition sums and that can also be applied to continuous action spaces . in order to achieve this",
    "we require a parametrized family of distributions @xmath22 to allow for a gradient ascent update rule . replacing @xmath17 in eq .  - by @xmath22 and inserting into eq .  , we can rewrite the rate - distortion objective @xmath23 as @xmath24 where @xmath25 and @xmath26 .",
    "the gradient with respect to @xmath27 is @xmath28 where in the first row we have used the equality @xmath29 , and in the second row we used the definition of the parametric conditional @xmath30 . due to the double expectation in eq .  , we can now approximate the gradient stochastically by single samples @xmath31 and @xmath32 and evaluating @xmath33 .",
    "the parameter updates are computed as usual with @xmath34 where @xmath35 is the learning rate .",
    "[ [ sampling ] ] sampling + + + + + + + +    for the approximation @xmath33 the decision - maker needs to generate a sample from @xmath36 after being given the sample @xmath37 from the environment .",
    "this can be achieved , for example , by a rejection sampling scheme .",
    "the rejection sampling algorithm takes samples from the proposal distribution @xmath22 and accepts or rejects them according to the acceptance - rejection rule @xmath38 where @xmath39 $ ] is sampled from a uniform distribution and the aspiration level of the decision - maker for the environment @xmath0 is @xmath40 .",
    "if the actions @xmath2 correspond to accepted samples , it can be shown that the decision - maker effectively generates samples from the conditional @xmath41 .",
    "the average number of samples required for acceptance given a particular @xmath0 is @xmath42 that is greater than the exponential of the relative entropy  connecting sampling complexity with the information - theoretic costs .",
    "the average number of required samples across all environments is naturally @xmath43",
    "while the proposed algorithm works for both continuous and discrete parametric distributions , here we demonstrate its performance in simulations for the discrete case , because this allows comparing the solutions to the optimal blahut - arimoto solutions .",
    "additionally , these simulations illustrate the impact of @xmath44 on learning and plateau performance and on the average number of samples required .",
    "[ [ the - parametric - prior ] ] the parametric prior + + + + + + + + + + + + + + + + + + + +    when using gradient methods in parameter space , it has to be made sure that the parameters do not violate given constraints  for example in the case of a discrete distribution over @xmath45 outcomes with parameters @xmath46 $ ] , the constraints @xmath47 and @xmath48 must be satisfied . often the constraints can be naturally satisfied by reparameterizing the distribution . in the discrete case",
    "we may define actions for example as vectors of the form @xmath49 $ ] with only one dimension having value @xmath50 and the rest @xmath51 for all @xmath52 where @xmath53",
    ". then we can reparameterize the discrete distribution as @xmath54 with the new parameter vector @xmath55 $ ] and where @xmath56 . the gradient @xmath57 from eq .",
    "can then be computed as @xmath58 note that with this reparameterization @xmath59 the parameters @xmath60 can vary across all reals , thereby naturally satisfying the constraints for @xmath61 .",
    "[ [ simulations-1 ] ] simulations + + + + + + + + + + +    in fig .",
    "[ fig : results ] we show the evolution of a bounded rational decision - maker that updates the prior @xmath62 according to eq .   and uses the rejection sampling scheme from eq .   to sample from the conditionals @xmath63 .",
    "for illustration we have chosen @xmath64 and @xmath65 and a utility function @xmath66 $ ] where the utility values for every @xmath2 and @xmath0 are sampled randomly between @xmath67 and @xmath68 .",
    "the first panel shows how the sampling - based solutions converge to the optimal blahut - arimoto solutions , as the relative entropy between the parameterized prior @xmath62 and the optimal prior ( the solution to eq . )",
    "is being reduced towards zero .",
    "it can be seen that the more rational decision - maker ( higher @xmath44 ) takes longer to converge . in the second panel ,",
    "we show the average number of samples according to eq . .",
    "it can be seen that for all decision - makers the number of required samples reduces over time as the prior gets closer to the optimal prior , indicating that the optimal solution is actually reducing the average information - theoretic distance between prior and conditionals .",
    "it can also be seen that more rational decision - makers require more samples . in the third panel ,",
    "we show how the average utility @xmath69 improves over time with a better prior .",
    "finally , the fourth panel shows how the prior is shaped by the rationality parameter @xmath44 . in a highly rational decision - maker",
    "the prior reflects an ( more ) evenly spread distribution between all the actions that are optimal in any of the environments . in a decision - maker with low rationality",
    "the prior will concentrate on the small set of actions that achieve on average a high utility across environments , because these decision - makers do not have resources to process environment - specific information .     and different @xmath44 for @xmath70 different environments and @xmath71 possible actions .",
    "at every iteration we sample an environment @xmath31 and take an accepted action @xmath72 according to the rule of eq .  [ eq : rejection_sampling ] . from left to right",
    "we show the relative entropy between the optimal solution and @xmath62 , the average number of samples needed for acceptance , the average utility and finally the shape of the prior after convergence . ]",
    "here we extend the information - theoretic model for bounded rational decision - making to allow for adaptation of the prior .",
    "this naturally leads to two time scales for information - processing .",
    "the first process is a slow process regarding the update of the prior distribution .",
    "we have shown that such an update can be estimated efficiently from samples and that it converges to the optimal solution obtained from rate distortion theory .",
    "the second process is the sampling process that a decision - maker with fixed prior would use to process information specific for a particular environment . a decision - maker with low rationality can be thought of as having no time for planning being forced to act according to the prior distribution that might not be a very good solution for the observed environment but that is the best possible prior given its computational resources .",
    "thus , this approach might not only provide a normative model for human decision - making processes occurring on different time scales , but also an efficient sampling - based solution for the rate distortion problem ."
  ],
  "abstract_text": [
    "<S> deviations from rational decision - making due to limited computational resources have been studied in the field of bounded rationality , originally proposed by herbert simon  @xcite . </S>",
    "<S> there have been a number of different approaches to model bounded rationality  @xcite ranging from optimality principles @xcite to heuristics  @xcite . here </S>",
    "<S> we take an information - theoretic approach to bounded rationality  @xcite , where information - processing costs are measured by the relative entropy between a posterior decision strategy and a given fixed prior strategy . in the case of multiple environments </S>",
    "<S> , it can be shown that there is an optimal prior rendering the bounded rationality problem equivalent to the rate distortion problem for lossy compression in information theory @xcite . </S>",
    "<S> accordingly , the optimal prior and posterior strategies can be computed by the well - known blahut - arimoto algorithm which requires the computation of partition sums over all possible outcomes and can not be applied straightforwardly to continuous problems . here </S>",
    "<S> we derive a sampling - based alternative update rule for the adaptation of prior behaviors of decision - makers and we show convergence to the optimal prior predicted by rate distortion theory . </S>",
    "<S> importantly , the update rule avoids typical infeasible operations such as the computation of partition sums . </S>",
    "<S> we show in simulations a proof of concept for discrete action and environment domains . </S>",
    "<S> this approach is not only interesting as a generic computational method , but might also provide a more realistic model of human decision - making processes occurring on a fast and a slow time scale . </S>"
  ]
}