{
  "article_text": [
    "as known to all , shannon s information theory deals mainly with the representation and transmission of information . in the development of both source and channel coding theorems , especially for their converses , fano s inequality serves as the key tool @xcite .",
    "@xmath1 and @xmath2 are two random variables following @xmath3 @xmath4 and @xmath5 .",
    "define @xmath6 , then @xmath7 where @xmath8 is the cardinality of @xmath1 and @xmath2 , @xmath9 is the binary entropy function @xmath10 $ ] for @xmath11 .",
    "thus fano inequality can be further relaxed by @xmath12 .",
    "usually , the left hand side of ( [ eq : fano ] ) is referred to as the equivocation , which is quantified by the conditional entropy .",
    "particularly , it represents the uncertainty whether the restored / decoded message @xmath2 is the same as the original one , i.e. , @xmath1 . on the other hand ,",
    "the right hand side implies the reliability of the source / channel coding in terms of a function of error probability @xmath13 .",
    "it was shown in @xcite that vanishing equivocation implies vanishing error probability . however , vanishing error probability does not necessarily guarantee a vanishing equivocation , especially for some @xmath14 of countably infinite alphabet .    in proving the converse of coding theorems ,",
    "one wants to find the upper bound on the size of the codebook given arbitrary code length and error probability .",
    "the following theorem is an immediate inference of fano s inequality , simple but useful .",
    "[ th : fano_ixy ] @xcite suppose @xmath1 and @xmath2 are two random variables that take values on the same finite set with cardinality @xmath15 and at least one of them is equiprobable .",
    "then the mutual information between them satisfies @xmath16 where @xmath6 and @xmath17 @xmath18 $ ] .    as an inference of this result",
    ", the following theorem gives an upper bound on the size of a code as a function of the average error probability .",
    "[ th : fano_converse ] @xcite every @xmath19-code ( average probability of error ) for a random transformation @xmath20 satisfies @xmath21 where @xmath22 , @xmath10 $ ] .",
    "although simple , these two theorems are insightful and easy to compute both in theory and numerically .",
    "the classical coding theorems are mainly based on the asymptotic equipartition property ( aep ) and typical / joint - typical decoder @xcite .",
    "however , applications of aep requires infinite long codewords , where the error probability goes either to 0 or 1 as the code length goes to infinity .",
    "although these coding theorems provide fundamental limits for modern communications , research on the finite blocklength coding schemes are more important in engineering applications . given the block length , upper bounds on the achievable error probability and the achievable code size were obtained in @xcite .",
    "most importantly , a tight approximation for the achievable maximal rate given the error probability and code length was presented .    in this paper",
    ", we consider the entropy of one random variable vector conditioned on another , and the corresponding probability of error in guessing one from the other , by proposing an extended fano s inequality .",
    "the extended fano s equality has better performance by taking advantage of a more careful consideration on the error patterns .",
    "it suits codings in the finite blocklength regime better and is useful in bounding the mutual information between random vectors , and the codebook size given the block length and average symbol error probability constraint .    in the following part of this paper",
    ", we present the extended fano s inequality in section [ sec : gener_fano ] first .",
    "the lower bounds on the mutual information between two random variable vectors and a upper bound on the codebook size given the block length and error probability are given in section [ sec : converse ] .",
    "an application of the the obtained result to the @xmath0-ary symmetric channels ( qsc ) are presented in section [ sec : app ] , which shows that the extended fano s inequality is tight for such channels . finally , we concluded the paper in section [ sec : conclusion ] . throughout the paper , vectors indicated by bold",
    "although fano s inequality has been used widely in the past few years , it can be improved by treating the error events more carefully . in this section ,",
    "a refinement of fano s inequality is presented , which is tighter and more applicable for finite blocklength coding design .",
    "fano s inequality extension[th : gner_fano ]    suppose that @xmath23 and @xmath24 @xmath25 are two @xmath26-dimension random vectors where @xmath27 and @xmath28 @xmath29 take values on the same finite set @xmath30 with cardinality @xmath31 .",
    "then the conditional entropy satisfies @xmath32 where @xmath33 is the discrete entropy function .",
    "@xmath34 is the error distribution , where the error probabilities are @xmath35 for @xmath36 .",
    "@xmath37 is the generalized hamming distance , defined as the number of symbols in @xmath38 that are different from the corresponding symbol in @xmath39 .",
    "define the error random variable as @xmath40 if @xmath41 for @xmath42 .",
    "according to the chain rule of the joint entropy , @xmath43 can be expressed in the following ways ,    [ dr : fano_g ] @xmath44    particularly , in ( [ dr : fano_g].a ) , it is clear that @xmath45 .",
    "then we have @xmath46 where ( a ) follows the fact that entropy increases if its condition is removed , i.e. , @xmath47 .",
    "particularly , we have @xmath48 .    according to its definition",
    ", we have @xmath49    when considering @xmath50 , we know that there are @xmath51 disaccord symbol pairs between @xmath38 and @xmath39 . for each fixed @xmath52 , every symbol in @xmath38 which belongs to a disaccord pair",
    "has @xmath53 possible choices except the one in @xmath54 .",
    "thus @xmath55 has @xmath56 choices .",
    "besides , there @xmath57 selections for the positions of error symbols for each given @xmath51 .",
    "therefore , the total number of possible codeword @xmath38 is @xmath58 , which means @xmath59    particularly , note that @xmath60 since there is no uncertainty in determining @xmath38 from @xmath39 , if they are the same .",
    "then , ( [ dr : condi_err ] ) can be written as @xmath61    by combining ( [ dr : hx_y ] ) and ( [ hey ] ) , the proof of the theorem is completed .",
    "in fact , the error distribution @xmath62 is easy to calculate , especially for some special channels .",
    "for example , the discrete @xmath0-ary symmetric channels is shown in fig [ fig : qbc ] . in this situation , @xmath63 .",
    "-ary symmetric channel , width=240 ]    it is clear that theorem [ th : gner_fano ] is a generalization of fano s inequality .",
    "specifically , when the block length is 1 , i.e. , @xmath64 , we have @xmath65 , @xmath66 , and @xmath67 .",
    "in this case , theorem [ th : gner_fano ] reduces to @xmath68 which is exactly the same as fano s inequality .    as a variant of theorem [ th : gner_fano ] ,",
    "the following theorem presents the conditional entropy in terms of relative entropy .",
    "[ th : gner_fanorela ] suppose that @xmath23 and @xmath69 are two @xmath26-dimension random vectors where @xmath27 and @xmath28 @xmath29 take values on the same finite set @xmath30 with cardinality @xmath31 .",
    "then the conditional entropy satisfies @xmath70 where @xmath71 is the discrete relative entropy function .",
    "the error probabilities are @xmath35 for @xmath36 .",
    "donate @xmath34 , and @xmath72 is a probability distribution with @xmath73    firstly , we know from the binomial theorem that @xmath74 is a probability distribution .",
    "let @xmath75 and @xmath76 , we know that @xmath77 is a probability distribution where @xmath73 .    according to theorem [ th : gner_fano ] , @xmath78 where ( a ) holds because @xmath79 for @xmath80 .    by the definition of @xmath62 , it is clear that it reflects the error performance of the channel and is totaly determined by the channel itself . on the contrary",
    ", @xmath77 is a distribution where each error patten is assumed to appear equiprobablely . in this situation ,",
    "the probability that there are @xmath51 error symbols in the codeword is @xmath81 .",
    "thus @xmath82 is the distance between the actual error pattern distribution and the uniform error pattern distribution .",
    "particularly , if the channel is an error free one , i.e. , @xmath83 and @xmath84 for @xmath85 , we have @xmath86 . according to theorem [ th : gner_fanorela ] ,",
    "we get @xmath87 , which is reasonable with the assumption on the channel . in this sense ,",
    "theorem [ th : gner_fanorela ] is tight .",
    "the fano s inequality has been playing an important role in the history of information theory because it built a close connection between conditional entropy and error probability . for extended fano s inequality given in theorem [",
    "th : gner_fano ] , it is especially applicable in finite blocklength coding .",
    "it also presents the relationship between conditional entropy and error probabilities , which are defined as follows .",
    "_ block error probability _",
    "@xmath88 is the average error probability of a block ( codeword ) , i.e. , @xmath89 .",
    "then we have @xmath90 .",
    "thus , we have @xmath91 for any @xmath92 .",
    "_ symbol error probability _",
    "@xmath93 is the average error probability of a symbol , i.e. , @xmath94 , which can be expressed by @xmath95 .    in many communication systems , especially those using error correction channel codings ,",
    "a block error does nt imply a system failure . on the contrary ,",
    "the error can be corrected or part of the block can still be used with some performance degradation . in this case , the symbol error is more useful than the block error .    particularly , a corollary following our result as shown below will answer this problem .",
    "[ cor:1 ] suppose that @xmath23 and @xmath69 are two @xmath26-dimension random vectors where @xmath27 and @xmath28 take values on the same finite set @xmath30 with cardinality @xmath31 .",
    "then the conditional entropy satisfies @xmath96 where @xmath97 is a probability distribution with @xmath98 .",
    "according to theorem [ th : gner_fano ] , one has@xmath99    since the distribution @xmath100 can be expressed as @xmath101 , which is a binomial distribution with the symbol error probability of @xmath102 .",
    "@xmath103 is a measure of the distance between the error probability distribution and the binomial distribution with parameter @xmath102 .",
    "if one takes @xmath64 , corollary [ cor:1 ] will reduces to @xmath104 , which is a frequently used form of fano s inequality .",
    "it is seen that the extended fano s inequality builds a natural connection between conditional entropy and symbol error and is especially applicable for finite length codings .",
    "based on the proposed generalized fano s inequality , the following lower bounds on the mutual information between @xmath38 and @xmath39 can be obtained .",
    "[ th : gner_fano_ixy ] suppose that @xmath23 and @xmath69 are two @xmath26-dimension random vectors that satisfy the following .    1 .   @xmath27 and @xmath28 @xmath29 take values on the same finite set @xmath30 with cardinality @xmath31 .",
    "either @xmath38 or @xmath39 is equiprobable .",
    "the error probabilities are @xmath35 for @xmath36 .",
    "donate the error distribution as @xmath34 .",
    "then the mutual information between @xmath38 and @xmath39 satisfies @xmath105 where @xmath33 is the entropy function .",
    "if @xmath38 is equiprobable , @xmath106 .    on the other hand , the mutual information is given by @xmath107 .",
    "together with theorem [ th : gner_fano ] , @xmath108    note that @xmath38 and @xmath39 are totally symmetric in ( [ dr : gener_ixy1 ] ) .",
    "therefore , if @xmath39 is assumed to be equiprobable at the beginning of the proof , one can get the same result .",
    "thus theorem [ th : gner_fano_ixy ] is proved .    by using theorem [ th : gner_fanorela ] ,",
    "the mutual information between @xmath38 and @xmath39 can be bounded by the following corollary .",
    "[ cor:4 ] suppose that @xmath23 and @xmath69 are two @xmath26-dimension random vectors that satisfy the following .    1 .",
    "@xmath27 and @xmath28 @xmath29 take values on the same finite set @xmath30 with cardinality @xmath31 .",
    "either @xmath38 or @xmath39 is equiprobable .",
    "3 .   the error probabilities are @xmath35 and @xmath34 .",
    "then the mutual information between @xmath38 and @xmath39 satisfies @xmath109 where @xmath71 is the discrete relative entropy function and @xmath72 is a probability distribution with @xmath73    the distribution @xmath77 means that the symbol in @xmath39 takes any value on @xmath110 with equal probability , regardless of what is sent in @xmath38 .",
    "so it is a pure random distribution when @xmath1 and @xmath2 are independent from each other .",
    "the most desirable coding scheme is that its error distribution @xmath62 is farthermost from @xmath77 , which also ensures a larger coding rate .",
    "suppose @xmath30 is a finite alphabet with cardinality @xmath31 .",
    "let s consider the input and output alphabets @xmath111 with @xmath112 and a channel to be a sequence of conditional probabilities @xcite @xmath113 .",
    "we donate a codebook with @xmath15 codewords by @xmath114 .",
    "a decoder is a random transformation @xmath115 where @xmath116 indicates that the decoder choose error .",
    "if messages are equiprobable , the average error probability is defined as @xmath117 .",
    "an codebook with @xmath15 codewords and a decoder whose average probability of error is smaller than @xmath118 are called an @xmath119-code .    an upper bound on the size of a code as a function of the average probability of symbol error follows the corollary [ cor:1 ] .",
    "[ th : conver_1 ] every @xmath119-code for a random transformation @xmath120 satisfies @xmath121 where @xmath34 is the error distribution with @xmath35 for @xmath36 , @xmath122 @xmath123 is a probability distribution with @xmath98 .",
    "since the messages are equiprobable , we have @xmath124 . according corollary [ cor:1 ] , @xmath125    solving @xmath126 from ( [ dr : th_con1 ] ) , one can get ( [ rt : conver_1 ] ) , which completes the proof .",
    "consider information transmission over a memoryless discrete @xmath0-ary symmetric channel with a channel code @xmath127 with crossover probability @xmath128 , as shown in fig .",
    "[ fig : qbc ] . in this case , the probability of symbol error is @xmath129 .",
    "then the error probabilities are @xmath130 and the block error probability is @xmath131    using the extended fano s inequality in theorem [ th : gner_fano ] , we have @xmath132    it is easy to see that the conditional entropy in theory is @xmath133    by corollary [ cor:4 ] , mutual information is lower bounded by@xmath134 .      \\end{split}\\ ] ] while the capacity of the memoryless qsc is given by @xmath135    and the relative entropy @xmath136 can be derived as @xmath137\\\\      \\end{split}\\ ] ]    for a given average symbol error probability constraint @xmath138 , the upper bound on the maximum codebook size given by theorem [ th : conver_1 ] is @xmath139 + n\\epsilon\\log(q-1 ) .",
    "\\end{split}\\ ] ]    on the other hand , by fano s inequality we have @xmath140 with @xmath88 given by ( [ eq : pb ] ) .",
    "then the lower bound of the mutual information is @xmath141    finally , the upper bound on the codebook size is @xmath142    suppose the qsc parameter are @xmath143 and @xmath144 , we calculated the bounds on conditional entropy , mutual information and codebook size by our proposed results and fano s inequality .",
    "firstly , the upper bound on the conditional entropy is presented in fig .",
    "[ fig : hxy ] .",
    "specially , @xmath145 is obtained by the extended fano s inequality ( [ sim : hexy ] ) , @xmath146 is calculated according to fano s inequality ( [ sim : hfxy ] ) and @xmath147 is the conditional entropy in theory ( [ sim : hxy ] ) .",
    "it is clear that theorem [ th : gner_fano ] is tighter than fano s inequality .",
    "particularly , we have @xmath148 for the qsc .",
    "this is because the error distributions are the same for any @xmath52 .",
    "so @xmath149 holds . besides",
    ", the error pattern is uniformly distributed for a given @xmath51 , regardless of @xmath54 and @xmath150 holds .",
    "therefore , the upper bound is tight .",
    "however , for fano s equality , there are relaxations in both @xmath149 to @xmath151 and @xmath152 to @xmath153 .    , width=259 ]    , width=278 ]    similarly",
    ", the lower bound on mutual information @xmath154 given by ( [ sim : iexy ] ) coincides with @xmath155 in theory , given by ( [ sim : ixy ] ) and is better than that given by fano s inequality ( [ sim : ifxy ] ) .",
    "when we use the upper bound on the codebook size in theorem [ th : conver_1 ] , it should be noted that it is presented as a function of symbol error probability @xmath93 .",
    "in fact , @xmath88 is always larger than @xmath93",
    ". therefore , we use the same fraction of them in the calculation of the bounds to make sense of the comparison , i.e. , @xmath156 for ( [ sim : mexy ] ) and @xmath157 for ( [ sim : mxy ] ) .",
    "it is also seen from fig .",
    "[ fig : ixy ] that our new developed result is tighter .",
    "the performances of theorem [ th : gner_fano_ixy ] and theorem [ th : conver_1 ] versus the qsc parameter @xmath128 are shown in fig .",
    "[ fig : epslong ] , where the block length is chosen as @xmath158 .",
    "as shown , the mutual information bound is tight and our results are much better than fano s inequality . in the calculation of the upper bounds on codebook size , the selection of the error probability constraints are also chosen as @xmath159 and @xmath160 so that they are comparable .    , @xmath161,width=278 ]",
    "in this paper , we revisited fano s inequality and extended it to a general form . particularly , the relation between the conditional entropy and error probability of two random vectors was considered , other than that between two random variables .",
    "this makes the developed results more suitable for source / channel codings in the finite blocklength regime . by investigating the block error pattern more detailedly ,",
    "the conditional entropy of the original random vector given the received one is upper bounded more tightly by the extended fano s inequality .",
    "furthermore , the extended fano s inequality is completely tight for some symmetric channels such the @xmath0-ary symmetric channels .",
    "converse results are also presented in terms of lower bounds on the mutual information and a upper bound on the codebook size under the blocklength and symbol error constraints , which also have better performances .",
    "this work was partially supported by inc research grant of chinese university of hongkong and the china major state basic research development program ( 973 program ) no .",
    "2012cb316100(2 ) .",
    "15 c. e. shannon,a mathematical theory of communication , \" _ bell syst .",
    "pp.623 - 656 , oct .",
    "r. m. fano , _ class notes for transmission of information _ , course6.574 .",
    "combridge , ma : mit .",
    "r. w. raymond , _ information theory and network coding _ ,",
    "berlin / newyork : springer . 2008 .",
    "han and s. verdu ,  generalizing the fano inequality , \" _ ieee trans .",
    "inform . theory _",
    "40 , no . 7 , pp .",
    "1247 - 1250 , jul . 1994 . s. w. ho and s. verdu ,  on the interplay between conditional entropy and error probability \" _ ieee trans .",
    "inform . theory _ ,",
    "vol.56 , no.12 , pp.5930 - 5942 , dec . 2010 .",
    "d. l. tebbe and s. j. dwyer , iii ,  uncertainty and probability of error , \" _ ieee trans .",
    "inform . theory _",
    "516 - 518 , may 1968 ."
  ],
  "abstract_text": [
    "<S> fano s inequality reveals the relation between the conditional entropy and the probability of error . </S>",
    "<S> it has been the key tool in proving the converse of coding theorems in the past sixty years . in this paper , an extended fano s inequality </S>",
    "<S> is proposed , which is tighter and more applicable for codings in the finite blocklength regime . </S>",
    "<S> lower bounds on the mutual information and an upper bound on the codebook size are also given , which are shown to be tighter than the original fano s inequality . </S>",
    "<S> especially , the extended fano s inequality is tight for some symmetric channels such as the @xmath0-ary symmetric channels ( qsc ) .    fano s inequality , finite blocklength regime , channel coding , shannon theory . </S>"
  ]
}