{
  "article_text": [
    "in addition to being very attractive at the theoretical level , sparse signal modeling has been shown to lead to numerous state - of - the - art results in signal processing .",
    "the standard model assumes that a signal can be efficiently represented by a sparse linear combination of atoms from a given or learned dictionary .",
    "the selected atoms form what is usually referred to as the _ active set _ , whose cardinality is significantly smaller than the size of the dictionary and the dimension of the signal . in recent years",
    ", it has been shown that adding structural constraints to this active set has value both at the level of representation robustness and at the level of signal interpretation ( in particular when the active set indicates some physical properties of the signal ) , see @xcite and references therein .",
    "this leads to _ group _ or _ structured _ sparse coding , where instead of considering the atoms as singletons , the atoms are grouped , and a few groups are active at a time .",
    "an alternative way to add structure ( and robustness ) to the problem is to consider the simultaneous encoding of multiple signals , requesting that they all share the same active set .",
    "this is a natural collaborative filtering approach to sparse coding , see @xcite and references therein .    in this work",
    "we extend these models in a number of directions .",
    "first , we present a hierarchical sparse model , where not only a few ( sparse ) groups of atoms are active at a time , but also each group enjoys internal sparsity . at the conceptual level",
    ", this means that the signal is represented by a few groups ( classes ) , and inside each group only a few members are active at a time .",
    "a simple example of this is a piece of music ( numerous applications in genomics ) , where only a few instruments are active at a time ( each instrument is a group ) , and the actual music played by the instrument is efficiently represented by a few atoms of the sub - dictionary / group corresponding to it .",
    "thereby , this proposed hierarchical sparse coding framework permits to efficiently perform source separation , where the individual sources ( classes / groups ) that generated the signal are identified at the same time as their efficient representation is reconstructed ( the sparse code inside the group ) .",
    "an efficient optimization procedure is proposed to solve this hierarchical sparse coding framework .",
    "then , we go a step beyond this .",
    "imagine now that we have multiple recordings of the same two instruments ( or different time windows of the same recording ) , each time playing different songs .",
    "then , if we apply this new hierarchical sparse coding approach collaboratively , we expect that the different recordings will share the same groups ( since they are of the same instruments ) , but each will have its unique sparsity pattern inside the group ( since each recording is a different melody ) .",
    "we propose a collaborative hierarchical sparse coding framework addressing exactly this .",
    "an efficient optimization procedure for this case is derived as well .    in the remainder of this paper , we introduce these new models and their corresponding optimization , present examples illustrating them , and provide possible directions of research opened by these new frameworks , including some theoretical ones .",
    "assume we have a set of data samples @xmath1 , and a dictionary of @xmath2 atoms , assembled as a matrix @xmath3 , @xmath4 $ ] .",
    "each sample @xmath5 can be written as @xmath6 , that is , as a linear combination of the atoms in the dictionary @xmath7 plus some perturbation @xmath8 . the basic underlying assumption in sparse coding is that , for all or most @xmath9 , the optimal reconstruction @xmath10 has only a few nonzero elements .",
    "formally , if we define the cost @xmath11 as the pseudo - norm counting the number of nonzero elements of @xmath10 , @xmath12 , we expect that @xmath13 for all or most @xmath9 .",
    "the @xmath14 optimization is non - convex and known to be np - hard , so a convex approximation to it is considered instead , which uses the @xmath15 norm cost , @xmath16 the above approximation is known as the lasso @xcite . a popular variant is to use the unconstrained version @xmath17 where @xmath18 is a parameter usually found by cross - validation .",
    "the @xmath19 regularizer induces sparsity in the solution @xmath10 .",
    "this is desirable not only from a regularization point of view , but also from a model selection point , where one wants to identify the relevant features or factors ( atoms ) that conform each sample @xmath5 . in many situations",
    ", however , one wants to represent the relevant factors not as single atoms but as groups of atoms . given a dictionary of @xmath2 atoms ,",
    "we define groups through their indexes , @xmath20 . given a group @xmath21",
    ", we define the subset of atoms of @xmath7 belonging to it as @xmath22 , and the corresponding set of linear reconstruction coefficients as @xmath23 .",
    "define @xmath24 to be a partition of @xmath25 .",
    "the group lasso problem was introduced in @xcite as @xmath26 where @xmath27 is the group lasso regularizer defined in terms of @xmath28 as @xmath29 .",
    "note that @xmath30 can be seen as an @xmath15 on euclidean norms of the vectors formed by coefficients belonging to the same group @xmath23 .",
    "this is a generalization of the @xmath15 regularizer , as the latter arises from the special case @xmath31 , and , as such its effect on the groups of @xmath32 is also a natural generalization of the one obtained with the lasso : it `` turns on '' or `` off '' atoms in groups .",
    "the group lasso trades sparsity at the single - coefficient level with sparsity at a group level , while , inside each group , the solution is dense ( actually it reduces to a least squares within the group ) .",
    "as we are interested in maintaining the sparsity at the coefficient level , we simply re - introduce the @xmath15 regularizer together with the group regularizer , leading to the proposed _ hierarchical lasso ( hilasso ) _ model , @xmath33 we refer to this regularizer as the @xmath34 .. ] in section  [ sec : opt ] we propose an efficient optimization for , while in section  [ sec : results ] we experimentally show the virtues of this model .      in numerous applications ,",
    "one expects that certain collections of samples @xmath5 share the same active components from the dictionary , that is , that the indexes of the nonzero coefficients in @xmath10 are the same for all the samples in the collection . imposing such dependency in the @xmath15 regularized regression problem gives rise to the so called collaborative ( also called `` multitask '' or `` simultaneous '' ) sparse coding problem @xcite .",
    "more specifically , if we consider the matrix of coefficients @xmath35 $ ] associated to the reconstruction of the samples @xmath36 $ ] , the collaborative sparse coding model is given by @xmath37 where @xmath38 is the @xmath39-th row of @xmath40 , that is , the vector of the @xmath41 different values that the coefficient associated to the @xmath39-th atom takes for each sample @xmath9 .",
    "if we now extend this idea to the group lasso , we obtain a collaborative group lasso formulation , @xmath42 where the regularizer @xmath30 for a matrix is defined as @xmath43 , being @xmath44 the submatix formed by all the rows belonging to group @xmath21 .",
    "we chose this notations since this regularizer is the natural extension of the regularizer in ( [ eq : group - lasso ] ) for the collaborative case .",
    "to the best of our knowledge , this combination has not yet been investigated in the literature . in this paper",
    "we are moving one step forward and treat this together with the hierarchical extension .",
    "the combined model that we propose for this problem ( _ c - hilasso _ ) can be written as follows @xmath45 the collaborative group lasso is a particular case of our model when @xmath46 is zero .",
    "on the other hand , one can obtain independent lasso for each @xmath47 by setting @xmath48 to zero .",
    "this new formulation is particularly well suited when the vectors have missing components . in this case",
    "combining the information from all the samples is very important in order to lead to a correct representation and model ( group ) selection .",
    "this can be done by slightly changing the data term in .",
    "for each data vector @xmath5 one computes the reconstruction error using only the observed elements .",
    "note that the missing components do not affect the other terms of the equation .",
    "in the last decade , optimization of problems of the form of and have been deeply studied and there exist very efficient algorithms for solving them . recently , wright et .",
    "al @xcite proposed a framework , , for solving the general problem @xmath49 under reasonable assumptions . to guarantee convergence",
    "@xmath50 needs to be a smooth and convex function while @xmath51 only needs to be finite in @xmath52 .",
    "when the regularizer , @xmath51 , is group separable , the optimization can be subdivided into smaller problems , one per group .",
    "the framework becomes powerful when these subproblems can be solved efficiently .",
    "this is the case of the lasso and group lasso settings but is not immediate when the regularizer is the proposed @xmath53 norm . in this work",
    "we combine the  with the alternating direction method of multipliers @xcite ( admom ) , to efficiently solve the hilasso problem .",
    "the  algorithm generates a sequence of iterates @xmath54 that , under certain conditions , converges to the solution of . at each iteration , @xmath55 is obtained solving @xmath56 for some sequence of parameters @xmath57 with @xmath58 .",
    "the conditions for which the algorithm converges depend on the choice of @xmath59 , see @xcite for details .",
    "it is easy to show that ( [ eq : sparsa - subproblem ] ) is equivalent to @xmath60 where @xmath61 in this new formulation , it is clear that the first term in the cost function can be separated element - wise .",
    "thus when the regularization function @xmath62 is group separable , so is the overall optimization , and one can solve ( [ eq : sparsa - subproblemeq ] ) independently for each group , @xmath63 which in the case of hilasso , this becomes , @xmath64 where @xmath65 and @xmath66 . this is a socp for which one could use generic solvers .",
    "however , this subproblem needs to be solved many times within the   iterations , so it is crucial to solve it efficiently . for this",
    "we use the admom method @xcite .",
    "the idea is to solve the artificially constrained equivalent problem , @xmath67 where @xmath68 . the algorithm generates a set of iterates @xmath69 which converges to the minimum of the augmented lagrangian of the problem @xmath70 where the elements of @xmath71 are the so called lagrangian multipliers , and @xmath72 is a fixed constant . at each iteration ,",
    "the variables @xmath73 and @xmath74 are updated , one at a time , by minimizing the augmented lagrangian while letting the remaining fixed : @xmath75 for convenience in the notation we omitted the super - indexes for the iterates at step @xmath76 , just explicitly indexing them at step @xmath77 .",
    "the update for @xmath73 is separable into scalar subproblems on the coordinates of @xmath73 .",
    "the optimality conditions on the subgradient of each of this scalar problems leads to a simple variant of the well known soft - thresholding operator , @xmath78 . for convenience",
    ", we use the notation @xmath79 to denote the vector obtained when applying the soft - thresholding operator ( with parameter @xmath18 ) to each element of @xmath80 . on the other hand ,",
    "the update for @xmath74 is not separable into scalar subproblems .",
    "however its optimality condition is given by @xmath81 , which is exactly the one leading to the vector shrinkage operator , @xmath82 , described in @xcite for the group lasso ( actually much simpler , since there is no matrix multiplication involved ) : @xmath83_{+}\\!{\\ensuremath{\\mathbf{b}}}.\\ ] ] then both updates can be written in closed form and computed very efficiently : @xmath84 the algorithm is very robust and converges in very few iterations to its optimum , thereby obtaining a very efficient approach to solve the subproblem .",
    "the  framework then becomes a very interesting approach for the proposed hilasso .",
    "the complete algorithm is summarized in algorithm  [ alg1 ] .",
    "an additional speed up is obtained by bypassing admom when a whole group is not active . from the optimality conditions of it",
    "follows that , if @xmath85 is a solution when @xmath86 ( standard group lasso ) , it is also a solution in the general case .",
    "this can be simply checked by evaluating @xmath87",
    ".    set @xmath88 choose a factor @xmath89 and constants @xmath90 and @xmath91 choose an initial @xmath92      we now propose an optimization algorithm to efficiently solve the collaborative hilasso .",
    "the main idea is to use admom to divide the overall problem into two subproblems : one that breaks the multi - signal problem into @xmath41 single - signal @xmath0 regressions , and another that treats the multi - signal case as a single group lasso - like problem . in this way",
    "we take advantage of the separability of each term as shown in figure  [ fig : structure ] .",
    "we define a constrained optimization problem , @xmath93 the admom iterations are given by ( we omitted the super - index for variables at iteration @xmath76 for notation convenience ) . @xmath94",
    "* solving for @xmath95 : * problem ( [ eq : col : problem1 ] ) can be separated into @xmath96 single - signal subproblems by updating one column of the matrix @xmath40 at a time , @xmath97 this problem can be solved using the  framework .",
    "the idea is to consider the first three terms of the cost as @xmath98 in equation .",
    "the associated computational cost is equivalent to the one of the lasso , since the regularizer is the standard @xmath0 norm .",
    "* solving for @xmath99 : * the problem given by ( [ eq : col : problem2 ] ) is group separable , as a direct consequence of the separability of @xmath30 .",
    "thus , we need to solve @xmath100 optimization problems of the form , @xmath101 where @xmath102 , @xmath103 and @xmath104 are the @xmath105 sub - matrices of @xmath106 , @xmath107 and @xmath108 associated with the group @xmath21 respectively . we express them as column vectors ( each with @xmath109 components ) by concatenating their columns , obtaining @xmath110 and @xmath111 respectively , and rewrite the optimization problem in vectorial form as @xmath112 this problem is identical to and can be reduced to a group lasso problem by simply changing variables and thus , it is solved using vectorial thresholding .",
    "structure of the problem in terms of coupling.,scaledwidth=45.0% ]",
    "we start by comparing our model with the standard lasso and group lasso using synthetic data .",
    "we created @xmath113 dictionaries , @xmath114 , with 64 atoms of dimension 64 , with i.i.d .",
    "gaussian entries .",
    "the columns were normalized to have unit @xmath115 norm .",
    "then we randomly chose two groups to be active at each time ( on all the signals ) .",
    "sets of @xmath116 testing signals were generated , one per active group , as linear combinations of @xmath117 elements of the dictionaries , @xmath118 .",
    "these signals were also normalized .",
    "the mixtures were created by summing these signals and ( eventually ) adding gaussian noise of standard deviation @xmath119 .",
    "the generated testing signals have a hierarchical sparsity structure and while they share groups , they do not necessarily share the sparsity pattern inside the groups .",
    "we built a single dictionary by concatenating the sub - dictionaries , @xmath120 $ ] , and use it to solve the lasso , group lasso , hilasso and c - hilasso problems .",
    "table  [ tab : multi - signal - mse ] summarizes the mean square error ( mse ) and hamming distance of the recovered coefficient vectors .",
    "we observe that our model is able to exploit the hierarchical structure of the data as well as the collaborative structure . from a modeling point of view , we observe that the group lasso selects in general the correct blocks but it does not give a sparse solution within them . on the other hand , lasso gives a solution that has nonzero elements belonging to groups that were not active in the original signal , leading to a wrong model selection .",
    "hilasso gives a sparse solution that picks atoms form the correct groups but still presents some minor mistakes .",
    "for the collaborative case , in all the tested cases , no coefficients were selected outside the correct active groups and the recovered coefficients are consistently the best ones .",
    "this robustness comes from the fact that the active groups are collaboratively found using the information present in all the signals .",
    ".[tab : multi - signal - mse ] active sets mse ( we show them multiplied by @xmath121 ) and hamming distance ( mse / hamming ) for the tested methods . in the first case we vary the noise level while we keep @xmath122 and @xmath123 fixed . in the two other tables the signals are noise free and we first set @xmath122 while changing @xmath39 , and then set @xmath124 while changing the number of groups . for each method",
    "the regularization parameters were the ones for which the best results where obtained . [ cols=\"^,^,^,^,^\",options=\"header \" , ]      + .",
    "( top ) the table shows the separating errors ( we show them multiplied by @xmath121 ) for the digits dataset .",
    "we show the results for separating digits 3 and 5 , and 2 and 7 , with and without additive noise of standard deviation @xmath125 .",
    "we used sets of 200 copies .",
    "( bottom ) active sets recovered for the group lasso , lasso , hilasso and c - hilasso for a given example .",
    "each block corresponds to the coefficients associated with the digits displayed bellow .",
    "the active coefficients are displayed in read .",
    "only c - hilasso manages to perfectly recover the correct models ( with the lowest separating error ) , while hilasso performs very well also . , title=\"fig:\",scaledwidth=42.6% ] +    finally , we used c - hilasso to separate overlapping textures in an image .",
    "we chose 8 textures form the brodatz dataset and trained one dictionary for each one of them ( these form the 8 groups of the dictionary ) .",
    "then we created an image as the sum of two textues ( the testing images were not used in the training stage ) . in figure",
    "[ fig : texture ] we show results .",
    "the overall group hamming distance obtained for c - hilasso is 0.003 , showing that the correct groups , and only them , were practically selected all the time .",
    "in this paper we have introduced a new framework of collaborative hierarchical sparse coding , where multiple signals collaborate in their encoding , sharing code groups ( models ) and having ( possible disjoint ) sparse representations inside the corresponding groups .",
    "an efficient optimization approach was developed , which guarantees convergence to the global minimum , and examples illustrating the power of this framework were presented .    .",
    "( top ) we show two examples ( one per row ) of the recovered digits from a mixture with 60% of missing components .",
    "we first show the original mixture image , then the image with the missing pixels highlighted in red , and finally the digits recovered .",
    "( bottom ) here we show a comparision of the active sets recovered using the lasso ( left ) and the c - hilasso ( right ) methods .",
    "the active sets for the set of signals ( as shown in figure  [ fig : digits ] ) are placed as columns .",
    "the coefficients corresponding to digits 3 and 5 fall inside the area delimited by the red horizontal lines . while c - hilasso recovers the correct sources in all the cases , the lasso method makes several mistakes .",
    ", title=\"fig:\",scaledwidth=11.0% ] .",
    "( top ) we show two examples ( one per row ) of the recovered digits from a mixture with 60% of missing components .",
    "we first show the original mixture image , then the image with the missing pixels highlighted in red , and finally the digits recovered .",
    "( bottom ) here we show a comparision of the active sets recovered using the lasso ( left ) and the c - hilasso ( right ) methods .",
    "the active sets for the set of signals ( as shown in figure  [ fig : digits ] ) are placed as columns .",
    "the coefficients corresponding to digits 3 and 5 fall inside the area delimited by the red horizontal lines .",
    "while c - hilasso recovers the correct sources in all the cases , the lasso method makes several mistakes .",
    ", title=\"fig:\",scaledwidth=11.0% ] .",
    "( top ) we show two examples ( one per row ) of the recovered digits from a mixture with 60% of missing components .",
    "we first show the original mixture image , then the image with the missing pixels highlighted in red , and finally the digits recovered .",
    "( bottom ) here we show a comparision of the active sets recovered using the lasso ( left ) and the c - hilasso ( right ) methods .",
    "the active sets for the set of signals ( as shown in figure  [ fig : digits ] ) are placed as columns .",
    "the coefficients corresponding to digits 3 and 5 fall inside the area delimited by the red horizontal lines . while c - hilasso recovers the correct sources in all the cases , the lasso method makes several mistakes . ,",
    "title=\"fig:\",scaledwidth=11.0% ] .",
    "( top ) we show two examples ( one per row ) of the recovered digits from a mixture with 60% of missing components .",
    "we first show the original mixture image , then the image with the missing pixels highlighted in red , and finally the digits recovered .",
    "( bottom ) here we show a comparision of the active sets recovered using the lasso ( left ) and the c - hilasso ( right ) methods .",
    "the active sets for the set of signals ( as shown in figure  [ fig : digits ] ) are placed as columns .",
    "the coefficients corresponding to digits 3 and 5 fall inside the area delimited by the red horizontal lines . while c - hilasso recovers the correct sources in all the cases , the lasso method makes several mistakes .",
    ", title=\"fig:\",scaledwidth=11.0% ] + .",
    "( top ) we show two examples ( one per row ) of the recovered digits from a mixture with 60% of missing components .",
    "we first show the original mixture image , then the image with the missing pixels highlighted in red , and finally the digits recovered .",
    "( bottom ) here we show a comparision of the active sets recovered using the lasso ( left ) and the c - hilasso ( right ) methods .",
    "the active sets for the set of signals ( as shown in figure  [ fig : digits ] ) are placed as columns .",
    "the coefficients corresponding to digits 3 and 5 fall inside the area delimited by the red horizontal lines . while c - hilasso recovers the correct sources in all the cases , the lasso method makes several mistakes . ,",
    "title=\"fig:\",scaledwidth=11.0% ] .",
    "( top ) we show two examples ( one per row ) of the recovered digits from a mixture with 60% of missing components .",
    "we first show the original mixture image , then the image with the missing pixels highlighted in red , and finally the digits recovered .",
    "( bottom ) here we show a comparision of the active sets recovered using the lasso ( left ) and the c - hilasso ( right ) methods .",
    "the active sets for the set of signals ( as shown in figure  [ fig : digits ] ) are placed as columns .",
    "the coefficients corresponding to digits 3 and 5 fall inside the area delimited by the red horizontal lines . while c - hilasso recovers the correct sources in all the cases , the lasso method makes several mistakes . ,",
    "title=\"fig:\",scaledwidth=11.0% ] .",
    "( top ) we show two examples ( one per row ) of the recovered digits from a mixture with 60% of missing components .",
    "we first show the original mixture image , then the image with the missing pixels highlighted in red , and finally the digits recovered .",
    "( bottom ) here we show a comparision of the active sets recovered using the lasso ( left ) and the c - hilasso ( right ) methods .",
    "the active sets for the set of signals ( as shown in figure  [ fig : digits ] ) are placed as columns .",
    "the coefficients corresponding to digits 3 and 5 fall inside the area delimited by the red horizontal lines . while c - hilasso recovers the correct sources in all the cases , the lasso method makes several mistakes .",
    ", title=\"fig:\",scaledwidth=11.0% ] .",
    "( top ) we show two examples ( one per row ) of the recovered digits from a mixture with 60% of missing components .",
    "we first show the original mixture image , then the image with the missing pixels highlighted in red , and finally the digits recovered .",
    "( bottom ) here we show a comparision of the active sets recovered using the lasso ( left ) and the c - hilasso ( right ) methods .",
    "the active sets for the set of signals ( as shown in figure  [ fig : digits ] ) are placed as columns .",
    "the coefficients corresponding to digits 3 and 5 fall inside the area delimited by the red horizontal lines . while c - hilasso recovers the correct sources in all the cases , the lasso method makes several mistakes . ,",
    "title=\"fig:\",scaledwidth=11.0% ] + .",
    "( top ) we show two examples ( one per row ) of the recovered digits from a mixture with 60% of missing components .",
    "we first show the original mixture image , then the image with the missing pixels highlighted in red , and finally the digits recovered .",
    "( bottom ) here we show a comparision of the active sets recovered using the lasso ( left ) and the c - hilasso ( right ) methods .",
    "the active sets for the set of signals ( as shown in figure  [ fig : digits ] ) are placed as columns .",
    "the coefficients corresponding to digits 3 and 5 fall inside the area delimited by the red horizontal lines .",
    "while c - hilasso recovers the correct sources in all the cases , the lasso method makes several mistakes .",
    ", title=\"fig:\",scaledwidth=23.0%,scaledwidth=23.0% ] .",
    "( top ) we show two examples ( one per row ) of the recovered digits from a mixture with 60% of missing components .",
    "we first show the original mixture image , then the image with the missing pixels highlighted in red , and finally the digits recovered .",
    "( bottom ) here we show a comparision of the active sets recovered using the lasso ( left ) and the c - hilasso ( right ) methods .",
    "the active sets for the set of signals ( as shown in figure  [ fig : digits ] ) are placed as columns .",
    "the coefficients corresponding to digits 3 and 5 fall inside the area delimited by the red horizontal lines . while c - hilasso recovers the correct sources in all the cases , the lasso method makes several mistakes . ,",
    "title=\"fig:\",scaledwidth=23.0%,scaledwidth=23.0% ]    at the practical level , we are currently working on the applications of this proposed framework in a number of directions , including collaborative instruments separation in music ; and signal classification , following the demonstrated capability to collectively select the correct groups / models .    at the theoretical level ,",
    "a whole family of new problems is opened by this proposed framework .",
    "a critical one is the overall capability of selecting the correct groups and thereby of performing correct model selection and source separation .",
    "let us consider for example the case of only two groups ( so no sparsity at the group level ) and a single signal composed by the linear combination of atoms from each group .",
    "then , it is easy to show that the cross - mutual coherence between the groups plays a critical role .",
    "let us call @xmath126 , @xmath127 , the internal coherence of the atoms of the group @xmath128 , and @xmath129 the one between the groups ( maximal normalized correlation between an atom of group @xmath130 with an atom of group @xmath131 ) .",
    "then it is easy to show that uniqueness of the separation can be guaranteed if @xmath132 and @xmath133 , with @xmath134 the respective sparsity levels inside each group ( this is a weaker bound that the more stringent one developed by @xcite ) .",
    "this needs to be extended to actual sparsity at the group level and to the collaborative case .",
    "note of course that considering a single active group is a particular case of our model ( see @xcite for works in this case ) , thereby an overall theoretical framework for our proposed collaborative hierarchical framework will automatically include numerous of the existing results in sparse coding .",
    "finally , we have also developed a framework for learning the dictionary for collaborative hierarchical sparse coding , meaning the optimization is simultaneously on the dictionary and the code . as it is the case with standard dictionary learning , this is expected to lead to significant performance improvements ( again , see @xcite for the particular case of this with a single group active at a time ) .    .",
    "results for the texture segmentation .",
    "one example of the mixture and the c - hilasso separated textures are shown .",
    "this is followed by the active set diagram ( as in figure [ fig : missing ] ) , lasso on top ( with class selection wrongly all over the 8 textures ) and c - hilasso on bottom , where only the 2 corresponding groups are selected.,scaledwidth=48.0% ]    * acknowledgments : * work partially supported by nsf , onr , nga , and aro . we thank dr .",
    "tristan nguyen , when we presented him this model , he motivated us to think in a hierarchical fashion and to look at this as just the particular case of a fully hierarchical sparse coding framework .",
    "we also thank prof .",
    "larry carin , dr .",
    "guoshen yu , and alexey castrodad for very stimulating conversations and for the fact that their own work also motivated the example with missing information .",
    "j.  peng , j.  zhu , a.  bergamaschi , w.  han , d.  noh , j.  pollack , and p.  wang , `` regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer , ''"
  ],
  "abstract_text": [
    "<S> sparse modeling is a powerful framework for data analysis and processing . traditionally , encoding in this framework is done by solving an @xmath0-regularized linear regression problem , usually called _ </S>",
    "<S> lasso_. in this work we first combine the sparsity - inducing property of the lasso model , at the individual feature level , with the block - sparsity property of the _ group lasso _ model , where sparse groups of features are jointly encoded , obtaining a sparsity pattern hierarchically structured . </S>",
    "<S> this results in the _ hierarchical lasso _ , which shows important practical modeling advantages . </S>",
    "<S> we then extend this approach to the collaborative case , where a set of simultaneously coded signals share the same sparsity pattern at the higher ( group ) level but not necessarily at the lower one . </S>",
    "<S> signals then share the same active groups , or classes , but not necessarily the same active set . </S>",
    "<S> this is very well suited for applications such as source separation . </S>",
    "<S> an efficient optimization procedure , which guarantees convergence to the global optimum , is developed for these new models . </S>",
    "<S> the underlying presentation of the new framework and optimization approach is complemented with experimental examples and preliminary theoretical results . </S>"
  ]
}