{
  "article_text": [
    "many authors @xcite have discussed the analogy between algorithmic entropy and entropy as defined in statistical mechanics : that is , the entropy of a probability measure @xmath9 on a set @xmath0 .",
    "it is perhaps insufficiently appreciated that algorithmic entropy can be seen as a _ special case _ of the entropy as defined in statistical mechanics .",
    "we describe how to do this in section [ entropy ] .",
    "this allows all the basic techniques of thermodynamics to be imported to algorithmic information theory .",
    "the key idea is to take @xmath0 to be some version of ` the set of all programs that eventually halt and output a natural number ' , and let @xmath9 be a gibbs ensemble on @xmath0 .",
    "a gibbs ensemble is a probability measure that maximizes entropy subject to constraints on the mean values of some observables  that is , real - valued functions on @xmath0 .    in most traditional work on algorithmic entropy ,",
    "the relevant observable is the length of the program .",
    "however , much of the interesting structure of thermodynamics only becomes visible when we consider several observables .",
    "when @xmath0 is the set of programs that halt and output a natural number , some other important observables include the output of the program and logarithm of its runtime .",
    "so , in section [ thermodynamics ] we illustrate how ideas from thermodynamics can be applied to algorithmic information theory using these three observables .",
    "to do this , we consider a gibbs ensemble of programs which maximizes entropy subject to constraints on :    * @xmath1 , the expected value of the logarithm of the program s runtime ( which we treat as analogous to the energy of a container of gas ) , * @xmath2 , the expected value of the length of the program ( analogous to the volume of the container ) , and * @xmath3 , the expected value of the program s output ( analogous to the number of molecules in the gas ) .",
    "this measure is of the form @xmath10 for certain numbers @xmath11 , where the normalizing factor @xmath12 is called the ` partition function ' of the ensemble .",
    "the partition function reduces to chaitin s number @xmath13 when @xmath14 , @xmath15 and @xmath16 .",
    "this number is uncomputable @xcite .",
    "however , we show that the partition function @xmath17 is computable when @xmath18 , @xmath19 , and @xmath20 .",
    "we derive an algorithmic analogue of the basic thermodynamic relation @xmath21 here :    * @xmath22 is the entropy of the gibbs emsemble , * @xmath23 is the ` algorithmic temperature ' ( analogous to the temperature of a container of gas ) . roughly speaking ,",
    "this counts how many times you must double the runtime in order to double the number of programs in the ensemble while holding their mean length and output fixed .",
    "* @xmath24 is the ` algorithmic pressure ' ( analogous to pressure ) .",
    "this measures the tradeoff between runtime and length . roughly speaking",
    ", it counts how much you need to decrease the mean length to increase the mean log runtime by a specified amount , while holding the number of programs in the ensemble and their mean output fixed .",
    "* @xmath25 is the ` algorithmic potential ' ( analogous to chemical potential ) . roughly speaking ,",
    "this counts how much the mean log runtime increases when you increase the mean output while holding the number of programs in the ensemble and their mean length fixed .    starting from this relation , we derive analogues of maxwell s relations and consider thermodynamic cycles such as the carnot cycle or stoddard cycle . for this",
    "we must introduce concepts of ` algorithmic heat ' and ` algorithmic work ' .",
    "charles babbage described a computer powered by a steam engine ; we describe a heat engine powered by programs !",
    "we admit that the significance of this line of thinking remains a bit mysterious .",
    "however , we hope it points the way toward a further synthesis of algorithmic information theory and thermodynamics .",
    "we call this hoped - for synthesis ` algorithmic thermodynamics ' .",
    "li and vitnyi use the term ` algorithmic thermodynamics ' for describing physical states using a universal prefix - free turing machine @xmath26 .",
    "they look at the smallest program @xmath9 that outputs a description @xmath27 of a particular microstate to some accuracy , and define the physical entropy to be @xmath28 where @xmath29 and @xmath30 embodies the uncertainty in the actual state given @xmath27 .",
    "they summarize their own work and subsequent work by others in chapter eight of their book @xcite . whereas they consider @xmath31 to be a microstate",
    ", we consider @xmath9 to be the microstate and @xmath27 the value of the observable @xmath26 .",
    "then their observables @xmath32 become observables of the form @xmath33 in our model .",
    "tadaki @xcite generalized chaitin s number @xmath13 to a function @xmath34 and showed that the value of this function is compressible by a factor of exactly @xmath35 when @xmath35 is computable .",
    "calude and stay @xcite pointed out that this generalization was formally equivalent to the partition function of a statistical mechanical system where temperature played the role of the compressibility factor , and studied various observables of such a system .",
    "tadaki @xcite then explicitly constructed a system with that partition function : given a total length @xmath1 and number of programs @xmath36 the entropy of the system is the log of the number of @xmath1-bit strings in @xmath37 the temperature is @xmath38 in a follow - up paper @xcite , tadaki showed that various other quantities like the free energy shared the same compressibility properties as @xmath34 . in this paper , we consider multiple variables , which is necessary for thermodynamic cycles , chemical reactions , and so forth .",
    "manin and marcolli @xcite derived similar results in a broader context and studied phase transitions in those systems .",
    "manin @xcite also outlined an ambitious program to treat the infinite runtimes one finds in undecidable problems as singularities to be removed through the process of renormalization . in a manner reminiscent of hunting for the proper definition of the `` one - element field '' @xmath39 he collected ideas from many different places and considered how they all touch on this central theme .",
    "while he mentioned a runtime cutoff as being analogous to an energy cutoff , the renormalizations he presented are uncomputable . in this paper , we take the log of the runtime as being analogous to the energy ; the randomness described by chaitin and tadaki then arises as the infinite - temperature limit .",
    "to see algorithmic entropy as a special case of the entropy of a probability measure , it is useful to follow solomonoff @xcite and take a bayesian viewpoint . in bayesian probability theory , we always start with a probability measure called a ` prior ' , which describes our assumptions about the situation at hand before we make any further observations .",
    "as we learn more , we may update this prior .",
    "this approach suggests that we should define the entropy of a probability measure _ relative to another probability measure _  the prior .",
    "a probability measure @xmath9 on a finite set @xmath0 is simply a function @xmath40 $ ] whose values sum to 1 , and its entropy is defined as follows : @xmath41 but we can also define the entropy of @xmath9 relative to another probability measure @xmath42 : @xmath43 this * relative entropy * has been extensively studied and goes by various other names , including ` kullback ",
    "leibler divergence ' @xcite and ` information gain ' @xcite .",
    "the term ` information gain ' is nicely descriptive .",
    "suppose we initially assume the outcome of an experiment is distributed according to the probability measure @xmath42 .",
    "suppose we then repeatedly do the experiment and discover its outcome is distributed according to the measure @xmath9",
    ". then the information gained is @xmath44 .",
    "we can see this in terms of coding .",
    "suppose @xmath0 is a finite set of signals which are randomly emitted by some source .",
    "suppose we wish to encode these signals as efficiently as possible in the form of bit strings .",
    "suppose the source emits the signal @xmath27 with probability @xmath45 , but we erroneously believe it is emitted with probability @xmath46",
    ". then @xmath47 is the expected extra message - length per signal that is required if we use a code that is optimal for the measure @xmath42 instead of a code that is optimal for the true measure , @xmath9 .",
    "the ordinary entropy @xmath48 is , up to a constant , just the relative entropy in the special case where the prior assigns an equal probability to each outcome . in other words : @xmath49 when @xmath50 is the so - called ` uninformative prior ' , with @xmath51 for all @xmath52 .",
    "we can also define relative entropy when the set @xmath0 is countably infinite . as before ,",
    "a probability measure on @xmath0 is a function @xmath40 $ ] whose values sum to 1 . and as before , if @xmath9 and @xmath42 are two probability measures on @xmath0 , the entropy of @xmath9 relative to @xmath42 is defined by @xmath53 but now the role of the prior becomes more clear , because there is no probability measure that assigns the same value to each outcome !    in what follows we will take @xmath0 to be  roughly speaking  the set of all programs that eventually halt and output a natural number . as we shall see , while this set is countably infinite , there are still some natural probability measures on it , which we may take as priors .    to make this precise",
    ", we recall the concept of a universal prefix - free turing machine . in what follows we use * string * to mean a bit string , that is , a finite , possibly empty , list of 0 s and 1 s .",
    "if @xmath27 and @xmath54 are strings , let @xmath55 be the concatenation of @xmath27 and @xmath56 a * prefix * of a string @xmath57 is a substring beginning with the first letter , that is , a string @xmath27 such that @xmath58 for some @xmath54 .",
    "a * prefix - free * set of strings is one in which no element is a prefix of any other .",
    "the * domain * @xmath59 of a turing machine @xmath60 is the set of strings that cause @xmath60 to eventually halt .",
    "we call the strings in @xmath59 * programs*. we assume that when the @xmath60 halts on the program @xmath27 , it outputs a natural number @xmath61",
    ". thus we may think of the machine @xmath60 as giving a function @xmath62 .",
    "a * prefix - free turing machine * is one whose halting programs form a prefix - free set . a prefix - free machine @xmath26 is * universal *",
    "if for any prefix - free turing machine @xmath60 there exists a constant @xmath63 such that for each string @xmath27 , there exists a string @xmath54 with @xmath64    let @xmath26 be a universal prefix - free turing machine .",
    "then we can define some probability measures on @xmath65 as follows .",
    "let @xmath66 be the function assigning to each bit string its length .",
    "then there is for any constant @xmath67 a probability measure @xmath9 given by @xmath68 here the normalization constant @xmath17 is chosen to make the numbers @xmath45 sum to 1 : @xmath69 it is worth noting that for computable real numbers @xmath19 , the normalization constant @xmath17 is uncomputable @xcite . indeed , when @xmath15",
    ", @xmath17 is chaitin s famous number @xmath13 .",
    "we return to this issue in section [ computability ] .",
    "let us assume that each program prints out some natural number as its output .",
    "thus we have a function @xmath70 where @xmath71 equals @xmath72 when program @xmath27 prints out the number @xmath72 .",
    "we may use this function to ` push forward ' @xmath9 to a probability measure @xmath42 on the set @xmath73 .",
    "explicitly : @xmath74 in other words , if @xmath72 is some natural number , @xmath75 is the probability that a program randomly chosen according to the measure @xmath9 will print out this number .    given any natural number @xmath76 , there is a probability measure @xmath77 on @xmath73 that assigns probability 1 to this number : @xmath78 we can compute the entropy of @xmath77 relative to @xmath42 : @xmath79 since the quantity @xmath80 is independent of the number @xmath76 , and uncomputable , it makes sense to focus attention on the other part of the relative entropy : @xmath81 if we take @xmath15 , this is precisely the * algorithmic entropy * @xcite of the number @xmath76 .",
    "so , up to the additive constant @xmath80 , we have seen that _ algorithmic entropy is a special case of relative entropy_.    one way to think about entropy is as a measure of surprise : if you can predict what comes next  that is , if you have a program that can compute it for you  then you are not surprised .",
    "for example , the first 2000 bits of the binary fraction for 1/3 can be produced with this short python program :    print `` 01 '' * 1000    but if the number is complicated , if every bit is surprising and unpredictable , then the shortest program to print the number does not do any computation at all !",
    "it just looks something like    print `` 101000011001010010100101000101111101101101001010 ''    levin s coding theorem @xcite says that the difference between the algorithmic entropy of a number and its * kolmogorov complexity *  the length of the shortest program that outputs it  is bounded by a constant that only depends on the programming language .",
    "so , up to some error bounded by a constant , _",
    "algorithmic information is information gain_. the algorithmic entropy is the information gained upon learning a number , if our prior assumption was that this number is the output of a randomly chosen program  randomly chosen according to the measure @xmath9 where @xmath15 .",
    "so , algorithmic entropy is not just _ analogous _ to entropy as defined in statistical mechanics : it is a _ special case _ , as long as we take seriously the bayesian philosophy that entropy should be understood as relative entropy .",
    "this realization opens up the possibility of taking many familiar concepts from thermodynamics , expressed in the language of statistical mechanics , and finding their counterparts in the realm of algorithmic information theory .    but to proceed , we must also understand more precisely the role of the measure @xmath9 . in the next section , we shall see that this type of measure is already familiar in statistical mechanics : it is a gibbs ensemble .",
    "suppose we have a countable set @xmath0 , finite or infinite , and suppose @xmath82 is some collection of functions . then we may seek a probability measure @xmath9 that maximizes entropy subject to the constraints that the mean value of each observable @xmath83 is a given real number @xmath84 : @xmath85 as nicely discussed by jaynes @xcite , the solution , if it exists , is the so - called * gibbs ensemble * : @xmath86 for some numbers @xmath87 depending on the desired mean values @xmath84 . here",
    "the normalizing factor @xmath17 is called the * partition function * : @xmath88    in thermodynamics , @xmath0 represents the set of * microstates * of some physical system . a probability measure on @xmath0",
    "is also known as an * ensemble*. each function @xmath89 is called an * observable * , and the corresponding quantity @xmath90 is called the * conjugate variable * of that observable .",
    "for example , the conjugate of the energy @xmath1 is the inverse of temperature @xmath4 , in units where boltzmann s constant equals 1 .",
    "the conjugate of the volume @xmath2  of a piston full of gas , for example  is the pressure @xmath5 divided by the temperature . and in a gas containing molecules of various types , the conjugate of the number @xmath91 of molecules of the @xmath72th type is minus the ` chemical potential ' @xmath92 , again divided by temperature . for easy reference",
    ", we list these observables and their conjugate variables below .    c|c + observable & conjugate",
    "variable + energy : @xmath1 & @xmath93 + volume : @xmath2 & @xmath94 + number : @xmath91 & @xmath95 +    now let us return to the case where @xmath96 .",
    "recalling that programs are bit strings , one important observable for programs is the length : @xmath97 we have already seen the measure @xmath98 now its significance should be clear !",
    "this is the probability measure on programs that maximizes entropy subject to the constraint that the mean length is some constant @xmath99 : @xmath100 so , @xmath101 is the conjugate variable to program length .",
    "there are , however , other important observables that can be defined for programs , and each of these has a conjugate quantity . to make the analogy to thermodynamics as vivid as possible , let us arbitrarily choose two more observables and treat them as analogues of energy and the number of some type of molecule .",
    "two of the most obvious observables are ` output ' and ` runtime ' . since levin s computable complexity measure @xcite uses the logarithm of runtime as a kind of ` cutoff ' reminiscent of an energy cutoff in renormalization , we shall arbitrarily choose the log of the runtime to be analogous to the energy , and denote it as @xmath102 following the chart above , we use @xmath103 to stand for the variable conjugate to @xmath1 .",
    "we arbitrarily treat the output of a program as analogous to the number of a certain kind of molecule , and denote it as @xmath104 we use @xmath105 to stand for the conjugate variable of @xmath3 . finally ,",
    "as already hinted , we denote program length as @xmath106 so that in terms of our earlier notation , @xmath107 .",
    "we use @xmath108 to stand for the variable conjugate to @xmath2 .",
    "c|c + observable & conjugate variable + log runtime : @xmath1 & @xmath93 + length : @xmath2 & @xmath94 + output : @xmath3 & @xmath109 +    before proceeding , we wish to emphasize that the analogies here were chosen somewhat arbitrarily .",
    "they are merely meant to illustrate the application of thermodynamics to the study of algorithms .",
    "there may or may not be a specific ` best ' mapping between observables for programs and observables for a container of gas !",
    "indeed , tadaki @xcite has explored another analogy , where length rather than log run time is treated as the analogue of energy .",
    "there is nothing wrong with this .",
    "however , he did not introduce enough other observables to see the whole structure of thermodynamics , as developed in sections [ elementary]-[cycles ] below .",
    "having made our choice of observables , we define the partition function by @xmath110 when this sum converges , we can define a probability measure on @xmath0 , the gibbs ensemble , by @xmath111 both the partition function and the probability measure are functions of @xmath8 and @xmath6 . from these",
    "we can compute the mean values of the observables to which these variables are conjugate : @xmath112 in certain ranges , the map @xmath113 will be invertible .",
    "this allows us to alternatively think of @xmath17 and @xmath9 as functions of @xmath114 and @xmath115 .",
    "in this situation it is typical to abuse language by omitting the overlines which denote ` mean value ' .",
    "the entropy @xmath22 of the gibbs ensemble is given by @xmath116 we may think of this as a function of @xmath8 and @xmath6 , or alternatively  as explained above  as functions of the mean values @xmath117 and @xmath3",
    ". then simple calculations , familiar from statistical mechanics @xcite , show that @xmath118    @xmath119    @xmath120    we may summarize all these by writing @xmath121 or equivalently @xmath122 starting from the latter equation we see : @xmath123    @xmath124    @xmath125    with these definitions , we can start to get a feel for what the conjugate variables are measuring . to build intuition ,",
    "it is useful to think of the entropy @xmath22 as roughly the logarithm of the number of programs whose log runtimes , length and output lie in small ranges @xmath126 , @xmath127 and @xmath128 .",
    "this is at best approximately true , but in ordinary thermodynamics this approximation is commonly employed and yields spectacularly good results .",
    "that is why in thermodynamics people often say the entropy is the logarithm of the number of microstates for which the observables @xmath129 and @xmath3 lie within a small range of their specified values @xcite .    if you allow programs to run longer , more of them will halt and give an answer .",
    "the * algorithmic temperature * , @xmath4 , is roughly the number of times you have to double the runtime in order to double the number of ways to satisfy the constraints on length and output .",
    "the * algorithmic pressure * , @xmath5 , measures the tradeoff between runtime and length @xcite : if you want to keep the number of ways to satisfy the constraints constant , then the freedom gained by having longer runtimes has to be counterbalanced by shortening the programs .",
    "this is analogous to the pressure of gas in a piston : if you want to keep the number of microstates of the gas constant , then the freedom gained by increasing its energy has to be counterbalanced by decreasing its volume .",
    "finally , the * algorithmic potential * describes the relation between log runtime and output : it is a quantitative measure of the principle that most large outputs must be produced by long programs .",
    "one of the first applications of thermodynamics was to the analysis of heat engines .",
    "the underlying mathematics applies equally well to algorithmic thermodynamics .",
    "suppose @xmath130 is a loop in @xmath131 space .",
    "assume we are in a region that can also be coordinatized by the variables @xmath132 .",
    "then the change in * algorithmic heat * around the loop @xmath130 is defined to be @xmath133 suppose the loop @xmath130 bounds a surface @xmath134 .",
    "then stokes theorem implies that @xmath135 however , equation ( [ differentials ] ) implies that @xmath136 since @xmath137 .",
    "so , we have @xmath138 or using stokes theorem again @xmath139    in ordinary thermodynamics , @xmath3 is constant for a heat engine using gas in a sealed piston . in this situation",
    "we have @xmath140 this equation says that the change in heat of the gas equals the work done on the gas  or equivalently , minus the work done _ by _ the gas .",
    "so , in algorithmic thermodynamics , let us define @xmath141 to be the * algorithmic work * done on our ensemble of programs as we carry it around the loop @xmath130 . beware : this concept is unrelated to ` computational work ' , meaning the amount of computation done by a program as it runs .    to see an example of a cycle in algorithmic thermodynamics ,",
    "consider the analogue of the heat engine patented by stoddard in 1919 @xcite .",
    "here we fix @xmath3 to a constant value and consider the following loop in the @xmath142 plane :    ( -.25,1)(-.25,4 ) ; ( -.25,1)(4,1 ) ; ( 1,2)(1,3.5 ) ; ( 1,3.5 ) .. controls ( 1.5 , 2.5 ) and ( 1.75 , 2.5 ) .. ( 3,2 ) ; ( 3,2)(3,1.5 ) ; ( 3,1.5 ) .. controls ( 1.5,1.75 ) and ( 1.75 , 1.75 ) .. ( 1,2 ) ; at ( -.25,2 ) [ left ] @xmath5 ; at ( 2,1 ) [ below ] @xmath2 ; at ( 1,2.75 ) [ left ] 1 ; at ( 2,2.5 ) [ above ] 2 ; at ( 3,1.75 ) [ right ] 3 ; at ( 2,1.7 ) [ below ] 4 ;    at ( 1,2 ) [ below left ] @xmath143 ; at ( 1,3.5 ) [ above left ] @xmath144 ; at ( 3,2 ) [ above right ] @xmath145 ; at ( 3,1.5 ) [ below right ] @xmath146 ;    we start with an ensemble with algorithmic pressure @xmath147 and mean length @xmath148 .",
    "we then trace out a loop built from four parts :    1 .   _",
    "isometric_. we increase the pressure from @xmath147 to @xmath149 while keeping the mean length constant .",
    "no algorithmic work is done on the ensemble of programs during this step .",
    "_ we increase the length from @xmath148 to @xmath150 while keeping the number of halting programs constant .",
    "high pressure means that we re operating in a range of runtimes where if we increase the length a little bit , many more programs halt . in order to keep the number of halting programs constant , we need to shorten the runtime significantly . as we gradually increase the length and",
    "lower the runtime , the pressure drops to @xmath151 .",
    "the total difference in log runtime is the algorithmic work done on the ensemble during this step .",
    "_ isometric .",
    "_ now we decrease the pressure from @xmath151 to @xmath152 while keeping the length constant .",
    "no algorithmic work is done during this step .",
    "_ isentropic .",
    "_ finally , we decrease the length from @xmath150 back to @xmath148 while keeping the number of halting programs constant .",
    "since we re at low pressure , we need only increase the runtime a little . as we gradually decrease the length and",
    "increase the runtime , the pressure rises slightly back to @xmath147 .",
    "the total increase in log runtime is minus the algorithmic work done on the ensemble of programs during this step .",
    "the total algorithmic work done on the ensemble per cycle is the difference in log runtimes between steps 2 and 4 .      from the elementary thermodynamic relations in section [ elementary ]",
    ", we can derive various others .",
    "for example , the so - called ` maxwell relations ' are obtained by computing the second derivatives of thermodynamic quantities in two different orders and then applying the basic derivative relations , equations ( [ derivatives1]-[derivatives3 ] ) . while trivial to prove , these relations say some things about algorithmic thermodynamics which may not seem intuitively obvious .    we give just one example here . since mixed partials commute",
    ", we have : @xmath153 using equation ( [ derivatives1 ] ) , the left side can be computed as follows : @xmath154 similarly , we can compute the right side with the help of equation ( [ derivatives2 ] ) : @xmath155 as a result , we obtain : @xmath156    we can also derive interesting relations involving derivatives of the partition function .",
    "these become more manageable if we rewrite the partition function in terms of the conjugate variables of the observables @xmath129 , and @xmath3 : @xmath157 then we have @xmath12    simple calculations , standard in statistical mechanics @xcite , then allow us to compute the mean values of observables as derivatives of the logarithm of @xmath17 with respect to their conjugate variables . here",
    "let us revert to using overlines to denote mean values : @xmath158 we can go further and compute the variance of these observables using second derivatives : @xmath159 and similarly for @xmath2 and @xmath3 .",
    "higher moments of @xmath129 and @xmath3 can be computed by taking higher derivatives of @xmath80 .",
    "so far we have postponed the crucial question of convergence : for which values of @xmath160 and @xmath6 does the partition function @xmath17 converge ?",
    "for this it is most convenient to treat @xmath17 as a function of the variables @xmath161 and @xmath162 introduced in equation ( [ new.variables ] ) .",
    "for which values of @xmath161 and @xmath162 does the partition function converge ?",
    "first , when @xmath163 the contribution of each program is 1 .",
    "since there are infinitely many halting programs , @xmath164 does not converge .",
    "second , when @xmath165 and @xmath166 the partition function converges to chaitin s number @xmath167 to see that the partition function converges in this case , consider this mapping of strings to segments of the unit interval :    ( 0,2)(0,0 ) ; ( 8,2)(8,0 ) ; ( 4,1.5)(4,0 ) ; ( 2,1)(2,0 ) ; ( 6,1)(6,0 ) ; ( 1,.5)(1,0 ) ; ( 3,.5)(3,0 ) ; ( 5,.5)(5,0 ) ; ( 7,.5)(7,0 ) ; at ( 4,1.75 ) empty ; at ( 2,1.25 ) 0 ; at ( 6,1.25 ) 1 ; at ( 1,0.75 ) 00 ; at ( 3,0.75 ) 01 ; at ( 5,0.75 ) 10 ; at ( 7,0.75 ) 11 ; at ( 0.5,.25 ) 000 ; at ( 1.5,.25 ) 001 ; at ( 2.5,.25 ) 010 ; at ( 3.5,.25 ) 011 ; at ( 4.5,.25 ) 100 ; at ( 5.5,.25 ) 101 ; at ( 6.5,.25 ) 110 ; at ( 7.5,.25 ) 111 ; at ( 4,-0.25 ) @xmath168 ; in 0,1,2,3,4 ( 0,/2)(8,/2 ) ;    each segment consists of all the real numbers whose binary expansion begins with that string ; for example , the set of real numbers whose binary expansion begins @xmath169 is [ 0.101 , 0.110 ) and has measure @xmath170 since the set of halting programs for our universal machine is prefix - free , we never count any segment more than once , so the sum of all the segments corresponding to halting programs is at most 1 .",
    "third , tadaki has shown @xcite that the expression @xmath171 converges for @xmath19 but diverges for @xmath172 it follows that @xmath173 converges whenever @xmath19 and @xmath174 .",
    "fourth , when @xmath175 and @xmath176 convergence depends on the machine .",
    "there are machines where infinitely many programs halt immediately .",
    "for these , @xmath177 does not converge .",
    "however , there are also machines where program @xmath27 takes at least @xmath178 steps to halt ; for these machines @xmath177 will converge when @xmath179 other machines take much longer to run . for these",
    ", @xmath177 will converge for even smaller values of @xmath180 .",
    "fifth and finally , when @xmath181 and @xmath182 , @xmath183 fails to converge , since there are infinitely many programs that halt and output 0 .      even when the partition function @xmath17 converges , it may not be computable .",
    "the theory of computable real numbers was independently introduced by church , post , and turing , and later blossomed into the field of computable analysis @xcite .",
    "we will only need the basic definition : a real number @xmath184 is * computable * if there is a recursive function that maps any natural number @xmath185 to an integer @xmath186 such that @xmath187 in other words , for any @xmath185 , we can compute a rational number that approximates @xmath184 with an error of at most @xmath188 . this definition can be formulated in various other equivalent ways : for example , the computability of binary digits .",
    "chaitin @xcite proved that the number @xmath189 is uncomputable .",
    "in fact , he showed that for any universal machine , the values of all but finitely many bits of @xmath13 are not only uncomputable , but random : knowing the value of some of them tells you nothing about the rest .",
    "they re independent , like separate flips of a fair coin .",
    "more generally , for any computable number @xmath19 , @xmath190 is ` partially random ' in the sense of tadaki @xcite .",
    "this deserves a word of explanation .",
    "a fixed formal system with finitely many axioms can only prove finitely many bits of @xmath190 have the values they do ; after that , one has to add more axioms or rules to the system to make any progress .",
    "the number @xmath13 is completely random in the following sense : for each bit of axiom or rule one adds , one can prove at most one more bit of its binary expansion has the value it does .",
    "so , the most efficient way to prove the values of these bits is simply to add them as axioms ! but for @xmath190 with @xmath67 , the ratio of bits of axiom per bits of sequence is less than than 1 .",
    "in fact , tadaki showed that for any computable @xmath19 , the ratio can be reduced to exactly @xmath191 .    on the other hand , @xmath183 is computable for all computable real numbers @xmath18 , @xmath19 and @xmath20 .",
    "the reason is that @xmath18 exponentially suppresses the contribution of machines with long runtimes , eliminating the problem posed by the undecidability of the halting problem .",
    "the fundamental insight here is due to levin @xcite .",
    "his idea was to ` dovetail ' all programs : on turn @xmath76 , run each of the first @xmath76 programs a single step and look to see which ones have halted . as they halt , add their contribution to the running estimate of @xmath17 .",
    "for any @xmath192 and turn @xmath193 , let @xmath194 be the location of the first zero bit after position @xmath195 in the estimation of @xmath17 . then because @xmath196 is a monotonically decreasing function of the runtime and decreases faster than @xmath194 , there will be a time step where the total contribution of all the programs that have not halted yet is less than @xmath197 .",
    "there are many further directions to explore . here",
    "we mention just three .",
    "first , as already mentioned , the ` kolmogorov complexity ' @xcite of a number @xmath76 is the number of bits in the shortest program that produces @xmath76 as output .",
    "however , a very short program that runs for a million years before giving an answer is not very practical . to address this problem , the * levin complexity * @xcite of @xmath76 is defined using the program s length plus the logarithm of its runtime , again minimized over all programs that produce @xmath76 as output . unlike the kolmogorov complexity",
    ", the levin complexity is computable .",
    "but like the kolmogorov complexity , the levin complexity can be seen as a _ relative entropy_at least , up to some error bounded by a constant .",
    "the only difference is that now we compute this entropy relative to a different probability measure : instead of using the gibbs distribution at infinite algorithmic temperature , we drop the temperature to @xmath198 .",
    "indeed , the kolmogorov and levin complexities are just two examples from a continuum of options . by adjusting the algorithmic pressure and temperature , we get complexities involving other linear combinations of length and log runtime .",
    "the same formalism works for complexities involving other observables : for example , the maximum amount of memory the program uses while running .",
    "second , instead of considering turing machines that output a single natural number , we can consider machines that output a finite list of natural numbers @xmath199 we can treat these as populations of different `` chemical species '' and define algorithmic potentials for each of them .",
    "processes analogous to chemical reactions are paths through this space that preserve certain invariants of the lists . with chemical reactions",
    "we can consider things like internal combustion cycles .",
    "finally , in ordinary thermodynamics the partition function @xmath17 is simply a number after we fix values of the conjugate variables .",
    "the same is true in algorithmic thermodynamics .",
    "however , in algorithmic thermodynamics , it is natural to express this number in binary and inquire about the algorithmic entropy of the first @xmath76 bits .",
    "for example , we have seen that for suitable values of temperature , pressure and chemical potential , @xmath17 is chaitin s number @xmath13 . for each universal machine",
    "there exists a constant @xmath63 such that the first @xmath76 bits of the number @xmath13 have at least @xmath200 bits of algorithmic entropy with respect to that machine .",
    "tadaki @xcite generalized this computation to other cases .",
    "so , _ in algorithmic thermodynamics , the partition function itself has nontrivial entropy_. tadaki has shown that the same is true for algorithmic pressure ( which in his analogy he calls ` temperature ' ) .",
    "this reflects the self - referential nature of computation .",
    "it would be worthwhile to understand this more deeply .",
    "we thank leonid levin and the denizens of the @xmath76-category caf for useful comments .",
    "ms thanks cristian calude for many discussions of algorithmic information theory .",
    "jb thanks bruce smith for discussions on relative entropy .",
    "he also thanks mark smith for conversations on physics and information theory , as well as for giving him a copy of reif s _ fundamentals of statistical and thermal physics_.          c.  s.  calude , l.  staiger , s.  a.  terwijn , on partial randomness , _ ann .",
    "pure logic_**138 * * ( 2006 ) 2030 .",
    "also available at http://www.cs.auckland.ac.nz/cdmtcs//researchreports/239cris.pdf[@xmath201http://www.cs.auckland.ac.nz/cdmtcs//researchreports/239cris.pdf@xmath202 .",
    "g.  chaitin , a theory of program size formally identical to information theory , _ journal of the acm _ * 22 * ( 1975 ) , 329340 . also available at http://www.cs.auckland.ac.nz/~chaitin/acm75.pdf[@xmath201http://www.cs.auckland.ac.nz/@xmath203chaitin/acm75.pdf@xmath202 .",
    "g.  chaitin , algorithmic entropy of sets , _ comput .",
    "appl .  _ * * 2 * * ( 1976 ) , 233245 . also available at http://www.cs.auckland.ac.nz/cdmtcs/chaitin/sets.ps[@xmath201http://www.cs.auckland.ac.nz/cdmtcs/chaitin/sets.ps@xmath202 .      e.  fredkin and t.  toffoli , conservative logic , _ _ intl .",
    "j.  theor .",
    "_ _ * * 21 * * ( 1982 ) , 219253 .",
    "also available at http://strangepaths.com/wp-content/uploads/2007/11/conservativelogic.pdf[@xmath201http://strangepaths.com/wp-content/uploads/2007/11/conservativelogic.pdf@xmath202 .",
    "e.  t.  janyes , information theory and statistical mechanics , _ phys .",
    "* 106 * ( 1957 ) , 620630 . also available at http://bayes.wustl.edu/etj/articles/theory.1.pdf[@xmath201http://bayes.wustl.edu/etj/articles/theory.1.pdf@xmath202 .",
    "e.  t.  janyes , _ probability theory : the logic of science _ , cambridge u.  press , cambridge , 2003 .",
    "draft available at http://omega.albany.edu:8008/jaynesbook.html[@xmath201http://omega.albany.edu:8008/jaynesbook.html@xmath202 .",
    "l.  a.  levin and a.  k.  zvonkin , the complexity of finite objects and the development of the concepts of information and randomness by means of the theory of algorithms , _ russian mathematics surveys _ * 256 * ( 1970 ) , 83124 also available at http://www.cs.bu.edu/fac/lnd/dvi/zl-e.pdf[@xmath201http://www.cs.bu.edu/fac/lnd/dvi/zl-e.pdf@xmath202 .",
    "m.  b.  pour - el and j.  i.  richards , _ computability in analysis and physics _ , springer , berlin , 1989 .",
    "also available at http://projecteuclid.org/euclid.pl/1235422916[@xmath201http://projecteuclid.org/euclid.pl/1235422916@xmath202 .",
    "a.  rnyi , on measures of information and entropy , _ proceedings of the 4th berkeley symposium on mathematics , statistics and probability _ , 1960 , pp .",
    "also available at http://digitalassets.lib.berkeley.edu/math/ucb/text/math_s4_v1_article-27.pdf[@xmath201http://digitalassets.lib.berkeley.edu/math/ucb/text/math_s4_v1_article-27.pdf@xmath202 .",
    "r.  j.  solomonoff , a formal theory of inductive inference , part i , _ inform .",
    "control _ * 7 * ( 1964 ) , 122 . also available at http://world.std.com/~rjs/1964pt1.pdf[@xmath201http://world.std.com/@xmath203rjs/1964pt1.pdf@xmath202",
    ".    e.  j.  stoddard , apparatus for obtaining power from compressed air , us patent 1,926,463 .",
    "available at http://www.google.com/patents?id=zlrfaaaaebaj[@xmath201http://www.google.com/patents?id=zlrfaaaaebaj@xmath202 .",
    "l.  szilard , on the decrease of entropy in a thermodynamic system by the intervention of intelligent beings , _ _ zeit .",
    "_ _ * * 53 * * ( 1929 ) 840856 .",
    "english translation in h.  s.  leff and a.  f.  rex ( eds . ) _",
    "maxwell s demon .",
    "entropy , information , computing , _ adam hilger , bristol , 1990 .",
    "k.  tadaki , a generalization of chaitin s halting probability @xmath13 and halting self - similar sets , _",
    "hokkaido math .",
    "j.  _ * * 31 * * ( 2002 ) , 219253 . also available as http://arxiv.org/abs/nlin.cd/0212001[arxiv:nlin.cd/0212001 ] .",
    "k.  tadaki , a statistical mechanical interpretation of algorithmic information theory iii : composite systems and fixed points .",
    "_ proceedings of the 2009 ieee information theory workshop , taormina , sicily , italy _ , to appear . also available as http://arxiv.org/abs/0904.0973[arxiv:0904.0973 ] ."
  ],
  "abstract_text": [
    "<S> algorithmic entropy can be seen as a special case of entropy as studied in statistical mechanics . </S>",
    "<S> this viewpoint allows us to apply many techniques developed for use in thermodynamics to the subject of algorithmic information theory . </S>",
    "<S> in particular , suppose we fix a universal prefix - free turing machine and let @xmath0 be the set of programs that halt for this machine </S>",
    "<S> . then we can regard @xmath0 as a set of ` microstates ' , and treat any function on @xmath0 as an ` observable ' . for any collection of observables </S>",
    "<S> , we can study the gibbs ensemble that maximizes entropy subject to constraints on expected values of these observables . </S>",
    "<S> we illustrate this by taking the log runtime , length , and output of a program as observables analogous to the energy @xmath1 , volume @xmath2 and number of molecules @xmath3 in a container of gas . </S>",
    "<S> the conjugate variables of these observables allow us to define quantities which we call the ` algorithmic temperature ' @xmath4 , ` algorithmic pressure ' @xmath5 and ` algorithmic potential ' @xmath6 , since they are analogous to the temperature , pressure and chemical potential . </S>",
    "<S> we derive an analogue of the fundamental thermodynamic relation @xmath7 , and use it to study thermodynamic cycles analogous to those for heat engines . </S>",
    "<S> we also investigate the values of @xmath8 and @xmath6 for which the partition function converges . at some points on the boundary of this domain of convergence , </S>",
    "<S> the partition function becomes uncomputable . </S>",
    "<S> indeed , at these points the partition function itself has nontrivial algorithmic entropy . </S>"
  ]
}