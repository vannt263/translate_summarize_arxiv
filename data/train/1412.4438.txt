{
  "article_text": [
    "the general convex optimization problems that arise in image processing take the form of a sum of two convex functions . often one function is the data fidelity energy term that is decided by the noise type and one wants to minimize , and the other function is the regularization term to make the solution have certain properties .",
    "for instance , the usual @xmath0 based regularization is used to obtain the sparse solution in the fields such as image restoration @xcite and compressed sensing @xcite . in this paper",
    ", we propose an efficient fixed - point algorithm to solve the optimization problem whose objective function is composed of two convex function , i.e. , @xmath1 where @xmath2 and @xmath3 are convex function , @xmath4 is a linear transform , and @xmath5 is differentiable with a @xmath6-lipschitz continuous gradient , i.e. ,    @xmath7    for any @xmath8 . despite its simplicity , many variational models in image processing can be formulated in the form of ( [ equ1.1 ] ) .",
    "for example , the classical total variation ( tv ) or wavelet sparsity prior based models @xcite , which are often considered in image restoration under gaussian noise , have the simple form as follows .",
    "@xmath9 where @xmath10 is a linear blurring operator , @xmath11 and @xmath12 is a regularization parameter . here",
    "@xmath13 denotes the sparse transform such as the gradient operator and the wavelet basis , and the image prior is imposed by using the term @xmath14 , which promotes the sparsity of image under the transform @xmath13 . with @xmath15 and @xmath16 the problem ( [ equ1.2 ] ) can be seen as a special case of ( [ equ1.1 ] ) .",
    "it is observed that the main difficulty of solving problem ( [ equ1.1 ] ) is that the function @xmath17 is non - differentiable .    in the last several years",
    ", many optimization algorithms have been developed for efficiently solving the variational models in image processing .",
    "the iterative shrinkage / thresholding ( ist ) algorithm is one of the most successful methods .",
    "consider the general minimization problem    @xmath18    where @xmath19 and @xmath20 are convex function , and @xmath20 is differentiable .",
    "the classical ist algorithm for problem ( [ equ1.3 ] ) is given by the following iterative formula    @xmath21    where @xmath22 is a step parameter . here",
    "@xmath23 is called the thresholding operator .",
    "it corresponds to the proximity operator @xmath24 , which is defined by @xcite    @xmath25    in different literatures , ist is also called iterative denoising method @xcite , landweber iteration @xcite , proximal forward - back splitting ( pfbs ) algorithm @xcite or fixed - point continuation ( fpc ) algorithm @xcite . in order to further accelerate the convergence speed , many new iterative shrinkage algorithms based on the ist , which include the sparsa ( sparse reconstruction by separable approximation ) @xcite , twist ( two - step ist ) @xcite , fista ( fast iterative shrinkage - thresholding algorithm ) @xcite were further proposed . notice that a proximity operator is needed to be computed in each iteration of the iterative shrinkage algorithms",
    "however , the proximity operators @xmath24 for the general case of @xmath26 often have no closed solutions .",
    "for example , if we choose @xmath27 and @xmath28 , then the minimization problem of ( [ equ1.5 ] ) is just the rudin - osher - fatemi ( rof ) denoising problem whose solution can not be obtained easily .",
    "therefore , inner iterative algorithm is needed for computing the proximity operators in most cases .    in recent years",
    ", a class of algorithms based on the splitting methods have been developed and shown to be efficient for computing the proximity operator .",
    "for instance , goldstein and osher @xcite proposed a splitting algorithm based on the bregman iteration , called the split - bregman method , to compute the solution of the minimization problem of ( [ equ1.5 ] ) especially for the case of rof denoising .",
    "this algorithm can be successfully applied for solving the general minimization problem ( [ equ1.1 ] ) , and theoretically it has been proved to be equivalent to the douglas - rachford splitting ( drs ) algorithm @xcite and the alternating direction of multiplier method ( admm ) @xcite . although the split - bregman framework has been shown to be very useful , a sub - minimization problem of solving the system of linear or nonlinear equations is included in each iteration and may time - consuming sometimes . very recently , alternating direction minimization methods based on the linearized technique @xcite have been widely investigated to overcome this and further improve the efficiency .",
    "another class of methods is the primal - dual algorithms .",
    "chambolle @xcite firstly proposed a dual algorithm for the rof denoising .",
    "later on , zhu et al .",
    "@xcite devised a primal - dual hybrid gradient ( pdhg ) method , which alternately update the primal and dual variables by the gradient descent scheme and gradient ascend scheme .",
    "the theoretical analysis on variants of the pdhg algorithm , and on the connection with the linearized version or variants of admm were widely investigated to bridge the gap between different methods .",
    "refer to @xcite and the references cited therein for details .    in this paper , we focus our attention on a new class of algorithms that has been developed very recently from the view of fixed - point . in @xcite ,",
    "jia and zhao proposed a fast algorithm for the rof denoising by simplify the original split - bregman framework .",
    "motivated by this idea , micchelli et al .",
    "@xcite designed a fixed - point algorithm based on proximity operator ( named @xmath29 ) for computing @xmath30 , which was proved to be more efficient than the splitting methods . later on ,",
    "several variants of fixed - point algorithms were proposed for special cases of image restoration .",
    "for instance , micchelli et al . @xcite",
    "further extended the @xmath29 algorithm to solve @xmath31 denoising model where @xmath32 in ( [ equ1.1 ] ) .",
    "chen et al .",
    "@xcite proposed a proximity operator based algorithm for solving indicator functions based @xmath0-norm minimization problems with application to compressed sample .",
    "krol et al .",
    "@xcite proposed a preconditioned alternating projection algorithm for emission computed tomography ( ect ) restoration , where a diagonal preconditioning matrix is used in the devised fixed - point algorithm .",
    "the extension of the @xmath29 algorithm to the more general case of the form of ( [ equ1.1 ] ) has also been investigated very recently @xcite .",
    "specifically , a primal - dual fixed point algorithm which combines the pfbs algorithm and only one inner iteration of @xmath29 has been proposed in @xcite .",
    "however , in the previous fixed - point algorithms , we observe that all the iterative formulas are composed of the gradient descent algorithm and the proximity point algorithm .",
    "it is well known that the gradient - based algorithms typically have a sub - linear convergence rate , while the newton method or the quasi - newton method has been presented with a super - linear convergence rate .",
    "this fact motivates us to propose a new fixed - point algorithm which combines the quasi - newton method and the proximity operator algorithm .",
    "furthermore , the global convergence of the proposed algorithm is investigated under certain assumption .",
    "the rest of this paper is organized in four sections . in section [ sec2 ]",
    "we briefly review the existing fixed - point algorithms based on the proximal operator , and further propose a fixed - point algorithm based on quasi - newton method . in section [ sec3 ] the global convergence of the proposed algorithm",
    "is further investigated from the point of the view of fixed point theory under certain conditions .",
    "the numerical examples on deblurring problem of images contaminated by additive gaussian noise and multiplicative noise are reported in section [ sec4 ] .",
    "the results there demonstrate that @xmath33 is superior to the recently proposed @xmath34 in the context of image deblurring .",
    "motivated by the fast algorithm proposed for the rof denoising in the literature @xcite , micchelli et al . @xcite designed a fixed - point algorithm named @xmath29 for the computation of the proximity operator @xmath35 for any @xmath36 .",
    "denote @xmath37 be the largest eigenvalue of @xmath38 .",
    "choose the parameter @xmath39 , and define the operator    @xmath40    then we can obtain the fixed - point iterative scheme which is just called @xmath29 algorithm as follows    @xmath41    where @xmath42 is the @xmath43-averaged operator of @xmath44 , i.e. , @xmath45 for any @xmath46 . calculate the fixed - point @xmath47 of the operator @xmath44 by the formula ( [ equ2.2 ] ) , and hence obtain that    @xmath48    the key technique served as the foundation of @xmath29 algorithm is the relationship between the proximity operator and the subdifferential of a convex function , as described in ( [ equ3.2 ] ) below .",
    "@xmath29 algorithm supplies a simple and efficient method of solving ( [ equ1.1 ] ) with the special case of @xmath49 in the classical framework of fixed - point iteration . in @xcite , this algorithm has been extended to the more general case that @xmath50 is bijective and the inverse can be computed easily . in particular , choose @xmath51 , where @xmath52 is a positive definite @xmath53 matrix .",
    "then ( [ equ1.1 ] ) can be reformulated as    @xmath54    define the operator    @xmath55    then the corresponding fixed - point iterative scheme for ( [ equ2.4 ] ) is given by @xmath56 and the solution @xmath57 of ( [ equ2.4 ] ) can be obtained by the formula    @xmath58    where @xmath47 is the fixed - point of the operator @xmath59 .    in order to deal with the general case of @xmath5",
    ", the authors in @xcite also combined @xmath29 and pfbs algorithms , and proposed a new algorithm named @xmath60 in which the proximity operator in the pfbs algorithm is calculated by using @xmath29 , i.e. , @xmath61 is calculated by @xmath29 .",
    "notice that a inner iteration of solving @xmath62 is included in @xmath60 , and it is problematic to set the approximate iteration number to balance the computational time and precision . in order to solve this issue , a primal - dual fixed points algorithm based on proximity operator ( @xmath63 )",
    "@xcite was proposed very recently . in this algorithm , instead of implementing @xmath29 for many iteration steps to calculate @xmath64 in @xmath60 , only one inner fixed - point iteration is adopted .",
    "suppose @xmath65 .",
    "then we can obtain the following iteration scheme ( @xmath63 ) :    @xmath66    it is obvious that @xmath67 is the primal variable related to ( [ equ1.1 ] ) , and according to the thorough study in @xcite we know that the variable @xmath68 is just the dual variable of the primal - dual form related to ( [ equ1.1 ] ) .",
    "therefore , @xmath63 also belongs to the class of primal - dual algorithm framework .",
    "similarly to @xmath29 , a relaxation parameter @xmath69 can be introduced to get the algorithm named @xmath34 . for more details",
    "refer to @xcite .",
    "in the @xmath63 algorithm , the iterative formulas consist of the proximity operator and the gradient descent algorithm .",
    "since the newton - type methods have been shown to have a faster convergence rate compared to the gradient - based methods , a very nature idea is to use the newton - type methods instead of the gradient descent step in the fixed - point algorithm . consider the minimization problem ( [ equ1.1 ] ) .",
    "we use the second - order taylor expansion of the convex function @xmath70 at the recent iterative point @xmath71 instead of it , i.e. , @xmath72 where @xmath73 is a positive definite symmetric matrix to approximate the second derivative @xmath74 .",
    "then ( [ equ1.1 ] ) can be reformulated as    @xmath75    it is observed that ( [ equ2.9 ] ) corresponds to the minimization problem ( [ equ2.4 ] ) with @xmath76 . therefore , the next iteration scheme @xmath77 can be obtained by the fixed point iteration algorithm shown in ( [ equ2.5])([equ2.7 ] ) . in order to avoid any inner iterations",
    ", we use only one inner fixed point iteration in the proposed algorithm . for this , choose @xmath65 , and define @xmath78 using the numerical solution @xmath79 for @xmath80 as the initial value , and only implementing one iteration of solving the fixed - point of @xmath81 , we can obtain the following iteration scheme    @xmath82    setting @xmath83 .",
    "it is observed that an intermediate iterative variable @xmath84 is generated by a quasi - newton method .",
    "therefore , we called the proposed algorithm a fixed - point algorithm based on quasi - newton method , and abbreviate it as @xmath85 , which is described as * algorithm 1 * below . for simplification of convergence analysis below",
    ", we set @xmath86 to be unchanged with different @xmath87 .",
    "set @xmath88 , @xmath89 , @xmath90 . +",
    "* main iteration * : +  @xmath91 ; +  @xmath92 ; +  @xmath93 .",
    "+    similarly to the literatures @xcite , we can introduce a relaxation parameter @xmath94 $ ] to obtain * algorithm 2 * , which is exactly the picard iterates with the parameter .",
    "set @xmath88 , @xmath89 , @xmath90 . +",
    "* main iteration * : +  @xmath91 ; +  @xmath95 ; +  @xmath96 .",
    "+  @xmath97 ; +  @xmath98 .",
    "let us start with some related notations and conclusions which will serve as the foundation for the proof below .",
    "[ def3.1 ] ( nonexpansive operator ) a nonlinear operator @xmath99 is called nonexpansive if for any @xmath100 , @xmath101 a nonlinear operator @xmath102 is called firmly nonexpansive if for any @xmath100 , @xmath103    by the application of the cauchy - schwarz inequality it is easy to show that a firmly nonexpansive operator is also nonexpansive .",
    "[ def3.2 ] ( picard sequence @xcite)for a given initial point @xmath104 and an operator @xmath102 , the sequence @xmath105 generated by @xmath106 is called the picard sequence of the operator @xmath107 .    for the picard sequence",
    "we have the following conclusion .    [ pro2 ] ( opial @xmath43-averaged theorem @xcite )",
    "let @xmath108 be a closed convex set in @xmath109 and let @xmath110 be a nonexpansive mapping with at least one fixed point .",
    "then for any @xmath111 and any @xmath69 , the picard sequence of @xmath112 converges to a fixed point of @xmath107 .    for any convex function @xmath113 ,",
    "the subdifferential of @xmath19 at @xmath36 is defined by    @xmath114    the following result illustrates the relationship between the proximity operator and the subdifferential of a convex function",
    ". this conclusion has appeared in many previous literatures , such as @xcite .",
    "[ pro1 ] if @xmath19 is a convex function defined on @xmath115 and @xmath116 , then @xmath117    in what follows , we establish a fixed - point formulation for the solution of the minimization problem ( [ equ1.1 ] ) based on the conclusion in proposition [ pro1 ] . to this end",
    ", we define the operator @xmath118 as    @xmath119    and the operator @xmath120 as    @xmath121    where @xmath122 is a positive parameter .",
    "denote the operator @xmath123 as @xmath124    [ the1 ] if @xmath57 is a solution of the minimization problem ( [ equ1.1 ] ) , then there exists @xmath125 that satisfies : @xmath126 which implies that @xmath127 is a fixed point of @xmath128 .",
    "conversely , if @xmath129 is a fixed point of @xmath128 , then @xmath57 is a solution of the minimization problem ( [ equ1.1 ] ) .",
    "since @xmath57 is one solution of the minimization problem ( [ equ1.1 ] ) , by the first - order optimality condition we have @xmath130 denote @xmath131 .",
    "then we obtain that    @xmath132    besides , using proposition [ pro1 ] we have @xmath133 inserting ( [ equ3.6 ] ) into ( [ equ3.7 ] ) we further obtain that @xmath134 based on ( [ equ3.6 ] ) and ( [ equ3.8 ] ) we infer that @xmath127 is a fixed point of @xmath128 .",
    "conversely , if there exists @xmath129 satisfying @xmath135 , we can derive that @xmath57 satisfies the first - order optimality condition of ( [ equ1.1 ] ) by the equivalent formulas above .",
    "therefore , we conclude that @xmath57 is a solution of ( [ equ1.1 ] ) .    according to the formulas in ( [ equ3.3 ] ) and ( [ equ3.4 ] ) we find out that the iterative scheme of @xmath85 can be reformulated as @xmath136 which is also equal to @xmath137 with @xmath138 .",
    "this implies that the sequence @xmath139 generated by @xmath85 is just the picard sequence of the operator @xmath128 . with the similar discussion we can find that the iterative formulas of @xmath33 is equal to @xmath140 , i.e.",
    ", the sequence @xmath139 generated by @xmath33 is the picard sequence of the operator @xmath141 .",
    "based on theorem [ the1 ] we know that the solution of the minimization problem ( [ equ1.1 ] ) is just equal to the fixed point of the operator @xmath128 .",
    "therefore , the convergence of @xmath33 can be guaranteed by verifying the nonexpansion of @xmath128 according to proposition [ pro2 ] .",
    "the proof here is similar to those presented in @xcite .",
    "however , note that the global convergence of proposed algorithms can not be directly obtained by applying results in @xcite , and hence it is included here for completion .    in the following , we give a crucial inequality for showing the nonexpansion of @xmath128 .",
    "denote @xmath142 @xmath143 here we assume that @xmath144 , and hence @xmath145 is a symmetric positive semi - definite matrix .",
    "therefore , we can define the semi - norm @xmath146 , and then define the norm @xmath147    [ lem1 ] for any two points @xmath148 and @xmath149 in @xmath150 , the following inequality @xmath151 comes into existence .    according to lemma  2.4 of @xcite",
    "we know that @xmath152 is firmly nonexpansive , and hence obtain    @xmath153    following the definition in ( [ equ3.4 ] ) we also have    @xmath154    according to ( [ equ3.10 ] ) and ( [ equ3.11 ] ) we further have    @xmath155    since @xmath5 has @xmath6-lipschitz continuous gradient , we get that    @xmath156    by the definition of @xmath145 we easily get    @xmath157    based on ( [ equ3.12])([equ3.14 ] ) we can obtain ( [ equ3.9 ] ) directly .    from the results in lemma [ lem1 ] we know that @xmath128 is nonexpansive with the norm of @xmath158",
    ". therefore , we are able to prove the convergence of @xmath33 according to proposition [ pro2 ] , which is described as follows .",
    "[ the2 ] assume that @xmath159 and @xmath90 .",
    "let @xmath160 be the sequence generated by @xmath33 .",
    "then @xmath160 converges to the fixed point of @xmath128 and @xmath71 converges to the solution of ( [ equ1.1 ] ) .",
    "note that the solution of ( [ equ1.1 ] ) is just one fixed point of @xmath128 . from lemma [ lem1 ]",
    "we know that the operator @xmath128 is nonexpansive , maps the set @xmath150 to itself , and has at least one fixed point . according to opial @xmath43-averaged theorem",
    ", we conclude that , for any @xmath161 and @xmath69 , the picard sequence of @xmath141 converges to a fixed point of @xmath128 . with this result",
    "we further infer that @xmath71 converges to the solution of ( [ equ1.1 ] ) .    finally , we process with the convergence of @xmath85 based on the inequality in lemma [ lem1 ] .",
    "[ the3 ] assume that @xmath159 and @xmath90 .",
    "let @xmath160 be the sequence generated by @xmath85 .",
    "then @xmath160 converges to the fixed point of @xmath128 and @xmath71 converges to the solution of ( [ equ1.1 ] ) .",
    "let @xmath162 be a fixed point of @xmath128 .",
    "substitute @xmath163 and @xmath164 in ( [ equ3.9 ] ) with @xmath138 and @xmath165 , we obtain that    @xmath166    summing ( [ equ3.15 ] ) from some @xmath167 to @xmath168 we obtain that    @xmath169    which implies that    @xmath170    @xmath171    @xmath172    according to ( [ equ3.18 ] ) we can easily deduce that @xmath173 , and combining it with ( [ equ3.19 ] ) we have    @xmath174    besides , from ( [ equ3.6 ] ) we know that @xmath175 , and hence obtain that    @xmath176    based on ( [ equ3.21 ] ) we immediately get    @xmath177    since @xmath178 , utilizing ( [ equ3.17 ] ) and ( [ equ3.18 ] ) we can deduce from ( [ equ3.22 ] ) that    @xmath179    combining ( [ equ3.20 ] ) and ( [ equ3.23 ] ) we further obtain that    @xmath180    according to ( [ equ3.15 ] ) we know that the sequence @xmath181 is non - increasing , and hence @xmath182 is bounded , which implies that there exists a convergent subsequence of @xmath183 such that    @xmath184    for some point @xmath185 . due to the operator @xmath128 is continuous , we further have @xmath186 . besides , we have    @xmath187    which implies that    @xmath188    based on ( [ equ3.24 ] ) and ( [ equ3.25 ] ) . therefore , @xmath189 is a fixed point of the operator @xmath128 . due to the proof",
    "is started with any fixed point @xmath165 , we can set @xmath190 . in this case",
    ", we see that the sequence @xmath191 is non - increasing . combining this with the formula ( [ equ3.25 ] )",
    "we infer that    @xmath192    from theorem [ the1 ] we know that @xmath193 is a solution of ( [ equ1.1 ] ) .",
    "this completes the proof .",
    "in this section , we will compare the proposed @xmath33 algorithm with @xmath34 @xcite through the experiments of image restoration . here",
    "two cases of additive and multiplicative noise types are considered .",
    "one is the additive gaussian noise which has been extensively investigated over the last decades . in this setting",
    ", the data fidelity term can be formulated as @xmath194 where @xmath195 is the blurring operator , and @xmath196 is the observed image .",
    "the other is the speckle noise which also appears in many real world image processing applications such as laser imaging , synthetic aperture radar ( sar ) imaging and ultrasonic imaging . in @xcite , this speckle noise followed by a rayleigh distribution",
    "is investigated . under this condition",
    ", the observed image can be modeled as corrupted with signal - dependent noise of this form    @xmath197    where @xmath198 is a zero - mean gaussian noise with standard deviation @xmath199 , i.e. , @xmath200 .",
    "based on the model ( [ equ4.1 ] ) and the characteristics of gaussian distribution , the corresponding fidelity term can be formulated as @xmath201    in the following experiments , we use total variation as the regularization term , and hence choose the function @xmath202 where @xmath203 is a discrete gradient operator . here",
    "we adopt the isotropic definition of total variation , and the proximity operator @xmath204 can be computed easily . for more details refer to @xcite .      in this subsection",
    ", we choose three gray - scale images , cameraman , barbara ( with size of @xmath205 ) , and boat ( with size of @xmath206 ) as the original images , and evaluate @xmath33 in four typical image blurring scenarios : strong blur with low noise ; strong blur with medium noise ; mild blur with low noise ; mild blur with medium noise , which are summarized in table [ tab4.1 ] ( @xmath199 and @xmath207 denote the standard deviation ) .",
    "[ htbp ]    [ tab4.1 ]    in the following , we discuss the selection of the parameters @xmath208 , @xmath122 and @xmath43 in both fixed point algorithms , the parameter @xmath209 in @xmath34 , and the matrix @xmath52 in @xmath33 . due to @xmath210 , we have @xmath211",
    ". therefore , we can choose @xmath212 .",
    "however , the blurring operator @xmath195 is ill - posed generally , and @xmath213 can not be used in the proposed algorithm due to the instability .",
    "therefore , we choose @xmath214 in our experiments . here",
    "@xmath215 is a small positive number and @xmath13 is a difference matrix .",
    "notice that the introduction of the term @xmath216 avoids the ill - posed condition , and @xmath217 can also be computed efficiently by fast fourier transforms ( ffts ) with periodic boundary conditions .",
    "the regularization parameter @xmath208 is decided by the noise level , and the adjustment of the parameters @xmath122 and @xmath209 does influence the convergence speed and stability of the fixed point algorithms . through",
    "many trials we use the rules of thumb : @xmath208 is set to @xmath218 and @xmath219 for @xmath220 and @xmath221 respectively ; @xmath122 is set to @xmath222 ; @xmath209 is chosen to be @xmath223 for @xmath34 ; and @xmath224 for @xmath33 . similarly to the literatures @xcite , we find that @xmath65 achieves the best convergence speed compared with other @xmath69 , and hence we choose @xmath65 for both algorithms .",
    "the performance of the restored images of the compared algorithms is measured quantitatively by means of the peak signal - to - noise ratio ( psnr ) , which is defined by    @xmath225    where @xmath67 and @xmath57 denote the original image and the restored image respectively . the stopping criterion for the fixed - point algorithms",
    "is defined such that the relative error is below some small constant , i.e. ,    @xmath226    where tol denotes a prescribed tolerance value . in our experiments we choose @xmath227 .",
    "the psnr values for the deblurred images , the number of iterations , and the cpu time are listed in table [ tab4.2 ] . in this table ,",
    "the four image blurring scenarios shown in table [ tab4.1 ] are considered , and @xmath228 represents the psnr values , iteration numbers and cpu time in sequence . from these results we observe that the recovered images obtained by @xmath33 can achieve better psnrs than those given by @xmath34 , and meanwhile , the corresponding iteration number and running time of @xmath33 is less than those of @xmath34 .",
    "figures [ fig4.1][fig4.3 ] show the recovery results of the @xmath34 and @xmath33 algorithms .",
    "it is observed that the visual qualities of images obtained by both algorithms are more or less the same .",
    "[ htbp ]    [ tab4.2 ]    figure [ fig4.4 ] shows the evolution curves of psnr values obtained by both fixed point algorithms for two cases including in table [ tab4.2 ] : one is the cameraman image blurred by @xmath229 box average kernel and added with gaussain noise with @xmath230 , the other is the boat image blurred by @xmath231 gaussian kernel and added with gaussain noise with @xmath232 . from the plots we can implicitly find that @xmath33 achieves the best solution ( with higher psnrs ) much faster than @xmath34 .      in this subsection",
    ", we further discuss the case of images contaminated by rayleigh noise .",
    "the corresponding minimization problem has been introduced above .",
    "the two blur kernels shown in table [ tab4.1 ] , and rayleigh noise with @xmath233 and @xmath234 are considered here .",
    "first of all , we illustrate the setting of the parameters in both fixed point algorithms . since @xmath235",
    ", we have that @xmath236 for any @xmath237 . notice that the value of @xmath74 changes with the iteration number , and the inverse of @xmath74 is difficult to be estimated .",
    "therefore , we use @xmath238 to approximate @xmath74 in the proposed fixed point algorithm . here",
    "the parameter @xmath239 is used to replace the unknown @xmath240 , and the term @xmath216 is included to avoid the ill - posed condition . in the following experiments , we find that @xmath241 and @xmath242 are two suitable selection through many trials .",
    "moreover , for parameters in both algorithms we use the following rules of thumb : the regularization parameter @xmath208 is chosen to be @xmath243 and @xmath244 for the noise level of @xmath233 and @xmath234 respectively ; @xmath122 is set to @xmath222 ; @xmath209 is chosen to be @xmath245 for @xmath34 .",
    "we also find out that the selection of @xmath65 is suitable for our experiments here .    in what follows , two images , pepper ( with size of @xmath205 ) and cameraman ,",
    "are used for our test .",
    "figures [ fig4.5][fig4.6 ] show the evolution curves of psnr ( db ) running both fixed point algorithms for the two images . from the plots we observe that the psnr values obtained by @xmath33 increase much faster than those by @xmath34 .",
    "this is due to the quasi - newton method included in @xmath33 is more efficient than the gradient descent algorithm involved in @xmath34 .",
    "figure [ fig4.7 ] shows the deblurred results of pepper image convoluted by @xmath229 box average kernel and contaminated by rayleigh noise with @xmath246 .",
    "the corresponding psnr values , the number of iterations , and the cpu time are also included . it is observed that @xmath33 can obtain higher psnr values with less iteration number and running time compared to @xmath34 .",
    "the recovery results of cameraman image blurred by @xmath231 gaussian kernel and corrupted by rayleigh noise with @xmath246 are also presented in figure [ fig4.8 ] .",
    "we also observe that @xmath33 is more efficient than @xmath34 , especially in the implementation time .",
    "in this article , we propose a fast fixed point algorithm based on the quasi - newton method , abbreviated as @xmath33 , for solving the minimization problems with the general form of @xmath247 .",
    "the main distinction between @xmath33 and previous fixed point algorithms lies in that the quasi - newton method , rather than the gradient descent algorithm , is included in the algorithm framework .",
    "the proposed algorithm framework is applied to solve tv - based image restoration problem .",
    "numerical experiments reported in this paper indicate that @xmath33 outperform the recently proposed @xmath34 , especially in the implementation time .",
    "the research was supported in part by the national natural science foundation of china under grant 61271014 .",
    "s. ma , w. yin , y. zhang , and a. chakraborty , an efficient algorithm for compressed mr imaging using total variation and wavelets , ieee international conference on computer vision and pattern recognition ( cvpr ) 2008 , ( 2008 ) .",
    "e. j. cand@xmath248s , j. romberg , and t. tao , robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , ieee transactions on information theory , 52 ( 2006 ) , pp .",
    "489 - 509 .",
    "hale e , yin w , zhang y. a fixed - point continuation method for l1-regularized minimization with applications to compressed sensing .",
    "rice university : department of computational and applied mathematics , 2007 .",
    "pock , t. , and chambolle , a. ( 2011 ,",
    "november ) .",
    "diagonal preconditioning for first order primal - dual algorithms in convex optimization . in computer vision",
    "( iccv ) , 2011 ieee international conference on ( pp .",
    "1762 - 1769 ) . ieee"
  ],
  "abstract_text": [
    "<S> solving an optimization problem whose objective function is the sum of two convex functions has received considerable interests in the context of image processing recently . in particular , we are interested in the scenario when a non - differentiable convex function such as the total variation ( tv ) norm is included in the objective function due to many variational models established in image processing have this nature . in this paper , we propose a fast fixed point algorithm based on the quasi - newton method for solving this class of problem , and apply it in the field of tv - based image deblurring . </S>",
    "<S> the novel method is derived from the idea of the quasi - newton method , and the fixed - point algorithms based on the proximity operator , which were widely investigated very recently . utilizing the non - expansion property of the proximity operator we further investigate the global convergence of the proposed algorithm . </S>",
    "<S> numerical experiments on image deblurring problem with additive or multiplicative noise are presented to demonstrate that the proposed algorithm is superior to the recently developed fixed - point algorithm in the computational efficiency .    </S>",
    "<S> newton method ; primal - dual ; fixed - point algorithm ; total variation ; rayleigh noise </S>"
  ]
}