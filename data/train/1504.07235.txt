{
  "article_text": [
    "in this paper , we focus on the idea of `` sign @xmath0-stable random projections '' and the applications in machine learning with massive ( and possibly streaming  @xcite ) data .",
    "consider two data vectors @xmath11 from a data matrix , the central idea is to multiply them with a random projection matrix @xmath12 , @xmath13 , whose entries , @xmath14 , are sampled i.i.d . from an @xmath0-stable distribution , denoted by @xmath15 .",
    "that is , @xmath16    the use of @xmath0-stable distributions was studied in the context of estimating frequency moments of data streams  @xcite and in the recent work on `` one scan 1-bit compressed sensing ''  @xcite . here",
    ", we adopt the parameterization  @xcite such that , if @xmath17 , then the characteristic function is @xmath18 . when @xmath7 , @xmath19 is equivalent to a gaussian distribution @xmath20 .",
    "when @xmath3 , @xmath21 is the standard cauchy distribution .",
    "although in general no closed - form density functions of @xmath0-stable distributions are available , one can easily sample from an @xmath0-stable distribution by ( e.g. , ) the classical cms  @xcite method .    stable distributions with @xmath22",
    "are also known to be `` heavy - tailed '' distributions because if @xmath23 , then unless @xmath7 , we always have @xmath24 if @xmath25 .",
    "this is probably the reason why stable distributions were rarely used in machine learning and data mining applications .      by property of stable distributions",
    ", we have @xmath26 and @xmath27 , @xmath28 .",
    "unless @xmath2 , it might be difficult to imagine how one can make use of these ( manually generated ) heavy - tailed data for of machine learning applications .",
    "indeed , we do not directly use the projected data . instead ,",
    "in this paper , we only utilize the projected data through their signs , i.e. , @xmath29 and @xmath30 , which are well - behaved and can be used for building tools for large - scale machine learning .",
    "if @xmath31 , we can code @xmath32 as a two - dimensional vector @xmath33 $ ] .",
    "if @xmath34 , then we code it as @xmath35 $ ] .",
    "then we concatenate @xmath36 such two - dimensional vectors to form a vector of length @xmath37 ( with @xmath36 1 s ) .",
    "we apply the same coding scheme to @xmath38 ( and all the projected data ) .",
    "the signs , @xmath29 and @xmath30 , are statistically dependent and it is interesting ( and in general challenging ) to find out how the signs are related . +",
    "when @xmath7 , the relationship between @xmath29 and @xmath30 is well - known  @xcite @xmath39 thus , the `` collision probability '' is monotonic in @xmath40 , which is the correlation coefficient .",
    "although @xmath41 is nonlinear , the estimator of the probability , i.e. , @xmath42 can be viewed as an inner product once we expand a sign as either @xmath33 $ ] or @xmath35 $ ] .",
    "in other words , we only need to pay the cost of linear learning to approximately train a classifier originally based on nonlinear kernels .",
    "it is not so straightforward to calculate the collision probability once @xmath22 .",
    "a recent work  @xcite focused on @xmath3 and showed that , when @xmath43 , we have @xmath44 note that the so - called @xmath4-kernel , @xmath45 , is popular in computer vision , for data generated from histograms .",
    "when @xmath5 , @xcite mentioned in the `` future work '' that the collision probability is related to the `` resemblance '' when the data are nonnegative : @xmath46 interestingly , this collision probability is essentially the same as the collision probability of `` 1-bit minwise hashing ''  @xcite . + for other @xmath0 values , at this moment we can not relate the collision probabilities to any known similarity measures . on the other hand ,",
    "the estimator @xmath42 ( which is an inner product ) is of course still a valid positive definite kernel for any @xmath0 .",
    "thus , we can anyway use sign @xmath0-stable random projections for building large - scale learning algorithms , where @xmath0 can be viewed as an important tuning parameter .",
    "what is missing in the literature is an extensive empirical study and our paper supplies such a study .",
    "as mentioned above , the collision probability of sign stable random projections at @xmath47 is related to the resemblance @xmath48 when the data ( e.g. , @xmath49 and @xmath50 ) are nonnegative . from the definition @xmath51 we can see that @xmath48 only makes sense when the data are sparse ( i.e. , most entries are zero ) .",
    "when the data are fully dense , we have @xmath52 always .",
    "this may seriously limit the use of resemblance when the data are not sparse .",
    "this issue can be largely fixed by the introduction of the min - max kernel which is defined as @xmath53 the recent work  @xcite also provides a variant , called the `` normalized min - max kernel '' : @xmath54    the resemblance is a popular measure of similarity for binary data and can be sampled efficiently by minwise hashing  @xcite .",
    "the min - max kernels can also be sampled using the technique called consistent weighted sampling ( cws )  @xcite .",
    "traditionally , each sample of cws consists of two values , one of which is unbounded .",
    "the so - called  0-bit  cws  @xcite simply discarded the unbounded value to make cws much more convenient for large - scale machine learning tasks .    because  @xcite experimented with a large collection of datasets , we hope to compare , shoulder - by - shoulder , sign stable random projections with 0-bit cws , although we should reiterate that 0-bit cws is only designed for nonnegative data and is hence not as general as sign stable random projections .",
    "we have experimented all the 34 datasets used in the recent paper for  0-bit cws ",
    "@xcite to provide a shoulder - by - shoulder comparison .",
    "the results are summarized in table  [ tab_data ] .",
    "the results show that , given enough projections , sign @xmath0-stable random projections can often achieve good accuracies ( and better than linear ) . the value of @xmath0 is an important parameter which needs to be individually tuned for each dataset .",
    "[ tab_data ]      figures  [ fig_covertype10ksrp ] to  [ fig_segmentsrp ] presents the detailed classification results of sign @xmath0-stable random projections for selected 4 datasets , using @xmath55-regularized linear svm ( with a regularization parameter @xmath56 $ ] ) . in each figure , we present the results for @xmath57 projections and @xmath58 .",
    "all experiments were conducted using liblinear  @xcite and we repeated each randomized experiment 5 times and reported the average results .",
    "the classification results are very stable ( i.e. , very small variance ) unless @xmath36 is too small .",
    "the results ( together with table  [ tab_data ] and other figures later in the paper ) show that , given enough projections ( e.g. , @xmath59 ) , the method of sign @xmath0-stable random projections can typically achieve good accuracies .",
    "figures  [ fig_cws1 ] to  [ fig_cws4 ] compare sign @xmath0-stable random projections with 0-bit cws  @xcite on selected datasets . for clarity",
    ", we only show the results of sign stable random projections for @xmath60 projections , and the results for 0-bit cws with @xmath61 samples .",
    "these results demonstrate that 0-bit cws requires much fewer samples , although we should keep in mind that 0-bit cws is only for nonnegative data .",
    "the paper on 0-bit cws  @xcite only experimented with datasets of moderate sizes for an important reason . to prove the correctness , they need to show that the result of 0-bit cws with enough samples could approach that of exact min - max kernel",
    ". a straightforward and faithful implementation of svm with min - max kernel is to use the libsvm pre - computed kernel functionality by computing the kernel explicitly and feeding it to svm from outside .",
    "this strategy , although most repeatable , is very expensive for datasets which are not even large  @xcite . on other hand",
    ", once we have proved the correctness of 0-bit cws , applying the method to larger datasets is easy , except that we would not be able to compute the exact result of min - max kernel .",
    "figure  [ fig_webspamn1 ] presents the detailed results on the _ webspamn1 _",
    "dataset , which has 350,000 examples .",
    "we use 50% of the examples for training and the other 50% for testing . with linear svm ,",
    "the test classification accuracy is about @xmath62 .",
    "both sign @xmath0-stable random projections and 0-bit cws can achieve @xmath63 accuracies given enough samples .",
    "the figure also confirm that 0-bit cws requires significantly fewer samples than the number of projections needed by sign stable random projections , to achieve comparable accuracies .",
    "this paper provides an extensive empirical study of sign @xmath0-stable random projections for large - scale learning applications .",
    "although the paper focuses on presenting the results on classification tasks , one should keep mind that the method is a general - purpose data processing tool which can be used for classification , regression , clustering , or near - neighbor search .",
    "given enough projections , the method can often achieve good performance . the comparison with 0-bit cws should be also interesting to practitioners .",
    "+ * future work * :   the processing cost of sign @xmath0-stale random projections can be substantially improved by `` very sparse stable random projections ''  @xcite .",
    "an empirical study is needed to confirm this claim .",
    "another interesting line of research is to combine sign stable random projections with 0-bit cws , for example , by a strategy similar to that in the recent work of `` core kernels ''  @xcite ."
  ],
  "abstract_text": [
    "<S> in this paper , we study the use of `` sign @xmath0-stable random projections '' ( where @xmath1 ) for building basic data processing tools in the context of large - scale machine learning applications ( e.g. , classification , regression , clustering , and near - neighbor search ) . after the processing by sign stable random projections , the inner products of the processed data approximate various types of nonlinear kernels depending on the value of @xmath0 . </S>",
    "<S> thus , this approach provides an effective strategy for approximating nonlinear learning algorithms essentially at the cost of linear learning . </S>",
    "<S> when @xmath2 , it is known that the corresponding nonlinear kernel is the arc - cosine kernel . </S>",
    "<S> when @xmath3 , the procedure approximates the arc - cos-@xmath4 kernel ( under certain condition ) . </S>",
    "<S> when @xmath5 , it corresponds to the resemblance kernel , which provides the exciting connection between two popular randomized algorithms : ( i ) stable random projections ( ii ) @xmath6-bit minwise hashing . </S>",
    "<S> no theoretical results are known so far for other @xmath0 values except for @xmath7 , 1 , or @xmath8 . </S>",
    "<S> + from practitioners perspective , the method of sign @xmath0-stable random projections is ready to be tested for large - scale learning applications , where @xmath0 can be simply viewed as a tuning parameter . </S>",
    "<S> what is missing in the literature is an extensive empirical study to show the effectiveness of sign stable random projections , especially for @xmath9 or 1 . </S>",
    "<S> the paper supplies such a study on a wide variety of classification datasets . in particular , we compare shoulder - by - shoulder sign stable random projections with the recently proposed `` 0-bit consistent weighted sampling ( cws ) ''  @xcite ( which is only for nonnegative data ) . </S>",
    "<S> we provide the detailed comparisons on all the 34 datasets used by  @xcite . </S>",
    "<S> in addition , we present the comparison on a larger dataset with 350,000 examples . for all datasets , we experiment with @xmath10 . for most datasets , </S>",
    "<S> sign stable random projections can approach ( or in some cases even slightly exceed ) the performance of 0-bit cws , given enough projections . </S>",
    "<S> typically , to reach the same accuracy , sign stable random projections would require significantly more projections than the number of samples needed by 0-bit cws . </S>",
    "<S> there are also datasets for which sign stable random projections could not achieve the same accuracy as 0-bit cws regardless of @xmath0 . </S>",
    "<S> + while the comparison results seem to favor 0-bit consistent weighted sampling ( which is only for nonnegative data ) , the distinct advantage of sign stable random projections is that the method is applicable to general data types , not only for nonnegative data . </S>",
    "<S> it is also an interesting research problem to combine 0-bit cws with sign stable random projections , for example , a strategy similar to `` core kernels ''  @xcite . </S>"
  ]
}