{
  "article_text": [
    "the problem of sorting a pile of objects using a robot is interesting in its own right , but in our case the problem is firmly rooted in an industrial application : waste sorting .",
    "zenrobotics robots have been sorting waste on industrial waste processing sites since 2014 .",
    "our robots have picked up approximately 4,200 tons of metal , wood , stone and concrete from the conveyor .",
    "performance of the robots in this environment is critical for paying back the investment .",
    "currently the robots are able to identify , pick and throw objects of up to 20 kg in less than 1.8 seconds , 24/7 . the current generation robot @xcite was taught to grasp objects using human annotations and a reinforcement learning algorithm .    because of the variability of waste , the ability to recognize , grasp and manipulate an extremely wide variety of objects is crucial . in order to provide this ability in a cost - effective way , new training methods , which do not rely on hardcoding or human annotation are required .",
    "for example , changing the shape of the gripper or adding degrees of freedom might require all picking logic to be rewritten or at least labor - intensive retraining unless the system is able to learn to sort using the new gripper or degrees of freedom by itself .    in order to make our robots suitable for an industrial site",
    ", they are built to be as simple and robust as possible : the robots are built from cots ( common off - the - shelf ) parts , having only four degrees of freedom for position and one for gripper opening .",
    "the robot used for the experiment in this article is a prototype of our current production version .      in this section ,",
    "we discuss the robotic waste sorting problem in a more formal setting .",
    "objects that belong to various object classes arrive and are manipulated by the robotic system into different chutes ( end locations ) . for each object class",
    ", the chute into which it should be placed is defined .",
    "the waste sorting problem for the robot is then a multi - objective optimization problem with three criteria .",
    "the combination of the criteria that is optimized depends on the business case of the customer , but usually the true objective is a monotonous nonlinear function of these three criteria .",
    "the first criterion is the purity , i.e. , the percentage of the total weight of objects deposited into a chute that belong to that chute .",
    "purity essentially determines whether the pile of sorted objects is resalable ( i.e. , recyclable ) or not .",
    "the second criterion is the recovery rate , i.e. , the percentage of the total weight of objects of a class that were deposited into the correct chute .",
    "this is of interest in cases in which minimizing the amount of material that ends up in a landfill site is important .",
    "the third criterion is the throughput , i.e. , the sum of objects weights handled by the system per unit time .",
    "the throughput affects how many systems are required in parallel to sort a certain amount of waste .",
    "the waste sorting problem involves a manipulation task similar to the more studied problems of `` cleaning a table by grasping '' @xcite and bin picking @xcite , but also differs from them in several aspects :    1 .",
    "it is not sufficient to be able to move the objects , they need to be recognized as well in order to deposit each object into the correct chute .",
    "2 .   picking several objects at once is acceptable  even desirable , provided that they are of the same class .",
    "the objects are generally novel and there is a large selection of different objects .",
    "objects can be broken irregularly .",
    "the distribution of the objects has a long tail and completely unexpected objects occasionally appear .",
    "the objects are placed on the conveyor belt by a random process and easily form random piles .",
    "5 .   on the other hand",
    ", this problem is made slightly easier by the fact that it is not necessary to be gentle to the objects ; fragile objects will likely have been broken by previous processes already .",
    "scratching or colliding with objects does not cause problems as long as the robot itself can tolerate it ( see fig .",
    "[ fig : gripper ] ) .",
    "state - of - the - art results for grasping novel objects and grasping objects in unstructured environments such as in dense clutter generally use machine learning to evaluate and choose the best among possible grasps @xcite .",
    "a recent trend is making the robot learn a grasping task completely autonomously , using active learning . in order to achieve this , the robotic system must generate the feedback data autonomously .",
    "grasps are automatically annotated as success or failure based on sensors in the gripper @xcite or visual feedback , e.g. , by comparing images of the table before and after dropping a supposedly grasped object @xcite .",
    "once automatic feedback has been implemented , much larger amounts of training data can be obtained than before and the robotic system can be adapted to different circumstances simply by letting it learn the required behaviour in the new circumstances .",
    "our main contribution is an autonomous robotic system that is able to sort a densely cluttered pile of objects by class , provided there is a way to recognize an object s class in an uncluttered environment .",
    "the specific novel improvements to the current state - of - the - art models that learn to grasp or move objects are    * we make no attempt to explicitly segment or understand the objects / classes of objects in the working area . * in addition to the grasp success probability , the machine learning model is taught to predict the class distribution of the grasped objects ( enabling sorting ) . *",
    "feedback about object classes is obtained automatically from a more structured environment after the robotic manipulator has grasped and thrown the object .",
    "this makes it possible to generate a large amount of labeled data about the cluttered environment .",
    "* as the first , fixed - function stage of the system , we present an efficient algorithm for finding _ closed _ grasps for a two - fingered gripper from a heightmap .",
    "the overall architecture we propose is shown in fig .  [",
    "fig : overallsystem ] .",
    "the change from existing systems @xcite is that instead of using a single binary grasp success - failure feedback using , e.g. , the gripper force sensors , we use the robot to move and drop the grasped objects to a different area in which we can generate richer feedback by identifying them .     ]     ]    after dropping the object(s ) , the system takes a picture to generate the feedback of how much of each class of object was visible at the drop area .",
    "after this , the drop area is cleared for the next attempt ( in our example system , the dropped objects are taken away by a conveyor ) .",
    "this architecture fulfills the requirements in @xcite for learning behaviours ( i.e. , 1 .",
    "choice of behaviours , 2 .",
    "reliable feedback on whether a behaviour was successful , 3 .",
    "complementary behaviour : returning the world to the original state after a behaviour , and 4 .",
    "parameters for complementary behaviour from chosen behaviour ) .",
    "however , the drop area being cleared is , instead of a complementary behaviour , a _ nullifying _ behaviour that brings the system back to its original state without any parameters .",
    "to simplify the recognition part of the system , we used color as a proxy for a more complete recognition system .",
    "the task for the system was to sort the objects into three classes : red , yellow and blue - green .",
    "we spray - painted a number of real and simulated waste objects with bright colors .",
    "both the task and the painting were chosen to make recognizing the objects at the drop zone as simple as possible .    the weights of the objects ranged from under 100  g for light - weight plastic objects to over 4  kg for large pieces of concrete .",
    "many of the objects such as the heaviest objects were purposefully selected so as to be difficult for the gripper being used , in order to present a challenge for the system .",
    "the overall hardware configuration of the experiment is shown in fig .",
    "[ fig : hw ] .      we used a 4-dof gantry - type robot with beckhoff servos for positioning .",
    "the degrees of freedom are three translations and a rotation around the vertical axis .",
    "the gripper used has a wide opening and a large - angle compliance system ( fig .",
    "[ fig : gripper ] ) .",
    "the gripper has evolved in previous versions of our product step by step to be morphologically well - adapted to the task .",
    "the gripper is pneumatically position - controllable and has a sensor giving its current opening .      for cycling the objects through the system",
    ", we used the waste merry - go - round depicted in fig .",
    "[ fig : hw ] .",
    "this system makes it possible to run long training sessions with a large number of objects .    due to the merry - go - round",
    ", the system does not actually place the sorted objects in different places by default ",
    "but it makes a prediction before throwing an object . in a separate test ( see accompanying video ) , we made the system successfully throw the objects it predicted to be red to the other side of the conveyor to ensure it is able to really sort objects .",
    "both the working area camera and the drop zone camera were microsoft kinect one time - of - flight rgbd cameras .",
    "the overall algorithm of the sorting loop is described in fig .",
    "[ fig : datagen ] .",
    "this component is responsible for evaluating the situation in the working area and carrying out a pickup .",
    "wait for next cam1 frame @xmath0 @xmath1 latest trained models @xmath2 @xmath3 avoid making too many failures perform pickup @xmath4 and obtain feedback @xmath5 add @xmath6 into the training data in another thread in the background , record frames from cam2 for 5 s , @xmath7 add @xmath6 into the training data forever    @xmath8 picture from working area rgbd camera @xmath9 @xmath10 @xmath11 @xmath12 @xmath13 @xmath14    @xmath15 @xmath16 @xmath17 index of target color @xmath18 @xmath19 expected recovered pixels @xmath20    the purityvalue function used in grasp evaluation . ]",
    "calculate background level for each pixel as the 20th percentile of the pixel s depth values over time ( this is a stable estimate unaffected by objects that are visible at each pixel in only few of the frames ) for each frame , form foreground mask as pixels that are at least 6 mm closer than the background level within a hand - specified region of interest , calculate , for each frame , the volume above background within the foreground mask in order to reduce noise , apply minimum filter over time with window of 9 and choose the best frame by maximum filtered volume .",
    "counts @xmath21 of different target colors within the foregound mask ( different target colors are defined simply by rectangular boxes in the hsv color space )    \\a )   images from various points of the software pipeline .",
    "best viewed in color .",
    "a ) the cam1 , depth and rgb .",
    "b ) the cam1 , projected to be seen from above as a heightmap .",
    "c ) grasp locations .",
    "left : a sample of grasps generated by the fixed - function first stage .",
    "this sample is of size 20 ; the one used by the algorithm is size 2,000 .",
    "right : the grasp to be executed by the system , chosen by the machine learning algorithms .",
    "d ) cam2 rgb images after the robot has executed the grasp and throw . left : frames from the output camera , superimposed .",
    "right : the selected frame ( see fig .  [",
    "fig : cam2result ] ) .",
    "e ) processed output camera images .",
    "left : fg - bg segmentation based on cam2 depth , with background darkened right : the fixed - function color decisions made by the system for the output image , superimposed on the original image .",
    "this is the feedback based on which the machine learning system learns to make pure pickups ( in this case that all of the picked object was red).,title=\"fig:\",width=113 ]   images from various points of the software pipeline .",
    "best viewed in color .",
    "a ) the cam1 , depth and rgb .",
    "b ) the cam1 , projected to be seen from above as a heightmap .",
    "c ) grasp locations . left : a sample of grasps generated by the fixed - function first stage .",
    "this sample is of size 20 ; the one used by the algorithm is size 2,000 .",
    "right : the grasp to be executed by the system , chosen by the machine learning algorithms .",
    "d ) cam2 rgb images after the robot has executed the grasp and throw . left : frames from the output camera , superimposed .",
    "right : the selected frame ( see fig .  [",
    "fig : cam2result ] ) .",
    "e ) processed output camera images . left : fg - bg segmentation based on cam2 depth , with background darkened right : the fixed - function color decisions made by the system for the output image , superimposed on the original image",
    "this is the feedback based on which the machine learning system learns to make pure pickups ( in this case that all of the picked object was red).,title=\"fig:\",width=113 ] + b )   images from various points of the software pipeline .",
    "best viewed in color .",
    "a ) the cam1 , depth and rgb .",
    "b ) the cam1 , projected to be seen from above as a heightmap .",
    "c ) grasp locations . left : a sample of grasps generated by the fixed - function first stage .",
    "this sample is of size 20 ; the one used by the algorithm is size 2,000 .",
    "right : the grasp to be executed by the system , chosen by the machine learning algorithms .",
    "d ) cam2 rgb images after the robot has executed the grasp and throw . left : frames from the output camera , superimposed . right : the selected frame ( see fig .  [",
    "fig : cam2result ] ) .",
    "e ) processed output camera images . left : fg - bg segmentation based on cam2 depth , with background darkened right : the fixed - function color decisions made by the system for the output image , superimposed on the original image",
    "this is the feedback based on which the machine learning system learns to make pure pickups ( in this case that all of the picked object was red).,title=\"fig:\",width=113 ]   images from various points of the software pipeline .",
    "best viewed in color .",
    "a ) the cam1 , depth and rgb .",
    "b ) the cam1 , projected to be seen from above as a heightmap .",
    "c ) grasp locations . left : a sample of grasps generated by the fixed - function first stage .",
    "this sample is of size 20 ; the one used by the algorithm is size 2,000 .",
    "right : the grasp to be executed by the system , chosen by the machine learning algorithms .",
    "d ) cam2 rgb images after the robot has executed the grasp and throw . left : frames from the output camera , superimposed .",
    "right : the selected frame ( see fig .",
    "[ fig : cam2result ] ) .",
    "e ) processed output camera images . left : fg - bg segmentation based on cam2 depth , with background darkened right : the fixed - function color decisions made by the system for the output image , superimposed on the original image",
    "this is the feedback based on which the machine learning system learns to make pure pickups ( in this case that all of the picked object was red).,title=\"fig:\",width=113 ] + c )   images from various points of the software pipeline .",
    "best viewed in color .",
    "a ) the cam1 , depth and rgb .",
    "b ) the cam1 , projected to be seen from above as a heightmap .",
    "c ) grasp locations . left : a sample of grasps generated by the fixed - function first stage .",
    "this sample is of size 20 ; the one used by the algorithm is size 2,000 .",
    "right : the grasp to be executed by the system , chosen by the machine learning algorithms .",
    "d ) cam2 rgb images after the robot has executed the grasp and throw . left : frames from the output camera , superimposed .",
    "right : the selected frame ( see fig .  [",
    "fig : cam2result ] ) .",
    "e ) processed output camera images . left : fg - bg segmentation based on cam2 depth , with background darkened right : the fixed - function color decisions made by the system for the output image , superimposed on the original image",
    "this is the feedback based on which the machine learning system learns to make pure pickups ( in this case that all of the picked object was red).,title=\"fig:\",width=113 ]   images from various points of the software pipeline .",
    "best viewed in color .",
    "a ) the cam1 , depth and rgb .",
    "b ) the cam1 , projected to be seen from above as a heightmap .",
    "c ) grasp locations . left : a sample of grasps generated by the fixed - function first stage .",
    "this sample is of size 20 ; the one used by the algorithm is size 2,000 .",
    "right : the grasp to be executed by the system , chosen by the machine learning algorithms .",
    "d ) cam2 rgb images after the robot has executed the grasp and throw . left : frames from the output camera , superimposed .",
    "right : the selected frame ( see fig .  [",
    "fig : cam2result ] ) .",
    "e ) processed output camera images . left : fg - bg segmentation based on cam2 depth , with background darkened right : the fixed - function color decisions made by the system for the output image , superimposed on the original image",
    "this is the feedback based on which the machine learning system learns to make pure pickups ( in this case that all of the picked object was red).,title=\"fig:\",width=113 ] + d )   images from various points of the software pipeline .",
    "best viewed in color .",
    "a ) the cam1 , depth and rgb .",
    "b ) the cam1 , projected to be seen from above as a heightmap .",
    "c ) grasp locations . left : a sample of grasps generated by the fixed - function first stage .",
    "this sample is of size 20 ; the one used by the algorithm is size 2,000 .",
    "right : the grasp to be executed by the system , chosen by the machine learning algorithms .",
    "d ) cam2 rgb images after the robot has executed the grasp and throw . left : frames from the output camera , superimposed .",
    "right : the selected frame ( see fig .",
    "[ fig : cam2result ] ) .",
    "e ) processed output camera images . left : fg - bg segmentation based on cam2 depth , with background darkened right : the fixed - function color decisions made by the system for the output image , superimposed on the original image .",
    "this is the feedback based on which the machine learning system learns to make pure pickups ( in this case that all of the picked object was red).,title=\"fig:\",width=113 ]    images from various points of the software pipeline .",
    "best viewed in color .",
    "a ) the cam1 , depth and rgb .",
    "b ) the cam1 , projected to be seen from above as a heightmap .",
    "c ) grasp locations . left : a sample of grasps generated by the fixed - function first stage .",
    "this sample is of size 20 ; the one used by the algorithm is size 2,000 .",
    "right : the grasp to be executed by the system , chosen by the machine learning algorithms .",
    "d ) cam2 rgb images after the robot has executed the grasp and throw .",
    "left : frames from the output camera , superimposed .",
    "right : the selected frame ( see fig .  [",
    "fig : cam2result ] ) .",
    "e ) processed output camera images . left : fg - bg segmentation based on cam2 depth , with background darkened right : the fixed - function color decisions made by the system for the output image , superimposed on the original image",
    "this is the feedback based on which the machine learning system learns to make pure pickups ( in this case that all of the picked object was red).,title=\"fig:\",width=113 ] + e )    images from various points of the software pipeline .",
    "best viewed in color .",
    "a ) the cam1 , depth and rgb .",
    "b ) the cam1 , projected to be seen from above as a heightmap .",
    "c ) grasp locations . left : a sample of grasps generated by the fixed - function first stage .",
    "this sample is of size 20 ; the one used by the algorithm is size 2,000 .",
    "right : the grasp to be executed by the system , chosen by the machine learning algorithms .",
    "d ) cam2 rgb images after the robot has executed the grasp and throw . left : frames from the output camera , superimposed .",
    "right : the selected frame ( see fig .  [",
    "fig : cam2result ] ) .",
    "e ) processed output camera images . left : fg - bg segmentation based on cam2 depth , with background darkened right : the fixed - function color decisions made by the system for the output image , superimposed on the original image",
    "this is the feedback based on which the machine learning system learns to make pure pickups ( in this case that all of the picked object was red).,title=\"fig:\",width=113 ]    images from various points of the software pipeline .",
    "best viewed in color .",
    "a ) the cam1 , depth and rgb .",
    "b ) the cam1 , projected to be seen from above as a heightmap .",
    "c ) grasp locations .",
    "left : a sample of grasps generated by the fixed - function first stage .",
    "this sample is of size 20 ; the one used by the algorithm is size 2,000 .",
    "right : the grasp to be executed by the system , chosen by the machine learning algorithms .",
    "d ) cam2 rgb images after the robot has executed the grasp and throw . left : frames from the output camera , superimposed .",
    "right : the selected frame ( see fig .  [",
    "fig : cam2result ] ) .",
    "e ) processed output camera images . left : fg - bg segmentation based on cam2 depth , with background darkened right : the fixed - function color decisions made by the system for the output image , superimposed on the original image",
    "this is the feedback based on which the machine learning system learns to make pure pickups ( in this case that all of the picked object was red).,title=\"fig:\",width=113 ] +    \\a )    data used in grasp selection . to make the learning problem easy ,",
    "we give images centered on the gripper and both fingers of the gripper . before being given to the machine learning algorithm ,",
    "these images are shrunk as described in the text .",
    "a ) a debug image of center images , showing the gripper fingers superimposed on the heightmap with a colormap .",
    "b ) the center images : heightmap , rgb and unknown map ( i.e. , which pixels are invisible in the projected heightmap .",
    "c ) the left finger images , similar .",
    "d ) the right finger images , similar . ,",
    "title=\"fig:\",width=75 ] + b )   data used in grasp selection . to make the learning problem easy ,",
    "we give images centered on the gripper and both fingers of the gripper . before being given to the machine learning algorithm ,",
    "these images are shrunk as described in the text .",
    "a ) a debug image of center images , showing the gripper fingers superimposed on the heightmap with a colormap .",
    "b ) the center images : heightmap , rgb and unknown map ( i.e. , which pixels are invisible in the projected heightmap .",
    "c ) the left finger images , similar .",
    "d ) the right finger images , similar . ,",
    "title=\"fig:\",width=75 ]    data used in grasp selection . to make the learning problem easy ,",
    "we give images centered on the gripper and both fingers of the gripper . before being given to the machine learning algorithm ,",
    "these images are shrunk as described in the text .",
    "a ) a debug image of center images , showing the gripper fingers superimposed on the heightmap with a colormap .",
    "b ) the center images : heightmap , rgb and unknown map ( i.e. , which pixels are invisible in the projected heightmap .",
    "c ) the left finger images , similar .",
    "d ) the right finger images , similar . ,",
    "title=\"fig:\",width=75 ]    data used in grasp selection . to make the learning problem easy , we give images centered on the gripper and both fingers of the gripper . before being given to the machine learning algorithm ,",
    "these images are shrunk as described in the text .",
    "a ) a debug image of center images , showing the gripper fingers superimposed on the heightmap with a colormap .",
    "b ) the center images : heightmap , rgb and unknown map ( i.e. , which pixels are invisible in the projected heightmap .",
    "c ) the left finger images , similar .",
    "d ) the right finger images , similar . ,",
    "title=\"fig:\",width=75 ] + c )   data used in grasp selection . to make the learning problem easy ,",
    "we give images centered on the gripper and both fingers of the gripper . before being given to the machine learning algorithm ,",
    "these images are shrunk as described in the text .",
    "a ) a debug image of center images , showing the gripper fingers superimposed on the heightmap with a colormap .",
    "b ) the center images : heightmap , rgb and unknown map ( i.e. , which pixels are invisible in the projected heightmap .",
    "c ) the left finger images , similar .",
    "d ) the right finger images , similar . ,",
    "title=\"fig:\",width=75 ]    data used in grasp selection . to make the learning problem easy",
    ", we give images centered on the gripper and both fingers of the gripper . before being given to the machine learning algorithm ,",
    "these images are shrunk as described in the text .",
    "a ) a debug image of center images , showing the gripper fingers superimposed on the heightmap with a colormap .",
    "b ) the center images : heightmap , rgb and unknown map ( i.e. , which pixels are invisible in the projected heightmap .",
    "c ) the left finger images , similar .",
    "d ) the right finger images , similar . ,",
    "title=\"fig:\",width=75 ]    data used in grasp selection . to make the learning problem easy ,",
    "we give images centered on the gripper and both fingers of the gripper . before being given to the machine learning algorithm ,",
    "these images are shrunk as described in the text .",
    "a ) a debug image of center images , showing the gripper fingers superimposed on the heightmap with a colormap .",
    "b ) the center images : heightmap , rgb and unknown map ( i.e. , which pixels are invisible in the projected heightmap .",
    "c ) the left finger images , similar .",
    "d ) the right finger images , similar . ,",
    "title=\"fig:\",width=75 ] + d )   data used in grasp selection . to make the learning problem easy ,",
    "we give images centered on the gripper and both fingers of the gripper . before being given to the machine learning algorithm ,",
    "these images are shrunk as described in the text .",
    "a ) a debug image of center images , showing the gripper fingers superimposed on the heightmap with a colormap .",
    "b ) the center images : heightmap , rgb and unknown map ( i.e. , which pixels are invisible in the projected heightmap .",
    "c ) the left finger images , similar .",
    "d ) the right finger images , similar . ,",
    "title=\"fig:\",width=75 ]    data used in grasp selection . to make the learning problem easy ,",
    "we give images centered on the gripper and both fingers of the gripper . before being given to the machine learning algorithm ,",
    "these images are shrunk as described in the text .",
    "a ) a debug image of center images , showing the gripper fingers superimposed on the heightmap with a colormap .",
    "b ) the center images : heightmap , rgb and unknown map ( i.e. , which pixels are invisible in the projected heightmap .",
    "c ) the left finger images , similar .",
    "d ) the right finger images , similar . ,",
    "title=\"fig:\",width=75 ]    data used in grasp selection . to make the learning problem easy ,",
    "we give images centered on the gripper and both fingers of the gripper . before being given to the machine learning algorithm ,",
    "these images are shrunk as described in the text .",
    "a ) a debug image of center images , showing the gripper fingers superimposed on the heightmap with a colormap .",
    "b ) the center images : heightmap , rgb and unknown map ( i.e. , which pixels are invisible in the projected heightmap .",
    "c ) the left finger images , similar .",
    "d ) the right finger images , similar . ,",
    "title=\"fig:\",width=75 ] +      we project the rgbd camera image into a heightmap with 5 mm pixel resolution , with orthogonal projection by rendering it through the opengl 3d api into a buffer ( see fig .",
    "[ fig : pipelineimages ] ) . in order to avoid colliding to occluded objects , the projection code marks pixels that are occluded by objects to their maximum possible heights and additionally generates a mask indicating such unknown pixels .",
    "this is accomplished by rendering a frustum for each pixel , the sides of the frustum being rendered with a special `` unknown '' color if the d coordinate difference between the pixel and its neighbour exceeds a certain limit .",
    "@xmath22 @xmath23 @xmath24 ,        \\mathrm{min\\_opening } + \\mathrm{finger\\_thickness } ,        \\mathrm{max\\_opening } + \\mathrm{finger\\_thickness}}$ ] interpret each @xmath25 in @xmath26 as the 3d grasp rectangle @xmath27\\times[y-\\mathrm{finger\\_width}/2,y+\\mathrm{finger\\_width}/2]\\times\\{z\\}$ ] , rotate it by @xmath28 about the center of @xmath29 , and append to @xmath30 together with the value @xmath31    @xmath22 @xmath32 @xmath33 @xmath34 @xmath35 , h[i])$ ] @xmath36 - h[\\mathrm{top } ] + h[i\\!-\\!1 ] - h[i]$ ] @xmath37 * break * @xmath38    possible grasps are modeled using a rectangle representation similar to those used in @xcite .",
    "the left side of a grasp rectangle specifies the position and the extent of the inner side of the left gripper finger , and the right side specifies the position and the extent of the inner side of the right gripper finger .",
    "the width of the rectangle specifies the gripper opening .",
    "the rectangle also has a @xmath39 coordinate , specifying the height at which to grasp ( not visible in figures ) .",
    "the possible grasps are generated starting from an exhaustive search of so called _ closed grasps _ , grasps in which the fingers of the gripper touch the heightmap from both sides and the heightmap rises between the two points , see fig .",
    "[ fig : graspfinding ] . a random sample of closed grasps , weighted by a rudimentary metric of grasp quality ( see fig .  [ fig : graspfindingalg1d ] ) , is generated for further evaluation .",
    "the applyopenings procedure duplicates each closed grasp for all possible extra openings allowed by the heightmap .",
    "it uses a geometric model of the nonlinear opening movement of the gripper and reads the heightmap to determine how much the gripper can be opened from the original closed grasp position without colliding with the heightmap .",
    "the @xmath39-coordinate is increased if necessary , but only if the grasp remains a closed grasp ( i.e. , the inner sides of the gripper fingers touch the heightmap from both sides at the closed grasp position ) .      as discussed in section  [ sec : formalwastesorting ] , the system is solving a multi - objective optimization problem .",
    "as the single objective to optimize , we define a slightly ad hoc utility function as the product of the expected amount of correct color in the target area , multiplied by a nonlinear function of purity ( see fig .  [",
    "fig : purity_value ] ) .",
    "this nonlinear function strongly prefers grasps with expected purity above 80% ( at a real site , this threshold would probably be made significantly higher ) and multiplying by the expected recovery avoids making very narrow picks that are likely to fail but would yield high purity if they succeeded .",
    "parallel to the main sorting loop , the model learning loop is run constantly . in the very beginning , without any data , a `` null model '' is produced , which predicts 1.0 grasp success probability for all grasps and estimates the output to be a constant number of pixels of a special `` unknown '' color .    on each iteration ,",
    "two models are trained from scratch : one classifier model to predict the probability of success of a pickup , and one regression model to predict class proportions that end up in the drop zone ( for successful pickups only ) .",
    "all training data is used on every iteration . for both models ,",
    "we used extremely randomized trees @xcite as implemented by scikit - learn @xcite .",
    "this algorithm was chosen because they rarely overfit and are fast to train and apply .",
    "for each proposed grasp rectangle , a number of features are gathered for machine learning .",
    "a portion of the rgbd heightmap is rotated to the grasp rectangle .",
    "different features are used for the two models . for grasp success probability model",
    "( see fig .",
    "[ fig : featureimages ] ) :    * @xmath40 pixel ( @xmath41  cm ) slices of the heightmap , rgb image , and unknown mask aligned at the left finger , center , and right finger of the gripper ( including a margin of 4  cm around the grasp rectangle ) , downscaled by a factor of four in both directions , * the opening of the grasp and extra opening to be applied when grasping , and * the height of the grasp ( which is also subtracted from the heightmap slices so as to yield translation invariant features ) , and * the position and angle of the grasp rectangle on heightmap ( to allow learning , e.g. , boundary effects on the work area ) .    for the color model ,",
    "the image features are replaced by fewer and more downscaled ones :    * @xmath40 pixel ( @xmath41  cm ) slices of the heightmap and rgb image aligned at the center of the gripper , downscaled by a factor of 8 in both directions . * the opening , height and position features as above .",
    "the training data includes the above features for each attempted pick , and the result is given by the numbers of pixels @xmath21 of each target color .",
    "the success probability model is trained to predict the grasp result , which is 0 , if @xmath42 and 1 , otherwise .",
    "the color model is only trained on successful grasps .",
    "it is trained to estimate the expected amounts @xmath21 of different colors in the target area . however , the system worked significantly better in practice , when , instead of absolute pixel counts , we trained the system on proportions of different colors within the foreground mask of the target area .",
    "this normalization was used in the experiment .",
    "the experiment was run in eight half hour parts .",
    "cam1 was calibrated in the beginning and in the middle of the experiment .",
    "1,743 pickups were carried out , and the model was allowed to learn the whole time .",
    "conveyor 1 ( see fig .  [",
    "fig : hw ] ) was controlled manually in small steps so as to let the robot clear the piles .",
    "otherwise the system worked autonomously while it was running .",
    "conveyor 2 was run constantly and conveyor 3 was stopped so as to keep the drop zone clear for recording feedback .",
    "conveyor 3 was cleared from any missed objects during experiment breaks , and occasionally the system was stopped for removing any stuck objects between the belts .",
    "the system learned quickly to grasp the objects and sort them by color .",
    "figure  [ fig : results ] shows the success probability of the grasps over blocks of 25 trials , as well as the overall purity of the picks ( total number of pixels of the target fraction divided by the total number pixels seen in the drop zone ) over blocks of 25 trials .",
    "figure  [ fig : chutes ] shows a visualization of the sorting result over the whole experiment .",
    "representative example grasps are shown in fig .  [",
    "fig : representative - picks ] .    the cycle time ( between two consecutive pickups ) was about 8 s in total , including approximately 23 s of computation for generating the best grasp from the cam1 image .",
    "graphs illustrating the success probability of the grasps and the overall purity of the picks over time in the experiment.,title=\"fig : \" ] graphs illustrating the success probability of the grasps and the overall purity of the picks over time in the experiment.,title=\"fig : \" ]    visualization of the sorting result over the whole experiment : cam2 foreground segment images from objects thrown are stacked for picks with unknown , red , blue - green , and yellow target fraction ; time goes up in each column .",
    "the time in each column is independent .",
    "gray circles indicate failed picks.,width=234 ]    [ cols=\"<,^,^,^,^,^,^ \" , ]",
    "we have demonstrated a system that learns to sort a densely cluttered pile of objects by class .",
    "the system is general and should be able to learn just about any sorting task , provided it is run long enough .",
    "the only theoretical limitation we see for the proposed system is that it might be able to learn to fool the feedback processing .",
    "for example , the system could learn to throw multiple objects so that only the object of the right class is seen ( e.g. , it ends up on top of the others ) . in this case",
    ", the system would be making many impure throws .",
    "this behaviour was not observed in our experiment but might appear in a more complex task .",
    "the system can only be as good as the feedback it gets .",
    "possible ways of alleviating this problem if it were to occur are , e.g. , taking feedback pictures while the objects are flying through the air or having the robot manipulate the thrown objects more to ensure there are only ones from the correct class .",
    "the practical demonstration of the proposed system is still relatively limited .",
    "for example , the region around a grasp that the system sees as features is small due to performance reasons ; this prevents it from learning to not pick the end of a plank that is under a pile of other objects ( except indirectly by learning that such pickups sometimes cause more problems than ones at larger height from the belt ) .",
    "this limitation is not inherent in our architecture : replacing the learning component with a more powerful one ( e.g. , one using deep learning ) and increasing the input region size will likely make handling this situation possible .",
    "another possible practical limitation stems from the fact that the system has to re - learn the classification of objects on the working area . while being also a strength ( objects might look different in the clutter of the working area ) , this may make learning slow if the classes are difficult",
    "this can be alleviated in several ways , such as using the results of existing classifiers as input features on the working area , which might work , especially if the regression model is biased to be symmetric with respect to change of classes .",
    "next steps of future work include sorting objects based on classes not determined by color as well as systems that specifically aim at picking more than one object at a time .",
    "the authors would like to thank the zenrobotics research assistants , especially risto sirvi for supervising many of the experiments and risto sirvi and sara vogt for annotating experiment data .",
    "the authors would also like to thank risto bruun , antti lappalainen , arto liuha , and ronald tammepld for discussions and plc work , matti kriinen , timo tossavainen , and olli - pekka kahilakoski for many discussions , and risto bruun , juha koivisto , and jari siitari for hardware work .",
    "this work also makes use of the contributions of the whole zenrobotics team through the parts of our product that were reused in this prototype .",
    "t.  j. lukka , t.  tossavainen , j.  v. kujala , and t.  raiko , `` zenrobotics recycler ",
    "robotic sorting using machine learning , '' in _ proceedings of the international conference on sensor - based sorting ( sbs ) _ , 2014 .",
    "d.  rao , q.  v. le , t.  phoka , m.  quigley , a.  sudsang , and a.  y. ng , `` grasping novel objects with depth segmentation , '' in _ proceedings of the international conference on intelligent robots and systems ( iros ) _ , 2010 , pp .",
    "25782585 .",
    "y.  domae , h.  okuda , y.  taguchi , k.  sumi , and t.  hirai , `` fast graspability evaluation on single depth maps for bin picking with general grippers , '' in _ proceedings of the international conference on robotics and automation ( icra ) _ , 2014 , pp .",
    "19972004 .",
    "d.  holz , m.  nieuwenhuisen , d.  droeschel , j.  stckler , a.  berner , j.  li , r.  klein , and s.  behnke , `` active recognition and manipulation for mobile robot bin picking , '' in _ gearing up and accelerating cross - fertilization between academic and industrial robotics research in europe_.1em plus 0.5em minus 0.4emspringer , 2014 , pp .",
    "133153 .",
    "m.  nieuwenhuisen , d.  droeschel , d.  holz , j.  stuckler , a.  berner , j.  li , r.  klein , and s.  behnke , `` mobile bin picking with an anthropomorphic service robot , '' in _ proceedings of the international conference on robotics and automation ( icra ) _",
    ", 2013 , pp . 23272334 .",
    "m.  kopicki , r.  detry , m.  adjigble , r.  stolkin , a.  leonardis , and j.  l. wyatt , `` one - shot learning and generation of dexterous grasps for novel objects , '' _ the international journal of robotics research _ , vol .",
    "35 , no .  8 , pp . 959976 , 2016 .",
    "d.  katz , a.  venkatraman , m.  kazemi , j.  a. bagnell , and a.  stentz , `` perceiving , learning , and exploiting object affordances for autonomous pile manipulation , '' _ autonomous robots _ ,",
    "37 , no .  4 , pp . 369382 , 2014 .",
    "s.  levine , p.  pastor , a.  krizhevsky , and d.  quillen , `` learning hand - eye coordination for robotic grasping with deep learning and large - scale data collection , '' _ arxiv preprint arxiv:1603.02199 _",
    ", 2016 .",
    "l.  pinto and a.  gupta , `` supersizing self - supervision : learning to grasp from 50k tries and 700 robot hours , '' in _ proceedings of the international conference on robotics and automation ( icra ) _ , 2016 , pp .",
    "34063413 .",
    "y.  jiang , s.  moseson , and a.  saxena , `` efficient grasping from rgbd images : learning using a new rectangle representation , '' in _ proceedings of the international conference on robotics and automation ( icra ) _ , 2011 , pp .",
    "33043311 .",
    "f.  pedregosa , g.  varoquaux , a.  gramfort , v.  michel , b.  thirion , o.  grisel , m.  blondel , p.  prettenhofer , r.  weiss , v.  dubourg , j.  vanderplas , a.  passos , d.  cournapeau , m.  brucher , m.  perrot , and e.  duchesnay , `` scikit - learn : machine learning in python , '' _ journal of machine learning research _ , vol .",
    "28252830 , 2011 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of sorting a densely cluttered pile of unknown objects using a robot . </S>",
    "<S> this yet unsolved problem is relevant in the robotic waste sorting business .    by extending previous active learning approaches to grasping , </S>",
    "<S> we show a system that learns the task autonomously . instead of predicting </S>",
    "<S> just whether a grasp succeeds , we predict the classes of the objects that end up being picked and thrown onto the target conveyor . </S>",
    "<S> segmenting and identifying objects from the uncluttered target conveyor , as opposed to the working area , is easier due to the added structure since the thrown objects will be the only ones present .    instead of trying to segment or otherwise understand the cluttered working area in any way </S>",
    "<S> , we simply allow the controller to learn a mapping from an rgbd image in the neighborhood of the grasp to a predicted result  all segmentation etc </S>",
    "<S> .  in the working area is implicit in the learned function . </S>",
    "<S> the grasp selection operates in two stages : the first stage is hardcoded and outputs a distribution of possible grasps that sometimes succeed . </S>",
    "<S> the second stage uses a purely learned criterion to choose the grasp to make from the proposal distribution created by the first stage .    in an experiment </S>",
    "<S> , the system quickly learned to make good pickups and predict correctly , in advance , which class of object it was going to pick up and was able to sort the objects from a densely cluttered pile by color . </S>"
  ]
}