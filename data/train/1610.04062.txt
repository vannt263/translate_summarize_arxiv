{
  "article_text": [
    "video - fill - in - the - blank ( vifitb ) is a new computer vision problem .",
    "( figure [ fig_example ] shows an example ) .",
    "it is related to the standard video - question - and - answer ( viqaa ) problem , but have significant differences .",
    "a major difference is that , for a standard vqaa problem , it has a complete sentence of the `` question '' , and requires the methods to output the `` answer '' . in this framework , it is easier to encode the `` question '' in the model ( e.g. neural networks ) and then use it to predict an output .",
    "however , in the video - fill - in - the - blank ( vifitb ) problem , it is much trickier to encode the `` question '' , since the `` question '' is broken into parts and is much more challenging to encode the pieces of question to find the missing word of `` blank '' efficiently .    in this paper",
    ", we propose a new method to encode the sentence fragments before and after the blank using two lstms ( long short term memory ) , and use these two lstms outputs to find the correct answer .",
    "this is an intuitive way to encode the semantic and contextual information from the sentence efficiently and makes the model easier to train .",
    "similar to traditional visual - question - and - answer ( vqaa ) , we encode both textural and visual cues . from the experiments we found that both cues are crucial to improve the performance .",
    "the visual - fill - in - the - blank ( vfitb ) problem is related to visual - captioning ( vc ) problem @xcite , for which the deep neural networks , especially recurrent neural networks ( e.g. lstm : long short term memory ) are intensely employed @xcite .",
    "it s also highly related to the visual - question - and - answer ( vqaa ) problem @xcite .",
    "for vqaa problem , usually there is a `` question '' with a question mark , and the answer is a word from the dictionary @xcite .",
    "the answer can be numbers , colors , objects , etc .",
    "some of the datasets @xcite provide multiple choices as the candidates for the answer .",
    "strictly speaking , a visual - fill - in - the - blank ( vfitb ) problem can be considered as a vqaa problem ; however , this question is dramatically different from standard vqaa problems .",
    "there are already some work for image - fill - in - the - blank ( imfitb ) problem @xcite , however , the blanks in the dataset are usually at the end of the sentence and it is similar to sentence prediction . also there is a dataset for video - fill - in - the - blank ( vifitb ) problem @xcite , however , the dataset was collected in a multiple choice fashion and limited number of words are available for the blank and mostly are about objects or actions . to our knowledge , `` movie fill - in - the - blank '' dataset @xcite is the first large - scale dataset for the vifitb problem in the wild and we show superior performance of the proposed method on the dataset .",
    "the proposed method encodes the sentence with two lstms , and encodes the video by max - pooling over features coming out of a cnn .",
    "an attention model @xcite helps us to leverage more spatial information from videos .",
    "figure [ fig_framework ] shows an illustration of the proposed method .",
    "each of the words from the sentence is converted into a feature vector using the word2vec approach @xcite , and then pass to lstms one by one . the left part ( before blank ) and the right part ( after blank )",
    "are fed into two different lstms , aiming to capture the structure of the sentence .",
    "we use @xmath0 and @xmath1 to represent the output from the left and right lstms respectively , and we combine them using by @xmath2),\\ ] ] where @xmath3 is the final output , @xmath4 is a trainable parameter matrix , and `` @xmath5 '' means concatenation of left and right vectors .",
    "the output @xmath3 from the two lstms are then input into the visual attention model . on the other hand ,",
    "the video frames are processed by a pre - trained vgg19 network @xcite and the last pooling layer is extracted .",
    "a max - pooling has been applied to all the frames features and one @xmath6 tensor is the outcome of this part as a visual feature . within the attention model , this visual feature and the textural feature @xmath3 are combined by @xmath7 where @xmath8 is output feature , @xmath9 and @xmath10 are trainable parameter matrices , @xmath11 is the visual feature , and `` @xmath12 '' means summation of the feature vector @xmath3 wit all @xmath13 vectors in @xmath11 .",
    "the combined feature @xmath8 is then used to find attention model : @xmath14 where @xmath15 is the trainable parameter matrix .",
    "the attention model @xmath16 is employed to output the weights for @xmath13 visual features .",
    "finally , the textural and weighted visual features @xmath17 are combined to predict the word in the blank @xmath18",
    "there are two methods to train the proposed deep network : end - to - end training , and incremental training . for end - to - end training ,",
    "we train the whole network together , and for incremental training , we first train the sentence network ( lstms ) , then combine it with the visual attention network and train them together again .",
    "we found the incremental training method has better performance .",
    "we did our experiments on the `` movie fill - in - the - blank '' dataset @xcite .",
    "we use categorical cross - entropy as our loss function and adagrad optimizer with early stoping strategy by observing the validation loss .",
    "we show the performances of the proposed method with different setups , and compared it with several state - of - the - art methods .",
    "table [ table_results ] shows some of the quantitative results.the  end - to - end \" is our method which trains all the parameters from scratch .",
    "however , our  incremental \" method uses the method  sentence \" to initialize two lstms and the matrix @xmath4 , since they are in common between both of them .",
    "the  left sentence \" method just uses the left part of the blank to answer the question .  visual lstm \"",
    "method uses just cnn features coming out of last fully connected layer of vgg-19 and pass them through an lstm to get the answer with out considering the question .",
    ".results on `` movie fill - in - the - blank '' dataset . [ cols=\"<,<,^\",options=\"header \" , ]     [ table_results ]",
    "we have proposed a new method for the video - fill - in - the - blank ( vifitb ) problem which takes advantage of the sentence structure before and after the blank and employed two lstms to encode the textural information efficiently .",
    "we have demonstrated that , by incorporating the visual cues with the spatial attention model , the performance can be further improved .",
    "we verified our ideas on the new `` movie fill - in - the - blank '' dataset of 2016 large scale movie description and understanding challenge ( lsmdc ) , and showed improved results compared with several baseline methods . in experiments ,",
    "we have considered the output dimension of @xmath3 vector as @xmath19 .",
    "also the batch size in training stage is @xmath20 .",
    "each epoch takes about @xmath21 seconds in our implementation settings .",
    "s.  antol , a.  agrawal , j.  lu , m.  mitchell , d.  batra , c.  lawrence  zitnick , and d.  parikh .",
    "vqa : visual question answering . in _ proceedings of the ieee international conference on computer vision _ , pages 24252433 , 2015 .",
    "t.  mikolov , i.  sutskever , k.  chen , g.  s. corrado , and j.  dean . distributed representations of words and phrases and their compositionality . in _ advances in neural information processing systems _ ,",
    "pages 31113119 , 2013 .",
    "s.  venugopalan , m.  rohrbach , j.  donahue , r.  mooney , t.  darrell , and k.  saenko .",
    "sequence to sequence - video to text . in _ proceedings of the ieee international conference on computer vision _ , pages 45344542 , 2015 .",
    "l.  yao , a.  torabi , k.  cho , n.  ballas , c.  pal , h.  larochelle , and a.  courville .",
    "describing videos by exploiting temporal structure . in _ proceedings of the ieee international conference on computer vision _ ,",
    "pages 45074515 , 2015 ."
  ],
  "abstract_text": [
    "<S> given a video and its incomplete textural description with missing words , the * video - fill - in - the - blank ( vifitb ) * task is to automatically find the missing word . </S>",
    "<S> the contextual information of the sentences are important to infer the missing words ; the visual cues are even more crucial to get a more accurate inference . in this paper </S>",
    "<S> , we presents a new method which intuitively takes advantage of the structure of the sentences and employs merging lstms ( to merge two lstms ) to tackle the problem with embedded textural and visual cues . in the experiments </S>",
    "<S> , we have demonstrated the superior performance of the proposed method on the challenging `` movie fill - in - the - blank '' dataset @xcite . </S>"
  ]
}