{
  "article_text": [
    "least squares residuals are quite important numerically . the residuals measure the quality of fits in regression analysis , and forming orthogonal projections is an essential step in many iterative algorithms for linear equations or matrix eigenvalues .    this paper determines a tight estimate for the condition number of the residual in full rank least squares problems .",
    "equivalently , the condition number of orthogonal projections into the span of linearly independent vectors is also estimated .",
    "the condition numbers are with respect to the matrix of least squares coefficients and with respect to scaled @xmath0-norms .",
    "the condition number of the residual , like the solution , is the value of an optimization problem that does not have an explicit formula but which does have a tight estimate .",
    "this introduction provides some background material .",
    "section [ sec : definition ] discusses the evaluation of condition numbers from jacobian matrices .",
    "section [ sec : description ] describes the tight estimate of the condition number and provides an example ; this material is appropriate for classroom presentation .",
    "section [ sec : derivation ] proves that the condition number varies from the estimate within a factor of @xmath1 ; the linear algebra is complicated but straightforward given an identity from a previous paper @xcite .",
    "section [ sec : comparison ] compares the results to the literature .",
    "section [ sec : discuss ] discusses the application to projections and to iterative algorithms .",
    "conditioning with respect to perturbations of the matrix @xmath2 is the most interesting aspect of least squares problems , @xmath3 the condition numbers @xmath4 , of @xmath5 with respect to @xmath2 for various norms , have been studied in dozens of papers and books since @xcite discovered the condition number for @xmath0-norms can depend on the square of the matrix condition number .",
    "thus , it was equally surprising when discovered the conditioning of the residual is independent of the square . and @xmath6 , @xmath7 .",
    "] even so , bjrck s original formula turned out to be an overestimate .",
    "roughly the same formula is still found in many textbooks ( section [ sec : gvlh ] ) .    derived a perturbation bound for the residual with respect to @xmath2 again for @xmath0-norms .",
    "this paper shows that wedin s bound contains an estimate for @xmath8 that is accurate within a factor of @xmath0 .",
    "wedin noted that his perturbation bound could be `` almost attained '' ( p.  225 ) .",
    "however , ( p.  226 ) he also remarked that his paper only demonstrated near attainment for a perturbation bound on the least squares solution ( not the residual ) .",
    "thus the published literature has no prior proof of attainment for an error bound of the residual .",
    "@xcite and @xcite have used jacobian matrices to derive condition numbers , or estimates of condition numbers , for least squares solutions .",
    "their results and those of @xcite , @xcite , and @xcite for the condition number of the solution are summarized by @xcite .",
    "there has been no similar determination of condition numbers based on jacobian matrices for the residual .",
    "the spectral condition number of the residual , like the solution , is the value of an optimization problem that does not have an explicit formula but which does have a tight estimate .",
    "no tight estimates for the condition number of the least squares residual have been established previously .",
    "`` perturbation bounds '' are used in numerical analysis to limit the sensitivity of the solution of a problem to changes in the initial data .",
    "such bounds are customarily derived using matrix - vector algebra and norms ; the coefficients of the data perturbations in these bounds are sometimes referred to as condition numbers .",
    "for example , in one of the earliest books on rounding error analysis , wrote `` we shall refer to [ the coefficients ] as condition numbers  . ''",
    "many numerical analysts probably agree with wilkinson in the interest of deriving error bounds , but the name `` condition number '' is used sparingly because the coefficients are only upper limits for condition numbers unless the error bounds are the smallest possible , equivalently , unless the error bounds are attained",
    ". observed , `` the bounds are commonly accepted as condition numbers , and any discussion about their sharpness is usually avoided . ''",
    "the oldest way to derive perturbation bounds is by differential calculus .",
    "if @xmath9 is a vector valued function of the vector @xmath5 whose partial derivatives are continuous , then the partial derivatives give the best estimate of the change to @xmath10 for a given change to @xmath5 @xmath11 where @xmath12 is the jacobian matrix of the partial derivatives of @xmath10 with respect to @xmath5 .",
    "the magnitude of the error in the first order approximation ( [ eqn : approximation-1 ] ) is bounded by landau s little @xmath13 for all sufficiently small @xmath14 .",
    "agreement is independent of the norm because all norms for finite dimensional spaces are equivalent . ]",
    "thus @xmath15 is the unique linear approximation to @xmath16 in the vicinity of @xmath5 .",
    "differs from @xmath16 by @xmath17 and therefore does not provide a @xmath13 approximation . ] taking norms produces a perturbation bound , @xmath18 equation ( [ eqn : calculus-1 ] ) is the smallest possible bound on @xmath19 in terms of @xmath14 provided the norm for the jacobian matrix is induced from the norms for @xmath16 and @xmath20 . in this case for each @xmath5 there is some @xmath20 , which is nonzero but may be chosen arbitrarily small , so the bound ( [ eqn : calculus-1 ] ) is attained to within the higher order term , @xmath21",
    ". there may be many other ways to define condition numbers , but because equation ( [ eqn : calculus-1 ] ) is the smallest possible bound , any definition of a condition number for use in bounds equivalent to ( [ eqn : calculus-1 ] ) must arrive at the same value , @xmath22",
    ". the matrix norm may be too complicated to have an explicit formula , but tight estimates can be derived as in this paper .",
    "many problems depend on two parameters @xmath23 , @xmath24 which may consist of the entries of a matrix and a vector ( for example ) . in principle",
    "it is possible to treat the parameters altogether .",
    "a condition number for @xmath10 with respect to joint changes in @xmath23 and @xmath24 requires a common norm for perturbations to both .",
    "such a norm is @xmath25 a single condition number then follows that appears in an optimal error bound , @xmath26 the jacobian matrix @xmath27 contains the partial derivatives of @xmath28 with respect to the entries of both @xmath23 and @xmath24 .",
    "the value of the condition number is again @xmath29 where the matrix norm is induced from the norm for @xmath16 and the norm in equation ( [ eqn : joint - norm ] ) .    because @xmath23 and @xmath24 may enter into the problem in much different ways , it is customary to treat each separately",
    "this approach recognizes that the jacobian matrix is a block matrix @xmath30}\\ ] ] where the functions @xmath31 and @xmath32 have @xmath24 and @xmath23 fixed , respectively .",
    "the first order differential approximation ( [ eqn : approximation-1 ] ) is unchanged but is rewritten with separate terms for @xmath23 and @xmath24 , @xmath33 and a perturbation bound is obtained by applying the triangle inequality , @xmath34 the coefficients @xmath35 and @xmath36 are the separate condition numbers of @xmath10 with respect to @xmath23 and @xmath24 , respectively .",
    "these two different approaches lead to error bounds ( [ eqn : single ] , [ eqn : double ] ) that differ by at most a factor of @xmath0 .",
    "this fact is a property of induced norms .",
    "consider a @xmath37 block matrix @xmath38 } { \\left [ { \\relax}\\begin { array } { c } u\\\\ v \\end { array } \\right]}\\ ] ] and suppose norms are given for @xmath39 , @xmath40 and @xmath41 as spaces of column vectors .",
    "a norm can be defined for @xmath42 as @xmath43 } \\right\\| = \\max \\big\\ { \\| u \\| , \\",
    ", \\| v \\| \\big\\ } \\ , .\\ ] ] these norms for @xmath39 , @xmath40 , @xmath41 , and @xmath42 induce norms for @xmath2 , @xmath44 , and @xmath45}$ ] , @xmath46 } \\right\\| = \\max_{u \\ne 0 \\ ; \\mbox { \\scriptsize or } \\ ; v \\ne 0 }   { \\| a u + b v \\| \\over \\max \\big\\ { \\| u \\| , \\ , \\| v \\| \\big\\ } } \\ , .\\ ] ] the norm of the block matrix has a simple upper bound , @xmath47 } \\right\\| & = & \\max_{u \\ne 0 \\ ; \\mbox { \\scriptsize or } \\ ; v \\ne 0 }   { \\| a u + b v \\| \\over \\max \\big\\ { \\| u \\| , \\ , \\| v \\| \\big\\ } } \\\\",
    "\\noalign { \\smallskip }   \\nonumber & \\le & \\max_{u \\ne 0 \\ ; \\mbox { \\scriptsize or } \\ ; v \\ne 0 }   { \\| a u \\| \\over \\max \\big\\ { \\| u \\| , \\ , \\| v \\| \\big\\ } } + \\max_{u \\ne 0 \\ ; \\mbox { \\scriptsize or } \\ ; v \\ne 0 }   { \\| b v \\| \\over \\max \\big\\ { \\| u \\| , \\ , \\| v \\| \\big\\ } } \\\\ \\noalign { \\smallskip } \\nonumber & = & \\max_{u \\ne 0 }   { \\| a u \\| \\over \\| u \\| } + \\max_{v \\ne 0 }   { \\| b v \\| \\over \\| v \\| } \\\\",
    "\\noalign { \\smallskip } \\label { eqn : lemma1 } & = & \\| a \\| + \\| b \\| \\ , , \\end{aligned}\\ ] ] and a simple lower bound , @xmath48 } \\right\\| \\ , , \\end{aligned}\\ ] ] and similarly @xmath49 } \\|$ ] .",
    "altogether , from equations ( [ eqn : lemma1 ] , [ eqn : lemma2 ] ) , @xmath50 } \\right \\| \\le \\| a \\| + \\| b \\| \\,\\ ] ] which means that @xmath51 overestimates @xmath52 } \\|$ ] by at most a factor of @xmath0 . returning to the jacobian matrices @xmath53",
    ", @xmath54 , and @xmath45 } = j_f ( u , v)$ ] , equation ( [ eqn : lemma3 ] ) can be rewritten @xmath55 thus , for the purpose of deriving tight estimates of joint condition numbers , it suffices to consider @xmath56 and @xmath57 separately .",
    "for any matrix @xmath2 and any similarly sized column vector @xmath6 , the linear least squares problem ( [ eqn : original - problem ] ) need not have an unique solution @xmath5 , but it always has an unique residual @xmath58 where @xmath59 is the orthogonal projection into the column space of @xmath2 , @xmath60 , and where @xmath61 is the pseudoinverse of @xmath2 . if @xmath2 has full column rank , then @xmath62 .",
    "changes to @xmath2 affect @xmath63 differently when @xmath2 does not have full column rank . in that case ,",
    "@xmath64 for every nonzero right null vector @xmath65 , so small changes to @xmath2 can produce large changes to @xmath63 .",
    "in other words , a condition number of @xmath63 with respect to rank deficient @xmath2 does not exist or is `` infinite . ''",
    "perturbation bounds in the rank deficient case can be found by restricting changes to the matrix , for which see and .",
    "that theory is beyond the scope of the present discussion .",
    "this section summarizes the results and presents an example .",
    "proofs are in section [ sec : derivation ] . it is assumed that @xmath2 has full column rank and neither the solution @xmath5 nor the residual @xmath63 of the least squares problem are zero .",
    "the residual is proved to have a condition number @xmath66 with respect to @xmath2 within the limits , @xmath67 the quantities @xmath68 , @xmath69 , and @xmath70 are written bold to emphasize they are the only quantities affecting the tight estimate of the condition number ; they are defined below .",
    "there is also a condition number with respect to @xmath6 , @xmath71 these are condition numbers when the following scaled @xmath0-norms are used to measure the perturbations to @xmath2 , @xmath6 , and @xmath5 , @xmath72 like equation ( [ eqn : double ] ) , the two condition numbers appear in error bounds of the form , and @xmath73 could be discarded from the @xmath74 terms because only the order of magnitude of the terms is pertinent . ]",
    "@xmath75 where @xmath76 is the residual of the perturbed problem , @xmath77    the quantities in the formulas ( [ eqn : chia ] , [ eqn : chib ] ) are @xmath78 where @xmath68 is the spectral matrix condition number of @xmath2 ( @xmath79 is the smallest singular value of @xmath2 ) , @xmath70 is van der sluis s ratio between @xmath80 and @xmath68 , and @xmath81 , respectively , so it seems best to choose roman @xmath70 for van der sluis . ] and @xmath69 is the angle between @xmath6 and @xmath60 .    1 .",
    "@xmath68 depends only on the extreme singular values of @xmath2 .",
    "2 .   @xmath69 depends only on the `` angle of attack '' of @xmath6 with respect to @xmath60 .",
    "3 .   if @xmath2 is fixed , then @xmath70 depends on the orientation of @xmath6 to @xmath60 but not on @xmath69 .",
    "has full column rank , @xmath82 and @xmath5 can only vary proportionally when their directions are fixed . ]",
    "please refer to figure [ fig : schematic ] as needed . if @xmath60 is fixed , then @xmath68 and @xmath69 are separate sources of ill - conditioning for the residual .",
    "the ratio @xmath70 can never cause ill - conditioning because it only appears in the denominator of equation ( [ eqn : chia ] ) and @xmath70 is always at least @xmath80 . indeed ,",
    "if @xmath82 has comparatively large components in singular vectors corresponding to the largest singular values , then @xmath83 and @xmath70 might lessen the ill - conditioning caused by a small @xmath69 .    , and the angle @xmath69 between @xmath82 and @xmath6 .",
    "]      this example illustrates the effects of @xmath68 , @xmath70 , and @xmath69 on @xmath8 .",
    "it is based on the example of .",
    "let @xmath84 , \\quad   b = \\left [ \\begin { array } { c } { \\beta}\\cos ( \\phi)\\\\ { \\beta}\\sin ( \\phi ) \\\\ 1 \\end { array } \\right ] \\ , , \\quad   { \\delta a}= \\left [ \\begin { array } { r r } 0 & 0\\\\ 0 & 0\\\\ - \\epsilon & - \\epsilon \\end { array } \\right ] , \\ ] ] where @xmath85 , and @xmath86 . in this example , @xmath87 , \\quad   r = \\left [ \\begin { array } { c } 0\\\\ 0\\\\ 1 \\end { array } \\right ] , \\quad   { \\delta r}= \\left [ \\begin { array } { c } 1 \\\\ { 1 \\over { \\alpha } } \\\\ { \\beta}\\cos ( \\phi ) + { { \\beta}\\over { \\alpha } } \\sin ( \\phi ) \\end { array } \\right ] \\epsilon",
    "+ { \\mathcal o } ( \\epsilon^2)\\ , .\\ ] ] the three terms in the condition number are @xmath88 ^ 2 + [ \\sin ( \\phi)]^2 } } \\ , , \\qquad \\cot ( { { \\boldsymbol \\theta } } ) = { \\beta}\\ , .\\ ] ] these values can be manipulated by choosing @xmath89 , @xmath90 and @xmath91 .",
    "the tight upper bound on the condition number with respect to @xmath2 is @xmath92 ^ 2 + [ { \\beta}\\sin ( \\phi)]^2 } \\ , .\\ ] ] the relative change to the residual , @xmath93 ^ 2 } \\ , \\epsilon + { \\mathcal o } ( \\epsilon^2 ) \\ , , \\ ] ] can made be close to the bound on @xmath8 times @xmath94 .",
    "these formulas have been verified using mathematica @xcite , as have formulas throughout the paper .",
    "in theoretical numerical analysis especially for least squares problems the @xmath0-norm is preferred because for it the matrix condition number of @xmath95 is the square of the matrix condition number of @xmath2 .",
    "the norms used in this paper and in many other papers are defined as , @xmath96 where the choice of scale factors is left open .",
    "the scaling makes the size of the changes relative to the particular problem of interest .",
    "the scaling used in equations ( [ eqn : chia][eqn : specific - scaled - norms ] ) is @xmath97 some authors prefer to measure the residual relative to @xmath6 by choosing @xmath98 .",
    "other authors have no scaling , @xmath99 .",
    "all of these cases are accommodated by the notation in equation ( [ eqn : norms ] ) .",
    "the effect of the choice for @xmath100 is discussed in section [ sec : residual - scaling ] .",
    "the formula for the jacobian matrix @xmath101 of the residual @xmath102 b$ ] with respect to @xmath6 is clear .",
    "would introduce a name , @xmath103 , for the function by which @xmath63 varies with @xmath6 when @xmath2 is held fixed , @xmath104 , so that the notation for the jacobian matrix is then @xmath105 .",
    "this pedantry will be discarded here to write @xmath106 for the matrix of partial derivatives of @xmath63 with respect to @xmath6 with @xmath2 held fixed . ] for derivatives with respect to the entries of @xmath2 , it is necessary to use the `` @xmath107 '' construction to order the matrix entries into a column vector ; @xmath108 is the column of entries @xmath109 with @xmath110 in co - lexicographic order .",
    "the first order approximation ( [ eqn : approximation-2 ] ) is then @xmath111}\\ , { \\mbox { vec}}({\\delta a } ) + j_r ( b ) \\ , { \\delta b}+ \\mbox { higher order terms in $ { \\delta a}$ and $ { \\delta b}$}\\ ] ] and upon taking norms @xmath112}\\ , { \\mbox { vec}}({\\delta a } ) \\|_{{\\mathcal { r } } } } + { \\| j_r ( b ) \\ , { \\delta b}\\|_{{\\mathcal { r } } } } + o \\ , ( \\dots ) \\\\ \\noalign { \\medskip } \\label { eqn : differential - bound } & \\le & \\underbrace { \\| j_r [ { \\mbox { vec}}({\\delta a } ) ] \\|}_{\\displaystyle \\chi_r ( a ) } \\ , { \\| { \\delta a}\\|_{{\\mathcal { a } } } } + \\underbrace { \\displaystyle \\| j_r ( b ) \\|}_{\\displaystyle \\chi_r ( b ) } \\ , { \\| { \\delta b}\\|_{{\\mathcal { b } } } } + o \\ , ( \\dots ) \\end{aligned}\\ ] ] where the norms of the two jacobian matrices are induced from @xmath113 , @xmath114 and from @xmath113 , @xmath115 , respectively . the high order term in equation ( [ eqn : differential - bound ] ) is @xmath116 because from equation ( [ eqn : calculus-1 ] ) the norm @xmath117 has been given to the space that jointly consists of matrices @xmath2 and vectors @xmath6 .",
    "for the orthogonal projection @xmath118 defined in section [ sec : reason ] , from @xmath119 follows @xmath120 , hence @xmath121 which is equation ( [ eqn : chib ] ) for the choice of scale factors in equation ( [ eqn : scale - factors ] ) .",
    "evaluating the condition number of the residual requires a formula for the jacobian matrix @xmath122}$ ] .",
    "differentiating the entries of @xmath123 b\\ ] ] by those of @xmath2 seems to be a daunting task .",
    "instead , @xmath122}$ ] is constructed from the total differential of the identity , @xmath124 } { \\left [ { \\relax}\\begin { array } { c } r\\\\ x \\end { array } \\right ] } - { \\left [ { \\relax}\\begin { array } { c } b\\\\ 0 \\end { array } \\right ] } =   0 \\ , .\\ ] ] assuming @xmath6 is fixed because it already has been treated in section [ sec : conditionwrtb ] , the total differential is @xmath124 } { \\left [ { \\relax}\\begin { array } { c } dr\\\\ dx \\end { array } \\right ] } + { \\left [   \\setlength { \\arraycolsep } { 0.25em } \\begin { array } { ccccc } x_1 i & x_2 i & \\cdots & x_n i\\\\ e_1 r^t & e_2 r^t & \\cdots & e_n r^t \\end { array } \\right]}{\\mbox { vec}}(da ) = 0 \\ , , \\ ] ] where @xmath125 is the @xmath126-th entry of @xmath5 and where @xmath127 is the @xmath126-th column of the @xmath128 identity matrix . hence @xmath129 } & = & - { { \\left [ { \\relax}\\begin { array } { c c } i & a\\\\ a^t & 0 \\end { array } \\right]}}^{-1 } { \\left [   \\setlength { \\arraycolsep } { 0.25em } \\begin { array } { ccccc } x_1 i & x_2 i & \\cdots & x_n i\\\\ e_1 r^t & e_2 r^t & \\cdots & e_n r^t \\end { array } \\right]}{\\mbox { vec}}(da )   \\\\ \\noalign { \\smallskip } \\nonumber & = & - { \\left [ { \\relax}\\begin { array } { c c } i - p\\ ; & a ( a^t a)^{-1}\\\\ ( a^t a)^{-1 } a^t\\ ; & - ( a^t a)^{-1 } \\end { array } \\right ] } { \\left [   \\setlength { \\arraycolsep } { 0.25em } \\begin { array } { ccccc } x_1 i & x_2 i & \\cdots & x_n i\\\\ e_1 r^t & e_2 r^t & \\cdots & e_n r^t \\end { array } \\right]}{\\mbox { vec}}(da )   \\\\ \\noalign { \\smallskip } \\label { eqn : jrx(a ) } & = & { \\left [ { \\relax}\\begin { array } { c } { j_r [ { \\mbox { vec}}(a)]}\\\\ { j_x [ { \\mbox { vec}}(a)]}\\end { array } \\right ] } { \\mbox { vec}}(da ) \\end{aligned}\\ ] ] in which @xmath62 is the orthogonal projection into the column space of @xmath2 .",
    "the two matrix blocks in equation ( [ eqn : jrx(a ) ] ) are the jacobian matrices of @xmath63 and @xmath5 as functions of the entries of @xmath2 with @xmath6 held fixed .",
    "the desired condition number is the norm induced from the norms in equation ( [ eqn : norms ] ) .",
    "@xmath130}\\| & = & \\displaystyle \\max_{{\\delta a } } { { \\| { j_r [ { \\mbox { vec}}(a)]}\\ , { \\mbox { vec}}({\\delta a } ) \\|_{{\\mathcal { r } } } } \\over { \\| { \\mbox { vec}}({\\delta a } ) \\|_{{\\mathcal { a } } } } } \\\\ \\noalign { \\bigskip } & = & \\displaystyle { { \\mathcal { a}}\\over { \\mathcal { r } } } \\max_{{\\delta a } } { \\| { j_r [ { \\mbox { vec}}(a)]}\\ , { \\mbox { vec}}({\\delta a } ) \\|_2 \\over \\| { \\delta a}\\|_2 }   \\end { array}\\ ] ] the numerator and denominator are vector and matrix @xmath0-norms , respectively . if @xmath2 is an @xmath131 matrix , then this maximization is a large problem with @xmath132 degrees of freedom .",
    "the identity for the norm of the transposed operator can be applied to reduce the degrees to @xmath133 , @xmath134}\\| =   { { \\mathcal { a}}\\over { \\mathcal { r } } } \\max_{{\\delta r } } { \\| { j_r [ { \\mbox { vec}}(a)]}^t { \\delta r}\\|_2^ * \\over \\| { \\delta r}\\|_2^ * } \\ , .\\ ] ] here , the identical norm for the transposed jacobian matrix is induced from the duals of the @xmath0-norms for matrices and vectors .",
    "the vector @xmath0-norm is its own dual .",
    "the dual of the matrix @xmath0-norm is determined in @xcite to be the sum of the singular values of the matrix , including multiplicities .",
    "this norm is sometimes called the nuclear norm or the trace norm .",
    "the application of equation ( [ eqn : induced ] ) requires the evaluation of the matrix - vector product in the numerator .",
    "note that for any vectors @xmath135 and @xmath136 , @xmath137}^t { \\left [ { \\relax}\\begin { array } { c } r^\\prime\\\\ x^\\prime \\end { array } \\right ] } = { \\mbox { vec}}[r^\\prime x^t + r \\ , ( x^\\prime)^t ] \\ , .\\ ] ] with this identity it is now possible to compute , from equation ( [ eqn : jrx(a ) ] ) , @xmath138}\\}^t { \\delta r } & = & { \\left [ { \\relax}\\begin { array } { c } { j_r [ { \\mbox { vec}}(a)]}\\\\ { j_x [ { \\mbox { vec}}(a)]}\\end { array } \\right]}^t { \\left [ { \\relax}\\begin { array } { c } { \\delta r}\\\\ { 0 } \\end { array } \\right ] } \\\\ & = & - { \\left [   \\setlength { \\arraycolsep } { 0.25em } \\begin { array } { ccccc } x_1 i & x_2 i & \\cdots & x_n i\\\\ e_1 r^t & e_2 r^t & \\cdots & e_n r^t \\end { array } \\right]}^t { \\left [ { \\relax}\\begin { array } { c c } i - p\\ ; & a ( a^t a)^{-1}\\\\ ( a^t a)^{-1 } a^t\\ ; & - ( a^t a)^{-1 } \\end { array } \\right ] }   { \\left [ { \\relax}\\begin { array } { c } { \\delta r}\\\\ { 0 } \\end { array } \\right ] } \\\\ \\noalign { \\medskip } & = & - { \\left [   \\setlength { \\arraycolsep } { 0.25em } \\begin { array } { ccccc } x_1 i & x_2 i & \\cdots & x_n i\\\\ e_1 r^t & e_2 r^t & \\cdots & e_n r^t \\end { array } \\right]}^t   { \\left [ { \\relax}\\begin { array } { c } ( i - p ) { \\delta r}\\\\ ( a^t a)^{-1 } a^t \\ , { \\delta r}\\end { array } \\right ] } \\\\ \\noalign { \\medskip } & = & { \\mbox { vec}}(u_1^ { } v_1^t + u_2^ { } v_2^t ) \\ , , \\end{aligned}\\ ] ] in which @xmath139 thus equation ( [ eqn : induced ] ) is the following optimization , @xmath140}\\| & = & \\displaystyle { { \\mathcal { a}}\\over { \\mathcal { r } } } \\max_{{\\delta r } } { \\| u_1^ { } v_1^t + u_2^ { } v_2^t \\|^*_2 \\over \\| { \\delta r}\\|_2}\\ , . \\\\ \\noalign { \\medskip } \\label { eqn : induced - transpose } & = & \\displaystyle { { \\mathcal { a}}\\over { \\mathcal { r } } } \\max_{\\| { \\delta r}\\|_2 = 1 } \\| u_1^ { } v_1^t + u_2^ { } v_2^t \\|^*_2 \\ , .\\end{aligned}\\ ] ]    for ease of notation ,",
    "let @xmath141 be the objective function in equation ( [ eqn : induced - transpose ] ) .",
    "in @xcite it is shown that @xmath142 where @xmath143 is the angle between @xmath144 and @xmath145 , and @xmath146 is the angle between @xmath147 and @xmath148 , and both angles should be taken from @xmath149 to @xmath150 .",
    "evaluating the maximum has two parts .",
    "the first step shows @xmath151 can be restricted so that @xmath152 .",
    "the vector @xmath151 always could be decomposed into a component in @xmath60 and a component orthogonal to this subspace .",
    "let the component inside @xmath60 be @xmath153 .",
    "further , the component outside can be decomposed into components parallel to @xmath63 and orthogonal to @xmath63 , say @xmath154 for some coefficient @xmath155 . with these choices",
    "to express @xmath151 , @xmath156 the vectors in equation ( [ eqn : u - and - v ] ) are @xmath157 and the angles are @xmath158 thus the sign of @xmath155 affects only the angle @xmath143 in equation ( [ eqn : numerator ] ) , so it can be chosen to place @xmath143 in the same quadrant as @xmath146 ( either from @xmath149 to @xmath159 , or from @xmath159 to @xmath150 ) and hence @xmath152 .",
    "this means the maximum of equation ( [ eqn : induced ] ) can be restricted to those @xmath151 for which @xmath160    the second step chooses @xmath151 to maximize the upper bound @xmath161 .",
    "as before , the vector @xmath151 always can be decomposed into a component in @xmath60 and a component in the orthogonal complement . without loss of generality ,",
    "assume @xmath162 where @xmath163 and @xmath164 are unit vectors in @xmath60 and the complement , respectively , and where the coefficients are determined by an angle @xmath91 between @xmath149 and @xmath159 . and",
    "@xmath165 are non - negative so the choice of @xmath91 does not affect the choice of sign needed for equation ( [ eqn : lowerandupper ] ) . ] the vectors in equation ( [ eqn : u - and - v ] ) for this representation of @xmath151 are @xmath166 the largest @xmath167 occurs when @xmath163 is a left singular vector for the smallest singular value of @xmath2 , @xmath79 , in which case @xmath168 ; altogether @xmath169 the maximum of this formula with respect to @xmath91 determines an optimal @xmath170 where the upper bound is @xmath171 the maximum has been verified using mathematica @xcite .    the formula in equation ( [ eqn : upper - bound ] ) is the maximum of the upper bounds , which is not to say it is the maximum of equation ( [ eqn : induced ] ) .",
    "the objective function @xmath172 and the lower and upper bounds @xmath173 and @xmath174 , when evaluated at @xmath170 and @xmath175 , must be arranged as follows , @xmath176 these inequalities have the following justifications : ( a ) equation ( [ eqn : lowerandupper ] ) , ( b ) choice of @xmath177 , ( c ) equation ( [ eqn : upper - bound ] ) , and ( d ) choice of @xmath170 .",
    "therefore equation ( [ eqn : upper - bound ] ) is an upper bound for the maximum . from the formula for @xmath178 in equation ( [ eqn : lowerandupper ] ) ,",
    "the upper bound is at most @xmath1 times larger than a lower bound for the maximum .",
    "note that to complete the limits and the condition number , these values must be scaled by the coefficient @xmath179 in equation ( [ eqn : induced ] ) .",
    "[ spectral condition numbers ] [ thm : condition - numbers ] for the full rank linear least squares problem with solution @xmath180 and residual @xmath181 , and for the scaled norms in equation ( [ eqn : norms ] ) with scale factors @xmath182 , @xmath183 , and @xmath100 , @xmath184 where @xmath79 is the smallest singular value of @xmath2 .",
    "these formulas simplify to those in section [ sec : brief ] for the choice of scale factors in equation ( [ eqn : scale - factors ] ) .",
    "section [ sec : conditionwrtb ] derives @xmath185 , and sections [ sec : begin][sec : conditionwrtacontinued ] derive the bounds on @xmath66 .",
    "table [ tab : published ] lists the condition number estimates in some textbook error bounds for the least squares residual . all the values exceed the upper estimate of theorem [ thm : condition - numbers ] to varying degrees .",
    "the very early formula of is also reported in the more recent textbook of .",
    "it is for perturbations only to @xmath2 , that is for the choice @xmath186 , and for the choice of scale factors @xmath187 .",
    "the value exceeds the estimate in theorem [ thm : condition - numbers ] by at most the factor @xmath1 , so it is at most double the condition number .",
    "the other two values in table [ tab : published ] can be severe overestimates .",
    "@xmath188 { wedin1973 } , \\citet [ p.\\ 30 , eqn.\\ 1.4.27 ] { bjorck1996}\\vrule depth1.5ex height2ex width{0pt}\\end { minipage } } & { \\begin { array } { c } \\displaystyle { \\hspace*{0.25em } \\| { \\delta a}\\|_2 \\hspace * { 0.25em}}\\\\ \\noalign { \\smallskip } \\hspace * { -1em } \\displaystyle { \\mathcal { a}}= 1 \\hspace*{-1em}\\end { array } } & \\displaystyle { \\delta b}= 0 & { \\begin { array } { c } \\displaystyle { \\| { \\delta r}\\|_2}\\\\ \\noalign { \\smallskip } \\hspace * { -1em } \\displaystyle { \\mathcal { r}}= 1 \\hspace*{-1em}\\end { array } } & \\displaystyle { \\| r \\|_2 \\over { \\sigma_{\\min } } } + \\| x \\|_2 & 2 \\\\ \\hline { \\begin { minipage}{8.5em}\\footnotesize \\vrule depth0ex height3.0ex width{0pt}\\raggedright \\citet [ p.\\ 655 ] { stewart1977b } , \\citet [ p.\\ 160 , sec.\\ 5.2 ] { stewart1990}\\vrule depth1.5ex height2ex width{0pt}\\end { minipage } } & { \\begin { array } { c } \\displaystyle { \\| { \\delta a}\\|_2}\\\\ \\noalign { \\smallskip } \\hspace * { -1em } \\displaystyle { \\mathcal { a}}= 1 \\hspace*{-1em}\\end { array } } & \\displaystyle { \\delta b}= 0 & { \\begin { array } { c } \\displaystyle { \\| { \\delta r}\\|_2}\\\\ \\noalign { \\smallskip } \\hspace * { -1em } \\displaystyle { \\mathcal { r}}= 1 \\hspace*{-1em}\\end { array } } & \\displaystyle { \\| b \\|_2 \\over { \\sigma_{\\min } } }   & \\sqrt 2 \\ , { { \\boldsymbol \\kappa^{}_{\\boldsymbol 2}}}\\\\ \\hline { \\begin { minipage}{8.5em}\\footnotesize \\vrule",
    "depth0ex height3.0ex width{0pt}\\raggedright \\citet [ p.\\ 242 , eqn.\\ 5.3.9 ] { golub1996 } , \\citet [ p.\\ 382 , eqn.\\ 20.2 ] { higham2002}$^{**}$\\vrule depth1.5ex height2ex width{0pt}\\end { minipage } } & \\multicolumn { 2 } { c | } { { \\begin { array } { c } \\displaystyle { \\hspace*{-0.66em } \\max \\left\\ { { \\| { \\delta a}\\|_2 \\over \\| a \\|_2 } , { \\| { \\delta b}\\|_2 \\over \\| b \\|_2 } \\right\\ } \\hspace*{-0.66em}}\\\\ \\noalign { \\smallskip } \\hspace * { -1em } \\displaystyle { \\mathcal { a}}= \\| a \\|_2 \\quad { \\mathcal { b}}= \\| b \\|_2 \\hspace*{-1em}\\end { array } } } & { \\begin { array } { c } \\displaystyle { { \\vrule depth2ex height3.5ex width{0pt}}{\\| { \\delta r}\\|_2 \\over \\| b \\|_2 } { \\vrule depth2ex height3.5ex width{0pt}}}\\\\ \\noalign { \\smallskip } \\hspace * { -1em } \\displaystyle { \\mathcal { r}}= \\| b \\|_2 \\hspace*{-1em}\\end { array } } & \\displaystyle 2 \\ , { \\| a \\|_2 \\over { \\sigma_{\\min } } } + 1^ { * * } & { { \\boldsymbol \\kappa^{}_{\\boldsymbol 2}}}\\\\ \\hline { \\begin { minipage}{8.5em}\\footnotesize \\vrule depth0ex height3.0ex width{0pt}\\raggedright theorem \\ref { thm : condition - numbers},\\\\ equation ( \\ref { eqn : thm})\\vrule depth3ex height0pt width{0pt}\\vrule depth1.5ex height2ex width{0pt}\\end { minipage } } & \\displaystyle { \\| { \\delta a}\\|_2 \\over \\vphantom { ( } { \\mathcal { a } } } & { \\delta b}= 0 & \\displaystyle { \\| { \\delta r}\\|_2 \\over { \\mathcal { r } } } & \\vrule depth3.5ex height5.5ex width{0pt}\\displaystyle { { { \\mathcal { a}}\\over { \\mathcal { r } } } \\ , \\sqrt { \\left({\\| r \\|_2 \\over { \\sigma_{\\min}}}\\right)^2 + \\| x \\|_2 ^ 2 } } & \\sqrt 2 \\\\",
    "\\hline \\multicolumn { 6 } { l } { \\footnotesize \\hspace*{1em } \\begin { minipage } { 4.75 in } \\raggedright { \\vrule depth1.125ex height2.375ex width{0pt}}\\llap { $ \\;^{**}\\;$}the formula of golub and van loan and of higham amounts to an estimate for $ \\chi_r ( a ) + \\chi_r ( b)$\\\\[-0.5ex ] and is compared against the sum of $ \\chi_r ( b)$ and the tight estimate for $ \\chi_r ( a)$. see section \\ref",
    "{ sec : gvlh}.\\end { minipage } } \\end { array}\\ ] ]      the value of is also reported by . it again is for choices @xmath186 and @xmath187 .",
    "some assembly is required .",
    "let @xmath189 be the perturbed matrix .",
    "assume @xmath190 so that @xmath44 also has full rank .",
    "for any matrix @xmath191 , let @xmath192 be the orthogonal projection into the column space of @xmath191 . with @xmath186",
    "the difference between the residuals of the original and the perturbed problems ( [ eqn : original - problem ] , [ eqn : perturbed - problem ] ) is @xmath193 so it is always true that @xmath194 remarks that @xmath195 is to be bounded by applying an earlier result .",
    "he does not intend @xmath196 ( p.  651",
    ", eqn .  4.1 ) which converts equation ( [ eqn : stewart-1 ] ) into the useless @xmath197 .",
    "stewart means a complicated expression that introduces @xmath198 into the bound .",
    "this expression requires some preparation that is more easily followed in the presentation of .    continuing the assembly of the bound ,",
    "let the singular value decomposition of @xmath2 be @xmath199}}^t a v = { \\left [ { \\relax}\\begin { array } { c } a_1\\\\ 0 \\end { array } \\right]}\\ ] ] where @xmath200 $ ] and @xmath201 are square orthonormal matrices and where @xmath202 is the square diagonal matrix of singular values .",
    "let the corresponding factorization of @xmath203 be @xmath199}}^t { \\delta a}v = { \\left [ { \\relax}\\begin { array } { c } e_1\\\\ e_2 \\end { array } \\right]}\\ ] ] where @xmath204 for @xmath205 .",
    "define @xmath206 from the triangle inequality and from the neumann series expansion for @xmath207 , @xmath208 these last two equations combine to @xmath209    the final step applies a bound that requires some further hypotheses . for any matrix @xmath191 ,",
    "similar to @xmath192 , let @xmath210 be the orthogonal projection into the row space of @xmath191 ( viewing the rows as column vectors ) .",
    "since @xmath211 is a continuous function of @xmath203 , therefore both @xmath212 and @xmath213 converge to @xmath149 as @xmath203 approaches @xmath149 . if @xmath198 is sufficiently small that both @xmath214 and @xmath215 , then it can be shown @xmath216^{1/2 } }",
    "\\ , .\\ ] ] altogether , combining equations ( [ eqn : stewart-1][eqn : stewart-3 ] ) leaves @xmath217 which is the bound from which the condition estimate in table [ tab : published ] is taken .",
    "the estimate for @xmath8 in equation ( [ eqn : stewart-4 ] ) can be obtained from theorem [ thm : condition - numbers ] by increasing the second term in the upper bound ( [ eqn : thm ] ) by the factor @xmath218 , @xmath219 consequently , stewart and sun s value can overestimate the upper bound for @xmath8 by as much as @xmath70 depending on circumstances .",
    "the worst situation is illustrated by the example of section [ sec : example ] with @xmath220 and @xmath221 , @xmath222      the condition estimate of and of is for the choices @xmath223 and @xmath224 with the scale factors @xmath225 and @xmath226 .",
    "they take the approach of equation ( [ eqn : single ] ) that uses a single quantity , @xmath227 , to measure the perturbations to @xmath2 and @xmath6 , @xmath228 since @xmath229 , this approach can transform the bound ( [ eqn : differential - bound ] ) as follows , @xmath230}\\| \\ , { { \\mathcal { a}}\\over { \\mathcal { b } } } \\ , { \\| { \\delta a}\\|_2 \\over { \\mathcal { a } } } + \\| j_r ( b ) \\| \\ , { \\| { \\delta b}\\| \\over { \\mathcal { b } } } + o ( \\varepsilon ) \\\\ \\noalign { \\smallskip } \\nonumber \\label { eqn : gvlh-1 } & \\le & \\left [   \\| { j_r [ { \\mbox { vec}}(a)]}\\| \\ , { { \\mathcal { a}}\\over { \\mathcal { b } } } + \\| j_r ( b ) \\| \\right ] \\varepsilon + o   ( \\varepsilon)\\\\ \\noalign { \\smallskip } \\label { eqn : gvlh-2 } & = & \\big [   \\chi_r ( a ) + \\chi_r ( b ) \\big ] \\varepsilon + o ( \\varepsilon ) \\ , .\\end{aligned}\\ ] ] from theorem [ thm : condition - numbers ] with the choices @xmath226 , @xmath231    golub and van loan and higham state a larger value , @xmath232 . that was originally stated by . ]",
    "since these formulas can be derived from the sum @xmath7 they are not joint condition numbers in the sense of equation ( [ eqn : single ] ) . moreover",
    ", the derivation inserts @xmath70 into equation ( [ eqn : gvlh-3 ] ) , so the result can overestimate the sum by as much as a factor of @xmath68 . close to the worst situation for the specific value @xmath232 of golub , van loan and higham",
    "is again illustrated by the example of section [ sec : example ] with @xmath220 and @xmath221 , @xmath233    note the bound ( [ eqn : gvlh-3 ] ) is not sensitive to the angle @xmath69 between @xmath63 and @xmath60 because of the choice for the scale factor @xmath98 .",
    "choices for @xmath100 are discussed in section [ sec : residual - scaling ] .",
    "as mentioned in section [ sec : norms ] , scaled changes to the residual are typically measured by choosing @xmath236 or @xmath73 .",
    "the two cases are contrasted in table [ tab : scaling ] .",
    "the choice @xmath98 makes it appear that @xmath69 is not a source of ill - conditioning because the sensitivity of @xmath63 to @xmath2 is masked by measuring changes to @xmath63 against the always larger vector @xmath6 .",
    "the choice @xmath236 measures perturbations relatively .",
    "the next section [ sec : iterative ] describes a situation when the relative measure is appropriate .",
    "@xmath237      many iterative methods proceed by building orthogonal bases from the residuals of least squares projections .",
    "for example , for a symmetric matrix @xmath2 and a unit vector @xmath147 , the lanczos iteration @xmath238 produces a sequence of orthonormal vectors @xmath147 ,",
    "@xmath148 , @xmath239 ,  .",
    "this algorithm can be viewed as repeatedly evaluating a residual @xmath240 for either of two orthogonal projections : ( 1 ) the projection of @xmath241 into the span of @xmath242 and @xmath243 , or ( 2 ) the orthogonal projection of @xmath244 into the krylov subspace spanned by @xmath147 , @xmath245 ,  , @xmath246 .",
    "the appropriate scale factor for measuring perturbations to @xmath247 is @xmath248 .",
    "the relative error in @xmath247 becomes the absolute error in the normalized vector , @xmath249 , which continues the lanczos iteration . in ideal circumstances the vectors @xmath242 and @xmath243 are close to orthonormal .",
    "if @xmath250 $ ] is an orthonormal matrix , then @xmath251 so the tight estimate in table [ tab : scaling ] simplifies to @xmath252 where @xmath69 is the angle between @xmath253 and @xmath60 .",
    "thus @xmath247 is ill - conditioned when @xmath69 is small .      in the least squares problem ,",
    "the condition number of the orthogonal projection @xmath82 is essentially that of the residual .",
    "in addition to equations ( [ eqn : norms ] , [ eqn : scale - factors ] ) , it is necessary to specify the scaled norm for the projection : @xmath254 from @xmath255 follows @xmath256 hence @xmath257 .",
    "since @xmath258 so @xmath259 . with @xmath260 replacing @xmath100 in the formulas , the condition numbers of the orthogonal projection are @xmath261 both @xmath68 and @xmath69 are independent sources of ill - conditioning .      the linear least squares residual is invariant with respect to transformations of the matrix columns , so there is reason to seek changes to the columns that might reduce the condition number of the residual . if @xmath2 is replaced by @xmath262 for some nonsingular matrix @xmath191 that makes @xmath263 an orthonormal matrix ,",
    "then with the scale factors of equation ( [ eqn : scale - factors ] ) it has been noted in section [ sec : iterative ] that the tight estimate is @xmath264 which leaves only @xmath69 as a source of ill - conditioning .",
    "a less costly transformation is @xmath265 for a diagonal matrix @xmath266 .",
    "two reasons suggest choosing @xmath266 to equilibrate the columns of @xmath267 .",
    "first , least squares problems typically are solved using the @xmath268 factorization .",
    "the errors of that calculation can be accounted for by backward rounding errors whose relative size in each column is roughly the same across all columns .",
    "second , equilibrating the columns is approximately the optimal column scaling to reduce the matrix condition number @xcite .",
    "nevertheless , even if @xmath269 , the scaling also alters van der sluis s ratio in equation ( [ eqn : three ] ) , so it is unclear whether the net change to the condition number in equation ( [ eqn : chia ] ) is for the better .",
    "i thank the editor prof .",
    "d.  oleary and the three referees for corrections and suggestions that much improved this paper ."
  ],
  "abstract_text": [
    "<S> a simple formula is proved to be a tight estimate for the condition number of the full rank linear least squares residual with respect to the matrix of least squares coefficients and scaled @xmath0-norms . </S>",
    "<S> the tight estimate reveals that the condition number depends on three quantities , two of which can cause ill - conditioning . </S>",
    "<S> the numerical linear algebra literature presents several estimates of various instances of these condition numbers . </S>",
    "<S> all the prior values exceed the formula introduced here , sometimes by large factors .    </S>",
    "<S> residual , projection , linear least squares , condition number , applications of functional analysis    65f35 , 62j05 , 15a60 </S>"
  ]
}