{
  "article_text": [
    "one aspect of the information technology revolution is that huge amounts of data have become available .",
    "for example , every transaction in many financial markets is recorded and bio - informatics technology allows us today to monitor genome wide gene expression .",
    "these informations may allow for a quite detailed description of such complex systems as a financial market or a cell , to quote just two examples .",
    "however our understanding of a complex systems is , in many cases , limited by our ability to efficiently organize massive streams of information . as a result",
    ", methods for handling , organizing or mining large data sets have become of great practical importance .",
    "data clustering deals with the problem of classifying a set of @xmath0 objects into groups so that objects within the same group are more similar than objects belonging to different groups .",
    "each object is identified by a number @xmath1 of measurable features : hence object @xmath2 can be represented as a point @xmath3 in a @xmath1 dimensional space .",
    "data clustering aims at identifying clusters as more densely populated regions in this vector space .",
    "more precisely , a configuration of clusters is represented by a set @xmath4 of integer labels , where @xmath5 is the cluster to which object @xmath6 belongs .",
    "we focus on unsupervised approaches , which do not require any further information apart from that contained in the data .",
    "these are of great practical importance specially in the presence of huge data sets .",
    "we assume , in other words , that the data set is homogeneous : all features are equally important or are already appropriately weighted .",
    "the classic approaches to data clustering are partitioning methods and hierarchical clustering . partitioning methods",
    "are based on two elements : _ 1 ) _ a distance between objects , which allows one to measure their similarity and _ 2 ) _ a cost function whose minima correspond to `` optimal '' clustering configurations .",
    "for example , a typical k - means ( km ) approach takes as cost function the sum of squared distances of objects to the centroid @xmath7 of the cluster in which they are classified @xcite : @xmath8 where @xmath9 is the number of objects in cluster @xmath10 , and @xmath11 defines the euclidean distance .    apart from cases where a cost function is naturally suggested by the problem itself distribution centers which should serve @xmath0 cities .",
    "it is natural to look for the locations which minimize the sum of distances . ]",
    ", the choice of the cost function or of a distance is an element of arbitrariness .",
    "a further problem of these approaches is that one needs to predefine the number @xmath12 of clusters from the beginning .",
    "note , for example that @xmath13 of eq .",
    "( [ hkm ] ) attains its minimal value @xmath14 when each object is in a different cluster ( @xmath15 ) .",
    "the number @xmath12 of clusters should then be fixed _ a priori _ or one has to introduce a @xmath12-dependent term in the cost function .",
    "this is a further element of arbitrariness",
    ".    the problem of finding the cluster structure which minimizes the cost function may be quite difficult : depending on the form of the cost function and on the data set the `` cost landscape '' can be either simple , with a single minimum which is easily accessible dynamically , or complex , with many metastable minima .",
    "data clustering methods also differ for the specific algorithms used to reach a ( local ) minimum .",
    "a second , very popular approach to data clustering ",
    "called hierarchical clustering  is based on the definition of a distance between objects and clusters of objects and a very simple algorithm : given a configuration with @xmath16 clusters , it merges the two closest clusters into a single one . in this way",
    ", starting from the configuration with @xmath17 clusters , the algorithm generates a sequence of configurations as @xmath12 varies from @xmath0 to @xmath18 .",
    "this sequence of configurations and their hierarchic organization , can be represented by a convenient and compact graphical tool called dendrogram @xcite . the minimal spanning tree algorithm",
    " also called single linkage ( sl )  the average linkage ( al ) and the centroid linkage ( cl ) algorithms are examples of hierarchical clustering methods and @xmath19 is the minimal ( average ) distance between items in cluster @xmath10 and items in cluster @xmath19 ; in cl the distance @xmath20 between centroids is used . ] . rather than a single cluster structure , this algorithm provides a hierarchic sequence of cluster structures .",
    "the choice of the best cluster structure is left arbitrary .",
    "applications of this approach to real world data is discussed in refs .",
    "@xcite .",
    "many other approaches to data clustering have been proposed : for example refs .",
    "@xcite used singular value decomposition to identify clusters .",
    "identifying principal components with cluster structures imposes an orthogonality constraint between clusters which may be unnatural .",
    "expectation minimization is a further approach @xcite where the density of points is modeled as a mixture of gaussians whose centering and scale parameters are fit by maximum likelihood .",
    "kohonen et al .",
    "@xcite have proposed an algorithm based on self - organizing maps ( som ) whereas blatt _",
    "proposed the super - paramagnetic clustering ( spc ) method @xcite .",
    "the latter is based on a mapping to an interacting particle system whose ",
    "magnetic \" properties describe the cluster structure of data and has been applied to a range of problems ( see e.g. @xcite ) .",
    "these methods rely on _ ad hoc _ definitions of a dynamics ( som ) or of the particle - particle interaction ( spc ) which are tuned by several parameters or functions .    in the following sections we briefly review yet a different approach to dataclustering that we recently proposed @xcite",
    ", we describe possible algorithms based on it , and we finally compare the outcomes with those of standard data clustering methods , as those described above .",
    "we have devised @xcite a fully unsupervised , parameter free approach to data clustering which derives from a maximum likelihood ( ml ) principle .",
    "the key idea is that objects are similar if they have something in common . in a correct classification",
    ", objects belonging to the same cluster should share a common component :    @xmath21    here @xmath22 is the vector of features of object @xmath6 , normalized so that @xmath23 and @xmath24 ^ 2=d$ ] for all @xmath25 ) by adding a constant term @xmath26 and a scale factor @xmath27 .",
    "the maximum likelihood estimates of these parameters are the mean and the variance .",
    "subtracting the mean and rescaling by the variance , leaves us with the normalized data set .",
    "the parameters @xmath28 and @xmath27 are irrelevant as far as the cluster structure is concerned .",
    "the latter indeed only depends on the `` internal structure '' of correlations . ] ; @xmath5 is the label of the cluster to which it belongs .",
    "@xmath29 is the vector of features of cluster @xmath10 and @xmath30 tunes the similarity of objects within cluster @xmath10 : for @xmath31 all objects with @xmath32 are identical whereas when @xmath30 is small objects are very different . the cluster index @xmath10 ranges from @xmath18 to @xmath0 in order to allow also for the case of @xmath0 clusters of one object each .",
    "@xmath33 describes the deviation of the features of object @xmath6 from the cluster s features and measurement errors .",
    "we take eq .",
    "( [ ansatz ] ) as a statistical hypothesis and assume that both @xmath29 and @xmath34 ( for all @xmath35 ) are gaussian vectors with zero mean and variance @xmath36=e[(\\epsilon_i^{(t)})^2]=1 $ ] .",
    "for any given set of parameters @xmath37 it is possible @xcite to compute the probability ( density ) @xmath38 of observing the data set @xmath39 as a realization of eq .",
    "( [ ansatz ] ) . from this",
    "it is possible @xcite to compute the likelihood is invoked by bayes formula in this passage .",
    "we take @xmath40 which means that every cluster structure @xmath41 is _ a priori _ equiprobable . ]",
    "it turns out @xcite that the resulting expression depends only on the pearson s coefficient    @xmath43    for a given cluster structure @xmath44 , the likelihood is maximal when the parameters @xmath30 take the values    @xmath45    if @xmath46 and @xmath47 if @xmath48 . here",
    "@xmath49 is the number of objects in cluster @xmath10 and    @xmath50    note indeed that when @xmath51 , as for uncorrelated objects , then @xmath52 whereas if objects are very similar , @xmath53 and @xmath54 .",
    "the maximum likelihood of structure @xmath44 can be written as @xmath55 , where the log - likelihood per feature @xmath56 is given by    @xmath57 . \\label{hc}\\ ] ]    this depends on the original data through the coefficients @xmath58 of eq .",
    "( [ nscs ] ) .",
    "the function @xmath56 provides a likelihood measure for cluster structures .",
    "the ml structure is that which maximizes @xmath56 .",
    "there are several interesting features of @xmath56 :    * if the objects are unrelated ( @xmath47 or @xmath59 ) or if they are classified in singleton clusters ( @xmath15 for all @xmath10 ) we find @xmath60 . loosely speaking",
    ", @xmath61 measures the amount of structure present in the data - set .",
    "* the maxima of @xmath56 do not necessarily coincide with a single cluster containing all objects  as in the spc approach of ref .",
    "@xcite  nor with the configuration with all objects in different clusters  as for @xmath62 ) can be written as @xmath63 . ]",
    "* @xmath56 does not depend on any parameter . *",
    "the number @xmath12 of clusters is not fixed _ a priori_. rather it is predicted . *",
    "the interpretation of the results is transparent in terms of the model ( [ ansatz ] ) .",
    "( [ ansatz ] ) may not be the most appropriate description for a particular data set . in much the same way",
    ", a straight line may not be the best description of a set of points on a plane .",
    "still in this example , least squares provide an unambiguous method to compute the coefficients and a statistical measure of the goodness of fit .",
    "the same is true for our method : a sharp maximum of @xmath64 indicates a robust cluster structure whereas when @xmath64 is small and has several local maxima the cluster structure is not statistically significant .",
    "the statistical hypothesis eq .",
    "( [ ansatz ] ) is specially helpful for high dimensional data sets ( @xmath65 ) where geometric intuition becomes problematic . from the point of view of the computational cost",
    ", the dimensionality @xmath1 of the data set enters only in the calculation of the matrix @xmath66 . for small @xmath1 methods based on geometry and visual inspection",
    "may be preferrable .",
    "our approach has some similarity with expectation maximization data clustering @xcite , which is also based on likelihood maximization .",
    "however the statistical hypothesis in ref .",
    "@xcite is very different from eq .",
    "( [ ansatz ] ) .",
    "the function @xmath67 is a measure of likelihood for cluster structures and it can be used to compare cluster structures produced by standard methods .",
    "for example , it allows one to compare classifications with a different number of clusters and hence to select the optimal number of clusters in km , sl , al or cl algorithms .",
    "we shall compare later the cluster structure found in this way with those found with algorithms based on @xmath56 itself .",
    "most importantly in fact , @xmath56 can be used as the basis for clustering algorithms .    a very powerful method",
    "to find ml configurations is simulated annealing ( sa ) @xcite with cost function @xmath68 .",
    "this is simply implemented via metropolis dynamics @xcite on the dynamical variables @xmath5 with progressively decreasing ( fictitious ) temperature @xmath69 .",
    "this method produces a cluster configuration , which , if the annealing schedule is appropriately chosen , has a good chance of being that of maximum likelihood .",
    "the behavior of the system as a function of the fictitious temperature is similar to that described in ref .",
    ". however , the spc method ( in its original formulation @xcite ) yields a trivial configuration with a single cluster in the limit @xmath70 .",
    "this is a consequence of the mapping of the data set into a particle system with ferromagnetic interactions , corresponding to positive correlations .",
    "this forces one to study intermediate temperatures .",
    "the ml configuration found in our method when @xmath70 is already non - trivial .",
    "further insight on the cluster structure can be obtained by studying the system at finite @xmath69 , as shown in ref .",
    "@xcite . for simplicity however , we will limit the present discussion to the @xmath71 limit .",
    "a second possibility is that of using a deterministic maximization ( dm ) technique .",
    "we discuss below a very simple algorithm : given a configuration @xmath72 , for all @xmath25 ( in some fixed order ) propose all moves @xmath73 and compute the corresponding variation @xmath74 of @xmath56 ; find @xmath75 and set @xmath76 .",
    "repeat these steps until no change occurs .",
    "this will lead in the end to a local maximum of @xmath56 which we shall call the @xmath77 configuration .",
    "finally we propose a merging algorithm ( mr ) in the spirit of hierarchical clustering based on @xmath56 .    1 .",
    "start from @xmath0 clusters composed of one object each ( e.g. @xmath78 @xmath79 ) .",
    "2 .   at each step of the algorithm merge two clusters into a single one in such a way that the cost @xmath56 of the resulting configuration is minimal .",
    "repeat step 1 ) @xmath80 times until the configuration with a single cluster is reached .",
    "let @xmath81 be the state with maximal @xmath56 found with this algorithm .",
    "@xmath81 is not necessarily a local maximum of @xmath56 with respect to single `` spin - flip '' moves @xmath82 , while @xmath83 and @xmath77 are local maxima .",
    "hence , we expect that @xmath84 . on the other hand ,",
    "the hierarchical clustering algorithm is definitely faster than both of the others : even if it delivers only an approximation to the maximum likelihood , faster execution can be crucial when dealing with huge data sets .",
    "in addition , the hierarchical clustering technique mr offers a very convenient graphical representation of the cluster structure in terms of dendrograms .",
    "usually dendrograms report the tree structure as a function of a resolution length . as this",
    "length increases , clusters are merged one by one until a single cluster remains when the resolution exceeds the maximal distance @xmath85 between data points .",
    "we draw the dendrogram as a function of the log - likelihood per feature @xmath56 . at each merging step @xmath86",
    "let @xmath87 , @xmath88 be the contribution of clusters @xmath10 and @xmath19 respectively to @xmath56 before merging and @xmath89 be the corresponding cluster log - likelihood of the merged cluster @xmath90 .",
    "in the dendrogram , we merge two points at height @xmath87 and @xmath88 into a single point at height @xmath89 .",
    "there are three types of branching points : _",
    "i ) _ if @xmath91 the step increases the likelihood of the configuration _ ii ) _ if @xmath92 but @xmath93 the global likelihood decreases but the cluster likelihood increases . finally _",
    "iii ) _ steps with @xmath94 represent unlikely merging moves .",
    "we discuss the application of the above ideas to two different data sets : the first consists of daily returns of the nyse from january the 1st 1987 to march the 30th 1999 .",
    "we consider @xmath95 of the assets traded most frequently throughout this period @xcite .",
    "if @xmath96 is the opening price of asset @xmath6 in day @xmath97 , we set @xmath98 where @xmath28 and @xmath27 enforce normalization ( @xmath99 and @xmath100 ) .",
    "actually it is convenient to introduce a further linear transformation in order to eliminate the so called `` market mode '' : @xmath101 where @xmath102 so that @xmath103 ( @xmath104 is a coefficient to restore @xmath105 ) .",
    "secondly we discuss the gene expression data set measured in @xcite and already studied in refs .",
    "@xcite with a variety of techniques .",
    "the data measure the expression of the 2467 genes of known function of the yeast _ saccharomyces cerevisi _ , whose complete genome has been sequenced . in particular , we focused on the 18 time values coming from the alpha - factor block - and - release experiment in @xcite ; these correspond to approximately two cell cycles .",
    "also in this case , we take the logarithm of the value of the gene expression , so that the data distribution is closer to a gaussian , and normalize the sets as required by eq .",
    "[ ansatz ] .    in both cases the data set",
    "is homogeneous by definition : features characterize the system  the market or the cell  at different times and there is no _ a priori _ reason to weight features differently .",
    "the main objective of data clustering for these data sets is that of identifying groups of assets with a similar market behavior or groups of genes with a similar function .",
    "this information is of considerable interest for risk management strategies and for the understanding of biological functions respectively in the two cases .",
    "we shall not enter here into details of either finance or genetics , but rather focus on the properties of data clustering algorithms for these two case studies .",
    "on one hand we are interested in identifying fast clustering algorithms , on the other it is desirable that the resulting cluster structures does not depend strongly on the detailed algorithm used . in few words",
    ", one would like to identify fast algorithms which provide results which are consistent with more elaborate and time consuming ones .",
    "note that the two data sets are very different in nature : for market data time series are very long ( @xmath106 ) and correlations are very weak while gene expression is recorded for few ( @xmath107 ) time points but correlations are very strong .",
    "thus they constitute a good testing ground for our analysis .    the cluster structure which emerges from the minimization of @xmath108",
    "are quite meaningful : for market data we find a strong overlap with the classification of stocks in sectors of economic activity . in the gene expression data set , as in ref .",
    "@xcite , our method identifies clusters related to biological functions .",
    "this information is preserved more or less in the ml cluster structures found with different methods .",
    "is plotted against the number of clusters for the different methods discussed in the text .",
    "points corresponds to the maximal likelihood configurations .",
    "the upper plot ( a ) refers to financial data , the bottom one ( b ) refers to gene expression data .",
    "configurations km and dm are obtained by deterministic minimization of @xmath13 and @xmath109 respectively , starting from all configurations generated by al and mr respectively .",
    "@xmath12 is held fixed in the minimization of @xmath13 but not in the maximization of @xmath56 .",
    "hence dm can have a number of clusters different from that of the starting mr configuration.,width=226,height=340 ]    .number of clusters @xmath12 , and likelihood per data for the maximum likelihood cluster structures obtained with different methods for the gene expression and the financial datasets . [ cols=\"<,^,^,^,^ \" , ]     [ table5 ]     ( i.e. 3 orders of magnitude larger than the noise background).,width=264,height=377 ]    the mr algorithm delivers information on the hierarchical organization of the data set .",
    "the dendrograms for the two case studies are shown in figs .",
    "[ figdendfin ] and [ figdendgene ] .",
    "the dendrogram of market data reveals a complex structure consistent with the scaling laws reported in ref .",
    "@xcite ( see inset ) .",
    "a much more uniform cluster distribution is found in gene expression data .",
    "[ figdendfin ] and [ figdendgene ] only report the first two types of branch points : _",
    "i ) _ those which increase the total likelihood and _ ii ) _ those which do not increase the total likelihood but where the likelihood of the unified cluster is larger than those of both individual clusters .",
    "these two sets of branching points are clearly separate and describe the cluster structure at different resolutions . in most branching of type _",
    "i ) _ a large cluster merges with a single object whereas type _ ii ) _ steps merge clusters into larger clusters .",
    "gene expression data shows a rich hierarchical structure of type _ ii ) _ branchings .",
    "the ml structure has a large number of clusters with @xmath110 elements .",
    "this suggests that the model eq .",
    "( [ ansatz ] ) does not provide an exhaustive description of the correlations of this data set .",
    "there are indeed sizeable correlations between clusters that are not taken into account . an alternative way to describe these correlations , besides that provided by the mr algorithm , is to perform a _ reclustering _ of the cluster patterns @xmath7 with the sa algorithm .",
    "more precisely this amounts to taking @xmath111 as the input data set .",
    "reclustering steps can be iterated until a configuration with singleton clusters is found .",
    "this leaves us with a sequence of nested reclustered configurations which provide a hierarchical description of the correlations present in the original data set . in fig .",
    "[ clust2 ] we show the effect of a second application of the sa clustering procedure on the biological data set .",
    "we have discussed a maximum likelihood approach to data clustering and its application to two examples . the method is based on a simple statistical description of data where similar objects have something in common ( eq . [ ansatz ] ) .",
    "the likelihood that a particular data set is described by such a model with a given cluster structure can be efficiently computed .",
    "this provides a parameter - free measure for cluster structures which can then be taken as the basis of clustering algorithms",
    ".    different algorithms to find maxima of the likelihood  or good approximations to them  have been introduced . on one hand",
    "we discuss computationally expensive methods , such as simulated annealing technique ( sa ) and deterministic minimization ( dm ) algorithms , which are expected to provide a good approximation to the maximum likelihood structure . at odd with most other methods ,",
    "the number of clusters should not be fixed _ a priori _ but is predicted by our algorithms . on the other hand ,",
    "we introduce a simple deterministic hierarchical clustering algorithm ( mr ) which provides a much faster approximation .",
    "we show that standard methods with distance based measures of clustering predict cluster structures which may be very different according to the specific algorithm used ( sl , al or km ) . on the contrary , the maximum likelihood structures predicted by different algorithms based on the measure @xmath56 ( mr , dm and sa ) are very similar .",
    "this suggests that , for instances where a meaningful cluster structure exists , the _ log - likelihood landscape _ has a broad and easily accessible maximum , whereas distance - based cost functions such as @xmath13 ( eq . [ hkm ] ) may give rise to a more complex landscape .",
    "data clustering has been regarded as an ill defined problem ( see e.g. @xcite ) .",
    "indeed in conventional approaches one needs to define a similarity measure and a resolution scale .",
    "the approach proposed in the present work does not suffer from these drawbacks .",
    "the cluster structure is entirely determined by the internal correlations of the data set .",
    "this makes data clustering a well defined problem .",
    "t. kohonen , k. mkisara , o. simula and j. kangas ( 1991 ) , _ artificaial networks _",
    ", amsterdam ; tamayo p. , slonim d. , mesirov j. , zhu q. , kitareewan s. , dmitrovsky e. , lander e.s .",
    ", golub t.r .",
    "( 1999 ) _ proc .",
    "usa _ * 96 * , 2907  2912 ."
  ],
  "abstract_text": [
    "<S> we address the problem of data clustering by introducing an unsupervised , parameter free approach based on maximum likelihood principle . </S>",
    "<S> starting from the observation that data sets belonging to the same cluster share a common information , we construct an expression for the likelihood of any possible cluster structure . </S>",
    "<S> the likelihood in turn depends only on the pearson s coefficient of the data . </S>",
    "<S> we discuss clustering algorithms that provide a fast and reliable approximation to maximum likelihood configurations . </S>",
    "<S> compared to standard clustering methods , our approach has the advantages that _ </S>",
    "<S> i ) _ it is parameter free , _ ii ) _ the number of clusters need not be fixed in advance and _ iii ) _ the interpretation of the results is transparent . in order to test our approach and compare it with standard clustering algorithms , we analyze two very different data sets : time series of financial market returns and gene expression data . </S>",
    "<S> we find that different maximization algorithms produce similar cluster structures whereas the outcome of standard algorithms has a much wider variability .    </S>",
    "<S> keyword dataclustering ; econophysics ; gene expression + pacs 02.50.le , 05.40.+j , 64.60.ak , 89.90.+n </S>"
  ]
}