{
  "article_text": [
    "a signal of interest @xmath0 , which is a random vector taking values in @xmath1 with ( prior ) distribution @xmath2 ( i.e. , @xmath0 is gaussian distributed with mean @xmath3 and @xmath4 covariance matrix @xmath5 ) .",
    "the signal @xmath0 is carried over a noisy channel to a sensor , according to the model @xmath6 where @xmath7 is a full rank channel matrix . for simplicity , in this paper",
    "we focus on the case where @xmath8 , though analogous results are obtained when @xmath9 .",
    "the problem is to compress @xmath10 realizations of @xmath11 ( @xmath12 , @xmath13 ) with @xmath10 measurements ( where @xmath10 is specified upfront ) .",
    "but the implementation of each compression has a noise penalty .",
    "so , the @xmath14th compressed measurement is @xmath15 where the compression matrix @xmath16 is @xmath17 .",
    "consequently , the measurement @xmath18 takes values in @xmath19 .",
    "assume that the measurement noise @xmath20 has distribution @xmath21 and channel noise @xmath22 has distribution @xmath23 .",
    "the measurement and channel noise sequences are independent over @xmath14 and independent of each other .",
    "equivalently , we can rewrite ( [ on ] ) as @xmath24 and consider @xmath25 as the total noise with distribution @xmath26 .",
    "we consider the following adaptive ( sequential ) compression problem .",
    "for each @xmath13 , we are allowed to choose the compression matrix @xmath16 ( possibly subject to some constraint ) . moreover ,",
    "our choice is allowed to depend on the entire history of measurements up to that point : @xmath27 .",
    "let the posterior distribution of @xmath0 given @xmath28 be @xmath29 .",
    "more specifically , @xmath30 can be written recursively for @xmath13 as @xmath31 where @xmath32 and @xmath33 .",
    "if this expression seems a little unwieldy , by the woodbury identity a simpler version is @xmath34 assuming that @xmath35 and @xmath36 are nonsingular .",
    "also define the _ entropy _ of the posterior distribution of @xmath0 given @xmath28 : @xmath37 the first term @xmath38 is actually proportional to the volume of the error concentration ellipse for @xmath39 $ ] .",
    "we focus on a common information - theoretic criterion for choosing the compression matrices : for the @xmath14th compression matrix , we pick @xmath16 to maximize the _ per - stage information gain _ , defined as @xmath40 . for reasons that will be made clear later , we refer to this strategy as a _",
    "greedy _ policy .",
    "the term _ policy _ simply refers to a rule for picking @xmath16 for each @xmath14 based on @xmath41 .",
    "suppose that the overall goal is to maximize the _ net information gain _",
    ", defined as @xmath42 .",
    "we ask the following questions : does the greedy policy achieve this goal ? if not , then what policy achieves it ? how much better is such a policy than the greedy one",
    "are there cases where the greedy policy does achieve this goal ? in section  [ gp ] , we analyze the greedy policy and compute its net information gain . in section  [ op ] , to find the net information gain of the optimal policy , we introduce a relaxed optimization problem , which can be solved as a water - filling problem . in section  [ g = o ] , we derive two sufficient conditions under which the greedy policy is optimal . in section  [ examples ] , we give examples for which the greedy policy is not optimal .",
    "we now explore how the _ greedy policy _ performs for the adaptive measurement problem . before proceeding ,",
    "we first make some remarks on the information gain criterion :    * information gain as defined in this paper also goes by the name _ mutual information _ between @xmath0 and @xmath18 in the case of per - stage information gain , and between @xmath0 and @xmath43 in the case of net information gain .",
    "* the net information gain can be written as the cumulative sum of the per - stage information gains : @xmath44 this is why the greedy policy is named as such ; at each stage @xmath14 , the greedy policy simply maximizes the immediate ( short - term ) contribution @xmath45 to the overall cumulative sum . * using the formulas ( [ pp0 ] ) and ( [ hh ] ) for @xmath46 and @xmath30 , we can write @xmath47 where @xmath48 is the @xmath4 identity matrix . in other words , at the @xmath14th stage , the greedy policy minimizes ( with respect to @xmath16 ) @xmath49 * equivalently , using the other formula ( [ pp ] ) for @xmath30 , the greedy policy maximizes @xmath50 at each stage . for the purpose of optimization , the @xmath51 function in the objective functions above can be dropped , owing to its monotonicity .",
    "it is worth noting that we may dispense with the assumption of gaussian distributed variables and argue that we are simply minimizing @xmath52 , which is proportional to the volume of the error concentration ellipse defined by @xmath53 .",
    "notice that the greedy policy does not use the values of @xmath54 ; its choice of @xmath16 depends only on @xmath35 , @xmath55 and @xmath56 .",
    "in fact , the formulas above show that information gain is a deterministic function of the model matrices ( in our particular setup ) .",
    "this implies that the optimal policy can be computed by deterministic dynamic programming . in general",
    ", we would not expect the greedy policy to solve such a dynamic programming problem .",
    "however , as we will see in following sections , there are cases where it does .",
    "this subsection is devoted to the special case where @xmath57 ( i.e. , each measurement is a scalar ) .",
    "accordingly , we can write @xmath58 , where @xmath59 , @xmath60 , and @xmath61 . accordingly , the scalar measurement @xmath62 is given by @xmath63 for @xmath13 .",
    "this problem is the problem of designing the columns of compression matrix @xmath64 $ ] sequentially , one at a time . in the special case @xmath65 ,",
    "the measurement model is @xmath66 where @xmath67 is called the measurement vector , and @xmath68 is a white gaussian noise vector . in this context , the construction of a `` good '' compression matrix @xmath69 to convey information about @xmath0 is also a topic of interest . when @xmath70 , this is a problem of greedy adaptive noisy compressive sensing .",
    "our solution is a more general solution than this for the more general problem ( [ ssm1 ] ) . in this more general problem ,",
    "the uncompressed measurement @xmath71 is a noisy version of the filtered state @xmath72 , and compression by @xmath73 introduces measurement noise @xmath74 and colors the channel noise @xmath75 .",
    "the concept of sequential scalar measurements in a closed - loop fashion has been discussed in a number of recent papers ; e.g. , @xcite .",
    "the objective function for the optimization here can take a number of possible forms , besides the net information gain .",
    "for example , in @xcite , the objective is to maximize the posterior variance of the expected measurement .",
    "if the @xmath73 can only be chosen from a prescribed _ finite _ set , the optimal design of @xmath69 is essentially a sensor selection problem ( see @xcite,@xcite ) , where the greedy policy has been shown to perform well .",
    "for example , in the problem of sensor selection under a submodular objective function subject to a uniform matroid constraint @xcite , the greedy policy is suboptimal with a provable bound on its performance , using bounds from optimization of submodularity functions @xcite,@xcite .    consider a constraint of the form @xmath76 for @xmath13 ( where @xmath77 is the euclidean norm in @xmath78 ) , which is much more relaxed than a prescribed finite set .",
    "the constraint that @xmath69 has unit - norm columns is a standard setting for compressive sensing @xcite .",
    "the expression in ( [ eqn : ig ] ) simplifies to @xmath79 this expression further reduces ( see ( * ? ? ?",
    "* lemma  1.1 ) ) to @xmath80 combining ( [ ig ] ) and ( [ ig2 ] ) , the information gain at the @xmath14th step is @xmath81 it is obvious that the greedy policy maximizes @xmath82 to obtain the maximal information gain in the @xmath14th step .",
    "clearly , the measurement @xmath62 may be written as @xmath83 then ( [ ratio ] ) is simply the ratio of variance components : the numerator is @xmath84 , @xmath85 $ ] , and the denominator is @xmath86 .",
    "so the goal for the greedy policy is to select @xmath73 to maximize signal - to - noise ratio , where the signal is taken to be the part of the measurement @xmath62 that is due to error @xmath87 in the state estimate and noise is taken to be the sum of @xmath88 and @xmath74 .",
    "this is reasonable , as @xmath89 is now fixed by @xmath90 , and only variance components can be controlled by the measurement vector @xmath73 .",
    "the greedy policy can be described succinctly in terms of certain eigenvectors , as follows .",
    "denote the eigenvalues of @xmath91 by @xmath92 . for simplicity ,",
    "when @xmath93 we may omit the superscript and write @xmath94 for @xmath95 . since @xmath5 is a covariance matrix , which is symmetric , @xmath96 is also symmetric , and there exist corresponding orthonormal eigenvectors @xmath97 .",
    "clearly , @xmath98 the equalities hold when @xmath99 equals @xmath100 , which is the eigenvector of @xmath96 corresponding to its largest eigenvalue @xmath101 ; we take this to be what the greedy policy picks .",
    "if eigenvalues are repeated , we simply pick the eigenvector with smallest index @xmath102 . after picking @xmath103 , by ( [ pp0 ] ) we have @xmath104 where @xmath105 .",
    "we can verify the following : @xmath106 and @xmath107 so we see that @xmath108 has the same collection of eigenvectors as @xmath96 , and the nonzero eigenvalues of @xmath108 are @xmath109 . by induction , we conclude that , when applying the greedy policy , all the @xmath110s for @xmath111 have the same collection of eigenvectors and the greedy policy always picks the compressors @xmath73 , @xmath13 , from the set of eigenvectors @xmath112 .",
    "the implication is that this basis for the invariant subspace @xmath113 for the prior measurement covariance @xmath96 may be used to define a prescribed finite set of compression vectors from which compressors are to be drawn .",
    "the greedy policy then amounts to selecting the compressor @xmath73 to be the eigenvector of @xmath110 with eigenvalue @xmath114 .",
    "in other words , the greedy policy simply re - sorts the eigenvectors of @xmath96 , step - by - step , and selects the one with maximum eigenvalue .",
    "consequently , after applying @xmath10 iterations of the greedy policy , the net information gain is @xmath115 where @xmath116 , the largest eigenvalue of @xmath117 , is computed iteratively from the sequence @xmath118 .",
    "suppose that the uncompressed measurements are @xmath119 , @xmath13 , with @xmath120 , indicating no prior indication of shape for the error covariance matrix .",
    "assume that @xmath121 and @xmath122 .",
    "the choice of orthonormal eigenvectors for @xmath123 is arbitrary , with @xmath124 $ ] ( the standard basis for @xmath1 ) a particular choice that minimizes the complexity of compression .",
    "so compressed measurements will consist of the noisy measurements @xmath125 .    after picking @xmath126 ,",
    "the eigenvalues of @xmath127 are @xmath128 , @xmath129 .",
    "analogously , after picking @xmath130 , the eigenvalues of @xmath131 are @xmath132 , @xmath133 , and so on .",
    "if @xmath134 , then after @xmath10 iterations of the greedy policy the eigenvalues of @xmath135 are @xmath136 , @xmath137 . in the first @xmath10 iterations ,",
    "the per - step information gain is @xmath138 .    if @xmath139 , after @xmath140 iterations of the greedy policy , @xmath141 .",
    "we now simply encounter a similar situation as in the very beginning .",
    "we update @xmath142 and @xmath143 .",
    "the analysis above then applies again , leading to a round - robin selection of measurements .",
    "in this subsection we consider the problem of maximizing the net information gain , subject to the unit - norm constraint : @xmath144 the policy that maximizes ( [ un ] ) is called the _ optimal policy_.    the objective function can be written as @xmath145 where @xmath146:=\\left[\\frac{{\\mathbf{a}}_1}{\\sqrt{\\|{\\mathbf{a}}_1\\|^2\\sigma_n^2+\\sigma_w^2}},\\ldots,\\frac{{\\mathbf{a}}_m}{\\sqrt{\\|{\\mathbf{a}}_m\\|^2\\sigma_n^2+\\sigma_w^2}}\\right].\\ ] ] assume that the eigenvalue decomposition @xmath147 , where @xmath148 and @xmath149 $ ] .",
    "( the notation @xmath150 means the diagonal matrix with diagonal entries @xmath151 . ) then , continuing from ( [ ig3 ] ) , @xmath152 where @xmath153:={\\mathbf{v}}^t { \\mathbf{c}}.\\ ] ] since @xmath154 is nonsingular , the map @xmath155 is one - to - one .",
    "the constraint @xmath76 implies that @xmath156 , so the constraint in ( [ un ] ) can be written as @xmath157 for @xmath158 .",
    "the problem ( [ un ] ) is actually equivalent to the maximum a posteriori probability ( map ) problem ( see @xcite and @xcite ) .      to help characterize the optimal policy ( solution to ( [ un ] ) )",
    ", we now consider an alternative optimization problem with the same objective function in ( [ un ] ) but a relaxed constraint : @xmath159 i.e. , the columns of @xmath69 have _ average _ unit norm .",
    "we will call the policy that maximizes ( [ aun ] ) the _ relaxed optimal policy_.    the average unit - norm constraint in ( [ aun ] ) is equivalent to @xmath160 . with the scaling @xmath161",
    "the constraint @xmath162 becomes @xmath163 .",
    "hence , the relaxed optimization problem ( [ aun ] ) is equivalent to @xmath164 where @xmath165 and @xmath166 , for @xmath167 .    to solve ( [ max ] ) ,",
    "let us recall the following known results from @xcite .",
    "[ wit_l1 ] given any @xmath168 , there exists a unique integer @xmath169 , with @xmath170 , such that for @xmath171 we have @xmath172 while for indices @xmath14 , if any , satisfying @xmath173 we have @xmath174    for @xmath168 and @xmath169 as in lemma  [ wit_l1 ] , the sequence @xmath175 is strictly increasing .    by ( * ? ? ? * theorem  2 ) , the optimal value of the relaxed maximization problem ( [ max ] ) is @xmath176 where @xmath169 is defined by lemma  [ wit_l1 ] .",
    "specifically , @xmath169 is defined by the largest eigenvalues @xmath177 of @xmath96 , where in our case we set @xmath178 .",
    "in fact , the optimal value ( [ opt ] ) may also be derived from the solution to the well - known water - filling problem ( see @xcite for details ) .",
    "it is known from @xcite that the optimal value of the maximization problem @xmath179 is @xmath180 this optimal value is only obtained when @xmath181 where @xmath182 is called the",
    "_ water level_. by taking a close look at ( [ p ] ) , we can see that @xmath183 and @xmath184 .",
    "figure  [ fig : waterfilling ] illustrates the relation among @xmath185 , @xmath186 , and water level @xmath187 .    with the values of @xmath186 defined in ( [ p ] )",
    ", we can determine the @xmath188 that solves the maximization problem ( [ max ] ) .",
    "the optimal @xmath188 is obtained for , and only for , the following two cases .",
    "let @xmath189 be the @xmath190 matrix with @xmath191 , @xmath192 , and all other elements zero .",
    "* case 1 . @xmath193 or @xmath194 .",
    "then @xmath195 where @xmath196 is any @xmath197 orthonormal matrix . *",
    "@xmath198 for and only for @xmath199 with @xmath200 , @xmath201 .",
    "then @xmath202 where @xmath203 is any @xmath197 orthonormal matrix and @xmath204 any @xmath205 orthonormal matrix .",
    "this case is only possible when @xmath206 .",
    "( the notation @xmath207 denotes a block diagonal matrix with diagonal blocks @xmath208 . )    after obtaining @xmath188 , we can extract the optimal solution @xmath64 $ ] for the relaxed constraint problem ( [ aun ] ) by using ( [ gtg ] ) , ( [ ge ] ) , and ( [ ea ] ) .        our main motivation to relax the constraint to an _ average",
    "_ unit - norm constraint is our knowledge of the relaxed optimal solution .",
    "specifically , for the multivariate gaussian signal @xmath0 the maximal net information gain under the relaxed constraint is given by the water - filling solution .",
    "this helps us to identify cases where the greedy policy is in fact optimal , as discussed in the next section .",
    "in the preceding sections , we have discussed three types of policies : the greedy policy , the optimal policy , and the relaxed optimal policy .",
    "denote by @xmath209 , @xmath210 , and @xmath211 the net information gains associated with these three policies respectively .",
    "clearly , @xmath212 in the rest of this section , we characterize @xmath209 , @xmath210 , and @xmath211 . in general",
    ", we do not expect to have @xmath213 ; in other words , in general , greedy is not optimal .",
    "however , it is interesting to explore cases where greedy _ is _ optimal . in the rest of this section , we provide sufficient conditions for the greedy policy to be optimal .    before proceeding , we make the following observation on the net information gain . in ( [ max ] ) denote @xmath214 ; then the determinant in the objective function becomes @xmath215 under the unit - norm constraint , @xmath216    [ ra ] in the maximization problem ( [ un ] ) , if the @xmath73s were only picked from @xmath217 , by ( [ v ] ) @xmath218 where each @xmath219 is an integer multiple of @xmath220 and @xmath221 .",
    "this integer @xmath219 would be determined by the multiplicity of appearances of @xmath222 among @xmath223 .",
    "thus the net information gain would be @xmath224 where we use the fact that @xmath225 . clearly , to maximize the net information gain by selecting compressors from @xmath217 ,",
    "we should never pick @xmath73 from @xmath226 , because ( [ objequal ] ) is not a function of @xmath227 .",
    "in particular , the greedy policy picks @xmath73 from @xmath112 .",
    "after @xmath10 iterations of the greedy policy , the net information gain can be computed by the right hand side of ( [ objequal ] ) .",
    "we now provide two sufficient conditions ( in theorems  [ mr2 ] and [ thm1 ] ) under which @xmath213 holds for the sequential scalar measurements problem ( [ ssm1 ] ) .    [ mr2 ] suppose that @xmath73 , @xmath13 , can only be picked from the prescribed set @xmath228 , which is a subset of the orthonormal eigenvectors of @xmath96 . if @xmath229 , then the greedy policy is optimal , i.e. , @xmath213 .",
    "see appendix [ app1 ] .",
    "next , assume that we can pick @xmath73 to be any arbitrary vector with unit norm . in this much more complicated situation , we show @xmath213 by directly showing that @xmath230 , which implies that @xmath213 in light of ( [ order ] ) .",
    "[ thm1 ] assume that @xmath73 , @xmath13 , can be selected to be any vector with @xmath231 .",
    "if @xmath232 , where @xmath233 is some nonnegative integer , for @xmath234 , and @xmath169 divides @xmath235 , then the greedy policy is optimal , i.e. , @xmath213    see appendix [ app2 ]    the two theorems above furnish conditions under which greedy is optimal . however , these conditions are quite restrictive .",
    "indeed , as pointed out earlier , in general the greedy policy is not optimal .",
    "the restrictiveness of the sufficient conditions above help to highlight this fact . in the next section ,",
    "we provide examples of cases where greedy is _ not _ optimal .",
    "in this subsection we give an example where the greedy policy is not optimal for the scenario @xmath236 and @xmath237 .",
    "suppose that we are restricted to a set of only three choices for @xmath16 : @xmath238 note that @xmath239 . in this case , @xmath240",
    ". moreover , set @xmath241 , @xmath242 , and @xmath243 .",
    "let us see what the greedy policy would do in this case . for @xmath244",
    ", it would pick @xmath245 to maximize @xmath246 a quick calculation shows that for @xmath247 or @xmath248 , we have @xmath249 whereas for @xmath250 , @xmath251 so the greedy policy picks @xmath250 , which leads to @xmath252 .    for @xmath253",
    ", we go through the same calculations : for @xmath254 or @xmath248 , we have @xmath255 whereas for @xmath256 , @xmath257 so , this time the greedy policy picks @xmath258 ( or @xmath248 ) , after which @xmath259 .",
    "consider the alternative policy that picks @xmath247 and @xmath260 . in this case ,",
    "@xmath261 and so @xmath262 , which is clearly provides greater net information gain than the greedy policy . call this alternative policy the _ alternating policy _ ( because it alternates between @xmath263 and @xmath248 ) .    in conclusion , for this example the greedy policy is not optimal with respect to the objective of maximizing the net information gain . how much worse is the objective function of the greedy policy relative to that of the optimal policy ? on the face of it , this question seems easy to answer in light of the well - known fact that the net information gain is a submodular function .",
    "as mentioned before , in this case we would expect to be able to bound the suboptimality of the greedy policy compared to the optimal policy ( though we do not explicitly do that here ) .    nonetheless , it is worthwhile exploring this question a little further .",
    "suppose that we set @xmath264 and let the third choice in @xmath265 be @xmath266 , where @xmath267 is some small number .",
    "( note that the numerical example above is a special case with @xmath268 . ) in this case , it is straightforward to check that the greedy policy picks @xmath269 and @xmath254 ( or @xmath248 ) if @xmath270 is sufficiently small , resulting in @xmath271 which increases unboundedly as @xmath272 .",
    "however , the alternating policy results in @xmath273 which converges to @xmath274 as @xmath272 . hence ,",
    "letting @xmath270 get arbitrarily small , the ratio of @xmath275 for the greedy policy to that of the alternating policy can be made arbitrarily large .",
    "insofar as we accept minimizing @xmath275 to be an equivalent objective to maximizing the net information gain ( which differs by the normalizing factor @xmath276 and taking @xmath51 ) , this means that _ the greedy policy is arbitrarily worse than the alternating policy_.    what went wrong ?",
    "the greedy policy was `` fooled '' into picking @xmath277 at the first stage , because this choice maximizes the per - stage information gain in the first stage .",
    "but once it does that , it is stuck with its resulting covariance matrix @xmath127 .",
    "the alternating policy trades off the per - stage information gain in the first stage for the sake of better net information gain over two stages .",
    "the first measurement matrix @xmath263 `` sets up '' the covariance matrix @xmath127 so that the second measurement matrix @xmath248 can take advantage of it to obtain a superior covariance matrix @xmath131 after the second stage , embodying a form of `` delayed gratification . ''",
    "interestingly , the argument above depends on the value of @xmath270 being sufficiently small . for example , if @xmath278 , then the greedy policy has the same net information gain as the alternating policy , and is in fact optimal .",
    "an interesting observation to be made here is that the submodularity of the net information gain as an objective function depends crucially on including the @xmath51 function .",
    "in other words , although for the purpose of optimization we can dispense with the @xmath51 function in the objective function in view of its monotonicity , bounding the suboptimality of the greedy policy with respect to the optimal policy turns on submodularity , which relies on the presence of the @xmath51 function in the objective function . in particular , if we adopt the volume of the error concentration ellipse as an equivalent objective function , we can no longer bound the suboptimality of the greedy policy relative to the optimal policy  the greedy policy is provably _ arbitrarily worse _ in some scenarios , as our example above shows .",
    "consider the channel model @xmath236 and scalar measurements @xmath279 .",
    "assume that @xmath280,\\ ] ] @xmath243 , and set @xmath241 .",
    "our goal is to find @xmath281 such that @xmath99 , @xmath282 maximize the net information gain : @xmath283 by simple computation , we know that the eigenvalues of @xmath5 are @xmath284 and @xmath285 . if we follow the greedy policy , the eigenvalues of @xmath127 are @xmath286 and @xmath287 . by ( [ greedy ] ) ,",
    "the net information gain for the greedy policy is @xmath288    next we solve for the optimal solution .",
    "let @xmath289^t$ ] . by ( [ pp ] )",
    ", we have @xmath290.\\ ] ] we compute that @xmath291 when we choose @xmath282 in the second stage , we can simply maximize the information gain in that stage . in this special case",
    "when @xmath241 , the second stage is actually the last one .",
    "if @xmath99 is given , maximizing the net information gain is equivalent to maximizing the information gain in the second stage .",
    "therefore , the second step is equivalent to a greedy step . by ( [ greedy ] ) , @xmath292 by ( [ infogain ] ) , we know @xmath293 using @xmath294 , we simplify ( [ i1 ] ) and ( [ i2 ] ) to obtain @xmath295 this expression reaches its maximal value when @xmath296 .",
    "so the optimal net information gain is @xmath297 , when @xmath298^t \\ ] ] and @xmath299^t . \\ ] ] this implies that the greedy policy is not optimal .",
    "if @xmath73 , @xmath13 , can only be picked from @xmath112 , then by ( [ objequal ] ) the net information gain is @xmath300 .",
    "we can simply manage @xmath219 in each channel to maximize the net information gain . rewrite @xmath301 as we claimed before , @xmath302 where @xmath303 , @xmath304 , is an integer multiple of @xmath220 .",
    "inspired by the water - filling algorithm , we can consider @xmath305 as an allocation of @xmath10 blocks ( each with size @xmath220 ) into @xmath140 channels .",
    "in contrast to water - filling , we refer to this problem as _ block - filling _ ( or , to be more evocative , _ ice - cube - filling _ ) .",
    "the original heights of these channels are @xmath306 .",
    "finally , the net information gain is determined by the product @xmath307 of the final heights .",
    "the optimal solution can be extracted from an optimal allocation that maximizes ( [ heights ] ) .",
    "because @xmath308 , to maximize @xmath309 we should allocate nonzero values of @xmath303 in the first @xmath310 channels .",
    "accordingly , there exists an optimal solution @xmath311 such that @xmath312    assume that we pick @xmath73 , @xmath13 , using the greedy policy . by ( [ v1 ] ) and ( [ v2 ] )",
    ", we see that the @xmath14th iteration of the greedy algorithm only changes @xmath313 into @xmath314 , which is equivalent to changing @xmath315 into @xmath316 .",
    "consider this greedy policy in the viewpoint of block - filling .",
    "the greedy policy fills blocks to the lowest channel one by one .",
    "if there are more than one channel having the same lowest height , it adds to the channel with the smallest index .",
    "likewise , since the original heights of the channels are @xmath306 , the greedy policy only fills blocks to the first @xmath317 channels , i.e. , greedy solution @xmath318 also satisfies @xmath319    we now provide a necessary condition for both optimal and greedy solutions .",
    "[ necess ] assume that an allocation @xmath320 is determined by either an optimal solution or a greedy solution .",
    "if @xmath321 is nonzero , then @xmath322 is bounded in the interval @xmath323 .",
    "moreover , it suffices for the optimal and greedy solutions to pick from the set @xmath324 .",
    "first , assume that @xmath320 is given by an optimal solution .",
    "recall that @xmath322 is the final height of the @xmath14th channel . by examining the total volumes of water and blocks , we deduce the following .",
    "if @xmath325 and @xmath326 for some @xmath327 , where @xmath187 is the water level defined in ( [ waterlevel ] ) , then there exists some channel @xmath328 such that @xmath329 . for the purpose of proof by contradiction , let us assume that @xmath330 .",
    "we move the top block of the @xmath102th channel to the @xmath331th channel to get another allocation @xmath332 .",
    "clearly , @xmath333 and @xmath320 have the same entries except the @xmath102th and @xmath331th components .",
    "the argument in this paragraph is illustrated in figure  [ fig : etagamma ] .     from @xmath320.,width=321 ]    for simplicity , denote @xmath334 for @xmath13 .",
    "so @xmath335 because @xmath336 .",
    "thus @xmath333 gives a better allocation , which contradicts the optimality of @xmath320 . by a similar argument",
    ", we obtain that for any optimal solution @xmath320 , there also does not exist @xmath102 such that @xmath325 and @xmath337 . in conclusion ,",
    "the final height @xmath338 , @xmath192 , in each channel in the optimal solution is bounded in the interval @xmath323 .",
    "additionally , in both cases when @xmath339 and @xmath340 , @xmath341 .",
    "this means that it suffices for the optimal solution to pick from the set @xmath324 .",
    "next , we assume that @xmath320 is determined by a greedy solution .",
    "if @xmath325 and @xmath326 , for some @xmath327 , then there exists a channel with index @xmath328 such that @xmath342 . for the purpose of proof by contradiction , let us assume that @xmath343 .",
    "this implies that when the greedy algorithm fills the top block to the @xmath102th channel , it does not add that block to the @xmath331th channel with a lower height .",
    "this contradicts how the the greedy policy actually behaves . by a similar argument",
    ", there does not exist some channel @xmath102 such that @xmath325 and @xmath337 . in conclusion ,",
    "the final height @xmath338 , @xmath192 , in each channel in the greedy solution is bounded in the interval @xmath323 . moreover , @xmath341 .",
    "this means that it suffices for the greedy solution to pick from the set @xmath324 .",
    "we now proceed to the equivalence between the optimal solution and the greedy solution . to show this equivalence ,",
    "let @xmath344 be an arbitrary allocation of @xmath10 blocks satisfying the necessary condition in lemma  [ necess ] .",
    "next , we will show how to modify @xmath345 to obtain an optimal allocation .",
    "after that , we will also show how to modify @xmath345 to obtain an allocation that is generated by the greedy policy .",
    "it will then be evident that these two resulting allocations have the same information gain .    to obtain an optimal allocation from @xmath345 ,",
    "we first remove the top block from each channel whose height is above @xmath187 to get an auxiliary allocation @xmath346 .",
    "assume that the total number of removed blocks is @xmath347 .",
    "this auxiliary @xmath348 is unique , because each @xmath349 is simply the maximal number of blocks can be filled in the @xmath14th channel to obtain a height not above the water level : this number is uniquely determined by @xmath350 , @xmath187 , and @xmath10 .",
    "we now show how to re - allocate the removed @xmath347 blocks , so that , together with @xmath348 , we have an optimal allocation of all @xmath10 blocks .",
    "note that by lemma  [ necess ] , to obtain an optimal solution we can not allocate more than one block to any channel , because that would make the height of that channel above @xmath351 .",
    "we claim that the optimal allocation simply re - allocates the @xmath347 removed blocks to the lowest @xmath347 channels in @xmath348 .",
    "we can show this by contradiction .",
    "assume that the optimal allocation adds one block to the @xmath102th channel instead of a lower @xmath331th channel in @xmath348 .",
    "this means that @xmath352 , @xmath353 , and @xmath354 . by an argument similar to ( [ move ] ) ,",
    "if we move the top block in the @xmath102th channel to the @xmath331th channel , we would obtain a better allocation ( which gives a larger net information gain ) .",
    "this contradiction verifies our claim .",
    "next , we concentrate on the allocation provided by the greedy policy .",
    "first , we recall that at each step of the greedy algorithm it never fills a block to some higher channel instead of a lower one .",
    "so after the greedy algorithm fills one block to some channel , its height can not differ from a lower channel by more than @xmath220 .",
    "if we apply the greedy policy for picking @xmath73 , @xmath355 , then we obtain the same allocation as @xmath348 .",
    "this is because any other allocation of @xmath356 blocks would result in a channel , after its top block filled , with a height deviating by more than @xmath220 from some other channel .",
    "this allocation contradicts the behavior of the greedy policy . continuing with @xmath348 ,",
    "the greedy policy simply allocates the remaining @xmath347 blocks to the lowest @xmath347 channels one by one .",
    "so the greedy policy gives the same final heights as the optimal allocation .",
    "the only possible difference is the order of these heights .",
    "therefore , the greedy solution is equivalent to the optimal solution in the sense of giving the same net information gain , i.e. , @xmath213 .",
    "this completes the proof of theorem [ mr2 ] .",
    "we have studied the performance of the greedy policy in the viewpoint of block - filling in the proof of theorem [ mr2 ] . for the purpose of simplicity ,",
    "we rewrite @xmath232 as @xmath357 where @xmath358 .",
    "after @xmath359 iterations of the greedy policy , the heights in the first @xmath169 channels give a flat top , which is illustrated in figure [ fig : integer ] .",
    "there are @xmath360 blocks remaining after @xmath361 iterations .",
    "if @xmath169 divides @xmath360 , the final heights of the first @xmath169 channels still give a flat top coinciding with @xmath187 in each channel .",
    "therefore @xmath230 . from ( [ order ] )",
    ", we conclude that @xmath213 .    9 a. ashok , j. l. huang , and m. a. neifeld , `` information - optimal adaptive compressive imaging , '' _ proc . of the asilomar conf . on signals , systems , and computers _ , pacific grove , ca , nov .",
    "2011 , pp .",
    "12551259 . s. boyd and l. vandenberghe , _",
    "convex optimization_. cambridge , ma : cambridge university press , 2004 .",
    "g. calinescu , c. chekuri , m. pal , and j. vondrak , `` maximizing a monotone submodular function subject to a matroid constraint , '' _ the 20th sicomp conf .",
    "_ , 2009 . w. r. carson , m. chen , m. r. d. rodrigues , r. calderbank , and l. carin , `` communications - inspired projection design with application to compressive sensing , '' preprint .",
    "r. castro , j. haupt , r. nowak , and g. raz , `` finding needles in noisy haystacks , '' _ proc .",
    "ieee intl .",
    "conf .  on acoustics , speech and signal processing _",
    ", las vegas , nv , apr . 2008 , pp .  51335136 . j. ding and a. zhou , `` eigenvalues of rank - one updated matrices with some applications , '' _ applied mathematics letters _ , vol .",
    "20 , no .  12 , pp .  12231226 , 2007 .",
    "d. l. donoho , `` compressed sensing , '' _ ieee trans .",
    "inf . theory _ ,",
    "52 , no .  4 , pp .  12891306 , 2006 . m. elad , `` optimized projections for compressed sensing , '' _ ieee trans .",
    "signal process .",
    "_ , vol .",
    "55 , no .  12 , pp .  56955702 , 2007 .",
    "r. g. gallager , _ information theory and reliable communication_. new york : john wiley & sons , inc . , 1968",
    ". j. haupt , r. castro , and r. nowak , `` distilled sensing : adaptive sampling for sparse detection and estimation , '' preprint , jan .",
    "2010 [ online ] .",
    "available : http://www.ece.umn.edu/@xmath362jdhaupt/publications/sub10_ds.pdf j. haupt , r. castro , and r. nowak , `` improved bounds for sparse recovery from adaptive measurements , '' _ isit 2010 _ , austin , tx , jun .",
    "r. a. horn and c. r. johnson , _ matrix analysis_. cambridge , ma : cambridge university press , 1985 .",
    "s. ji , d. dunson , and l. carin , `` multitask compressive sensing , '' _ ieee trans .",
    "signal process .",
    "57 , no .  1 ,",
    "pp .  92106 , 2009 .",
    "s. ji , y. xue and l. carin , `` bayesian compressive sensing , '' _ ieee trans .",
    "signal process .",
    "_ , vol .",
    "56 , no .  6 , pp .",
    "23462356 , 2008 . s. joshi and s. boyd , `` sensor selection via convex optimization , '' _ ieee trans .",
    "signal process .",
    "_ , vol .",
    "57 , no .  2 , pp .  451462 , 2009 . j. ke , a. ashok , and m. a. neifeld , `` object reconstruction from adaptive compressive measurements in feature - specific imaging '' , _ applied optics _",
    "49 , no .",
    "34 , pp .  h27-h39 , 2010 .",
    "e. liu and e. k. p. chong , `` on greedy adaptive measurements , '' _ proc .",
    "ciss _ , 2012 .",
    "e. liu , e. k. p. chong , and l. l. scharf `` greedy adaptive measurements with signal and measurement noise , '' submitted to asilomar conf . on signals , systems , and computers , mar .",
    "g. l. nemhauser and l. a. wolsey , `` best algorithms for approximating the maximum of a submodular set function , '' _ math .",
    "oper . research _ ,",
    "vol .  3 , no .  3 , pp .  177188 , 1978 .",
    "f. prez - cruz , m. r. rodrigues , and s. verd , `` mimo gaussian channels with arbitrary inputs : optimal precoding and power allocation , '' _ ieee trans .",
    "inf . theory _ ,",
    "56 , no .  3 , pp .",
    "10701084 , 2010 . h. rowaihy , s. eswaran , m. johnson , d. verma , a. bar - noy , t. brown , and t. l. portal , `` a survey of sensor selection schemes in wireless sensor networks , '' _ proc .",
    "spie _ , 2007 , vol .",
    "m. shamaiah , s. banerjee and h. vikalo , `` greedy sensor selection : leveraging submodularity , '' _ proc .  of the 49th ieee conf . on decision and control",
    "_ , atlanta , ga , dec . 2010 .",
    "d. p. wipf , j. a. palmer , and b. d. rao , `` perspectives on sparse bayesian learning , '' _ neural information processing systems ( nips ) _ , vancouver , canada , dec .",
    "h. s. witsenhausen , `` a determinant maximization problem occurring in the theory of data communication , '' _ siam j.  appl .",
    "math _ , vol .",
    "29 , no .  3 , pp .",
    "515522 , 1975 ."
  ],
  "abstract_text": [
    "<S> the purpose of this article is to examine greedy adaptive measurement policies in the context of a linear gaussian measurement model with an optimization criterion based on information gain . in the special case of sequential scalar measurements , </S>",
    "<S> we provide sufficient conditions under which the greedy policy actually is optimal in the sense of maximizing the net information gain . </S>",
    "<S> we also discuss cases where the greedy policy is provably not optimal .    </S>",
    "<S> entropy , information gain , compressive sensing , compressed sensing , greedy policy , optimal policy . </S>"
  ]
}