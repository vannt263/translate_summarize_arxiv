{
  "article_text": [
    "generating natural language descriptions for images has became an attractive research topic in recent years .",
    "the task is to generate sentences or phrases to summarize and describe the contents shown in images . with this technique ,",
    "the machines are enabled to imitate the behaviour of human beings who are able to capture the semantic meaning encoded in images . some previous work from , and designed templates for the sentence descriptions .",
    "the task is to fill in the templates based on the images .",
    "however , these approaches strongly limited the capability of models to generate sentence descriptions to only fixed patterns .",
    "other approaches transfer this task into a multimodal embedding problem .",
    "these work from , , , overlap with the scope of information retrieval .",
    "the goal is to map the images with sentences appearing in the training dataset together in a multimodal space .",
    "however , these models are only capable of returning sentence descriptions that existed in the training dataset .",
    "most of the state - of - the - art approaches are based on neural networks .",
    "these work combined convolutional neural network ( cnn ) with recurrent neural network ( rnn ) to generate image descriptions . develop a multimodal rnn for this task . in this neural network , the image features extracted from the vggnet ( a pretrained cnn proposed in )",
    "are fed into a rnn .",
    "conditioned on the image features and previous words , the rnn will generate a sequence of words recurrently to describe the images .",
    "similar to kaparthy s work , vinyals used the googlenet cnn to extract image features and train a lstm ( in ) as sequence generator . report a deep complex multimodal rnn for sentence generation .    in our approach",
    ", vggnet is employed to extract image features and a deep multilayer rnn is chosen as a sequence generator , on top of which we informatively added memory gate that controls image feeding . in each time step of rnn",
    ", we feed word in current time step as well as the image features into the hidden layer of rnn .",
    "inspired by the ideas in that the visual perception depends on short - term memory and has the recurrent natural , a memory gate is designed to control the input of image features to the hidden layer .",
    "the output of memory gate depends on the output of hidden layer at the previous time step . before feeding into the hidden layer ,",
    "the image features are multiplied by the output of gate element - wisely .",
    "therefore , the memory gates act as memory cells for image features .",
    "our model is trained on the flickr8k and flickr30k datasets from .",
    "we evaluate the bleu score ( proposed in ) of our model on the test datasets of both flickr8k and flickr30k .",
    "the preliminary results show that the performance of our model outperforms the state - of - the - art work .",
    "cnn has been proved as a powerful tool to extract image features , and has been widely used in image classification ( ) , object detection ( ) and other tasks . in this paper , we select the deep and powerful vggnet to extract image features . specifically , each raw image is fed into the vggnet as input . after the forward propagation , the last fully - connected layer will output a 4096 dimensions vector as the image features for each image .      the sentence can be represented as a sequence of single word .",
    "the time step @xmath0 is defined as the index of @xmath0 th word in the sentence to represent the position of each word .",
    "suppose the sentence contains @xmath1 words , the time step of first word is @xmath2 , the second word is @xmath3 , and for the last word is @xmath4 .",
    "for each sentence , we add a special start token at the first time step to indicate the start of the sentence , as well as the end token at the last time step as the end of each sentence .",
    "the single word is represented as a vector .",
    "some pretrained word vector models have been developed such as word2vec by and glove by .",
    "however , in this model , we trained the word vectors from scratch instead of directly adopted the pretrained model , since generally the retrained word vector will achieve higher performance for specific task .",
    "the standard rnn model can be expressed as , @xmath5 where @xmath6 is the output of the hidden layer at time step @xmath0 , @xmath7 is the word vector for the word at @xmath0 , @xmath8 is the output of hidden layer for the previous time step @xmath9 , @xmath10 is the activation function .",
    "@xmath11 has the dimension of @xmath12 by @xmath13 where @xmath12 is the dimension of hidden layer and @xmath13 is the dimension of word vector .",
    "@xmath14 has the dimension of @xmath12 by @xmath12 .",
    "@xmath15 has the dimension of @xmath16 by @xmath12 with @xmath16 as the vocabulary size . @xmath17 and @xmath18 are the bias terms .",
    "vector @xmath19 represents the probability of each word in the vocabulary to be the next word conditioned on the input words from time step @xmath20 to @xmath0 .    in our model , to improve the model capacity , we increase the depth of rnn by adding multiple hidden layers which is the same as deep transition rnn ( dt - rnn ) model reported by . shows that the dt - rnn is able to increase the size of family of functions it can represent in language modeling . unlike the standard rnn in equation  [ eq : std_rnn ] with only a single hidden layer at each time step ,",
    "@xmath21 hidden layers are stacked together at each time step in dt - rnn .",
    "the forward propagation of this deep rnn model is , @xmath22 where @xmath23 represent the output of @xmath21 hidden layers at @xmath0 .",
    "the word vector @xmath7 as well as the output of last hidden layer at previous time step @xmath24 are fed into the first hidden layer @xmath25 at @xmath0 .",
    "then , output of current hidden layer feeds into the next hidden layer consecutively .",
    "the output @xmath19 depends on the output of last hidden layer at current time step @xmath26 . in our model , the deep rnn in equation  [ eq : rnn ] is chosen as the sequence learner of sentences .",
    "we consider how to control feeding the image features into the deep rnn .",
    "instead of feeding the image features directly , we add a gate to control the magnitude of image feature feeds .",
    "the value of the gate depends on the state of hidden layers at previous time step .",
    "@xmath27 where @xmath28 represents the raw image , and @xmath29 is the image features extracted by cnn . @xmath30 has the dimension of @xmath12 by 4096 which maps the image features to the same space of hidden layers of rnn .",
    "@xmath31 is the output of gate , and @xmath32 is the element - wise multiplication . @xmath33 transfers the value of the last hidden layer in the previous time step ( @xmath34 in the equation ) to the gate @xmath31 .",
    "@xmath35 and @xmath36 are the bias terms . here",
    "we use the @xmath37 activation function and the value of @xmath31 ranges from @xmath38 to @xmath20 .    based on equation  [ eq : memory ] , the image features are fed into the first hidden layer at each time step , multiplied by the output of gate . since the value of gate depends on the last hidden layer of previous time step , the gate controls how much information from image is still needed for the current time step . in the case of @xmath39 ,",
    "the image features are not fed into rnn , while for @xmath40 , we feed full image features at each time step .    combining equations [ eq : rnn ] and [ eq : memory ] together , this model can be represented as : @xmath41 figure  [ fig : model ] shows the architecture of this model .",
    "recall the work of , image features are only fed at the first time step of rnn . due to the vanishing gradient problem , image features",
    "will not be learned well with long sentence and deep network .",
    "however , our model feed image features into rnn at each time step .",
    "therefore , our model is still able to learn information from the image even for larger time steps .",
    "the magnitude of image features is conditioned on the hidden state of previous time step . in another word ,",
    "the image features are encoded based on the status of how well our model has learned .",
    "compared with other work of based on lstm and work of based on multimodal embeddings , our model has the advantage of lower model complexity and easier to train .",
    "we experimented on the flickr8k and flickr30k datasets introduced in . each image in these datasets is described by @xmath42 independent sentences .",
    "therefore , for each image , we can create @xmath42 samples with each one as an image - sentence pair .",
    "we have @xmath43 and @xmath44 images for flickr8k and flickr30k respectively .",
    "each dataset has been splited into development data with @xmath45 images , test data with @xmath45 images and the rest images as training data .",
    "the data preprocessing procedure is the same as the work of .      during training ,",
    "cross entropy loss was chosen as the loss function .",
    "stochastic gradient descent ( sgd ) with minibatch size of @xmath46 image - sentence pairs was used during training . to make the model converge faster ,",
    "rmsprop annealing policy was adopted , where the step size of each parameter is scaled by the window - averaged norm of its gradient .    to overcome the vanishing gradient problem , relu is chosen as the activation function .",
    "also , we adopted the element - wise clip gradient tricks , where we clipped the gradient to @xmath42 . to regularize the model , we add l2 norm of weights to the loss function , and as suggested , we used dropout ratio of @xmath47 to all the layers except for the hidden layers .",
    "as equation  [ eq : model ] indicates , a model with large @xmath21 has deeper hidden layers , which leads to a large capacity . considering the size of the dataset is not large and in order to prevent overfitting , we adopt a small @xmath48 with 2 hidden layers in the experiments in equation  [ eq : model ] .",
    "we find @xmath49 epochs are enough to train this model for both datasets , and the hidden size was tuned to @xmath50 to achieve the best performance .",
    "the sentence description for each image in test dataset is generated by feeding the image features into the trained model with a start token . at each time step",
    ", we can directly choose the word corresponds to the one with highest probability in vocabulary as the output word , which is also the input word of next time step . following this method",
    ", we can generate a sentence recurrently until we reach the end token .    to evaluate the performance",
    ", we use the bleu score as evaluation metrics which has been widely adopted in the papers focus on this topic ( , , ) .",
    "the bleu score will evaluate the similarity of the generated sentences with the ground truth sentences .",
    "table  [ tab : flickr8k ] and table  [ tab : flickr30k ] show the bleu score for several models .",
    ".[tab : flickr8k ] the bleu score on flickr8k for different models .",
    "* b - n * is the bleu score up to n - gram . [ cols= \" < , > , < , < , < \" , ]     as shown on table  [ tab : flickr8k ] and table  [ tab : flickr30k ]",
    ", our model outperforms the results from and . while the performance of our model is lower than the original work from .",
    "however , this is because in the original work of , the authors used the googlenet ( in ) to extract the image features , while we used vggnet .",
    "therefore , it is unfair to directly compare the bleu score of our model with results reported by .    to make a fair comparison with the network in , we have downloaded the reproduced version of vinyals model from http://cs.stanford.edu / people / karpathy / neuraltalk/. in this reproduced model trained on flickr8k , the image features feed into vinyals model are extracted by vggnet , which is the same as the case in our model . from the last row of table  [",
    "tab : flickr8k ] , we can find that the performance of our model is better than the model in if both models use the vggnet image features .",
    "note that even though the reproduced model of based on flickr30k dataset is unavailable now , our model still outperforms other state - of - the - art works .",
    "we also tried to feed image features only at first time step ( i.e. , set @xmath39 except for the first time step ) as well as feed full image features at each time step ( i , e .",
    ", set @xmath40 for all time steps ) .",
    "but the results show that the performance all of these two schemes are lower than feeding image features at each time step with memory cells .",
    "in this paper , we developed a new model for generating image descriptions .",
    "the image features extracted from vggnet are fed into each time step of a multilayer deep rnn , where the image features vector is element - wisely multiplied by a memory vector determined by the state of the hidden layer at previous time step .",
    "experiments on flickr8k and flickr30k datasets show that this model achieves higher performance on bleu score .",
    "our model also benefit from its low complexity and ease of training .    as the extension of this work",
    ", we will train our model on a larger dataset such as mscoco , and will increase the number of hidden layers at each time step to further improve the performance of our model .",
    "we will also try to adopt other cnns such as googlenet to extract image features .",
    "also , in this work , we do not fine - tune the cnns on the new datasets , in future , we will try to train the model and tune the cnns together ."
  ],
  "abstract_text": [
    "<S> generating natural language descriptions for images is a challenging task . </S>",
    "<S> the traditional way is to use the convolutional neural network ( cnn ) to extract image features , followed by recurrent neural network ( rnn ) to generate sentences . in this paper </S>",
    "<S> , we present a new model that added memory cells to gate the feeding of image features to the deep neural network . </S>",
    "<S> the intuition is enabling our model to memorize how much information from images should be fed at each stage of the rnn . experiments on flickr8k and flickr30k datasets showed that our model outperforms other state - of - the - art models with higher bleu scores . </S>"
  ]
}