{
  "article_text": [
    "in academia , latent dirichlet allocation ( lda )  @xcite is a popular topic modeling technique , which is an unsupervised learning algorithm to infer semantic word sets called topics .",
    "however , very few successes of lda have been reported in industry .",
    "the major reason is that the largest lda models reported in literature  @xcite have up to @xmath0 topics , which can not cover completely the long - tail semantic word sets in big data .",
    "industrial applications like search engine and online advertising require the capability of learning many semantic word sets ( or topics ) that cover a large part of human knowledge , in particular , the long - tail part .",
    "as reported by linguistic data consortium ( ldc ) , there are millions of vocabulary words in either english , chinese , spanish or arabic  @xcite . taking polysemy and",
    "synonyms into consideration , a rough estimate of the number of word senses is close to the same magnitude of vocabulary words , i.e. , @xmath1 or @xmath3 topics for semantics of long - tail word sets .",
    "to the best of our knowledge , the number of topics in applications mentioned above is around two to three orders of magnitude larger than that in the current state - of - the - art  @xcite .",
    "the effectiveness of the large number of topics is inspired by  @xcite , which proposes a mapreduce - based _ frequent itemset mining _ algorithm to find the long - tail word sets .",
    "we notice that there is a word set containing the words `` whorf piraha chomsky anthropology linguistics '' . using web search , we find that this word set has a clear semantic meaning on the research by whorf and chomsky , which is related to anthropology and linguistics based on their study of the piraha language . however , it is a regret that the _ frequent itemset mining _ algorithm can not interpret new documents out of the training corpus .",
    "fortunately , lda overcomes this shortcoming and can infer highly interpretable and semantically coherent topics from new documents .    .",
    "]    to illustrate the advantage of the large number of topics @xmath4 , we use the point - wise mutual information ( pmi ) to measure the interpretability or semantic coherence of topics  @xcite .",
    "the higher pmi score corresponds to a better topic quality .",
    "first , we learn lda models with different number of topics @xmath5 on the soso data set described in subsection  [ sec : data ] .",
    "second , we calculate the pmi score of each topic and obtain the topic histograms over the pmi score bucket of different lda models . figure  [ fig : pmi ] shows the percentage of topics over the pmi score by four lda models with @xmath6 , @xmath0 , @xmath7 and @xmath1 topics , respectively . with the increase of topics from @xmath6 to @xmath1 , we see that the topic histograms shift toward the larger mean pmi score , i.e. , more and more topics have higher pmi scores .",
    "this phenomenon suggests that larger lda models tend to encode more interpretable and semantically coherent topics .    in this paper",
    ", we confirm that big lda models with at least @xmath1 topics can achieve a significant improvement in two industrial applications such as search engine and online advertising systems .",
    "this finding motivates us to pursue scalable topic modeling systems for big data . to achieve this goal",
    ", we develop a hierarchical distributed learning system called peacock that can generate a much larger number of topics than before .",
    "for example , peacock can learn at least @xmath1 topics from @xmath2 search queries .",
    "this improvement is nontrivial and raises many new technical challenges .",
    "first , how to make the system scalable to process big query data as well as lda parameters with fault tolerance ? second , how to do real - time topic prediction for new queries and how to remove duplicate topics to obtain high - quality ones ? finally , how to integrate big lda models into existing search engine and online advertising systems for a better performance ?",
    "we address these technical issues and summarize our contributions as follows :    * we design a new hierarchical distributed architecture including model parallelism to handle a large number of lda parameters as well as data parallelism to handle massive training corpora .",
    "we also use the pipeline and lock - free techniques to reduce communication and synchronization costs .",
    "this architecture runs on a computer cluster including thousands of cpu cores , which can learn @xmath8 topics from @xmath9 search queries , around two orders of magnitude larger than the current state - of - the - art reported in literature  @xcite . * when performing topic modeling for big data , we solve two new practical problems in real - world applications : real - time prediction and topic de - duplication . a new real - time prediction algorithm called rt - lda is developed to infer the topic distributions of unseen queries in search engine and online advertising systems .",
    "as far as topic de - duplication is concerned , we use two methods : 1 ) learning asymmetric dirichlet priors  @xcite ; 2 ) clustering similar topics by their @xmath10-similarities . *",
    "we examine the effectiveness of big lda models in two online industrial applications : search engine and online advertising systems .",
    "the performance improvements in both systems grow with the increasing number of the learned topics .",
    "we observe a significant improvement on search relevance when the number of topics increases from @xmath0 to @xmath7 .",
    "also , the topic features significantly improve the accuracy of ad click - through rate prediction when the number of topics increases from @xmath7 to @xmath1 .",
    "the rest of this paper is organized as follows . in the next section ,",
    "we discuss related work . in section",
    "[ sec : peacock ] , we present the hierarchical distributed architecture , real - time prediction and topic de - duplication in the proposed peacock system . in section",
    "[ sec : parallel ] , we show that peacock is more scalable to the larger number of topics than the state - of - the - art industrial solution yahoo!lda  @xcite . section  [ sec : industry ] shows two online industrial applications of peacock : search engine and online advertising systems . in section  [ sec : conclusion ] , we make conclusions and envision future work .",
    "there are five categories of batch inference algorithms proposed to estimate lda parameters : variational bayes ( vb )  @xcite , gibbs sampling ( gs )  @xcite , expectation propagation ( ep )  @xcite , belief propagation ( bp )  @xcite , and collapsed variational bayes ( cvb or cvb0 with zero - order approximation )  @xcite . except for gs",
    ", all other inference methods are based on the coordinate ascent algorithm  @xcite , which first calculates the topic posterior distribution over each word token , and then updates the parameters based on the inferred posterior distribution .",
    "although cvb0 and bp converge much faster and produce higher held - out log - likelihood  @xcite than gs , they require storing the posterior probability matrix of all words in memory .",
    "the size of this matrix increases linearly with the number of unique document / word pairs and the number of topics .",
    "it is difficult to distribute this big matrix to a common computer cluster when the number of words and the number of topics are very large .",
    "in addition , cvb0 and bp store the parameters of document - topic and topic - word distributions in double precision , consuming more memory to handle sparse data sets .",
    "most parallel inference solutions of lda choose batch gs algorithms because they are more memory - efficient than other algorithms  @xcite . for example , gs does not need to maintain the large posterior matrix in memory .",
    "in addition , gs stores lda parameters using the integer type by sparse matrices , and often obtains higher topic modeling accuracy ( e.g. , higher held - out log - likelihood ) than vb  @xcite .",
    "so , gs is generally agreed to be a more scalable choice in many parallel lda solutions .",
    "we will discuss how to distribute an accelerated gs algorithm with low time and space complexities called sparselda in subsection  [ sec : sparselda ] .",
    "previous distributed lda systems have explored two main architectures : 1 ) parallel computing using processors tightly coupled with shared memory  @xcite , and 2 ) distributed computing using processors loosely connected via network  @xcite .",
    "yahoo!lda  @xcite can be viewed as a hybrid parallel architecture using the shared memory technique over multiple machines based on the _ memcached _ technique .",
    "lda  @xcite distributes the batch vb algorithm to the mapreduce framework , which requires frequent i / o operations causing the slow speed .",
    "however , all reported distributed lda systems learn up to @xmath0 topics , while @xmath0 might be far less than the real number of semantics or word senses in human language . on the other hand , lda is a non - negative matrix factorization method  @xcite , so that many parallel matrix factorization architectures can be used .",
    "however , recent parallel matrix factorization architectures  @xcite have difficulty in learning @xmath1 topics because they focus on only low - rank approximation to the big sparse matrix .",
    "the rank is often very low such as @xmath11 , where the rank in matrix factorization has the same meaning with the number of topics in lda .",
    "unlike previous distributed lda solutions , peacock introduces both data and model parallelism for big data ( @xmath9 documents ) and big lda models ( @xmath9 parameters including topic assignment vector and topic - word matrix ) within a hierarchical distributed architecture , which uses pipeline and lock - free techniques to reduce both communication and synchronization costs .",
    "in addition , peacock has two new components , real - time prediction and topic de - duplication , which play important roles in real - world applications .",
    "recently , online lda algorithms  @xcite have attracted intensive research interests with two reasons .",
    "first , except for online bayesian updating  @xcite , most online lda algorithms combine the stochastic optimization framework  @xcite with the corresponding batch lda algorithms , which theoretically can converge to the local optimal point of the lda objective function .",
    "second , online algorithms partition the entire data set into several mini - batches .",
    "they load each mini - batch in memory for online processing , and discard it after one look .",
    "this streaming method significantly reduces the memory consumption for big data and big models .",
    "however , given the same amount of training samples , batch algorithms converge significantly faster and yield higher held - out log - likelihood than online counterparts  @xcite .",
    "the main reason is that the convergence rate of stochastic algorithms is slower than that of batch algorithms which has been discussed in  @xcite .",
    "if we have enough computing resources , it is better to distribute batch algorithms than online ones .",
    "if the memory is limited and the data come in the streaming manner , we prefer distributing online algorithms . for example , d - sgld  @xcite is based on a stochastic gradient mcmc ( monte carlo markov chain ) and is distributed with adaptive load balance by making the faster workers work longer until the slower workers finish their tasks .",
    "pobp  @xcite parallelizes the online belief propagation algorithm , and has a dynamic communication scheduling scheme to reduce the overall cost in processing big data streams .",
    "as discussed above , both batch and online algorithms have their advantages and disadvantages .",
    "although peacock is designed for distributed batch algorithms , its architecture can be readily extended to distributed online algorithms for streaming data . in this case ,",
    "peacock just replaces batch algorithms with online counterparts without changing the hierarchical architecture , which will be studied in our future work .    other topic models such as hierarchical dirichlet processes ( hdp )  @xcite and author - topic models ( atm )  @xcite have similar batch inference algorithms with lda such as gs and vb  @xcite .",
    "these inference algorithms estimate similar parameters as those in lda , for example , the multinomial parameters for topic - word distributions .",
    "therefore , the parallel implementation of these inference algorithms can be also deployed in peacock .",
    "for instance , atm can be implemented in peacock by replacing the multinomial parameters over documents with those over authors  @xcite .",
    "[ notations ]    in this section , we first introduce an accelerated gs algorithm called sparselda  @xcite for learning lda . then , we show how to distribute the gs algorithm in the hierarchical architecture with model and data parallelism to handle the large number of lda parameters and training samples . in this new architecture , we focus on implementing the following techniques : 1 ) the distributed gs algorithm , 2 ) pipeline efficient communication , 3 ) lock - free synchronization , and 4 ) fault tolerance . finally , we introduce how to solve two new problems of big lda models in real - world industrial applications : 1 ) real - time topic prediction of new unseen queries / documents , and 2 ) topic de - duplication for better quality and performance . table  [ notations ] summarizes some important notations used in this paper .",
    "lda allocates a set of thematic topic labels , @xmath12 , to explain the word tokens , @xmath13 , in the document - word co - occurrence matrix @xmath14 , where @xmath15 is the word token index , @xmath16 denotes the word index in the vocabulary , @xmath17 denotes the document index in the corpus , and @xmath18 denotes the topic index .",
    "usually , the number of topics @xmath4 is provided by users .",
    "the objective of lda is to maximize the joint probability @xmath19 , where @xmath20 and @xmath21 are two non - negative matrices of multinomial parameters for document - topic and topic - word distributions , satisfying @xmath22 and @xmath23 .",
    "both multinomial matrices are generated by two dirichlet distributions with hyperparameters @xmath24 and @xmath25 .",
    "since we aim to learn @xmath26 topic from @xmath27 search queries , we choose to distribute an accelerated sparse gibbs sampling ( gs ) inference algorithm called sparselda  @xcite , whose time and space complexities are insensitive to the number of topics @xmath4 . in gs , the memory is used to maintain three lda parameter count matrices : a matrix @xmath28 in which each element is the total number of the vocabulary word @xmath29 assigned to the topic @xmath30 , a matrix @xmath31 in which each element is the total number of topic @xmath30 assignments in each document @xmath32 , and a count vector @xmath33 , in which each element is the number of topic @xmath34 assignments in the training corpus .",
    "the relation between lda multinomial parameters and count matrices are as follows : @xmath35 in each iteration , the gs algorithm updates the topic assignment @xmath36 of every observed word token @xmath37 in the training corpus by randomly drawing a topic @xmath36 from the collapsed posterior distribution , @xmath38 where @xmath39 means that the corresponding word token and topic @xmath36 is excluded from the count matrices . after the new topic assignment @xmath40",
    "is sampled , the corresponding elements in the count matrices @xmath41 , @xmath42 and @xmath43 are updated immediately .",
    "sparselda divides equation   into three parts : @xmath44 due to sparsity of the topic posterior probability @xmath45 , randomly sampling these three parts does not need to calculate @xmath4 times . as a result , sparselda has a time complexity insensitive to the number of topics @xmath4 .",
    "moreover , it has a low space complexity because it stores only the topic assignment vector @xmath46 rather than the matrix @xmath31 in memory , where the size of @xmath46 is equal to the total number of word tokens in corpus , which is irrelevant with the number of topics @xmath4 .",
    "sparselda re - organizes the corresponding @xmath47 into the document - specific vector @xmath43 on the fly .",
    "if a vocabulary word @xmath29 has a total of @xmath48 occurrences in training corpus , the @xmath29th row of the parameter matrix @xmath42 only needs to store up to @xmath49 rather than @xmath4 values .          .",
    "the second layer contains @xmath50 aggregation servers and one coordinator server for global parameter synchronization and asymmetric prior estimation .",
    "this architecture can readily scale up to hundreds of machines having thousands of cores to learn at least @xmath1 topics from @xmath2 search queries . ]",
    "the key challenge is to store the word tokens @xmath51 , the topic assignment vector @xmath46 , and the large lda parameter matrix @xmath28 , when @xmath52 , @xmath26 and @xmath27 in industrial applications .",
    "for example , the count matrix @xmath28 alone takes at least tens of gigabytes when learning @xmath1 topics , while modern computer clusters are composed of commodity computers  @xcite with few gigabytes of memory ( e.g. , @xmath53 gb memory ) .",
    "therefore , we propose the hierarchical distributed architecture to solve this large - scale problem , which contains a configuration of servers to handle both big data and big lda model in figure  [ fig : config ] .",
    "more specifically , the distributed gs algorithm can be executed by a configuration of the following servers :    1 .",
    "_ model parallelism _ : we partition the parameter @xmath28 matrix by rows @xmath16 into @xmath54 model shards , @xmath55 .",
    "we use @xmath50 _ sampling servers _ , where the @xmath56th sampling server maintains the @xmath57 shard and local copies of @xmath58 and @xmath59 in memory .",
    "the value of @xmath50 should make each model shard @xmath57 small enough to fit in the memory of a sampling server .",
    "each sampling server runs the sparselda algorithm to update the topic assignments @xmath60 in training blocks sent from the data servers .",
    "the sampling algorithm will update the @xmath61 shard at @xmath56th sampling server . at the end of the training process",
    ", all sampling servers output their @xmath61 model shards",
    "data parallelism _ : we partition the word tokens @xmath14 and their topic assignments @xmath46 by rows into @xmath54 shards , @xmath62 .",
    "we use @xmath50 _ data servers _",
    ", where each loads a data shard @xmath63 and the corresponding @xmath64 shard in memory .",
    "the value of @xmath50 should make each data shard and its corresponding @xmath46 shard small enough to fit in the memory of a data server .",
    "the data servers send word tokens and their topic assignments shards to the corresponding sampling servers , which update the topic assignments @xmath64 and the model parameters @xmath57 . after processing each segment of data shards ,",
    "the sampling servers send back the changed topic assignments @xmath65 to data servers , which write @xmath65 back to disks for fault recovery .    as a result",
    ", the entire data has been partitioned into @xmath66 blocks and each data server stores @xmath50 blocks .",
    "if the sampling server simultaneously obtains the same column of data segment sent by all data servers , it will read and write the topic assignments of the same vocabulary word under race conditions .",
    "for example in figure  [ fig : config ] , if the sampling server @xmath67 receives simultaneously three data blocks @xmath68 sent by three data servers , it has a higher likelihood to change the topic assignments of the same vocabulary words , which causes serious read / write locks and i / o delays . to solve this problem",
    ", we design a lock - free parallel strategy similar to  @xcite .",
    "the sampling servers process blocks on the main diagonal sent by the data servers . since only @xmath69 and",
    "@xmath70 are required for processing the first data block , @xmath71 and @xmath72 for the second , and @xmath73 and @xmath74 for the third , these three blocks can be processed simultaneously without conflicts in accessing @xmath42 and @xmath75 .",
    "it is analogous to processing the data blocks on the second and the third diagonals . because the global parameter @xmath41 is needed for all block computation , we store a local copy of @xmath76 in each sampling server and synchronize @xmath76 by a coordinator server after processing each diagonal of blocks .",
    "we refer to each of non - conflicting @xmath50 shards as a _",
    "segment_. figure  [ fig : config ] shows three segments , @xmath77 , @xmath78 and @xmath79 , when @xmath80 .",
    "the scalability of one configuration in figure  [ fig : config ] is limited .",
    "increasing the number of sampling servers @xmath50 indicates the increasing of vertical partitions of the training corpus @xmath14 as well as the model parameters @xmath28 in rows .",
    "so , the number of data servers @xmath50 would be less than the size of vocabulary @xmath81 . however , the size of vocabulary words in one sampling server should be larger than a value ( e.g. , @xmath82 ) for a better efficiency . in this case , @xmath50 can not be very large in practice . when @xmath83 , it is difficult to use the limited @xmath50 data servers to store big data in one configuration . as a result , we need to build multiple configurations and use a set of @xmath50 _ aggregation servers _ to synchronize the global model parameter @xmath42 from different configurations .",
    "figure  [ fig : layer2 ] shows the hierarchical distributed architecture containing two layers . in the layer @xmath67",
    ", there are @xmath84 configurations as shown in figure  [ fig : config ] . for simplicity",
    ", we do not illustrate the data servers in each configuration . in the layer @xmath53",
    ", there are @xmath50 aggregation servers to connect corresponding sampling servers . at the end of each gs iteration ,",
    "all sampling servers in all configurations report their model parameter change @xmath85 to aggregation servers  @xcite .",
    "notice that the @xmath56th sampling server in the layer @xmath67 configuration reports only to the @xmath56th aggregation server in the layer @xmath53 .",
    "after the aggregation from all configurations , the aggregation servers distribute the updated global model @xmath28 to all configurations in the layer @xmath67 . in practice , the aggregation does not have to be done in every iteration .",
    "each configuration can run independently for several iterations before the model aggregation in the layer @xmath53 .",
    "this strategy is analogous to the robbins - monro stochastic optimization  @xcite , where a minibatch estimate of model parameters can be viewed as a stochastic approximation to them .",
    "therefore , this asynchronous parameter update method in peacock is likely to work because robbins - monro stochastic optimization works .",
    "previous results  @xcite also confirm that the asynchronous and delayed synchronization of the global parameters will not degrade lda accuracy very much .    in the layer @xmath53",
    ", we also use a _ coordinator _ to control the cooperative work of sampling , data and aggregation servers .",
    "the coordinator is also responsible for updating and re - distributing the global copy of @xmath41 to sampling servers , and optimizing the asymmetric dirichlet prior @xmath86 . to optimize @xmath86",
    ", the coordinator has to maintain a histogram of document lengths , @xmath87 , and collects a matrix , @xmath88 , storing the number of documents in which topic @xmath34 appears @xmath89 times  @xcite from the data servers . likewise to the synchronization of @xmath90 in different configurations , we estimate these global parameters in the coordinator and distribute them to sampling servers in the asynchronous manner . for example , after all configurations run a few iterations , the coordinator aggregates @xmath91 ( the aggregation cost is small due to a simple sum operation ) and broadcasts the updated global parameter to all sampling servers .",
    "all these servers are developed using google s go programming language , a compiled concurrent system programming language .",
    "the parallel machine learning systems could benefit a lot from the native support of concurrent programming and convenient implementation of remote procedure calls ( rpc ) provided by go .",
    "@xmath92 sampling - server[@xmath56 ] .",
    "data - server[@xmath56 ] . + data - server[(m+dig)%m ] . as follows : @xmath94 sampling - server[@xmath56 ] .",
    "sampling - server[@xmath56 ] .",
    "data - server[@xmath56 ] .",
    "@xmath95 data - server[@xmath56 ] .",
    "figure  [ fig : algo ] shows the distributed gs algorithm executed by the coordinator , where the dot symbol denotes an rpc call .",
    "for example , in line  @xmath96 , the coordinator calls procedure setalpha , which is exposed and executed by a sampling server , where the sampling follows equation  .",
    "the * par - for * in figure  [ fig : algo ] denotes the concurrent version of the commonly - used control structure * for*. the * par - for * flattens the loop body and executes it in parallel .",
    "the * par - for * can be implemented using go language elements of _ channel _ and _ goroutine _ , as shown by the open source project https://github.com/wangkuiyi/parallel , which is used in peacock .",
    "c / c++ programmers can use the * par - for * provided by openmp .    in each iteration",
    ", the procedure rungibbsiteration invokes samplesegment to update the topic assignments segment by segment , where samplesegment coordinates the @xmath50 data servers and the @xmath50 sampling servers to run the parallel sparselda algorithm .",
    "each sampling server keeps a local @xmath97 and a global @xmath98 , and updates the topic assignment @xmath99 sent by the data server in line @xmath100 .",
    "samplesegment also collects the matrix , @xmath88 , which records the number of documents in which the topic assignment @xmath34 occurs for @xmath89 times .",
    "this matrix is used by the procedure optimizehyperparams , which is described in  @xcite , to optimize the asymmetric prior , @xmath86 , at the end of each gs iteration .",
    "in addition to @xmath88 , optimizehyperparameters also requires @xmath87 , the vector recording the document lengths , which is counted in the first iteration and saved in coordinator for later use .      [",
    "tab : pipeline ]    figure  [ fig : algo ] shows that all network communications in peacock happen in the form of rpcs .",
    "the largest fraction of _ communication cost _ lies in workwithsampler , where the data servers send data blocks to the sampling servers , and wait for responses containing the updated topic assignment @xmath60 .",
    "we reduce the communication cost using the _ pipeline _ technique . to avoid the overflow of network communication buffer",
    ", the data server sends just a few document fragments known as a _ package _ rather than sending a block in an rpc to the sampling server . instead of waiting for the response from the sampling server before sending the next package , the data server sends @xmath101 packages concurrently . on the sampling server",
    ", there are multiple _ goroutines _ , a kind of light - weighted thread scheduled by the go runtime system , to process these packages and respond to the data server .",
    "the data server maintains a data structure with @xmath101 slots , and each keeps track of an out - going package .",
    "the data server clears a slot after receiving a response of the corresponding package , or getting a timeout .",
    "once there are empty slots and packages to be processed , the data server would continue sending packages .",
    "in general , this pipeline optimizes the throughput by overlaps the sending , processing and responding of packages .    maximizing the throughput depends on finding the optimal configuration of two parameters : package size @xmath102 and pipeline capacity @xmath101 .",
    "the product , @xmath103 , is proportional to the size of memory used as communication buffer . in practice",
    ", there would be an upper limit of buffer size , @xmath104 , and we would make full use of it to get the maximum throughput .",
    "this can be written as the constraint function , @xmath105 , which is a curve on the two dimensional space @xmath102 and @xmath101 .",
    "the best configuration would be a point on this curve . in our computing environment ,",
    "a practical @xmath104 value is @xmath106 mb .",
    "the measure of time consumption with respect to the curve @xmath105 is shown in table  [ tab : pipeline ] , where the time consumption is larger at both ends of this curve , and the optimal configuration lies in the middle of the curve .",
    "the variance is too small to be shown .",
    "data - server[@xmath56 ] .",
    "data - server[(m+dig)%m ] .",
    "@xmath94 sampling - server[@xmath56 ] .",
    "sampling - server[@xmath56 ] .",
    "data - server[@xmath56 ] . @xmath95 data - server[@xmath56 ] .",
    "after issuing parallel executions of the loop body in figure  [ fig : algo ] , the * par - for * does a synchronization operation that waits for the completions of all executions of sampling servers called the _ synchronization lock problem_. we address this problem by three lock - free strategies . first , to avoid data skewness in each data block , we randomly shuffle data @xmath14 by rows and columns so that each block contains almost equal number of word tokens in practice  @xcite .",
    "second , we can further reduce the synchronization cost of the * par - for * on line  @xmath107 of figure  [ fig : per - segment ] by balancing the workload of sampling servers . because each sampling server processes a row of corpus blocks , it is desirable that the block rows contain similar word frequencies .",
    "this can be achieved using a pre - training scheduler , which assigns vocabulary words to @xmath57 shards . as with plda+  @xcite",
    ", we use the weighted round - robin method for this word assignment .",
    "we first sort vocabulary words in descending order by their frequency , and pick the word with the largest frequency and assign it to the @xmath61 shard with the accumulative word frequency .",
    "then , we update the accumulated word frequency of @xmath61 .",
    "this placement process is repeated until all words have been assigned .",
    "weighted round - robin has been empirically shown to achieve a balanced load with a high probability  @xcite .",
    "finally , in figure  [ fig : algo ] , the nested loop starting from line  @xmath108 invokes the * par - for * many times and introduces many waits .",
    "the problem can be relieved by swapping the inner and outer loop as shown in figure  [ fig : per - segment ] . in this change , two * par - for * structures at line  @xmath109 and line  @xmath110",
    "are moved one upper level , which relaxes the aggregation and redistribution of vector @xmath41 from a per - diagonal granularity to a per - segment granularity .",
    "this relaxed aggregation does not affect the correctness of the distributed gs algorithm analogous to the stochastic optimization framework  @xcite .",
    "indeed , similar asynchronous optimization methods for updating model parameters have been confirmed to work well in lock - free parallel stochastic gradient descent algorithms  @xcite . after swapping the * par - for * at line  @xmath111 with its outer loop at line  @xmath108 , a data server might work with more than one sampling server simultaneously .",
    "however , this would introduce conflicts in accessing the @xmath112 shard maintained by the data server .",
    "we address this conflict problem by two methods .",
    "first , we use @xmath113 data servers as shown in figure  [ fig : config ] denoted by  free \" .",
    "these  free \" data servers provide additional conflict - free data blocks for computing without waiting for the completion of other sampling servers .",
    "the coordinator will schedule the finished sampling server to those conflict - free data blocks having the minimum number of visits . in this way",
    ", we assure that all data blocks will have almost equal number of visits .",
    "second , each data server maintains two copies of the @xmath47 shard : @xmath114 and @xmath115 without conflicts . after receiving the response of updated package from the sampling server , the data server applies the difference between the response and @xmath114 to @xmath115 .",
    "data parallelism also helps fault recovery , which is critical in large - scale machine learning .",
    "consider that a parallel learning job may take days or even weeks , it is very likely that some workers fail or be preempted during the period .",
    "if the system can not recover when it fails , we would have to restart the job from the beginning . since the restart might fail again",
    ", the learning job would never finish .",
    "as the configurations in the layer @xmath67 work independently within every few iterations , it is straightforward to restart any failed configuration based on the independent checkpoint on hard disks . the restart can be implemented simply using the ssh command , or sophisticated cluster management systems like apache yarn .",
    "this architecture is similar to google s architecture for parallel deep learning  @xcite , which refers to configurations as models .",
    "more than achieving fine - grained fault recovery , this design also improves the parallelism and makes peacock highly scalable .",
    "it is critical in online applications like search engine and online advertising systems to predict latent semantics of new user queries in real - time based on the large number of topics . in typical internet services ,",
    "the response time of backend servers is measured in milliseconds . given lda models with at least @xmath1 topics ,",
    "few inference algorithms are efficient enough to do real - time prediction .",
    "hence , we propose a real - time inference algorithm , rt - lda , especially for prediction of new queries .",
    "the basic idea of rt - lda is to replace the sampling operation in equation   by the @xmath116 operation .",
    "this makes rt - lda a hill climbing or coordinate ascent algorithm whose search path consists of line segments aligned with axes of the topic space , which is similar to the coordinate ascent using one - dimensional newton step  @xcite widely used in learning regression models .",
    "the @xmath116 operation in rt - lda can be optimized using a cache - based technique .",
    "according to equation  , the @xmath116 operator in rt - lda is @xmath117 } p(z_{ivd}=k , x_{ivd}=1 \\mid { \\boldsymbol{z}}_{\\neg ivd } , { \\boldsymbol{x}}_{\\neg ivd } , \\alpha , \\beta ) \\\\ = & \\max_{k\\in[1,k ] } \\phi_{vk } ( \\theta_{dk } + \\alpha_k )   \\\\ = & \\max_{k\\in[1,k ] } \\phi_{vk } \\theta_{dk } + \\phi_{vk } \\alpha_k , \\end{split}\\ ] ] where @xmath118 is the empirical probability matrix computed by equation  . in prediction ,",
    "@xmath118 and @xmath86 are constants , whereas @xmath43 changes with the updating of topic assignments .",
    "this makes it viable to precompute @xmath119 , whose result is a sparse matrix @xmath120 , @xmath121 we save @xmath120 in a compact data structure which contains only @xmath81 non - zero elements .",
    "the compact @xmath120 is an approximation to equation  , where the error of the approximation is caused by non - zero elements in @xmath43 .",
    "we rewrite equation   as @xmath122 } p(z_{ivd}=k , x_{ivd}=1 \\mid { \\boldsymbol{z}}_{\\neg ivd } , { \\boldsymbol{x}}_{\\neg ivd } , \\alpha , \\beta ) \\\\",
    "= & \\max_k \\left [ r_{vk}^ * , \\max_{\\substack{k\\in[1,k ] \\\\ s.t . \\theta_{dk}>0 } } \\phi_{vk } ( \\theta_{dk}+ \\alpha_k ) \\right ] , \\end{split}\\ ] ] where @xmath123 denotes the non - zero element in the column of @xmath120 corresponding to the vocabulary word @xmath29 .",
    "different from the @xmath116 operation in equation  , which iterates over @xmath124 $ ] , the first @xmath116 operation in equation   compares two values , and the second @xmath116 operation visits only non - zero elements in @xmath43 .",
    "suppose that the maximum number of non - zero elements in a query @xmath32 is the length of the query , equation   makes rt - lda significantly faster than sparselda when the number of topics @xmath26",
    ".    figure  [ fig : rt - lda]a compares the prediction speed between rt - lda and sparselda .",
    "we use the the number of query - per - second ( qps ) of rt - lda and sparselda on a real backend inference server with @xmath125 cpu load .",
    "the @xmath126-axis is the cache size .",
    "generally , the larger cache leads to faster prediction , but the performance reaches the upper bound with the increase of cache size in gigabytes ( gb ) .",
    "this setting implies that the response time of prediction is the reciprocal of the qps .",
    "we see that rt - lda is about an order of magnitude faster than sparselda .",
    "figure  [ fig : rt - lda]b compares rt - lda and sparselda for their topic modeling accuracy measured in the predictive perplexity  @xcite , which is a standard performance measure for topic modeling accuracy .",
    "the lower perplexity means a higher topic modeling accuracy on new test data set .",
    "this experiment uses @xmath127 wikipedia titles by clustering them into @xmath128 groups with various lengths . for all these groups ,",
    "we randomly select @xmath129 as test data set and retain the remaining @xmath130 as training set .",
    "we see that the accuracy of the two algorithms , measured in perplexity , are very close .",
    "rt - lda loses some tolerable topic modeling accuracy to get a faster speed compared with sparselda .",
    "we can further improve the effectiveness of rt - lda by running multiple line searches in parallel and then averaging results of all these parallel trails . because rt - lda is much more efficient than sparselda",
    ", the peacock system can afford many parallel trails to extract topic features from new queries .",
    "although topic duplication has been rarely discussed in previous literature , it becomes a main challenge in topic feature engineering .",
    "as noted in  @xcite , when learning lda , frequent words often dominate more than one topics , and the learned topics are similar to each other .",
    "we refer to these similar topics as _",
    "duplicates_. generally , when learning @xmath8 topics , around @xmath131 topics have duplicates in practice .",
    "if a query @xmath32 is @xmath132 about the topic @xmath133 and @xmath134 about the topic @xmath135 , the query @xmath32 is mainly about the topic @xmath133 .",
    "however , if a topic @xmath133 has three duplicates , @xmath136 , @xmath137 and @xmath138 , the gs algorithm would follow equation   to scatter the @xmath132 weight of @xmath133 in @xmath32 to @xmath136 , @xmath137 and @xmath138 .",
    "if each duplicate gets @xmath139 of the original @xmath132 topic @xmath133 , the interpretation of the query would become mainly about topic @xmath135 , though the truth is that the query is mainly about topic @xmath133 .",
    "we remove topic duplicates by two methods .",
    "first , we learn the asymmetric dirichlet prior @xmath86 over the document - topic distributions  @xcite , which substantially increases the robustness of lda to variations in the number of topics and to the highly skewed word frequency distributions common in natural language .",
    "asymmetric priors over document - topic distributions automatically combine similar topics into one large topic , rather than splitting topics more uniformly by symmetric priors . in practice , we can set a very large @xmath140 value at initial learning time , and prune duplicates by the asymmetric dirichlet prior .",
    "those topics with very small dirichlet priors would be automatically weighted trivial by rt - lda ( subsection  [ sec : real - time - inference ] ) at serving time .",
    "we find this approach prevents common words from dominating many topics , thus leaves the room for long - tail topics .",
    "second , we cluster topic duplicates if their @xmath10-distance is below a threshold .",
    "the lower @xmath10-distance threshold means that we would remove more duplicates from large number of topics .",
    "we evaluate peacock s topic modeling performance for big data by three performance measures : 1 ) speedup : the runtime ratio over a sequential gs algorithm as we increase the number of computing cores available ; 2 ) scalability : the ability to handle a growing number of topics ; 3 ) accuracy : the held - out log - likelihood of lda achieved by increasing number of iterations  @xcite .",
    "the baseline is the state - of - the - art industrial solution yahoo!lda  @xcite with open source codes .",
    "likewise , yahoo!lda also distributes sparselda  @xcite over multiple machines but using a shared memory environment based on the _ memcached _ technique . in the layer @xmath53 of peacock ,",
    "we use @xmath67 coordinator server and @xmath141 aggregation servers ( as shown in figure  [ fig : layer2 ] ) . in the layer @xmath67 , we set @xmath142 configurations , each of which contains @xmath141 sampling servers and @xmath143 data servers .      for a fair comparison , we use the same publicly available data set pubmed , which contains @xmath144 million documents with an average length of around @xmath145 word tokens each .",
    "the vocabulary size of pubmed is @xmath146 .",
    "we also compose the training corpus of search queries received in recent months called soso .",
    "the pre - processing of the corpus contains five steps : 1 ) transform each query into word tokens , and count word frequencies .",
    "2 ) remove those words with low frequency , which are likely typos .",
    "3 ) remove those words with very high frequency , because common words tend to dominate all topics  @xcite .",
    "4 ) de - duplicate queries : if a query appears multiple times , we keep only one appearance in corpus .",
    "this allows us to include a large variety of user intentions within a certain amount of training corpus .",
    "this also lowers the weight of frequent queries in the corpus .",
    "5 ) remove those queries containing only one word , because single - word queries do not provide word co - occurrence counts , which is a clue used by lda to infer topics .",
    "the processed corpus contains one billion search queries with @xmath147 word tokens per query on average and takes @xmath148 gb storage space .",
    "the vocabulary size of soso is around @xmath149 . obviously , soso is around @xmath96 times larger than pubmed .",
    "the speedup and scalability performance . ]",
    "the topic modeling accuracy and convergence speed . ]",
    "comparisons between peacock and the single machine . ]",
    "figure  [ fig : speed ] shows the speedup performance ( fixing @xmath150 ) , where the @xmath126-axis is the number of cores and @xmath104-axis is the runtime ratio of @xmath151 cores over that of other number of cores in @xmath126-axis .",
    "the variance is too small to be shown .",
    "we see that peacock on average achieves around @xmath152 speedup when the number of cores is @xmath153 , which implies that the communication and synchronization in peacock take about half of the training time .",
    "yahoo!lda has a much better speedup than peacock when the number of cores is small ( @xmath154 ) .",
    "however , its speedup performance drops significantly when the number of cores increases from @xmath153 to @xmath155 .",
    "the possible reason is that yahoo!lda does not consider the lock - free synchronization problem . in practice ,",
    "the very large number of cores will cause longer waiting time when accessing the shared memory based on the _ memcached _ technique .",
    "peacock scales much better to the large number of cores by pipeline communication and lock - free synchronization ( subsections  [ communication ] and  [ synchronization ] ) .",
    "we see that peacock is slower than yahoo!lda when the number of cores is small .",
    "the reason is that we use more separate sampling servers for model parallelism leading to the additional communication and synchronization costs , which remains almost a constant ratio in training time by pipeline techniques .",
    "figure  [ fig : speed ] also shows the training time per iteration with the increasing number of topics ( fixing the number of cores to @xmath156 ) , @xmath157 . with @xmath4 increasing from @xmath6 to @xmath7 ,",
    "the corresponding training time increase of peacock is small .",
    "when @xmath4 increases by @xmath108 times from @xmath7 to @xmath1 , the training time is close to the linear growth .",
    "the scalability of peacock with respect to @xmath4 comes mainly from the model and data parallelism ( subsection  [ sec : hierarchical ] ) resulting in very fast sampling performance with a small memory footprint . from @xmath158 to @xmath159",
    ", peacock consumes significantly more training time because the topic sparseness of each word token becomes lower in sparselda . as a comparison",
    ", yahoo!lda has out of memory problem when the the number of topics @xmath160 because it does not consider storing a big topic - word count matrix @xmath42 .",
    "peacock divides this big parameter matrix into small model shards to handle increasing number of topics .",
    "since peacock uses additional communication to synchronize more sampling servers , its per iteration training time is slightly longer than that of yahoo!lda .",
    "we use the iteration annealed importance sampling ( iteration - ais )  @xcite method to evaluate the predictive performance ( measured by the held - out log - likelihood ) of peacock .",
    "we randomly select @xmath7 documents from pubmed and @xmath1 queries from soso as the held - out data sets .",
    "figure  [ fig : logllhood ] shows the held - out log - likelihood as a function of training iterations and time ( fixing @xmath150 and the number of cores @xmath156 ) while variance is too small to be shown .",
    "the higher held - out log - likelihood corresponds to the better model quality .",
    "we observe that peacock has a rise after @xmath161 iterations because we start asymmetric prior optimization and topic de - duplication ( subsection  [ sec : topic - dups ] ) , which can improve the topic model quality .",
    "although both peacock and yahoo!lda use sparselda , peacock converges to a higher log - likelihood level .",
    "the reason is partly because yahoo!lda uses the approximate synchronization to speedup its performance leading to a slightly worse model quality , which has been also observed in their own work  @xcite .",
    "figure  [ fig : logllhood ] also shows that peacock uses less training time to achieve a higher held - out log - likelihood than yahoo!lda . to see",
    "if peacock can produce the same model quality as that generated by a sequential gs inference on a single machine , we show their held - out log - likelihoods as a function of training iterations in figure  [ fig : single ] while variance is too small to be shown . since the single machine can not train the large number of samples due to the memory constraint , we randomly select a subset of the training set @xmath3 queries , and select @xmath7 queries as the held - out set .",
    "we see that the held - out log - likelihood curve generated by peacock locates closely to that produced by the sequential gs on the single machine .",
    "this result confirms that the approximate synchronization techniques used in peacock do not affect the model quality very much .    to summarize",
    ", we see that yahoo!lda is more efficient for small - scale ( @xmath162 ) topic modeling tasks , while peacock is more suitable for solving large - scale ( @xmath26 ) topic modeling problems in industrial applications .",
    "after peacock learns @xmath26 topics from @xmath27 queries , we need to integrate the topic features into existing search engine and online advertising systems .",
    "we extract topic features from new queries based on the topic distribution over words @xmath118 in equation   learned by peacock .",
    "given the word tokens of a new query @xmath32 , we use rt - lda to predict its topic distribution @xmath163 by fixing @xmath118 . employing the bayes rule",
    ", we calculate the likelihood @xmath164 of a vocabulary word @xmath29 given a query @xmath32 : @xmath165 the @xmath81-length vector @xmath164 is compatible with the standard word vector space model .",
    "if we rank @xmath164 in descending order , we obtain top likely topic features in the input query .",
    "search engine uses the well - known vector space model in information retrieval and compute cosine similarity between queries and documents in their vector representations .",
    "we accelerate this process by using the weak - and algorithm  @xcite .",
    "peacock replaces the word vector features of each query by top @xmath166 likely topic features in equation   ( top @xmath166 largest values from @xmath81-length vector @xmath164 ) inferred by rt - lda in the head of each posting list used by the weak - and algorithm , which makes the query - document similarity computing efficient enough to be deployed in a real search engine .",
    "online advertising has been a fundamental financial support of the many free internet services  @xcite .",
    "most contemporary online advertising systems follow the generalized second price ( gsp ) auction model  @xcite , which requires that the system is able to predict the click - through rate ( pctr ) of an ad , where pctr is an important clue in gsp to ranking ads and pricing clicks .",
    "one of the key questions with the pctr is the availability of suitable input features or predictor variables that allow accurate ctr prediction for a given ad impression  @xcite .",
    "these features can be grouped into three categories : ad features including bid phrases , ad title , landing page , and a hierarchy of advertiser account , campaign , ad group and ad creative .",
    "user features include recent search queries , and user behavior data .",
    "context features include display location , geographic location , content of page under browsing , and time .",
    "most of these features are text data in word vector space  @xcite .",
    "we learn an @xmath10-regularized log - linear model  @xcite as the baseline for pctr , which uses a set of text and other features such as ad title , content of page under browsing , content of landing page , ad group i d , demographic information of users , categories of ad group and categories of the page under browsing . as a comparison",
    ", peacock adds all topic features  , i.e. , the @xmath81-length topic feature vector @xmath164 , in baseline text and other features as input to @xmath10-regularized log - linear model  @xcite .    in both applications , the training data set of peacock is soso described in subsection  [ sec : data ] . for peacock , we set @xmath142 configurations with @xmath67 coordinator server and @xmath141 aggregation servers .",
    "each configuration contains @xmath141 sampling servers and @xmath143 data servers .       distance . ]    for information retrieval , our test - bed is a real search engine , www.soso.com , which ranks the fourth largest in china market .",
    "the test data is used for routinely relevance evaluation , containing @xmath167 randomly selected queries and @xmath168 query - url pairs with human labeled relevance rate .",
    "every query - url pair was rated by three human editors and the average rate was taken .",
    "we compute mean average precision ( map ) using trec evaluation tool  @xcite .",
    "the higher map means the better retrieval performance .",
    "figure  [ fig : retrieval]a shows that the topic features improve the relevance measure by map with small variance .",
    "the relevance improvement grows steadily with the increasing number of topics from @xmath6 to @xmath1 .",
    "however , the growth of map becomes less salient when the number of topics changes from @xmath7 to @xmath1 .",
    "this is mainly attributed to the problem of _ topic duplication_. figure  [ fig : retrieval]b shows that topic de - duplication method can further improve the relevance of information retrieval .",
    "the map value of retrieval grows when we prune more duplicated topics ( lower @xmath10 distance can prune more similar topics in subsection  [ sec : topic - dups ] ) .",
    "usually , the map stops increasing when we prune duplicates from the initial @xmath3 to around @xmath1 topics .",
    "this result implies that @xmath1 is a critical number of topics to describe subtle word senses in big query data with @xmath149 vocabulary words .",
    "if the @xmath10 distance is too small such as @xmath169 , it will degrade the map performance by removing more non - duplicate topics .",
    "the online advertising experiment is conducted on a real contextual advertising system , https://tg.qq.com/. this system logs every ad shown to a particular user in a particular page view as an _",
    "ad impression_. it also logs every click of an ad . by taking each impression as a training instance , and labeling it by whether it was clicked ,",
    "we obtain @xmath170 billion training samples and @xmath171 billion test samples .",
    "we train @xmath172 _ hypothetical models _ ,",
    "whose topic features are extracted using @xmath172 different lda models with @xmath6 , @xmath173 , @xmath0 , @xmath7 and @xmath1 topics , respectively . following the judgment rule of task @xmath53 in kdd cup @xmath174 , a competition of ad pctr , we compare our hypothetical models with the baseline by their prediction performance measured in area under the curve ( auc ) . figure  [ fig : advertising ] shows that all hypothetical models gain relative auc improvement ( @xmath175 ) than the baseline ( auc @xmath176 ) .",
    "variance is too small to be shown .",
    "this verifies the value of big lda models .",
    "also , the auc improvement grows with the increase of the number of topics learned by peacock .",
    "the reason that the performance of @xmath7 is lower than that of @xmath0 is because of many topic duplicates in @xmath7 topics . after using automatic topic de - duplication by asymmetric dirchlet prior learning ( subsection  [ sec : topic - dups ] ) , the performance of @xmath1 becomes better than that of @xmath7 .",
    "this result is consistent with those in figure  [ fig : retrieval ] .",
    "topic modeling techniques for big data are needed in many real - world applications . in this paper",
    ", we confirm that a big lda model with at least @xmath1 topics inferred from @xmath2 search queries can achieve a significant improvement in industrial applications like search engine and online advertising systems . we propose a unified solution peacock to do topic modeling for big data .",
    "peacock uses a hierarchical distributed architecture to handle large - scale data as well as lda parameters . in addition",
    ", peacock addresses some novel problems in big topic modeling , including real - time prediction and topic de - duplication .",
    "we show that peacock is scalable to more topics than the current state - of - the - art industrial solution yahoo!lda . through two online applications",
    ", we also obtain the following experiences :    * the good performance is often achieved when the number of topics is approximately equal to or more than the number of vocabulary words . in our experiments ,",
    "the vocabulary size is @xmath149 so that the number of topics @xmath26 . in other industrial applications ,",
    "the vocabulary size may reach a few millions or even a billion .",
    "the peacock system can do topic feature learning when @xmath177 is needed . *",
    "the topic de - duplication method is a key technical component to ensure that @xmath26 topics can provide high - quality topic features .",
    "better topic de - duplication techniques remain to be an open research issue . *",
    "the real - time topic prediction method for a large number of topics is also important in industrial applications . if @xmath177 , faster prediction methods are needed and remain to be a future research issue .    in our future work",
    ", we will study how to deploy online lda algorithms in peacock , and how to implement inference algorithms to learn other topic models such as hdp  @xcite and author - topic models  @xcite .",
    "this work was supported by national grant fundamental research ( 973 program ) of china under grant 2014cb340304 , nsfc ( grant no . 61373092 and 61033013 ) , natural science foundation of the jiangsu higher education institutions of china ( grant no .",
    "12kja520004 ) , and innovative research team in soochow university ( grant no .",
    "sdt2012b02 ) .",
    "this work was partially supported by collaborative innovation center of novel software technology and industrialization .",
    "ddnn jeffrey dean , greg corrado , rajat monga , kai chen , matthieu devin , quoc  v. le , mark  z. mao , marcaurelio ranzato , andrew  w. senior , paul  a. tucker , ke yang , and andrew  y. ng . 2012 . .",
    "in _ nips_. 12321240 ."
  ],
  "abstract_text": [
    "<S> latent dirichlet allocation ( lda ) is a popular topic modeling technique in academia but less so in industry , especially in large - scale applications involving search engine and online advertising systems . </S>",
    "<S> a main underlying reason is that the topic models used have been too small in scale to be useful ; for example , some of the largest lda models reported in literature have up to @xmath0 topics , which cover difficultly the long - tail semantic word sets . in this paper </S>",
    "<S> , we show that the number of topics is a key factor that can significantly boost the utility of topic - modeling systems . </S>",
    "<S> in particular , we show that a  big \" lda model with at least @xmath1 topics inferred from @xmath2 search queries can achieve a significant improvement on industrial search engine and online advertising systems , both of which serving hundreds of millions of users . </S>",
    "<S> we develop a novel distributed system called peacock to learn big lda models from big data . </S>",
    "<S> the main features of peacock include hierarchical distributed architecture , real - time prediction and topic de - duplication . </S>",
    "<S> we empirically demonstrate that the peacock system is capable of providing significant benefits via highly scalable lda topic models for several industrial applications .    </S>",
    "<S> [ data mining ]    author s addresses : y. wang , x. zhao , z. sun , h. yan , z. jin , l. wang and c. law , tencent ; y. gao and j. zeng , school of computer science and technology , soochow university , shuzhou 215006 , china & collaborative innovation center of novel software technology and industrialization ; j. zeng , huawei noah s ark lab , hong kong . </S>",
    "<S> j. zeng is the corresponding author : zeng.jia@acm.org . </S>"
  ]
}