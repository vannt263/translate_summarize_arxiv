{
  "article_text": [
    "recent advances in dialogue modeling , speech recognition , and natural language processing have made it possible to build spoken dialogue agents for a wide variety of applications .",
    "potential benefits of such agents include remote or hands - free access , ease of use , naturalness , and greater efficiency of interaction . however , a critical obstacle to progress in this area is the lack of a general framework for evaluating and comparing the performance of different dialogue agents .",
    "one widely used approach to evaluation is based on the notion of a reference answer  @xcite .",
    "an agent s responses to a query are compared with a predefined key of minimum and maximum reference answers ; performance is the proportion of responses that match the key .",
    "this approach has many widely acknowledged limitations  @xcite , e.g. , although there may be many potential dialogue strategies for carrying out a task , the key is tied to one particular dialogue strategy .",
    "in contrast , agents using different dialogue strategies can be compared with measures such as inappropriate utterance ratio , turn correction ratio , concept accuracy , implicit recovery and transaction success  @xcite .",
    "consider a comparison of two train timetable information agents  @xcite , where agent a in dialogue 1 uses an explicit confirmation strategy , while agent b in dialogue 2 uses an implicit confirmation strategy :    danieli and gerbino found that agent a had a higher transaction success rate and produced less inappropriate and repair utterances than agent b , and thus concluded that agent a was more robust than agent b.    however , one limitation of both this approach and the reference answer approach is the inability to generalize results to other tasks and environments  @xcite . such generalization requires the identification of factors that affect performance  @xcite . for example , while danieli and gerbino found that agent a s dialogue strategy produced dialogues that were approximately twice as long as agent b s , they had no way of determining whether agent a s higher transaction success or agent b s efficiency was more critical to performance .",
    "in addition to agent factors such as dialogue strategy , task factors such as database size and environmental factors such as background noise may also be relevant predictors of performance .",
    "these approaches are also limited in that they currently do not calculate performance over subdialogues as well as whole dialogues , correlate performance with an external validation criterion , or normalize performance for task complexity .",
    "this paper describes paradise , a general framework for evaluating spoken dialogue agents that addresses these limitations .",
    "paradise supports comparisons among dialogue strategies by providing a task representation that decouples _ what _ an agent needs to achieve in terms of the task requirements from _ how _ the agent carries out the task via dialogue .",
    "paradise uses a decision - theoretic framework to specify the relative contribution of various factors to an agent s overall _ performance_. performance is modeled as a weighted function of a task - based success measure and dialogue - based cost measures , where weights are computed by correlating user satisfaction with performance .",
    "also , performance can be calculated for subdialogues as well as whole dialogues .",
    "since the goal of this paper is to explain and illustrate the application of the paradise framework , for expository purposes , the paper uses simplified domains with hypothetical data throughout .",
    "section 2 describes paradise s performance model , and section 3 discusses its generality , before concluding in section 4 .",
    "paradise uses methods from decision theory  @xcite to combine a disparate set of performance measures ( i.e. , user satisfaction , task success , and dialogue cost , all of which have been previously noted in the literature ) into a single performance evaluation function .",
    "the use of decision theory requires a specification of both the objectives of the decision problem and a set of measures ( known as attributes in decision theory ) for operationalizing the objectives .",
    "the paradise model is based on the structure of objectives ( rectangles ) shown in figure  [ objectives - fig ] .",
    "the paradise model posits that performance can be correlated with a meaningful external criterion such as usability , and thus that the overall goal of a spoken dialogue agent is to maximize an objective related to usability .",
    "user satisfaction ratings  @xcite have been frequently used in the literature as an external indicator of the usability of a dialogue agent .",
    "the model further posits that two types of factors are potential relevant contributors to user satisfaction ( namely task success and dialogue costs ) , and that two types of factors are potential relevant contributors to costs  @xcite .",
    "in addition to the use of decision theory to create this objective structure , other novel aspects of paradise include the use of the kappa coefficient  @xcite to operationalize task success , and the use of linear regression to quantify the relative contribution of the success and cost factors to user satisfaction .",
    "the remainder of this section explains the measures ( ovals in figure 1 ) used to operationalize the set of objectives , and the methodology for estimating a quantitative performance function that reflects the objective structure .",
    "section  [ avm - sec ] describes paradise s task representation , which is needed to calculate the task - based success measure described in section  [ succ - sec ] .",
    "section  [ cost - sec ] describes the cost measures considered in paradise , which reflect both the efficiency and the naturalness of an agent s dialogue behaviors .",
    "section  [ perf - func - sec ] describes the use of linear regression and user satisfaction to estimate the relative contribution of the success and cost measures in a single performance function . finally , section  [ subdialogues - sec ] explains how performance can be calculated for subdialogues as well as whole dialogues , while section  [ summary - sec ] summarizes the method .",
    "a general evaluation framework requires a task representation that decouples _ what _ an agent and user accomplish from _ how _ the task is accomplished using dialogue strategies .",
    "we propose that an _",
    "attribute value matrix ( avm ) _ can represent many dialogue tasks .",
    "this consists of the information that must be exchanged between the agent and the user during the dialogue , represented as a set of ordered pairs of attributes and their possible values .    as a first illustrative example , consider a simplification of the train timetable domain of dialogues 1 and 2 , where the timetable only contains information about rush - hour trains between four cities , as shown in table 1 .",
    "this avm consists of four attributes ( abbreviations for each attribute name are also shown ) . in table 1 , these attribute - value pairs are annotated with the direction of information flow to represent who acquires the information , although this information is not used for evaluation . during the dialogue",
    "the agent must acquire from the user the values of dc , ac , and dr , while the user must acquire dt .",
    ".attribute value matrix , simplified train timetable domain [ cols=\"<,<,^\",options=\"header \" , ]     [ ci - avm ]    figure 6 is tagged with the attributes from table 7 .",
    "smith and gordon s tagging of this dialogue according to their subtask representation was as follows : turns 1 - 4 were i , turns 5 - 14 were a , turns 15 - 16 were d , turns 17 - 18 were r , and turns 19 - 35 were t. note that there are only two differences between the dialogue structures yielded by the two tagging schemes .",
    "first , in our scheme ( figure 6 ) , the greetings ( turns 1 and 2 ) are tagged with all the attributes .",
    "second , smith and gordon s single tag a corresponds to two attribute tags in table 7 , which in our scheme defines an extra level of structure within assessment subdialogues .",
    "this paper presented the paradise framework for evaluating spoken dialogue agents .",
    "paradise is a general framework for evaluating spoken dialogue agents that integrates and enhances previous work .",
    "paradise supports comparisons among dialogue strategies with a task representation that decouples _ what _ an agent needs to achieve in terms of the task requirements from _ how _ the agent carries out the task via dialogue .",
    "furthermore , this task representation supports the calculation of performance over subdialogues as well as whole dialogues .",
    "in addition , because paradise s success measure normalizes for task complexity , it provides a basis for comparing agents performing _ different _ tasks .",
    "the paradise performance measure is a function of both task success ( @xmath0 ) and dialogue costs ( @xmath1 ) , and has a number of advantages .",
    "first , it allows us to evaluate performance at any level of a dialogue , since @xmath0 and @xmath1 can be calculated for any dialogue subtask . since performance can be measured over any subtask , and since dialogue strategies can range over subdialogues or the whole dialogue , we can associate performance with individual dialogue strategies .",
    "second , because our success measure @xmath0 takes into account the complexity of the task , comparisons can be made across dialogue tasks .",
    "third , @xmath0 allows us to measure partial success at achieving the task .",
    "fourth , performance can combine both objective and subjective cost measures , and specifies how to evaluate the relative contributions of those costs factors to overall performance . finally , to our knowledge",
    ", we are the first to propose using user satisfaction to determine weights on factors related to performance .    in addition",
    ", this approach is broadly integrative , incorporating aspects of transaction success , concept accuracy , multiple cost measures , and user satisfaction . in our framework",
    ", transaction success is reflected in @xmath0 , corresponding to dialogues with a p(a ) of 1 .",
    "our performance measure also captures information similar to concept accuracy , where low concept accuracy scores translate into either higher costs for acquiring information from the user , or lower @xmath0 scores .",
    "one limitation of the paradise approach is that the task - based success measure does not reflect that some solutions might be better than others .",
    "for example , in the train timetable domain , we might like our task - based success measure to give higher ratings to agents that suggest express over local trains , or that provide helpful information that was not explicitly requested , especially since the better solutions might occur in dialogues with higher costs .",
    "it might be possible to address this limitation by using the interval scaled data version of @xmath0  @xcite .",
    "another possibility is to simply substitute a domain - specific task - based success measure in the performance model for @xmath0 .",
    "the evaluation model presented here has many applications in apoken dialogue processing .",
    "we believe that the framework is also applicable to other dialogue modalities , and to human - human task - oriented dialogues .",
    "in addition , while there are many proposals in the literature for algorithms for dialogue strategies that are cooperative , collaborative or helpful to the user @xcite , very few of these strategies have been evaluated as to whether they improve any measurable aspect of a dialogue interaction . as we have demonstrated here ,",
    "any dialogue strategy can be evaluated , so it should be possible to show that a cooperative response , or other cooperative strategy , actually improves task performance by reducing costs or increasing task success .",
    "we hope that this framework will be broadly applied in future dialogue research .",
    "we would like to thank james allen , jennifer chu - carroll , morena danieli , wieland eckert , giuseppe di fabbrizio , don hindle , julia hirschberg , shri narayanan , jay wilpon , steve whittaker and three anonymous reviews for helpful discussion and comments on earlier versions of this paper .",
    "chu - carrol , jennifer and sandra carberry .",
    "response generation in collaborative negotiation . in _ proceedings of the conference of the 33rd annual meeting of the association for computational linguistics _ , pages 136143 .",
    "danieli , m. , w.  eckert , n.  fraser , n.  gilbert , m.  guyomard , p.  heisterkamp , m.  kharoune , j.  magadur , s.  mcglashan , d.  sadek , j.  siroux , and n.  you d .",
    "1992 . dialogue manager design evaluation .",
    "technical report project esprit 2218 sundial , wp6000-d3 .",
    "danieli , morena and elisabetta gerbino . 1995 . metrics for evaluating dialogue strategies in a spoken language system . in _ proceedings of the 1995 aaai spring symposium on empirical methods in discourse interpretation and generation _ ,",
    "pages 3439 .",
    "gale , william , ken  w. church , and david yarowsky .",
    "1992 . estimating upper and lower bounds on the performance of word - sense disambiguation programs . in _ proc .",
    "of 30th acl _ , pages 249256 , newark , delaware .",
    "hirschberg , julia and christine nakatani .",
    "1996 . a prosodic analysis of discourse segments in direction - giving monologues . in _ 34th annual meeting of the association for computational linguistics _ ,",
    "pages 286293 .",
    "hirschman , lynette , deborah  a. dahl , donald  p. mckay , lewis  m. norton , and marcia  c. linebarger . 1990 . beyond class",
    "a : a proposal for automatic evaluation of discourse . in _ proceedings of the speech and natural language workshop _ , pages 109113 .",
    "litman , diane and james allen .",
    "1990 . recognizing and relating discourse intentions and task - oriented plans . in philip cohen ,",
    "jerry morgan , and martha pollack , editors , _ intentions in communication_. mit press .",
    "polifroni , joseph , lynette hirschman , stephanie seneff , and victor zue .",
    "experiments in evaluating interactive spoken language systems . in _ proceedings of the darpa speech and nl workshop _ , pages 2833 .",
    "pollack , martha , julia hirschberg , and bonnie webber .",
    "user participation in the reasoning process of expert systems . in _",
    "proceedings first national conference on artificial intelligence _ , pages pp . 358361 .",
    "shriberg , elizabeth , elizabeth wade , and patti price .",
    "1992 . human - machine problem solving using spoken language systems ( sls ) : factors affecting performance and user satisfaction . in",
    "_ proceedings of the darpa speech and nl workshop _ , pages 4954 ."
  ],
  "abstract_text": [
    "<S> this paper presents paradise ( paradigm for dialogue system evaluation ) , a general framework for evaluating spoken dialogue agents . </S>",
    "<S> the framework decouples task requirements from an agent s dialogue behaviors , supports comparisons among dialogue strategies , enables the calculation of performance over subdialogues and whole dialogues , specifies the relative contribution of various factors to performance , and makes it possible to compare agents performing different tasks by normalizing for task complexity . </S>"
  ]
}