{
  "article_text": [
    "structured data , such as sequences , trees and graphs , are prevalent in a number of interdisciplinary areas such as protein design , genomic sequence analysis , and drug design  @xcite .",
    "to learn from such complex data , we have to first transform such data explicitly or implicitly into some vectorial representations , and then apply machine learning algorithms in the resulting vector space .",
    "so far kernel methods have emerged as one of the most effective tools for dealing with structured data , and have achieved the state - of - the - art classification and regression results in many sequence  @xcite and graph datasets  @xcite .",
    "the success of kernel methods on structured data relies crucially on the design of kernel functions  positive semidefinite similarity measures between pairs of data points  @xcite . by designing a kernel function ,",
    "we have implicitly chosen a corresponding feature representation for each data point which can potentially has infinite dimensions . later learning algorithms for various tasks and with potentially very different nature",
    "can then work exclusively on these pairwise kernel values without the need to access the original data points .",
    "such modular structure of kernel methods has been very powerful , making them the most elegant and convenient methods to deal with structured data . thus designing kernel for different structured objects , such as strings , trees and graphs",
    ", has always been an important subject in the kernel community .",
    "however , in the big data era , this modular framework has also limited kernel methods in terms of their ability to scale up to millions of data points , and exploit discriminative information to learn feature representations .",
    "for instance , a class of kernels are designed based on the idea of `` bag of structures '' ( bos ) , where each structured data point is represented as a vector of counts for elementary structures .",
    "the spectrum kernel and variants for strings  @xcite , subtree kernel  @xcite , graphlet kernel  @xcite and weisfeiler - lehman graph kernel  @xcite all follow this design principle . in other words ,",
    "the feature representations of these kernels are fixed before learning , with each dimension corresponding to a substructure , independent of the supervised learning tasks at hand .",
    "since there are many unique substructures which may or may not be useful for the learning tasks , the explicit feature space of such kernels typically has very high dimensions .",
    "subsequently algorithms dealing with the pairwise kernel values have to work with a big kernel matrix squared in the number of data points .",
    "the square dependency on the number of data points largely limits these bos kernels to datasets of size just thousands .",
    "a second class of kernels are based on the ingenious idea of exploiting the ability of probabilistic graphical models ( gm ) in describing noisy and structured data to design kernels .",
    "for instance , one can use hidden markov models for sequence data , and use pairwise markov random fields for graph data .",
    "the fisher kernel  @xcite and probability product kernel  @xcite are two representative instances within the family .",
    "the former method first fits a common generative model to the entire dataset , and then uses the empirical fisher information matrix and the fisher score of each data point to define the kernel ; the latter method instead fits a different generative model for each data point , and then uses inner products between distributions to define the kernel .",
    "typically the parameterization of these gm kernels are chosen before hand .",
    "although the process of fitting generative models allow the kernels to adapt to the geometry of the input data , the resulting feature representations are still independent of the discriminative task at hand . furthermore , the extra step of fitting generative models to data can be a challenging computation and estimation task by itself , especially in the presence of latent variables .",
    "very often in practice , one finds that bos kernels are easier to deploy than gm kernels , although the latter is supposed to capture the additional geometry and uncertainty information of data .    in this paper",
    ", we wish to revisit the idea of using graphical models for kernel or feature space design , with the goal of scaling up kernel methods for structured data to millions of data points , and allowing the kernel to learn the feature representation from label information .",
    "our idea is to model each structured data point as a latent variable model , then embed the graphical model into feature spaces  @xcite , and use inner product in the embedding space to define kernels . instead of fixing a feature or embedding space beforehand , we will also learn the feature space by directly minimizing the empirical loss defined by the label information .",
    "the resulting embedding algorithm , structure2vec , runs in a scheme similar to graphical model inference procedures , such as mean field and belief propagation . instead of performing probabilistic operations ( such as sum , product and renormalization ) , the algorithm performs nonlinear function mappings in each step , inspired by kernel message passing algorithm in  @xcite .",
    "furthermore , structure2vec is also different from the kernel message passing algorithm in several aspects .",
    "first , structure2vec deals with a different scenario , , learning similarity measure for structured data .",
    "second , structure2vec learns the nonlinear mappings using the discriminative information . and third , a variant of structure2vec can run in a mean field update fashion , different from message passing algorithms .    besides the above novel aspects , structure2vec is also very scalable in terms of both memory and computation requirements .",
    "first , it uses a small and explicit feature map for the nonlinear feature space , and avoids the need for keeping the kernel matrix .",
    "this makes the subsequent classifiers or regressors order of magnitude smaller compared to other methods .",
    "second , the nonlinear function mapping in structure2vec can be learned using stochastic gradient descent , allowing it to handle extremely large scale datasets .",
    "finally in experiments , we show that structure2vec compares favorably to other kernel methods in terms of classification accuracy in medium scale sequence and graph benchmark datasets including scop and nci .",
    "furthermore , structure2vec can handle extremely large data set , such as the 2.3 million molecule dataset from harvard clean energy project , run 2 times faster , produce model @xmath0 times smaller and achieve state - of - the - art accuracy .",
    "these strong empirical results suggest that the graphical models , theoretically well - grounded methods for capturing structure in data , combined with embedding techniques and discriminative training can significantly improve the performance in many large scale real - world structured data classification and regression problems .",
    "we denote by @xmath1 a random variable with domain @xmath2 , and refer to instantiations of @xmath1 by the lower case character , @xmath3 .",
    "we denote a density on @xmath2 by @xmath4 , and denote the space of all such densities by @xmath5 .",
    "we will also deal with multiple random variables , @xmath6 , with joint density @xmath7 .",
    "for simplicity of notation , we assume that the domains of all @xmath8 $ ] are the same , but the methodology applies to the cases where they have different domains . in the case when @xmath2 is a discrete domain , the density notation should be interpreted as probability , and integral should be interpreted as summation instead .",
    "furthermore , we denote by @xmath9 a hidden variable with domain @xmath10 and distribution @xmath11 .",
    "we use similar notation convention for variable @xmath9 and @xmath1 . + [ -2 mm ]    * kernel methods .",
    "* suppose the structured data is represented by @xmath12 .",
    "kernel methods owe the name to the use of kernel functions , @xmath13 , which are symmetric positive semidefinite ( psd ) , meaning that for all @xmath14 , and @xmath15 , and @xmath16 , we have @xmath17 .",
    "a signature of kernel methods is that learning algorithms for various tasks and with potentially very different nature can work exclusively on these pairwise kernel values without the need to access the original data points .",
    "+ * kernels for structured data .",
    "* each kernel function will correspond to some feature map @xmath18 , where the kernel function can be expressed as the inner product between feature maps ,  , @xmath19 . for structured input domain , one can design kernels using counts on substructures .",
    "for instance , the spectrum kernel for two sequences @xmath20 and @xmath21 is defined as  @xcite @xmath22 where @xmath23 is the set of possible subsequences , @xmath24 counts the number occurrence of subsequence @xmath25 in @xmath3 . in this case",
    ", the feature map @xmath26 corresponds to a vector of dimension @xmath27 .",
    "similarly , the graphlet kernel  @xcite for two graphs @xmath20 and @xmath21 can also be defined as  , but @xmath23 is now the set of possible subgraphs , and @xmath28 counts the number occurrence of subgraphs .",
    "we refer to this class of kernels as `` bag of structures '' ( bos ) kernel .",
    "kernels can also be defined by leveraging the power of probabilistic graphical models .",
    "for instance , the fisher kernel  @xcite is defined using a parametric model @xmath29 around its maximum likelihood estimate @xmath30 ,  , @xmath31 where @xmath32 and @xmath33 $ ] is the fisher information matrix .",
    "another classical example along the line is the probability product kernel  @xcite .",
    "different from the fisher kernel based on generative model fitted with the whole dataset , the probability product kernel is calculated based on the models @xmath34 fitted to individual data point , , @xmath35 where @xmath36 and @xmath37 are the maximum likelihood parameters for data point @xmath20 and @xmath21 respectively .",
    "we refer to this class of kernels as the `` graphical model '' ( gm ) kernels .",
    "+ [ -2 mm ]    * hilbert space embedding of distributions . *",
    "hilbert space embeddings of distributions are mappings of distributions into potentially _",
    "infinite _ dimensional feature spaces  @xcite , @xmath38 where the distribution is mapped to its expected feature map ,  ,  to a point in a feature space .",
    "kernel embedding of distributions has rich representational power .",
    "some feature map can make the mapping injective  @xcite , meaning that if two distributions , @xmath4 and @xmath39 , are different , they are mapped to two distinct points in the feature space . for instance , when @xmath40 , the feature spaces of many commonly used kernels , such as the gaussian rbf kernel @xmath41 , can make the embedding injective .",
    "alternatively , one can treat an injective embedding @xmath42 of a density @xmath4 as a sufficient statistic of the density .",
    "any information we need from the density is preserved in @xmath42 : with @xmath42 one can uniquely recover @xmath4 , and any operation on @xmath4 can be carried out via a corresponding operation on @xmath42 with the same result .",
    "for instance , this property will allow us to compute a functional @xmath43 of the density using the embedding only ,  , @xmath44 where @xmath45 is a corresponding function applied on @xmath42 .",
    "similarly the property can also be generalized to operators . for instance , applying an operator @xmath46 to a density can also be equivalently carried out using its embedding ,  , @xmath47 where @xmath48 is the alternative operator working on the embedding . in our later sections",
    ", we will extensively exploit this property of injective embeddings , by assuming that there exists a feature space such that the embeddings are injective .",
    "we include the discussion of other related work in appendix  [ sec : more_related_work ] .",
    "without loss of generality , we assume each structured data point @xmath20 is a graph , with a set of nodes @xmath49 and a set of edges @xmath50",
    ". we will use @xmath51 to denote the value of the attribute for node @xmath52 .",
    "we note the node attributes are different from the label of the entire data point .",
    "for instance , each atom in a molecule will correspond to a node in the graph , and the node attribute will be the atomic number , while the label for the entire molecule can be whether the molecule is a good drug or not .",
    "other structures , such as sequences and trees , can be viewed as special cases of general graphs .",
    "we will model the structured data point @xmath20 as an instance drawn from a graphical model .",
    "more specifically , we will model the label of each node in the graph with a variable @xmath53 , and furthermore , associate an additional hidden variable @xmath54 with it .",
    "then we will define a pairwise markov random field on these collection of random variables @xmath55 where @xmath56 and @xmath57 are nonnegative node and edge potentials respectively . in this model ,",
    "the variables are connected according to the graph structure of the input data point .",
    "that is to say , we use the graph structure of the input data directly as the conditional independence structure of an undirected graphical model .",
    "figure  [ fig : diagram_mf_lbp ] illustrates two concrete examples in constructing the graphical models for strings and graphs .",
    "one can design more complicated graphical models which go beyond pairwise markov random fields , and consider longer range interactions with potentials involving more variables .",
    "we will focus on pairwise markov random fields for simplicity of representation .",
    "we note that such a graphical model is built for each individual data point , and the conditional independence structures of two graphical models can be different if the two data points @xmath20 and @xmath21 are different .",
    "furthermore , we do not observe the value for the hidden variables @xmath58 , which makes the learning of the graphical model potentials @xmath57 and @xmath56 even more difficult .",
    "thus , we will not pursue the standard route of maximum likelihood estimation , and rather we will consider the sequence of computations needed when we try to embed the posterior of @xmath58 into a feature space .",
    "we will embed the posterior marginal @xmath59 of a hidden variable using a feature map @xmath60 ,  , @xmath61 the exact form of @xmath60 and the parameters in mrf @xmath59 is not fixed at the moment , and we will learn them later using supervision signals for the ultimate discriminative target . for now , we will assume that @xmath62 is a finite dimensional feature space , and the exact value of @xmath63 will determined by cross - validation in later experiments .",
    "however , compute the embedding is a very challenging task for general graphs : it involves performing an inference in graphical model where we need to integrate out all variables expect @xmath54 ,  , @xmath64 only when the graph structure is a tree , exact computation can be carried out efficiently via message passing  @xcite .",
    "thus in the general case , approximate inference algorithms , , mean field inference and loopy belief propagation ( bp ) , are developed . in many applications , however , these variational inference algorithms exhibit excellent empirical performance  @xcite .",
    "several theoretical studies have also provided insight into the approximations made by loopy bp , partially justifying its application to graphs with cycles  @xcite .    in the following subsection",
    ", we will explain the embedding of mean field and loopy bp .",
    "the embedding of other variational inference methods , , double - loop bp , damped bp , tree - reweighted bp , and generalized bp will be explained in appendix  [ appendix : other_embedding ] .",
    "we show that the iterative update steps in these algorithms , which are essentially minimizing approximations to the exact free energy , can be simply viewed as function mappings of the embedded marginals using the alternative view in and .",
    "the vanilla mean - field inference tries to approximate @xmath65 with a product of _ independent _ density components @xmath66 where each @xmath67 is a valid density , such that @xmath68 .",
    "furthermore , these density components are found by minimizing the following variational free energy  @xcite , @xmath69 one can show that the solution to the above optimization problem needs to satisfy the following fixed point equations for all @xmath70    @xmath71    where @xmath72 . here",
    "@xmath73 are the set of neighbors of variable @xmath54 in the graphical model , and @xmath74 is a constant .",
    "the fixed point equations in imply that @xmath75 is a functional of a set of neighboring marginals @xmath76 ,  , @xmath77 if for each marginal @xmath78 , we have an injective embedding @xmath79 then , using similar reasoning as in , we can equivalently express the fixed point equation from an embedding point of view ,  , @xmath80 , and consequently using the operator view from , we have @xmath81 for the embedded mean field  , the function @xmath82 and operator @xmath83 have complicated nonlinear dependencies on the potential functions @xmath56 , @xmath57 , and the feature mapping @xmath84 which is unknown and need to be learned from data . instead of first learning the @xmath56 and @xmath57 , and then working out @xmath83 , we will pursue a different route where we directly parameterize @xmath83 and later learn it with supervision signals .    in terms of the parameterization",
    ", we will assume @xmath85 where @xmath63 is a hyperparameter chosen using cross - validation .",
    "for @xmath83 , one can use any nonlinear function mappings .",
    "for instance , we can parameterize it as a neural network @xmath86 where @xmath87 is a rectified linear unit applied elementwisely to its argument , and @xmath88 .",
    "the number of the rows in @xmath89 equals to @xmath63 . with such parameterization , the mean field iterative update in the embedding space",
    "can be carried out as algorithm  [ alg : mf_msg ] .",
    "we could also multiply @xmath90 with @xmath91 to rescale the range of message embeddings if needed .",
    "in fact , with or without @xmath91 , the functions will be the same in terms of the representation power . specifically , for any @xmath92 , we can always find another ` equivalent ' parameters @xmath93 where @xmath94 .",
    "parameter @xmath89 in @xmath83 initialize @xmath95 , for all @xmath70 @xmath96 @xmath97 return @xmath98       parameter @xmath89 in @xmath99 and @xmath100 initialize @xmath101 , for all @xmath102 @xmath103 @xmath104 return @xmath105      loopy belief propagation is another variational inference method , which essentially optimizes the bethe free energy taking _ pairwise _ interactions into account  @xcite , @xmath106 subject to pairwise marginal consistency constraints : @xmath107 , @xmath108 , and @xmath109 .",
    "one can obtain the fixed point condition for the above optimization for all @xmath110 , @xmath111 where @xmath112 is the intermediate result called the message from node @xmath52 to @xmath113 .",
    "furthermore , @xmath112 is a nonnegative function which can be normalized to a density , and hence can also be embedded .",
    "similar to the reasoning in the mean field case , the   implies the messages @xmath112 and marginals @xmath75 are functionals of messages from neighbors , , @xmath114 with the assumption that there is an injective embedding for each message @xmath115 and for each marginal @xmath116 , we can apply the reasoning from   and  , and express the messages and marginals from the embedding view , @xmath117 we will also use parametrization for loopy bp embedding similar to the mean field case , , neural network with rectified linear unit @xmath118 .",
    "specifically , assume @xmath119 , @xmath120 @xmath121    \\widetilde\\mu_i & =   \\sigma\\big(w_3x_i + w_4\\sum_{k\\in \\ncal(i)}\\widetilde\\nu_{ki}\\big)\\end{aligned}\\ ] ] where @xmath122 are matrices with appropriate sizes .",
    "note that one can use other nonlinear function mappings to parameterize @xmath123 and @xmath124 as well .",
    "overall , the loopy bp embedding updates is summarized in algorithm  [ alg : lbp_msg ] .    with similar strategy as in mean field case",
    ", we will learn the parameters in @xmath123 and @xmath124 later with supervision signals from the discriminative task .",
    "in fact , there are many other variational inference methods , with different forms of free energies or different optimization algorithms , resulting different message update forms , , double - loop bp  @xcite , damped bp  @xcite , tree - reweightd bp  @xcite , and generalized bp  @xcite .",
    "the proposed embedding method is a general technique which can be tailored to these algorithms .",
    "the major difference is the dependences in the messages . for",
    "the details of embedding of these algorithms , please refer to appendix  [ appendix : other_embedding ] .",
    "similar to kernel bp  @xcite and kernel ep  @xcite , our current work exploits feature space embedding to reformulate graphical model inference procedures",
    ". however , different from the kernel bp and kernel ep , in which the feature spaces are chosen beforehand and the conditional embedding operators are learned locally , our approach will learn both the feature spaces , the transformation @xmath83 , as well as the regressor or classifier for the target values end - to - end using label information .",
    "specifically , we are provided with a training dataset @xmath125 , where @xmath126 is a structured data point and @xmath127 , where @xmath128 for regression or @xmath129 for classification problem , respectively . with the feature embedding procedure introduced in section  [ sec : lvm_embedding ]",
    ", each data point will be represented as a set of embeddings @xmath130 .",
    "now the goal is to learn a regression or classification function @xmath131 linking @xmath132 to @xmath133 .",
    "more specifically , in the case of regression problem , we will parametrize function @xmath134 as @xmath135 , where @xmath136 is the final mapping from summed ( or pooled ) embeddings to output .",
    "the parameters @xmath137 and those @xmath89 involved in the embeddings are learned by minimizing the empirical square loss @xmath138 note that each data point will have its own graphical model and embedded features due to its individual structure , but the parameters @xmath137 and @xmath89 , are shared across these graphical models .    in the case of @xmath139-class classification problem",
    ", we denote @xmath140 is the @xmath141-of-@xmath139 representation of @xmath142 , , @xmath143 , @xmath144 if @xmath145 , and @xmath146 , @xmath147 . by",
    "adopt the softmax loss , we obtain the optimization for embedding parameters and discriminative classifier estimation as , @xmath148 where @xmath149 , @xmath150 are the parameters for mapping embedding to output .",
    "dataset @xmath125 , loss function @xmath151 .",
    "initialize @xmath152 randomly .",
    "sample @xmath153 uniform randomly from @xmath154 .",
    "construct latent variable model @xmath155 as  .",
    "embed @xmath155 as @xmath156 by algorithm  [ alg : mf_msg ] or  [ alg : lbp_msg ] with @xmath157 .",
    "update @xmath158 .",
    "return @xmath159    the same idea can also be generalized to other discriminative tasks with different loss functions .",
    "as we can see from the optimization problems  ( [ eq : regression ] ) and ( [ eq : classification ] ) , the objective functions are directly related to the corresponding discriminative tasks , and so as to @xmath89 and @xmath160 .",
    "conceptually , the procedure starts with representing each datum by a graphical model constructed corresponding to its _ individual _ structure with _ sharing _ potential functions , and then , we embed these graphical models with the _ same _ feature mappings . finally the embedded marginals are aggregated with a prediction function for a discriminative task . the shared potential functions , feature mappings and final prediction functions are all learned together for the ultimate task with supervision signals .",
    "we optimize the objective   or   with stochastic gradient descent for scalability consideration .",
    "however , other optimization algorithms are also applicable , and our method does not depend on this particular choice .",
    "the gradients of the parameters @xmath89 are calculated recursively similar to recurrent neural network for sequence models . in our case",
    ", the recursive structure will correspond the message passing structure .",
    "the overall framework is illustrated in algorithm  [ alg : framework ] .",
    "for details of the gradient calculation , please refer to appendix  [ appendix : derivative ] .",
    "below we first compare our method with algorithms using prefixed kernel on string and graph benchmark datasets",
    ". then we focus on harvard clean energy project dataset which contains 2.3 million samples .",
    "we demonstrate that while getting comparable performance on medium sized datasets , we are able to handle millions of samples , and getting much better when more training data are given .",
    "the two variants of structure2vec are denoted as de - mf and de - lbp , which stands for discriminative embedding using mean field or loopy belief propagation , respectively .",
    "our algorithms are implemented with c++ and cuda , and experiments are carried out on clusters equipped with nvidia tesla k20 .",
    "the code is available on https://github.com/hanjun-dai/graphnn .",
    "we compare our algorithm on string benchmark datasets with the kernel method with existing sequence kernels , , the spectrum string kernel  @xcite , mismatch string kernel  @xcite and fisher kernel with hmm generative models  @xcite . on graph",
    "benchmark datasets , we compare with subtree kernel  @xcite ( r&g , for short ) , random walk kernel@xcite , shortest path kernel  @xcite , graphlet kernel@xcite and the family of weisfeiler - lehman kernels ( wl kernel )  @xcite . after getting the kernel matrix , we train svm classifier or regressor on top .",
    "we tune all the methods via cross validation , and report the average performance .",
    "specifically , for structured kernel methods , we tune the degree in @xmath161 ( for mismatch kernel , we also tune the maximum mismatch length in @xmath162 ) and train svm classifier  @xcite on top , where the trade - off parameter @xmath163 is also chosen in @xmath164 by cross validation . for fisher kernel that using hmm as generative model , we also tune the number of hidden states assigned to hmm in @xmath165 .    for our methods ,",
    "we simply use one - hot vector ( the vector representation of discrete node attribute ) as the embedding for observed nodes , and use a two - layer neural network for the embedding ( prediction ) of target value .",
    "the hidden layer size @xmath166 of neural network , the embedding dimension @xmath167 of hidden variables and the number of iterations @xmath168 are tuned via cross validation .",
    "we keep the number of parameters small , and use early stopping  @xcite to avoid overfitting in these small datasets .      here",
    "we do experiments on two string binary classification benchmark datasets .",
    "the first one ( denoted as scop ) contains 7329 sequences obtained from scop ( structural classification of proteins ) 1.59 database  @xcite .",
    "methods are evaluated on the ability to detect members of a target scop family ( positive test set ) belonging to the same scop superfamily as the positive training sequences , and no members of the target family are available during training .",
    "we use the same 54 target families and the same training / test splits as in remote homology detection  @xcite .",
    "the second one is fc and res dataset ( denoted as fc_res ) provided by crispr / cas9 system , on which the task it to tell whether the guide rna will direct cas9 to target dna .",
    "there are 5310 guides included in the dataset .",
    "details of this dataset can be found in @xcite .",
    "we use two variants for spectrum string kernel : 1 ) kmer - single , where the constructed kernel matrix @xmath169 only consider patterns of length @xmath170 ; 2 ) kmer - concat , where kernel matrix @xmath171 .",
    "we also find the normalized kernel matrix @xmath172 helps .",
    ".mean auc on string classification datasets [ cols=\"<,^,^\",options=\"header \" , ]     to understand the effect of the inference embedding in the proposed algorithm framework , we further compare our methods with different number of fixed point iterations in figure  [ fig : cep_exp_detail ] .",
    "it can see that , higher number of fixed point iterations will lead to faster convergence , though the number of parameters of the model in different settings are the same .",
    "the mean field embedding will get much worse result if only one iteration is executed .",
    "compare to the loopy bp case with same setting , the latter one will always have one more round message passing since we need to aggregate the messages from edge to node in the last step . and also",
    ", from the quality of prediction we find that , though making slightly higher prediction error for molecules with high pce values due to insufficient data , the variants of our algorithm are not overfitting the ` easy ' ( i.e. , the most popular ) range of pce value .",
    "we propose , structure2vec , an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces , and learning such feature spaces using discriminative information .",
    "interestingly , structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures , such as mean field and belief propagation . in applications involving millions of data points",
    ", we showed that structure2vec runs 2 times faster , produces models @xmath0 times smaller , while at the same time achieving the state - of - the - art predictive performance .",
    "structure2vec provides a nice example for the general strategy of combining the strength of graphical models , hilbert space embedding of distribution and deep learning approach , which we believe will become common in many other learning tasks .",
    "* acknowledgements .",
    "* this project was supported in part by nsf / nih bigdata 1r01gm108341 , onr n00014 - 15 - 1 - 2340 , nsf iis-1218749 , and nsf career iis-1350983 .",
    "54 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    andreeva , a. , howorth , d. , brenner , s.  e. , hubbard , t.  j. , chothia , c. , and murzin , a.  g. scop database in 2004 : refinements integrate structure and sequence family data . _ nucleic acids research _ , 320 ( suppl 1):0 d226d229 , 2004 .",
    "borgwardt , k.  m. _ graph kernels_. phd thesis , ludwig - maximilians - university , munich , germany , 2007 .",
    "borgwardt , k.  m. and kriegel , h .-",
    "shortest - path kernels on graphs . in _ icdm _ , 2005 .",
    "bruna , j. , zaremba , w. , szlam , a. , and lecun , y. spectral networks and locally connected networks on graphs .",
    "_ arxiv preprint arxiv:1312.6203 _ , 2013 .",
    "chang , c.  c. and lin , c.  j. _ libsvm : a library for support vector machines _",
    "software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm .",
    "chen , l.  c. , schwing , a.  g. , yuille , a.  l. , and urtasun , r. learning deep structured models .",
    "_ arxiv preprint arxiv:1407.2538 _ , 2014 .",
    "debnath , a.  k. , lopez  de compadre , r.  l. , debnath , g. , shusterman , a.  j. , and hansch , c. structure - activity relationship of mutagenic aromatic and heteroaromatic nitro compounds .",
    "correlation with molecular orbital energies and hydrophobicity .",
    "_ j med chem _ , 34:0 786797 , 1991 .",
    "dobson , p.  d. and doig , a.  j. distinguishing enzyme structures from non - enzymes without alignments . _ j mol biol _ , 3300 ( 4):0 771783 , jul 2003 .",
    "doench , j.  g. , hartenian , e. , graham , d.  b. , tothova , z. , hegde , h. , smith , i. , sullender , m. , ebert , b.  l. , xavier , r.  j. , and root , d.  e. rational design of highly active sgrnas for crispr - cas9-mediated gene inactivation . _ nature biotechnology _ , 320 ( 12):0 12621267 , 2014 .",
    "duvenaud , d.  k. , maclaurin , d. , iparraguirre , j. , bombarell , r. , hirzel , t. , aspuru - guzik , a. , and adams , r.  p. convolutional networks on graphs for learning molecular fingerprints . in _ advances in neural information processing systems",
    "_ , pp . 22152223 , 2015 .",
    "fusi , n. , smith , i. , doench , j. , and listgarten , j. in silico predictive modeling of crispr / cas9 guide efficiency .",
    "_ biorxiv _ , 2015 .",
    "doi : 10.1101/021568 .",
    "url http://biorxiv.org/content/early/2015/06/26/021568 .",
    "grtner , t. , flach , p.a . , and wrobel , s. on graph kernels : hardness results and efficient alternatives . in schlkopf , b. and warmuth , m.  k. ( eds . ) , _ proceedings of annual conference .",
    "computational learning theory _ , pp .",
    "springer , 2003 .",
    "caruana , r. , lawrence , s. , and giles , l. overfitting in neural nets : backpropagation , conjugate gradient , and early stopping . in _ advances in neural information processing systems 13",
    "_ , volume  13 , pp .   402 . mit press , 2001 .",
    "hachmann , j. , olivares - amaya , r. , atahan - evrenk , s. , amador - bedolla , c. , snchez - carrera , r.  s. , gold - parker , a. , vogt , l. , brockway , a.  m. , and aspuru - guzik , a. the harvard clean energy project : large - scale computational screening and design of organic photovoltaics on the world community grid . _ the journal of physical chemistry letters _ , 20",
    "( 17):0 22412251 , 2011 .",
    "henaff , m. , bruna , j. , and lecun , y. deep convolutional networks on graph - structured data .",
    "_ arxiv preprint arxiv:1506.05163 _ , 2015 .",
    "hershey , j.  r. , roux , j.  l. , and weninger , f. deep unfolding : model - based inspiration of novel deep architectures .",
    "_ arxiv preprint arxiv:1409.2574 _ , 2014 .",
    "heskes , t. stable fixed points of loopy belief propagation are local minima of the bethe free energy .",
    "_ advances in neural information processing systems _ , pp .   343350 . mit press , 2002 .",
    "jaakkola , t.  s. and haussler , d. exploiting generative models in discriminative classifiers . in kearns , m.  s. , solla , s.  a. , and cohn , d.  a. ( eds . ) ,",
    "_ advances in neural information processing systems 11 _ , pp .",
    "mit press , 1999 .",
    "jebara , t. , kondor , r. , and howard , a. probability product kernels .",
    "_ j. mach .",
    "_ , 5:0 819844 , 2004 .",
    "jitkrittum , w. , gretton , a. , heess , n. , eslami , s. m.  a. , lakshminarayanan , b. , sejdinovic , d. , and szab , z. kernel - based just - in - time learning for passing expectation propagation messages . in _ proceedings of the thirty - first conference on uncertainty in artificial intelligence ,",
    "uai 2015 , july 12 - 16 , 2015 , amsterdam , the netherlands _ , pp .   405414 , 2015 .",
    "kuang , r. , ie , e. , wang , k. , wang , k. , siddiqi , m. , freund , y. , and leslie , c. profile - based string kernels for remote homology detection and motif extraction .",
    "_ journal of bioinformatics and computational biology _ , 30 ( 03):0 527550 , 2005 .",
    "landrum , g. rdkit : open - source cheminformatics ( 2013 ) , 2012 .",
    "leslie , c. , eskin , e. , and noble , w.  s. the spectrum kernel : a string kernel for svm protein classification . in _ proceedings of the pacific symposium on biocomputing _ , pp .",
    "564575 , singapore , 2002 . world scientific publishing .",
    "leslie , c. , eskin , e. , weston , j. , and noble , w.  s. mismatch string kernels for svm protein classification . in _ advances in neural information processing systems _ , volume  15 , cambridge , ma , 2002 . mit press .",
    "li , y. , tarlow , d. , brockschmidt , m. , and zemel , r .. gated graph sequence neural networks .",
    "_ arxiv preprint arxiv:1511.05493 _ , 2015 .",
    "lin , g. , shen , c. , reid , i. , and van den hengel , a. deeply learning the messages in message passing inference . in _ advances in neural information processing systems _ , 2015 .",
    "minka , t. .",
    "_ see www .",
    "edu / minka / papers / learning .",
    "html , august _ , 2001 .",
    "mou , l. , li , g. , zhang , l. , wang , t. , and jin , z .. convolutional neural networks over tree structures for programming language processing . in _ proceedings of the thirtieth aaai conference on artificial intelligence _ , 2016 .",
    "murphy , k.  p. , weiss , y. , and jordan , m.  i. loopy belief propagation for approximate inference : an empirical study . in _",
    "uai _ , pp .   467475 , 1999 .",
    "pearl , j. _ probabilistic reasoning in intelligent systems : networks of plausible inference_. morgan kaufman , 1988 .",
    "pyzer - knapp , e.  o. , li , k. , and aspuru - guzik , a. learning from the harvard clean energy project : the use of neural networks to accelerate materials discovery .",
    "_ advanced functional materials _ , 250 ( 41):0 64956502 , 2015 .",
    "ramon , j. and grtner , t. expressivity versus efficiency of graph kernels .",
    "technical report , first international workshop on mining graphs , trees and sequences ( held with ecml / pkdd03 ) , 2003 .",
    "ross , s. , munoz , d. , hebert , m. , and bagnell , j.  a. learning message - passing inference machines for structured prediction . in _",
    "ieee conference on computer vision and pattern recognition _ , pp .",
    "ieee , 2011 .",
    "scarselli , f. , gori , m. , tsoi , a.  c. , hagenbuchner , m. , and monfardini , g. the graph neural network model . _ ieee transactions on neural networks _",
    ", 200 ( 1):0 6180 , 2009 .",
    "schlkopf , b. , tsuda , k. , and vert , j .-",
    "_ kernel methods in computational biology_. mit press , cambridge , ma , 2004 .",
    "schlkopf , b. , and smola , a.  j. _ learning with kernels_. press , cambridge , ma , 2002 .",
    "shervashidze , n. , vishwanathan , s.  v.  n. , petri , t. , mehlhorn , k. , and borgwardt , k. efficient graphlet kernels for large graph comparison .",
    "_ proceedings of international conference on artificial intelligence and statistics_. society for artificial intelligence and statistics , 2009 .",
    "shervashidze , n. , schweitzer , p. , van  leeuwen , e.  j. , mehlhorn , k. , and borgwardt , k.  m. weisfeiler - lehman graph kernels . _ the journal of machine learning research _ , 12:0 25392561 , 2011 .",
    "smola , a.  j. , gretton , a. , song , l. , and schlkopf , b. a hilbert space embedding for distributions . in _ proceedings of the international conference on algorithmic learning theory _ ,",
    "volume 4754 , pp .",
    "springer , 2007 .",
    "song , l. , huang , j. , smola , a.  j. , and fukumizu , k. hilbert space embeddings of conditional distributions . in _ proceedings of the international conference on machine learning _ , 2009 .",
    "song , l. , gretton , a. , and guestrin , c. nonparametric tree graphical models . in",
    "_ 13th workshop on artificial intelligence and statistics _ ,",
    "volume  9 of _ jmlr workshop and conference proceedings _ , pp .   765772 , 2010 .",
    "song , l. , gretton , a. , bickson , d. , low , y. , and guestrin , c. kernel belief propagation . in _ proc .",
    "conference on artificial intelligence and statistics _ ,",
    "volume  10 of _ jmlr workshop and conference proceedings _ , 2011 .",
    "sriperumbudur , b. , gretton , a. , fukumizu , k. , lanckriet , g. , and schlkopf , b. injective hilbert space embeddings of probability measures . in _ proceedings of annual conference .  computational learning theory _ , pp . 111122 , 2008 .",
    "sugiyama , m. and borgwardt , k. halting in random walk kernels . in _ advances in neural information processing systems _ , pp .",
    "16301638 , 2015 .",
    "vishwanathan , s.  v.  n. and smola , a.  j. fast kernels for string and tree matching . in becker ,",
    "s. , thrun , s. , and obermayer , k. ( eds . ) , _ advances in neural information processing systems 15 _ , pp .",
    "mit press , cambridge , ma , 2003 .",
    "vishwanathan , s.  v.  n. , schraudolph , n.  n. , kondor , i.  r. , and borgwardt , k.  m. graph kernels .",
    "_ journal of machine learning research _ , 2010 .",
    "url http://www.stat.purdue.edu/~vishy/papers/visschkonbor10.pdf . in press .",
    "wainwright , m. , jaakkola , t. , and willsky , a. tree - reweighted belief propagation and approximate ml estimation by pseudo - moment matching . in _",
    "9th workshop on artificial intelligence and statistics _ , 2003 .",
    "wainwright , m.  j. and jordan , m.  i. graphical models , exponential families , and variational inference . _ foundations and trends in machine learning _ , 10 ( 1  2):0 1305 , 2008 .    wale , n. , watson , i.  a. , and karypis , g. comparison of descriptor spaces for chemical compound retrieval and classification .",
    "_ knowledge and information systems _ , 140 ( 3):0 347375 , 2008 .",
    "yedidia , j.  s. , freeman , w.  t. , and weiss , y. generalized belief propagation .",
    "_ advances in neural information processing systems _ , pp .   689695 . mit press , 2001 .",
    "yedidia , j.s . ,",
    "freeman , w.t . , and",
    "weiss , y. bethe free energy , kikuchi approximations and belief propagation algorithms . technical report , mitsubishi electric research laboratories , 2001 .",
    "yedidia , j.s . ,",
    "freeman , w.t . , and weiss , y. constructing free - energy approximations and generalized belief propagation algorithms .",
    "_ ieee transactions on information theory _ , 510 ( 7):0 22822312 , 2005 .",
    "yuille , a.  l. cccp algorithms to minimize the bethe and kikuchi free energies : convergent alternatives to belief propagation .",
    "_ neural computation _ , 14:0 2002 , 2002 .",
    "zheng , s. , jayasumana , s. , romera - paredes , b. , vineet , b. , su , z. , du , d. , huang , c. , and torr , p. conditional random fields as recurrent neural networks .",
    "_ arxiv preprint arxiv:1502.03240 _ , 2015 .",
    "neural network is also a powerful tool on graph structured data .",
    "@xcite proposed a neural network which generates features by solving a heuristic nonlinear system iteratively , and is learned using almeida - pineda algorithm . to guarantee the existence of the solution to the nonlinear system",
    ", there are extra requirements for the features generating function . from this perspective",
    ", the model in  @xcite can be considered as an extension of  @xcite where the gated recurrent unit is used for feature generation . rather than these heuristic models",
    ", our model is based on the principled graphical model embedding framework , which results in flexible embedding functions for generating features .",
    "meanwhile , the model can be learned efficiently by traditional stochastic gradient descent .",
    "there are several work transferring locality concept of convolutional neural networks  ( cnn ) from euclidean domain to graph case , using hierarchical clustering , graph laplacian  @xcite , or graph fourier transform @xcite .",
    "these models are still restricted to problems with the same graph structure , which is not suitable for learning with molecules .",
    "@xcite proposed a convolution operation on trees , while the locality are defined based on parent - child relations .",
    "@xcite used cnn to learn the circulant fingerprints for graphs from end to end .",
    "the dictionary of fingerprints are maintained using softmax of subtree feature representations , in order to obtain a differentiable model .",
    "if we unroll the steps in algorithm  [ alg : framework ] , it can also be viewed as an end to end learning system .",
    "however , the structures of the proposed model are deeply rooted in graphical model embedding , from mean field and loopy bp , respectively . also , since the parameters will be shared across different unrolling steps , we would have more compact model . as will be shown in the experiment section ,",
    "our model is easy to train , while yielding good generalization ability .      by recognizing inference as computational expressions , inference machines  @xcite",
    "incorporate learning into the messages passing inference for crfs .",
    "more recently , @xcite designed specific recurrent neural networks and convolutional neural networks for imitating the messages in crfs .",
    "although these methods share the similarity , , bypassing learning potential function , to the proposed framework , there are significant differences comparing to the proposed framework .",
    "the most important difference lies in the learning setting . in these existing messages learning work  @xcite",
    ", the learning task is still estimating the messages represented graphical models with designed function forms , , rnn or cnn , by maximizing loglikelihood . while in our work , we represented each structured data as a distribution , and the learning task is regression or classification over these distributions .",
    "therefore , we treat the embedded models as samples , and learn the nonlinear mapping for embedding , and regressor or classifier , @xmath173 , over these distributions jointly , with task - dependent user - specified loss functions .    another difference is the way in constructing the messages forms , and thus , the neural networks architecture . in the existing work ,",
    "the neural networks forms are constructed _ strictly _ follows the message updates forms   or  . due to such restriction",
    ", these works only focus on discrete variables with finite values , and is difficult to extend to continuous variables because of the integration .",
    "however , by exploiting the embedding point of view , we are able to build the messages with more _ flexible _ forms without losing the dependencies",
    ". meanwhile , the difficulty in calculating integration for continuous variables is no longer a problem with the reasoning   and .",
    "in this section , we derive the fixed - point equation for mean - field inference in section  [ sec : lvm_embedding ] .",
    "as we introduced , the mean - field inference is indeed minimizing the variational free energy , @xmath174 plug the mrf   into objective , we have @xmath175 where @xmath176 .",
    "take functional derivatives of @xmath177 w.r.t .",
    "@xmath75 , and set them to zeros , we obtain the fixed - point condition in section  [ sec : lvm_embedding ] , @xmath178    this fixed - point condition could be further reduced due to the independence between @xmath179 and @xmath180 given @xmath181 , , @xmath182 where @xmath72 .",
    "the derivation of the fixed - point condition for loopy bp can be found in  @xcite . however , to keep the paper self - contained , we provide the details here .",
    "the objective of loopy bp is @xmath183    denote @xmath184 is the multiplier to marginalization constraints @xmath185 , the lagrangian is formed as @xmath186 with normalization constraints @xmath109 .",
    "take functional derivatives of @xmath187 with respect to @xmath188 and @xmath75 , and set them to zero , we have @xmath189 we set @xmath190 , therefore , @xmath191 plug it into @xmath188 and @xmath75 , we recover the loopy bp update for marginal belief and @xmath192 the update rule for message @xmath112 can be recovered using the marginal consistency constraints , @xmath193    moreover , we also obtain the other important relationship between @xmath112 and @xmath194 by marginal consistency constraint and the definition of @xmath112 , @xmath195",
    "the proposed embedding is a general algorithm and can be tailored to other variational inference methods with message passing paradigm . in this section ,",
    "we discuss the embedding for several alternatives , which optimize the primal and dual bethe free energy , its convexified version and kikuchi free energy , respectively .",
    "we will parametrize the messages with the same function class , , neural networks with relu .",
    "more generally , we can also treat the weights as parameters and learn them together .",
    "noticed the bethe free energy can be decomposed into the summation of a convex function and a concave function , @xcite utilizes cccp to minimize the bethe free energy , resulting the double - loop algorithm .",
    "take the gradient of lagrangian of the objective function , and set to zero , the primal variable can be represented in dual form , @xmath196 the algorithm updates @xmath197 and @xmath198 alternatively , @xmath199 consider the @xmath200 as messages , with the injective embedding assumptions for corresponding messages , follow the same notation , we can express the messages in embedding view @xmath201 therefore , we have the parametrization form as @xmath202 where @xmath120 , @xmath203 , and @xmath204 are matrices with appropriate size .      instead of the primal form of bethe free energy , @xcite investigates the duality of the optimization , @xmath205 subject to @xmath206 . define message as @xmath207 the messages updates are @xmath208 and the @xmath209 which results the embedding follows the same injective assumption and notations , @xmath210 and the parametrization , @xmath211    it is interesting that after parametrization , the embeddings of double - loop bp and damped bp are essentially the same , which reveal the connection between double - loop bp and damped bp .      different from loopy bp and its variants which optimizing the bethe free energy , the tree - reweighted bp  @xcite is optimizing a convexified bethe energy , @xmath212 subject to pairwise marginal consistency constraints : @xmath107 , @xmath108 , and @xmath109 .",
    "the @xmath213 represents the probabilities that each edge appears in a spanning tree randomly chose from all spanning tree from @xmath214 under some measure .",
    "follow the same strategy as loopy bp update derivations , , take derivatives of the corresponding lagrangian with respect to @xmath78 and @xmath215 and set to zero , meanwhile , incorporate with the marginal consistency , we can arrive the messages updates , @xmath216 similarly , the embedded messages and the marginals on nodes can be obtained as @xmath217 parametrize these message in the same way , we obtain , @xmath218 notice the tree - weighted bp contains extra parameters @xmath219 which is in the spanning tree polytope as  @xcite .",
    "the kikuchi free energy is the generalization of the bethe free energy by involving _",
    "high - order _ interactions .",
    "more specifically , given the mrfs , we denote @xmath220 to be a set of regions , , some basic clusters of nodes , their intersections , the intersections of the intersections , and so on .",
    "we denote the @xmath221 or @xmath222 , , subregions or superregions of @xmath223 , as the set of regions completely contained in @xmath223 or containing @xmath223 , respectively .",
    "let @xmath224 be the state of the nodes in region @xmath223 , then , the kikuchi free energy is @xmath225 where @xmath226 is over - counting number of region @xmath223 , defined by @xmath227 with @xmath228 if @xmath223 is the largest region in @xmath220 .",
    "it is straightforward to verify that the bethe free energy is a special case of the kikuchi free energy by setting the basic cluster as pair of nodes .",
    "the generalized loopy bp  @xcite is trying to seek the stationary points of the kikuchi free energy under regional marginal consistency constraints and density validation constraints by following messages updates , @xmath229 where @xmath230 \\mbar_{r , s}(h_s ) = \\prod_{\\{r',s'\\}\\in m(r , s)}m_{r ' , s'}(h_{s'}),\\\\[-2 mm ] \\overline\\psi(h_r , x_{r\\setminus s})=\\prod_{i , j\\in r } \\psi(h_i , h_j)\\prod_{i\\in r\\setminus s}\\phi(h_i , x_i).\\end{aligned}\\ ] ] the @xmath231 denotes the indices of messages @xmath232 that @xmath233 , and @xmath234 is outside @xmath223 .",
    "@xmath235 is the set of indices of messages @xmath232 where @xmath236 and @xmath237 .    with the injective embedding assumption for each message @xmath238 and @xmath239 , following the reasoning   and",
    ", we can express the embeddings as @xmath240 following the same parameterization in loopy bp , we represent the embeddings by neural network with rectified linear units , @xmath241    \\widetilde\\mu_i & = \\sigma\\big(\\sum_{i\\in r}w_4^ix_i + w_5\\sum_{m(r)}\\widetilde\\nu_{r ' , s'}\\big)\\end{aligned}\\ ] ] where @xmath242 are matrices with appropriate sizes .",
    "the generalized bp embedding updates will be almost the same as algorithm  [ alg : lbp_msg ] except the order of the iterations .",
    "we start from the messages into the smallest region first  @xcite .",
    "* remark : * the choice of basis clusters and the form of messages determine the dependency in the embedding",
    ". please refer to @xcite for details about the principles to partition the graph structure , and several other generalized bp variants with different messages forms .",
    "the algorithms proposed for minimizing the bethe free energy  @xcite can also be extended for kikuchi free energy , resulting in different embedding forms .",
    "we can use the chain rule to obtain the derivatives with respect to @xmath243 . according to equation",
    "[ eq : regression ] and equation  [ eq : classification ] , the message passed to supervised label @xmath133 for @xmath244th sample can be represented as @xmath245 , and the corresponding derivative can be denoted as @xmath246            in mean field embedding , we unfold the fixed point equation by the iteration index @xmath253 . at @xmath254th iteration , the partial derivative is denoted as @xmath255 .",
    "the partial derivative with respect to the embedding obtained by last round fixed point iteration is already defined above : @xmath256        similar as above case , we can first obtain the derivatives with respect to embeddings of hidden variables @xmath260 .",
    "since the last round of message passing only involves the edge - to - node operations , we can easily get the following derivatives .",
    "@xmath261    now we consider the partial derivatives for the pairwise message embeddings for different @xmath262 .",
    "again , the top level one is trivial , which is given by @xmath263 . using similar recursion trick",
    ", we can get the following chain rule for getting partial derivatives with respect to each pairwise message in each stage of fixed point iteration .",
    "@xmath264 ) } \\end{aligned}\\ ] ]      @xmath266 ) } x_i^t \\\\    \\nabla_{w_2 } l(f(\\widetilde\\mu^n ; \\ub ) , y_n ) = & \\sum_{t=1}^{t-1 } \\sum_{(i , j)\\in \\ecal_n } \\frac{\\partial l}{\\partial \\widetilde\\nu_{ij}^{n(t+1 ) } } \\frac{\\partial \\sigma}{\\partial ( w_1x_i + w_2 \\sum_{k \\in \\ncal(i ) \\setminus j}[\\widetilde\\nu_{ki}^{n(t ) } ] ) } ( \\sum_{k \\in \\ncal(i ) \\setminus j}[\\widetilde\\nu_{ki}^{n(t)}])^t\\\\\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> kernel classifiers and regressors designed for structured data , such as sequences , trees and graphs , have significantly advanced a number of interdisciplinary areas such as computational biology and drug design . </S>",
    "<S> typically , kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models , and then a discriminative classifier is learned based on the kernels via convex optimization . however , such an elegant two - stage approach also limited kernel methods from scaling up to millions of data points , and exploiting discriminative information to learn feature representations .    </S>",
    "<S> we propose , structure2vec , an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces , and learning such feature spaces using discriminative information . </S>",
    "<S> interestingly , structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures , such as mean field and belief propagation . in applications involving millions of data points </S>",
    "<S> , we showed that structure2vec runs 2 times faster , produces models which are @xmath0 times smaller , while at the same time achieving the state - of - the - art predictive performance . </S>"
  ]
}