{
  "article_text": [
    "with the accelerated growth of the internet and a wide range of web services such as electronic commerce and online video streaming , people are easily overwhelmed by massive amounts of information and therefore recommender systems are indispensable tools to alleviate the information overload problem . at the heart of each recommender system",
    ", there is an algorithm that handles the rating prediction task and the accuracy of the rating prediction algorithm is the foundation of the system .",
    "the most successful and widely used approach to implement such an algorithm is collaborative filtering with matrix factorization ( mf ) .",
    "such an approach has the advantage of high accuracy , robustness and scalability , and it is thus more favorable than the other approaches , such as the neighborhood - based approach and the graph - based approach @xcite .",
    "the mf approach proved its success in the netflix prize competition @xcite as the winning submission of this competition was heavily relied on it to predict unobserved ratings .",
    "the mf approach decomposes a user - item rating matrix into two low - rank matrices which directly profile _ users _ and _ items _ to the latent factor space respectively and these representative latent factors form the main basis for further prediction in the future .    although mf is the state - of - the - art approach that can successfully process the relational rating data , its capability of capturing the temporal dynamics of users preferences is quite limited . as we are facing the fast - moving business environment ,",
    "the real world is not static but full of dynamics .",
    "there are a great variety of sources that can cause the changes of users behavior , including shifting trends in the community , the arrival of new products , the changes in users social networks , and so on .",
    "recent research in @xcite considered the aspect of personal development and pointed out that user s expertise may change from amateurs to connoisseurs as they become more experienced . to satisfy users current taste and need , a key building block for recommender systems",
    "is to accurately model such user preferences as they evolve over time .",
    "the need to model the temporal dynamics of user preferences raises some fundamental challenges .",
    "first of all , the amount of available data is significantly reduced in a specific time step and the sparsity problem of recommender systems is more severe in this situation .",
    "in addition , how can we generally incorporate the temporal dimension and further capture the evolution of preferences at the individual level for every time step ?",
    "finally , what is the principled method to model this kind of transition for every user in order to make more accurate predictions in the future ? toward this end , we propose a general and principled temporal dynamic model for tracking concept drift in each individual user latent vector .",
    "such an approach can further effectively and efficiently achieve a lower rmse than that of mf .",
    "the main contributions of this paper include :    * we propose a temporal matrix factorization approach ( tmf ) for tracking concept drift in each individual user latent vector .",
    "such a method not only breaks the limit of using static decompositions in the original mf approach , but also provides a tool for recommender systems to better serve `` valuable '' customers in the future .",
    "* we develop a modified stochastic gradient descent method to learn an individual user latent vector at each time step by using both the overall rating logs and the rating logs within the specific time step . * by using the lasso regression for the user latent vector at every time step , we learn a linear system model that can be used for modelling the transition pattern at the individual level .",
    "* we conduct comprehensive experiments on a synthetic dataset and four real datasets , ciao , epinions , flixster and movielens . in comparison with the original mf ,",
    "our experimental results show that our tmf approach is able to achieve lower root mean square errors ( rmse ) for both the synthetic and real datasets . in particular",
    ", there is roughly a 17 - 26% improvement on the synthetic dataset and a 5% improvement on the ciao dataset .",
    "such an improvement is quite significant . *",
    "our experiments also reveal one interesting finding .",
    "the performance gain in rmse is mostly from those users who indeed have concept drift in their user latent vectors at the time of prediction . in particular , for the synthetic dataset and the ciao dataset , there are quite a few users with that property and the performance gains for these two datasets are more significant than those for the other datasets .",
    "the rest of paper is organized as follows . in section 2",
    ", we provide a review of related work .",
    "we define the rating prediction problem in section 3 and propose the method of incorporating temporal dynamics including capturing and predicting the user preferences in section 4 . in section 5 ,",
    "we conduct experiments on both synthetic and several real datasets to validate our proposed temporal method .",
    "finally , we conclude our work and point out the future research directions in section 6 .",
    "in this section , we first briefly review the mf approach for recommender systems and several recent approaches that intend to incorporate temporal dynamics with mf , including time - dependent collaborative filtering , tensor factorization , and collaborative kalman filter .",
    "matrix factorization ( mf ) performs well in the rating prediction task and has attracted considerable attention .",
    "the rationale behind the mf approach is to characterize each user and item by a series of latent factors that can be used for representing or approximating the interactions between users and items from the historical rating logs . specifically , given an @xmath0 rating matrix @xmath1 with @xmath2 users and @xmath3 items , the mf approach considers the following optimization problem : @xmath4 where @xmath5 and @xmath6 are the latent matrices which record the latent factors of users and items respectively .",
    "also , @xmath7 is the @xmath8 column of @xmath5 , @xmath9 is the @xmath10 column of @xmath6 , and @xmath11 is an indicator function that is equal to @xmath12 if user @xmath13 rated item @xmath14 and equal to @xmath15 otherwise .",
    "the vector @xmath7 , called the user latent vector of user @xmath13 , is commonly used for representing ( latent ) user preferences , and the vector @xmath9 , called the user latent vector of item @xmath14 , is commonly used for representing ( latent ) item characteristics .",
    "the regularization terms are added in the optimization problem to prevent overfitting .",
    "we can also view mf from a probabilistic perspective .",
    "probabilistic matrix factorization ( pmf ) @xcite defines the following conditional distribution over the observed ratings based on the linear model with gaussian observation noise : @xmath16^{i_{ij}},\\ ] ] where @xmath17 denotes the probability density function of the gaussian distribution with mean @xmath18 and variance @xmath19 . with placing zero - mean spherical gaussian priors on the latent factors ,",
    "the problem of maximizing the log - posterior with the fixed variance is equivalent to the optimization problem in .",
    "note that both @xmath20 and @xmath21 are unknowns in and the objective function is not _ convex _ @xcite .",
    "simon funk @xcite popularized a stochastic gradient descent ( sgd ) algorithm which loops through all ratings in the training set to find the latent matrices @xmath5 and @xmath6 .",
    "for each training example @xmath22 , one first computes the associated prediction error @xmath23 one then updates @xmath20 and @xmath21 in the opposite direction of the gradient as follows : @xmath24 where @xmath25 is the learning rate and @xmath26 is the regulator parameter .",
    "this incremental and iterative approach provides a practical way to scale the mf method to large datasets .      in order to provide recommendations that fit users present preferences , time - dependent collaborative filtering ( cf ) @xcite employs the availability of temporal information ( time stamps ) associated with user - item rating logs to put more emphasis on the recent ratings .",
    "such an approach is based on the plausible assumption that recent logs have bigger influence on future events than old and obsolete logs .",
    "there are many prior works on time - dependent collaborative filtering , including neighborhood - based cf @xcite , social influence analysis @xcite , temporal bipartite projection @xcite and timesvd++ @xcite . among all these prior works ,",
    "@xcite is perhaps the most related work to mf . in @xcite , koren proposed adding a time - varying rating bias for each user and each item to the estimate from the original mf .",
    "as such , the temporal dynamics of user latent vectors are only modelled by a simple sum of three factors , the stationary portion , a possible gradual change with linear equation of a deviation function , and a day - specific parameter for sudden drift .",
    "even so , it was reported in @xcite that timesvd++ significantly outperforms svd and svd++ @xcite ( that considered implicit feedback ) .",
    "the idea of using a time - decaying function in instance weighting or time window in instance selection @xcite are presented in lots of prior work such as neighborhood - based cf @xcite and social influence analysis @xcite .",
    "one of the earliest research presented by @xcite suggested using the time weighting scheme to incorporate both user - dependent and item - dependent decay functions to adjust the similarities to previously rated items .",
    "lathia et al .",
    "@xcite formalize cf as a time - dependent iterative prediction problem and therefore proposed a temporally adaptive neighborhood - based method that automatically assigns and updates the neighborhood sizes for every user instead of setting global parameters .",
    "for the temporal effect of social influence , it was proposed in @xcite to learn the time - variant influence probabilities between users in the continuous time model with exponential decay of time .",
    "moreover , plovics et al .",
    "@xcite trained a blend of matrix factorization methods that incorporates the temporal influence as a slowly decreasing logarithmic function . incorporating the graph - based approach , wu et al .",
    "@xcite proposed temporal bipartite projection method to characterize the transition tendencies among items which can be viewed as a social aggregated behavior . by taking the temporal information into account and dealing with the huge amount of data in a temporal sequence of bipartite networks",
    ", this approach provides better results of the link prediction problem for new links .    instead of using a time - decaying function for rating logs , koren @xcite proposed timesvd++ that adds a time - varying rating bias for each user and each item to the estimate from the original mf . in timesvd++",
    ", the temporal dynamics of user latent vectors are only modelled by a simple sum of three factors , the stationary portion , a possible gradual change with linear equation of a deviation function , and a day - specific parameter for sudden drift .",
    "even so , it was reported in @xcite that timesvd++ significantly outperforms svd and svd++ @xcite ( that considered implicit feedback ) .",
    "furthermore , a rich bias model was used in @xcite for modeling the different temporal dynamics in the yahoo ! music dataset . one key difference between timesvd++ and the approach of using a time - decaying function",
    "is the time period of rating logs that are used for modeling the temporal dynamics : the former only takes the temporal effect in a specific time window while the latter considers the whole life span of all data .",
    "although these methods improve the accuracy of the prediction compared to the baseline mf estimator , there are some difficulties in the time - dependent cf approach .",
    "the system model in timesvd++ for the user latent vectors is too simple to have any structural characterizations or constraints on their parameters .",
    "as such , these parameters ( in various aspects and time steps ) have to be learned individually and need lots of efforts on fine tuning . thus , timesvd++ maybe too data - specific to be used as a general model . also , the assumption that claims recent ratings are always more important than old data may be oversimplified .",
    "tensor factorization ( tf ) extends mf into a three - dimen - sional tensor by incorporating the temporal features into the prediction model .",
    "the underlying physical meaning of tf is that the given ratings not only depend on the user preferences and the item characteristics but also the current trend .",
    "there are two kinds of popular tensor factorization models in cf @xcite : the candecomp / parafac ( cp ) model that decomposes the tensor into same rank of latent factors , and the tucker model that considers the problem as the higher - order pca .",
    "there are some works that adopt the tf model for exploiting temporal information associated with user - item interactions .",
    "the bayesian probabilistic tensor factorization ( bptf ) @xcite extended pmf to candecomp / parafac tensor factorization that models each rating as the inner product of the latent factors of user , item , and time slice as well .",
    "it also imposes constraints that the adjacent time slices should share similar latent factors .",
    "the advantage of bptf is its almost parameter - free probabilistic tensor factorization algorithm with a fully bayesian treatment derivation while the drawback is it is not sensitive enough to capture the local changes of preferences compared with timesvd++ . recently , rafailidis and nanopoulos @xcite modeled continuous user - item interactions over time and defined a new measure of user preference dynamics to capture the shifting rate for each user . in a broader sense",
    ", recommendation can be regarded as a bipartite link prediction problem that aims to infer new interactions between users and items which are likely to occur in the near future .",
    "based on this idea , dunlavy et al .",
    "@xcite considered bipartite graphs that evolve over time and demonstrated that tensor - based methods are effective for temporal data with varying periodic patterns . apart from incorporating the temporal information ,",
    "tensor factorization is a popular approach to integrate further information such as context of implicit feedback in content - based recommender systems .",
    "for instance , moghaddam et al .",
    "@xcite added _ review _ as the third dimension based on the tucker tensor model to address the problem of personalized review quality prediction and shi et al .",
    "@xcite directly trained the tensor model for creating an optimally ranked list of items for individual users in the context - aware recommender systems .",
    "tensor factorization provides a principled and well - struc - tured approach to incorporate the temporal dynamics in recommender systems ; however , the structure also limits the flexibility of the model so that it is hard to process and solve the decomposition especially for a large - scale and sparse tensor",
    ". given the same amount of rating data , the higher order the tensor model is , the more severe the sparsity problem is .",
    "the sparsity problem leads to time - consuming computing , high space complexity and the convergence issues in the decomposition procedure .      inspired by the success of pmf that places gaussian priors on the latent factors and formulates the matrix factorization problem as an optimization problem for obtaining the maximum - a - posteriori ( map ) estimate ,",
    "there are some recent works that compute the map optimally by using the kalman filter @xcite . considering the observed measurements over time with noise and uncertainties , the kalman filter is the optimal linear estimator of unknown variables .",
    "its recursive structure also allows new measurements to be processed as they arrive .",
    "the kalman filter can be conceptualized in two phases : the _ predict _ phase is called a priori estimate which produces an estimation without the observation at the current time step , and the _ update _ phase is known as a posteriori estimate which refines the estimation with the current observation .",
    "the refinements of the state and covariance estimates are based on the optimal kalman gain computed at every time step .",
    "the paper @xcite by lu et al . might be the first paper to use the kalman filter in recommender systems . in that paper",
    ", they exploited the kalman filter to model the change of user preferences in its temporal component .",
    "though they provided a new perspective to the recommender systems , their approach is still not general enough as the transition matrix used in the kalman filter was only modeled by an _ identity _ matrix . as such , one can only capture the drifts of user preferences .",
    "in another recent paper @xcite , gultekin and paisley proposed the collaborative kalman filter ( ckf ) approach that used a geometric brownian motion to model the dynamically evolving drift of each latent factor .",
    "the dynamic state space model proposed by @xcite is most related to our work . to solve the system identification problem for the linear system in the kalman filter",
    ", they develop an em algorithm that performs the kalman filter and the rts smoother .",
    "the em algorithm is an iterative two - pass algorithm that yields estimates for the model parameters by using all observations in the expectation step , and then refines the estimates of the model parameters in the maximization step .",
    "although the model is comprehensive and provides better results compared to the svd and timesvd++ approaches , there are some limitations in practice : ( i ) it makes a very strong assumption that assumes the transition matrix is homogeneous for all users .",
    "such a homogeneous assumption is needed to simplify the model ( as otherwise it is very difficult , if not impossible , to determine all their parameters form the em algorithm @xcite ) , and ( ii ) it is not suitable for large datasets due to the tractability and runtime performance .    in this paper",
    ", we will remove the assumption that the transition matrix is homogeneous for all users . by doing so",
    ", we allow our system to track concept drift in each individual user latent vector .",
    "our experiments further verify that users do have different transition matrices .",
    "some of them are simply governed by the identity matrix and have no concept drift in their latent vectors .",
    "on the other hand , some of them have significant changes of their latent vectors and the improvement of the rating for those users is the key factor for lowering rmse in our temporal approach .",
    "in this paper , we study the rating prediction problem with time stamped logs . specifically , there are @xmath2 users , indexed from @xmath27 , @xmath2 , and @xmath3 items , indexed from @xmath28 . for these users and items ,",
    "we are given a set of time stamped logs , where each log is represented by the four tuple : @xmath29 we assume that every rating is a real - valued number and each item can be rated by a user at most once . if we neglect the time stamps of these logs , then the ratings of these logs can be represented by an @xmath0 matrix @xmath30 , where @xmath22 is the rating of user @xmath13 on item @xmath14 if item @xmath14 has been rated by user @xmath13 . on the other hand ,",
    "if item @xmath14 has not been rated by user @xmath13 , then @xmath22 is said to be _",
    "missing_. in practice , the matrix @xmath31 is a _ sparse _ matrix and there are many missing values .",
    "the rating prediction problem is then to predict the missing values in the matrix @xmath31 .    to evaluate the performance of a rating prediction algorithm ,",
    "the rating logs are partitioned into two sets : the training set and the testing set .",
    "the training set is given to a rating prediction algorithm to `` learn '' the needed parameters for rating prediction .",
    "on the other hand , the testing set is not revealed to a rating prediction algorithm and is only available for testing the accuracy of a rating prediction algorithm .",
    "though there are many metrics for evaluating the performance of rating prediction algorithms , in this paper we adopt the root mean square error ( rmse ) that can be computed as follows : @xmath32 where @xmath33 is the prediction for @xmath22 via a rating prediction algorithm .",
    "the rmse has been widely used in the literature , including the competition for the netflix prize .",
    "although the range of rmse might be quite small , there is evidence ( see e.g. , @xcite ) that small improvement in rmse can have a significant impact on the quality of the top few recommendations from a rating prediction algorithm .",
    "the original mf does not use the information of time stamps and simply decomposes the matrix @xmath31 approximately into a product of two matrices : the user latent matrix and the item latent matrix . though the item latent matrix could be quite stationary with respect to time , it is a general believe ( see e.g. , @xcite ) there might be concept drift in the user latent matrix as users tend to change their mind over time . in view of this , our aim is to develop a temporal dynamic model for tracking concept drift in each individual user latent vector by using the time stamps of rating logs . by doing so , we can effectively and efficiently achieves a lower rmse than that of mf .",
    "for the rating prediction problem described in the previous section , we propose a temporal matrix factorization ( tmf ) approach that is capable of tracking concept drift in each individual user latent vector .",
    "our approach is based on the following assumptions that were previously used in the literature ( see e.g. , @xcite ) :    ( i ) : :    like the original mf , there are a user latent matrix    @xmath34 and an item latent matrix    @xmath35 that can be used for    approximating the rating matrix @xmath31 . the    @xmath8 column of the user latent matrix @xmath5 ,    denoted by @xmath7 , is called the user latent vector of user    @xmath13 , @xmath36 , that can be viewed as    the preferences of user @xmath13 for the @xmath37 latent    factors . similarly , the @xmath10 column vector of the item    latent matrix @xmath6 , denoted by @xmath9 , is called    the item latent vector of item @xmath14 ,    @xmath28 . the rating for user @xmath13 on    item @xmath14",
    "is then predicted by the inner product of    @xmath7 and @xmath9 .",
    "( ii ) : :    there is concept drift in each individual user latent vector as people    might change their preferences over time . for this",
    ", we denote by    @xmath38 the user latent matrix at time @xmath39 , and    @xmath40 the user latent vector of user @xmath13 at    time @xmath39 , @xmath36 .",
    "( iii ) : :    as the characteristics of items are stationary , we assume that the    item latent matrix @xmath6 is invariant with respect to time .    in view of these assumptions , the key ingredient of our tmf approach is to use the training data set to capture the dynamics of the concept drift in each individual user latent vector . for this",
    ", our approach consists of the following steps :    ( i ) : :    use the rating logs in the training data set to construct a time    series of @xmath0 rating matrices ,    @xmath41 .",
    "( ii ) : :    use the time series of rating matrices    @xmath42 , @xmath43 to learn a    time series of @xmath44 user latent vectors ,    @xmath45 ,    @xmath36 .",
    "( iii ) : :    for each user @xmath13 , use the time series of user latent    vectors , @xmath45 to learn the    dynamics of the concept drift in the user latent vector .",
    "( iv ) : :    use the dynamics of the concept drift in each individual user latent    vector to predict the user latent vector at time @xmath46 ,    i.e. , @xmath47 .",
    "then use the product of @xmath47    and the item latent matrix @xmath6 to predict the missing    values in the testing data set .      the simplest way to construct a time series of rating matrices",
    "@xmath41 is to partition rating logs into equally spaced time slices according to their time stamps .",
    "but , as the original rating matrix in a real data set might have already been very sparse , further partitioning of the rating logs might yield a time series of extremely sparse rating matrices that might not have any statistical significance at all . in view of the sparsity problem",
    ", the number of time slices @xmath46 can not be too large . to further mitigate the sparsity problem",
    ", one can consider a sliding window approach that merges the rating logs in several consecutive time slices into a single step . by doing so , there are _ overlapping _ rating logs in such a time series of rating matrices .",
    "such an approach can not only mitigate the sparsity problem but also ensure smooth change of rating matrices so that prediction could be possible .      to learn a time series of user latent vectors for each user , we first perform mf for the rating matrix @xmath31 to obtain the user latent matrix @xmath5 and the item latent matrix @xmath6 . as we assume that the item latent matrix @xmath6 is invariant with respect to time",
    ", one might expect that @xmath48 where @xmath49 is the rating vector for user @xmath13 on the @xmath3 items ( that can be extracted from the rating matrix @xmath50 ) . in view of this , a nive way to learn a time series of @xmath44 user latent vectors , @xmath45 , is to simply compute the moore - penrose pseudoinverse of @xmath51 from ( [ eq:4.1 ] ) .",
    "it is well - known that the moore - penrose pseudo inverse computes a `` best fit , '' i.e. , the least squared solution to a system of linear equations and its uniqueness follows from the svd theorem in matrix algebra .",
    "such an approach works fine if all the entries in the vector @xmath49 are known . in reality , there are many missing values in the vector @xmath49 and thus make a direct computation of the moore - penrose pseudo inverse infeasible .",
    "one way to remedy this is to pad the missing values in @xmath49 with the predicted values of user @xmath13 by mf , i.e. , @xmath52 . in particular",
    ", one can generate another vector @xmath53 as a linear combination of these two vectors , i.e. , @xmath54 if @xmath55 is small , the padded values in @xmath53 are all from the vector @xmath52 . as such , the vectors @xmath56 ,",
    "are all very similar and the corresponding moore - penrose pseudo inverse vectors also very similar . as a result",
    ", there is basically no change of the user latent vectors and that defeats the purpose of tracking the dynamics of user latent vectors . on the other hand ,",
    "if @xmath55 is large , then we basically ignore all the missing values in @xmath49 and that causes great fluctuation of the user latent vectors which makes it extremely difficult to track the dynamics of user latent vectors . also , as there are many missing values in @xmath49 , it is not clear whether the user latent vectors obtained this way possess any statistical significance .    the key insight to tackle",
    "this problem is that the user preferences at a specific time step are not only related to the ratings during that specific time step but also related to his / her overall behavior . in view of this",
    ", we first set @xmath57 as the original user latent vector @xmath20 .",
    "then we use the observed ratings during that time step to `` learn '' @xmath40 .",
    "specifically , we propose the following modified stochastic gradient descent method : @xmath58.\\label{eq:4.5}\\end{aligned}\\ ] ] unlike the standard stochastic gradient descent method for mf in ( [ eq:2.3])([eq:2.3b ] ) , here we only update the latent vector @xmath57 for every rating provided by user @xmath13 at time @xmath39 ( as the item matrix @xmath6 is stationary ) . by doing so",
    ", the user latent vector @xmath40 only updates his / her preferences for those items rated during time @xmath39 and thus retains his / her overall behavior for those items not rated during time @xmath39 .",
    "such an approach not only overcomes the obstacle of data sparsity but also possesses meaningful user preferences in the temporal setting .      to track concept drift in the user latent vector",
    ", we need to identify a system model for the dynamics of the time series of the user latent vectors .",
    "such a problem is known as the _ system identification _ problem in the literature @xcite .",
    "one of the most commonly used models for system identification problems is the linear system model . as such , we consider the linear system model for the latent vector of each user .",
    "specifically , we consider @xmath40 as the state vector at time @xmath39 and model the evolution of the state vector by @xmath59 where @xmath60 is a @xmath61 matrix and @xmath62 is a @xmath44 vector .",
    "the matrix @xmath60 is called the _ transition _",
    "matrix for user @xmath13 and @xmath62 is called the _ bias _ vector of user @xmath13 .",
    "it seems plausible to assume that the user latent vectors do not vary a lot in each time step . as such",
    ", we replace the transition matrix @xmath63 by @xmath64 in ( [ eq:4.2 ] ) .",
    "this then leads to @xmath65 where @xmath66 by doing so , we expect that the matrix @xmath67 is sparse and it only contains a small number of nonzero entries .",
    "it is known @xcite that the lasso regression provides parameter shrinkage and variable selection that limit the number of nonzero elements in the parameters . as such",
    ", we apply the lasso regression to estimate @xmath68 and @xmath62 in ( [ eq:4.10 ] ) from the @xmath69 `` observations '' of the output @xmath70 with the input @xmath71 .",
    "specifically , for each factor @xmath72 , @xmath73 , we let @xmath74 be the @xmath75 row of @xmath60 , @xmath76 be the @xmath75 element in @xmath62 , @xmath77 be the @xmath75 element in @xmath78 and consider the following optimization problem : @xmath79 where @xmath80 is the @xmath81-norm of the vector @xmath82 and @xmath26 is a nonnegative regulator parameter for the lasso regression . as @xmath26 increases , the number of nonzero elements in the vector @xmath83 decreases . in our experiments",
    ", we will use the matlab tool @xcite to solve the above lasso regression .    to solve the system identification problem for each user ,",
    "we define two matrices @xmath84 and @xmath85 as follows : @xmath86 , \\label{eq:4.6}\\\\",
    "y_{i}=\\left [ p_{i}^{t}(2 ) \\ ; p_{i}^{t}(3)\\cdots p_{i}^{t}(t-1)\\right ] . \\label{eq:4.7}\\end{aligned}\\ ] ] in view of ( [ eq:4.2 ] ) , we then have @xmath87 where @xmath88 is the matrix with the vector @xmath89 in every column .",
    "it seems plausible to assume that the user latent vectors do not varies a lot in each time step . as such",
    ", we replace the transition matrix @xmath63 by @xmath64 in ( [ eq:4.8 ] ) .",
    "this then leads to @xmath90 where @xmath91 by doing so , we expect the matrix @xmath67 to be sparse and it only contains a small number of nonzero entries . it is known @xcite that the lasso regression provides parameter shrinkage and variable selection that limit the number of nonzero elements in the parameters . as such , we formulate the following optimization problem : @xmath92 where we add the regularization term to penalize the large frobenius norm of matrix @xmath67 to prevent overfitting",
    ". then we solve this matrix minimization problem _ separately _ for each factor @xmath72 , @xmath73 , by using the lasso regression to solve the corresponding @xmath81-regularized least - squares problem for each row in @xmath67 and @xmath93 .",
    "therefore , the problem is converted to the following optimization problem to find @xmath83 and @xmath94 for row @xmath95 @xmath96 where @xmath97 is the @xmath75 row of the matrix @xmath98 .",
    "once we obtain the transition matrix @xmath63 and the bias vector @xmath62 , we can use the system dynamic in ( [ eq:4.2 ] ) to predict the latent vector of user @xmath13 at time @xmath46 by the following equation : @xmath99 as in the original mf , the missing values in the testing data set are then predicted by using the product of the user latent vector and the item latent matrix , i.e. , @xmath100",
    "in this section , we perform various experiments to evaluate the performance and efficiency of our temporal method via a synthetic dataset and four real datasets . all our experiments are implemented in matlab and executed on a server equipped with an intel core i7 ( 4.2ghz ) processor and 64 g memory on the linux system .",
    "we first conduct our experiments on a synthetic data .",
    "the main reason of doing this is to test our method in a _",
    "controllable _ environment so that we can gain insights of the effects of various parameters and thus better understand when our method could be effective .    to generate the synthetic data , we set @xmath101 and @xmath102 , i.e. , there 10,000 users and 10,000 items .",
    "the density of the rating matrix @xmath31 is set to 1% , and that gives 1,000,000 ratings .",
    "we generate all the entries in both the initial user latent matrix @xmath103 and the item latent matrix @xmath6 by uniformly distributed random variables over @xmath104 . to model the evolution of the user latent vector for each user @xmath13 , the transition matrix @xmath63 is generated by the sum of the identity matrix and a random matrix @xmath105 with all its entries generated from a uniform distribution .",
    "the entries in the bias vector @xmath89 are also generated from a uniform distribution .",
    "various ranges of the entries in @xmath105 and @xmath62 are specified in our experiments ( see table [ table : table1 ] ) .",
    "the number of steps @xmath46 is set to 10 and the rating logs are then generated according to equations and .    in table [ table : table1 ] , we report the rmse for both the original mf ( implemented by the libmf library @xcite ) and our method for various parameter settings . the parameters that we choose for mf are the learning rate @xmath25= 0.01 , the regulator parameter @xmath106 , and the number of factors @xmath107 .",
    "we run 50 iterations of sgd to obtain the latent factors in the original mf .",
    "the learning rate @xmath25 and the regulator parameter @xmath26 in equation for computing the user latent vector at every time step in our method are set to be the same as those for mf .",
    "as can be seen from this table , our method consistently and significantly outperforms mf in all the parameter settings .",
    "the improvement depends on the transition matrix and the bias vector that are selected to control the concept drift in the user latent vector .",
    "a more careful examination reveals that the improvement for our method is relatively small if the range of the entries in the random matrix @xmath108 is small , e.g. , @xmath109 .",
    "this is because the transition matrix in such a scenario is very close to the identity matrix and there is almost no change of the user latent vectors . as such",
    ", mf performs well and yields a low rmse . on the other hand ,",
    "if such a range is large , the prediction accuracy of mf is low as mf relies on the assumption of stationary user preferences .",
    "as our method is capable of tracking the concept drift in the user latent vector , our method achieves roughly 17 - 26% improvement in terms of rmse .",
    "next , we study the effect of the range of @xmath89 . in table",
    "[ table : table1 ] , we consider two different ranges of @xmath62 .",
    "the experimental results show that the values of rmse are larger when the given range is larger ( under the condition of using the same transition matrix ) .",
    "moreover , the performance gain from our method is also larger .",
    "this shows the importance of adding the bias vectors in our linear system model .",
    ".the rmse results on the synthetic dataset for various parameter settings . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     [ table : table9 ]",
    "after investigating the overall performance expressed in rmse metric , the evolution of user preferences and the rating prediction results in previous section , we try to discuss the characteristics of transition matrices which play a significant role in this work . with the help of regularization term in lasso regression ,",
    "the values in transition matrices are small enough to capture the evolution of user preferences and prevent the transition matrix from overfitting at the mean while .",
    "specifically , the transition matrix is a matrix with lots of zero entries in a result of parameter shrinkage and variable selection provided by lasso regression ; moreover , the amount of these elements is related to the value of regulator @xmath26 in equation , which controls the magnitude of elements in transition matrix . during the experimental procedure , we observe that we can obtain the better rmse result with @xmath110 in synthetic dataset but it should be set as @xmath111 in real datasets .",
    "this is also an evidence that the temporal effect in real datasets is not as distinct as in synthetic dataset .    besides the ones on the main diagonal that retain the original information , a common property of the transition matrices is the non - zero values are usually centralized in several columns and the patterns are different for every user in the synthetic dataset .",
    "the possible underlying physical meaning is that we find the representative dimensions which are more important in capturing the user preferences than other dimension of factors . in the real datasets , we observed two types of transition matrices learned in our experiment .",
    "the majority type is the identity matrix which expresses the preferences of most users are relatively static .",
    "the second type has non - zero entries outside the main diagonal which represents the preference shifts among latent factors . slightly different from the results in synthetic one , the non - zero entries are usually in the first column .",
    "it seems that the user preferences in these real datasets are not as complex as our expectation .",
    "although mf is the state - of - the - art approach with high accuracy , robustness and scalability , it is suffered from the cold start problem that the system can not provide useful recommendations due to insufficient information about the inactive users and items@xcite . here",
    "we try to filter out these  cold users \" and  cold items \" as the traditional settings in collaborative filtering and see the effect of overall rmse performance .",
    "we only conduct this experiment on epinions and flixster because the ratings are largely reduced after the filtering process in ciao dataset and the publicly released movielens dataset has already processed to retain users who rated at least 20 movies .",
    "we observe an interesting phenomena that the improvement obtained by temporal method is reduced from 0.87% to 0.07% in epinions and from 0.78% to 0.18% in flixster .",
    "the possible reason is that filtering out cold users and cold items loses too much information especially in this sparse data environment .",
    "these information are usually neglected by traditional collaborative filtering approach but are useful in analyzing the temporal dynamics for user preferences . it shows the potential of temporal method to alleviate the cold start problem in the future .",
    "in this paper , we proposed a temporal matrix factorization approach ( tmf ) for tracking concept drift in each individual user latent vector .",
    "there are two key innovative steps in our approach : ( i ) a modified stochastic gradient descent method to learn an individual user latent vector at each time step , and ( ii ) a linear model for the transition of the individual user latent vectors by the lasso regression . in comparison with the other approaches that intend to incorporate temporal dynamics with mf in the literature ,",
    "there are several distinctive features of our temporal method : ( i ) in comparison with timesvd++ @xcite , our systematic approach is more structured and does not require fine tuning a lot of unstructured parameters .",
    "\\(ii ) our modified stochastic gradient descent method is able to alleviate the data sparsity problem for learning the user preferences at a certain time step .",
    "this overcomes the data sparsity problem in tensor factorization .",
    "\\(iii ) unlike the ckf approach @xcite , we do not need to assume the transition matrix is _",
    "homogeneous_. thus , we are allowed to track concept drift in each individual user latent vector . in comparison with the original mf ,",
    "our temporal method is able to achieve lower root mean square errors ( rmse ) for both the synthetic and real datasets .",
    "one interesting finding is that the performance gain in rmse is mostly from those users who indeed have concept drift in their user latent vectors at the time of prediction . as our temporal method",
    "is specifically designed for each user , one can save a lot of efforts by only tracking those users who indeed have concept drift in their user latent vectors at the time of prediction .",
    "however , identifying those users is not an easy task and might require further study .",
    "one possible approach for this is to examine the transition matrix for each user . in our experiments",
    ", we found that there are many users whose transition matrices are the identity matrix and those users are not worth tracking .",
    "another research direction is to study the effect of cold start users ( who have very few ratings ) .",
    "one might think cold start users are difficult to predict and then immediately filter out their ratings in the preprocessing step .",
    "however , in our temporal method , the ratings of cold start users might be valuable as they contribute to the item latent matrix @xmath6 which in turn affects the accuracy of estimating the time series of the latent vectors of other users .    in this paper , we study the rating prediction problem which is the core in recommender systems .",
    "we proposed a principled and general mf - based approach to incorporate the temporal dynamics into recommendation . by adequately utilizing stochastic gradient descent method to capture the evolution of user preferences and learning the transition patterns at individual level with lasso regression ,",
    "our temporal dynamic model is able to achieve more accurate predictions compared to the mf approach by 20% on the synthetic dataset and 5% on ciao dataset .",
    "besides , we discuss the characteristics of evolution which are the main reason for extent of advancement on the different real datasets . in future work , we can incorporate additional information such as social network analysis or content - based features into the temporal dynamic model based on the well - structured mf - based framework we developed .",
    "moreover , this comprehensive model with a variety of information is more capable of addressing the cold start problem which is another aspect of future research direction .",
    "chin , y.  zhuang , y .- c .",
    "juan , and c .- j .",
    "lin . a learning - rate schedule for stochastic gradient methods to matrix factorization .",
    "in _ advances in knowledge discovery and data mining _ , pages 442455 .",
    "springer , 2015 .",
    "a.  goyal , f.  bonchi , and l.  v. lakshmanan .",
    "learning influence probabilities in social networks . in _ proceedings of the third acm international conference on web search and data mining _",
    ", pages 241250 .",
    "acm , 2010 .",
    "y.  koren .",
    "factorization meets the neighborhood : a multifaceted collaborative filtering model . in _ proceedings of the 14th acm",
    "sigkdd international conference on knowledge discovery and data mining _ ,",
    "pages 426434 .",
    "acm , 2008 .",
    "n.  lathia , s.  hailes , and l.  capra .",
    "temporal collaborative filtering with adaptive neighbourhoods . in _ proceedings of the 32nd international acm sigir conference on research and development in information retrieval _ , pages 796797 .",
    "acm , 2009 .",
    "j.  j. mcauley and j.  leskovec . from amateurs to connoisseurs",
    ": modeling the evolution of user expertise through online reviews . in _ proceedings of the 22nd international conference on world wide web _ , pages 897908 .",
    "international world wide web conferences steering committee , 2013 .",
    "s.  moghaddam , m.  jamali , and m.  ester .",
    "etf : extended tensor factorization model for personalizing prediction of review helpfulness . in _ proceedings of the fifth acm international conference on web search and data mining _ , pages 163172 .",
    "acm , 2012 .",
    "r.  plovics , a.  a. benczr , l.  kocsis , t.  kiss , and e.  frig .",
    "exploiting temporal influence in online recommendation . in _ proceedings of the 8th acm conference on recommender systems _ ,",
    "pages 273280 .",
    "acm , 2014 .",
    "y.  shi , a.  karatzoglou , l.  baltrunas , m.  larson , a.  hanjalic , and n.  oliver .",
    "tfmap : optimizing map for top - n context - aware recommendation . in _ proceedings of the 35th international acm sigir conference on research and development in information retrieval _ , pages 155164 .",
    "acm , 2012 .",
    "j.  z. sun , k.  r. varshney , and k.  subbian .",
    "dynamic matrix factorization : a state space approach . in _ acoustics ,",
    "speech and signal processing ( icassp ) , 2012 ieee international conference on _ , pages 18971900 .",
    "ieee , 2012 .",
    "t.  wu , s .- h .",
    "yu , w.  liao , and c .- s .",
    "temporal bipartite projection and link prediction for online social networks . in _ big data ( big data ) , 2014 ieee international conference on _ , pages 5259 .",
    "ieee , 2014 ."
  ],
  "abstract_text": [
    "<S> the matrix factorization ( mf ) technique has been widely adopted for solving the rating prediction problem in recommender systems . the mf technique utilizes the latent factor model to obtain _ </S>",
    "<S> static _ user preferences ( user latent vectors ) and item characteristics ( item latent vectors ) based on historical rating data . however , in the real world user preferences are not static but full of dynamics . </S>",
    "<S> though there are several previous works that addressed this time varying issue of user preferences , it seems ( to the best of our knowledge ) that none of them is specifically designed for tracking concept drift in _ individual user preferences_. motivated by this , we develop a temporal matrix factorization approach ( tmf ) for tracking concept drift in each individual user latent vector . </S>",
    "<S> there are two key innovative steps in our approach : ( i ) we develop a modified stochastic gradient descent method to learn an individual user latent vector at each time step , and ( ii ) by the lasso regression we learn a linear model for the transition of the individual user latent vectors . </S>",
    "<S> we test our method on a synthetic dataset and several real datasets . in comparison with the original mf , </S>",
    "<S> our experimental results show that our temporal method is able to achieve lower root mean square errors ( rmse ) for both the synthetic and real datasets . one interesting finding is that the performance gain in rmse is mostly from those users who indeed have concept drift in their user latent vectors at the time of prediction . </S>",
    "<S> in particular , for the synthetic dataset and the ciao dataset , there are quite a few users with that property and the performance gains for these two datasets are roughly 20% and 5% , respectively . </S>"
  ]
}