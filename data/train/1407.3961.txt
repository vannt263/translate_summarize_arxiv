{
  "article_text": [
    "the density - based minimum divergence approach , which includes both @xmath1 type ( csisar , 1963 ) and bregman ( bregman , 1967 ) divergences , has long history .",
    "a prominent member of the class of density - based divergences is the pearson s @xmath1 ( pearson , 1900 ) which started its journey from the very early days of formal research in statistics . from the robustness perspective , however , beran s 1977 work is the first useful reference in the literature of density - based minimum divergence inference . in the present paper we focus on a new subclass of density based divergences which encompasses some variants of the power divergence measure of cressie and read ( 1984 ) and the density power divergence of basu et al .",
    "( 1998 ) and discuss possible applications in statistical inference . among many other things ,",
    "our analysis highlights the limitation of the first order influence function analysis as an indicator of the robustness of these procedures .",
    "[ [ section ] ]    in this article our primary aim is to describe some statistical uses of the proposed superfamily of divergences . to keep this focus clear , we will push most of the technical details including the proofs of the asymptotic distribution to a separate article , and",
    "will simply state the relevant theoretical results appropriately in the present context .",
    "the asymptotic results will be presented in maji , ghosh and basu ( 2014 ) .",
    "[ [ section-1 ] ]    the rest of the paper is organized as follows .",
    "given two densities @xmath2 and @xmath3 , the measure @xmath25 represents a genuine statistical divergence for all @xmath26 and @xmath27 .",
    "a simple application of holder s inequality establishes the above result .",
    "consider a parametric class of model densities @xmath28 and suppose that our interest is in estimating @xmath29 .",
    "let @xmath30 denote the distribution function corresponding to the true density @xmath2 .",
    "the minimum lsd functional @xmath31 at @xmath30 is defined through the relation @xmath32 a simple differentiation gives us the estimating equation for @xmath29 , which is @xmath33 for @xmath34 , the equation becomes the same as the estimating equation of the logarithmic power divergence family with parameter @xmath5 . for @xmath35 , on the other hand , it is the estimating equation for the ldpd measure .",
    "it takes the value @xmath29 when the true density @xmath36 is in the model ; when it does not , @xmath37 represents the best fitting parameter , and @xmath38 is the model element closest to @xmath2 in terms of logarithmic super divergence . for simplicity in the notation , we suppress the scripts and refer to @xmath39 as simply @xmath29 when there is no scope for confusion .",
    "the influence function is one of the most important heuristic tools in robust inference .",
    "consider the minimum lsd functional @xmath31 .",
    "the value @xmath40 solves the equation ( [ eq : lsd_est_eqn ] ) .",
    "consider the estimating equation at the mixture contamination density @xmath41 where @xmath42 is the indicator function at @xmath43 .",
    "let @xmath44 be the corresponding functional which solves the estimating equation in this case .",
    "taking a derivative of both sides of this estimating equation and evaluating at @xmath45 , the influence function is found to be @xmath46}{j(\\theta)},\\ ] ] where @xmath40 , @xmath47 @xmath48 in the above @xmath49 , where @xmath50 represents the gradient with respect to @xmath29 .",
    "when the model holds , so that @xmath51 for some @xmath29 , the influence function becomes , @xmath52}\\right),\\end{aligned}\\ ] ] where @xmath53   \\left[\\int f_\\theta^{1+\\beta}~u_\\theta\\right]^t}\\right).\\end{aligned}\\ ] ] when @xmath21 , @xmath54 reduces to @xmath55 , the fisher information .",
    "the remarkable observation in ( [ eq : if_0 ] ) and ( [ eq : j_0_form ] ) is that the influence function at the model is independent of @xmath5 and depends only on @xmath4 . from figure",
    "[ fig : lsd_if_colour ] it is clear that the first order influence function is unbounded for @xmath21 whereas for other values of @xmath4 the function is bounded and redescending .",
    "we will demonstrate the limitations of this measure in our context in the subsequent sections .",
    "distribution at the model with @xmath56,width=491,height=377 ]",
    "under the parametric set - up of section [ sec : lsd_est_eqn ] , consider a discrete family of distributions .",
    "we will use the term `` density function ''",
    "generally for the sake of a unified notation , irrespective of whether the distribution is discrete or continuous .",
    "let @xmath57 be a random sample from the true distribution having density function @xmath2 and let the distribution have support @xmath58 .",
    "denote the relative frequency at @xmath59 from the data by @xmath60 .",
    "representing the logarithmic @xmath0-divergence in terms of the parameter @xmath4 and @xmath5 ( as given in section [ sec : lsd_defn ] ) , let @xmath61 be the estimator obtained by minimizing @xmath62 over @xmath63 , where @xmath64 is a suitable nonparametric density estimate of @xmath2 ; in the discrete case the vector of relative frequencies @xmath65 based on the sample data is the canonical choice for @xmath64 .",
    "[ [ section-2 ] ]    in this paper we will primarily describe the statistical applications of the minimum distance procedures that are generated by the logarithmic @xmath0-divergence . however , for the sake of completeness , we also present the asymptotic distribution of the estimators which has been separately established in maji , ghosh and basu ( 2014 ) .",
    "[ [ section-3 ] ]    when @xmath2 is replaced by @xmath65 , the estimating equation ( [ eq : lsd_est_eqn ] ) may be expressed as @xmath66 where @xmath67,\\ ] ] @xmath68 @xmath69 and @xmath70 .",
    "define , @xmath71   -   \\sum_x   m(\\delta_g^g(x ) )   f_{\\theta}^{1+\\beta}(x ) \\nabla w_{\\theta^g}(x ) \\nonumber \\\\   & &   - ( 1+\\beta ) \\sum_x",
    "m(\\delta_g^g(x ) )   f_{\\theta}^{1+\\beta}(x ) w_{\\theta^g}(x ) u_{\\theta^g}(x )   \\end{aligned}\\ ] ] and @xmath72.\\end{aligned}\\ ] ] note that the matrices @xmath73 in ( [ eq : j_functional_form ] ) and ( [ k_j_exp ] ) are identical . then , under standard regularity conditions ( see maji , ghosh and basu , 2014 ) , it follows that @xmath61 is consistent for @xmath74 and has the asymptotic distribution given by @xmath75 as @xmath73 and @xmath76 are as defined in ( [ k_j_exp ] ) and [ eq : k_form ] .",
    "see maji , ghosh and basu ( 2014 ) for the technical details of the proof .",
    "when the true distribution @xmath30 belongs to the model family , i.e. , @xmath77 for some @xmath63 , then @xmath78 has asymptotic distribution as @xmath79 , where @xmath80 \\nonumber \\\\ & = & \\sum_x \\{b(\\theta ) u_\\theta(x)-a(\\theta)\\ } u_\\theta^t(x ) f_\\theta^{1+\\beta}(x ) .",
    "% \\int u_\\theta(x)u_\\theta^t(x ) % f_\\theta^{1+\\beta}(x ) dx   \\\\",
    "k = k_\\beta(\\theta ) & = & v_g[w_\\theta(x ) f_\\theta^{\\beta}(x ) ] \\nonumber \\\\ & = & \\sum_x \\{b(\\theta ) u_\\theta(x)-a(\\theta)\\ }   \\{b(\\theta ) u_\\theta(x)-a(\\theta)\\}^t f_\\theta^{1 + 2\\beta}(x ) \\nonumber \\\\ & & - \\xi \\xi^t , % \\int u_\\theta(x)u_\\theta^t(x ) % f_\\theta^{1 + 2\\beta}(x ) dx - \\xi \\xi^t ,   \\\\       \\xi = \\xi_\\beta(\\theta ) & = & e_g[w_\\theta(x ) f_\\theta^{\\beta}(x ) ] = \\sum_x \\{b(\\theta ) u_\\theta(x)-a(\\theta)\\ }   f_\\theta^{1+\\beta}(x ) .",
    "% \\int u_\\theta(x ) % f_\\theta^{1+\\beta}(x)dx . \\\\\\nonumber\\end{aligned}\\ ] ]    note that , under model ( @xmath51 ) both @xmath81 and @xmath82 depend only on @xmath4 . thus , the asymptotic distribution of the minimum lsd estimators do not depend on the parameter @xmath5 .",
    "we consider a parametric family of densities @xmath83 as introduced earlier .",
    "suppose we are given a random sample @xmath84 of size @xmath85 from the population . based on this sample ,",
    "we want to test the hypothesis @xmath86 when the model is correctly specified and the null hypothesis is correct , @xmath87 is the data generating density .",
    "we consider the test statistics based on the lsd with parameter @xmath4 and @xmath5 defined by @xmath88 where @xmath89 has the form given in ( [ lsd_form ] ) .",
    "then the following theorem becomes useful in obtaining the critical values of the test statistics in ( [ eq : w_define ] ) .",
    "the asymptotic distribution of the test statistic @xmath90 , under the null hypothesis @xmath91 , coincides with the distribution of @xmath92 where @xmath93 are independent standard normal variables , @xmath94 are the nonzero eigenvalues of @xmath95 , with @xmath96 and @xmath97 as defined in ( [ k_j_exp ] ) and the matrix @xmath98 is defined as @xmath99    % = \\left ( ( 1+\\alpha ) \\int f_{\\theta_0}^{\\alpha -1 }   % \\frac{\\delta f_{\\theta_0}}{\\delta \\theta_i } \\frac{\\delta f_{\\theta_0 } } % { \\delta \\theta_j } \\right)_{i , j=1,\\ldots , p}\\ ] ] and @xmath100 here @xmath50 represents the gradient with respect to @xmath29 .    to see the robustness properties of the lsd based test , we study the influence function analysis of the test statistics as in hampel et al .",
    "( 1986 ) , ghosh and basu ( 2014 ) etc .",
    "we define the corresponding lsd based test functional ( lsdt ) for one sample simple hypothesis problem as described above as ( ignoring the sample size dependent multiplier ) @xmath101 where @xmath31 is the minimum lsd functional defined in section [ sec : if_mlsde ] .",
    "then , considering the contaminated distribution @xmath102 associated with @xmath103 , hampel s first - order influence function of the lsdt functional turns out to be zero at the null distribution @xmath104 .",
    "however , corresponding second order influence function of the lsdt functional at the null distribution has a non - zero form given by @xmath105 therefore the robustness of the lsdt functional depends directly on the robustness of the minimum lsd estimator used in constructing the test statistics .",
    "so , following the arguments of section [ sec : if_mlsde ] it follows that , the proposed test will have bounded influence function whenever @xmath106 implying its robustness and has unbounded influence function at @xmath21 implying the lack of robustness .",
    "figure [ fig : lsdt_if ] shows the second order influence function of the @xmath107 model at the simple null @xmath56 ; the equivalence with the corresponding influence function of the minimum lsd estimator presented in figure [ fig : lsd_if_colour ] is quite clear .    ,",
    "width=491 ]      again consider a parametric family of densities @xmath108 as above in one sample problem , but here we are given two random samples @xmath84 of size @xmath85 and @xmath109 of size @xmath110 from two populations having parameters @xmath111 and @xmath112 respectively and based on these two samples , we want to test for the homogeneity of the two samples , i.e. to test the hypothesis @xmath113 we will consider the estimator @xmath114 and @xmath115 of @xmath111 and @xmath112 respectively , obtained by minimizing the lsd having parameter @xmath116 and then as before , we consider the test statistic based on the lsd with parameter @xmath4 and @xmath5 as follows @xmath117 we present the asymptotic distribution of the test statistics + @xmath118 under @xmath119 in the following theorem .",
    "[ th_two ] the asymptotic distribution of the test statistic + @xmath120 , under the null hypothesis @xmath121 , coincides with the distribution of @xmath122 where @xmath93 are independent standard normal variables , @xmath123 are the nonzero eigenvalues of @xmath124 , with + @xmath125 , @xmath126 and @xmath127 as defined in previous section and @xmath128",
    "to explore the performance of the proposed minimum lsd estimators , we have done several simulation studies under the poisson model with sample size of @xmath129 .",
    "we simulate data from a poisson distribution with parameter @xmath130 and compute the empirical bias and the mse of the minimum lsd estimators of @xmath29 based on 1000 replications .",
    "the results obtained are reported in tables [ tab : bias_uncontaminated ] and [ tab : mse_uncontaminated ] respectively .",
    "clearly both the bias and mse are quite small for any @xmath131 combination ; however the mse increases slightly with @xmath4 .",
    "next to study the robustness properties of the minimum lsd estimators we repeat the above study , but introduce a contamination in the simulated samples by replacing @xmath132 of it by @xmath133 observations .",
    "the corresponding values of the empirical bias and mse , against the target value of @xmath134 , are presented in tables [ tab : bias_contaminated ] and [ tab : mse_contaminated ] respectively .",
    "note that , the minimum lsd estimators are seen to be robust for all @xmath135 $ ] if @xmath136 and for suitably large values of @xmath4 if @xmath137 .",
    "however , the estimators corresponding to small @xmath4 close to zero and @xmath137 .",
    "the numeral examples and simulation results presented in the previous section clearly shows that the robustness of minimum lsd estimators in terms of its bias and mse under data contamination depends on the parameter @xmath5 for smaller values of @xmath4 .",
    "however , according to the classical literature , its first order influence function suggests that ( see section [ sec : if_mlsde ] ) its robustness will be independent of the parameter @xmath5 for all values of @xmath4 .",
    "thus , the classical approach of robustness measure through the first order influence fails in the case of minimum divergence estimation with the logarithmic super divergence family .",
    "similar limitations of the first order influence functions was also observed by lindsay ( 1994 ) and ghosh et al .",
    "( 2013 ) for the case of power divergence family and the @xmath0-divergence family ; accordingly they have proposed some alternative measure of robustness . in this section ,",
    "we use some of those alternative measures to explain the robustness of the proposed minimum lsd estimators .",
    "the higher ( second ) order influence function analysis for studying the robustness of a minimum divergence estimators was used by lindsay ( 1994 ) for the case of pd family and recently by ghosh et al .",
    "( 2013 ) for the @xmath0-divergence family ; both the work have shown this approach to provide significantly improved prediction of the robustness of corresponding estimators . here , we present a similar analysis for the minimum lsd estimator .    for any functional @xmath138 , @xmath139 quantifies the amount of bias under contamination as a function of contamination proportion @xmath140 , which can be approximated using the first - order taylor expansion as @xmath141 .",
    "hence the first order influence function gives an approximation to the predicted bias up to first order .",
    "when this first order approximation fails , we can consider a second order ( approximate ) bias prediction by @xmath142 .",
    "the term @xmath143 is interpreted as the second order influence function and the ratio @xmath144\\epsilon}{2}\\ ] ] serve as a measure of adequacy of the first - order approximation and hence of the first order influence analysis ; the two approximation may differ significantly for fairly small values of @xmath140 when the first order approximation is inadequate .",
    "our next theorem present the expression of the second order approximation @xmath143 for the minimum lsd estimator with a scalar parameter ; this can be routinely extended to the case of vector parameter also .",
    "let us define , for the model family @xmath145 with a scalar @xmath29 , the quantities @xmath146 and @xmath147 u_\\theta^i f_\\theta^{1+\\beta}$ ] for @xmath148 .    under the above mentioned set - up with a scalar parameter @xmath29 , if true distribution belonging to the model family then the second order influence function of the minimum lsd estimator defined by the estimating equation ( [ eq : lsd_est_eqn ] ) is @xmath149 where @xmath150 where @xmath151    * example ( poisson mean ) : * let us now consider a numerical simulation to study the performance of the above second order influence analysis through its application in case of the poisson model with mean @xmath29",
    ". using the special structure of one parameter exponential family , of which poisson distribution is a special case , we compute the first and second order bias approximation using their respective expressions as given above and in section [ sec : if_mlsde ] .",
    "however , for brevity , we will only present some particular simulation result with @xmath152 , the contamination point @xmath153 and specific @xmath154 combinations and the corresponding bias plots are shown in figures [ fig : second_order_a ] , [ fig : second_order_b ] and [ fig : second_order_c ] respectively for @xmath155 , @xmath156 and @xmath136 .",
    "_ comments on figure [ fig : second_order_a ] @xmath157 _ : as expected both first order and second order influence function for @xmath21 gives a straight line .",
    "the bias approximation decreases as @xmath4 increases for both first and second order influence function .",
    "the difference of approximation between first and second order decreases as @xmath4 increases .",
    "_ comments on figure [ fig : second_order_b ] @xmath158 _ : keeping @xmath5 fixed the difference between bias approximation among first and second order decreases as @xmath4 increases . _",
    "comments on figure [ fig : second_order_c ] @xmath159 _ : as expected for this case the bias approximation is more for the first order influence function compared to the second order but the difference among two types of influence function shows same behavior compared to the case @xmath160 .",
    "+      +      +       another popular alternative to the influence function analysis is the breakdown point theory ; following simpson ( 1987 ) we will say that the estimator @xmath161 breaks down for contamination level @xmath140 if @xmath162 as @xmath163 for some sequence @xmath164 and @xmath165 . although the derivation of a general breakdown result is difficult , several authors have used it for some suitable subclass of probability distributions ; see park and basu ( 2004 ) , ghosh et al .",
    "( 2013 ) for breakdown results on some related minimum divergence estimators .",
    "now we derive the breakdown point of the minimum lsd functional + @xmath166 under the special class of location family @xmath167 . the particular property of this family , that helps to make the calculations simpler , is @xmath168 which is independent of the parameter @xmath29 . using this and the increasing nature of the logarithmic function ,",
    "the minimum lsd estimator for a location model is seen to be the maximizer of only the one integral term @xmath169 whenever @xmath170 and @xmath171 . however , under the same location model the minimum @xmath0-divergence estimator of ghosh et al .",
    "( 2013 ) can also be seen to be the maximizer of the same integral .",
    "therefore , under the location family of densities , the minimum lsd estimator with @xmath170 and @xmath171 coincides with corresponding minimum @xmath0-divergence estimators .",
    "then it follows from ghosh et al .",
    "( 2013 ) that , under certain assumptions ( assumptions bp1 to bp3 of their paper ) the asymptotic breakdown point @xmath172 of the minimum lsd estimator @xmath173 with @xmath170 and @xmath171 is at least @xmath174 at the model family .",
    "this section will describe the testing of hypotheses simulation example .",
    "we have taken sample from @xmath175 for @xmath176 and various sample sizes @xmath177 .",
    "all simulations have been replicated @xmath178 times .",
    "tables [ tab : level_20_nc ] , [ tab : level_50_nc ] and [ tab : level_100_nc ] give us the observed levels for no contamination case and tables [ tab : level_20 ] , [ tab : level_50 ] and [ tab : level_100 ] for contamination case while testing @xmath179 and the powers given in tables [ tab : power_20_nc ] , [ tab : power_50_nc ] and [ tab : power_100_nc ] for no contamination case and [ tab : power_20 ] , [ tab : power_50 ] and [ tab : power_100 ] for contamination case considering the testing problem @xmath180 . here the observed level has been taken as @xmath181 . usually for both @xmath4 and @xmath5 close to @xmath182",
    "we get level close to @xmath181 under no contamination case . for @xmath183 ,",
    "level does not go under @xmath184 for any @xmath5 . as @xmath5 becomes distant from @xmath182 in both positive and negative direction level moves from @xmath181 under no contamination . under contamination set - up",
    ", empirical level usually does not go below than @xmath185 . for smaller sample size like @xmath186 level",
    "never become lower than @xmath185 whereas for large sample size as @xmath187 , only when @xmath188 and @xmath189 level becomes lower than @xmath185 and for moderately large sample size @xmath190 , the situation does not differ very significantly . under contamination set - up for @xmath191 and @xmath160 ,",
    "the level is very high and for sample size @xmath187 it goes to @xmath192 also .",
    "the empirical power is very high under no contamination . for sample size",
    "@xmath187 the power is @xmath192 for most of the values of @xmath4 and @xmath5 .",
    "though for sample size @xmath190 the power does not reach @xmath192 but it is usually very close to @xmath192 .",
    "power usually does not go to that close to @xmath192 for sample size @xmath186 except for high negative value of @xmath193 and lower value of @xmath194 . under contamination",
    "set - up and for sample size @xmath186 the power usually does not go to @xmath192 but for low @xmath4 and high negative value of @xmath5 it goes very close . for @xmath5 close to @xmath182 and low @xmath4 , the power becomes less than @xmath195 but this is not much common throughout the table . for sample size @xmath187 ,",
    "the power is usually becomes @xmath192 except for very few combinations of @xmath154 and this fact is maintained for sample size @xmath190 also .",
    "as shown earlier , for @xmath23 , the divergence is independent of @xmath5 , that fact is also evident from the result that both level and power for all values of @xmath5 is same for @xmath23 .",
    "logarithmic super divergence family acts as a super family of both lpd and ldpd family . its usage in both statistical estimation and testing of hypotheses have been studied .",
    "along with the limitation of the first order influence function and the breakdown point under location model have also extensively studied .",
    "computational exercises have shown that there exist a region of the parameter which usually performs better where outliers are present in the observations .",
    "99 basu , a. , i. r. harris , n. l. hjort , and m. c. jones ( 1998 ) . robust and efficient estimation by minimising a density power divergence .",
    "_ biometrika _ , * 85 * , 549559 .",
    "beran , r. j. ( 1977 ) .",
    "minimum hellinger distance estimates for parametric models .",
    "_ annals of statistics _ , * 5 * , 445463 .",
    "bregman , l. m. ( 1967 ) .",
    "the relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming .",
    "_ ussr computational mathematics and mathematical physics _",
    ", * 7 * , 200217 .",
    "original article is in _ zh .",
    "vychisl . mat . mat .",
    "_ , * 7 * , pp .",
    "620631 , 1967 .",
    "cressie , n. and t. r. c. read ( 1984 ) .",
    "multinomial goodness - of - fit tests .",
    "_ journal of the royal statistical society b _ , * 46 * , 440464 .",
    "csisar , i. ( 1963 ) .",
    "eine informations theoretische ungleichung und ihre anwendung auf den beweis der ergodizitat von markoffschen ketten .",
    "_ , * 3 * , 85107 .",
    "fujisawa , h. and s. eguchi .",
    "robust parameter estimation with a small bias against heavy contamination .",
    "_ journal of multivariate analysis _ , * 99 * , 20532081 .",
    "fujisawa , h. ( 2013 ) .",
    "normalized estimating equation for robust parameter estimation . _ electronic journal of statistics _ , * 7 * , 15871606 .",
    "ghosh , a. , i.r .",
    "harris , a. maji , a. basu , and l. pardo ( 2013 ) . the robust parametric inference based on a new family of generalized density power divergence measures . technical report , bayesian and interdisciplinary research unit , indian statistical institute , india .",
    "ghosh , a. , and a. basu ( 2014 ) . on robustness of a divergence based test of simple statistical hypothesis .",
    "technical report , bayesian and interdisciplinary research unit , indian statistical institute , india .",
    "hampel , f. r. , e. ronchetti , p. j. rousseeuw , and w. stahel ( 1986 ) .",
    "_ robust statistics : the approach based on influence functions_. new york , usa : john wiley @xmath196 sons .",
    "jones , m. c. , n. l. hjort , i. r. harris , and a. basu ( 2001 ) .",
    "a comparison of related density - based minimum divergence estimators . _ biometrika _ , * 88 * , 865873 .",
    "kumar , . and a. basu ( 2014 )",
    ". technical report , bayesian and interdisciplinary research unit , indian statistical institute , india .",
    "lindsay , b. g. ( 1994 ) .",
    "efficiency versus robustness : the case for minimum hellinger distance and related methods .",
    "_ annals of statistics _ , * 22 * , 10811114 .",
    "maji , a. , a. ghosh , and a. basu ( 2014 ) .",
    "the logarithmic super divergence and asymptotic inference properties . technical report ,",
    "bayesian and interdisciplinary research unit , indian statistical institute , india .",
    "maji , a. , s. chakraborty , and a. basu ( 2014 ) .",
    "statistical inference based on the logarithmic power divergence .",
    "technical report , bayesian and interdisciplinary research unit , indian statistical institute , india .",
    "park , c. and a. basu ( 2004 ) .",
    "minimum disparity estimation : asymptotic normality and breakdown point results .",
    "_ bulletin of informatics and cybernetics _",
    ", * 36 * , 1933",
    ". special issue in honor of professor takashi yanagawa .",
    "pearson , k. ( 1900 ) . on the criterion",
    "that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling . philosophical magazine , * 50 * , 157175 .",
    "renyi , a. ( 1961 ) . on measures of entropy and information . _ in proceedings of fourth berkeley symposium on mathematical statistics and probability _ , * volume i * , pages 547561",
    ". university of california .",
    "simpson , d. g. ( 1987 ) .",
    "minimum hellinger distance estimation for the analysis of count data .",
    "_ journal of the american statistical association _ , * 82 * , 802807 ."
  ],
  "abstract_text": [
    "<S> this paper introduces a new superfamily of divergences that is similar in spirit to the @xmath0-divergence family introduced by ghosh et al .  </S>",
    "<S> ( 2013 ) . </S>",
    "<S> this new family serves as an umbrella that contains the logarithmic power divergence family ( renyi , 1961 ; maji , chakraborty and basu 2014 ) and the logarithmic density power divergence family ( jones et al . , </S>",
    "<S> 2001 ) as special cases . </S>",
    "<S> various properties of this new family and the corresponding minimum distance procedures are discussed with particular emphasis on the robustness issue ; these properties are demonstrated through simulation studies . </S>",
    "<S> in particular the method demonstrates the limitation of the first order influence function in assessing the robustness of the corresponding minimum distance procedures .    _ </S>",
    "<S> * keywords * _ : breakdown point , influence function , logarithmic density power divergence , logarithmic power divergence , robustness , @xmath0-divergence . </S>"
  ]
}