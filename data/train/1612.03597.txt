{
  "article_text": [
    "users personal data , such as a user s historic interaction with the search engine ( e.g. , submitted queries , clicked documents ) , have been shown useful to personalize search results to the users information need @xcite .",
    "crucial to effective search personalization is the construction of user profiles to represent individual users interests @xcite .",
    "a common approach is to use main topics discussed in the user s clicked documents @xcite , which can be obtained by using a human generated ontology as in @xcite or using an unsupervised topic modeling technique as in @xcite .",
    "however , using the user profile to directly personalize a search has been not very successful with a _ minor _ improvement @xcite or even _ deteriorate _ the search performance @xcite .",
    "the reason is that each user profile is normally built using only the user s relevant documents ( e.g. , clicked documents ) , ignoring user interest - dependent information related to input queries .",
    "alternatively , the user profile is utilized as a feature of a multi - feature learning - to - rank ( l2r ) framework @xcite . in this case , apart from the user profile , dozens of other features has been proposed as the input of an l2r algorithm @xcite . despite being successful in improving search quality , the contribution of the user profile is not very clear . to handle these problems , in this paper , we propose a new _ embedding _ approach to constructing a user profile , using both the user s input queries and relevant documents .",
    "we represent each user profile using two projection matrices and a user embedding .",
    "the two projection matrices is to identify the user interest - dependent aspects of input queries and relevant documents while the user embedding is to capture the relationship between the queries and documents in this user interest - dependent subspace .",
    "we then _ directly _ utilize the user profile to re - rank the search results returned by a commercial search engine .",
    "experiments on the query logs of a commercial web search engine demonstrate that modeling user profile with embeddings helps to significantly improve the performance of the search engine and also achieve better results than other comparative baselines @xcite do .",
    "we start with our new embedding approach to building user profiles in section [ ssec : profile ] , using pre - learned document embeddings and query embeddings .",
    "we then detail the processes of using an unsupervised topic model ( i.e. , latent dirichlet allocation ( lda ) @xcite ) to learn document embeddings and query embeddings in sections [ ssec : topics ] and [ ssec : query ] , respectively .",
    "we finally use the user profiles to personalize the search results returned by a commercial search engine in section [ ssec : rank ] .",
    "let @xmath0 denote the set of queries , @xmath1 be the set of users , and @xmath2 be the set of documents .",
    "let @xmath3 represent a triple @xmath4 .",
    "the query @xmath5 , user @xmath6 and document @xmath7 are represented by vector embeddings @xmath8 , @xmath9 and @xmath10 , respectively .",
    "our goal is to select a _ score function _",
    "@xmath11 such that the implausibility value @xmath12 of a correct triple @xmath3 ( i.e. @xmath13 is a relevant document of @xmath14 given @xmath15 ) is _ smaller _ than the implausibility value @xmath16 of an incorrect triple @xmath17 ( i.e. @xmath18 is not a relevant document of @xmath19 given @xmath20 ) .",
    "inspired by embedding models of entities and relationships in knowledge bases @xcite , the score function @xmath11 is defined as follows :    @xmath21    here we represent the profile for the user @xmath14 by two matrices @xmath22 and @xmath23 and a vector embedding @xmath9 , which represents the user s topical interests .",
    "specifically , we use the interest - specific matrices @xmath22 and @xmath24 to identify the interest - dependent aspects of both query @xmath15 and document @xmath13 , and use vector @xmath9 to describe the relationship between @xmath15 and @xmath13 in this interest - dependent subspace .    in this paper , @xmath25 and @xmath8 are pre - determined by employing the lda topic model @xcite , which are detailed in next sections [ ssec : topics ] and [ ssec : query ] .",
    "our model parameters are only the user embeddings @xmath9 and matrices @xmath22 and @xmath24 . to learn these user embeddings and matrices , we minimize the margin - based objective function :    @xmath26    where @xmath27 is the margin hyper - parameter , @xmath28 is the training set that contains only correct triples , and @xmath29 is the set of incorrect triples generated by corrupting the correct triple @xmath30 ( i.e. replacing the relevant document / query @xmath31 in @xmath30 by irrelevant documents / queries @xmath32 ) .",
    "we use stochastic gradient descent ( sgd ) to minimize @xmath33 , and impose the following constraints during training : @xmath34 , @xmath35 and @xmath36 .",
    "first , we initialize user matrices as identity matrices and then fix them to only learn the randomly initialized user embeddings . then in the next step",
    ", we fine - tune the user embeddings and user matrices together . in all experiments shown in section [ sec : expsetup ] , we train for 200 epochs during each two optimization step .      in this paper",
    ", we model document embeddings by using topics extracted from relevant documents .",
    "we use lda @xcite to _ automatically _ learn @xmath37 topics from the relevant document collection . after training an lda model to calculate the probability distribution over topics for each document , we use the topic proportion vector of each document as its document embedding . specifically , the @xmath38 element ( @xmath39 ) of the vector embedding for document @xmath13 is : @xmath40 where @xmath41 is the probability of the topic @xmath42 given the document @xmath13 .",
    "we also represent each query as a probability distribution @xmath43 over topics , i.e. the @xmath38 element of the vector embedding for query @xmath15 is defined as : @xmath44 where @xmath45 is the probability of the topic @xmath42 given the query @xmath15 .",
    "following @xcite , we define @xmath45 as a mixture of lda topic probabilities of @xmath42 given documents related to @xmath15 .",
    "let @xmath46 be the set of top @xmath47 ranked documents returned for a query @xmath15 ( in the experiments we select @xmath48 ) .",
    "we define @xmath45 as follows :    @xmath49 where @xmath50 is the exponential decay function of @xmath51 which is the rank of @xmath52 in @xmath53 .",
    "and @xmath54 is the decay hyper - parameter ( @xmath55 ) .",
    "the decay function is to specify the fact that a higher ranked document is more relevant to user in term of the lexical matching ( i.e. we set the larger mixture weights to higher ranked documents ) .",
    "we utilize the user profiles ( i.e. , the learned user embeddings and matrices ) to re - rank the original list of documents produced by a commercial search engine as follows : ( 1 ) we download the top @xmath47 ranked documents given the input query @xmath15 .",
    "we denote a downloaded document as @xmath13 .",
    "( 2 ) for each document",
    "@xmath13 we apply the trained lda model to infer the topic distribution @xmath56 .",
    "we then model the query @xmath15 as a topic distribution @xmath43 as in section [ ssec : query ] .",
    "( 3 ) for each triple @xmath3 , we calculate the implausibility value @xmath12 as defined in equation [ equa : stranse ] .",
    "we then sort the values in the ascending order to achieve a new ranked list .",
    "* dataset : * we evaluate our new approach using the search results returned by a commercial search engine .",
    "we use a dataset of query logs of of 106 anonymous users in 15 days from 01 july 2012 to 15 july 2012 .",
    "a log entity contains a user identifier , a query , top-@xmath57 urls ranked by the search engine , and clicked urls along with the user s dwell time .",
    "we also download the content documents of these urls for training lda @xcite to learn document and query embeddings ( sections [ ssec : topics ] and [ ssec : query ] ) .",
    "bennett _ et al .",
    "_ @xcite indicate that short - term ( i.e. session ) profiles achieved better search performance than the longer - term profiles .",
    "short - term profiles are usually constructed using the user s search interactions within a search session and used to personalize the search within the session @xcite . to identify a search session , we use 30 minutes of user inactivity to demarcate the session boundary . in our experiments ,",
    "we build short - term profiles and utilize the profiles to personalize the returned results . specifically , we uniformly separate the last log entries within search sessions into a _ test set _ and a _ validation set_. the remainder of log entities within search sessions are used for _ training _ ( e.g. to learn user embeddings and matrices in our approach ) .",
    "* evaluation methodology : * we use the sat criteria detailed in @xcite to identify whether a clicked url is relevant from the query logs ( i.e. , a sat click ) . that is either a click with a dwell time of at least 30 seconds or the last result click in a search session .",
    "we assign a positive ( relevant ) label to a returned url if it is a sat click .",
    "the remainder of the top-10 urls is assigned negative ( irrelevant ) labels .",
    "we use the rank positions of the positive labeled urls as the ground truth to evaluate the search performance before and after re - ranking .",
    "we also apply a simple pre - processing on these datasets as follows . at first , we remove the queries whose positive label set is empty from the dataset . after that",
    ", we discard the domain - related queries ( e.g. facebook , youtube ) . to this end ,",
    "the training set consists of 5,658 correct triples . the test and validation sets contain 1,210 and 1,184 correct triples , respectively .",
    "table [ table:1 ] presents the dataset statistics after pre - processing .",
    ".basic statistics of the dataset after pre - processing [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     by directly learning user profiles and applying them to re - rank the search results , our embedding approach achieves the highest performance of search personalization . specifically , our mrr score is significantly ( @xmath58 ) higher than that of _ sp _ ( with the relative improvement of 4% over sp ) .",
    "likewise , the p@1 score obtained by our approach is significantly higher than that of the baseline _ sp _ ( @xmath59 ) with the relative improvement of 11% .    in table",
    "[ tb1 ] , we also present the performances of a simplified version of our embedding approach where we fix the user matrices as identity matrices and then only learn the user embeddings . table [ tb1 ] shows that our simplified version achieves second highest scores compared to all others . )",
    "than our simplified version with 4% relative improvement . ]",
    "specifically , our simplified version obtains significantly higher p@1 score ( with @xmath58 ) than _",
    "in this paper , we propose a new embedding approach to building user profiles .",
    "we model each user profile using a user embedding together with two user matrices .",
    "the user embedding and matrices are then learned using lda - based vector embeddings of the user s relevant documents and submitted queries .",
    "applying it to web search , we use the profile to re - rank search results returned by a commercial web search engine .",
    "our experimental results show that the proposed method can stably and significantly improve the ranking quality . *",
    "acknowledgments * : the first two authors contributed equally to this work .",
    "dat quoc nguyen is supported by an international postgraduate research scholarship and a nicta nrpa top - up scholarship ."
  ],
  "abstract_text": [
    "<S> recent research has shown that the performance of search personalization depends on the richness of user profiles which normally represent the user s topical interests . in this paper </S>",
    "<S> , we propose a new embedding approach to learning user profiles , where users are embedded on a topical interest space . </S>",
    "<S> we then directly utilize the user profiles for search personalization . </S>",
    "<S> experiments on query logs from a major commercial web search engine demonstrate that our embedding approach improves the performance of the search engine and also achieves better search performance than other strong baselines . </S>"
  ]
}