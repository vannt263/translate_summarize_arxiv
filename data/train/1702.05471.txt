{
  "article_text": [
    "let @xmath0 and @xmath1 be two mean zero and unit variance random variables .",
    "pearson s correlation @xcite defined as @xmath2\\end{aligned}\\ ] ] is a basic statistical parameter and plays a central role in many statistical and machine learning methods such as linear regression @xcite , principal component analysis @xcite , and support vector machines @xcite , partially owing to its simplicity and computational efficiency .",
    "pearson s correlation however has two main weaknesses : firstly it only captures linear dependency between variables , and secondly for discrete ( categorical ) variables the value of pearson s correlation depends somewhat arbitrarily on the labels . to overcome these weaknesses , _ maximal correlation _ ( mc ) has been proposed and studied by hirschfeld @xcite , gebelein @xcite , sarmanov @xcite and rnyi @xcite , and is defined as @xmath3,\\\\ & { \\mathbb{e}}[\\phi_i(x_i)]=0,\\quad i=1,2,\\nonumber\\\\ & { \\mathbb{e}}[\\phi_i(x_i)^2]=1,\\quad i=1,2.\\nonumber\\end{aligned}\\ ] ] transformation functions @xmath4 are assumed to be borel measurable whose ranges are in @xmath5 . mc has also been studied by witsenhausen @xcite , ahlswede and gcs @xcite , and lancaster @xcite .",
    "mc tackles the two main drawbacks of the pearson s correlation : it models a family of nonlinear relationships between the two variables . for discrete variables ,",
    "the mc value only depends on the joint distribution and does not rely on labels .",
    "moreover the mc value between @xmath0 and @xmath1 is zero iff they are independent @xcite .    for the multivariate case with variables",
    "@xmath6, ... ,@xmath7 where @xmath8 , pearson s correlation can be extended naturally to the covariance matrix @xmath9 where @xmath10 $ ] ( assuming @xmath11 has zero mean and unit variance ) . similarly to the bivariate case , the covariance matrix analysis suffers from two weaknesses of only capturing linear dependencies among variables and being label dependent when variables are discrete ( categorical ) .",
    "one way to extend the idea of mc to the multivariate case is to consider the set of covariance matrices of transformed variables .",
    "let @xmath12 be the vector of transformed variables with zero means and unit variances .",
    "i.e. , @xmath13=0 $ ] and @xmath14=1 $ ] for @xmath15 .",
    "let @xmath16 be the covariance matrix of transformed variables @xmath17 where @xmath18 $ ] .",
    "the set of covariance matrices of transformed variables is defined as follows :    @xmath19=0,~{\\mathbb{e}}[\\phi_i(x_i)^2]=1,~ 1\\leq i ,",
    "i'\\leq p \\big\\}.\\end{aligned}\\ ] ]    similarly to the bivariate case , functions @xmath20 are assumed to be borel measurable whose ranges are in @xmath5 . if variables @xmath21 are continuous , functions @xmath22 are assumed to be continuous .",
    "the set @xmath23 includes infinitely many covariance matrices corresponding to different transformations of variables . in order to have an operational extension of mc to the multivariate case",
    ", we need to select one ( or finitely many ) members of @xmath23 through an optimization .",
    "so that variance of the transformed data can be explained maximally by a few meta features .",
    "feature transformations are unknown , can be nonlinear and are computed in an optimization . ]    here we propose the following optimization over @xmath23 that aims to select a covariance matrix @xmath24 with the maximum @xmath25-ky fan norm ( i.e. , with the maximum sum of top @xmath25 eigenvalues ) : @xmath26 since the trace of all matrices in @xmath23 is equal to @xmath27 , maximizing the ky fan norm over @xmath23 results in a low rank or an approximately low rank covariance matrix .",
    "we refer to this optimization as _ maximally correlated principal component analysis _ with parameter @xmath25 or for simplicity , the mcpca optimization .",
    "the optimal mcpca value is denoted by @xmath28 .",
    "when no confusion arises we use @xmath29 to refer to it .    principal component analysis ( pca ) @xcite aims to find @xmath25 eigenvectors corresponding to the top eigenvalues of the covariance matrix .",
    "these are called principal components ( pcs ) . on the other hand , we show that the mcpca optimization aims to find possibly nonlinear transformations of variables that can be approximated optimally by @xmath25 orthonormal vectors .",
    "thus , mcpca can be viewed as a generalization of pca over possibly nonlinear transformations of variables with zero means and unit variances .",
    "we summarize our main contributions below :    * we introduce mcpca as a multivariate extension of mc and a generalization of pca . * for jointly gaussian variables",
    "we show that the covariance matrix corresponding to the identity ( or the negative of the identity ) transformations majorizes covariance matrices of non - identity functions . using this result",
    "we characterize global mcpca optimizers for nonlinear functions of jointly gaussian variables for every @xmath25 . * for finite discrete variables , * * we compute a globally optimal mcpca solution when @xmath30 based on the leading eigenvector of a matrix computed using pairwise joint distributions . * * for an arbitrary @xmath25 we propose a block coordinate descend algorithm and show its convergence to stationary points of the mcpca optimization .",
    "* we study the consistency of sample mcpca ( an mcpca optimization computed using empirical distributions ) for both finite discrete and continuous variables .",
    "we compare mcpca with pca and other state - of - the - art nonlinear dimensionality reduction methods including isomap @xcite , lle @xcite , multilayer autoencoders ( neural networks ) @xcite , kernel pca @xcite , probabilistic pca @xcite and diffusion maps @xcite on several synthetic and real datasets .",
    "our real dataset experiments include breast cancer , parkinson s disease , diabetic retinopathy , dermatology , gene splicing and adult income datasets .",
    "we show that mcpca consistently provides improved performance compared to other methods .",
    "mcpca can be viewed as a dimensionality reduction method whose goal is to find possibly nonlinear transformations of variables with a low rank covariance matrix .",
    "other nonlinear dimensionality reduction methods include manifold learning methods such as isomap @xcite , locally linear embedding ( lle ) @xcite , kernel pca @xcite , maximum variance unfolding @xcite , diffusion maps @xcite , laplacian eigenmaps @xcite , hessian lle @xcite , local tangent space analysis @xcite , sammon mapping @xcite , multilayer autoencoders @xcite , among others . for a comprehensive review of these methods , see reference @xcite .",
    "although these techniques show an advantage compared to pca in artificial datasets , their successful applications to real datasets have been less convincing @xcite . the key challenge is to have an appropriate balance among generality of the model , computational complexity of the method and statistical significance of inferences .",
    "mcpca is more general than pca since it considers both linear and nonlinear feature transformations . in kernel",
    "pca methods , transformations of variables are _ fixed _ in advance .",
    "this is in contrast to mcpca that optimizes over transformations resulting in an optimal low rank approximation of the data .",
    "manifold learning methods such as isomap and lle aim to find a low dimensional representation of the data such that sample distances in the low dimensional space are the same , up to a scaling , to sample geodistances ( i.e. , distances over the manifold ) , assuming there exists such a manifold that the data lies on .",
    "these methods can be viewed as extensions of pca fitting a nonlinear model to the data .",
    "performance of these methods has been shown to be sensitive to noise and model parameters @xcite . through experiments on several synthetic and real datasets",
    "we show that the performance of mcpca is robust against these factors .",
    "note that mcpca allows features to be transformed only individually , thus avoiding a combinatorial optimization and resulting in statistically significant inferences .",
    "however because of this mcpca can not capture low dimensional structures such as the swiss roll example since underlying transformation depend on pairs of variables .    unlike existing dimensionality reduction methods that are only suitable for data with continuous features , mcpca is suitable for both categorical and continuous data .",
    "the reason is that even if the data is categorical , transformed values computed by mcpca are real .",
    "moreover we compare computational and memory complexity of mcpca and manifold learning methods ( isomap and lle ) in remark [ remark : complexity ] .",
    "unlike isomap and lle methods whose computational and memory complexity scales in a quadratic or cubic manner with the number of samples , computational and memory complexity of the mcpca algorithm scales linearly with the number of samples , making it more suitable for data sets with large number of samples .",
    "mcpca can be viewed as a multivariate extension of mc .",
    "other extensions of mc to the multivariate case have been studied in the literature .",
    "for example , reference @xcite introduces an optimization over @xmath23 that aims to maximize sum of arbitrary chosen elements of the matrix @xmath31 .",
    "@xcite shows that this optimization can be useful in nonlinear regression and graphical model inference .",
    "moreover , @xcite provides an algorithm to find local optima of the proposed optimization .",
    "reference @xcite introduces another optimization that aims to select a covariance matrix whose minimum eigenvalue is maximized .",
    "@xcite briefly discuses computational and operational aspects of the proposed optimization .",
    "for matrices we use bold - faced upper case letters , for vectors we use bold - faced lower case letters , and for scalars we use regular lower case letters . for random variables we use regular upper case letters . for example , @xmath32 represents a matrix , @xmath33 represents a vector , @xmath34 represents a scalar number , and @xmath35 represents a random variable .",
    "@xmath36 and @xmath37 are the identity and all one matrices of size @xmath38 , respectively .",
    "when no confusion arises , we drop the subscripts .",
    "@xmath39 is the indicator function which is equal to one if @xmath40 , otherwise it is zero .",
    "@xmath41 and @xmath42 represent the trace and the transpose of the matrix @xmath32 , respectively .",
    "@xmath43 is a diagonal matrix whose diagonal elements are equal to @xmath33 , while @xmath44 is a vector of the diagonal elements of the matrix @xmath32 .",
    "@xmath45 is the second norm of the vector @xmath33 .",
    "when no confusion arises , we drop the subscript .",
    "@xmath46 is the operator norm of the matrix @xmath32 .",
    "@xmath47 is the inner product between vectors @xmath33 and @xmath48 .",
    "@xmath49 indicates that vectors @xmath33 and @xmath48 are orthogonal .",
    "the matrix inner product is defined as @xmath50 .",
    "the eigen decomposition of the matrix @xmath51 is denoted by @xmath52 , where @xmath53 is the @xmath54-th largest eigenvalue of the matrix @xmath32 corresponding to the eigenvector @xmath55 .",
    "we have @xmath56 .",
    "@xmath55 has a unit norm .",
    "similarly the singular value decomposition of the matrix @xmath58 is denoted by @xmath59 where @xmath60 is the @xmath54-th largest singular value of the matrix @xmath61 corresponding to the left and right singular eigenvectors @xmath62 and @xmath63 , respectively .",
    "we have @xmath64 .",
    "@xmath65 . @xmath62 and @xmath63 are unit norm vectors .",
    "in reference @xcite , rnyi shows that mc between the two variables @xmath0 and @xmath1 is zero iff they are independent , while mc is one iff the two variables are strictly dependent ( i.e. , there exist mean zero , unit variance transformations of variables that are equal . ) .",
    "here we study some of these properties for the multivariate case of mcpca :    [ thm : prop - mcpca ] let @xmath29 be the optimal mcpca value for random variables @xmath0, ... ,@xmath66 .",
    "* @xmath67 , for @xmath68 . *",
    "@xmath69 iff @xmath11 and @xmath70 are independent , for @xmath71 . * @xmath72 iff @xmath0, ... ,@xmath66 are strictly dependent .",
    "i.e. , there exist zero mean , unit variance transformation functions @xmath22 such that for all @xmath73 , @xmath74 . * if @xmath22 are one - to - one transformation functions , @xmath75 .",
    "to prove part ( i ) , for any @xmath31 , we have @xmath76 because @xmath77 has zero mean and unit variance for @xmath15 . moreover , since @xmath78 , we have @xmath79 .",
    "thus , @xmath67 , for @xmath68 .",
    "this completes the proof of part ( i ) .    to prove part ( ii ) , suppose @xmath80 .",
    "thus , for every @xmath31 , we have @xmath81 .",
    "however since the sum of all eigenvalues are equal to @xmath27 , we have @xmath82 for every @xmath31 and @xmath15",
    ". therefore , @xmath83 for every @xmath31 .",
    "this means @xmath84 , for @xmath71 , which indicates that @xmath11 and @xmath70 are independent @xcite . to prove the other direction of part ( ii ) , if @xmath11 and @xmath70 are independent , for every zero mean and unit variance functions @xmath85 and @xmath86 , we have @xmath87=0 $ ] @xcite .",
    "thus , for every @xmath31 , we have @xmath83 .",
    "this completes the proof of part ( ii ) .    to prove part ( iii ) , let @xmath72 .",
    "thus , @xmath88 .",
    "it means that there exist transformation functions @xmath89 with zero means and unit variances such that for all @xmath73 , @xmath90=1 $ ] .",
    "it means that for @xmath15 , @xmath91 where @xmath92 has zero mean and unit variance .",
    "the proof of the inverse direction is straightforward .",
    "this completes the proof of part ( iii ) .    to prove part ( iv )",
    ", we note that if @xmath22 are one - to - one transformations , @xmath93 .",
    "thus , @xmath75 .",
    "this completes the proof of part ( iv ) .    in the following proposition ,",
    "we show that the increase ratio of the optimal mcpca value ( i.e. , @xmath94 ) is bounded above by @xmath95 which decreases as @xmath25 increases .",
    "[ prop : submodular - increase ] let @xmath29 be the optimal mcpca value for random variables @xmath0, ... ,@xmath66 .",
    "we have @xmath96    let @xmath97 be an optimal mcpca solution for @xmath98 . since @xmath29 is an optimal mcpca value with parameter @xmath25 , we have @xmath99 by summing over all @xmath100 , we have @xmath101 .",
    "this completes the proof .      a vector @xmath102 _ weakly majorizes _ vector @xmath103 ( in symbols , @xmath104 ) if @xmath105}\\geq \\sum_{r=1}^{q}y_{[r]}$ ] , for all @xmath68 .",
    "the symbols @xmath106}\\geq x_{[2]}\\geq \\cdots \\geq x_{[p]}$ ] stand for the elements of the vector @xmath33 sorted in a decreasing order .",
    "if @xmath104 and @xmath107 , then we say vector @xmath33 _ majorizes _ vector @xmath48 and denote it by @xmath108 .",
    "let @xmath109 and @xmath110 be two hermitian matrices in @xmath111 .",
    "we say @xmath109 majorizes @xmath110 is @xmath112 .",
    "we have the following equivalent formulation for matrix majorization that we will use in later parts of the paper .",
    "[ lem : unitary - eq ] the following conditions for hermitian matrices @xmath109 and @xmath110 are equivalent :    * @xmath113 * there exist unitary matrices @xmath114 and positive numbers @xmath115 such that @xmath116 where @xmath117",
    ".    see theorem 7.1 in @xcite .",
    "the following proposition makes a connection between an optimal mcpca solution and the majorization of covariance matrices in @xmath23 .",
    "[ prop : connection - major ] if @xmath24 majorizes all @xmath31 , then @xmath97 is an optimal solution of the mcpca optimization , for @xmath68 .    since @xmath24 majorizes all @xmath31 , @xmath118 , for all @xmath68 .",
    "thus @xmath97 is an optimal solution of optimization , for @xmath68 .",
    "the feasible set of optimization includes functions of variables with zero means and unit variances . in the following we consider an alternative optimization whose feasible set includes functions of variables with unit variances and",
    "show the relationship between its optimal solutions with the ones of the mcpca optimization .",
    "this formulation becomes useful in simplifying the mcpca optimization for finite discrete variables ( section [ sec : discrete ] ) .",
    "[ lem : mc - pca - variance ] consider the following optimization : @xmath119,\\quad 1\\leq i , i'\\leq p\\nonumber\\\\ & var(\\phi_i(x_i))=1,\\quad 1\\leq i \\leq p\\nonumber,\\end{aligned}\\ ] ] where @xmath120 denotes the variance of a random variables and @xmath121 $ ] .",
    "let @xmath122 and @xmath123 be optimal values of objective functions of optimizations and , respectively .",
    "we have @xmath124 .",
    "moreover if @xmath125 is an optimal solution of optimization , then @xmath126 is an optimal solution of optimization , where @xmath127 , and vice versa .",
    "first we have the following lemma :    [ thm : mc - pca - trace ] let @xmath128 be an optimal solution of the following optimization : @xmath129,\\quad 1\\leq i , i'\\leq p\\nonumber\\\\ & { \\mathbb{e}}[\\phi_i(x_i)^2]=1,\\quad 1\\leq i \\leq p\\nonumber\\\\ & { \\mathbb{e}}[\\phi_i(x_i)]=0,\\quad 1\\leq i \\leq p.\\nonumber\\end{aligned}\\ ] ] then @xmath97 is an optimal solution of optimization and @xmath130 .",
    "the proof follows from the fact that the @xmath25 ky fan norm of a matrix @xmath131 is the solution of the following optimization @xcite : @xmath132    consider the trace formulation of optimizations and according to lemma [ thm : mc - pca - trace ] :    @xmath133\\label{opt : trace1}\\\\ & tr({\\mathbf{w}})=q,\\nonumber\\\\ & 0 \\preceq { \\mathbf{w}}\\preceq i\\nonumber\\\\ & { \\mathbb{e}}[\\phi_i(x_i)^2]=1,\\quad 1\\leq i \\leq p\\nonumber\\\\ & { \\mathbb{e}}[\\phi_i(x_i)]=0,\\quad 1\\leq i \\leq p,\\nonumber\\\\\\nonumber\\\\ \\max_{{\\mathbf{w } } , \\{\\phi_i\\}_{i=1}^{p}}\\quad & \\sum_{i , i ' } w_{i , i ' } { \\mathbb{e}}\\left[\\left(\\phi_i(x_i)-\\bar{\\phi_i}(x_i)\\right)\\left(\\phi_{i'}(x_{i'}-\\bar{\\phi_{i'}}(x_{i'})\\right)\\right]\\label{opt : trace2}\\\\ & tr({\\mathbf{w}})=q,\\nonumber\\\\ & 0 \\preceq { \\mathbf{w}}\\preceq i\\nonumber\\\\ & var(\\phi_i(x_i))=1,\\quad 1\\leq i \\leq p.\\nonumber\\end{aligned}\\ ] ]    let @xmath134 and @xmath135 be an optimal solution of .",
    "the set of functions @xmath89 and @xmath135 is feasible for optimization .",
    "thus , @xmath136 .",
    "moreover , let @xmath137 and @xmath135 be an optimal solution of optimization .",
    "let @xmath138 .",
    "the set of functions @xmath139 and @xmath135 is feasible for optimization .",
    "thus , we have @xmath136 .",
    "therefore , we have that @xmath140 .",
    "this completes the proof .",
    "let @xmath141 be zero mean unit variance jointly gaussian random variables with the covariance matrix @xmath142 .",
    "thus @xmath143 where @xmath144 is the correlation coefficient between variables @xmath11 and @xmath70 .",
    "let @xmath145 for @xmath146 .",
    "a sign vector @xmath147 is a vector in @xmath148 where @xmath149 for @xmath15 .",
    "let @xmath150 be the @xmath151-th hermite - chebyshev polynomial for @xmath152 .",
    "these polynomials form an orthonormal basis with respect to the gaussian distribution @xcite : @xmath153= ( \\rho_{i , i'})^{j } \\mathbf{1}\\{j = j'\\}.\\nonumber\\end{aligned}\\ ] ] moreover , because hermite - chebyshev polynomials have zero means over a gaussian distribution we have @xmath154= \\mathbf{1}\\{j=0\\ } , ~~ 1\\leq i\\leq p.\\end{aligned}\\ ] ] using a basis expansion approach similar to @xcite we have @xmath155 where @xmath156 is the vector of projection coefficients .",
    "the constraint @xmath14=1 $ ] translates to @xmath157 while the constraint @xmath13=0 $ ] is simplified to @xmath158 for @xmath159 .",
    "we also have @xmath160 thus the mcpca optimization can be re - written as follows : @xmath161 since @xmath162 for @xmath146 , @xmath163 as @xmath164 .",
    "thus we can approximate optimization with the following optimization @xmath165 for sufficiently large @xmath166 .",
    "let @xmath29 and @xmath167 be optimal values of optimizations and , respectively . for a given @xmath168 , there exists @xmath169 such that if @xmath170 we have @xmath171 .",
    "the proof follows from the fact that the ky fan norm of a matrix is a continuous function of its elements and also @xmath163 as @xmath164 .    for the bivariate case ( @xmath172 )",
    ", the mcpca optimization simplifies to the maximum correlation optimization . for jointly gaussian variables the maximum correlation optimization results in global optimizers @xmath173 for @xmath174 @xcite .",
    "sign variables @xmath175 s are chosen so that the correlation between @xmath176 and @xmath177 is positive .",
    "this can be immediately seen from the formulation as well : maximizing the off - diagonal entry of a @xmath178 covariance matrix maximizes its top eigenvalue . for the bivariate case",
    "the global optimizer of optimization is @xmath179 for @xmath174 since @xmath180 for @xmath181 . using and since @xmath182 is the identity function , we obtain @xmath173 for @xmath174 .",
    "let @xmath183 be the set of covariance matrices of variables @xmath184 where @xmath185 for @xmath15 .",
    "in the bivariate case we have @xmath186 note that covariance matrices in @xmath183 have similar eigenvalues .",
    "moreover in the bivariate case every covariance matrix @xmath187 can be written as a convex combination of covariance matrices in @xmath183 .",
    "thus , it is majorized by covariance matrices in @xmath183 ( lemma [ lem : unitary - eq ] ) .",
    "however in the multivariate case we may have covariance matrices that are not in the convex hull of @xmath183 . to illustrate this ,",
    "let @xmath188 and consider @xmath189 one can show that the covariance matrix @xmath190 is not included in the convex hull of covariance matrices in @xmath183 .",
    "this covariance matrix results from having @xmath191 for @xmath192 .",
    "thus techniques used to characterize global optimizers of the bivariate case may not extend to the multivariate case .      here",
    "we characterize global optimizers of optimization .",
    "our main result is as follows :    [ thm : gauss - major ] @xmath142 majorizes every @xmath193 .",
    "this theorem along with lemma [ prop : connection - major ] results in the following corollary .",
    "@xmath194 where @xmath195 for @xmath15 provides a globally optimal solution for the mcpca optimization for @xmath68 .",
    "below we present the proof of theorem [ thm : gauss - major ] .",
    "first we prove the following lemma :    [ lem : a - decompos - diag ] let @xmath131 be a @xmath196 positive semidefinite matrix with unit diagonal elements .",
    "let @xmath197 be the @xmath151-th hadamard power of @xmath131 .",
    "then there exist diagonal matrices @xmath198 for @xmath199 such that @xmath200 where @xmath201 .",
    "we prove this lemma for @xmath202 .",
    "the case of @xmath203 can be shown by a successive application of the proof technique . since @xmath131 is a positive semidefinite matrix we can write @xmath204 . since diagonal elements of @xmath131 are one we have @xmath205 where @xmath206 is the @xmath54-th column of @xmath207 .",
    "then we have @xmath208 where @xmath209",
    "moreover we have @xmath210    next we prove the following result on matrix majorization :    [ lem : major - a - b ] let @xmath131 be a @xmath196 positive semidefinite matrix with unit diagonal elements .",
    "let @xmath211 where @xmath212 s are diagonal matrices such that @xmath213 .",
    "then @xmath214 .    using lemma [ lem : a - decompos - diag ]",
    "we can write @xmath215 where @xmath216 and @xmath217 . then using theorem 1 of @xcite completes the proof .",
    "let @xmath218 be the covariance matrix of transformed variables @xmath219 .",
    "using and for sufficiently large @xmath166 we have @xmath220 where @xmath221 since @xmath222 we have @xmath223 . using lemma [ lem : major - a - b ] completes the proof .",
    "let @xmath11 be a discrete random variable with distribution @xmath224 over the alphabet @xmath225 . without loss of generality , we assume all alphabets have positive probabilities as otherwise they can be neglected , i.e. , @xmath226 for @xmath227 .",
    "let @xmath228 be a function of random variable @xmath11 with zero mean and unit variance . using a basis expansion approach similar to @xcite",
    ", we have @xmath229 where @xmath230 note that @xmath231 form an orthonormal basis with respect to the distribution of @xmath11 because @xmath232=1,\\quad 1\\leq j\\leq |{\\mathcal{x}}_i|\\\\ & { \\mathbb{e}}[\\psi_{i , j}(x_i)\\psi_{i , j'}(x_i)]=0,\\quad 1\\leq j\\neq j'\\leq |{\\mathcal{x}}_i|.\\nonumber\\end{aligned}\\ ] ] moreover we have @xmath233=\\sqrt{p_{x_i}(j)},\\quad 1\\leq j\\leq |{\\mathcal{x}}_i|.\\end{aligned}\\ ] ] let @xmath234 be the joint distribution of discrete variables @xmath11 and @xmath70 .",
    "define a matrix @xmath235 whose @xmath236 element is @xmath237 this matrix is called the @xmath238-matrix of the distribution @xmath234 .",
    "note that @xmath239={\\mathbf{q}}_{i , i'}(j , j').\\end{aligned}\\ ] ] for @xmath240 , let @xmath241    [ thm : mcpca - opt - ai ] let @xmath242 be an optimal solution of the following optimization : @xmath243 then , @xmath97 is an optimal solution of mcpca optimization .",
    "consider @xmath244 in the feasible region of mcpca optimization .",
    "we have @xmath245,~~ 1\\leq i , i'\\leq p\\end{aligned}\\ ] ] where @xmath13=0 $ ] , and @xmath14=1 $ ] for all @xmath246 . using , we can represent these functions in terms of the basis functions : @xmath247 using , the constraint @xmath14=1 $ ] would be translated into @xmath248 for @xmath15 . moreover using ,",
    "the constraint @xmath249=0 $ ] is simplified to @xmath250 for @xmath15",
    ". we also have @xmath251= \\sum_{j=1}^{|{\\mathcal{x}}_{i}| } \\sum_{j'=1}^{|{\\mathcal{x}}_{i'}| } a_{i , j } a_{i',j'}\\ \\mathbb{e}[\\psi_{i , j } ( x_i ) \\psi_{i',j ' } ( x_{i'})]={\\mathbf{a}}_i^t { \\mathbf{q}}_{i , i ' } { \\mathbf{a}}_{i'}.\\end{aligned}\\ ] ] this shows every feasible point of optimization corresponds to a feasible point of optimization .",
    "the inverse argument is similar .",
    "this completes the proof .",
    "recall that @xmath252 is the @xmath151-th largest singular value of the matrix @xmath253 corresponding to left and right singular vectors @xmath254 and @xmath255 , respectively .",
    "[ lem : sing - value - q ] @xmath256 , @xmath257 and @xmath258 .",
    "first we show that the maximum singular value of the matrix @xmath253 is less than or equal to one . to show that , it is sufficient to show that for every vectors @xmath259 and @xmath260 such that @xmath261 and @xmath262 , we have @xmath263 . to show this , we define random variables @xmath264 and @xmath265 such that @xmath266 using cauchy - schwartz inequality , we have @xmath267 \\leq \\sqrt{{\\mathbb{e}}[\\upsilon_1 ^ 2 ] { \\mathbb{e}}[\\upsilon_2 ^ 2]}=||\\mathbf{a}_1|| \\",
    "||\\mathbf{a}_2||= 1.\\end{aligned}\\ ] ] therefore , the maximum singular value of @xmath253 is at most one .",
    "moreover @xmath268 and @xmath269 are right and left singular vectors of the matrix @xmath253 corresponding to the singular value one because @xmath270 and @xmath271 .    in the following we use similar techniques to the ones employed in @xcite to formulate an alternative and equivalent optimization to without orthogonality constraints which proves to be useful in characterizing a globally optimal mcpca solution when @xmath30 .",
    "consider the matrix @xmath272 .",
    "this matrix is positive semidefinite and the only vectors in its null space are @xmath273 and @xmath274 .",
    "this is because for any vector @xmath33 we have @xmath275 where the cauchy - schwartz inequality and @xmath276 are used .",
    "the inequality becomes an equality if and only if @xmath277 or @xmath278 .",
    "moreover we have @xmath279 for @xmath280 because @xmath281 where the last equality follows from the fact that @xmath282 is orthogonal to @xmath283 .",
    "define @xmath284 as follows : @xmath285 \\left [ { \\mathbf{u}}_1({\\tilde{i}}_i ) , \\dots , { \\mathbf{u}}_{|{\\mathcal{x}}_i|-1}({\\tilde{i}}_i ) \\right]^t\\bigg).\\end{aligned}\\ ] ]    [ thm : mcpca - opt - bi ] let @xmath286 be an optimal solution of the following optimization : @xmath287 then , @xmath242 is an optimal solution of optimization where @xmath288 .",
    "we consider unit variance formulation of the mcpca optimization .",
    "we have @xmath289= \\mathbb{e}[\\phi_i(x_i ) \\phi_{i'}(x_{i ' } ) ] - \\bar{\\phi_i}(x_i ) \\bar{\\phi_{i'}}(x_{i ' } ) \\nonumber \\\\ & = \\mathbf{a}_i^t { \\mathbf{q}}_{i , i ' } \\mathbf{a}_{i'}- ( \\mathbf{a}_i^t \\sqrt{\\mathbf{p}_i } ) ( \\mathbf{a}_{i'}^t \\sqrt{\\mathbf{p}_{i'}})= \\mathbf{a}_i^t \\left ( { \\mathbf{q}}_{i , i'}- \\sqrt{\\mathbf{p}_i } \\sqrt{\\mathbf{p}_{i'}}^t \\right ) \\mathbf{a}_{i'}.\\nonumber\\end{aligned}\\ ] ] moreover we have @xmath290- ( \\mathbb{e}[\\phi_i(x_i)])^2= ||\\mathbf{a}_i||_2 ^ 2- ( \\mathbf{a}_i^t \\sqrt{\\mathbf{p}_i})^2= \\mathbf{a}_i^t \\left ( i - \\sqrt{\\mathbf{p}_i } \\sqrt{\\mathbf{p}_i}^t",
    "\\right ) \\mathbf{a}_i.\\nonumber\\end{aligned}\\ ] ] therefore optimization can be written as @xmath291 we can write @xmath292 ( since @xmath293 is positive semidefinte ) where @xmath294    define @xmath295 .",
    "thus , @xmath296 can be written as @xmath297 .",
    "the vector @xmath274 is the eigenvector corresponding to eigenvalue zero of the matrix @xmath298 ( @xmath299 ) .",
    "other eigenvalues of @xmath298 is equal to one .",
    "since @xmath298 is not invertible , there are many choices for @xmath300 as a function of @xmath301 . @xmath302",
    "[ { \\mathbf{u}}_1({\\mathbf{b}}_i ) , \\dots , { \\mathbf{u}}_{|\\mathcal{x}_i|-1}({\\mathbf{b}}_i)]^t \\big)\\mathbf{b}_i+ \\alpha_i \\sqrt{\\mathbf{p}_i}= { \\mathbf{a}}_i \\mathbf{b}_i + \\alpha_i   \\sqrt{\\mathbf{p}_i},\\end{aligned}\\ ] ] where @xmath303 can be an arbitrary scalar ( note that @xmath304 ) .",
    "however since the desired @xmath305 of optimization is orthogonal to the vector @xmath274 , we choose @xmath306 ( i.e. , according to lemma [ lem : mc - pca - variance ] , in order to obtain a mean zero solution of the mcpca optimization , we subtract the mean from the optimal solution of optimization . )",
    "therefore we have @xmath307 moreover using lemma [ lem : sing - value - q ] , we have @xmath308 thus , @xmath309 where equality ( i ) comes from expanding @xmath254 over the basis @xmath310 and the fact that @xmath311 for @xmath181 .",
    "using equation in completes the proof .      in this part",
    "first we characterize an upper bound for the objective value of optimization for @xmath68 .",
    "then , we construct a solution that achieves this upper bound for @xmath30 .",
    "define a matrix @xmath312 such that @xmath313 optimization can be written as @xmath314 where @xmath110 has the structure defined in , and @xmath315 where @xmath316    [ prop : upper - bound ] the optimal value of optimization is upper bounded by @xmath317 .",
    "define @xmath318 .",
    "we have @xmath319 thus , @xmath320 is a relaxation of optimization .",
    "the optimal solution of this optimization is achieved when @xmath321 for @xmath322 .",
    "this completes the proof .",
    "[ thm : q=1 ] let @xmath323 where @xmath324 .",
    "then , @xmath325 is an optimal solution of optimization when @xmath30 .",
    "let @xmath326 .",
    "choosing @xmath327 and @xmath328 according to achieves the upper bound provided in lemma [ prop : upper - bound ] for the case of @xmath30 .",
    "this completes the proof .",
    "here we provide a block coordinate descend algorithm to solve the mcpca optimization for finite discrete variables with a general distribution for an arbitrary @xmath68 .",
    "we then show that the algorithm converges to a stationary point of the mcpca optimization .",
    "let @xmath329 .",
    "optimization can be written as @xmath330    [ lem : update - ai ] let @xmath331 if all variables except @xmath332 are fixed in the feasible set of optimization , then @xmath333 is an optimal solution of the constrained optimization if @xmath334 . if @xmath335 , every unit norm vector @xmath336 is an optimal solution of the constrained optimization .    under the condition of lemma [ lem : update - ai ] , optimization is simplified to the following optimization : @xmath337 writing @xmath338 , we have @xmath339 since @xmath340 .",
    "this completes the proof .",
    "[ lem : update - vr ] if all variables except @xmath341 are fixed in the feasible set of optimization , then @xmath342 where @xmath343 .",
    "the proof follows from the eigen decomposition of the covariance matrix @xmath131 .",
    "we use lemmas [ lem : update - ai ] and [ lem : update - vr ] to propose a block coordinate descend algorithm [ alg : mcpca ] to compute mcpca .",
    "[ alg : mcpca ] * input : * @xmath234 for @xmath73 , @xmath25 * initialization : * @xmath344 and @xmath345 * for * @xmath346 * for * @xmath347 * compute : * @xmath348 * update : * @xmath349 , if @xmath350 * compute : * @xmath351 where @xmath352 * update : * @xmath353 , for @xmath322 @xmath354 * end *    [ thm : greedy - convergance ] the sequence @xmath355 in algorithm [ alg : mcpca ] is monotonically increasing and convergent . moreover , if @xmath351 has top @xmath25 simple eigenvalues and @xmath350 for @xmath356 and @xmath152 , then @xmath357 converges to stationary points of optimization .    according to lemmas [ lem : update - ai ] and [ lem : update - vr ] , the sequence @xmath355 is increasing .",
    "moreover , since it is bounded above ( theorem [ thm : prop - mcpca ] , part [ i ] ) , it is convergent . moreover , under the conditions of theorem [ thm : greedy - convergance ] , at each step , lemmas [ lem : update - ai ] and [ lem : update - vr ] provide a unique optimal solution for optimizing variables @xmath358 and @xmath341 .",
    "thus , @xmath357 converges to a stationary point of optimization ( @xcite ) .",
    "principal component analysis is often applied to an observed data matrix whose rows and columns represent samples and features , respectively . in this part , first we review pca and then formulate the sample mcpca optimization ( an mcpca optimization computed over empirical distributions ) .",
    "we then study the consistency of sample mcpca for both finite discrete and continuous variables .",
    "let @xmath359 be a data matrix : @xmath360 where @xmath361 and @xmath362 represent its @xmath54-th row and @xmath151-th column , respectively .",
    "let @xmath363 , or interchangeably @xmath364 , denote the @xmath365-th element of @xmath35 .",
    "pca aims to find orthonormal vectors @xmath366 where @xmath367 and @xmath368 such that the average mean squared error between @xmath369 and @xmath370 for @xmath371 is minimized : @xmath372 let @xmath373 @xmath374 and @xmath375 are the empirical covariance matrix and the empirical mean of the data , respectively .",
    "[ thm : pca ] @xmath375 and @xmath376, ... ,@xmath377 provide an optimal solution for optimization .",
    "see reference @xcite .    by subtracting @xmath375 from rows of the input matrix ,",
    "the mean of each column becomes zero .",
    "this procedure is called _ centring _ the input data .",
    "let @xmath0 , ... , @xmath66 be discrete variables with joint distribution @xmath378 .",
    "let the alphabet size of variables ( i.e. , @xmath379 ) be finite .",
    "we observe @xmath380 independent samples @xmath381 from this distribution .",
    "let @xmath359 be the data matrix .",
    "sample mcpca aims to find possibly nonlinear transformations of the data ( i.e. , @xmath382 for @xmath15 ) to minimize the mean squared error ( mse ) between the transformed data and its low rank approximation by @xmath25 orthonormal vectors @xmath383, ... ,@xmath384 :    @xmath385    the constraint @xmath386 is similar to the centring step in the standard pca where columns of the data matrix are transformed to have empirical zero means ( theorem [ thm : pca ] ) .",
    "the additional constraint @xmath387 makes columns of the transformed matrix to have equal norms .",
    "let @xmath388 be @xmath27 finite discrete random variables whose joint probability distribution @xmath389 is equal to the empirical distribution of observed samples @xmath381 .",
    "i.e. , @xmath390 for @xmath391 .",
    "[ thm : sample - mcpca ] let @xmath97 be an optimal solution of the mcpca optimization over variables @xmath392 corresponding to transformation functions @xmath393 .",
    "then , @xmath394 provide an optimal solution for optimization .",
    "define @xmath395 as follows : @xmath396 thus @xmath397 we have @xmath398 let @xmath399 . note that @xmath400 .",
    "therefore we have @xmath401 @xmath402 .",
    "since @xmath392 is distributed according to the empirical distribution of samples @xmath403 , we have @xmath404.\\end{aligned}\\ ] ] similarly the constraint @xmath386 is simplified to the constraint @xmath405=0 $ ] , while the constraint @xmath387 is translated to the constraint @xmath406=1 $ ] .",
    "therefore , optimization can be written as @xmath407 moreover using , we have @xmath408 let @xmath409 . since @xmath410",
    ", @xmath411 is an eigenvector of @xmath131 corresponding to eigenvalue @xmath412 .",
    "this simplifies optimization to optimization and completes the proof .",
    "the following theorem discusses the consistency of sample mcpca for finite discrete variables .",
    "[ thm : const - discrete ] let @xmath29 and @xmath413 be optimal mcpca values over variables @xmath21 and @xmath392 .",
    "let @xmath27 and @xmath25 be fixed . as @xmath414 , with probability one , @xmath415 .",
    "the proof follows form the fact that for a fixed @xmath27 and @xmath25 , as @xmath416 , eigenvalues of the empirical covariance matrix converge to the eigenvalues of the true covariance matrix , with probability one .",
    "[ alg : sample - mcpca ] * input : * @xmath32 , @xmath25 * initialization : * @xmath417 and @xmath345 * for * @xmath346 * for * @xmath347 * compute : * @xmath418 * update : * @xmath419/||{\\mathbb{e}}[{\\mathbf{w}}_k^{(j)}|y_k]||$ ] , if @xmath420||\\neq 0 $ ] * compute : * @xmath351 where @xmath421 * update : * @xmath353 , for @xmath322 @xmath354 * end *      one way to compute sample mcpca is to use empirical pairwise joint distributions in algorithm [ alg : mcpca ] .",
    "however , forming and storing these empirical pairwise joint distributions may be expensive .",
    "below , we discuss computation of the sample mcpca optimization without forming pairwise joint distributions .",
    "let @xmath329 .",
    "the sample mcpca optimization can be written as follows : @xmath422    let @xmath388 be @xmath27 finite discrete random variables whose joint probability distribution @xmath389 is equal to the empirical distribution of observed samples @xmath381 . define the vector @xmath423 as follows : @xmath424    [ lem : update - phi ] if all variables except @xmath425 are fixed in the feasible set of optimization , then @xmath426/||{\\mathbb{e}}[{\\mathbf{w}}_k|y_k]||,\\end{aligned}\\ ] ] is the optimal solution of the constrained optimization if @xmath427||\\neq 0 $ ] .",
    "if @xmath427||=0 $ ] , every mean zero and unit norm @xmath428 is an optimal solution of the constrained optimization .    if all variables except @xmath425 are fixed , optimization can be simplified to @xmath429 note that since there exists @xmath425 such that @xmath430 , the constraint @xmath431 can be replaced by the constraint @xmath432 .",
    "now consider the following optimization : @xmath433 we show that the optimal solution of optimization has zero mean .",
    "for simplicity , we use @xmath425 instead of @xmath434 .",
    "we proceed by contradiction .",
    "suppose @xmath428 is an optimal solution of optimization whose mean is not zero ( i.e. , @xmath435 ) . consider the following solution : @xmath436 note that @xmath437 .",
    "thus , @xmath438 belongs to the feasible set of optimization .",
    "moreover we have @xmath439 therefore , @xmath440 using and the fact that @xmath441 , @xmath438 leads to a strictly larger objective value of optimization than the one of @xmath428 , which is a contradiction .",
    "therefore , the optimal solution of optimization has zero mean .",
    "thus , optimization is a tight relaxation of optimization .",
    "define @xmath442 $ ] .",
    "thus , @xmath443 $ ] . moreover , @xmath444 $ ]",
    "therefore , optimization is simplified to the following optimization : @xmath445\\\\ & { \\mathbb{e}}[y_k^2]\\leq 1\\nonumber.\\end{aligned}\\ ] ] using the cauchy - schwartz inequality completes the proof .    to update variables @xmath341",
    ", one can use lemma [ lem : update - vr ] . similarly to algorithm [ alg : mcpca ] , to solve the sample mcpca optimization for finite discrete variables , we propose algorithm [ alg : sample - mcpca ] which is based on a block coordinate descend approach .",
    "[ thm : greedy - convergance - sample ] the sequence @xmath355 in algorithm [ alg : sample - mcpca ] is monotonically increasing and convergent .",
    "moreover , if @xmath351 has top @xmath25 simple eigenvalues and @xmath420||\\neq 0 $ ] for @xmath356 and @xmath152 , then @xmath446 converges to stationary points of optimization .",
    "the proof is similar to the one of theorem [ thm : greedy - convergance ] .",
    "[ prop : complexity ] each iteration of algorithm [ alg : sample - mcpca ] has a computational complexity of @xmath447 and a memory complexity of @xmath448 .",
    "[ remark : complexity ]      in this part , we consider the case where @xmath0 , ... , @xmath66 are continuous variables with the density function @xmath449 . here",
    "we assume @xmath0, ... ,@xmath66 have bounded ranges . without loss of generality , let @xmath450 $ ] for @xmath15 .",
    "moreover , let the density function satisfy @xmath451 for @xmath452 $ ] and @xmath15 .",
    "we observe @xmath380 independent samples @xmath381 from this distribution .",
    "the data matrix @xmath453 is defined according to .",
    "since @xmath0, ... ,@xmath66 are continuous , with probability one , each column of the matrix @xmath35 has @xmath380 distinct values .",
    "thus , with probability one , there exists @xmath454 such that @xmath455 for @xmath15 , where @xmath456 is a vector in @xmath457 whose mean is zero and its norm is equal to @xmath458 .",
    "therefore , with probability one , the optimal value of optimization is equal to @xmath27 .    in the continuous case",
    ", the space of feasible transformation functions has infinite _ degrees of freedom_. thus , by observing @xmath380 samples from these continuous variables , we over - fit functions to observed samples .",
    "note that in the case of having observations from finite discrete variables , transformation functions have finite degrees of freedom and if the number of samples are sufficiently large , over - fitting issue does not occur ( theorem [ thm : const - discrete ] ) .",
    "one approach to overcome the over - fitting issue in the continuous case is to restrict the feasible set of optimization to functions whose degrees of freedom are smaller than the number of observed samples @xmath380 .",
    "one such family of functions is piecewise linear functions with @xmath459 degrees of freedom :    [ def : piecewise - linear ] let @xmath460 .",
    "@xmath461 is defined as the set of all functions @xmath462\\to \\mathbb{r}$ ] such that @xmath463 moreover , @xmath464 .",
    "let @xmath403 be observed sample from continuous variables @xmath0, ... ,@xmath66 .",
    "sample mcpca aims to solve the following optimization : @xmath465    [ thm : sample - mcpca - cont ] consider the following optimization : @xmath466 let @xmath97 be an optimal solution of optimization corresponding to transformation functions @xmath89 .",
    "then , @xmath467 provide an optimal solution of optimization .",
    "the proof is similar to the one of theorem [ thm : sample - mcpca ] .",
    "[ prop : m=1-samplemcpca = pca ] let columns of the data matrix @xmath32 have zero means and unit variances . if @xmath468 , the sample mcpca optimization is equivalent to the pca optimization .    for @xmath468 , @xmath469 only contains linear functions .",
    "since columns of the data matrix @xmath32 are assumed to be normalized , optimization is equivalent to optimization .",
    "define discrete variables @xmath470 whose alphabets are @xmath471 and @xmath472    below we establish a connection between solutions of the mcpca optimization over continuous variables and their discretized versions .",
    "we will use this connection to compute mcpca and sample mcpca over continuous variables .",
    "[ thm : computation - cont - dist ] let @xmath29 and @xmath473 be optimal values of the mcpca optimization over continuous variables @xmath21 and discrete variables @xmath474 , respectively . as @xmath475 , with probability one , @xmath476 .",
    "moreover , let @xmath477 be an optimal solution of the mcpca optimization over discrete variables @xmath474 .",
    "let @xmath478 . then , as @xmath475 , with probability one",
    ", @xmath479 is an optimal solution of the mcpca optimization over continuous variables @xmath480 .    for @xmath15 ,",
    "let @xmath481\\to\\mathbb{r}$ ] be a feasible function in the mcpca optimization over continuous variables @xmath21 .",
    "define @xmath482 such that @xmath483 below we show that as @xmath475 , with probability one , @xmath484 is feasible in the mcpca optimization over discrete variables @xmath474 .",
    "we have @xmath485&=\\sum_{j=1}^{d } pr(y_{i , d}=j ) { \\hat{\\phi}}_{i , d}(j)\\\\ & = \\sum_{j=1}^{d } \\int_{x=(j-1)/d}^{j / d } f_{x_i}(x ) \\phi_i(\\frac{j-1}{d})\\nonumber\\\\ & \\to \\sum_{j=1}^{d } \\int_{x=(j-1)/d}^{j / d } \\phi_i(x ) f_{x_i}(x ) dx\\nonumber\\\\ & = \\int_{x=0}^{1 } \\phi_i(x ) f_{x_i}(x ) dx=0.\\nonumber\\end{aligned}\\ ] ] similarly as @xmath475 , with probability one , @xmath486=1 $ ] , and @xmath487={\\mathbb{e}}[\\phi_i(x_i)\\phi_{i'}(x_{i'})].\\end{aligned}\\ ] ]    now consider @xmath488 as a feasible point for the mcpca optimization over discrete variables @xmath474 . for @xmath15 , define @xmath489 note that @xmath490\\to\\mathbb{r}$ ] .",
    "similarly to the previous argument , as @xmath475 , with probability one , @xmath491 is a feasible point in the mcpca optimization over continuous variables @xmath21 .",
    "moreover , as @xmath475 , with probability one , we have @xmath492={\\mathbb{e}}[{\\hat{\\phi}}_{i , d}(y_{i , d } ) { \\hat{\\phi}}_{i',d}(y_{i',d})].\\end{aligned}\\ ] ]    consider @xmath89 as an optimal solution of optimization over continuous variables @xmath21 with the optimal value @xmath29 .",
    "construct @xmath493 according to equation . as @xmath475 , with probability one , @xmath493 is a feasible point for the mcpca optimization over discrete variables @xmath474 which leads to the mcpca objective value @xmath494 .",
    "thus , @xmath495 .",
    "now consider @xmath496 as an optimal solution of optimization over discrete variables @xmath474 which leads to the mcpca objective value @xmath473 .",
    "construct @xmath497 according to equation .",
    "as @xmath475 , with probability one , @xmath497 is a feasible point for the mcpca optimization over continuous variables @xmath21 with the optimal value @xmath498 .",
    "thus , @xmath499 .",
    "this completes the proof .",
    "theorem [ thm : computation - cont - dist ] simplifies the mcpca computation over continuous variables @xmath21 to the mcpca computation over discrete variables @xmath500 which can be solved using algorithm [ alg : mcpca ] .",
    "a similar approach can be taken to simplify the sample mcpca optimization over continuous variables to the one of the discrete variables which can be solved using algorithm [ alg : sample - mcpca ] .",
    "variable @xmath470 provides a discretized version of the continuous variable @xmath11 where the position of knots ( i.e. , discretization thresholds ) are uniformly spaced in the range of the variable .",
    "however the argument of theorem [ thm : computation - cont - dist ] can be extended to consider other nonuniform and data - dependent discretization as well .",
    "for example , in the case that we observe @xmath380 samples from @xmath11 , one can choose the position of discretization knots to have equal number of samples in each discretization level .",
    "in the sample mcpca implementation for continuous variables , we use such a nonuniform discretization approach .     values .",
    "( b , c ) an illustration of @xmath501 ky fan norm of of latent , observed , and mcpca covariance matrices with different @xmath25 values for @xmath502 ( panel b ) , and @xmath503 ( panel c ) . ]",
    "first , we illustrate performance of mcpca over simulated discrete data . we generate @xmath504 independent samples from @xmath505 discrete variables",
    "whose covariance matrix is shown in figure [ fig : heatmap - p75]-a ( left panel ) .",
    "these samples are generated as discretized version of continuous jointly gaussian samples .",
    "alphabet sizes of variables ( i.e. , the number of quantization levels ) are equal to 10 .",
    "we then apply unknown random functions ( with zero means and unit variances ) to samples of each variable .",
    "the covariance matrix of observed samples ( i.e. , samples from transformed variables ) is shown in figure [ fig : heatmap - p75]-a ( right panel ) .",
    "owing to transformations of variables , the block diagonal structure of the latent covariance matrix has been faded in the observed one .",
    "we apply the sample mcpca algorithm [ alg : sample - mcpca ] with parameter @xmath25 to the observed data matrix .",
    "we use 10 random initializations and 10 repeats of algorithm [ alg : sample - mcpca ] .",
    "figure [ fig : heatmap - p75]-b illustrates the covariance matrix computed by the mcpca algorithm with parameter @xmath506 .",
    "mcpca with @xmath30 highlights some of the block diagonal structure in the latent covariance matrix .",
    "mcpca with larger @xmath25 recovers all the blocks .",
    "note that the mcpca algorithm aims to find a covariance matrix of transformed variables with the largest ky fan norm and is not tailored to infer a specific hidden structure in the data . nevertheless inferring",
    "a low rank covariance matrix often captures such hidden structures in the data .",
    "figure [ fig : heatmap - p75]-c , d shows the @xmath501 ky fan norm for the latent covariance matrix , for the observed covariance matrix ( i.e. , the pca objective value ) , and for covariance matrices computed by mcpca with different @xmath25 values . for @xmath30 , theorem [ thm : q=1 ] provides a globally optimal solution for the mcpca optimization .",
    "we include that solution as well as the mcpca solution computed in algorithm [ alg : sample - mcpca ] .",
    "figure [ fig : heatmap - p75]-c shows that the ky fan norm of covariance matrices computed by mcpca are significantly larger than the one of the pca . in figure",
    "[ fig : heatmap - p75]-d , we show the @xmath501 ky fan norm for @xmath503 for different covariance matrices .",
    "note that the method of theorem [ thm : q=1 ] provides a globally optimal solution for @xmath501 ky fan norm maximization when @xmath507 , while the mcpca algorithm [ alg : sample - mcpca ] provides a locally optimal solution . in this case (",
    "figure [ fig : heatmap - p75]-d , the left panel ) , the gap between global and local optimal values is small .",
    "moreover for the case of @xmath508 ( figure [ fig : heatmap - p75]-d , the right panel ) , the mcpca solution with parameter @xmath509 is outperforming other solutions .",
    "finally in the case considered in figure [ fig : heatmap - p75]-c , d , we observe that the ky fan norm of the covariance matrix computed by the mcpca algorithm is not sensitive to parameter @xmath25 .",
    "next , we compare performance of different dimensionality reduction methods including mcpca , pca , isomap , lle , multilayer autoencoders ( neural networks ) , kernel pca , probabilistic pca and diffusion maps on synthetic datasets .",
    "we assess the performance of different dimensionality reduction methods based on how much sample distances in the inferred and true low dimensional spaces match with each other .",
    "more precisely , let @xmath510 be a matrix whose rank is @xmath511 .",
    "let @xmath512 be the distance between sample @xmath54 and @xmath151 in the @xmath25 dimensional representation of @xmath513 .",
    "let @xmath514 be the noise matrix .",
    "let @xmath515 be the observed data matrix whose columns are transformations of columns of the matrix @xmath516 .",
    "these transformations are assumed to be continuous and bijective .",
    "let @xmath517 be the distance between sample @xmath54 and @xmath151 in the inferred @xmath25 dimensional representation of @xmath32 .",
    "we asses the performance of the dimensionality reduction method by computing the spearman s rank correlation between @xmath512 and @xmath517 for @xmath518 .",
    "we generate @xmath513 as @xmath519 where @xmath520 and @xmath395 .",
    "elements of @xmath207 and @xmath521 are generated according to a gaussian distribution with zero mean and unit variance . in the noiseless case",
    ", @xmath522 is an all zero matrix . in the noisy case ,",
    "elements of @xmath522 are generated according to a gaussian distribution with zero mean and unit variance .",
    "we consider two types of transformations to generate columns of @xmath32 using columns of the matrix @xmath516 : ( i ) a polynomial transformation where for each variable we randomly select a transformation from the set @xmath523 , and ( ii ) a piecewise linear transformation according to definition [ def : piecewise - linear ] where @xmath524 has an exponential distribution with parameter @xmath525 .",
    "the positions of knots are chosen so that each bin has equal number of samples .",
    "we use default parameters for different dimensionality reduction methods .",
    "isomap and lle have a parameter @xmath526 which determines the number of neighbors considered in their distance graphs .",
    "@xmath526 is set to be 12 .",
    "moreover for the continuous data , mcpca has a parameter @xmath459 which restricts the optimization to a set of piecewise linear functions with degree @xmath459 .",
    "we set @xmath527 . for other methods we use implementations of reference @xcite .",
    "experiments have been repeated 10 times in each case .    in figure",
    "[ fig : cmp - all]-a we consider a relatively easy setup where @xmath528 , @xmath30 , transformation functions are polynomials , and there is no added noise to observed samples . in this setup ,",
    "all methods except lle and kernel pca have good performance .",
    "gaussian kernel pca performed poorly in these experiments .",
    "thus , we only illustrate performance of polynomial kernel pca in this figure .",
    "it further highlights sensitivity of kernel pca to the model setup . in figure",
    "[ fig : cmp - all]-b we consider a similar setup to the one of panel ( a ) but we increase @xmath25 to be 5 .",
    "mcpca continues to have a good performance while the performance of other methods drop significantly .",
    "next , we increase @xmath27 to 50 and @xmath25 to 10 .",
    "we also add noise to observed samples as described above .",
    "mcpca continues to have a good performance outperforming all other methods ( figure [ fig : cmp - all]-c ) . in figure",
    "[ fig : cmp - all]-d we change nonlinear transformations from polynomials to piecewise linear functions compared to panel ( c ) .",
    "again , in this setup mcpca outperforms all other methods .",
    "these experiments highlight robustness of mcpca against model parameters and noise .",
    "performance of other methods appears to be sensitive to these factors .      having illustrated effectiveness of mcpca on synthetic datasets , we apply it to real datasets .",
    "we consider six data sets from the uci machine learning repository data sets @xcite , namely breast cancer data set , gene splicing data set , dermatology data set , adult income data set , parkinsons disease data set , and diabetic retinopathy data set .",
    "these data sets have been chosen to span various types of input data .",
    "some of them have discrete features , some have continuous features , while some have mixed discrete and continuous features .",
    "the number of samples ( @xmath380 ) and the number of features ( @xmath27 ) vary across these data sets .",
    "samples in five of these data sets have binary labels while in one of them the number of sample classes is six .",
    "basic properties of these data sets have been summarized in table [ tab : dataset - prop ] .",
    "below we explain some of these properties with more details :    * the breast cancer data set has 683 individuals with breast cancer , among which 444 are benign and 239 are malignant ( we remove 16 samples with missing values from the original data set . ) .",
    "attributes in this data set include features such as clump thickness , uniformity of cell size , mitoses , etc .",
    "values of these features are discrete in the set of @xmath529 . for more information about this data set ,",
    "see @xcite . *",
    "the gene splicing data set has 3,175 samples .",
    "each sample is a 60 base pair subset of genome .",
    "the goal is to classify two types of splice junctions in dna sequences : exon / intron ( ei ) or intron / exon ( ie ) sites .",
    "values of features are discrete in the set of @xmath530 . for more information about this data set ,",
    "see @xcite . *",
    "the dermatology data set has 366 samples and 33 features ( we ignore the age feature from the original data since it has missing values . ) .",
    "the classification of erythemato - squamous diseases is a difficult task in dermatology since they share clinical features of erythema and with similar scaling .",
    "this data set have samples with six diseases : psoriasis , seboreic dermatitis , lichen planus , pityriasis rosea , cronic dermatitis , and pityriasis rubra pilaris .",
    "the number of samples of each disease are 112 , 61 , 72 , 49 , 52 , 20 , respectively .",
    "features include 12 clinical features and 21 histopathological features .",
    "variables are discrete whose alphabet sizes are 2 ( for one feature ) , 3 ( for one feature ) , and 4 ( for 31 features ) . for more information about this data set ,",
    "see @xcite . * the adult income data set is the largest data set we consider in this section",
    "it has 30,162 samples ( after removing samples from the original training data with missing values . ) .",
    "the task is to classify individuals to two groups based on their income .",
    "this data set includes 22,654 individuals with income @xmath531 and @xmath532 individuals with income @xmath533 .",
    "features include variables such as age , sex , race , education , work class , capital gain , capital loss , hours per week , etc .",
    "all features except one has fewer than 120 distinct alphabet values . for more information about this data set ,",
    "see @xcite . *",
    "the parkinsons disease data set has 195 samples where 47 of them come from healthy individuals and 147 of them come from parkinsons patients .",
    "each feature is a particular voice measure such as average vocal fundamental frequency , measures of variation in amplitude , measures of frequency variation , etc .",
    "features are continuous with alphabet sizes ranges from 20 to 195 . for more information about this data set ,",
    "see @xcite .",
    "* the diabetic retinopathy data set has 1,151 samples where 540 samples have no signs of the disease .",
    "the data contains 19 features extracted from the messidor image set to predict whether an image contains signs of diabetic retinopathy or not .",
    "the alphabet size of features range from 2 to 1,151 . for more information on this data set ,",
    "see @xcite .",
    "pca and mcpca aim to maximize the amount of explained variance in the data ( or in the transformation of the data ) using low dimensional features .",
    "pca restricts its optimization to merely linear transformations while mcpca considers a more general family of nonlinear transformation functions .",
    "more precisely , let @xmath534 be the covariance matrix of transformations of variables .",
    "then @xmath535 is the fraction of explained variance in the transformation of the data using its optimal @xmath25 dimensional representation .",
    "we normalize features to have zero means and unit variances .",
    "we perform a two - fold cross validation analysis : we choose half of the data uniformly randomly for training",
    ". then we test performance of the methods in the remaining half of the data . in discrete data sets ( i.e. , breast cancer , gene splicing and dermatology data sets ) we use sample mcpca algorithm [ alg : sample - mcpca ] to compute optimal transformations of features in the training data for each @xmath25 value . then , we apply those transformations to the test data . in the adult income data set all features except one has fewer than 120 distinct alphabet values . for the only continuous feature in this data set we use @xmath536 . in continuous data sets ( i.e. , parkinsons disease and diabetic retinopathy data sets ) we use the procedure explained in section [ subsec : comp - cont ] . in these experiments",
    "@xmath527 is fixed .",
    "we repeat each experiment 10 times .",
    "figure [ fig : variance ] shows the fraction of explained variance using top @xmath25 meta features computed by pca and mcpca in a two - fold cross validation analysis . in breast cancer and adult income datasets",
    "mcpca significantly outperforms pca for all values of @xmath25 , while in other datasets their performance is comparable . the fact that mcpca shows higher or comparable performance to pca in holdout datasets indicates that mcpca captures meaningful nonlinear correlations among features whenever they exist .",
    "next , we examine how predictive of phenotype extracted meta features are . similarly to the previous experiment we use a two - fold cross validation analysis . we choose half of samples",
    "uniformly randomly to train the methods , and test their performance in the remaining half .",
    "we repeat each experiment 10 times . in continuous data sets ,",
    "we consider @xmath537 .",
    "for the isomap in the training phase we consider @xmath538 . in the isomap case , since the method does not have the so - called _ parametric out - of - sample _",
    "property @xcite ( meaning that we can not use the low dimensional embedding of the training data to compute a low dimensional embedding of the test data ) , we run the method on the test data using optimal parameters learned in the training step .",
    "this issue occurs in other nonlinear dimensionality reduction methods . in those cases we run the methods in the test data using their default parameters @xcite .",
    "figure [ fig : corr - cross ] shows the correlation between the top extracted meta feature using different dimensionality reduction methods and phenotype .",
    "the implementation of lle crashed in these experiments , thus excluded from this figure . in all cases mcpca",
    "consistently outperforms all other methods in different ranges of correlation between the meta feature and phenotype .",
    "for example , correlation between the meta feature and phenotype is high in the breast cancer dataset , is average in the adult income dataset , and is low in gene splicing and diabetic retinopathy datasets .",
    "nevertheless , in all cases mcpca shows a significant gain over all other methods .",
    "here we introduced maximally correlated principal component analysis ( mcpca ) as a multivariate extension of maximal correlation and a generalization of pca .",
    "mcpca computes , possibly nonlinear , transformations of variables whose covariance matrix has the largest ky fan norm .",
    "mcpca resolves two weaknesses of pca by considering nonlinear correlations among features and being suitable for both continuous and categorical data .",
    "although the mcpca optimization is non - convex , we characterized its global optimizers for nonlinear functions of jointly gaussian variables , and for categorical variables under some conditions . for general categorical variables , we proposed a block coordinate descend algorithm and showed its convergence to stationary points of the mcpca optimization .",
    "given the widespread applicability of pca and the improved and robust performance of mcpca compared to state - of - the - art dimensionality reduction methods , we expect the proposed method to find broad use in different areas of science . moreover , techniques developed for efficiently optimizing feature transformations over a broad family of linear and nonlinear functions can be employed in several other statistical and machine learning problems such as nonlinear regression and deep learning .",
    "h.  gebelein , `` das statistische problem der korrelation als variations - und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung , '' _ zamm - journal of applied mathematics and mechanics _ , vol .",
    "21 , no .  6 , pp .",
    "364379 , 1941 .",
    "s.  lafon and a.  b. lee , `` diffusion maps and coarse - graining : a unified framework for dimensionality reduction , graph partitioning , and data set parameterization , '' _ ieee transactions on pattern analysis and machine intelligence _ , vol .",
    "28 , no .  9 , pp . 13931403 , 2006 .",
    "k.  q. weinberger , f.  sha , and l.  k. saul , `` learning a kernel matrix for nonlinear dimensionality reduction , '' in _ proceedings of the twenty - first international conference on machine learning_.1em plus 0.5em minus 0.4emacm , 2004 , p. 106 .",
    "d.  l. donoho and c.  grimes , `` hessian eigenmaps : locally linear embedding techniques for high - dimensional data , '' _ proceedings of the national academy of sciences _ , vol .",
    "100 , no .",
    "10 , pp . 55915596 , 2003 .",
    "zhang and h .- y .",
    "zha , `` principal manifolds and nonlinear dimensionality reduction via tangent space alignment , '' _ journal of shanghai university ( english edition ) _ , vol .  8 , no .  4 , pp . 406424 , 2004 .",
    "h.  a. gvenir , g.  demirz , and n.  ilter , `` learning differential diagnosis of erythemato - squamous diseases using voting feature intervals , '' _ artificial intelligence in medicine _ , vol .  13 , no .  3 , pp .",
    "147165 , 1998 .",
    "m.  a. little , p.",
    "e. mcsharry , e.  j. hunter , j.  spielman , l.  o. ramig _",
    "_ , `` suitability of dysphonia measurements for telemonitoring of parkinson s disease , '' _ ieee transactions on biomedical engineering _ , vol .",
    "56 , no .  4 , pp .",
    "10151022 , 2009 ."
  ],
  "abstract_text": [
    "<S> in the era of big data , reducing data dimensionality is critical in many areas of science . widely used principal component analysis ( pca ) addresses this problem by computing a low dimensional data embedding that maximally explain variance of the data . however , pca has two major weaknesses . </S>",
    "<S> firstly , it only considers linear correlations among variables ( features ) , and secondly it is not suitable for categorical data . </S>",
    "<S> we resolve these issues by proposing maximally correlated principal component analysis ( mcpca ) . </S>",
    "<S> mcpca computes transformations of variables whose covariance matrix has the largest ky fan norm . </S>",
    "<S> variable transformations are unknown , can be nonlinear and are computed in an optimization . </S>",
    "<S> mcpca can also be viewed as a multivariate extension of maximal correlation . for jointly gaussian variables </S>",
    "<S> we show that the covariance matrix corresponding to the identity ( or the negative of the identity ) transformations majorizes covariance matrices of non - identity functions . using this result </S>",
    "<S> we characterize global mcpca optimizers for nonlinear functions of jointly gaussian variables for every rank constraint . for categorical variables we characterize global mcpca optimizers for the rank one constraint based on the leading eigenvector of a matrix computed using pairwise joint distributions . for a general rank constraint </S>",
    "<S> we propose a block coordinate descend algorithm and show its convergence to stationary points of the mcpca optimization . </S>",
    "<S> we compare mcpca with pca and other state - of - the - art dimensionality reduction methods including isomap , lle , multilayer autoencoders ( neural networks ) , kernel pca , probabilistic pca and diffusion maps on several synthetic and real datasets . </S>",
    "<S> we show that mcpca consistently provides improved performance compared to other methods . </S>"
  ]
}