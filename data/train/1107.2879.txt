{
  "article_text": [
    "the increasing drive towards quantitative technologies in biology has brought with it a renewed interest in the modeling of biological systems .",
    "models of biological systems and other complex phenomena are generally nonlinear with uncertain parameters , many of which are often unknown and/or unmeasurable @xcite .",
    "crucially , the values of the parameters dictate not only the quantitative but also the qualitative behaviour of such models @xcite . a fundamental task in quantitative and systems biology is to use experimental data to infer parameter values that minimise the discrepancy between the behaviour of the model and experimental observations .",
    "the parameters thus obtained can then be cross - validated against unused data before employing the fitted model as a predictive tool @xcite .",
    "ideally , this process could help close the modelling - experiment loop by : suggesting specific experimental measurements ; identifying relevant parameters to be measured ; or discriminating between alternative models @xcite .",
    "the problem of parameter estimation and data fitting is classically posed as the minimisation of a cost function ( i.e. , the error ) @xcite . in the case of overdetermined linear systems with quadratic error functions , this problem leads to least - square solutions , convex optimisations that can be solved efficiently and globally based on the singular value decomposition of the covariance matrix of the data @xcite .",
    "however , data fitting in nonlinear systems with small amounts of data remains difficult , as it usually leads to non - convex optimisations with many local minima @xcite .",
    "a classic case in biological modeling is the description of the time evolution of a system through ordinary differential equations ( odes ) , usually based on mechanistic functional forms .",
    "examples include models of biochemical reactions , infectious spread and neuronal dynamics @xcite .",
    "typically , optimal parameters of the nonlinear odes must be inferred from experimental time courses but the associated optimisation is far from straightforward .",
    "standard optimisation techniques that require an explicit cost function are unsuitable for this problem due to the difficulty to obtain full analytical solutions for nonlinear odes @xcite . spline - based methods , which approximate the solution though an implicit integration of the differential equation @xcite , require linearity in the parameters and are therefore not applicable to models with nonlinear parameter dependencies , e.g. michaelis - menten and hill kinetics .    implicit techniques , such as direct search methods @xcite , simulated annealing @xcite , evolutionary algorithms @xcite or sequential monte carlo @xcite , do not require an explicit cost function .",
    "however , if as is usually the case , the cost function is a complicated ( hyper)surface in parameter space with many local minima , gradient and direct search methods tend to get trapped in local minima due to their use of local information .",
    "although still a local method , simulated annealing alleviates some of the problems related to local minima through the use of stochasticity .",
    "however , this comes at the cost of high computational overhead and slow convergence and , yet , with no guarantee of finding the global minimum .    instead of an optimisation based on local criteria , evolutionary algorithms ( ea )",
    "produce an ensemble of possible answers and evolve them globally through random mutation and cross - over followed by ranking and culling of the worst solutions @xcite .",
    "this heuristic has been shown to provide an efficient protocol for parameter fitting in the life sciences @xcite . however , ea methods can be inefficient when the feasible region in parameter space is too large , a case typical of models with large uncertainty in the parameters .",
    "probabilistic methods , such as sequential monte - carlo ( smc ) @xcite , propose a different conceptual framework . rather than finding a _ unique _ optimal parameter set , smc maps a prior probability distribution of the parameters onto a posterior constructed from samples with low errors until reaching a converged posterior .",
    "recently , smc has been combined with approximate bayesian computation ( abc ) and applied to data fitting and model selection @xcite .",
    "however , methods such as abc - smc are not only computationally expensive but also require that the starting prior include the _ true _ value of the parameters .",
    "this requirement dents its applicability to many biological models , in which not even the order of magnitude of the parameters is known . in that case",
    ", the support of the starting priors must be made overly large ( leading to extremely slow convergence ) in order to avoid the risk of excluding the true parameter value from the search space .    in this work",
    ", we present an optimisation algorithm for data fitting that takes inspiration from ea , smc and direct search optimisation .",
    "our method iterates and refines samples from a probability distribution of the parameters in a squeeze - and - breathe sequence . at each iteration",
    "the probability distribution is ` squeezed ' by the consecutive application of local optimisation followed by ranking and culling of the local optima .",
    "the parameter distribution is then allowed to ` breathe ' through a random update from a historical prior that includes the union of all past supports of the solutions ( fig .",
    "[ fig : bpm - panel ] ) .",
    "this iteration proceeds until convergence of the distribution of solutions and their average error .",
    "a key feature of the algorithm is the accelerated step - to - step convergence through a combination of local optimisation and of culling of local solutions .",
    "importantly , the method can also find parameters that lie outside of the range of the initial prior , and can deal with parameter values that extend across several orders of magnitude .",
    "we now provide definitions and a full description of our algorithm and showcase its applicability to different biological models of interest .",
    "let @xmath0 $ ] denote the state of a system with @xmath1 variables at time @xmath2 .",
    "the time evolution of the state is described by a system of ( possibly nonlinear ) odes : @xmath3 here is the vector of @xmath4 parameters of our model .",
    "the experimental data set is formed by @xmath5 observations of some of the variables of the system : @xmath6 ideally , @xmath7 since @xmath8 experiments are enough for unequivocal identification of an ode model with @xmath4 parameters when no measurement error is present @xcite .    the _ cost function _",
    "( i.e. , the error ) to be minimised is : @xmath9 where @xmath10 is a relevant vector norm .",
    "a standard choice is the euclidean norm ( or 2-norm ) which corresponds to the sum of squared errors : @xmath11 where we assume that @xmath12 variables are observed .",
    "the cost function maps a @xmath4-dimensional parameter vector onto its corresponding error , thus quantifying how far the data and the model predictions are for that particular parameter set .",
    "the aim of the data fitting procedure is to find the parameter vector @xmath13 that minimises the error globally subject to restrictions dictated by the problem of interest : @xmath14      * _ data set : _",
    "@xmath15 , a set of @xmath5 observations , as defined in eq .  ( [ eq : data ] ) . *",
    "_ parameter set : _ @xmath16 \\in { \\mathbb{r}}^n_{+}$ ] . due to the nature of the models considered , @xmath17 . *",
    "_ objective function : _",
    "@xmath18 , the error function to be minimised , as defined in eq .",
    "( [ eq : obj - fun - euclidean ] ) . * _ set of local minima of : _",
    "@xmath19 where @xmath20 is a neighbourhood of @xmath21 . *",
    "_ global minimum of : _ @xmath13 , a parameter set such that @xmath22 , @xmath23 . clearly , @xmath24 . * _ local minimisation mapping : _ .",
    "local minimisation maps @xmath25 onto a local minimum : . *",
    "_ ranking and culling of local minima _ : @xmath26",
    ". this operation ranks @xmath27 parameter sets and selects the @xmath28 parameter sets with the lowest @xmath29 . *",
    "_ joint probability distributions of the parameters at iteration @xmath30 : _",
    "@xmath31 ( prior ) and @xmath32 ( posterior ) . *",
    "_ marginal probability distribution of the @xmath33 component of @xmath25 _ : for instance , @xmath34 * _ historical prior at iteration @xmath30 : _ @xmath35 where @xmath36 here @xmath37 is a uniform distribution with support in @xmath38 $ ] and @xmath39 is the union of the supports of @xmath40 and @xmath41 . *",
    "_ update of the prior at iteration @xmath30 : _",
    "@xmath42 with @xmath43 that is , a convex mixture of the posterior and the historical prior with weight @xmath44 .",
    "* _ re - population : _ obtain population of @xmath27 random points simulated from the prior @xmath45 . *",
    "_ convergence criterion for the error : _ the difference between the means of the errors of the posteriors in consecutive iterations is smaller than the pre - determined tolerance : @xmath46 * _ convergence criterion for the empirical distributions : _ the samples of the posteriors in consecutive iterations are indistinguishable at the 5% significance level according to the nonparametric mann - whitney rank sum test : @xmath47          set running parameters of algorithm : @xmath48 , @xmath49 $ ] , _ tol _ choose initial priors @xmath50 and @xmath51 . set @xmath52 and @xmath53 .",
    "let @xmath54 .",
    "simulate @xmath27 points from @xmath45 through re - population .",
    "obtain local minimum @xmath55 . store the pair @xmath56 $ ] in @xmath57 .",
    "rank and cull the set of local minima : @xmath58 define the posterior @xmath32 from the sample @xmath57 .",
    "update @xmath59 from @xmath60 and @xmath32 .",
    "update the prior @xmath61 .",
    "@xmath62 .",
    "algorithm  [ alg : method ] presents the pseudo - code for our method using the definitions above .",
    "the iterations produce progressively more refined distributions of the parameter vector . at each iteration",
    "@xmath30 , a population simulated from the prior distribution @xmath45 is locally minimised followed by ranking and culling of the local minima to create a posterior distribution @xmath63 ( squeeze step ) .",
    "this distribution is then combined with an encompassing historical prior to generate the updated prior @xmath64 ( breathe step ) .",
    "the iteration loop terminates when the difference between the mean errors of consecutive posteriors is smaller than the tolerance and the samples of the posteriors are indistinguishable .",
    "we now explain these steps in detail ( fig .",
    "[ fig : bpm - panel ] ) through the bpm model ( see sec .",
    "[ sec : bpm ] ) .    1 .",
    "_ formulation of the optimisation : _ the data set @xmath15 and the model equations parameterised by @xmath25 allow us to define an error function @xmath18 whose global minimum corresponds to the best model . + in our illustrative example , the bpm model  ( [ eq : bpm - syst ] ) has the parameter vector @xmath65 $ ] and the error function is depicted in fig .",
    "[ fig : bpm - panel]a .",
    "the global optimisation on the rugged landscape of this function is computationally hard .",
    "initialisation : _ * set the running parameters of the algorithm : the size of the simulated population , @xmath27 ; the size of the surviving population after culling , @xmath28 ; the update probability , @xmath44 ; and the tolerance , @xmath66 .",
    "+ in this example , @xmath67 , @xmath68 , @xmath69 and @xmath70 .",
    "* choose @xmath50 , the initial prior distribution of the parameter vector .",
    "+ in this case , we take @xmath71 and @xmath72 to be independent and uniformly distributed : @xmath73 .",
    "* initialise @xmath74 , the historical prior of the parameters .",
    "* simulate @xmath27 points from @xmath75 to generate the initial sample @xmath76 .",
    "iteration ( step @xmath30 ) : _ repeated until termination criterion is satisfied .",
    "figure  [ fig : bpm - panel ] shows the first iteration of our method applied to the bpm example . 1 .",
    "_ local minimisation : _ apply local minimisation to the simulated parameters from the prior @xmath77 and map them onto local minima of @xmath18 to generate @xmath78 .",
    "+ here we use the nelder - mead simplex method @xcite , though others can be used .",
    "figure  [ fig : bpm - panel]b shows the simulated points from @xmath75 ( grey squares ) and its corresponding histograms ( in grey ) . after local minimisation , this sample is mapped onto the dark blue triangles in fig .",
    "[ fig : bpm - panel]b ( histograms in dark blue ) .",
    "note how the local minima align with the level curves of @xmath29 with a markedly different distribution to the uniform prior .",
    "note also that many of the optimised values of @xmath71 lie outside the range of the prior @xmath79 and are now distributed over the interval @xmath80 . on the other hand ,",
    "the values of @xmath72 have collapsed inside @xmath81 .",
    "2 .   _ ranking and culling : _ rank the @xmath82 local minima from the @xmath83 and @xmath30 iterations , select the @xmath28 points with the lowest @xmath29 and cull ( discard ) the rest : @xmath84 denote the best parameter vector of this set as @xmath85 .",
    "we consider @xmath86 to be a sample from the optimised ( ` posterior ' ) distribution , @xmath32 .",
    "+ the @xmath68 best parameter sets are shown ( light blue squares ) in fig .",
    "[ fig : bpm - panel]c ( light blue histograms ) .",
    "_ termination criterion : _ check that the difference between the mean errors of the consecutive optimised samples is smaller than the tolerance : @xmath87 .",
    "we also gauge the ` convergence ' of the posteriors through the mann - whitney ( mw ) test to determine if the samples from consecutive posteriors are distinguishable : @xmath88 where @xmath89 is a @xmath90-@xmath91 flag .",
    "the mw test gives additional information about the change of the optimised posteriors from one iteration to the next .",
    "+ figure  [ fig : bpm - panel]d shows the convergence check for the first iteration of the bpm model : ( i )  top , errors of the sampled prior ( grey , left ) with errors of the local minima ( dark blue , right ) and the @xmath28 surviving points ( light blue ) ; ( ii )  bottom , histograms of the prior ( grey ) and the posterior ( light blue ) .",
    "clearly , in this iteration neither the error nor the distributions have converged so the algorithm does not stop .",
    "update of historical prior and generation of new sample : _ if convergence is not achieved , update the historical prior @xmath59 as a uniform distribution over the union of the supports of the existing historical prior and the calculated posterior  ( [ eq : hist - prior - unif ] ) .",
    "equivalently , the support of the historical prior extends over the union of the sequence of all historical priors @xmath92 and of all posteriors @xmath93 . + as shown in fig .",
    "[ fig : bpm - panel]e for the bpm example , the marginal of the historical prior for @xmath71 is expanded to @xmath94 , since the optimised parameter sets have reached values as high as 200 .",
    "meanwhile , the @xmath72 marginal of the historical prior remains unchanged as @xmath95 because there has been no expansion of the support .",
    "+ the historical prior is used to mutate the updated prior before the next iteration by constructing a weighted mixture of the posterior and the historical prior with weight @xmath44 , as shown in  ( [ eq : update - prior ] ) .",
    "we re - populate from this updated prior by simulating from the posterior with probability @xmath96 and from the historical prior with probability @xmath97 to generate the new sample @xmath98 and iterate back .",
    "+ figure  [ fig : bpm - panel]e shows the sample of @xmath27 points simulated from the new prior .",
    "the @xmath71-components of most points are between 100 and 200 and the @xmath72-components are between 0.1 and 1.0 , but there are a few that lie outside the support of the posterior .",
    "the process in panels b.  c ,  d , and  e of fig .",
    "[ fig : bpm - panel ] is iterated for this new set of points .",
    "_ output of the algorithm : _ when the convergence criteria have been met , the iteration stops at iteration @xmath99 and the last @xmath100 is presented as the optimal parameter set for the model .",
    "we can also examine the sequence of optimised parameter distributions @xmath101 obtained for all iterations ( fig .",
    "[ fig : bpm - panel]f ) .",
    "we apply our algorithm to three biological examples of interest . the first two correspond to simulated data from models in the literature , while in the third example we apply our algorithm to unpublished experimental data of the dynamical response of an inducible genetic promoter constructed for an application in synthetic biology .",
    "170mmx|xxxxxx & min .",
    "& & & conv . & conv . & + @xmath30 & error & @xmath102 & @xmath103 & @xmath104 & @xmath105 & @xmath106 + 1 & 56.0941 & 193.7447 & 0.1304 & - & - & - + 2 & 28.2735 & 246.7510 & 0.1528 & no & no &",
    "133.9020 + 3 & 27.2083 & 248.7557 & 0.1532 & no & no & 6.8542 + 4 & 26.9838 & 250.3593 & 0.1536 & no & no & 0.6532 + 5 & 26.6504 & 251.7189 & 0.1538 & no & no & 0.3281 + 6 & 26.6504 & 251.7189 & 0.1538 & no & no & 0.1963 + 7 & 26.6504 & 251.7189 & 0.1538 & yes & yes & 0.0118 + 8 & 26.6504 & 251.7189 & 0.1538 & no & no & 0.0131 + 9 & 26.6504 & 251.7189 & 0.1538 & yes & yes & @xmath107 +    the bliss - painter - marr ( bpm ) model @xcite describes the behaviour of a gene - enzyme - product control unit with a negative feedback loop : @xmath108 here , @xmath109 and @xmath110 are the concentrations ( in arbitrary units ) of mrna , enzyme and product , respectively .",
    "the degradation rate of the product has an explicit time dependence , which in this case has the form of a ramp saturation : @xmath111 the model represents a gene that codes for an enzyme which in turn catalyses a product that inhibits the transcription of the gene .",
    "this self - inhibition can lead to oscillations , which have been shown to occur in the tryptophan operon in _",
    "e.  coli _ @xcite .",
    "we construct a data set from simulations of this model with @xmath112= [ 240 , 0.15]$ ] and initial conditions @xmath113 .",
    "the data set @xmath15 consists of 10 measurements of @xmath114 at particular times with added gaussian noise drawn from @xmath115 ( table  [ tab : bpm - data ] ) . the error function @xmath18  ( [ eq : obj - fun - euclidean ] ) corresponds to a non - convex optimisation landscape : a complex rugged surface with many local minima making global optimisation hard ( fig .  [ fig : bpm - panel]a ) .",
    "we use algorithm  [ alg : method ] to estimate the ` unknown ' parameter values from the ` measurements ' of @xmath116 , as illustrated in sec .",
    "[ sec : algorithm ] and fig .",
    "[ fig : bpm - panel ] .",
    "feigning ignorance of the true values , we choose a uniform prior distribution with range @xmath117 $ ] for both parameters : the rest of the paramters are set to : @xmath67 , @xmath68 , @xmath69 and @xmath70 .",
    "note that the _ true _ value of @xmath71 falls outside of the assumed range of our initial prior , while the range of @xmath72 in our initial prior is two orders of magnitude larger than its true value .",
    "this level of uncertainty about parameter values is typical in data fitting for biological models .",
    "figure  [ fig : bpm - panel ] highlights a key aspect of our algorithm : the local minimisation can lead to local minima outside of the range of the initial prior .",
    "furthermore , our definition of the historical prior ensures that successive iterations can find solutions within the largest hypercube of optimised solutions in parameter space . in this example , the algorithm moves away from the @xmath95 prior for @xmath71 and finds a distribution around 240 ( the true value ) after three iterations , while in the case of @xmath72 , the distribution collapses to values around 0.15 after one iteration .",
    "although the algorithm finds the minimum @xmath118 after 5 iterations , the algorithm is terminated after 9 iterations , when the posterior distributions are similar ( according to the mw test ) and the mean errors have also converged ( table  [ tab : bpm - results ] ) .",
    "the estimated parameters for this noisy data set are @xmath119 $ ] . in fact",
    ", the error of the estimated parameter set is lower than that of the real parameters : @xmath120 , due to the noise introduced in the data .",
    "when a data set without noise is used , the algorithm finds the true value of the parameters to 9 significant digits ( not shown ) .",
    "susceptible - infected - recovered ( sir ) models are widely used in epidemiology to describe the evolution of an infection in a population @xcite . in its simplest form , the sir model has three variables : the susceptible population @xmath121 , the infected population @xmath122 and the recovered population @xmath116 : @xmath123 the first equation describes the change in the susceptible population , growing with birth rate @xmath71 and decreasing by the rate of infection @xmath124 and the rate of death @xmath125 .",
    "the infected population grows by the rate of infection @xmath124 and decreases by the rate of recovery @xmath126 and the rate of death @xmath127 .",
    "the recovered population grows by the rate of recovery @xmath126 and decreases by the death rate @xmath128 .",
    "here we use the same form of the equations as @xcite .    the data generated from the model  ( [ eq : sir - model ] ) ( see table  [ tab : sir - data ] )",
    "was obtained directly from @xcite .",
    "hence the original parameter values were not known to us and further we assumed the initial conditions also to be unknown and fitted them as parameters .",
    "we used algorithm  [ alg : method ] to estimate @xmath71 , @xmath129 , @xmath130 , and @xmath1 and initial conditions @xmath131 , @xmath132 , and @xmath133 . the prior marginal distributions for all parameters were set as @xmath95 .",
    "the other parameters were set to : @xmath134 , @xmath68 , @xmath96 and @xmath70 .",
    "the algorithm converged after six iterations .",
    "figure  [ fig : sir - panel]a shows the prediction of the model  ( [ eq : sir - model ] ) with the best parameters estimated by our algorithm .",
    "the fit is good with little difference between the curves obtained using the real initial conditions and the ones estimated by our method .",
    "the posterior distributions after six iterations of the algorithm are shown on fig .",
    "[ fig : sir - panel]b .",
    "the errors obtained after each local minimisation in a decreasing order on each iteration are shown on a semilogarithmic scale in fig .",
    "[ fig : sir - panel]c .",
    "we can observe how the errors decrease several orders of magnitude over the first three iterations and converge steadily during the last three iterations until @xmath135 .",
    ", a negatively regulated environment - responsive promoter .",
    "the repressor r@xmath136 promoted by @xmath110 regulates @xmath137 .",
    "the switch is responsive to an exogenous inducer @xmath138 , which binds to r@xmath136 to relieve its repression on @xmath137 and to turn on the transcription of the downstream target gene , such as a _",
    "gfp_. the ribosome binding site ( rbs ) is used to tune the translation efficiency of the downstream gene . _ plot",
    "_ : fluorescent response of the switch with _",
    "gfp_-34 to different doses of iptg ( circles ) .",
    "stationary solutions of eq .",
    "( [ eq : gfp ] ) using the parameters obtained with algorithm  [ alg : method ] ( solid lines ) . *",
    "b * : time course of the fluorescent response of the switch with _",
    "gfp_-34 to several doses of iptg ( circles ) and time - dependent solutions of eq .",
    "( [ eq : gfp ] ) using the parameters obtained with algorithm  [ alg : method ] ( solid lines ) .",
    "similarly good fits were obtained for responses to ( not shown ) . ]",
    "the use of inducible genetic switches is widespread in synthetic biology and bioengineering as building blocks for more complicated gene circuit architectures .",
    "an example is shown schematically in the inset of fig .",
    "[ fig : gfp - panel]a .",
    "this environment - responsive switch is used to control the expression of a target gene @xmath139 ( usually tagged with green fluorescent protein or _ gfp _ ) through the addition of an exogenous small molecule @xmath138 ( e.g. , isopropyl thiogalactopyranoside or iptg ) .",
    "the input - output behaviour of this system can be described by the following ordinary differential equation @xcite : @xmath140 here , @xmath141 is the basal activity of the promoter @xmath137 and @xmath142 is the linear degradation term . the second term is a hill function that models the cooperative transcription activation in response to the inducer @xmath138 with maximum expression rate @xmath143 , constant  @xmath144 and hill coefficient  @xmath145 .    the @xmath146@xmath147 switch has been characterised experimentally in response to different doses of iptg in @xcite .",
    "equation  ( [ eq : gfp ] ) can be solved explicitly and one can use nonlinear least squares and the analytical solution to fit data at stationarity ( i.e. , at long times ) and estimate @xmath71 , @xmath145 , @xmath144 , and the ratio @xmath148 .",
    "these estimates have been obtained assuming equilibrium ( ) and initial condition @xmath149 by @xcite ( table  [ tab : gfp - data ] ) .",
    "in fact , the experiments measured time series of the expression of @xmath139 every 20 minutes from for different doses of inducer @xmath150 mm , with two different reporters ( _ gfp_-30 and _ gfp_-34 ) . see tables  [ tab : gfp30-data ] and  [ tab : gfp34-data ] instead of assuming equilibrium and using only the data for @xmath151 min as done previously @xcite , we apply algorithm  [ alg : method ] to all the data with the full dynamical equation  ( [ eq : gfp ] ) to estimate . in this case , we used initial priors @xmath152 for @xmath71 and @xmath145 ; and @xmath153 for @xmath143 , @xmath144 and @xmath1 .",
    "the other parameters were set to : @xmath134 , @xmath68 , @xmath96 , and @xmath70 .",
    "our algorithm converged after five iterations to the parameter values in table  [ tab : gfp - data ] .",
    "the parameter estimates provide good fits to both the time courses ( fig .",
    "[ fig : gfp - panel]b ) and to the dose response data ( fig .",
    "[ fig : gfp - panel]a ) .",
    "the values of @xmath154 and @xmath155 obtained here are similar those obtained in @xcite by using only stationary data .",
    "this is reassuring since these parameters are related to the dose threshold to half maximal response and to the steepness of the sigmoidal response , both static properties . on the other hand ,",
    "the values of @xmath71 and the ratio @xmath148 differ to some extent due to the ( imperfect ) assumption in @xcite that steady state had been reached at @xmath156 min . as fig .",
    "[ fig : gfp - panel]b shows , @xmath139 is not at steady state then .",
    "hence the parameter values obtained with our method should give a more faithful representation of the true dynamical response of the switch ."
  ],
  "abstract_text": [
    "<S> * motivation : * estimating parameters from data is a key stage of the modelling process , particularly in biological systems where many parameters need to be estimated from sparse and noisy data sets . over the years , a variety of heuristics have been proposed to solve this complex optimisation problem , with good results in some cases yet with limitations in the biological setting . + </S>",
    "<S> * results : * in this work , we develop an algorithm for model parameter fitting that combines ideas from evolutionary algorithms , sequential monte carlo and direct search optimisation . </S>",
    "<S> our method performs well even when the order of magnitude and/or the range of the parameters is unknown . </S>",
    "<S> the method refines iteratively a sequence of parameter distributions through local optimisation combined with partial resampling from a historical prior defined over the support of all previous iterations . </S>",
    "<S> we exemplify our method with biological models using both simulated and real experimental data and estimate the parameters efficiently even in the absence of _ a priori _ knowledge about the parameters . + * availability : * matlab code available from the authors upon request . </S>"
  ]
}