{
  "article_text": [
    "the problem of predicting a density function for future observation is an important field in practical applications of statistical methodology .",
    "since predictive density estimation has been revealed to be parallel to shrinkage estimation for location parameter , it has extensively been studied in the literature .",
    "particularly , the bayesian prediction for a multivariate ( vector - valued ) normal distribution has been developed by komaki ( 2001 ) , george et al .",
    "( 2006 ) and brown et al .",
    "see george et al .",
    "( 2012 ) for a broad survey including a clear explanation of parallelism between density prediction and shrinkage estimation .",
    "this paper addresses bayesian predictive density estimation for a matrix - variate normal distribution .",
    "denote by @xmath0 the @xmath1 matrix - variate normal distribution with mean matrix @xmath2 and positive definite covariance matrix @xmath3 , where @xmath2 , @xmath4 and @xmath5 are , respectively , @xmath1 , @xmath6 and @xmath7 matrices of parameters and @xmath3 represents the kronecker product of the positive definite matrices @xmath4 and @xmath5 .",
    "let @xmath8 be the transpose of a matrix @xmath9 and let @xmath10 and @xmath11 be , respectively , the trace and the determinant a square matrix @xmath9 . also , let @xmath12 be the inverse of a nonsingular matrix @xmath9 .",
    "if an @xmath1 random matrix @xmath13 is distributed as @xmath0 , then @xmath13 has density of the form @xmath14.\\ ] ] for more details of matrix - variate normal distribution , see muirhead ( 1982 ) and gupta and nagar ( 1999 ) .",
    "it is assumed in this paper that the covariance matrix of a matrix - variate normal distribution is known .",
    "then the prediction problem is more precisely formulated as follows : let @xmath15 and @xmath16 , where @xmath17 is a common @xmath18 matrix of unknown parameters , @xmath19 and @xmath20 are known positive values and @xmath21 stands for the identity matrix of order @xmath22 .",
    "assume that @xmath23 and @xmath24 and @xmath25 are independent .",
    "let @xmath26 and @xmath27 be the densities of @xmath24 and @xmath25 , respectively . consider here the problem of estimating @xmath27 based only on the observed @xmath24",
    ". denote by @xmath28 an estimated density for @xmath27 and hereinafter @xmath29 is referred to as a predictive density of @xmath25 .",
    "define the kullback - leibler ( kl ) loss as @xmath30 \\non\\\\ & = \\int_{\\re^{r\\times q } } p(y\\mid \\th)\\log { p(y\\mid\\th)\\over\\ph(y\\mid x)}\\dd y.\\end{aligned}\\ ] ] the performance of a predictive density @xmath29 is evaluated by the risk function with respect to the kl loss ( [ eqn : loss ] ) , @xmath31\\\\ & = \\int_{\\re^{r\\times q}}\\int_{\\re^{r\\times q}}p(x\\mid\\th)p(y\\mid\\th)\\log { p(y\\mid\\th)\\over\\ph(y\\mid x)}\\dd y\\dd x.\\end{aligned}\\ ] ]",
    "let @xmath32 be a proper / improper density of prior distribution for @xmath17 , where we assume that the marginal density of @xmath24 , @xmath33 is finite for all @xmath34 . denote the frobenius norm of a matrix @xmath9 by @xmath35 .",
    "let @xmath36 note that @xmath37 is finite if @xmath38 is finite . here",
    "@xmath37 can be rewritten as @xmath39 where @xmath40 and @xmath41 with @xmath42 . from aitchison ( 1975 ) , a bayesian predictive density relative to the kl loss ( [ eqn : loss ] ) is given by @xmath43 see george et al .",
    "( 2006 , lemma 2 ) for the multivariate ( vector - valued ) normal case .",
    "it is noted that @xmath44 is the bayesian predictive density with respect to the uniform prior @xmath45 . under the predictive density estimation problem",
    "relative to the kl loss ( [ eqn : loss ] ) , @xmath44 is the best invariant predictive density with respect to a location group . using the same arguments as in george et al .",
    "( 2006 , corollary 1 ) gives that , for any @xmath22 and @xmath46 , @xmath44 is minimax relative to the kl loss ( [ eqn : loss ] ) and has a constant risk .",
    "recently , matsuda and komaki ( 2015 ) constructed an improved bayesian predictive density on @xmath44 by using a prior density of the form @xmath47 the prior ( [ eqn : pr_em ] ) is interpreted as an extension of stein s ( 1973 , 1981 ) harmonic prior @xmath48 in the context of bayesian estimation for mean matrix , ( [ eqn : pr_em ] ) yields a matricial shrinkage estimator , while ( [ eqn : pr_js ] ) does a scalar shrinkage one .",
    "note that , when @xmath49 , typical examples of the matricial and the scalar shrinkage estimators for @xmath17 are , respectively , the efron - morris ( 1972 ) estimator @xmath50 and the james - stein ( 1961 ) like estimator @xmath51 the two estimators @xmath52 and @xmath53 are minimax relative to a quadratic loss .",
    "also , @xmath52 and @xmath53 are characterized as empirical bayes estimators , but they are not generalized bayes estimators which minimize the posterior expected quadratic loss .",
    "the purposes of this paper are to construct some bayesian predictive densities with different priors from ( [ eqn : pr_em ] ) and ( [ eqn : pr_js ] ) and to discuss their decision - theoretic properties such as admissibility and minimaxity .",
    "section [ sec : preliminaries ] first lists some results on the kullback - leibler risk and the differentiation operators .",
    "section [ sec : properminimax ] applies an extended faith s ( 1978 ) prior to our predictive density estimation problem and provides sufficient conditions for minimaxity of the resulting bayesian predictive densities .",
    "also , an admissible and minimax predictive density is obtained by considering a proper hierarchical prior . in section [ sec : superharmonic ] , we utilize stein s ( 1973 , 1981 ) ideas for deriving some minimax predictive densities with superharmonic priors . section [ sec : mcstudies ] investigates numerical performance in risk of some bayesian minimax predictive densities .",
    "first , we state some useful lemmas in terms of the kullback - leibler ( kl ) risk .",
    "the lemmas are based on stein ( 1973 , 1981 ) , george et al .",
    "( 2006 ) and brown et al .",
    "( 2008 ) and play important roles in studying decision - theoretic properties of a bayesian predictive density .    from george et al .",
    "( 2006 , lemma 3 ) , we observe that @xmath54 for all @xmath55 if @xmath56 for all @xmath34 .",
    "note also that @xmath57 and @xmath58 namely , the mean of a predictive distribution for @xmath25 is the same as the posterior mean of @xmath17 given @xmath24 or , equivalently , the generalized bayes estimator relative to a quadratic loss for a mean of @xmath24",
    ".    hereafter denote by @xmath59 a density of @xmath60 with a positive value @xmath61 . in order to prove minimaxity of a bayesian predictive density , we require the following lemma , which implies that our bayesian prediction problem can be reduced to the bayesian estimation problem for the normal mean matrix relative to a quadratic loss .    [ lem : identity ] the kl risk difference between @xmath44 and @xmath62",
    "can be written as @xmath63-\\er^{w|\\th}[\\vert\\thh_\\pi-\\th\\vert^2]\\}\\dd v,\\ ] ] where @xmath64 stands for expectation with respect to @xmath65 and @xmath66    * proof . *   this",
    "is verified by the same arguments as in brown et al .",
    "( 2008 , theorem 1 and its proof ) .",
    "@xmath67    let @xmath68 be an @xmath18 matrix of differentiation operators with respect to an @xmath18 matrix @xmath69 of full row rank . for a scalar function @xmath70 of @xmath65 ,",
    "the operation @xmath71 is defined as an @xmath18 matrix whose @xmath72-th element is @xmath73 .",
    "also , for a @xmath74 matrix - valued function @xmath75 of @xmath65 , the operation @xmath76 are defined as an @xmath77 matrix whose @xmath72-th element of @xmath76 is @xmath78 .",
    "stein ( 1973 ) showed that for a @xmath79 matrix @xmath80 @xmath81 namely , @xmath82",
    "= v \\er^{w|\\th}[\\tr\\{\\nabla_wg(w)\\}].\\ ] ] this identity is referred to as the stein identity in the literature . using the stein identity ,",
    "we can easily obtain the following lemma .",
    "[ lem : identity2 ] use the same notation as in lemma [ lem : identity ] .",
    "then we obtain @xmath83}{m_\\pi(w;v)}-\\frac{\\vert\\nabla_w m_\\pi(w;v)\\vert^2}{\\{m_\\pi(w;v)\\}^2}\\bigg]\\dd v.\\end{aligned}\\ ] ]    * proof .",
    "*   this lemma can be shown by the same arguments as in stein ( 1973 , 1981 ) .",
    "we provide only an outline of proof .    note from brown ( 1971 ) that @xmath84 , given in lemma [ lem : identity ] , can be represented as @xmath85 by some manipulation after using the stein identity , we have @xmath86-\\er^{w|\\th}[\\vert\\thh_\\pi-\\th\\vert^2]\\ } \\\\ & = -v\\er^{w|\\th}\\bigg[2\\frac{\\tr[\\nabla_w\\nabla_w^\\top m_\\pi(w;v)]}{m_\\pi(w;v)}-\\frac{\\vert\\nabla_w m_\\pi(w;v)\\vert^2]}{\\{m_\\pi(w;v)\\}^2}\\bigg].\\end{aligned}\\ ] ] combining this identity and lemma [ lem : identity ] completes the proof .",
    "@xmath67    using lemma [ lem : identity2 ] immediately establishes the following proposition .",
    "[ prp : cond_mini ] @xmath87 is minimax relative to the kl loss ( [ eqn : loss ] ) if @xmath88-\\frac{\\vert\\nabla_w m_\\pi(w;v)\\vert^2}{m_\\pi(w;v)}\\leq 0\\ ] ] for @xmath89 .",
    "next , some useful formulae are listed for differentiation with respect to a symmetric matrix .",
    "the formulae are applied to evaluation of the kullback - leibler risks of our bayesian predictive densities .",
    "let @xmath90 be an @xmath91 symmetric matrix of full rank .",
    "let @xmath92 be an @xmath91 symmetric matrix of differentiation operators with respect to @xmath93 , where the @xmath72-th element of @xmath92 is @xmath94 with the kronecker delta @xmath95 .",
    "let @xmath96 be a scalar - valued and differentiable function of @xmath90 .",
    "also let @xmath97 be an @xmath91 matrix , where all the elements @xmath98 are differentiable functions of @xmath93 .",
    "the operations @xmath99 and @xmath100 are , respectively , @xmath91 matrices , where the @xmath72-th elements of @xmath99 and @xmath100 are defined as , respectively , @xmath101    first , the product rule in terms of @xmath92 is expressed in the following lemma due to haff ( 1982 ) .",
    "[ lem : diff1 ] let @xmath102 and @xmath103 be @xmath91 matrices such that all the elements of @xmath102 and @xmath103 are differentiable functions of @xmath93 .",
    "then we have @xmath104 in particular , for differentiable scalar - valued functions @xmath105 and @xmath106 , @xmath107    denote by @xmath108 the eigenvalue decomposition of @xmath93 , where @xmath109 is an orthogonal matrix of order @xmath22 and @xmath110 is a diagonal matrix of order @xmath22 with @xmath111 .",
    "the following lemma is provided by stein ( 1973 ) .",
    "[ lem : diff2 ] define @xmath112 , whose diagonal elements are differentiable functions of @xmath113 .",
    "then we obtain    1 .",
    "@xmath114  @xmath115 , 2 .",
    "@xmath116 , where @xmath117 with @xmath118    [ lem : diff3 ] let @xmath119 and @xmath120 be constants and let @xmath121 be a symmetric constant matrix @xmath121 .",
    "then it holds that    1 .",
    "@xmath122 , 2 .",
    "@xmath123 , 3 .   @xmath124 .",
    "4 .   @xmath125 if @xmath126 is nonsingular .    * proof . *   for proofs of parts ( i ) , ( ii ) and",
    "( iii ) , see haff ( 1982 ) and magnus and neudecker ( 1999 ) . using ( i ) of lemma [ lem : diff2 ] gives that @xmath127 which implies part ( iv ) .",
    "let @xmath128 be the same @xmath18 differentiation operator matrix as in the preceding subsection .",
    "if @xmath129 , then we have the following lemma , where the proof is referred to in konno ( 1992 ) .",
    "[ lem : diff4 ] let @xmath130 be an @xmath91 symmetric matrix , where all the elements of @xmath130 are differentiable function of @xmath129 .",
    "then it holds that    1 .",
    "@xmath131 , 2 .",
    "in this section , we consider a class of hierarchical priors inspired by faith ( 1978 ) and derive a sufficient condition for minimaxity of the resulting bayesian predictive density .",
    "also , a proper bayes and minimax predictive density is provided .",
    "let @xmath133 be the set of @xmath91 symmetric matrices .",
    "for @xmath9 and @xmath134 , write @xmath135 or @xmath136 if @xmath137 is a positive ( semi-)definite matrix .",
    "the set @xmath138 is defined as @xmath139 where @xmath140 is the @xmath91 zero matrix .",
    "denote the boundary of @xmath138 by @xmath141 .",
    "it is noted that if @xmath142 then @xmath143 and also then @xmath144 or @xmath145 .    consider a proper / improper hierarchical prior @xmath146",
    "the priors @xmath147 and @xmath148 are specified as follows : assume that a prior distribution of @xmath17 given @xmath149 is @xmath150 , where @xmath151 is a known constant satisfying @xmath152 then the first - stage prior density @xmath147 can be written as @xmath153.\\ ] ] assume also that @xmath148 , a second - stage prior density for @xmath149 , is a differentiable function on @xmath138 .",
    "denote by @xmath154 the resulting bayesian predictive density with respect to the hierarchical prior @xmath155 .",
    "assume that a marginal density of @xmath65 with respect to @xmath155 is finite when @xmath156 .",
    "the marginal density is given by @xmath157 where @xmath158 is a posterior density of @xmath17 given @xmath149 and @xmath65 .",
    "to make it easy to derive sufficient conditions that @xmath159 is minimax , we show the following lemma .",
    "[ lem : alter_m(w ) ] the marginal density @xmath160 can alternatively be represented as @xmath161 where @xmath162\\ ] ] with @xmath163.\\ ] ]    * proof .",
    "*   let @xmath164 where @xmath165 .",
    "since @xmath166 , we observe that @xmath167   + \\frac{1}{v}\\tr(\\la ww^\\top),\\end{aligned}\\ ] ] so @xmath168 is proportional to @xmath169\\big],\\ ] ] namely , @xmath170 . integrating out ( [ eqn : m(w ) ] ) with respect to @xmath17",
    "gives that @xmath171\\dd\\om.\\ ] ] note that @xmath172 and the jacobian of the transformation from @xmath149 to @xmath173 is given by @xmath174=v_1^{r(r+1)/2}|v_1i_r+(1-v_1)\\la)|^{-r-1}.\\ ] ] hence making the transformation from @xmath149 to @xmath173 for ( [ eqn : m(w)-1 ] ) completes the proof .",
    "let @xmath175 be an @xmath91 symmetric matrix of differentiation operators with respect to @xmath176 , where the @xmath72-th element of @xmath175 is @xmath177 proposition [ prp : cond_mini ] and lemma [ lem : alter_m(w ) ] are utilized to get sufficient conditions for minimaxity of @xmath159 .",
    "[ thm : faith ] let @xmath178 and @xmath179 be defined as in lemma [ lem : alter_m(w ) ] .",
    "let @xmath180 assume that @xmath181 then @xmath159 is minimax relative to the kl loss @xmath182 if @xmath183 , where @xmath184 with @xmath185 provided all the integrals are finite .",
    "* proof . *   from proposition",
    "[ prp : cond_mini ] , @xmath159 is minimax when @xmath186-\\frac{\\vert\\nabla_w m(w)\\vert^2}{m(w)}\\leq 0.\\ ] ]    it is seen from lemma [ lem : alter_m(w ) ] that @xmath187 and @xmath188 hence we obtain @xmath189,\\ ] ] where @xmath190f_\\pi(\\la;w)\\dd\\la,\\\\ e_2(w)&=\\frac{1}{v}\\tr\\bigg[ww^\\top\\bigg\\{\\int_{\\rc_r}\\la f_\\pi(\\la;w)\\dd\\la\\bigg\\}^2\\bigg ] \\\\ & = \\frac{1}{v}\\int_{\\rc_r}\\tr(mww^\\top\\la)f_\\pi(\\la;w)\\dd\\la.\\end{aligned}\\ ] ]    using lemmas [ lem : diff1 ] and [ lem : diff3 ] yields that @xmath191f_\\pi(\\la;w),\\ ] ] so that @xmath192&=\\tr[\\la^2\\dc_\\la f_\\pi(\\la;w)]+f_\\pi(\\la;w)\\tr[\\dc_\\la\\la^2]\\\\ & = \\frac{1}{2}\\big[\\frac{2}{\\pi_2^j(\\la)}\\tr[\\la^2\\dc_\\la \\pi_2^j(\\la)]-\\big\\{\\frac{1}{v}\\tr(ww^\\top\\la^2)-q\\tr\\la\\big\\ } \\\\ & \\qquad + 2(r+1)\\tr\\la \\big]f_\\pi(\\la;w).\\end{aligned}\\ ] ] thus @xmath193 can be expressed as @xmath194\\dd\\la.\\end{aligned}\\ ] ]    similarly , we observe that from lemmas [ lem : diff1 ] and [ lem : diff3 ] @xmath195 & = \\tr[\\la m\\dc_\\la f_\\pi(\\la;w)]+f_\\pi(\\la;w)\\tr[m\\dc_\\la\\la]\\\\ & = \\frac{1}{2}\\big[(q+r+1)\\tr m+\\frac{2}{\\pi_2^j(\\la)}\\tr[\\la m\\dc_\\la \\pi_2^j(\\la ) ] \\\\ & \\qquad -\\frac{1}{v}\\tr(mww^\\top\\la)\\big]f_\\pi(\\la;w),\\end{aligned}\\ ] ] which leads to @xmath196\\dd\\la .\\end{aligned}\\ ] ] combining ( [ eqn : d2_mw1 ] ) , ( [ eqn : e1 ] ) and ( [ eqn : e2 ] ) gives that @xmath197\\dd\\la \\non\\\\ & \\qquad   + \\frac{2}{vm(w)}\\int_{\\rc_r}\\tr[\\dc_\\la\\{f_\\pi(\\la;w)\\la\\}m]\\dd\\la.\\end{aligned}\\ ] ] if we can show that two integrals in ( [ eqn : d2_mw2 ] ) are , respectively , equal to zero , then the proof is complete .",
    "let @xmath198 be an @xmath91 symmetric matrix such that all the elements of @xmath130 are differentiable functions of @xmath199 .",
    "denote @xmath200 which is a @xmath201-dimensional column vector .",
    "denote an outward unit normal vector at a point @xmath173 on @xmath141 by @xmath202 if @xmath203 is integrable on @xmath138 then it is seen that @xmath204 by symmetry of @xmath173 and @xmath130 . from the gauss divergence theorem",
    ", we obtain @xmath205 where @xmath206 stands for lebesgue measure on @xmath141 .",
    "note that @xmath195=\\tr[\\dc_\\la\\{f_\\pi(\\la;w)\\la m\\}]=\\tr[\\dc_\\la\\{f_\\pi(\\la;w)m\\la\\}]\\end{aligned}\\ ] ] because @xmath207 is symmetric and does not depend on @xmath173 .",
    "it is observed that @xmath208 and @xmath209 are symmetric for @xmath199 , so that @xmath210\\dd\\la=\\int_{\\partial\\rc_r}\\nu^\\top\\vec(\\la^2)f_\\pi(\\la;w)\\dd\\si,\\ ] ] and @xmath211\\dd\\la \\non\\\\ & = \\frac{1}{2}\\int_{\\rc_r}\\tr[\\dc_\\la\\{f_\\pi(\\la;w)\\la m+f_\\pi(\\la;w)m\\la\\}]\\dd\\la \\non\\\\ & = \\frac{1}{2}\\int_{\\partial\\rc_r}\\nu^\\top \\vec(\\la m+m\\la)f_\\pi(\\la;w)\\dd\\si .",
    "\\label{eqn : gdt2}\\end{aligned}\\ ] ] recall that @xmath2 is finite and @xmath212 for @xmath213 , so that @xmath214 and @xmath215 are bounded . since @xmath216 for any @xmath213 , ( [ eqn : gdt1 ] ) and ( [ eqn : gdt2 ] ) are , respectively , equal to zero , which completes the proof .",
    "@xmath67      define a second - stage prior density for @xmath149 as @xmath217 where @xmath119 and @xmath120 are constants and @xmath218 is a normalizing constant .",
    "the hierarchical prior ( [ eqn : pr_th ] ) with ( [ eqn : pr_gb ] ) is a generalization of faith ( 1978 ) in bayesian minimax estimation of a normal mean vector .",
    "faith s ( 1978 ) prior has also been discussed in detail by maruyama ( 1998 ) .",
    "when @xmath219 and @xmath220 , @xmath221 is proper and the distribution of @xmath149 is often called the matrix - variate beta distribution .",
    "konno ( 1988 ) showed that @xmath222 for other properties of the matrix - variate beta distribution , see muirhead ( 1982 ) and gupta and nagar ( 1999 ) .",
    "let @xmath223 be the generalized bayesian predictive density with respect to ( [ eqn : pr_th ] ) and ( [ eqn : pr_gb ] ) .",
    "a sufficient condition for minimaxity of @xmath223 is given as follows .",
    "[ prp : mini_gb ] assume that @xmath224 .",
    "then @xmath223 is minimax relative to the kl loss ( [ eqn : loss ] ) if @xmath225 there exist constants @xmath119 and @xmath120 satisfying ( [ eqn : upper0 ] ) if @xmath226    recall that @xmath44 is minimax and has a constant risk relative to the kl loss ( [ eqn : loss ] ) . hence if @xmath223 is proper bayes then it is admissible .",
    "assume that @xmath224 .",
    "then @xmath223 is admissible and minimax relative to the kl loss ( [ eqn : loss ] ) if @xmath227 thus , there exist constants @xmath119 and @xmath120 satisfying ( [ eqn : upper1 ] ) if @xmath228 .",
    "since @xmath229 , it is observed that @xmath230 we also obtain the following corollary .",
    "[ cor : proper2 ] assume that @xmath231 .",
    "then , for any @xmath19 , @xmath20 and @xmath232 , @xmath223 is admissible and minimax relative to the kl loss ( [ eqn : loss ] ) if @xmath233     of @xmath234 for admissibility and minimaxity ]    * proof of proposition [ prp : mini_gb ] . *   using theorem [ thm : faith ]",
    ", we will derive a sufficient condition for minimaxity of @xmath223 .    denote @xmath235",
    "let @xmath236\\\\ & = k_{a , b}v_1^{r(r+b-1)/2}|\\la|^{a/2 - 1}|i_r-\\la|^{b/2 - 1}|v_1i_r+(1-v_1)\\la|^{-c/2}.\\end{aligned}\\ ] ] when @xmath237 it follows that for any @xmath213 @xmath238=0.\\ ] ]    define @xmath239 since @xmath240 and @xmath199 , it holds that @xmath241 which implies that @xmath242 thus if @xmath243 and @xmath220 then @xmath244 and @xmath245 are finite .",
    "it is seen from lemmas [ lem : diff1 ] and [ lem : diff3 ] that @xmath246,\\ ] ] so that @xmath247f_{gb}(\\la;w)\\dd\\la\\\\ & = 2(a-2)\\tr m_{gb}-2(b-2)\\int_{\\rc_r}\\tr[(i_r-\\la)^{-1}\\la^2]f_{gb}(\\la;w)\\dd\\la\\\\ & \\quad -2(1-v_1)c\\int_{\\rc_r}\\tr[\\{v_1i_r+(1-v_1)\\la\\}^{-1}\\la^2]f_{gb}(\\la;w)\\dd\\la.\\end{aligned}\\ ] ] note that @xmath248=-\\tr\\la+\\tr[(i_r-\\la)^{-1}\\la],\\ ] ] which leads to @xmath249f_{gb}(\\la;w)\\dd\\la \\non\\\\ & \\quad -2(1-v_1)c\\int_{\\rc_r}\\tr[\\{v_1i_r+(1-v_1)\\la\\}^{-1}\\la^2]f_{gb}(\\la;w)\\dd\\la.\\end{aligned}\\ ] ] similarly , lemmas [ lem : diff1 ] and [ lem : diff3 ] are used to see that @xmath250}{\\pi_{gb}^j(\\la ) } & = ( a-2)\\tr m_{gb}-(b-2)\\tr[m_{gb}(i_r-\\la)^{-1}\\la ] \\\\ & \\quad -(1-v_1)c\\tr[m_{gb}\\{v_1i_r+(1-v_1)\\la\\}^{-1}\\la],\\end{aligned}\\ ] ] which yields that @xmath251}{\\pi_{gb}^j(\\la)}f_{gb}(\\la;w)\\dd\\la \\non\\\\ & = ( a-2)\\tr m_{gb } -\\frac{b-2}{m_{gb}}\\int_{\\rc_r}\\tr[m_{gb}(i_r-\\la)^{-1}\\la]f_{gb}(\\la;w)\\dd\\la \\non\\\\ & \\quad -\\frac{(1-v_1)c}{m_{gb}}\\int_{\\rc_r}\\tr[m_{gb}\\{v_1i_r+(1-v_1)\\la\\}^{-1}\\la]f_{gb}(\\la;w)\\dd\\la.\\end{aligned}\\ ] ] hence combining ( [ eqn : de1 ] ) and ( [ eqn : de2 ] ) gives that @xmath252 where @xmath253f_{gb}(\\la;w)\\dd\\la \\\\ & \\qquad + \\frac{b-2}{m_{gb}}\\int_{\\rc_r}\\tr[m_{gb}(i_r-\\la)^{-1}\\la]f_{gb}(\\la;w)\\dd\\la , \\\\",
    "\\de_4 & = -2(1-v_1)c\\int_{\\rc_r}\\tr[\\{v_1i_r+(1-v_1)\\la\\}^{-1}\\la^2]f_{gb}(\\la;w)\\dd\\la \\\\ & \\qquad + \\frac{(1-v_1)c}{m_{gb}}\\int_{\\rc_r}\\tr[m_{gb}\\{v_1i_r+(1-v_1)\\la\\}^{-1}\\la]f_{gb}(\\la;w)\\dd\\la.\\end{aligned}\\ ] ] here , it can easily be verified that @xmath254 is finite for @xmath243 and @xmath255 .    for notational simplicity",
    ", we use the notation @xmath256=\\int_{\\rc_r}g(\\la)f_{gb}(\\la;w)\\dd\\la \\big/\\int_{\\rc_r}f_{gb}(\\la;w)\\dd\\la\\ ] ] for an integrable function @xmath257 .",
    "then from ( [ eqn : de0 ] ) , @xmath258 - 2\\tr\\big [ \\er_\\la\\{(i_r-\\la)^{-1}\\la\\}\\big]\\big]\\non\\\\ & + ( 1-v_1)c\\big [ \\tr\\big [ \\er_\\la(\\la)\\er_\\la\\{(v_1i_r+(1-v_1)\\la)^{-1}\\la\\}\\big ] \\non\\\\ & \\qquad\\qquad\\qquad -2\\tr\\big [ \\er_\\la\\{(v_1i_r+(1-v_1)\\la)^{-1}\\la^2\\}\\big]\\big]\\end{aligned}\\ ] ] for @xmath259 .",
    "note that @xmath212 and @xmath260 . since @xmath261 and @xmath262 \\geq \\tr\\la$ ] , the second term in the r.h.s .",
    "of ( [ eqn : de00 ] ) is evaluated as @xmath263 - 2\\tr\\big [ \\er_\\la\\{(i_r-\\la)^{-1}\\la\\}\\big ] \\\\ & \\leq - \\tr\\big [ \\er_\\la\\{(i_r-\\la)^{-1}\\la\\}\\big ] \\leq - \\tr \\er_\\la(\\la).\\end{aligned}\\ ] ] since @xmath255 , we have @xmath264 \\non\\\\ & \\qquad\\qquad\\qquad -2\\tr\\big [ \\er_\\la\\{(v_1i_r+(1-v_1)\\la)^{-1}\\la^2\\}\\big]\\big].\\end{aligned}\\ ] ] it is here observed that @xmath265 which is used to get @xmath266 \\\\",
    "& \\qquad\\qquad -2\\tr\\big [ \\er_\\la\\{(v_1i_r+(1-v_1)\\la)^{-1}\\la^2\\}\\big]\\big ] \\\\ & \\qquad = -c\\tr \\er_\\la(\\la)\\\\ & \\qquad\\qquad + cv_1 \\big [ 2 \\tr\\big [ \\er_\\la\\{(v_1i_r+(1-v_1)\\la)^{-1}\\la\\}\\big ] \\\\ & \\qquad\\qquad\\qquad-\\tr\\big [ \\er_\\la(\\la ) \\er_\\la\\{(v_1i_r+(1-v_1)\\la)^{-1}\\}\\big]\\big].\\end{aligned}\\ ] ] substituting this quantity into ( [ eqn : de01 ] ) gives @xmath267 \\non\\\\ & \\qquad\\qquad -\\tr\\big [ \\er_\\la(\\la ) \\er_\\la\\{(v_1i_r+(1-v_1)\\la)^{-1}\\}\\big]\\big].\\end{aligned}\\ ] ]    to evaluate the second term in the r.h.s .",
    "of ( [ eqn : de02 ] ) , note that @xmath268 in the case of @xmath269 , it is seen from ( [ eqn : inq ] ) that @xmath270-\\tr\\big [ \\er_\\la(\\la ) \\er_\\la\\{(v_1i_r+(1-v_1)\\la)^{-1}\\}\\big]\\big ] \\\\ & \\leq cv_1 \\big\\ { { 2\\over v_1}\\tr \\er_\\la(\\la ) - \\tr \\er_\\la(\\la)\\big\\ } = c ( 2-v_1 ) \\tr \\er_\\la(\\la),\\end{aligned}\\ ] ] which implies that @xmath271 because @xmath272 .",
    "it is noted that @xmath273 because @xmath274 and @xmath255 .",
    "thus , one gets a sufficient condition given by @xmath275 in the case of @xmath276 , it is seen from ( [ eqn : inq ] ) that @xmath270-\\tr\\big [ \\er_\\la(\\la ) \\er_\\la(v_1i_r+(1-v_1)\\la)^{-1}\\big]\\big ] \\\\ &",
    "\\leq cv_1 \\big\\ { 2\\tr \\er_\\la(\\la ) - { 1\\over v_1}\\tr \\er_\\la(\\la)\\big\\ } = c ( 2v_1 - 1 ) \\tr \\er_\\la(\\la),\\end{aligned}\\ ] ] which implies that @xmath277 hence , it holds true that @xmath278 if @xmath279 combining ( [ eqn : sc1 ] ) and ( [ eqn : sc2 ] ) yields the condition @xmath280 , namely , @xmath281 . from ( [ eqn : bound - cond ] ) , the sufficient conditions on @xmath282 for minimaxity can be written as @xmath274 , @xmath255 and @xmath283 if @xmath284 thus the proof is complete .",
    "@xmath67    take @xmath285 .",
    "let @xmath286 .",
    "consider the problem of estimating the mean matrix @xmath17 under the squared frobenius norm loss @xmath287 .",
    "the bayesian estimator with respect to ( [ eqn : pr_th ] ) and ( [ eqn : pr_gb ] ) is expressed as @xmath288\\dd\\om}{\\int_{\\rc_r}|\\om|^{(q+a)/2 - 1}|i_r-\\om|^{b/2 - 1}\\exp[-\\tr(\\om xx^\\top)/2]\\dd\\om}\\bigg]x.\\ ] ] then the same arguments as in this section yield that @xmath289 is proper bayes and minimax if @xmath219 , @xmath255 , @xmath290 and @xmath291 .",
    "in estimation of the normal mean vector , stein ( 1973 , 1981 ) discovered an interesting relationship between superharmonicity of prior density and minimaxity of the resulting generalized bayes estimator .",
    "the relationship is very important and useful in bayesian predictive density estimation . in this section",
    "we derive some bayesian minimax predictive densities with superharmonic priors .",
    "let @xmath292 be a bayesian predictive density with respect to a prior @xmath32 , where @xmath32 is twice differentiable and the marginal density @xmath38 is finite .",
    "all the results in this section are based on the following key lemma .",
    "[ lem : superharmonic ] denote by @xmath293 the @xmath18 differentiation operator matrix with respect to @xmath17 .",
    "then @xmath294 is minimax relative to the kl loss @xmath182 if @xmath32 is superharmonic , namely , @xmath295=\\sum_{i=1}^r\\sum_{j=1}^q\\frac{\\partial^2",
    "\\pi(\\th)}{\\partial \\th_{ij}^2}\\leq 0.\\ ] ]    * proof .",
    "*   this lemma can be proved along the same arguments as in stein ( 1981 ) .",
    "see also george et al .",
    "( 2006 ) and brown et al .",
    "@xmath67    define a class of prior densities as @xmath296 where @xmath297 is twice differentiable with respect to @xmath5 .",
    "let @xmath298 be an @xmath91 matrix of differential operator with respect to @xmath299 such that the @xmath72 element of @xmath298 is @xmath300 where @xmath95 stands for the kronecker delta .",
    "let @xmath301 namely , @xmath130 is an @xmath91 symmetric matrix such that @xmath302 .",
    "[ lem : condition1 ] @xmath294 with respect to @xmath303 is minimax relative to the kl loss @xmath182 if @xmath304=2[(q - r-1)\\tr(g)+2\\tr(\\dc_\\si \\si g)]\\leq 0,\\ ] ] where @xmath305 .",
    "*   using ( i ) and ( ii ) in lemma [ lem : diff4 ] gives that @xmath306 & = 2\\tr(\\nabla_\\th \\th^\\top \\dc_\\si g(\\si))=2\\tr(\\nabla_\\th \\th^\\top g)\\\\ & = 2\\big[(q - r-1)\\tr(g)+2\\tr(\\dc_\\si \\si g)\\big].\\end{aligned}\\ ] ] from lemma [ lem : superharmonic ] , the proof is complete .",
    "let @xmath307 be ordered eigenvalues of @xmath308 , where @xmath309 , and let @xmath310",
    ". denote by @xmath311 an @xmath91 orthogonal matrix such that @xmath312 .",
    "assume that @xmath313 is orthogonally invariant , namely , @xmath314 for any orthogonal matrix @xmath315 .",
    "then , we can assume that @xmath316 without loss of generality .",
    "[ prp : condition2 ] assume that @xmath316 and @xmath257 is a twice differentiable function of @xmath173 .",
    "then @xmath294 with @xmath317 is minimax relative to the kl loss @xmath182 if @xmath318\\\\ & = 2\\sum_{i=1}^r\\bigg\\{(q - r+1)\\phi_i(\\la)+\\sum_{j\\ne i}^r\\frac{\\la_i\\phi_i(\\la)-\\la_j\\phi_j(\\la)}{\\la_i-\\la_j}+2\\la_i\\frac{\\partial\\phi_i(\\la)}{\\partial\\la_i}\\bigg\\}\\leq 0,\\end{aligned}\\ ] ] where @xmath319 .    * proof .",
    "*   since from ( i ) of lemma [ lem : diff2 ] @xmath320 it is observed that by the chain rule @xmath321 where @xmath322 .",
    "using lemma [ lem : condition1 ] and ( ii ) of lemma [ lem : diff2 ] gives that @xmath318 \\\\ & = 2[(q - r-1)\\tr\\{\\ga\\phi(\\la)\\ga^\\top\\}+2\\tr\\{\\dc_\\si \\ga\\la\\phi(\\la)\\ga^\\top\\ } ] \\\\ & = 2\\sum_{i=1}^r\\bigg[(q - r-1)\\phi_i(\\la)+\\sum_{j\\ne i}^r\\frac{\\la_i\\phi_i(\\la)-\\la_j\\phi_j(\\la)}{\\la_i-\\la_j}+2\\frac{\\partial}{\\partial\\la_i}\\{\\la_i\\phi_i(\\la)\\}\\bigg]\\\\ & = 2\\sum_{i=1}^r\\bigg\\{(q - r+1)\\phi_i(\\la)+\\sum_{j\\ne i}^r\\frac{\\la_i\\phi_i(\\la)-\\la_j\\phi_j(\\la)}{\\la_i-\\la_j}+2\\la_i\\frac{\\partial\\phi_i(\\la)}{\\partial\\la_i}\\bigg\\}.\\end{aligned}\\ ] ] hence the proof is complete .",
    "using proposition [ prp : condition2 ] , we give some examples of bayesian predictive densities with respect to superharmonic priors . consider a class of shrinkage prior densities , @xmath323 where @xmath324 and @xmath325 are nonnegative constants .",
    "the class @xmath326 includes both harmonic priors @xmath327 and @xmath328 , which are given in ( [ eqn : pr_em ] ) and ( [ eqn : pr_js ] ) , respectively .",
    "indeed , @xmath326 is the same as @xmath327 if @xmath329 and @xmath330 and as @xmath328 if @xmath331 and @xmath332 .",
    "it is noted that @xmath333 combining ( [ eqn : d_pi_g ] ) , ( [ eqn : dd_pi_g ] ) and proposition [ prp : condition2 ] , we obtain @xmath334 \\non\\\\ & = \\pi_{sh}(\\th)\\sum_{i=1}^r\\bigg[\\{\\al_i^2-(q - r-1)\\al_i\\}\\frac{1}{\\la_i}-2\\sum_{j > i}^r\\frac{\\al_i-\\al_j}{\\la_i-\\la_j}+\\frac{2\\al_i\\be}{\\tr(\\th\\th^\\top)}\\bigg ] \\non\\\\ & \\qquad + \\pi_{sh}(\\th)\\frac{\\be^2-(qr-2)\\be}{\\tr(\\th\\th^\\top)}. \\label{eqn : dd - pi_g}\\end{aligned}\\ ] ]    [ exm:1 ] let @xmath335 where @xmath324 are nonnegative constants .",
    "assume that @xmath336 .",
    "note that @xmath337 from ( [ eqn : dd - pi_g ] ) , it is seen that @xmath338 \\\\ & \\leq\\pi_{st}(\\th)\\sum_{i=1}^r\\bigg\\{\\al_i^2-(q - r-1)\\al_i-2(r - i)\\al_i+2\\sum_{j > i}^r\\al_j\\bigg\\}\\frac{1}{\\la_i}\\\\ & = \\pi_{st}(\\th)\\sum_{i=1}^r\\bigg\\{\\al_i^2-(q+r-2i-1)\\al_i+2\\sum_{j > i}^r\\al_j\\bigg\\}\\frac{1}{\\la_i}.\\end{aligned}\\ ] ] here , assume additionally that @xmath339 with @xmath340 for @xmath341 . for each @xmath342",
    "we observe that @xmath343 which implies that @xmath344\\leq 0 $ ] if @xmath336 and @xmath339 for each @xmath342 .",
    "then the resulting bayesian predictive density is minimax under the kl loss ( [ eqn : loss ] ) .",
    "@xmath67    [ exm:2 ] consider a prior density of the form @xmath345 where @xmath346 .",
    "combining example [ exm:1 ] and ( [ eqn : dd - pi_g ] ) gives that @xmath347\\\\ & \\leq\\pi_{ms1}(\\th)\\sum_{i=1}^r\\frac{\\al_i^{st}\\be^{ms}}{\\tr(\\th\\th^\\top ) } + \\pi_{ms1}(\\th)\\frac{(\\be^{ms})^2-(qr-2)\\be^{ms}}{\\tr(\\th\\th^\\top)}=0.\\end{aligned}\\ ] ] hence the bayesian predictive density with respect to @xmath348 is minimax relative to the kl loss ( [ eqn : loss ] ) .",
    "@xmath67    in the literature , many shrinkage estimators have been developed in estimation of a normal mean matrix .",
    "it is worth pointing out that the bayesian predictive densities with superharmonic prior @xmath326 correspond to such shrinkage estimators .",
    "let @xmath349 and denote an estimator of @xmath17 by @xmath350 .",
    "consider the problem of estimating the mean matrix @xmath17 relative to quadratic loss @xmath351 .",
    "then the generalized bayes estimator of @xmath17 with the prior density @xmath352 is expressed as @xmath353 if @xmath352 is superharmonic then @xmath354 is minimax relative to the quadratic loss @xmath355 .    since @xmath356 , the integration by parts gives that @xmath357\\pi_{sh}(\\th)\\dd\\th}{\\int_{\\re^{r\\times q } } \\exp(-\\vert x-\\th \\vert^2/(2v_x))\\pi_{sh}(\\th)\\dd\\th } \\\\ & = x+v_x \\frac{\\int_{\\re^{r\\times q } } \\exp(-\\vert x-\\th \\vert^2/(2v_x))[\\nabla_\\th\\pi_{sh}(\\th)]\\dd\\th}{\\int_{\\re^{r\\times q } } \\exp(-\\vert x-\\th \\vert^2/(2v_x))\\pi_{sh}(\\th)\\dd\\th}.\\end{aligned}\\ ] ] here using ( i ) of lemma [ lem : diff4 ] and ( i ) of lemma [ lem : diff2 ] gives that @xmath358 which leads to @xmath359,\\ ] ] where @xmath360 stands for the posterior expectation with respect to a density proportional to @xmath361 .",
    "denote by @xmath362 the eigenvalue decomposition of @xmath363 , where @xmath109 is an orthogonal matrix of order @xmath22 and @xmath110 is a diagonal matrix of order @xmath22 with @xmath111 . substituting @xmath364 for @xmath365 in the second term of the r.h.s . of ( [ eqn : th_ms ] )",
    ", we obtain an empirical bayes shrinkage estimator @xmath366    the shrinkage estimator @xmath367 is equivalent to @xmath53 , given in ( [ eqn : js ] ) , when @xmath331 and @xmath332 , and to @xmath52 , given in ( [ eqn : em ] ) , when @xmath329 and @xmath330 . in estimation of the normal mean matrix relative to the quadratic loss @xmath355 , @xmath53 and @xmath52 are minimax .",
    "if @xmath367 with certain specified @xmath324 and @xmath325 has good performance , the prior density @xmath352 with the same @xmath324 and @xmath325 would produce a good bayesian predictive density . from tsukuma ( 2008 ) , @xmath367 is a minimax estimator dominating @xmath52 when @xmath368 for @xmath341 and @xmath369 .",
    "a reasonable choice for @xmath325 is @xmath346 and this suggests that we should consider a prior density of the form @xmath370 the prior density @xmath371 is not superharmonic , and it is not known whether the resulting bayesian predictive density is minimax or not . in the next section ,",
    "we verify risk behavior of the bayesian predictive density with respect to @xmath371 through monte carlo simulations .",
    "this section briefly reports some numerical results so as to compare performance in risk of some bayesian predictive densities for @xmath372 and @xmath373 .",
    "first we investigate risk behavior of generalized bayes predictive densities @xmath234 with @xmath374 in the following six cases : @xmath375 for the second - stage prior ( [ eqn : pr_gb ] ) . when @xmath372 and @xmath373 , @xmath234 with the above six cases are minimax and , in particular , @xmath234 with @xmath376 is proper bayes for any @xmath19 and @xmath20 ( see corollary [ cor : proper2 ] ) .",
    "the risk has been simulated by 100,000 independent replications of @xmath24 and @xmath25 , where @xmath15 and @xmath16 with @xmath377 and @xmath378 .",
    "it has been assumed that a pair of the maximum and the minimum eigenvalues of @xmath379 is @xmath380 or @xmath381 .",
    "note that the best invariant predictive density @xmath382 has a constant risk and its risk is approximately given by @xmath383 when @xmath372 and @xmath373 .",
    "denote by @xmath384 the matrix - variate beta distribution having the density ( [ eqn : pr_gb ] ) . using ( [ eqn : m(w)-1 ] ) with @xmath385 and @xmath386 , we can rewrite @xmath234 as @xmath387}{\\er^{\\om}[g_{v_x}(\\om|x)]}\\ph_u(y|x),\\ ] ] where @xmath388 indicates expectation with respect to @xmath389 and @xmath390\\big]\\ ] ] for an @xmath18 matrix @xmath13 . hence in our simulations ,",
    "the expectation @xmath391 $ ] was estimated by @xmath392 , where @xmath393 and the @xmath394 are independent replications from @xmath395 .",
    "@xmath396                     ( 1 , 1 ) & ( \\ 0,\\ 0)&10.4 &   5.3 &   6.9 &   7.7 &   2.8 &   4.6 &   1.6 \\\\         & ( 24,\\ 0 ) &     &   6.9 &   8.0 &   8.4 &   5.3 &   6.3 &   4.6 \\\\         & ( 24,24 )   &     &   8.7 &   9.0 &   9.2 &   7.9 &   8.0 &   8.4 \\\\ [ 6pt ]                     ( 1,0.1)&(\\ 0,\\ 0)&36.0 & 15.2 & 24.0 & 28.2 &   9.6 & 17.7 &   6.8 \\\\         & ( 24,\\ 0 ) &     & 23.6 & 28.5 & 30.9 & 20.1 & 24.5 & 18.7 \\\\         & ( 24,24 )   &     & 32.7 & 33.2 & 33.5 & 31.0 & 31.4 & 32.4 \\\\",
    "\\hline \\end{array } $ ]    the simulated results for risk of @xmath234 are given in table [ tab:1 ] .",
    "when the pair of eigenvalues of @xmath379 is @xmath397 , our simulations suggest that the risk of @xmath234 decreases as @xmath119 increases under which @xmath120 is fixed or under which @xmath398 is fixed and also that the risk of @xmath234 increases as @xmath120 increases under which @xmath119 is fixed .",
    "it is observed that @xmath234 with @xmath399 is superior to others .    when the pair of eigenvalues of @xmath379 is @xmath381 , @xmath234 with @xmath400 or @xmath401 is best , but the improvement over @xmath382 is little . when the pair of eigenvalues of @xmath379 is @xmath402 , @xmath234 with @xmath399 is best .",
    "next , we investigate the risk of bayesian predictive densities based on superharmonic priors when @xmath372 and @xmath373 . if @xmath403 is a superharmonic prior , then the bayesian predictive density ( [ eqn : bpd ] ) can be expressed as @xmath404}{\\er^{\\th|x}[\\pi_s(\\th)]}\\ph_u(y|x),\\ ] ] where @xmath405 and @xmath360 stand , respectively , for expectations with respect to @xmath406 and @xmath407 . in our simulations , @xmath408 was estimated by means of @xmath409 where @xmath410 and the @xmath411 and the @xmath412 are , respectively , independent replications from @xmath413 and @xmath414 .    @xmath415gb@xmath416js@xmath417em@xmath418ms1@xmath416ms2@xmath419            1 & 1 & ( \\ 0,\\ 0)&10.4 &   1.6 &   0.7 &   2.1 &   5.2 &   0.7 \\\\     &    & ( 24,\\ 0 ) &     &   4.6 &   5.5 &   4.9 &   7.0 &   4.4 \\\\     &    & ( 24,\\ 4 ) &     &   5.7 &   5.9 &   5.9 &   7.4 &   5.4 \\\\     &    & ( 24,\\ 8) &     &   6.6 &   6.3 &   6.6 &   7.7 &   6.2 \\\\     &    & ( 24 , 12 ) &     &   7.2 &   6.6 &   7.1 &   8.0 &   6.7 \\\\     &    & ( 24 , 24 ) &     &   8.4 &   7.3 &   7.9 &   8.4 &   7.6 \\\\ [ 6pt ]                       1 & 0.1&(\\ 0,\\ 0)&36.0 &   6.8 &   2.4 &   7.2 & 18.0 &   2.4 \\\\     &    & ( 24,\\ 0 ) &     & 18.7 & 25.8 & 18.9 & 26.1 & 18.2 \\\\     &    & ( 24,\\ 4 ) &     & 25.5 & 26.8 & 25.7 & 28.9 & 25.0 \\\\     &    & ( 24,\\ 8) &     & 28.1 & 27.7 & 28.2 & 30.2 & 27.6 \\\\     &    & ( 24 , 12 ) &     & 29.7 & 28.4 & 29.5 & 30.9 & 28.9 \\\\     &    & ( 24 , 24 ) &     & 32.4 & 30.0 & 31.3 & 32.0 & 30.8 \\\\ \\hline \\end{array } $ ]    the risk is based on 100,000 independent replications of @xmath24 and @xmath25 for some pairs of two eigenvalues of @xmath379 .",
    "the simulation results are provided in table [ tab:2 ] , where gb , js , em , ms1 and ms2 are the bayesian predictive densities with the following priors .",
    "when the pair of eigenvalues of @xmath379 is @xmath397 , js and ms2 are superior .",
    "when the pair of eigenvalues of @xmath379 is @xmath381 , js has nice performance but it is bad if the two eigenvalues of @xmath379 are much different .",
    "* acknowledgments . *  the research of the first author was supported by grant - in - aid for scientific research ( 15k00055 ) from japan society for the promotion of science ( jsps ) .",
    "the research of the second author was supported in part by grant - in - aid for scientific research ( 15h01943 and 26330036 ) from jsps .",
    "konno , y. ( 1992 ) . improved estimation of matrix of normal mean and eigenvalues in the multivariate @xmath423-distribution , doctoral dissertation , institute of mathematics , university of tsukuba .",
    "this is downloadable from his website ( http://mcm-www.jwu.ac.jp/~konno/ ) ."
  ],
  "abstract_text": [
    "<S> this paper deals with the problem of estimating predictive densities of a matrix - variate normal distribution with known covariance matrix . </S>",
    "<S> our main aim is to establish some bayesian predictive densities related to matricial shrinkage estimators of the normal mean matrix . </S>",
    "<S> the kullback - leibler loss is used for evaluating decision - theoretical optimality of predictive densities . </S>",
    "<S> it is shown that a proper hierarchical prior yields an admissible and minimax predictive density . </S>",
    "<S> also , superharmonicity of prior densities is paid attention to for finding out a minimax predictive density with good numerical performance .    _ ams 2010 subject classifications : _ primary 62c15 , 62c20 ; secondary 62c10 .    _ key words and phrases : _ admissibility , gauss divergence theorem , generalized bayes estimator , inadmissibility , kullback - leibler loss , minimaxity , shrinkage estimator , statistical decision theory . </S>"
  ]
}