{
  "article_text": [
    "this paper presents an importance sampling based approximate optimal planning and control algorithm .",
    "optimal motion planning in deterministic and continuous systems is computationally np - complete @xcite except for linear time invariant systems . for nonlinear systems",
    ", there is a vast literature on approximate solutions and algorithms . in optimal planning ,",
    "the common approximation scheme is discretization - based . by discretizing the state and input spaces ,",
    "optimal planning is performed by solving the shortest path problem in the discrete transition systems obtained from abstracting the continuous dynamics , using heuristic - based search or dynamic programming . comparing to discretization - based methods , _ sampling - based graph search _ ,",
    "includes probabilistic roadmap ( prm ) @xcite , rrt @xcite , rrt * @xcite , are more applicable for high - dimensional systems .",
    "while rrt has no guarantee on the optimality of the path @xcite , rrt * compute an optimal path asymptotically provided the cost functional is lipschitz continuous .",
    "however , such lipschitz conditions may not be satisfied for some cost functions under specific performance consideration .",
    "the key idea in the proposed sampling - based planning method builds on a unification of importance sampling and approximate optimal control @xcite .",
    "in approximate optimal control , the objective is to approximate both the value function , i.e. , optimal cost - to - go , and the optimal feedback policy function by weighted sums of _ known _ basis functions . as a consequence",
    ", the search space is changed from infinite trajectory space or policy space to a continuous space of weight vectors , given that each weight vector corresponds to a unique feedback controller . instead of solving the approximate optimal control through training actor and critic neural networks ( nns ) using trajectory data @xcite",
    ", we propose a sampling - based method for sampling the weight vectors for a policy function approximation and searching for the optimal one .",
    "this method employs @xcite , a probabilistic complete global optimization algorithm , for searching the optimal weight vector that parametrizes the approximate optimal feedback policy .",
    "the fundamental idea is to treat the weight vector as a random variable over a parameterized distribution and the optimal weight vector corresponds to a dirac s delta function which is the target distribution .",
    "the algorithm iteratively estimates the parameter that possesses the minimum kullback - leibler divergence with respect to an intermediate reference model , which assigns a higher probability mass on a set of weights of controllers with improved performance over the previous iteration . at the meantime ,",
    "a set of sampled weight vectors are generated using the parameterized distribution and the performance of their corresponding policies are evaluated via simulation - based policy evaluation . under mild conditions ,",
    "the parameterized distribution converges , with probability one , to the target distribution that concentrates on the optimal weight vector with respect to given basis functions .    resembles another adaptive search algorithm called cross - entropy(ce ) method and provides faster and stronger convergence guarantee for being less sensitive to input parameters @xcite",
    "previously , ce algorithm has been introduced for motion planning @xcite based on sampling in the trajectory space . the center idea is to construct a probability distribution over the set of feasible paths and to perform the search for an optimal trajectory using ce .",
    "the parameters to be estimated is either a sequence of motion primitives or a set of via - points for interpolation - based trajectory planning .",
    "differ to these methods , ours is the first to integrate importance sampling to estimate parameterization of the optimal policy function approximation for continuous nonlinear systems .",
    "since the algorithm performs direct policy search , we are able to enforce robustness and stability conditions to ensure the computed policy is both robust and approximate optimal , provided these conditions can be evaluated efficiently .    to conclude",
    ", the contributions of this paper are the following : first , we introduce a planning algorithm by a novel integration of model reference adaptive search and approximate optimal control .",
    "second , based on contraction theory , we introduce a modification to the planning method to directly generate stabilizing and robust feedback controllers in the presence of bounded disturbances . last but not the least , through illustrative examples , we demonstrate the effectiveness and efficiency of the proposed methods and share our view on interesting future research along this direction .",
    "notation : the inner product between two vectors @xmath0 is denoted @xmath1 or @xmath2 . given a positive semi - definite matrix @xmath3 , the @xmath3-norm of a vector is denoted @xmath4 .",
    "we denote @xmath5 for @xmath3 being the identity matrix .",
    "@xmath6 is the indicator function , i.e. , @xmath7 if event @xmath8 holds , otherwise @xmath9 . for a real @xmath10",
    ", @xmath11 is the smallest integer that is greater than @xmath12 .",
    "we consider continuous - time nonlinear systems of the form    @xmath13    where @xmath14 is the state , @xmath15 is the control input , @xmath16 is the initial state , and @xmath17 is a vector field .",
    "we assume that @xmath18 and @xmath19 are compact .",
    "a feedback controller @xmath20 takes the current state and outputs a control input .",
    "the objective is to find a feedback controller @xmath21 that minimizes a finite - horizon cost function for a nonlinear system @xmath22 where @xmath23 is the stopping time , @xmath24 defines the running cost when the state trajectory traverses through @xmath25 and the control input @xmath26 is applied and @xmath27 defines the terminal cost . as an example",
    ", a running cost function can be a quadratic cost @xmath28 for some positive semi - definite matrices @xmath29 and @xmath30 , and a terminal cost can be @xmath31 where @xmath32 is a goal state .",
    "we denote the set of feedback policies to be @xmath33 . for infinite horizon optimal control ,",
    "the optimal policy is independent of time and a feedback controller suffices to be a minimizing argument of ( see ref .",
    "@xcite ) . for finite - horizon optimal control ,",
    "the optimal policy is time - dependent .",
    "however , for simplicity , in this paper , we only consider time - invariant feedback policies and assume the time horizon @xmath23 is of sufficient length to ignore the time constraints .      algorithm , introduced in @xcite , aims to solve the following problem : @xmath34 where @xmath35 is the solution space and @xmath36 is a deterministic function that is bounded from below .",
    "it is assumed that the optimization problem has a unique solution , i.e. , @xmath37 and for all @xmath38 , @xmath39 .",
    "the following regularity conditions need to be met for the applicability of .",
    "[ assume1 ] for any given constant @xmath40 , the set @xmath41 has a strictly positive lebesgue or discrete measure .",
    "this condition ensures that any neighborhood of the optimal solution @xmath42 will have a positive probability to be sampled .",
    "[ assume2 ] for any constant @xmath43 , @xmath44 , where @xmath45 , and we define the supremum over the empty set to be @xmath46 .    *",
    "selecte a sequence of reference distributions @xmath47 with desired convergence properties .",
    "specifically , the sequence @xmath47 will converge to a distribution that concentrates only on the optimal solution .",
    "* selecte a parametrized family of distribution @xmath48 over @xmath18 with parameter @xmath49 .",
    "* optimize the parameters @xmath50 iteratively by minimizing the following kl distance between @xmath51 and @xmath52 .",
    "@xmath53 where @xmath54 is the lebesgue measure defined over @xmath55 .",
    "the sample distributions @xmath56 can be viewed as compact approximations of the reference distributions and will converge to an approximate optimal solution as @xmath57 converges provided certain properties of @xmath47 is retained in @xmath58 .",
    "note that the reference distribution @xmath47 is unknown beforehand as the optimal solution is unknown .",
    "thus , the algorithm employs the estimation of distribution algorithms @xcite to estimate a reference distribution that guides the search . to make the paper",
    "self - contained , we will cover details of in the development of the planning algorithm .",
    "in this section , we present an algorithm that uses in a distinguished way for approximate optimal feedback motion planning .      the _ policy function approximation _ @xmath59 is a weighted sum of basis functions , @xmath60 where @xmath61 are basis functions , and the coefficients @xmath62 are the weight parameters , @xmath63 .",
    "an example of basis function can be polynomial basis @xmath64 $ ] for one - dimensional system .",
    "a commonly used class of basis functions is .",
    "it can be constructed by determining a set of centers @xmath65 , and then constructing basis functions @xmath66 , for each center @xmath67 , where @xmath68 is a pre - defined parameter .    in vector form",
    ", a policy function approximation is represented by @xmath69 where vector @xmath70^\\intercal$ ] and @xmath71^\\intercal$ ] .",
    "we let the domain of weight vector be @xmath72 and denote it by @xmath73 the set of all policies that can be generated by linear combinations of pre - defined basis functions . in the following context ,",
    "unless specifically mentioned , the vector of basis functions is @xmath74 .",
    "clearly , for any weight vector @xmath75 , @xmath76 .",
    "thus , we aim to solve @xmath77 so as to minimize the error in the optimal cost introduced by policy function approximation .",
    "[ def : optweight ] given a basis vector @xmath74 , a weight vector @xmath78 with respect to @xmath74 is _ optimal _ if and only if @xmath79 and for all @xmath80 such that @xmath81 , @xmath82 the approximate optimal feedback policy is @xmath83 .    by requiring @xmath84",
    ", it can be shown that the optimal weight vector @xmath78 minimizes the difference between the optimal cost achievable with policies in @xmath85 and the cost under the global optimal policy .",
    "for clarity in notation , we denote @xmath86 by @xmath87 as @xmath74 is a fixed basis vector throughout the development of the proposed method .",
    "clearly , if the actual optimal policy @xmath88 can be represented by a linear combination of selected basis functions , then we obtain the optimal policy by computing the optimal weight vector , i.e. , @xmath89 .",
    "[ [ remark ] ] remark : + + + + + + +    here , we assume a feedback policy can be represented by @xmath90 for some weight vector @xmath91 . in cases",
    "when the basis functions are continuous , a feedback policy must be a continuous function of the state .",
    "however , this requirement is hard to satisfy for many physical systems due to , for example , input saturation . in cases when a feasible controller is discontinuous",
    ", we can still use a continuous function to approximate , and then project the continuous function to the set @xmath33 of applicable controllers .",
    "using function approximation , we aim to solve the optimal feedback planning problem in approximately by finding the optimal weight vector with respect to a pre - defined basis vector .",
    "the main algorithm is presented next .      in this section",
    ", we present an adaptive search - based algorithm to compute the approximate optimal feedback policy .",
    "the algorithm is `` near '' anytime , meaning that it returns a feasible solution after a small number of samples .",
    "if more time is permitted , it will quickly converge to the globally optimal solution that corresponds to the approximate optimal feedback policy .",
    "the algorithm is probabilistic complete under regularity conditions of .",
    "we start by viewing the weight vector as a random variable @xmath92 governed by a multivariate gaussian distribution with a compact support @xmath72 .",
    "the distribution is parameterized by parameter @xmath93 , where @xmath94 is a @xmath95-dimensional mean vector and @xmath96 is the @xmath95 by @xmath95 covariance matrix .",
    "recall @xmath95 is the number of basis functions .",
    "the optimal weight vector @xmath78 can be represented as a _ target distribution _ @xmath97 as a dirac s delta , i.e. , @xmath98 and @xmath99 for @xmath100 .",
    "dirac s delta is a special case of multivariate gaussian distribution with zero in the limit case of vanishing covariance .",
    "thus , it is ensured that the target distribution can be arbitrarily closely approximated by multivariate gaussian distribution by a realization of parameter @xmath101 .",
    "recall that the probability density of a multivariate gaussian distribution is defined by @xmath102 where @xmath95 is the dimension of weight vector @xmath103 and @xmath104 is the determinant of @xmath96 .",
    "now , we are ready to represent the main algorithm , called , which includes the following steps .    1 .",
    "* initialization * : the initial distribution is selected to be @xmath105 , for @xmath106 which can generate a set of sample to achieve a good coverage of the sample space @xmath72 .",
    "for example , @xmath107 and @xmath108 which is an identity matrix .",
    "the following parameters are used in this algorithm : @xmath109 $ ] for specifying the quantile , the _ improvement parameter _",
    "@xmath110 , a _ sample increment percentage _ @xmath12 , an initial sample size @xmath111 , a _ smoothing coefficient _ @xmath112 $ ] .",
    "let @xmath113 .",
    "* sampling - based policy evaluation * : at each iteration @xmath114 , generate a set of @xmath115 samples @xmath116 from the current distribution @xmath117 . for each @xmath118 , using simulation we evaluate the cost @xmath119 from the initial state @xmath120 and the feedback policy @xmath121 with system model in . the cost @xmath87 is determined because the system is deterministic and has a unique solution .",
    "policy improvement with elite samples * : next , the set @xmath122 is ordered from largest ( worst ) to smallest ( best ) among given samples : @xmath123 we denote @xmath124 to be the estimated @xmath125-quantile of cost @xmath126 , i.e. , @xmath127 .",
    "the following cases are distinguished . *",
    "if @xmath113 , we introduce a threshold @xmath128 . *",
    "if @xmath129 , the following cases are further distinguished : * * @xmath130 , i.e. , the estimated @xmath125-quantile of cost has been reduced by the amount of @xmath131 from the last iteration , then let @xmath132 .",
    "let @xmath133 and continue to step 4 ) .",
    "* * otherwise @xmath134 , we find the largest @xmath135 , if it exists , such that the estimated @xmath136-quantile of cost @xmath137 satisfies @xmath138",
    ". then let @xmath139 and also let @xmath140 .",
    "continue to step 4 ) .",
    "however , if no such @xmath141 exists , then there is no update in the threshold @xmath142 but the sample size is increased to @xmath143 .",
    "let @xmath144 , @xmath145 , and continue to step 2 ) .",
    "* * parameter(policy ) update * : we update parameters @xmath146 for iteration @xmath147 .",
    "first , we define a set @xmath148 of _ elite samples_. note that the parameter update in @xmath101 is to ensure a higher probability for elite samples . to achieve that , for each elite sample @xmath149 , we associated a weight such that a higher weight is associated with a weight vector with a lower cost and a lower probability in the current distribution .",
    "the next parameter @xmath146 is selected to maximize the weighted sum of probabilities of elite samples . to this end",
    ", we update the parameter as follows .",
    "@xmath150\\end{gathered}\\ ] ] where @xmath151 is the expected value of a random variable @xmath152 given distribution @xmath153 , @xmath154 is a strictly decreasing and positive function . or @xmath155 if @xmath25 is strictly positive . ]",
    "@xmath156 is the weight for parameter @xmath75 .",
    "[ assume3 ] the optimal parameter @xmath157 is the interior point of @xmath158 for all @xmath114 .",
    "[ lma ] assuming  [ assume1],[assume2 ] , and [ assume3 ] and the compactness of @xmath72 , with probability one , @xmath159 where @xmath78 is the optimal weight vector and @xmath160 is an @xmath95-by-@xmath95 zero matrix .",
    "note that since @xmath161 converges in the limit a zero matrix , the stopping criterion is justified .",
    "building on the convergence result of , the proposed sampling - based planner ensures a convergence to a dirac delta function concentrating on the optimum . in practice",
    ", the parameter update is performed using the expectation  maximization ( em ) algorithm .",
    "* em - based parameter update / policy improvement * since our choice of probability distribution is the multivariate gaussian , the parameter @xmath162 is computed as follows @xmath163i_{w\\in e } w } { { \\mathbb{e}}_{\\theta_k } [ s(j(x_0,w))^k / p(w,\\theta_k)]i_{w\\in e } } \\\\",
    "\\approx \\frac{\\sum_{w\\in w_k } [      s(j(x_0,w))^k / p(w,\\theta_k)]i_{w\\in e } w } { \\sum_{w\\in        w_k } [ s(j(x_0,w))^k / p(w,\\theta_k)]i_{w\\in e}},\\end{gathered}\\ ] ] and @xmath164i_{w\\in        e } ( w - { \\mathbf{\\mu}})(w - { \\mathbf{\\mu}})^\\intercal}{{\\mathbb{e}}_{\\theta_k }      [ s(j(x_0,w))^k / p(w,\\theta_k)]i_{w\\in e}}\\\\ \\approx \\frac {      \\sum_{w\\in w_k } [ s(j(x_0,w))^k / p(w,\\theta_k)]i_{w\\in e } ( w -      { \\mathbf{\\mu}})(w - { \\mathbf{\\mu}})^\\intercal } { \\sum_{w\\in        w_k}[s(j(x_0,w))^k / p(w,\\theta_k)]i_{w\\in e}},\\end{gathered}\\ ] ] where we approximate @xmath165 with its estimate @xmath166 for @xmath167 and the fraction @xmath168 was canceled as the term is shared by the numerator and the denominator .    * smoothing * : due to limited sample size , a greedy maximization for parameter update can be premature if too few samples are used .",
    "to ensure the convergence to the _ global optimal solution _ , a _ smoothing _ update is needed . to this end",
    ", we select the parameter for the next iteration to be @xmath169 where @xmath170 is the smoothing parameter .",
    "let @xmath145 .",
    "we check if the iteration can be terminated based on a given stopping criterion .",
    "if the stopping criterion is met , then we output the latest @xmath171 .",
    "otherwise , we continue to update of @xmath101 by moving to step 2 ) .    *",
    "stopping criterion * given the probability distribution will converge to a degenerated one that concentrates on the optimal weight vector .",
    "we stop the iteration if the covariance matrix @xmath161 becomes near - singular given the convergence condition in lemma  [ lma ] .    to conclude , the proposed algorithm using is probabilistic complete and converges to the global optimal solution .",
    "if the assumptions are not met , the algorithm converges to a local optimum .",
    "being able to directly search within continuous control policy space , one major advantage is that one can enforce stability condition such that the search is restricted to stable and robust policy space . in this subsection",
    ", we consider contraction theory to compute conditions that need to be satisfied by weight vectors to ensure stability and robustness under bounded disturbances .",
    "@xcite given the system equation for the closed - loop system @xmath172 , a region of the state space is called a _ contraction region _ if the jacobian @xmath173 is uniformly negative definite in that region , that is , @xmath174 where @xmath175 is a positive definite matrix for all @xmath176 .",
    "@xcite [ thm : contraction ] given the system model @xmath177 , any trajectory , which starts in a ball of constant radius with respect to the matrix @xmath178 , centered about a given trajectory and contained at all times in a contraction region with respect to the matrix @xmath178 , remains in that ball and converges exponentially to this trajectory .",
    "furthermore , global exponential convergence to the given trajectory is guaranteed if the whole state space is a contraction region .",
    "theorem  [ thm : contraction ] provides a necessary and sufficient condition for exponential convergence of an autonomous system . under",
    "bounded disturbances , the key idea is to incorporate a contraction analysis in the planning algorithm such that it searches for a weight vector @xmath75 that is not only optimal in the nominal system but also ensures that the closed - loop actual system under the controller @xmath179 has contraction dynamics within a tube around the nominal trajectory . using a similar proof in @xcite",
    ", we can show that for systems with contracting dynamics , the actual trajectory under bounded disturbances will be ultimately uniformly bounded along the nominal trajectory .",
    "consider a closed - loop system @xmath180 where @xmath181 is a disturbance with @xmath182 , let a state trajectory @xmath183 be in the contraction region @xmath184 at all time @xmath185 , then for any time @xmath185 , the deviation between @xmath183 and the nominal trajectory @xmath186 , whose dynamic model is given by @xmath187 , satisfies @xmath188 in other words , the error is uniformly ultimately bounded with the ultimate bound @xmath189 .",
    "let s pick the lyapunov function @xmath190 whose time derivative is @xmath191 where the following property is used : @xmath192 for some @xmath193 $ ] if @xmath194 or @xmath195 $ ] otherwise .",
    "since the trajectories stays within the contraction region , the following condition holds @xmath196 , and we have @xmath197 meanwhile , @xmath198 as the trajectory @xmath183 stays within the region of contraction , and also @xmath199    we conclude that as @xmath200 , @xmath201    since @xmath202 and under the condition that @xmath203 , we obtain @xmath204 , and therefore @xmath205    thus , to search for the optimal and robust policies , we modify the algorithm by introducing the following step .",
    "* contraction verification step : * suppose the closed - loop system is subject to bounded disturbances , the objective is to ensure the trajectory is contracting within the time - varying tube @xmath206 , for all @xmath207 , where @xmath208 is the nominal state trajectory .",
    "the following condition translates the contraction condition into verifiable condition for a closed - loop system : choose positive constants @xmath209 , a positive definite symmetric and constant matrix @xmath210_{i=1,\\ldots , n , j=1,\\ldots , n}$ ] , and verify whether , at each time step along the nominal trajectory @xmath211 in the closed - loop system under control @xmath212 , the following condition holds .",
    "@xmath213 where @xmath214 is the @xmath215th component in the matrix @xmath216 .",
    "we verify this condition numerically at discrete time steps instead of continous time .",
    "further , if the function @xmath217 is semi - continuous , according to the extreme value theorem , this condition can be verified by evaluating @xmath217 at all critical points where @xmath218 and the boundary of the set @xmath219 .",
    "the modification to the planning algorithm is made in step 3 ) , if a controller @xmath220 of elite sample @xmath75 does not meet the condition , then @xmath75 is rejected from the set of elite samples . alternatively , one can do so implicitly by associating @xmath75 with a very large cost . however , since the condition is sufficient but not necessary as we have the matrix @xmath178 , constant @xmath221 and @xmath222 pre - fixed and @xmath178 is chosen to be a constant matrix , the obtained robust controller may not necessary be optimal among all robust controllers in @xmath85 . a topic for future work is to extend joint planning and control policies with respect to adaptive bound @xmath221 , @xmath222 , and a uniformly positive definite and time - varying matrix @xmath223 .",
    "in this section , we use two examples to illustrate the correctness and efficiency of the proposed method .",
    "the simulation experiments are implemented in matlab on a desktop with intel xeon e5 cpu and 16 gb of ram .      to illustrate the correctness and sampling efficiency in the planning algorithm",
    ", we consider an optimal control of systems with non - quadratic cost . for this class of optimal control problems ,",
    "since there is no admissible heuristic , one can not use any planning algorithm facilitated by the usage of a heuristic function .",
    "moreover , the optimal controller is nonlinear given the non - quadratic cost .",
    "consider a system @xmath224 where @xmath225 and @xmath226 with @xmath227 and @xmath228 .",
    "the initial state is @xmath229 $ ] .",
    "the cost functional is @xmath230    for a non - quadratic cost functional , the optimal controller is no longer linear and can not be computed by lqr unless the running cost can be written in the sum - of - square form .",
    "thus , we consider an approximate feedback controller with basis vector @xmath231^\\intercal$ ] .",
    "suppose the magnitude of external disturbance is bounded by @xmath232 .",
    "the following parameters are used in stability verification : @xmath233 , at any time @xmath207 , for all @xmath25 such that @xmath234 , the controller ensures @xmath235 because @xmath236 . with this choice for stability analysis , the constraint @xmath237 in this case ,",
    "if we select @xmath238 nonpositive , @xmath239 and @xmath240 , then closed - loop system , which is a nonlinear polynomial system , will become globally contracting .",
    "0.32   under feedback controller computed with .,title=\"fig : \" ]    0.33   under feedback controller computed with .,title=\"fig : \" ]    0.33    figures  [ fig : cost ] and [ fig : mean ] show the convergence result with in one simulation in terms of cost and the mean of the multivariant gaussian over iterations .",
    "the following parameters are used : initial sample size @xmath241 , improvement parameter @xmath242 , quantile percentage @xmath243 , smoothing parameter @xmath244 , sample increment parameter @xmath245 .",
    "the algorithm converges after @xmath246 iterations with @xmath247 samples to the mean @xmath248^\\intercal $ ] and the covariance matrix with a norm @xmath249 .",
    "each iteration took less than 10 seconds .",
    "the approximate optimal cost under feedback controller @xmath250 is @xmath251 .",
    "figure  [ fig : statelinnoquad ] shows the state trajectory for the closed - loop system with bounded disturbances . with 25 independent runs of , the mean of @xmath252 is @xmath253 and the standard deviation is @xmath254 , @xmath255 of the approximate optimal cost .",
    "note , if we only use linear feedback @xmath256 , the optimal cost is @xmath257 , which is about three times the optimal cost that can be achieved with a nonlinear controller .",
    "consider a dubins car dynamics @xmath258 where @xmath259 being the state ( coordinates and turning angle with respect to @xmath25-axis ) and @xmath26 and @xmath260 are control variables including linear and angular velocities .",
    "the system is kinematically constrained by its positive minimum turning radius @xmath261 which implies the following bound @xmath262 . without loss of generality",
    ", we assume @xmath263 and @xmath264 are the input constraints . the control objective is to reach the goal @xmath265 while avoiding static obstacles . the cost function @xmath266 where @xmath267 , the running cost is @xmath268 , and the terminal cost is @xmath269 .",
    "the initial state is @xmath270 . in simulation",
    ", we consider the robot reaches the target if @xmath271 for @xmath272 $ ] . in simulation , @xmath273 .",
    "we select as basis functions and define @xmath274^\\intercal$ ] for @xmath95 center points . in the experiment ,",
    "the center points are includes    uniform grids in @xmath275 coordinates with step sizes @xmath276 , @xmath277 ; and    vertices of the obstacle .",
    "we also include linear basis functions @xmath278 $ ] .",
    "the basis vector is @xmath279^\\intercal$ ] .",
    "we consider a bounded domain @xmath280 and @xmath281 and @xmath282 $ ] and thus the total number of basis functions is @xmath283 .",
    "the control input @xmath284^\\intercal$ ] where @xmath285 and @xmath286 .",
    "the total number of weight parameters is twice the number of bases and in this case @xmath287 .    0.45   computed using the mean of multivariate gaussian over iterations ( from the lightest to the darkest ) .",
    "( b ) the convergence of the covariance matrix .",
    "( c ) the total cost evaluated at the mean of the multivariate gaussian over iterations .",
    ", title=\"fig : \" ]    0.45    the following parameters are used : initial sample size @xmath288 , improvement parameter @xmath289 , smoothing parameter @xmath244 , sample increment percentage @xmath290 , and @xmath291 . in fig .",
    "[ fig : dubinstraj ] we show the trajectory computed using the estimated mean of multivariate gaussian distribution over iterations , from the lightest ( @xmath292-th iteration ) to the darkest ( the last iteration when stopping criterion is met ) .",
    "the optimal trajectory is the darkest line . in fig .",
    "[ fig : dubinscost ] we show the cost computed using the mean of multivariate gaussian over iterations .",
    "converges after 22 iterations with @xmath293 samples and the optimal cost is @xmath294 .",
    "each iteration took about @xmath295 to @xmath296 seconds .",
    "however , it generates a collision - free path only after @xmath297 iterations . due to input saturation",
    ", the algorithm is only ensured to converge to a local optimum .",
    "however , in 24 independent runs , all runs converges to a local optimum closer to the global one , as shown in the histogram in fig .",
    "[ fig : dubins_histo ] .",
    "our current work is to implement trajectory - based contraction analysis using time - varying matrices @xmath223 and adaptive bound @xmath221 , which are needed for nonlinear dubins car dynamics .",
    "in this paper , an importance sampling - based approximate optimal planning and control method is developed . in the control - theoretic formulation of optimal motion planning ,",
    "the planning algorithm performs direct policy computation using simulation - based adaptive search for an optimal weight vector corresponding to an approximate optimal feedback policy .",
    "each iteration of the algorithm runs time linear in the number of samples and in the time horizon for simulated runs .",
    "however , it is hard to quantify the number of iterations required for to converge .",
    "one future work is to consider incorporate multiple - distribution importance sampling to achieve faster and better convergence results .",
    "based on contraction analysis of the closed - loop system , we show that by modifying the sampling - based policy evaluation step in the algorithm , the proposed planning algorithm can be used for joint planning and robust control for a class of nonlinear systems under bounded disturbances . in future extension of this work ,",
    "we are interested in extending this algorithm for stochastic optimal control .",
    "l.  e. kavraki , p.  svestka , j .- c .",
    "latombe , and m.  h. overmars , `` probabilistic roadmaps for path planning in high - dimensional configuration spaces , '' _ ieee transactions on robotics and automation _ , vol .  12 , no .  4 , pp . 566580 , 1996 .",
    "s.  c. livingston , e.  m. wolff , and r.  m. murray , `` cross - entropy temporal logic motion planning , '' in _ proceedings of the 18th international conference on hybrid systems : computation and control_.1em plus 0.5em minus 0.4emacm , 2015 , pp ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose a sampling - based planning and optimal control method of nonlinear systems under non - differentiable constraints . motivated by developing scalable planning algorithms </S>",
    "<S> , we consider the optimal motion plan to be a feedback controller that can be approximated by a weighted sum of given bases . given this approximate optimal control formulation , </S>",
    "<S> our main contribution is to introduce importance sampling , specifically , model - reference adaptive search algorithm , to iteratively compute the optimal weight parameters , i.e. , the weights corresponding to the optimal policy function approximation given chosen bases . </S>",
    "<S> the key idea is to perform the search by iteratively estimating a parametrized distribution which converges to a dirac s delta that infinitely peaks on the global optimal weights . </S>",
    "<S> then , using this direct policy search , we incorporated trajectory - based verification to ensure that , for a class of nonlinear systems , the obtained policy is not only optimal but robust to bounded disturbances . </S>",
    "<S> the correctness and efficiency of the methods are demonstrated through numerical experiments including linear systems with a nonlinear cost function and motion planning for a dubins car . </S>"
  ]
}