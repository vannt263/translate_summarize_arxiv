{
  "article_text": [
    "the mm principle is a device for constructing optimization algorithms @xcite . in essence",
    ", it replaces the objective function @xmath1 by a simpler surrogate function @xmath2 anchored at the current iterate @xmath3 and majorizing or minorizing @xmath1 . as a byproduct of optimizing @xmath2 with respect to @xmath4 ,",
    "the objective function @xmath1 is sent downhill or uphill , depending on whether the purpose is minimization or maximization .",
    "the next iterate @xmath5 is chosen to optimize the surrogate @xmath2 subject to any relevant constraints .",
    "majorization combines two conditions : the tangency condition @xmath6 and the domination condition @xmath7 for all @xmath4 . in minimization these conditions and the definition of @xmath5 lead to the descent property @xmath8 minorization reverses the domination inequality and produces an ascent algorithm . under appropriate regularity conditions ,",
    "an mm algorithm is guaranteed to converge to a stationary point of the objective function @xcite . from the perspective of dynamical systems , the objective function serves as a liapunov function for the algorithm map .",
    "the mm principle simplifies optimization by : ( a ) separating the variables of a problem , ( b ) avoiding large matrix inversions , ( c ) linearizing a problem , ( d ) restoring symmetry , ( e ) dealing with equality and inequality constraints gracefully , and ( f ) turning a nondifferentiable problem into a smooth problem . choosing a tractable surrogate function @xmath2 that hugs the objective function @xmath1 as tightly as possible requires experience and skill with inequalities .",
    "the majorization relation between functions is closed under the formation of sums , nonnegative products , limits , and composition with an increasing function .",
    "hence , it is possible to work piecemeal in majorizing complicated objective functions .",
    "it is impossible to do justice to the complex history of the mm principle in a paragraph .",
    "the celebrated em ( expectation - maximization ) principle of computational statistics is a special case of the mm principle @xcite .",
    "specific mm and em algorithms appeared years before the principle was well understood @xcite .",
    "the widely applied projected gradient and proximal gradient algorithms can be motivated from the mm perspective , but the early emphasis on operators and fixed points obscured this distinction .",
    "although dempster , laird , and rubin @xcite formally named the em algorithm , many of their contributions were anticipated by baum @xcite and sundberg @xcite .",
    "the mm principle was clearly stated by ortega and rheinboldt @xcite .",
    "de leeuw @xcite is generally credited with recognizing the importance of the principle in practice .",
    "the em algorithm had an immediate and large impact in computational statistics .",
    "the more general mm principle was much slower to take hold .",
    "the papers @xcite by the dutch school of psychometricians solidified its position .",
    "( in this early literature the mm principle is called iterative majorization . )",
    "the related dinklebach @xcite maneuver in fractional linear programming also highlighted the importance of the descent property in algorithm construction .    before moving on ,",
    "let us record some notational conventions .",
    "all vectors and matrices appear in boldface .",
    "the @xmath9 superscript indicates a vector or matrix transpose .",
    "the euclidean norm of a vector @xmath4 is denoted by @xmath10 and the frobenius norm of a matrix @xmath11 by @xmath12 . for a smooth real - valued function @xmath1 ,",
    "we write its gradient ( column vector of partial derivatives ) as @xmath13 , its first differential ( row vector of derivatives ) as @xmath14 , and its second differential ( hessian matrix ) as @xmath15 .",
    "in convex programming it simplifies matters notationally to replace a convex inequality constraint @xmath16 by the concave constraint @xmath17 .",
    "barrier methods operate on the relative interior of the feasible region where all @xmath18 . adding an appropriate barrier term to the objective function @xmath1 keeps an initially inactive constraint @xmath19 inactive throughout an optimization search .",
    "if the barrier function is well designed , it should adapt and permit convergence to a feasible point @xmath20 with one or more inequality constraints active .",
    "we now briefly summarize an adaptive barrier method that does not follow the central path @xcite . because the logarithm of a concave function is concave , the bregman majorization @xcite @xmath21 acts as a convex barrier for a smooth constraint @xmath22 . to make the barrier adaptive , we scale it by the current value @xmath23 of the constraint .",
    "these considerations suggest an mm algorithm based on the surrogate function @xmath24 for @xmath25 inequality constraints . minimizing the surrogate subject to relevant linear equality constraints @xmath26 produces the next iterate @xmath5 .",
    "the constant @xmath27 determines the tradeoff between keeping the constraints inactive and minimizing @xmath1 .",
    "one can show that the mm algorithm with exact minimization converges to the constrained minimum of @xmath1 @xcite .    in practice",
    "one step of newton s method is usually adequate to decrease @xmath1 .",
    "the first step of newton s method minimizes the second - order taylor expansion of @xmath2 around @xmath3 subject to the equality constraints .",
    "given smooth functions , the two differentials @xmath28 are the core ingredients in the quadratic approximation of @xmath2 .",
    "unfortunately , one step of newton s method is neither guaranteed to decrease @xmath1 nor to respect the nonnegativity constraints .",
    "adaptive barrier method for linear programming    for instance , the standard form of linear programming requires minimizing a linear function @xmath29 subject to @xmath30 and @xmath31 .",
    "the quadratic approximation to the surrogate @xmath2 amounts to @xmath32 the minimum of this quadratic subject to the linear equality constraints occurs at the point @xmath33 here @xmath34 is the diagonal matrix with @xmath35th diagonal entry @xmath36 , and the increment @xmath37 satisfies the linear equality constraint @xmath38 .    one can overcome the objections to newton updates by taking a controlled step along the newton direction @xmath39 .",
    "the key is to exploit the theory of self - concordant functions @xcite .",
    "a thrice differentiable convex function @xmath40 is said to be self - concordant if it satisfies the inequality @xmath41 for some constant @xmath42 and all @xmath43 in the essential domain of @xmath40 .",
    "all convex quadratic functions qualify as self - concordant with @xmath44 .",
    "the function @xmath45 is self - concordant with constant 1 .",
    "the class of self - concordant functions is closed under sums and composition with linear functions . a convex function @xmath46 with domain @xmath47",
    "is said to be self - concordant if every slice @xmath48 is self - concordant .    rather than conduct an expensive one - dimensional search along the newton direction @xmath49 , one can majorize the surrogate function @xmath50 along the half - line @xmath51 .",
    "the clever majorization @xmath52 \\label{self_concordant_majorization}\\end{aligned}\\ ] ] serves the dual purpose of guaranteeing a decrease in @xmath1 and preventing a violation of the inequality constraints @xcite . here",
    "@xmath53 is the self - concordance constant associated with the surrogate .",
    "the optimal choice of @xmath43 reduces to the damped newton update @xmath54 the first two derivatives of @xmath40 are clearly @xmath55 ^ 2 .\\end{aligned}\\ ] ] the first of these derivatives is nonpositive because @xmath56 is a descent direction for @xmath1 .",
    "the second is generally positive because all of the contributing terms are nonnegative .",
    ".performance of the adaptive barrier method in linear programming .",
    "[ table0 ] [ cols=\"^,^,^,^,^,^ \" , ]     table [ tab : mm_vs_cvx_spm_table ] compares the performance of the mm algorithm to that of the r ` glasso ` package @xcite .",
    "the sample precision matrix @xmath57 was generated by filling the diagonal and first three subdiagonals of the banded lower triangular matrix @xmath58 with standard normal deviates .",
    "filling @xmath11 with standard normal deviates and choosing @xmath59 imposed a small amount of noise obscuring the band nature of @xmath60 .",
    "all table statistics represent averages over 10 runs started at @xmath61 with @xmath62 equal to the true number of nonzero entries in @xmath60 .",
    "the mm algorithm performs better in minimizing average loss and recovering nonzero entries .",
    "the mm principle offers a unique and potent perspective on high - dimensional optimization .",
    "the current survey emphasizes proximal distance algorithms and their applications in nonlinear programming .",
    "our construction of this new class of algorithms relies on the exact penalty method of clarke @xcite and majorization of a smooth approximation to the euclidean distance to the constraint set .",
    "well - studied proximal maps and euclidean projections constitute the key ingredients of seven realistic examples .",
    "these examples illustrate the versatility of the method in handling nonconvex constraints , its improvement as problem dimension increases , and the pitfalls in sending the tuning constants @xmath27 and @xmath63 too quickly to their limits .",
    "certainly , the proximal distance algorithm is not a panacea for optimization problems .",
    "for example , the proximal distance algorithm as formulated exhibits remarkably fickle behavior on linear programming problems . for linear programming , we ensure numerical stability and guard against premature convergence only by great care in parameter tuning and updating",
    ". nonetheless , we are sufficiently encouraged to pursue this research further , particularly in statistical applications where model fitting and selection are compromised by aggressive penalization .",
    "kenneth lange was supported by nih grants from the national human genome research institute ( hg006139 ) and the national institute of general medical sciences ( gm053275 ) .",
    "kevin l. keys was supported by a national science foundation graduate research fellowship under grant number dge-0707424 .",
    "we are grateful to hua zhou for carefully analyzing the proximal distance algorithm in linear programming .",
    "9 baum le ( 1972 ) an inequality and associated maximization technique in statistical estimation for probabilistic functions of markov processes .",
    "_ inequalities _ 3:18 bauschke hh , borwein jm , li w ( 1999 ) strong conical hull intersection property , bounded linear regularity , jameson s property ( g ) , and error bounds in convex optimization . _ math programming , series a _",
    "86:135160 bauschke hh , combettes pl ( 2011 ) _ convex analysis and monotone operator theory in hilbert spaces .",
    "_ springer , new york borg i , groenen pjf ( 2007 ) _ modern multidimensional scaling : theory and applications_. springer , new york boyd s , vandenberghe l ( 2004 ) _",
    "convex optimization_. cambridge university press , cambridge borwein jm , lewis as ( 2000 ) _ convex analysis and nonlinear optimization : theory and examples .",
    "_ springer , new york bregman lm ( 1967 ) the relaxation method of finding the common points of convex sets and its application to the solution of problems in convex programming .",
    "_ ussr computational math and mathematical physics _",
    "7:200217 cands ej , recht b ( 2009 ) exact matrix completion via convex optimization . _",
    "foundations computational math _ 9:717772 chi e , zhou h , lange k ( 2013 ) distance majorization and its applications .",
    "_ math programming series a _ ( in press ) clarke fh ( 1983 ) _ optimization and nonsmooth analysis_. wiley - interscience demyanov vf ( 2010 ) nonsmooth optimization , in _ nonlinear optimization _ ( editors di  pillo g , schoen f ) , springer , new york demyanov vf , di  pillo g , facchinei f ( 1998 ) exact penalization via dini and hadamard conditional derivatives . _",
    "optimization methods and software _",
    "9:1936 de leeuw j ( 1977 ) applications of convex analysis to multidimensional scaling . _ recent developments in statistics _ , edited by barra jr , brodeau f ,",
    "romier g , van cutsem b , north holland publishing company , pp .",
    "133146 de leeuw j ( 1990 ) multivariate analysis with optimal scaling . _",
    "progress in multivariate analysis _ , edited by das gupta s , sethuraman j , indian statistical institute dempster ap , laird nm , rubin db ( 1977 ) maximum likelihood from incomplete data via the em algorithm ( with discussion ) .",
    "_ j roy stat soc b _ 39:138 deutsch f ( 2001 ) _ best approximation in inner product spaces . _",
    "springer , new york dinkelbach w ( 1967 ) . on nonlinear fractional programming .",
    "_ management science _",
    "13:492498 dykstra rl ( 1983 ) an algorithm for restricted least squares estimation .",
    "_ jasa _ 78:837842 friedman j , hastie t , tibshirani r ( 2008 ) sparse inverse covariance estimation with the graphical lasso .",
    "_ biostatistics _",
    "9:432441 golub gh , van loan cf ( 1996 ) _ matrix computations _ , 3rd ed .",
    "johns hopkins university press , baltimore , md ( 2013 ) cvx : matlab software for disciplined convex programming , version 2.0 beta hartley ho ( 1958 ) maximum likelihood estimation from incomplete data .",
    "_ biometrics _ 14:174194 heiser wj ( 1995 ) , convergent computing by iterative majorization : theory and applications in multidimensional data analysis .",
    "_ recent advances in descriptive multivariate analysis _ , edited by krzanowski wj ) , oxford university press , pp .",
    "157189 hoffman aj ( 1952 ) on approximate solutions of systems of linear inequalities .",
    "_ j res nat bur stand _",
    "49:263265 hunter dr , lange k ( 2004 ) a tutorial on mm algorithms . _",
    "amer statistician _",
    "58:3037 kiers h ( 1990 ) majorization as a tool for optimizing a class of matrix functions .",
    "_ psychometrika _ 55:417428 lange k ( 1994 ) an adaptive barrier method for convex programming .",
    "_ methods applications analysis _ 1:392402 lange k , hunter d , yang i ( 2000 ) optimization transfer using surrogate objective functions ( with discussion ) .",
    "_ j computational graphical stat _",
    "9:159 lange k ( 2010 ) @xmath64 _ numerical analysis for statisticians _ , 2nd ed .",
    "springer lange k ( 2013 ) _ optimization _",
    "springer ( 2004 ) yalmip : a toolbox for modeling and optimization in matlab .",
    "_ proceedings of the 2004 cacsd conference _ ,",
    "taipei , taiwan mckendrick ag ( 1926 ) applications of mathematics to medical problems .",
    "_ proc edinburgh math soc _",
    "44:134 mclachlan gj , krishnan t ( 2008 ) _ the em algorithm and extensions , _",
    "wiley , hoboken , nj mazumder r , hastie t , tibshirani r ( 2010 ) spectral regularization algorithms for learning large incomplete matrices .",
    "_ j machine learning res _",
    "11:22872322 nesterov y , nemirovski a ( 1994 ) _ interior - point polynomial algorithms in convex programming_. siam , philadelphia ortega jm , rheinboldt wc ( 1970 ) _ iterative solution of nonlinear equations in several variables_. academic , pp .",
    "253255 parikh n , boyd s ( 2013 ) proximal algorithms .",
    "_ foundations trends optimization _",
    "1:123231 smith cab ( 1957 ) counting methods in genetical statistics .",
    "_ ann hum genet _",
    "sundberg r ( 1976 ) an iterative method for solution of the likelihood equations for incomplete data from exponential families .",
    "_ communications stat b _",
    "5:5564 weiszfeld , e ( 1937 ) on the point for which the sum of the distances to @xmath65 given points is minimum .",
    "_ ann oper research _ 167:741 . translated from the french original in",
    "_ tohoku math j _ 43:335386 ( 1937 ) and annotated by frank plastria yates f ( 1934 ) the analysis of multiple classifications with unequal numbers in different classes",
    ". _ j amer stat assoc _"
  ],
  "abstract_text": [
    "<S> the mm principle is a device for creating optimization algorithms satisfying the ascent or descent property . </S>",
    "<S> the current survey emphasizes the role of the mm principle in nonlinear programming . for smooth functions </S>",
    "<S> , one can construct an adaptive interior point method based on scaled bregman barriers . </S>",
    "<S> this algorithm does not follow the central path . </S>",
    "<S> for convex programming subject to nonsmooth constraints , one can combine an exact penalty method with distance majorization to create versatile algorithms that are effective even in discrete optimization . </S>",
    "<S> these proximal distance algorithms are highly modular and reduce to set projections and proximal mappings , both very well - understood techniques in optimization . </S>",
    "<S> we illustrate the possibilities in linear programming , binary piecewise - linear programming , nonnegative quadratic programming , @xmath0 regression , matrix completion , and inverse sparse covariance estimation .    </S>",
    "<S> primary 90c59 ; secondary 65c60 .    </S>",
    "<S> majorization , convexity , exact penalty method , computational statistics . </S>"
  ]
}