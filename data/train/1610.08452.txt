{
  "article_text": [
    "the emergence and wide - spread usage of automated data - driven decision making systems in a wide variety of applications , ranging from content recommendations to pretrial risk assessment , has raised concerns about their potential unfairness towards people with certain traits  @xcite . anti - discrimination laws in various countries",
    "prohibit unfair treatment of individuals based on specific traits , also called _ sensitive _ attributes ( , gender , race ) .",
    "these laws typically distinguish between two different notions of unfairness  @xcite : * _ disparate treatment _ * , which focuses on the decision making _ process _ itself , and * _ disparate impact _ * , which focuses on the _ outcomes _ of the process .",
    "more specifically , there is disparate treatment if the decision making process uses sensitive attribute information , and there is disparate impact if the outcomes of the decision making process disproportionately benefit or hurt members of certain sensitive attribute value groups .",
    "a number of recent studies have focused on designing decision making systems which avoid one or both of the above types of unfairness  @xcite .",
    "these prior designs have attempted to tackle unfairness in decision making scenarios where the decisions in training data are _ biased _ ( , groups of people with certain sensitive attributes may have historically received unfair treatment ) and there is no ground truth about the _ correctness _ of historical decisions ( , one can not tell whether a decision in training data was right or wrong ) . however ,",
    "when the ground truth for historical decisions is available , disproportionately beneficial outcomes for certain sensitive attribute value groups can be justified and explained by the means of the ground truth .",
    "disparate impact would not be a suitable notion of unfairness in such scenarios .    in this paper",
    ", we propose an alternative notion of unfairness , * _ disparate mistreatment _ * , especially well - suited for scenarios where ground truth is available for decisions in training data .",
    "we identify a decision making process as suffering from disparate mistreatment with respect to a given sensitive attribute ( , race ) if the _ misclassification rates _ for groups of people having different values of the sensitive attribute ( , blacks and whites ) are different .",
    "for example , in the case of nypd stop - question - and - frisk program ( sqf ) , where people are stopped on the suspicion of having a weapon  @xcite , having different weapon discovery rates for different races would constitute a case of disparate mistreatment .",
    "in addition to _ all _ misclassifications in general , depending on the application scenario , one might want to define disparate mistreatment with respect to different kinds of misclassifications : false positives , false negatives , or both .",
    "for example , in pretrial risk assessments , the decision making process might only be required to ensure that false positive rates are equal since it may be more acceptable to let a guilty person go , rather than incarcerating an innocent person  @xcite . on the other hand , in loan approval systems",
    ", one might instead favor a decision making process in which the false negative rates are equal , to ensure that deserving ( positive class ) people with a certain sensitive attribute value are not denied ( negative class ) loans disproportionately .",
    "finally , in the nypd sqf program , one may argue for a decision making process in which both false positive and false negative rates ( on stopping someone on the suspicion of carrying a weapon ) are the same for both blacks and whites .",
    "in the remainder of the paper , we first define and formalize disparate treatment , disparate impact and disparate mistreatment in the context of ( binary ) classification .",
    "then , we introduce an intuitive measure of disparate mistreatment for decision boundary - based classifiers and show that , for a wide variety of linear and nonlinear classifiers , this measure can be incorporated into their formulation as a convex - concave constraint .",
    "the resulting formulation can be solved efficiently using recent advances in convex - concave programming  @xcite . finally , we experiment with synthetic as well as real world datasets and show that our methodology can be effectively used to avoid disparate mistreatment .",
    "in this section , we first explain the three different notions of unfairness in decision making scenarios using an illustrative example .",
    "next , we briefly discuss related work on removing two of the three notions namely , _",
    "disparate treatment _ and _ disparate impact_. to the best of our knowledge , there is no prior literature on removing the third notion of unfairness , _ disparate mistreatment _ , from automated decision making systems .",
    "* disparate mistreatment . *",
    "intuitively , disparate mistreatment can arise in any learning scenario , where the outputs ( decisions ) of the trained algorithm are not perfectly ( i.e. , 100% ) accurate .",
    "for instance , consider learning binary classification using logistic regression .",
    "if the data items in the two classes are not linearly separable ( as is often the case in many real - world application scenarios ) , the learned algorithm will misclassify ( i.e. , produce false positives , false negatives , or both ) some data items .",
    "if the learned algorithm misclassifies people of different genders ( males and females ) or races ( blacks and whites ) at different rates , then we refer to such decision making as unfair due to disparate mistreatment .",
    "we illustrate decision making scenarios with and without disparate mistreatment in figure  [ table : examples - disc ] .",
    "the scenario involves making a decision to either stop or not stop a pedestrian using their features such as gender , bulge in clothing and proximity to a crime scene . the `` ground truth '' data for whether a pedestrian actually possesses a weapon is also shown .",
    "we show decisions made by three different classifiers @xmath0 , @xmath1 and @xmath2 .",
    "we deem @xmath0 and @xmath1 as unfair due to disparate mistreatment because their rate of erroneous decisions for men and women are different : @xmath0 has different false negative rates for males and females ( @xmath3 and @xmath4 , respectively ) , whereas @xmath1 has different false positive rates ( @xmath3 and @xmath5 ) as well as different false negative rates ( @xmath3 and @xmath4 ) for males and females .    * disparate treatment . *",
    "in contrast to disparate mistreatment , disparate treatment occurs when the decisions of a classifier for two users who have the same ( or similar ) values of non - sensitive attributes , but different values of sensitive attributes , are different .",
    "for instance , in figure  [ table : examples - disc ] , we deem @xmath1 and @xmath2 to be unfair due to disparate treatment since @xmath6 s ( @xmath7 s ) decisions for @xmath8 and @xmath9 ( @xmath10 and @xmath11 ) are different even though they have the same values of non - sensitive attributes . here , disparate treatment corresponds to the very intuitive notion of fairness that two otherwise similar users should not be treated differently solely because of their differences in gender .",
    "* disparate impact .",
    "* finally , disparate impact is said to occur when the fraction of classifier decisions that benefit ( or do not benefit ) users belonging to different sensitive attribute groups are different ( here , a user benefits from a decision of not being stopped ) .",
    "for instance , in figure  [ table : examples - disc ] , we deem @xmath0 as unfair due to disparate impact because the fraction of males and females that were stopped are different ( @xmath5 and @xmath12 , respectively ) .    * application scenarios for disparate impact vs. disparate mistreatment . * note that unlike in the case of disparate mistreatment , the notion of disparate impact is independent of the `` ground truth '' information about the decisions , , whether or not the decisions are correct or valid .",
    "thus , the notion of disparate impact is particularly appealing in application scenarios where ground truth information for decisions does not exist and the decision labels in training data are not reliable and can not be trusted .",
    "unreliability of training labels for automated decision making systems is particularly concerning in scenarios like recruiting or loan approvals , where biased judgments by humans in the past may be used when training classifiers for the future .",
    "in such application scenarios , it is hard to distinguish correct and incorrect decisions , making it hard to assess or use disparate mistreatment as a notion of fairness .",
    "however , in application scenarios where ground truth information for decisions can be obtained , disparate impact can be quite misleading as a notion of fairness .",
    "that is , in scenarios where validity of decisions can be reliably ascertained , it would be possible to distinguish disproportionality in decision outcomes for sensitive groups that arises from justifiable reasons ( , qualification of the users ) and disproportionality in outcomes that arises from non - justifiable reasons ( i.e. , discrimination against sensitive group members ) . by requiring decision outcomes to be proportional , disparate impact risks introducing reverse - discrimination against qualified candidates . such practices",
    "have previously been deemed unlawful ( _ ricci vs. destefano , 2009 _ ) by courts .",
    "in contrast , when the correctness of decisions can be determined , disparate mistreatment can not only be accurately assessed , but also avoids reverse - discrimination , making it a more appealing notion of fairness",
    ".    * related work .",
    "* there have been a number of studies proposing methods for detecting @xcite and removing @xcite discrimination in the context of disparate treatment and disparate impact .",
    "however , as pointed out earlier , these notions might be less meaningful in scenarios where ground truth decisions are available .",
    "a number of recent studies have pointed out racial disparities in both automated @xcite as well as human @xcite decision making systems related to criminal justice .",
    "for example , a work by goel et al .",
    "@xcite detects racial disparities in nypd sqf program , inspired by a notion of unfairness similar to our notion of disparate mistreatment .",
    "more specifically , it uses ground truth ( stops leading to successful discovery of a weapon on the suspect ) to show that blacks were treated unfairly since false positive rates in stops were higher for them than for whites . the study s findings provide further justification for the need of data - driven decision making systems without disparate mistreatment .",
    "our formalization of disparate mistreatment draws inspiration from the analysis of false positive and false negative rates performed by these prior studies  @xcite .    finally , a recent study @xcite analyzing a recidivism risk assessment dataset pointed out that in scenarios where ground truth validation of decisions is available , ensuring disparate impact may lead to different false positive and false negative rates ( , disparate mistreatment ) .",
    "this is in line with our intuition that in the presence of ground truth validation , disparate impact might be an unsuitable notion of fairness .",
    "in a binary classification task , the goal is to learn a mapping @xmath13 between user feature vectors @xmath14 and class labels @xmath15 . learning this mapping",
    "is often achieved by finding a decision boundary @xmath16 in the feature space that minimizes a certain loss @xmath17 , , @xmath18 , computed on a training set @xmath19 .",
    "then , for a given _ unseen _ feature vector @xmath20 , the classifier predicts the class label @xmath21 if @xmath22 and @xmath23 otherwise , where @xmath24 denotes the signed distance from @xmath20 to the decision boundary .",
    "here , we also assume that each user has an associated sensitive feature @xmath25 .",
    "for ease of exposition , we assume @xmath25 to be binary taking values in @xmath26 , however , our setup can be easily generalized to categorical as well as multiple sensitive features .    given the above terminology , we can formally express the absence of disparate treatment , disparate impact and disparate mistreatment as follows :    * existing notion 1 : avoiding disparate treatment .",
    "* a binary classifier does not suffer from disparate treatment if : @xmath27 , if the probability that the classifier outputs @xmath28 given a feature vector @xmath20 does not change after observing the sensitive feature @xmath25 , there is no disparate treatment .    * existing notion 2 : avoiding disparate impact .",
    "* a binary classifier does not suffer from disparate impact if : @xmath29 , if the probability that a classifier assigns a user to the positive class , @xmath30 , is the same for both values of the sensitive feature @xmath25 , then there is no disparate impact .",
    "* new notion 3 : avoiding disparate mistreatment .",
    "* a binary classifier does not suffer from disparate mistreatment if the misclassification rates  be it all misclassifications combined , false positives or false negatives  for particular groups of people sharing certain sensitive attributes are the same .",
    "that is : @xmath31",
    "in this section , we describe how to train decision boundary - based classifiers ( , logistic regression , svms ) that do not suffer from disparate mistreatment .",
    "these classifiers generally learn the optimal decision boundary by minimizing a convex loss @xmath17 .",
    "the convexity of @xmath17 ensures that a global optimum can be found _",
    "efficiently_. in order to ensure that the learned boundary is fair  it does not suffer from disparate mistreatment",
    " one could incorporate the appropriate condition from eqs .",
    "[ eq : prob_misclass]-[eq : prob_misclass_y1 ] ( based on which kind of misclassifications disparate mistreatment is being defined for ) into the classifier formulation .",
    "for example : @xmath32 where @xmath33 and the smaller @xmath34 is , the more fair the decision boundary would be .",
    "the above formulation ensures that the classifier chooses the optimal decision boundary _ within _ the space of fair boundaries specified by the constraints .",
    "however , since the conditions in eqs .",
    "[ eq : prob_misclass]-[eq : prob_misclass_y1 ] may present many saddle points , solving the constrained optimization problem in eq .",
    "seems difficult .    to overcome the above difficulty",
    ", we propose a tractable proxy , inspired by the disparate impact proxy proposed by zafar et al .",
    "in particular , we propose to measure disparate mistreatment using the covariance between the users sensitive attributes and the signed distance between the feature vectors of misclassified users and classifier decision boundary , : @xmath35 \\\\ & \\approx & \\frac{1}{n } \\sum_{i=1}^{n } \\left(z_i - \\bar{z}\\right ) g_{\\thetab}(y_i , \\mathbf{x}_i ) , \\end{aligned}\\ ] ] where the term  @xmath36   \\bar{g}_{\\thetab}(\\mathbf{x})$ ] cancels out since @xmath36 = 0 $ ] and the function @xmath37 is defined as : @xmath38 which approximates , respectively , the conditions in eqs .",
    "[ eq : prob_misclass]-[eq : prob_misclass_y1 ] ) .",
    "note that , if a decision boundary satisfies eqs .",
    "[ eq : prob_misclass]-[eq : prob_misclass_y1 ] , the covariance defined above for that boundary will be close to zero , , @xmath39 . moreover , in linear models for classification , such as logistic regression or linear svms ,",
    "the decision boundary is simply the hyperplane defined by @xmath40 , therefore , @xmath41 .",
    "given the above proxy , one can rewrite eq .",
    "[ eq : cons_misclass_prob ] as : @xmath42 where the covariance threshold @xmath43 controls how _",
    "adherent _ to disparate mistreatment the decision boundary should be . defined in both eq .",
    "[ eq : g_fpr ] and eq .",
    "[ eq : g_fnr ] . ]    * solving the problem efficiently .",
    "* while the constraints proposed in eq .",
    "[ eq : cons_misclass_non_convex ] can be an effective proxy for fairness , they are non - convex , making it challenging to efficiently solve the optimization problem in eq .",
    "[ eq : cons_misclass_non_convex ] .",
    "next , we will convert these constraints into a disciplined convex - concave program ( dccp ) , which can be solved efficiently by using recently proposed heuristics @xcite .",
    "first , consider the constraint described in eq .",
    "[ eq : cons_misclass_non_convex ] , , @xmath44 where @xmath45 may denote ` @xmath46 ' or ` @xmath47 ' .",
    "also we drop the constant number @xmath48 for the sake of simplicity .",
    "since the sensitive feature @xmath25 is binary , , @xmath49 , we can split relation into two parts as : @xmath50 where @xmath51 and @xmath52 are the number of training points taking value @xmath53 and @xmath54 , respectively . moreover , since @xmath55 , we can rewrite [ eq : cons_nonconvex_1 ] as : @xmath56 which , given that @xmath57 is convex in @xmath58 , results into a convex - concave ( or , difference of convex ) function . then , substituiting eq .",
    "[ eq : cons_nonconvex_2 ] into eq .",
    "[ eq : cons_misclass_non_convex ] , we obtain :    @xmath59    which is a disciplined convex - concave program ( dccp ) for any convex loss @xmath17 , and can be efficiently solved using well - known heuristics such as the one presented in @xcite .",
    "next , we particularize the formulation given by eq .",
    "[ eq : cons_misclass_convex ] for a logistic regression classifier  @xcite .    * logistic regression without disparate mistreatment . * in logistic regression , the optimal decision boundary @xmath60 can be found by solving a maximum likelihood problem of the form @xmath61 in the training phase .",
    "hence , a fair logistic regressor can be trained by solving the following constrained optimization problem : @xmath62",
    "in this section , we conduct experiments on synthetic as well as real world datasets to evaluate the effectiveness of our scheme in controlling disparate mistreatment . to this end",
    ", we first generate several _ synthetic _ datasets to simulate different variations of disparate mistreatment and show that our method can effectively remove disparate mistreatment in each of the variations , often at small cost in terms of accuracy .",
    "then , we conduct experiments on propublica compas dataset @xcite to show the effectiveness of our method on a _ real world _ dataset .    in this section , for the sake of simplicity , we assume the sensitive attribute to be binary",
    ". however , our methodology and the evaluation can be easily generalized to non - binary , and various sensitive attributes .",
    "additionally , to ensure robustness of experimental findings , for all of the datasets , we repeatedly split the data into training ( 50% ) and testing ( 50% ) folds 5 times and report the average numbers for accuracy and fairness .",
    "* evaluation metrics . * following the definitions of disparate mistreatment provided in section  [ sec : format_setting ] , we define the following metrics to quantify fairness : @xmath63 where the closer the values of @xmath64 and @xmath65 to @xmath66 , the lower the amount of disparate mistreatment . more specifically , @xmath67 ( @xmath68 ) ensures the fulfillment of eq .",
    "[ eq : prob_misclass_yminus1 ] ( eq .",
    "[ eq : prob_misclass_y1 ] ) , , it ensures fairness with respect to the false positive ( negative ) rate . additionally , in order to ensure fairness with respect to both false positive and false negative rates , , to satisfy eq .  [ eq : prob_misclass ] , both @xmath64 and @xmath65 should be zero .",
    "finally , in the synthetic dataset experiments , we will refer to sensitive attribute values group @xmath69 as protected group , and @xmath70 as non - protected group .",
    "train a ( unfair ) logistic regressor @xmath71    compute predicted labels for the training set as @xmath72 and discrimination metric @xmath64 .",
    "set @xmath73 if @xmath74 , otherwise , @xmath75 .",
    "select @xmath76 and denote @xmath77 the rest of the training set .    * baseline . *",
    "as pointed out earlier , this work is the first to introduce a mechanism for removing disparate mistreatment in the outcomes of a classification task .",
    "since there are no methods available for performance comparison , we ourselves propose a baseline , which tries to remove disparate mistreatment by introducing different penalties for misclassified data points with different sensitive attribute values .",
    "this baseline consists of two main steps .",
    "first , we train a ( unfair ) logistic regressor over the training data and select the set of misclassified data points sharing a sensitive attribute value that presents the higher error rate . for example , if one wants to control for fairness in terms in false positive rate and @xmath74 in the unfair logistic regressor , then we select the set of misclassified data points in training set such that @xmath53 and @xmath78 .",
    "second , we iteratively re - train the logistic regressor with different ( higher ) penalties on this set of data points until a certain fairness level is achieved in the training set , in our example , until @xmath79 .",
    "algorithm in figure  [ alg : baseline ] particularizes this process to ensure fairness in terms of false positive rate .",
    "this process can be intuitively extended to account for fairness in term of false negative rate or for both false positive as well as false negative rates .      in this section",
    ", we empirically study the trade - off between fairness and accuracy , in a classifier that suffers from disparate mistreatment in its outcomes . to this end , we first start with a simple scenario in which the classifier is unfair only in terms of either false positive or false negative rate",
    ". then , we focus on a more complex scenario in which the classifier is unfair in terms of both false positive and negative rates .      the first scenario accounts for cases in which after training a classifier on the the ground truth labels , we find its outcomes to suffer from disparate mistreatment in terms of only the false positive ( or false negative ) rate , while they are fair with respect to the other misclassification rate , , @xmath80 and @xmath81 ( or , alternatively , @xmath82 and @xmath83 ) .",
    "* experimental setup . * to simulate such a scenario ,",
    "we first generate @xmath84 binary class labels ( @xmath15 ) and corresponding sensitive feature values ( @xmath85 ) , both uniformly at random , and assign a two - dimensional user feature vector ( @xmath20 ) to each of the points . to ensure different distributions for negative class ( so that the two sensitive attribute groups have different false positive rates )",
    ", the user feature vectors are sampled from the following distributions ( we sample @xmath86 points from each distribution ) : @xmath87 , [ 3 , 1 ; 1 , 3 ] ) \\nonumber \\\\ p(\\mathbf{x}|z=1 , y=+1 ) & = n([2,2 ] , [ 3 , 1 ; 1 , 3 ] ) \\nonumber \\\\ p(\\mathbf{x}|z=0 , y=-1 ) & = n([1,1 ] , [ 3 , 3 ; 1 , 3 ] ) \\nonumber \\\\ p(\\mathbf{x}|z=1 , y=-1 ) & = n([-2,-2 ] , [ 3 , 1 ; 1 , 3 ] ) \\nonumber\\end{aligned}\\ ] ]    then , we train a ( unconstrained ) logistic regression classifier on the data .",
    "the classifier is able to achieve an accuracy of @xmath88 , however , due to difference in feature distributions for the data points with different sensitive attribute values , while @xmath89 , @xmath90 , which constitutes a clear example of disparate mistreatment in terms of false positive rate .",
    "finally , we train several logistic regression classifiers on the same training data subject to fairness constraints on false positive rates , , we train a logistic regressor as in eq .",
    "[ eq : cons_misclass_convex_logreg ] , where @xmath91 is given by eq .",
    "[ eq : g_fpr ] .",
    "each classifier constrains the false positive covariance with a multiplicative factor ( @xmath92 $ ] ) of the covariance of the unconstrained classifier . additionally , we train the proposed baseline as detailed above .",
    "* results .",
    "* figure  [ fig : syn_no_corr ] summarizes the results for this scenario by showing ( a ) the relation between decision - boundary covariance and the false negative rates for both sensitive attribute values ; ( b ) the trade - off between accuracy and fairness ; and ( c ) the decision boundaries for both the unconstrained classifiers ( solid ) and the fair constrained classifier ( dashed ) . in this figure",
    ", we observe that : i ) as the fairness constraint value @xmath93 goes to zero , the false positive rates for both @xmath69 and @xmath70 converge , and hence , the outcomes of the classifier become more fair , , @xmath94 , while @xmath65 remains close to zero ( the invariance of @xmath65 may however change depending on the underlying distribution of the data ) ; ii ) ensuring lower values of disparate mistreatment leads to a larger drop in accuracy , however , iii ) the drop in accuracy for the constrained classifier ( @xmath95 and @xmath96 ) is still quite modest as compared to the baseline ( denoted by @xmath97 and @xmath98 ) .",
    "+     + [ fig : syn_same_fp_fn_fpr ] [ fig : syn_same_fp_fn_fnr ] [ fig : syn_same_fp_fn_both ]      in this section we consider a more complex scenario , compared to the one presented in previous section , in which the outcomes of the classifier suffer from disparate mistreatment with respect to both false positive as well as false negative rates , , both @xmath64 and @xmath65 are non - zero .",
    "this scenario can in turn be split into two cases : ( i ) @xmath64 and @xmath65 have opposite signs , , the decision boundary favors subjects sharing a sensitive attribute value to be in the positive class ( even when such assignments are misclassifications ) while assigning the subjects not sharing that value to the negative class and ( ii ) both @xmath64 and @xmath65 have the same sign , , both error rates are higher for the subjects sharing a sensitive attribute value .",
    "* case i. * the first case , where @xmath64 and @xmath65 have opposite signs , occurs when the decision boundary favors one sensitive attribute value group by misclassifying a relatively large number of its negative points into positive class ( resulting in a relatively high false positive rate ) and disfavors the other group by misclassifying a large number of its positive points into negative class ( resulting in a high false negative rate ) .    to simulate this scenario ,",
    "we first generate @xmath99 samples from each of the following distributions : @xmath100 , [ 5 , 1 ; 1 , 5 ] ) \\nonumber \\\\ p(\\mathbf{x}|z=1 , y=1 ) & = n([2,3 ] , [ 5 , 1 ; 1 , 5 ] ) \\nonumber \\\\ p(\\mathbf{x}|z=0 , y=-1 ) & = n([-1,-3 ] , [ 5 , 1 ; 1 , 5 ] ) \\nonumber \\\\ p(\\mathbf{x}|z=1 , y=-1 ) & = n([-1,0 ] , [ 5 , 1 ; 1 , 5 ] ) \\nonumber\\end{aligned}\\ ] ]    an unconstrained logistic regression classifier on this dataset attains an overall accuracy of @xmath101 but leads to a false positive rate of @xmath102 and @xmath103 ( , @xmath104 ) for the protected and the non - protected group ; and false negative rates of @xmath105 and @xmath106 ( , @xmath107 ) for the protected and the non - protected group . finally , we train three different type of fair classifiers , with fairness constraints on ( i ) false positive rates@xmath91 given by eq .",
    "[ eq : g_fpr ] , ( ii ) false negative rates@xmath91 given by eq .",
    "[ eq : g_fnr ] and ( iii ) on both false positive and false negative rates ",
    "separate constraints for @xmath91 given by eq .",
    "[ eq : g_fpr ] and eq .",
    "[ eq : g_fnr ] .",
    "* results .",
    "* figure  [ fig : syn_opposite_fp_fn ] summarizes the results for this scenario by showing the ( top ) trade - off between fairness and accuracy for the three constrained classifiers described above , and ( bottom ) the decision boundaries for the unconstrained classifier ( solid ) and the constrained fair classifiers . here",
    ", we can observe several interesting patterns .",
    "first , removing disparate mistreatment on only false positives causes a rotation in the decision boundary to move _ previously misclassified _ non - protected examples into negative class , _ decreasing _ false positive rate .",
    "however , in the process , it also moves previously _ well - classified _ non - protected examples into negative class , _ increasing _ the false negative rate for the non - protected group . as a consequence ,",
    "controlling disparate mistreatment on false positives ( figure  [ fig : syn_opposite_fp_fn](a ) ) , also removes disparate mistreatment on false negatives . note that , as similar effect occurs when we control disparate mistreatment only with respect to the false positive rate ( figure  [ fig : syn_opposite_fp_fn](b ) ) , and therefore , provides similar results that the constrained classifier for both false positive and negative rates ( figure  [ fig : syn_opposite_fp_fn](c ) ) .",
    "this effect is explained by the distribution of the data , where the centroids of the clusters for the protected group are shifted with respect to the ones for the non - protected group . finally",
    ", we remark that while the constrained classifiers ( denoted by @xmath95 and @xmath96 ) remove disparate mistreatment with respect to both false positive and negative rates at a moderate loss in accuracy , the baseline ( denoted by @xmath97 and @xmath98 ) do not remove disparate mistreatment with respect to both false positive and negative rates in any of the cases and result in a equal or higher loss in accuracy for the same fairness level ( , @xmath64 and @xmath65 values ) .",
    "* case ii .",
    "* the second case account for scenarios in which after training a ( unconstrained ) classifier , its outcomes results in disparate mistreatment where both @xmath64 and @xmath65 share the same sign , , both error rates are higher for the subjects sharing a certain sensitive attribute value .",
    "this scenario is likely to occur in datasets where the subjects with one sensitive attribute value ( , females ) are harder to classify than the subjects not sharing that sensitive attribute value ( , males)in a extreme case , the two classes may be linearly separable for males , but not for the females .",
    "* experimental setup . * to simulate this scenario , we first generate @xmath99 samples from each of the following distribution : @xmath108 , [ 5 , 2 ; 2 , 5 ] ) \\nonumber \\\\ p(\\mathbf{x}|z=1 , y=1 ) & = n([2,3 ] , [ 10 , 1 ; 1 , 4 ] ) \\nonumber \\\\ p(\\mathbf{x}|z=0 , y=-1 ) & = n([0,-1 ] , [ 7 , 1 ; 1 , 7 ] ) \\nonumber \\\\ p(\\mathbf{x}|z=1 , y=-1 ) & = n([-5,0 ] , [ 5 , 1 ; 1 , 5 ] ) \\nonumber\\end{aligned}\\ ] ] then , we train an unconstrained logistic regression classifier on this dataset .",
    "it attains an accuracy of @xmath109 but leads to @xmath110 and @xmath111 , which constitutes a clear case of disparate mistreatment in terms of both false positive and negative rates .",
    "then , similar to the previous scenario , we train three different kind of constrained classifiers to remove disparate mistreatment on ( i ) false positives , ( ii ) false negatives . and",
    "( iii ) both .",
    "* results .",
    "* figure  [ fig : syn_same_fp_fn ] summarizes the results by showing the ( top ) the trade - off between accuracy and fairness , and ( bottom ) the decision boundaries for both the unconstrained classifiers ( solid ) and the fair constrained classifier ( dashed ) when accounting for disparate mistreatment with respect to false positive rate , false negative rate or both . in this figure",
    ", we observe several interesting patterns .",
    "first , controlling disparate mistreatment for only false positives ( false negatives ) , leads to a minor drop in accuracy , but can exacerbate the disparate mistreatment on false negatives ( false positives ) .",
    "for example , while the decision boundary is moved to control for false negative disparate mistreatment to ensure that more protected examples are well - classified in positive class ( reducing false negative rate ) , it also moves previously well - classified negative examples into the positive class , hence increasing the false positive rate .",
    "a similar phenomenon occur when controlling disparate mistreatment with respect to only false positive rate .",
    "as a consequence , controlling for both types of disparate mistreatment simultaneously brings @xmath64 and @xmath65 close to zero , but causes a large drop in accuracy .",
    "second , while the constrained classifiers always remove the targeted disparate mistreatment often at a low cost in accuracy , the baseline is not able to ensure fairness in the final setting ( controlling for both ) and also presents a larger drop in accuracy in all settings .",
    ".statistics on compas data showing whether an offender recidivated within two years or not . [ cols=\"^,^,^,^\",options=\"header \" , ]     [ fig : compas_fpr ] [ fig : compas_fnr ] [ fig : compas_both ]      in this section , we experiment with compas risk assessment dataset compiled by propublica @xcite and show that our method can significantly reduce disparate mistreatment at a modest cost in terms of accuracy .",
    "* dataset and experimental setup .",
    "* propublica compiled a list of all offenders screened through correctional offender management profiling for alternative sanctions ( compas ) tool in broward county , florida during 2013 - 2014 .",
    "the data includes information about offenders demographic features ( gender , race , age ) , criminal history ( charge for which the person was arrested , number of prior offenses ) and the risk score assigned to the offender by compas .",
    "propublica also collected the _ ground truth _ on whether or not these individuals actually recidivated within two years after screening . for more information about how the data was collected",
    ", we point the reader to @xcite .",
    "finally , for the sake of simplicity , in this analysis , we will only consider a subset of offenders whose race was either black or white .",
    "the information about recidivism rates for each race are included in table [ table : compas - stats ] .    using this ground truth",
    ", we build a logistic regression classifier to predict whether ( positive class ) or not ( negative class ) an offender would recidivate within two years .",
    "the set of features used in the classification task are described in table [ table : compas - features ] , which is the same as the features used by propublica for their analysis of modeling risk scores .",
    "the logistic regression classifier leads to an accuracy of @xmath112 and auc of @xmath113 .",
    "however , the classifier yields in a false positive rate of @xmath114 and @xmath115 for respectively blacks and whites ( , @xmath116 ) , and false negative rates for respectively blacks and whites of @xmath117 and @xmath118 ( , @xmath119 ) .",
    "this results constitute a clear case of disparate mistreatmentin terms of both false positive and negative rates , where one sensitive feature group ( blacks ) is put on relative disadvantage by the classifier by disproportionally misclassifying negative ( good , did not recidivate ) examples into positive ( bad ) class ( false positives ) , while the other group ( whites ) is put on relative advantage by disproportionally misclassifying positive ( bad , recidivated ) examples into negative ( good ) class ( false negatives ) .",
    "note therefore that this scenario resembles our synthetic example case i in section [ sec : syn_both_disc ]",
    ".    finally , we train logistic regression classifiers with three types constraints : false positives , false negatives , and both .    * results .",
    "* figure  [ fig : compas ] summarizes the results by showing the trade - off between fairness and accuracy for the three constrained classifiers and the proposed baselines .",
    "similarly to the results in section [ sec : syn_both_disc ] , we observe here that the logistic regressor with constraints on false positive ( or alternatively negative ) rate is also controlling disparate mistreatmentin terms of false negative ( or alternatively positive ) rate , and it does so at a lower drop in accuracy than the baseline .",
    "however , when we control disparate mistreatmentin terms of both false positive and negative rates , the baseline provides a slightly better trade - off between accuracy and fairness than the proposed constrained logistic regressor .",
    "additionally , we observe that the constrained classifiers ( as well as the baseline classifier ) do not remove completely disparate mistreatment , , they do nt achieve zero @xmath120 or / and @xmath121 in any of the cases .",
    "this is probably due to the relatively small size of the dataset ( @xmath122 examples in the training set ) , which hinders a robust estimate of misclassification covariance ( eqs .",
    "[ eq : g_fpr ] and [ eq : g_fnr ] ) .",
    "this highlights the fact that our method can suffer from reduced performance for small datasets . in scenarios with sufficiently large training datasets ,",
    "we expect more reliable estimates of covariance , and hence , a better performance from our method .",
    "as shown in section [ sec : eval ] , our method provides a flexible tradeoff between disparate mistreatment - based fairness and accuracy .",
    "however , we would like to point out that the current formulation of fairness constraints may suffer from the following two main limitations .",
    "addressing these two limitations would be an interesting avenue for future work .",
    "first , the proposed formulation to train fair classifiers is not a convex program , but a disciplined convex - concave program ( dccp ) . as a consequence",
    ", our convex - concave formulation can be efficiently solved using recently proposed heuristics  @xcite , which work well in practice . however , unlike convex optimization , these heuristic - based methods do not provide any guarantees about the global optimality of the solution . secondly ,",
    "since computing the analytical covariance in fairness constraints is not a trivial task , we approximate the covariance using monte - carlo approximation on training set ( eq .  [ eq : fairness - proxy ] ) . while this approximation is expected to work well when a reasonable amount of training data is provided , it might be inaccurate for smaller datasets .",
    "this problem is exacerbated , in comparison to  @xcite , by the fact that we are not using the whole training set to approximate the covariance but only the misclassified points .",
    "therefore , for small error rates , our scheme may not be able to provide good estimates of covariance and lead to sub - optimal solutions .",
    "j.  angwin , j.  larson , s.  mattu , and l.  kirchner .",
    "machine bias : there s software used across the country to predict future criminals . and",
    "it s biased against blacks .",
    "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing , 2016 ."
  ],
  "abstract_text": [
    "<S> as the use of automated decision making systems becomes wide - spread , there is a growing concern about their potential unfairness towards people with certain traits . </S>",
    "<S> anti - discrimination laws in various countries prohibit unfair treatment of individuals based on specific traits , also called sensitive attributes ( , gender , race ) . </S>",
    "<S> in many learning scenarios , the trained algorithms ( classifiers ) make decisions with certain inaccuracy ( misclassification rate ) . as learning mechanisms target minimizing the error rate for _ all _ decisions </S>",
    "<S> , it is quite possible that the optimally trained algorithm makes decisions for users belonging to different sensitive attribute groups with different error rates ( e.g. , decision errors for females are higher than for males ) . to account for and avoid such unfairness when learning , in this paper </S>",
    "<S> , we introduce a new notion of unfairness , _ disparate mistreatment _ , which is defined in terms of misclassification rates . </S>",
    "<S> we then propose an intuitive measure of disparate mistreatment for decision boundary - based classifiers , which can be easily incorporated into their formulation as a convex - concave constraint . </S>",
    "<S> experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment , often at a small cost in terms of accuracy . </S>"
  ]
}