{
  "article_text": [
    "convolutional neural networks ( cnns ) have proven extremely succesful in finding structure in high - dimensional data , including time - sequences such as audio and video @xcite .",
    "some examples of promising real - world applications are speech- and video recognition , and automatic translation @xcite .",
    "challenges for software development include training of the networks using large gpu clusters and gathering huge , labeled datasets @xcite .",
    "practical user - side evaluation faces completely different challenges , including efficient and fast performance , low resource consumption , and responsiveness , such that the software responds to recognized events as quickly as possible @xcite .",
    "earlier work focusing on achieving these challenges include using less - parameter convolution filters @xcite , pruning obsolete weights @xcite , and using spiking networks @xcite .",
    "this paper deals with optimizing convolution of time - series , as used for example in 3d convolutional neural networks as they are applied in human action recognition @xcite .",
    "we observe that when forward propagating continuously updating time - sequences through a neural network that applies convolution in the time - dimension , many redundant calculations are made . in order to avoid these calculations , to save cpu resources and potentially battery life on mobile devices , we propose _ deep shifting _ , which copies results of convolution operations from earlier time steps , rather than re - calculating these over and over .",
    "this can save substantial calculation time , especially when the cnn looks at a large number of time - frames .",
    "this paper is organized as follows : section 2 shows how deep shifting performs cnn operations on time - sequences without performing redundant calculations .",
    "sections 3 and 4 examine the theoretical and practical benefits of deep shifting .",
    "section 5 investigates the possibilities of training a network using a minimal number of neurons and operations , and the paper finishes with a discussion and conclusion .",
    "let @xmath0 be an input with a matrix shape , having a time - axis and a context axis .",
    "the context axis holds what is called `` channels '' in image convolutions .",
    "a convolutional auto - encoder , for which we label the layers @xmath0 , @xmath1 and @xmath2 respectively , applies the following encoding operation with convolution over the time axis : @xmath3 here , @xmath4 is the size of the convolution window , @xmath5 is a set of @xmath4 weight matrices , @xmath6 denotes the time step of the input sequence , and @xmath7 labels the time axis of the weights .",
    "@xmath8 denotes an activation function , which we choose to be @xmath9 in our computational part .",
    "a schematic view is given in figure [ fig : conv_small ] .",
    "if the input consists of time - evolving data , we typically want to perform the encoding every time new information becomes available , e.g. at every new time step . however , due to the nature of convolution , many redundant calculations are made .",
    "let the actual time at which we perform an encoding be denoted by @xmath10 , and denote the neuronal activation at a specific time step by @xmath11 and @xmath12 .",
    "moreover , let the indices @xmath6 be defined relative to @xmath10 , such that the ` leftmost ' input neuron is @xmath13 .",
    "if at time @xmath14 , the network in figure [ fig : conv_normal ] received input time steps 0 through 5 initially , it has calculated @xmath15 up to @xmath16 through formula [ time_encoding ] . when we give the same network time steps 1 through 6 at a later time @xmath17 , it will calculate @xmath18 through @xmath19 . by formula",
    "[ time_encoding ] , @xmath20 , and the same goes for the other time steps @xmath21 and @xmath22 . by the natural flow of time",
    ", the activations in the network will have _ shifted _ through the network .",
    "therefore , we could have just as well stored these values , and copied them to the neighbouring neurons , without performing the the convolution calculation all over again .",
    "let @xmath23 denote the highest time step a hidden neuron can have at the time of encoding , e.g. @xmath24 .",
    "using the deep shifting method , the neural activations are calculated by : @xmath25 the same reasoning holds for an arbitrary number of layers , can be used to save calculations in down - sampling , and is easily extended to convolution in multiple dimensions , such as 3d convolutions that are often used on video data @xcite .",
    "moreover , it can avoid obsolete reconstruction operations : layer @xmath2 in figure [ fig : conv_normal ] will have @xmath26 fixed after @xmath27 , such that this value could also in principle be stored and shifted ( assuming the weights will not have changed significantly in the meantime ) .",
    "a graphical comparison is given in figure [ fig : comparison ] .",
    "how much exactly did we prune from the original network ?",
    "let one `` convolution operation '' be the calculation of one time - frame in the hidden layer , e.g. one full vector @xmath28 for some @xmath6 .",
    "if the layers @xmath0 , @xmath1 , @xmath2 would normally span @xmath29 , @xmath23 and @xmath30 time steps , we do nt have to store @xmath31 , @xmath32 and @xmath33 neurons in the three layers , and we saved ourselves calculating @xmath34 hidden convolution operations and @xmath33 output convolutions .",
    "recall that for most practical applications , convolutional auto - encoders are stacked in an hourglass shape , where the shallow layers span many time steps , whilst the deeper layers have increasingly smaller time axes .",
    "assume we start with the deepest hidden layer of a cae , with a time - axis of size @xmath35 , on which we will stack some number @xmath36 of shallower layers .",
    "any previous layers should then have a time - axis of size @xmath37 if a convolution window of size @xmath4 is used .",
    "the number of convolution operations @xmath38 performed in a forward pass through this network can be calculated to be : @xmath39 in the last line , we used @xmath40 , where in our case the sum runs up to @xmath41 because the shallowest layer s neurons are not calculated through convolution . clearly , due to the increasing size of the time - axes , the number of computations in a regular time - encoding cnn scales _ quadratically _ with the number of layers , assuming fixed @xmath35 .",
    "this case assumes the same @xmath4 in every layer , but this quadratic scaling holds even with @xmath4 varying per layer , as long as the windows have sizes of at least @xmath42 . on the other hand , a deep shifting architecture scales linearly with the number of layers : @xmath43    the situation is slightly different if we consider the case where the time - axis of the input @xmath29 is fixed , and we add increasing numbers of _ deeper layers _ , which have smaller time axes . in this case",
    ", the total number of convolutions upon encoding equals @xmath44 under these assumptions , deep shifting uses a number of convolution operations still equaling the number of layers . in this situation , we obtain a linear gain in performance : the average number of convolutions per layer is @xmath45 , compared to just @xmath46 for deep shifting .",
    "therefore , deep shifting requires @xmath45 times less convolution operations in the whole encoding process .",
    "we compare our own matlab implementation of a deep shifting network with the cnn from the ` deep learn toolbox ' by rasmus bergpalm @xcite . as the original version applies convolution in 2 dimensions for images specifically , we modified the code to apply convolution in only 1 dimension , representing time .",
    "we consider two networks : the first using the conventional cnn architecture ( formula [ time_encoding ] ) , the second exploiting deep shifting ( formula [ shift_encode ] and [ shift_shift ] ) .",
    "both networks receive a dataset representing part of a long time - sequence , and each subsequent input is the previous sequence shifted by one ` neuron ' in the time dimension .",
    "the average time required for forward propagation using various context axis sizes and convolution windows are displayed in figure [ fig : results ] .        as expected , the computation time on the regular cnn increases with increasing time - window input @xmath29 , whereas it remains constant using deep shifting .",
    "the shifting operation as implemented however requires a higher base computation time , and whether deep shifting does save computation time is then dependent on the parameters of the cnn . for realistic situations , such as a context size of 40 such as used in large - scale speech tasks @xcite , deep shifting becomes beneficial when more than roughly 20 time - frames are given as input to the cnn .",
    "moreover , the advantage of deep shifting becomes more noticeable when the size of the context axis increases .",
    "can we also use the sparse connectivity structure sufficient for deep shifting in the context of learning ?",
    "this section compares the _ training _ behaviour of deep shifting and a regular convolutional network .",
    "we train the networks as a convolutional auto - encoder ( cae ) . given an input @xmath0 , the hidden layer @xmath28 is calculated using formula [ time_encoding ] ( normal cae ) or formulas [ shift_encode ] and [ shift_shift ] ( deep shifting ) , using ` valid convolution ' ( the number of time steps in the hidden layer is @xmath47 ) .",
    "next , the network performs a reconstruction step , applying the adjoint of the earlier convolution to find the reconstruction @xmath48 .",
    "this involves ` full convolution ' , where the number of output time steps equals @xmath49 .",
    "the network weight and bias parameters are set to minimize the reconstruction error @xmath50 by using gradient descent for 100 epochs at a learning rate of @xmath51 .",
    "our test involves 2 datesets .",
    "the first is the spoken digit dataset , as used in ref .",
    "@xcite , consisting of the number ` zero ' to ` nine ' pronounced 10 times by 5 different speakers .",
    "the second dataset is the auslan dataset , a time - series representing hand - movements of the australian deaf - community sign language , obtained from ref .",
    "we test whether our networks can classify the right digit or sign , respectively .",
    "first , an mlp with 30 hidden units is trained to properly classify these on the _ training set _ , after which the percentage of correct classifications on a separate _ test dataset _ is measured .",
    "figure [ fig : class_vowel ] shows the results on the spoken digit dataset .",
    "the network denoted by ` shiftnet ' , which employed deep shifting , shows very competitive results compared to a regular cnn .",
    "note that both networks manage to achieve a very low percentage of erroneous classifications , indicating that the networks encode all important information in the dataset into a small number of hidden units .",
    "similarly to the approach of ref .",
    "@xcite , we used a random 60% of the 500 data points for training , and the others for testing .",
    "= 0     figure [ fig : class_auslan ] shows the results on the auslan dataset .",
    "the parameter ` len ' indicates the number of time - frames to which the input was scaled , and ` nh ' indicates the number of hidden units . on this much harder dataset ,",
    "a regular cnn is able to perform much better than the shiftnet , although the latter is still able to achieve very reasonable results despite the very limited connectivity . for this test ,",
    "10-fold cross validation is used , where 10% of the data is taken as test data , and another 10% used as validation data to limit overfitting by the mlp .    in general",
    ", we observe that deep shifting is able to achieve results comparable to normal cnns when the tests are ` easy ' , for example , when the classification errors are low , or when many hidden units are used .",
    "when classification errors get large , or the number of hidden units is small compared to the data size , deep shifting generally does not reach the performance of a regular cnn .",
    "we described the method of deep shifting , which can speed up the forward propagation of continuously updating inputs in convolutional auto - encoders . using both a theoretical and empirical analysis",
    ", we showed that deep shifting requires less convolution operations and computation time than a regular convolutional network when the number of input time - frames exceeds some threshold . for common practical applications",
    ", we found this threshold to lie around roughly 16 time - frames for 40 to 80 dimensional inputs with windows of size 10 .",
    "deep shifting is only relevant when time - sequences need to be continuously evaluated , and when the number of time - frames to be considered is larger than the size of the convolution window .",
    "the latter will always be the case if multiple layers are used .",
    "our analysis did not consider the use of graphical processing units : we are not sure how the speed of the copy operations of deep shifting compare against the large - scale parallel operations that gpu s are capable of , although we wonder if these can be used in practical user - end implementations .",
    "we also considered training a network with the deep shifting architecture .",
    "our tests show that a regular neural network layout is preferred on hard training tasks .",
    "in general , training a deep shifting architecture has very few applications , since networks are often trained on readily stored datasets , limiting the need for on - the - fly evaluation .",
    "still , deep shifting is an easily implementable trick to optimize deep neural networks working with time - evolving data , when speed or power consumption is relevant .",
    "this may be of benefit for mobile devices that aim to interpret sound , video , or other sensor data ."
  ],
  "abstract_text": [
    "<S> when a convolutional neural network is used for on - the - fly evaluation of continuously updating time - sequences , many redundant convolution operations are performed . </S>",
    "<S> we propose the method of deep shifting , which remembers previously calculated results of convolution operations in order to minimize the number of calculations . </S>",
    "<S> the reduction in complexity is at least a constant and in the best case quadratic . </S>",
    "<S> we demonstrate that this method does indeed save significant computation time in a practical implementation , especially when the networks receives a large number of time - frames . </S>"
  ]
}