{
  "article_text": [
    "linear mixed models and the model - based estimators including empirical bayes ( eb ) estimator or empirical best linear unbiased predictor ( eblup ) have been studied quite extensively in the literature from both theoretical and applied points of view . of these ,",
    "the small area estimation ( sae ) is an important application , and methods for sae have received much attention in recent years due to growing demand for reliable small area estimates . for a good review and account on this topic , see ghosh and rao ( 1994 ) , rao ( 2003 ) , datta and ghosh ( 2012 ) and pfeffermann ( 2014 ) .",
    "the linear mixed models used for sae are the fay - herriot model suggested by fay and herriot ( 1979 ) for area - level data and the nested error regression ( ner ) models given in battese , harter and fuller ( 1988 ) for unit - level data .",
    "especially , the ner model has been used in application of not only sae but also biological experiments and econometric analysis . besides the noise , a source of variation",
    "is added to explain the correlation among observations within clusters , or subjects , and to allow the analysis to ` borrow strength from other clusters .",
    "the resulting estimators , such as eb or eblup , for small - cluster means or subject - specific values provide reliable estimates with higher precisions than direct estimates like sample means .    in the ner model with @xmath0 small - clusters ,",
    "let @xmath1 be @xmath2 individual observations from the @xmath3-th cluster for @xmath4 , where @xmath5 is a @xmath6-dimensional known vector of covariates .",
    "the normal ner model is written as @xmath7 where @xmath8 and @xmath9 denote the random effect and samping error , respectively , and they are mutually independently distributed as @xmath10 and @xmath11 .",
    "the mean of @xmath12 is @xmath13 for regression coefficients @xmath14 , and the variance of @xmath12 is decomposed as @xmath15 = \\tau^2 + \\si^2 . \\label{hm0}\\ ] ] which is the same for all the clusters .",
    "however , jiang and nguyen ( 2012 ) illustrated that the within - cluster sample variances change dramatically from cluster to cluster for the data given in battese , et al .",
    "also , the normality assumptions for random effects and error terms are not always appropriate in practice .",
    "thus , we want to address the issue of relieving these assumptions of normal ner models in the two directions : heterogeneity of variances and non - normality of underlying distributions .    in real application",
    ", we often encounter the situation where the sampling variance @xmath16 is affected by the covariate @xmath5 . in such case ,",
    "the variance function is a useful tool for describing its relationship .",
    "variance function estimation has been studied in the literature in the framework of heteroscedastic nonparametric regression .",
    "for example , see cook and weisberg ( 1983 ) , hall and carroll ( 1989 ) , muller and stadtmuller ( 1987 , 1993 ) and ruppert , wand , holst and hossjer ( 1997 ) .",
    "thus , in this paper , we propose use of the technique to introduce the heteroscedastic variances into ner model without assuming normality of underlying distributions .",
    "the variance structure we consider is @xmath17 namely , the setup means that the sampling error @xmath9 has heteroscedastic variance @xmath18 .",
    "then we suggest the variance function model given by @xmath19 , where the details are explained in section [ sec : model ] .",
    "related to this paper , jiang and nguyen ( 2012 ) proposed the heteroscedastic nested error regression model with the setup that variance @xmath20 is proportional to @xmath21 , namely @xmath22 this is equivalent to the assumption that @xmath23 and @xmath24 . for setup ( [ hm2 ] ) ,",
    "jiang and nguyen ( 2012 ) assumed normality for @xmath8 and @xmath9 and demonstrated the quite interesting result that the maximum likelihood ( ml ) estimators of @xmath14 and @xmath25 are consistent for large @xmath0 , which implies that the resulting empirical bayes estimator estimates the bayes estimator consistently . in setup ( [ hm2 ] ) , however , there is no consistent estimator for the heteroscedastic variance @xmath21 , and the mean squared error ( mse ) of the eb can not be estimated consistently , since it depends on @xmath21 . to fix the inconsistent estimation of @xmath21 , maiti , ren and sinha ( 2014 ) suggested the hierarchical model such that @xmath21 s are random variables and @xmath26 has a gamma distribution .",
    "maiti , et al .",
    "( 2014 ) applied this setup to the fay - herriot model with statistics for estimating @xmath21 .",
    "however , the resulting eb estimator and the mse can not be expressed in closed forms .",
    "the same setup of @xmath21 was used recently by kubokawa , sugasawa , ghosh and choudhuri ( 2014 ) who derived explicit expressions of the eb estimator and the mse to second - order . in their simulation study , however , the finite sample properties of estimators of two hyper - parameters in the gamma prior distribution of @xmath21 are not so well . although the hierarchical models used in maiti , et al . ( 2014 ) and kubokawa , et al .",
    "( 2014 ) provide consistent estimators for model parameters and predictors , both models assume parametric hierarchical structures based on normal distributions of @xmath8 and @xmath9 . however , the normality assumption is not always appropriate and another heteroscedastic models are useful for such a situation when the normality assumption does not seem to be correct .",
    "in contrast to the existing results , the proposed model with variance function does not assume normality for either @xmath8 nor @xmath9 .",
    "the advantage of this paper is that the mse of the eb or eblup and its unbiased estimator are derived analytically in closed forms up to second - order without assuming normality for @xmath8 and @xmath9 .",
    "nonparametric approach to sae has been studied by jiang , lahiri and wan ( 2002 ) , hall and maiti ( 2006 ) , lohr and rao ( 2009 ) and others .",
    "most estimators of the mse have been given by numerical methods such as jackknife and bootstrap methods except for lahiri and rao ( 1995 ) , who provided an analytical second - order unbiased estimator of the mse in the fay - heriot model .",
    "hall and maiti ( 2006 ) developed a moment matching bootstrap method for nonparametric estimation of mse in nested error regression models .",
    "the suggested method is actually convenient but it requires bootstrap replication and has computational burden . in this paper , without assuming the normality , we derive not only second - order biases and variances of estimators for the model parameters , but also a closed expression for a second - order unbiased estimator of the mse in a closed form .",
    "thus our mse estimator does not require any resampling method and is useful in practical use .",
    "also our mse estimator can be regarded as a generalization of the robust mse estimator given in lahiri and rao ( 1995 ) .",
    "the paper is organized as follows : a setup of the proposed hner model and estimation strategy with asymptotic properties are given in section [ sec : model ] . in section [ sec : mse ] , we obtain the eblup and the second - order approximation of the mse .",
    "further , we provide the second - order unbiased estimators of mse by the analytical calculation . in section [ sec :",
    "sim ] , we investigate the performance of the proposed procedures through simulation and empirical studies . the technical proofs are given in the appendix .",
    "suppose that there are @xmath0 small clusters , and let @xmath27 be the pairs of @xmath2 observations from the @xmath3-th cluster , where @xmath5 is a @xmath6-dimensional known vector of covariates .",
    "we consider the heteroscedastic nested error regression model @xmath28 where @xmath14 is a @xmath6-dimenstional unknown vector of regression coefficients , and @xmath8 and @xmath9 are mutually independent random variables with mean zero and variances @xmath29 and @xmath18 , which are denoted by @xmath30 it is noted that no specific distributions are assumed for @xmath8 and @xmath9 .",
    "it is assumed that the heteroscedastic variance @xmath31 of @xmath9 is given by @xmath32 where @xmath33 is a @xmath34-dimensional known vector given for each cluster , and @xmath35 is a @xmath34-dimensional unknown vector .",
    "the variance function @xmath36 is a known ( user specified ) function whose range is nonnegative .",
    "some examples of the variance function are given below .",
    "the model parameters are @xmath14 , @xmath37 and @xmath35 , whereas the total number of the model parameters is @xmath38 .",
    "let @xmath39 , @xmath40 and @xmath41 . then the model ( [ model ] ) is expressed in a vector form as @xmath42 where @xmath43 is an @xmath44 vector with all elements equal to one , and the covariance matrix of @xmath45 is @xmath46 for @xmath47 and @xmath48 . it is noted that the inverse of @xmath49 is expressed as @xmath50 where @xmath51 .",
    "further , let @xmath52 , @xmath53 , @xmath54 and @xmath55 .",
    "then , the matricial form of ( [ model ] ) is written as @xmath56 , where @xmath57 .",
    "now we give some examples of the variance function @xmath58 in ( [ vf ] ) .",
    "\\(a ) in the case that the dispersion of the sampling error is proportional to the mean , it is reasonable to put @xmath59 and @xmath60 for the sub - vector @xmath61 of the covariate @xmath5 . for identifiability of @xmath35 , we restrict @xmath62 .",
    "\\(b ) consider the case that @xmath0 clusters are decomposed into @xmath34 homogeneous groups @xmath63 with @xmath64 .",
    "then , we put @xmath65 which implies that @xmath66 note that @xmath67 for @xmath68 .",
    "thus , the models assumes that the @xmath0 clusters are divided into known @xmath34 groups with their variance are equal over the same groups .",
    "jiang and nguyen ( 2012 ) used a similar setting and argued that the unbiased estimator of the heteroscedastic variance is consistent when @xmath69 as @xmath70 , where @xmath71 denotes the number of elements in @xmath72 .",
    "\\(c ) log linear functions of variance were treated in cook and weisberg ( 1983 ) and others .",
    "that is , @xmath73 is a linear function , and @xmath31 is written as @xmath74 . similarly to ( a ) , we put @xmath75 .    for the above two cases ( a ) and ( b ) , we have @xmath76 , while the case ( c ) corresponds to @xmath77 . in simulation and empirical studies in section",
    "[ sec : sim ] , we use the log - linear variance model .",
    "as given in subsequent section , we show consistency and asymptotic expression of estimators for @xmath35 as well as @xmath14 and @xmath37 .",
    "we here provide estimators of the model parameters @xmath14 , @xmath37 and @xmath35 . when values of @xmath35 and @xmath37 are given , the vector @xmath14 of regression coefficients is estimated by the generalized least squares ( gls ) estimator @xmath78 this is not a feasible form since @xmath35 and @xmath37 are unknown .",
    "when estimators @xmath79 and @xmath80 are for @xmath37 and @xmath35 , we get the feasible estimator @xmath81 by replacing @xmath37 and @xmath35 in @xmath82 with their estimators .    concerning estimation of @xmath37 , we use the second moment of observations @xmath12 s . from model ( [ model ] ) , it is seen that @xmath83=\\tau^2+\\si^2(\\z_{ij}'\\bga).\\ ] ] based on the ordinary least squares ( ols ) estimator @xmath84 , a moment estimator of @xmath37 is given by @xmath85 with substituting estimator @xmath80 into @xmath35 , where @xmath86 .    for estimation of @xmath35 ,",
    "we consider the within difference in each cluster .",
    "let @xmath87 be the sample mean in the @xmath3-th cluster , namely @xmath88 .",
    "it is noted that for @xmath89 , @xmath90 which dose not include the term of @xmath8 .",
    "then it is seen that @xmath91=\\left(1 - 2n_i^{-1}\\right)\\si^2(\\z_{ij}'\\bga)+n_i^{-2}\\sum_{h=1}^{n_i}\\si^2(\\z_{ih}'\\bga),\\ ] ] which motivates us to estimate @xmath35 by solving the following estimating equation given by @xmath92\\z_{ij}=\\0,\\ ] ] which is equivalent to @xmath93=\\0\\ ] ] where @xmath94 . it is noted that , in case of homoscedastic case , namely @xmath95 , the estimator @xmath96 and @xmath37 reduces to the estimator identical to prasad - rao estimator ( prasad and rao , 1990 ) up to the constant factor .",
    "note that the objective function ( [ bga ] ) for estimation of @xmath35 does not depend on @xmath14 and @xmath37 and that the estimator of @xmath37 depends on @xmath35 .",
    "these suggest the following algorithm for calculating the estimates of the model parameters : we first obtain the estimate @xmath80 of @xmath35 by solving ( [ bga ] ) , and then we get the estimate @xmath79 from ( [ tau ] ) with @xmath97 .",
    "finally we have the gls estimate @xmath98 with substituting @xmath80 and @xmath79 in ( [ bbe ] ) .",
    "in this section , we provide large sample properties of the estimators given in the previous subsection when the number of clusters @xmath0 goes to infinity , but @xmath2 s are still bounded . to establish asymptotic results",
    ", we assume the following conditions under @xmath70 .",
    "* assumption ( a ) *    * there exist @xmath99 and @xmath100 such that @xmath101 for @xmath102 .",
    "the dimensions @xmath6 and @xmath34 are bounded , namely @xmath103 .",
    "the number of clusters with one observaion , namely @xmath104 , is bounded . *",
    "the variance function @xmath36 is twice differentiable and its derivatives are denoted by @xmath105 and @xmath106 , respectively .",
    "* the following matrices converge to non - singular matrices : @xmath107 for @xmath108 and @xmath109 . *",
    "the forth moments of @xmath8 and @xmath9 exist , namely @xmath110<\\infty$ ] and @xmath111<\\infty$ ] .    the conditions 1 and 3 are the standard assumptions in small area estimation .",
    "the condition 2 is also non - restrictive , and the simple variance function @xmath76 and @xmath112 obviously satisfies the assumption .",
    "the moment condition 4 is necessary for existence of mse of the eblup , and it is satisfied by many continuous distributions , including normal , shifted gamma , laplace and @xmath113-distribution with degrees of freedom larger than 5 .",
    "in what follows , we use the notations @xmath114 for simplicity . to derive asymptotic approximations of the estimators , we define the following statistics in the @xmath3-th cluster : @xmath115.\\label{u2}\\end{aligned}\\ ] ] moreover , we define @xmath116 noting that @xmath117 and @xmath118 under assumption ( a )",
    ". then we obtain the asymptotically linear expression of the estimators .",
    "[ thm : asymp ] let @xmath119 be the estimator of @xmath120 . under assumption ( a )",
    ", it follows that @xmath121 with the asymptotically linear expression @xmath122 where @xmath123    from theorem [ thm : asymp ] , it follows that @xmath124 have an asymptotically normal distribution with mean vector @xmath125 and covariance matrix @xmath126 , where @xmath127 is a @xmath128 matrix partitioned as @xmath129 & e[\\psi_i^{\\bbe}\\psi_i^{\\bga ' } ] &   e[\\psi_i^{\\bbe}\\psi_i^{\\tau}]\\\\   e[\\psi_i^{\\bga}\\psi_i^{\\bbe ' } ] &   e[\\psi_i^{\\bga}\\psi_i^{\\bga ' } ] &   e[\\psi_i^{\\bga}\\psi_i^{\\tau}]\\\\   e[\\psi_i^{\\tau}\\psi_i^{\\bbe ' } ] &   e[\\psi_i^{\\tau}\\psi_i^{\\bga ' } ] &   e[\\psi_i^{\\tau}\\psi_i^{\\tau } ] \\end{array}\\right).\\end{aligned}\\ ] ] it is noticed that @xmath130=0 $ ] and @xmath131=0 $ ] when @xmath12 are normally distributed . in such a case , it follows @xmath132 and @xmath133 , namely @xmath14 and @xmath134 are asymptotically orthogonal .",
    "however , since we do not assume that normality for observations @xmath135 , @xmath14 and @xmath136 are not necessarily orthogonal .",
    "the asymptotic covariance matrix @xmath137 or @xmath127 can be easily estimated from samples .",
    "for example , @xmath138 $ ] can be estimated by @xmath139 where @xmath140 is obtained by replacing unknown parameters @xmath141 in @xmath142 with estimates @xmath143 .",
    "it is noted that the accuracy of estimation is given by @xmath144 from theorem [ thm : asymp ] and @xmath145 .",
    "the estimator @xmath146 will be used to get the estimators of mean squared errors of predictors in section [ sec : mse ] .",
    "we next provide the asymptotic properties of conditional covariance matrix given in the following corollary where the proof is given in the appendix .",
    "[ cor : cond ] under assumption ( a ) , for @xmath102 , it follows that @xmath147    this property is used for estimation and evaluating the mean squared errors of eblup discussed in the subsequent section . moreover , in the evaluation of the mean squared errors of eblup and derivation of its estimators , we need to obtain the conditional and unconditional asymptotic bias of estimators @xmath143 .",
    "let @xmath148 and @xmath149 be the second - order conditional asymptotic bias defined as @xmath150=&\\b_{\\bbe}^{(i)}(\\y_i)+o_p(m^{-1 } ) , \\ \\",
    "e[\\bgah-\\bga|\\y_i]=\\b_{\\bga}^{(i)}(\\y_i)+o_p(m^{-1 } ) , \\\\ & e[\\tah^2-\\tau^2|\\y_i]=\\b_{\\tau}^{(i)}(\\y_i)+o_p(m^{-1}).\\end{aligned}\\ ] ] in the following theorem , we provide the analytical expressions of @xmath148 and @xmath149 .",
    "define @xmath151 , @xmath152 and @xmath153 by @xmath154 @xmath155\\right)\\right\\}_r\\\\ & \\ \\ \\ \\ \\ \\ \\",
    "-\\sum_{k=1}^m\\sum_{j=1}^{n_k}\\z_{kj}\\si_{kj(2)}(\\z_{kj}-2n_k^{-1}\\z_{kj}+n_k^{-1}\\barz_k)'\\bom_{\\bga\\bga}\\z_{kj}\\bigg ] , \\end{split}\\ ] ] and @xmath156 where @xmath157 , @xmath158 , @xmath159 for @xmath160-th element @xmath161 of @xmath162 , @xmath163 for @xmath164 and @xmath165 are defined in the proof of theorem [ thm : cond.bias ] , and @xmath166 denotes a @xmath34-dimensional vector @xmath167 .",
    "it is noted that @xmath168 are of order @xmath169 .",
    "now we provide the second - order approximation to the conditional asymptotic bias .",
    "[ thm : cond.bias ] under assumption ( a ) , we have @xmath170 where @xmath171 , @xmath172 and @xmath173 are of order @xmath174 , and @xmath175 and @xmath176 are given in @xmath177 and @xmath178 , respectively .    from the above theorem",
    ", we immediately obtain the unconditional asymptotic bias of the estimators @xmath143 by taking expectation with respect to @xmath179 given in the following corollary .",
    "[ cor : bias ] under assumption ( a ) , it follows that @xmath180=(\\b_{\\bbe}',\\b_{\\bga}',b_{\\tau})'+o(m^{-1}),\\ ] ] where @xmath151 , @xmath152 and @xmath153 are given in @xmath181 .",
    "we now consider the prediction of @xmath182 where @xmath183 is a known ( user specified ) vector and @xmath8 is the random effect in model ( [ model ] ) .",
    "the typical choice of @xmath183 is @xmath184 which corresponds to the prediction of mean of the @xmath3-th cluster .",
    "a predictor @xmath185 of @xmath186 is evaluated in terms of the mse @xmath187 $ ] . in the general forms of @xmath185 , the minimizer ( best predictor ) of the mse",
    "can not be obtain without a distributional assumption for @xmath8 and @xmath9 .",
    "thus we focus on the class of linear and unbiased predictors , and the best linear unbiased predictor ( blup ) of @xmath186 in terms of the mse is given by @xmath188 this can be simplified to @xmath189 where @xmath190 for @xmath191 . in case of homogeneous variances , namely @xmath192",
    ", it is confirmed that the blp reduces to @xmath193 with @xmath194 as given in hall and maiti ( 2006 ) .",
    "the blup is not feasible since it depends on unknown parameters @xmath14 , @xmath35 and @xmath37 .",
    "plugging the estimators into @xmath195 , we get the empirical best linear unbiased predictor ( eblup ) @xmath196 for @xmath197 . in the subsequent section ,",
    "we consider the mean squared errors ( mse ) of eblup ( [ eblup ] ) without any distributional assumptions for @xmath8 and @xmath9 .",
    "to evaluate uncertainty of eblup given by ( [ eblup ] ) , we evaluate the mse defined as @xmath198 $ ] for @xmath134 .",
    "the mse is decomposed as @xmath199\\\\ & = e\\left[(\\mut_i-\\mu_i)^2\\right]+e\\left[(\\muh_i-\\mut_i)^2\\right]+2e\\left[(\\muh_i-\\mut_i)(\\mut_i-\\mu_i)\\right].\\end{aligned}\\ ] ] from the expression of @xmath195 , we have @xmath200 which leads to @xmath201=\\left(\\sum_{j=1}^{n_i}\\la_{ij}-1\\right)^2\\tau^2+\\sum_{j=1}^{n_i}\\la_{ij}^2\\si_{ij}^2=\\tau^2\\eta_i^{-1}.\\ ] ]    for the second term , however , we can not obtain an exact expression , so that we obtain the approximation up to @xmath169 . using the taylor series expansion and theorem [ thm : asymp ] , we have @xmath202&=e\\left[\\left\\{\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)'(\\bthh-\\bth)\\right\\}^2\\right]+o(m^{-1})\\\\ & = \\tr\\left\\{e\\left[\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)'e\\left((\\bthh-\\bth)(\\bthh-\\bth)'\\big | \\y_i\\right)\\right]\\right\\}+o(m^{-1})\\\\ & = \\tr\\left\\{e\\left[\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)'\\right]\\bom\\right\\}+o(m^{-1})\\equiv r_{2i}(\\bphi)+o(m^{-1}),\\end{aligned}\\ ] ] where we used corollary [ cor : cond ] and the fact that @xmath203 does not depend on @xmath204 except for @xmath179 .",
    "the straightforward calculation shows that @xmath205 where @xmath206 so that we have @xmath207 which is of order @xmath169 .",
    "we next evaluate the cross term @xmath208 $ ] .",
    "this term vanishes under the normality assumptions for @xmath8 and @xmath9 , but in general , it can not be neglected . as in the case of @xmath209",
    ", we obtain an approximation of @xmath208 $ ] up to @xmath169 . in the evaluation",
    ", we assume that @xmath210 . to this end , we expand @xmath211 as @xmath212 it follows that @xmath213 and then , @xmath214&=e\\left[\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)'(\\bthh-\\bth)w_i\\right]+\\frac12e\\left[\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)'(\\bthh-\\bth)(\\bthh-\\bth)'\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)w_i\\right]+o(m^{-1}).\\end{aligned}\\ ] ] using the expression of ( [ mu.deriv ] ) and corollary [ cor : cond ] , the straightforward calculation ( whose details are given in the appendix ) shows that @xmath215=o(m^{-1}),\\ ] ] under the assumption @xmath210 .",
    "moreover , from theorem [ thm : cond.bias ] , we obtain @xmath216=r_{31i}(\\bphi,\\bka)+o(m^{-1}),\\ ] ] for @xmath217 where @xmath218 and @xmath219 , @xmath220 is defined as @xmath221 and @xmath222 , respectively , and @xmath223 .",
    "the derivation of the expression of @xmath224 is also given in the appendix . from the expression ( [ r31 ] ) , it holds that @xmath225 .    under the normality assumption of @xmath8 and @xmath9",
    ", we immediately obtain @xmath226 and @xmath227 since @xmath228 .",
    "this leads to @xmath229 , which means that the cross term does not appear in the second - order approximated mse , that is our result is consistent to the well - known result .",
    "now , we summarize the result for the second - order approximation of the mse .",
    "[ thm : mse ] under assumption ( a ) and @xmath230=e[\\ep_{ij}^3]=0 $ ] , the second - order approximation of the mse is given by @xmath231 where @xmath232 , @xmath233 and @xmath224 are given in @xmath234 , @xmath235 and @xmath236 , respectively , and @xmath237 , @xmath238 and @xmath225 .",
    "the approximated mse given in theorem [ thm : mse ] depends on unknown parameters .",
    "thus , in the subsequent section , we derive the second - order unbiased estimator of the mse by the analytical and the matching bootstrap methods .",
    "we first derive the analytical second - order unbiased estimator of the mse . from theorem [ thm :",
    "mse ] , @xmath233 is @xmath169 , so that it can be estimated by the plug - in estimator @xmath239 with second - order accuracy , namely @xmath240=r_{2i}(\\bphi)+o(m^{-1})$ ] .",
    "for @xmath224 with order @xmath169 , if a consistent estimator @xmath241 is available for @xmath242 , this term can be estimated by the plug - in estimator with second - order unbiasedness . to this end , we construct a consistent estimator of @xmath242 using the expression of fourth moment of observations .",
    "the straightforward calculation shows that @xmath243\\\\ & \\ \\ \\ = \\kappa_{\\ep}n_i^{-4}(n_i-1)(n_i-2)(n_i^2-n_i-1)\\left(\\sum_{j=1}^{n_i}\\siij^4\\right)+3n_i^{-3}(2n_i-3)\\left\\{\\left(\\sum_{j=1}^{n_i}\\siij^2\\right)^2-\\sum_{j=1}^{n_i}\\siij^4\\right\\},\\end{aligned}\\ ] ] whereby we can estimate @xmath244 by @xmath245 , \\end{split}\\ ] ] where @xmath246 and @xmath98 is feasible gls estimator of @xmath14 given in section 2 . for @xmath219",
    ", it is observed that @xmath247=\\tau^4\\ka_{v}+6\\tau^2\\siij^2+\\ka_{\\ep}\\siij^4,\\end{aligned}\\ ] ] which leads to the estimator of @xmath219 given by @xmath248 from theorem [ thm : asymp ] , it is immediately follows that the estimators given in ( [ ka.ep ] ) and ( [ ka.v ] ) are consistent . using these estimators",
    ", we can estimate @xmath249 by @xmath250 with second - order accuracy .    finally , we consider the second - order unbiased estimation of @xmath251 .",
    "the situation is different than before since @xmath252 , which means that the plug - in estimator @xmath253 has the second - order bias with @xmath169 .",
    "thus we need to obtain the second - order bias of @xmath253 and correct them . by the taylor series expansion , we have @xmath254 from theorem [ thm : asymp ] .",
    "then , the second - order bias of @xmath253 is expressed as @xmath255&-r_{1i}(\\bphi)\\\\ & = \\left(\\frac{\\partial r_{1i}(\\bphi)}{\\partial\\bphi'}\\right)e[\\bphih-\\bphi]+\\frac12\\tr\\left\\{\\left(\\frac{\\partial^2 r_{1i}(\\bphi)}{\\partial\\bphi\\partial\\bphi'}\\right)e\\left[(\\bphih-\\bphi)(\\bphih-\\bphi)'\\right]\\right\\}+o(m^{-1})\\\\ & = \\left(\\frac{\\partial r_{1i}(\\bphi)}{\\partial\\bphi'}\\right)\\b_{\\bphi}+\\frac12\\tr\\left\\{\\left(\\frac{\\partial^2 r_{1i}(\\bphi)}{\\partial\\bphi\\partial\\bphi'}\\right)\\bom_{\\bphi}\\right\\}+o(m^{-1}),\\end{aligned}\\ ] ] where @xmath256 is the sub - matrix of @xmath127 with respect to @xmath136 , and @xmath257 is the second - order bias of @xmath258 given in corollary [ cor : bias ] .",
    "the straightforward calculation shows that @xmath259 where @xmath260 therefore , we obtain the expression of the second - order bias given by @xmath261 with @xmath262 . noting that @xmath263 can be estimated by @xmath264 with @xmath265=b_i+o(m^{-1})$ ] from theorem [ thm : asymp ] , we propose the bias corrected estimator of @xmath251 given by @xmath266 which is second - order unbiased estimator of @xmath251 , namely @xmath267=r_{1i}(\\bphi)+o(m^{-1}).\\ ] ] now , we summarize the result for the second - order unbiased estimator of mse in the following theorem .",
    "[ thm : mseest ] under assumption ( a ) and @xmath230=e[\\ep_{ij}^3]=0 $ ] , the second - order unbiased estimator of @xmath268 is given by @xmath269 that is , @xmath270={\\rm mse}_i+o(m^{-1})$ ] .",
    "it is remarked that the proposed estimator of mse does not require any resampling methods such as bootstrap .",
    "this means that the analytical estimator can be easily implemented and has less computational burden compared to bootstrap .",
    "moreover , we do not assume normality of @xmath8 and @xmath9 in the derivation of the mse estimator as in lahiri and rao ( 1995 ) .",
    "thus the proposed mse estimator is expected to have a robustness property , which will be investigated in the simulation studies .",
    "we first compare the performances of eblup obtained from the proposed hner with variance functions ( hnervf ) with the conventional ner and the hner with random dispersions ( hnerrd ) proposed in kubokawa , et al .",
    "( 2014 ) in terms of simulated mse . to this end , we consider the following data generating process : @xmath271 we take @xmath272 , @xmath273 , @xmath274 , @xmath275 . for the values of @xmath276 and @xmath277 , we consider two patterns : @xmath278 .",
    "note that @xmath279 indicates that the true model holds the heteroscedasticity in sampling variances while @xmath280 indicates the true model has homoscedastic variance in which both hner models are overfitted .",
    "we generate @xmath281 and @xmath282 from the uniform distribution on @xmath283 and @xmath284 , respectively , which are fixed through the simulation runs .",
    "following hall and maiti ( 2006 ) , we consider five patterns of distributions of @xmath8 and @xmath9 , that is , m1 : @xmath8 and @xmath9 are both normally distributed , m2 : @xmath8 and @xmath9 are both scaled @xmath113-distribution with degrees of freedom @xmath285 , m3 : @xmath8 and @xmath9 are both scaled and located @xmath286 distribution , m4 : @xmath8 are @xmath9 are scaled and located @xmath286 and @xmath287 distribution , respectively , and m5 : @xmath8 are @xmath9 are both logistic distribution .",
    "based on @xmath288 simulation runs , we calculate the mse of each area defined as @xmath289 where @xmath290 and @xmath291 are obtained values of the eblup and the true values of @xmath292 in the @xmath160-th iteration , respectively . for estimation of the variance component in the ner model ,",
    "we use the prasad and rao estimator ( prasad and rao , 1990 ) .",
    "the resulting simulated mse values for five distribution and two values of @xmath277 are given in figure [ comp1 ] ( in case of @xmath279 ) and figure [ comp2 ] ( in case of @xmath280 ) .",
    "from figure 1 , it is observed that the hnervf provides least values of mse in all areas .",
    "it is a natural result that the hnerrd provides second best prediction in terms of mse values , but the mse values are not so different from the ner model .",
    "thus the model specification is appropriate , the eblup obtained from hnervf performs so well compared to the existing models . on the other hand , in figure [ comp2 ] , the hnervf provides little larger mse values than the hnerrd and ner in normal case ( m1 ) .",
    "it is not surprising result since the parameter @xmath277 in the hnervf is 0 in the true model and the estimation error of @xmath277 inflates the mse values .",
    "however , in other cases ( m2@xmath293m5 ) , the hnervf provides the close mse values to the ner and hnerrd although the true model has homoscedastic variances .",
    "thus we may conclude that the hnervf has little disadvantages of over - specification in terms of mse values of the eblup .",
    "( heteroscedasticity ) .",
    "[ comp1 ] , title=\"fig:\",width=207 ]   ( heteroscedasticity ) .",
    "[ comp1 ] , title=\"fig:\",width=207 ]   ( heteroscedasticity ) .",
    "[ comp1 ] , title=\"fig:\",width=207 ] +   ( heteroscedasticity ) .",
    "[ comp1 ] , title=\"fig:\",width=207 ]   ( heteroscedasticity ) .",
    "[ comp1 ] , title=\"fig:\",width=207 ]     ( homoscedasticity ) .",
    "[ comp2 ] , title=\"fig:\",width=207 ]   ( homoscedasticity ) .",
    "[ comp2 ] , title=\"fig:\",width=207 ]   ( homoscedasticity ) .",
    "[ comp2 ] , title=\"fig:\",width=207 ] +   ( homoscedasticity ) .",
    "[ comp2 ] , title=\"fig:\",width=207 ]   ( homoscedasticity ) .",
    "[ comp2 ] , title=\"fig:\",width=207 ]      we next investigate the finite sample performances of the mse estimators given in theorem [ thm : mseest ] .",
    "we use the same data generating process given in ( [ dgp ] ) and we take @xmath274 , @xmath275 , @xmath294 and @xmath279 .",
    "moreover , we equally divided @xmath272 areas into four groups ( @xmath295 ) , so that each group has five areas and the areas in the same group has the same sample size @xmath296 . following the simulation study in the previous subsection",
    ", we again consider the five patterns of distributions for @xmath8 and @xmath9 .",
    "the simulated values of the mse are obtained from ( [ sim - mse ] ) based on @xmath288 simulation runs .",
    "then , based on @xmath297 simulation runs , we calculate the relative bias ( rb ) and coefficient of variation ( cv ) of mse estimators given by @xmath298 where @xmath299 is the mse estimator in the @xmath160-th iteration . in table",
    "[ mse - sim ] , we report mean and median values of @xmath300 and @xmath301 in each group . for comparison , results for the naive mse estimator , without any bias correction , are reported in table [ mse - sim ] as well .",
    "the naive mse estimator is the plug - in estimator of the asymptotic mse ( [ r1 ] ) , namely it is obtained by replacing @xmath37 and @xmath35 in formula ( [ r1 ] ) by @xmath79 and @xmath80 , respectively . in table",
    "[ mse - sim ] , the relative bias is small , less than 10% in many cases .",
    "when the underlying distributions leave from normality , the mse estimator still provides small relative bias although it has higher coefficient of variation .",
    "the naive mse estimator is more biased than the analytical mse estimator in all groups and models , so that the bias correction in mse estimator is successful .",
    "we now investigate empirical performances of the suggested model , the empirical bayes estimator and the second - order unbiased estimator of mse through analysis of real data .",
    "the data used here originates from the posted land price data along the keikyu train line in 2001 .",
    "this train line connects the suburbs in the kanagawa prefecture to the tokyo metropolitan area .",
    "those who live in the suburbs in the kanagawa prefecture take this line to work or study in tokyo everyday .",
    "thus , it is expected that the land price depends on the distance from tokyo .",
    "the posted land price data are available for 52 stations on the keikyu train line , and we consider each station as a small area , namely , @xmath302 . for the @xmath3-th station ,",
    "data of @xmath2 land spots are available , where @xmath2 varies around 4 and some areas have only one observation .    for @xmath303",
    ", @xmath12 denotes the value of the posted land price ( yen/10,000 ) for the unit meter squares of the @xmath304-th spot , @xmath305 is the time to take from the nearby station @xmath3 to the tokyo station around 8:30 in the morning , @xmath306 is the value of geographical distance from the spot @xmath304 to the station @xmath3 and @xmath307 denotes the floor - area ratio , or ratio of building volume to lot area of the spot @xmath304 .",
    "this data set is treated in kubokawa , et al .",
    "( 2014 ) , where they pointed out that the heteroscedasticity seem to be appropriate from boxplots of some areas and bartlet test for testing homoscedastic variance .",
    "figure [ plp ] is the plot of the pairs @xmath308 , where @xmath309 is ols residuals given by @xmath310 .",
    "it indicates that the residuals are more variable for small @xmath306 than for large @xmath306 , namely the variances seem functions of @xmath306 .",
    "thus we apply the following hner model with a variance function given by @xmath311 where @xmath312 and @xmath313 . for the variance function @xmath314 , we use @xmath315 motivated from figure [ plp ] . as a submodel of ( [ plp - model ] )",
    ", we also consider the homoscedastic variance model with @xmath316 .",
    "then the estimated values of parameters in these two models are given in the following : @xmath317    the estimated values of @xmath318 and @xmath319 , coefficients of @xmath320 and @xmath306 , in both models are negative values which leads to the natural result that the @xmath320 and @xmath306 have negative influence on @xmath12 .",
    "the sign of @xmath321 is negative .",
    "this corresponds to the variability illustrated in figure [ plp ] .",
    "the obtained values of eblup given in ( [ eblup ] ) are given in table [ plp - res ] for selected 15 areas .",
    "to see the difference of predicted values in terms of the degree of shrinkage , we compute @xmath322 for each two model and the results are given in figure [ plp2 ] .",
    "it is observed that @xmath323 in ner model decreases as the area sample size @xmath2 gets large .",
    "this is because the sample mean provides better estimates of the true mean as @xmath2 gets larger , so that the sample mean does not need to be shrunk . on the other hand , @xmath323 in hnervf",
    "is influenced by the estimated heteroscedastic variance @xmath324 as well as @xmath2 .",
    "thus the plot in figure [ plp2 ] shows that the shrinkage degrees in hnervf has more variability than that in ner . in table [ plp - res ] and figure [ plp ]",
    ", we also provide the estimates of squared root of mse ( smse ) given in theorem [ thm : mseest ] .",
    "it is revealed from table [ plp - res ] that the estimates of the smse in ner get smaller as @xmath2 gets larger . on the other hands ,",
    "the smse in hnervf do not have a similar property , because the smse in hnervf is affected by not only @xmath2 but also the heteroscedastic variance as indicated in the mse formula given in theorem [ thm : mse ] . from figure [ plp ] , we observe that the estimated smses of hnervf are smaller than that of ner in many areas .",
    "especially , in area 47 , 49 , 50 and 51 , the smse values of hnervf are dramatically small compared to ner . in some other areas , the smess of hnervf is larger than that of ner , but the differences are not so large .",
    "these observations and the residual plot in figure [ plp ] motivate us to utilize the hnervf in case of heteroscedastic variance explained by some covariates .",
    "( left ) and estimated mse in hnervf and ner ( right ) . , title=\"fig:\",width=309 ]   ( left ) and estimated mse in hnervf and ner ( right ) . , title=\"fig:\",width=309 ]     against area sample size @xmath2 in hnervf and ner , title=\"fig:\",width=309 ]   against area sample size @xmath2 in hnervf and ner , title=\"fig:\",width=309 ]    .the estimated results of plp data for selected 15 areas [ cols=\"^,^,^,^,^,^,^,^,^,^ \" , ]",
    "in the context of small - area estimation , homogeneous nested error regression models have been studied so far in the literature . however , some real data sets show heteroscedasticity in variances as pointed out in jiang and nguyen ( 2012 ) and kubokawa , et al . ( 2014 ) . in such a case ,",
    "the residuals often indicate that the heteroscedasticity can be explained by some covariates , which motivated us to propose and investigate the heteroscedastic nested error regression model with variance functions ( hnervf ) .",
    "we have proposed the estimating method for the model parameters and the asymptotic properties of these estimators have been established without any distributional assumptions for error terms . for measuring uncertainty of the empirical bayes estimator , the mean squared errors ( mse ) have been approximated up to second - order , and their second - order unbiased estimators have been provided in the closed form .    for estimation of mse in hnervf without distributional assumptions , we can utilize the moment matching bootstrap method proposed in hall and maiti ( 2006 ) . to derive the bootstrap estimator",
    ", we use the following representation of mse : @xmath325 + 2e\\left[(\\muh_i-\\mut_i)(\\mut_i-\\mu_i)\\right],\\ ] ] noting that the second and third terms are @xmath169 .",
    "then we can establish the second - order unbiased mse estimator via three - point distribution or @xmath113-distribution for approximation of distribution of error terms .",
    "however , the bootstrap method has computational burden , so that we did not treat in this paper .",
    "* acknowledgments . *",
    "+ the first author was supported in part by grant - in - aid for scientific research ( 15j10076 ) from japan society for the promotion of science ( jsps ) .",
    "the second author was supported in part by grant - in - aid for scientific research ( 23243039 and 26330036 ) from japan society for the promotion of science .",
    "proof of theorem 1 .",
    "*   since @xmath326 are mutually independent , the consistency of @xmath79 and @xmath80 follows from the standard argument of m - estimators , so that @xmath98 is also consistent . in what follows , we derive the asymptotic expressions of the estimators .",
    "for the asymptotic expansion of @xmath80 defined as the minimizer of ( [ bga ] ) .",
    "remember that the estimator @xmath80 is given as the solution of the estimating equation @xmath332=\\0\\ ] ] using taylor expansions , we have @xmath333 where @xmath334.\\ ] ] from the central limit theorem , it follows that @xmath335 so that the second terms in the expansion formula is @xmath336 .",
    "then we get @xmath337 under assumption ( a ) , we have @xmath338 from the independence of @xmath204 and the fact @xmath339 , we can use the central limit theorem to show that the leading term in the expansion of @xmath340 is @xmath341 .",
    "thus , @xmath342      finally we consider the asymptotic expansion of @xmath344 . from the expression in ( [ bbe ] )",
    ", it follows that @xmath345 since @xmath346 for @xmath347 , we have @xmath348 where @xmath349 under assumption ( a ) , we have @xmath350 for @xmath164 , whereby @xmath351 . since @xmath343 and @xmath352 as shown above , we get @xmath353 which completes the proof .",
    "proof of corollary [ cor : cond ] .",
    "*   let @xmath354 .",
    "note that @xmath355 does not depend on @xmath356 and that @xmath204 are mutually independent .",
    "then , @xmath357=\\frac{1}{m^2}\\sum_{j=1,j\\neq i}^{m}e\\left[\\psi_j^{\\theta_k}\\psi_j^{\\theta_l}\\right]+\\frac{1}{m^2}\\psi_i^{\\theta_k}\\psi_i^{\\theta_l}\\\\ & = \\bom_{kl}+\\frac{1}{m^2}\\left\\{\\psi_i^{\\theta_k}\\psi_i^{\\theta_l}-e\\left[\\psi_i^{\\theta_k}\\psi_i^{\\theta_l}\\right]\\right\\}=\\bom_{kl}+o_p(m^{-1}),\\end{aligned}\\ ] ] where @xmath358 is the @xmath359-element of @xmath127 and we used the fact that @xmath360=e[\\psi_j^{\\theta_k}]=0 $ ] for @xmath361 .",
    "hence , we get the result from the asymptotic approximation of @xmath143 given in theorem [ thm : asymp ] .",
    "proof of theorem [ thm : cond.bias ] . *",
    "we begin by deriving the conditional asymptotic bias of @xmath80 .",
    "let @xmath362 be the solution of the equation @xmath363=\\0\\ ] ] with @xmath364 . for notational simplicity",
    ", we use @xmath365 instead of @xmath366 without any confusion and @xmath367 denotes the @xmath160-th component of @xmath365 , namely @xmath368 . define the derivatives @xmath369 and @xmath370 by @xmath371 it is noted that @xmath372 . expanding @xmath373 , we obtain @xmath374 where @xmath375 for @xmath376 it is also noted that @xmath377 so that @xmath378 is non - stochastic .",
    "thus we have @xmath379=-(\\f_{(\\bga)})^{-1}\\left\\{e[\\f(\\bga;\\bbe)|\\y_i]+e\\left[\\f_{(\\bbe)}(\\obbeh-\\bbe)\\big|\\y_i\\right]+\\frac12e[\\t_1|\\y_i]+\\frac12e[\\t_2|\\y_i]\\right\\}+o_p(m^{-1}).\\end{aligned}\\ ] ] in what follows , we shall evaluate the each term in the parenthesis in the above expression .",
    "for the first term , since @xmath204 are mutually independent and @xmath339 , we have @xmath380={1\\over m } \\u_{2i}.\\ ] ] for evaluation of the second term , we define @xmath159 , where @xmath161 denotes the @xmath160-th element of @xmath162 .",
    "then it follows that @xmath381=-\\frac2n\\sum_{k=1}^m e\\left[(\\y_k-\\x_k\\bbe)'\\e_k\\z_{kr}\\e_k\\x_k(\\obbeh-\\bbe)\\bigg|\\y_i\\right]\\\\ & = -\\frac{2}{n}\\sum_{k=1,k\\neq i}^me\\left[(\\y_k-\\x_k\\bbe)'\\e_k\\z_{kr}\\e_k\\x_k(\\obbeh-\\bbe)\\big|\\y_i\\right]-\\frac{2}{n}(\\y_i-\\x_i\\bbe)'\\e_i\\z_{ir}\\e_i\\x_ie\\left[\\obbeh-\\bbe\\big|\\y_i\\right].\\end{aligned}\\ ] ] noting that it holds for @xmath382 and @xmath383 @xmath384=1_{\\{\\ell = k\\}}\\bsi_k , \\ \\ \\ \\ e[\\obbeh-\\bbe|\\y_i]=\\left(\\x'\\x\\right)^{-1}\\x_i'(\\y_i-\\x_i\\bbe),\\ ] ] we have @xmath385&=\\sum_{\\ell=1}^m\\tr\\left\\{\\e_k\\z_{kr}\\e_k\\x_k(\\x'\\x)^{-1}\\x_k'e\\left[(\\y_\\ell-\\x_\\ell\\bbe)(\\y_k-\\x_k\\bbe)'\\big|\\y_i\\right]\\right\\}\\\\ & = \\tr\\left\\{(\\x'\\x)^{-1}\\x_k'\\bsi_k\\e_k\\z_{kr}\\e_k\\x_k\\right\\},\\end{aligned}\\ ] ] which is @xmath169 and @xmath386=o_p(m^{-1}).\\ ] ] thus , we get @xmath387=-\\frac2m\\sum_{k=1}^m\\sum_{j=1}^{n_k } \\tr\\left\\{(\\x'\\x)^{-1}\\x_k'\\bsi_k\\e_k\\z_{kr}\\e_k\\x_k\\right\\}+o_p(m^{-1}),\\ ] ] where the leading term is @xmath169 . for the third and forth terms ,",
    "note that @xmath388 which are non - stochastic .",
    "then for @xmath389 , @xmath390&=-\\frac1n\\sum_{k=1}^m\\sum_{j=1}^{n_k}z_{kjr}\\si_{kj(2)}^2(\\z_{kj}-2n_k^{-1}\\z_{kj}+n_k^{-1}\\barz_k)'\\bom_{\\bga\\bga}\\z_{kj}+o_p(m^{-1}),\\\\ e[t_{2r}|\\y_i]&=\\frac2n\\sum_{k=1}^m\\tr\\left(\\x_k'\\e_k\\z_{kr}\\e_k\\x_k\\v_{\\rm ols}\\right)+o_p(m^{-1}),\\end{aligned}\\ ] ] for @xmath158 , where we used corollary [ cor : cond ] and @xmath391=\\v_{\\rm ols}+o_p(m^{-1}),\\ ] ] which follows from the similar argument in the proof of corollary [ cor : cond ]",
    ". thus we obtain @xmath392&=-\\frac1n\\sum_{k=1}^m\\sum_{j=1}^{n_k}\\z_{kj}\\si_{kj(2)}^2(\\z_{kj}-2n_k^{-1}\\z_{kj}+n_k^{-1}\\barz_k)'\\bom_{\\bga\\bga}\\z_{kj}+o_p(m^{-1}),\\\\ e[\\t_2|\\y_i]&=\\frac2n\\sum_{k=1}^m\\left\\{\\tr\\left(\\x_k'\\e_k\\z_{kr}\\e_k\\x_k\\v_{\\rm ols}\\right)\\right\\}_r+o_p(m^{-1}),\\end{aligned}\\ ] ] where @xmath393 denotes the @xmath34-dimensional vector @xmath394 .",
    "therefore , we establish the result for @xmath80 in ( [ cond.bias ] ) .",
    "we next derive the result for @xmath79 .",
    "let @xmath395 using the taylor series expansion , we have @xmath396 where we used the fact that @xmath397 .",
    "the straight calculation shows that @xmath398 which are non - stochastic .",
    "thus we obtain @xmath399=e[\\tat^2-\\tau^2|\\y_i]+\\left(\\frac{\\partial\\tat^2}{\\partial \\bga}\\right)'e\\left[\\bgah-\\bga|\\y_i\\right]+\\frac12\\tr\\left\\{\\left(\\frac{\\partial^2\\tat^2}{\\partial\\bga\\partial\\bga'}\\right)e\\left[(\\bgah-\\bga)(\\bgah-\\bga)'\\big|\\y_i\\right]\\right\\}\\\\ & + e\\left[\\left(\\frac{\\partial\\tat^2}{\\partial \\bbe}\\right)'(\\obbeh-\\bbe)\\bigg|\\y_i\\right]+\\frac12\\tr\\left\\{\\left(\\frac{\\partial^2\\tat^2}{\\partial\\bbe\\partial\\bbe'}\\right)e\\left[(\\obbeh-\\bbe)(\\obbeh-\\bbe)'\\big|\\y_i\\right]\\right\\}+o_p(m^{-1})\\\\ & \\equiv b_{\\tau 1}(\\y_i)+b_{\\tau 2}(\\y_i)+b_{\\tau 3}(\\y_i)+b_{\\tau 4}(\\y_i)+b_{\\tau 5}(\\y_i)+o_p(m^{-1}).\\end{aligned}\\ ] ] from the expression of @xmath400 , it holds that @xmath401 for @xmath175 defined in ( [ u1 ] ) .",
    "also , we immediately have @xmath402 for evaluation of @xmath403 , note that @xmath404 similarly to ( [ ols.formula ] ) , we get @xmath405\\\\ & = -\\frac{2}{n}\\sum_{k=1}^m\\tr\\left\\{(\\x'\\x)^{-1}\\x_k'\\bsi_k\\x_k\\right\\}+o_p(m^{-1}).\\end{aligned}\\ ] ] moreover , corollary [ cor : cond ] and ( [ ols.cond ] ) enable us to obtain the expression of @xmath406 and @xmath407 , whereby we get @xmath408 which completes the proof for @xmath79 in ( [ cond.bias ] ) .",
    "we finally derive the result for @xmath98 . by the taylor series expansion , @xmath409 since @xmath410 from @xmath411 as shown in the proof of theorem [ thm : asymp ] . from ( [ bbet.ast ] ) , we have @xmath412 and @xmath413 let @xmath414 $ ] and @xmath415 $ ]",
    ". then it can be shown that @xmath416=\\bom_{\\beta^{\\ast}\\ga_s}+o_p(m^{-1 } ) , \\ \\ \\ e[(\\bbet_{\\ga_s}^{\\ast}-\\bbe)(\\gah_s-\\ga_s)|\\y_i]=\\bom_{\\beta^{\\ast}\\tau}+o_p(m^{-1}),\\ ] ] which can be proved by the same arguments as in corollary [ cor : cond ] .",
    "thus from corollary [ cor : cond ] and the fact that @xmath417=\\left(\\x'\\bsi^{-1}\\x\\right)^{-1}\\x_i'\\bsi_i^{-1}(\\y_i-\\x_i\\bbe),\\ ] ] we obtain the result for @xmath98 in ( [ cond.bias ] ) .    * a4 .",
    "derivation of @xmath224 .",
    "*   since @xmath179 given @xmath418 is non - stochastic , we have @xmath419&=e\\left[e\\left[\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)'(\\bthh-\\bth)w_i\\bigg| v_i,\\bep_i\\right]\\right]\\\\ & = e\\left[e(\\bthh-\\bth |\\y_i)'\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)w_i\\right]\\\\ & = e\\left[\\b_{\\bbe}^{(i)}(\\y_i)'\\left(\\frac{\\partial\\mut_i}{\\partial\\bbe}\\right)w_i\\right]+e\\left[\\b_{\\bga}^{(i)}(\\y_i)'\\left(\\frac{\\partial\\mut_i}{\\partial\\bga}\\right)w_i\\right]+e\\left[b_{\\tau}^{(i)}(\\y_i)\\left(\\frac{\\partial\\mut_i}{\\partial\\tau}\\right)w_i\\right]+o(m^{-1})\\\\ & \\equiv r_{31i}(\\bphi)+o(m^{-1}).\\end{aligned}\\ ] ] it is noted that @xmath420 and @xmath421=e\\left[(v_i+\\ep_{ij})w_i\\right]=\\left(\\sum_{j=1}^{n_i}\\la_{ij}-1\\right)\\tau^2+\\sum_{j=1}^{n_i}\\la_{ij}\\siij^2=0.\\ ] ] using the expression ( [ cond.bias ] ) and ( [ mu.deriv ] ) , it follows that @xmath422&=\\left(\\c_i-\\sum_{j=1}^{n_i}\\la_{ij}\\x_{ij}\\right)'\\left(\\x'\\bsi^{-1}\\x\\right)^{-1}\\x_i'\\bsi_i^{-1}e\\big[(\\y_i-\\x_i\\bbe)w_i\\big]=0\\\\ e\\left[\\b_{\\bga}^{(i)}(\\y_i)'\\left(\\frac{\\partial\\mut_i}{\\partial\\bga}\\right)w_i\\right]&=\\eta_i^{-2}\\sum_{j=1}^{n_i}\\siij^{-2}\\bde_{ij}'\\left(\\sum_{k=1}^{m}\\sum_{h=1}^{n_k}\\si_{kh(1)}^2\\z_{kh}\\z_{kh}'\\right)^{-1}\\m_{2ij}(\\bphi,\\bka)\\\\ e\\left[b_{\\tau}^{(i)}(\\y_i)\\left(\\frac{\\partial\\mut_i}{\\partial\\tau}\\right)w_i\\right]&=m^{-1}\\eta_i^{-2}\\sum_{j=1}^{n_i}\\siij^{-2}\\bigg\\{m_{1ij}(\\bphi,\\bka)-\\t_1(\\bga)'\\t_2(\\bga)\\m_{2ij}(\\bphi,\\bka)\\bigg\\},\\end{aligned}\\ ] ] where @xmath423 , \\ \\ \\ \\   m_{1ij}(\\bphi,\\bka)=e\\left[u_{1i}(y_{ij}-\\x_{ij}'\\bbe)w_i\\right].\\ ] ]    to evaluate @xmath424 and @xmath425 , we first prove the following result for fixed @xmath426 .",
    "@xmath427&=\\tau^2\\eta_i^{-1}\\bigg[\\tau^2(3-\\kappa_v)+\\kappa_{\\ep}\\siij^21_{\\{j = k=\\ell\\}}+\\siij^2(1_{\\{j = k\\neq \\ell\\}}-1_{\\{j = k\\}})\\\\ & \\ \\ \\ \\ \\ + \\siij^2(1_{\\{j=\\ell\\neq k\\}}-1_{\\{j=\\ell\\}})+\\si_{ik}^2(1_{\\{k=\\ell\\neq j\\}}-1_{\\{k=\\ell\\}})\\bigg ] .",
    "\\end{split}\\ ] ] to show ( [ iden2 ] ) , we note that the left side can be rewritten as @xmath428+\\sum_{h=1}^{n_i}\\la_{ih}e\\left[(v_i+\\ep_{ij})(v_i+\\ep_{ik})(v_i+\\ep_{i\\ell})\\ep_{ih}\\right]\\ ] ] from the definition of @xmath429 .",
    "using the fact that @xmath430 and @xmath8 are independent , the first term in ( [ iden3 ] ) is calculated as @xmath431=\\ka_v\\tau^4+\\tau^2\\left(\\siij^21_{\\{j = k\\}}+\\siij^21_{\\{j=\\ell\\}}+\\si_{ik}^21_{\\{k=\\ell\\}}\\right).\\end{aligned}\\ ] ] moreover , we have @xmath432=e\\left[\\ep_{ih}(\\ep_{ij}+\\ep_{i\\ell}+\\ep_{ik})v_i^2+\\ep_{ij}\\ep_{ik}\\ep_{i\\ell}\\ep_{ih}\\right]\\\\ & = \\tau^2\\si_{ih}^2\\left(1_{\\{h = j\\}}+1_{\\{h = k\\}}+1_{\\{h=\\ell\\}}\\right)+\\ka_{\\ep}\\siih^41_{\\{j = k=\\ell = h\\}}+\\siih^2\\left(\\siij^21_{\\{j = k\\neq \\ell = h\\}}+\\siij^21_{\\{j=\\ell\\neq k = h\\}}+\\si_{ik}^21_{\\{j = h\\neq k=\\ell\\}}\\right),\\end{aligned}\\ ] ] whereby the second term in ( [ iden3 ] ) can be calculated as @xmath433,\\ ] ] where we used the expression @xmath434 .",
    "then we established the result ( [ iden2 ] ) . from ( [ iden2 ] ) , we immediately have @xmath435&=\\tau^2\\eta_i^{-1}\\left[n_i\\tau^2(3-\\ka_v)+\\siij^2(\\ka_{\\ep}-3)1_{\\{j = k\\}}\\right]\\\\ & = e\\big[(v_i+\\ep_{ij})(v_i+\\ep_{ik})^2w_i\\big].\\end{aligned}\\ ] ]    now , we return to the evaluation of @xmath424 and @xmath425 .",
    "it follows that @xmath436\\\\ & = mn^{-1}\\eta_i^{-1}\\tau^2\\big\\{n_i\\tau^2(3-\\ka_v)+\\siij^2(\\ka_{\\ep}-3)\\big\\}\\end{aligned}\\ ] ] and @xmath437\\\\ & = \\frac{m}{n}\\sum_{h=1}^{n_i}\\z_{ih}\\bigg\\{e\\left[(v_i+\\ep_{ih})^2(v_i+\\ep_{ij})w_i\\right]-2n_i^{-1}\\sum_{k=1}^{n_i}e\\left[(v_i+\\ep_{ij})(v_i+\\ep_{ik})(v_i+\\ep_{ih})w_i\\right]\\\\ & \\ \\ \\ \\ + n_i^{-2}\\sum_{k=1}^{n_i}\\sum_{\\ell=1}^{n_i}e\\left[(v_i+\\ep_{ij})(v_i+\\ep_{ik})(v_i+\\ep_{i\\ell})w_i\\right]\\bigg\\}.\\end{aligned}\\ ] ] using the identity given in ( [ iden2 ] ) , we have @xmath438 which completes the result in ( [ r31 ] ) .",
    "evaluation of @xmath439 .",
    "*   since @xmath179 given @xmath418 is non - stochastic , we have @xmath440\\\\ & = \\frac12e\\left[e\\left[\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)'(\\bthh-\\bth)(\\bthh-\\bth)'\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)w_i\\bigg| v_i,\\bep_i\\right]\\right]\\\\ & = \\frac12e\\left[\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)'e\\left[(\\bthh-\\bth)(\\bthh-\\bth)'|\\y_i\\right]\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)w_i\\right]=\\frac12\\tr\\left\\{\\bom e\\left[\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)\\left(\\frac{\\partial\\mut_i}{\\partial\\bth}\\right)'w_i\\right]\\right\\}+o(m^{-1}),\\end{aligned}\\ ] ] where we used corollary [ cor : cond ] in the last equation .",
    "note that @xmath441&=-\\eta_i^{-1}e\\left[(v_i+\\ep_{ij})^2v_i\\right]+\\sum_{h=1}^{n_i}e\\left[(v_i+\\ep_{ij})^2\\ep_{ih}\\right]=0\\end{aligned}\\ ] ] since @xmath230=0 $ ] and @xmath442=0 $ ] . using the expression ( [ mu.deriv ] ) of @xmath443 with the above moment results , we obtain @xmath444=0,\\ ] ] which leads to @xmath445 ."
  ],
  "abstract_text": [
    "<S> the article considers a nested error regression model with heteroscedastic variance functions for analyzing clustered data , where the normality for the underlying distributions is not assumed . </S>",
    "<S> classical methods in normal nested error regression models with homogenous variances are extended in the two directions : heterogeneous variance functions for error terms and non - normal distributions for random effects and error terms . </S>",
    "<S> consistent estimators for model parameters are suggested , and second - order approximations of their biases and variances are derived . the mean squared errors of the empirical best linear unbiased predictors are expressed explicitly to second - order . </S>",
    "<S> second - order unbiased estimators of the mean squared errors are provided analytically in closed forms . the proposed model and the resulting procedures are numerically investigated through simulation and empirical studies . </S>"
  ]
}