{
  "article_text": [
    "gaussian process classifiers are a very effective family of non - parametric methods for supervised classification @xcite . in the binary case ,",
    "the class label @xmath0 associated to each data instance @xmath1 is assumed to depend on the sign of a function @xmath2 which is modeled using a gaussian process prior . given some data @xmath3 , learning is performed by computing a posterior distribution for @xmath2 .",
    "nevertheless , the computation of such a posterior distribution is intractable and it must be approximated using methods for approximate inference @xcite .",
    "a practical disadvantage is that the cost of most of these methods scales like @xmath4 , where @xmath5 is the number of training instances .",
    "this limits the applicability of gaussian process classifiers to small datasets with a few data instances at most .",
    "recent advances on gaussian process classification have led to sparse methods of approximate inference that reduce the training cost of these classifiers .",
    "sparse methods introduce @xmath6 inducing points or pseudoinputs , whose location is determined during the training process , leading to a training cost that is @xmath7 @xcite .",
    "a notable approach combines in @xcite the sparse approximation suggested in @xcite with stochastic variational inference @xcite .",
    "this allows to learn the posterior for @xmath2 and the hyper - parameters ( inducing points , length - scales , amplitudes and noise ) using stochastic gradient ascent .",
    "the consequence is that the training cost is @xmath8 , which does not depend on the number of instances @xmath5 .",
    "similarly , in a recent work , expectation propagation ( ep ) @xcite is considered as an alternative to stochastic variational inference for training these classifiers @xcite .",
    "that work shows ( i ) that stochastic gradients can also be used to learn the hyper - parameters in ep , and ( ii ) that ep performs similarly to the variational approach , but does not require one - dimensional quadratures .",
    "a disadvantage of the approach described in @xcite is that the memory requirements scale like @xmath9 since ep stores in memory @xmath10 parameters for each data instance .",
    "this is a severe limitation when dealing with very large datasets with millions of instances and complex models with many inducing points . to reduce the memory cost , we investigate in this extended abstract , as an alternative to ep , the use of stochastic propagation ( sep ) @xcite . unlike ep",
    ", sep only stores a single global approximate factor for the complete likelihood of the model , leading to a memory cost that scales like @xmath11 .",
    "we now explain the method for gaussian process classification described in @xcite .",
    "consider @xmath12 the observed labels .",
    "let @xmath13 be a matrix with the observed data .",
    "the assumed labeling rule is @xmath14 , where @xmath15 is a non - linear function following a zero mean gaussian process with covariance function @xmath16 , and @xmath17 is standard normal noise that accounts for mislabeled data .",
    "let @xmath18 be the matrix of inducing points ( _ i.e. _ , virtual data that specify how @xmath2 varies ) .",
    "let @xmath19 and @xmath20 be the vectors of @xmath2 values associated to @xmath21 and @xmath22 , respectively .",
    "the posterior of @xmath23 is approximated as @xmath24 , with @xmath25 a gaussian that approximates @xmath26 , _",
    "i.e. _ , the posterior of the values associated to @xmath22 . to get @xmath25 , first the full independent training conditional approximation ( fitc ) @xcite of @xmath27",
    "is employed to approximate @xmath26 and to reduce the training cost from @xmath4 to @xmath7 : @xmath28    where @xmath29 , @xmath30 and @xmath31 , with @xmath32 , @xmath33 , @xmath34 and @xmath35 is the marginal likelihood .",
    "furthermore , @xmath36 is a matrix with the prior covariances among the entries in @xmath37 , @xmath38 is a row vector with the prior covariances between @xmath39 and @xmath37 and @xmath40 is the prior variance of @xmath39 .",
    "finally , @xmath41 denotes the p.d.f of a gaussian distribution with mean vector equal to @xmath42 and covariance matrix equal to @xmath43 .",
    "next , the r.h.s . of ( [ eq : posterior ] ) is approximated in @xcite via expectation propagation ( ep ) to obtain @xmath25 .",
    "for this , each non - gaussian factor @xmath44 is replaced by a corresponding un - normalized gaussian approximate factor @xmath45 .",
    "that is , @xmath46 , where @xmath47 is a @xmath48 dimensional vector , and @xmath49 , @xmath50 and @xmath51 are parameters estimated by ep so that @xmath44 is similar to @xmath45 in regions of high posterior probability as estimated by @xmath52 .",
    "namely , @xmath53 , where @xmath54 is the kullback leibler divergence .",
    "we note that each @xmath45 has a one - rank precision matrix and hence only @xmath10 parameters need to be stored per each @xmath45 . the posterior approximation @xmath25 is obtained by replacing in the r.h.s . of ( [ eq : posterior ] )",
    "each exact factor @xmath44 with the corresponding @xmath45 .",
    "namely , @xmath55 , where @xmath56 is a constant that approximates @xmath35 , which can be maximized for finding good hyper - parameters via type - ii maximum likelihood @xcite .",
    "finally , since all factors in @xmath25 are gaussian , @xmath25 is a multivariate gaussian .    in order for gaussian process classification to work well",
    ", hyper - parameters and inducing points must be learned from the data .",
    "previously , this was infeasible on big datasets using ep . in @xcite",
    "the gradient of @xmath57 w.r.t @xmath58 ( _ i.e. _ , a parameter of the covariance function @xmath59 or a component of @xmath22 ) is : @xmath60    where @xmath61 and @xmath62 are the expected sufficient statistics under @xmath25 and @xmath63 , respectively , @xmath64 are the natural parameters of @xmath63 , and @xmath65 is the normalization constant of @xmath66 .",
    "we note that ( [ eq : gradient ] ) has a sum across the data .",
    "this enables using stochastic gradient ascent for hyper - parameter learning .    a batch iteration of ep updates in parallel each @xmath45 .",
    "after this , @xmath25 is recomputed and the gradients of @xmath57 with respect to each hyper - parameter are used to update the model hyper - parameters .",
    "the ep algorithm in @xcite can also process data using minibatches of size @xmath67 . in this case , the update of the hyper - parameters and the reconstruction of @xmath25 is done after processing each minibatch .",
    "the update of each @xmath45 corresponding to the data contained in the minibatch is also done in parallel . when computing the gradient of the hyper - parameters , the sum in the r.h.s .",
    "of ( [ eq : gradient ] ) is replaced by a stochastic approximation , _",
    "i.e. _ , @xmath68 , with @xmath69 the set of indices of the instances of the current minibatch . when using minibatches and stochastic gradients the training cost is @xmath8",
    "the method described in the previous section has the disadvantage that it requires to store in memory @xmath70 parameters for each approximate factor @xmath45 .",
    "this leads to a memory cost that scales like @xmath9 .",
    "thus , in very big datasets where @xmath5 is of the order of several millions , and in complex models where the number of inducing points @xmath48 may be in the hundreds , this cost can lead to memory problems . to alleviate this",
    ", we consider training via stochastic expectation propagation ( sep ) as an alternative to expectation propagation @xcite .",
    "sep reduces the memory requirements by a factor of @xmath5 .    r0.5    ll + & for each approximate factor @xmath45 to update : + 1.1 : & + 1.2 : & @xmath71 + 2 : & reconstruct @xmath25 : @xmath72 +    ll + & set the new global factor @xmath73 to be uniform .",
    "+ 2 : & for each exact factor @xmath44 to incorporate : + 2.1 : & + 2.2 : & @xmath71 + 2.3 : & @xmath74 + 3 : & reconstruct @xmath25 : @xmath75 +    ll + & set @xmath25 to the prior . for each @xmath44 to process :",
    "+ 1.1 : & + 1.2 : & @xmath71 + 2 : & update @xmath25 : @xmath76 +    in sep the likelihood of the model is approximated by a single global gaussian factor @xmath77 , instead of a product of @xmath5 gaussian factors @xmath45 .",
    "the idea is that the natural parameters @xmath78 of @xmath77 approximate the sum of the natural parameters @xmath79 of the ep approximate factors @xmath45 .",
    "this approximation reduces by a factor of @xmath5 the memory requirements because only the natural parameters @xmath78 of @xmath77 need to be stored in memory , and the size of @xmath78 is dominated by the precision matrix of @xmath77 , which scales like @xmath11 .    when sep is used instead of ep for finding @xmath25 some things change .",
    "in particular , the computation of the cavity distribution @xmath80 is now replaced by @xmath81 , @xmath82 .",
    "furthermore , in the case of the batch learning method described in the previous section , the corresponding approximate factor @xmath45 for each instance is computed as @xmath53 to then set @xmath83 .",
    "this is equivalent to adding natural parameters , _",
    "i.e. _ , @xmath84 . in the case of minibatch training with minibatches of size",
    "@xmath85 the update is slightly different to account for the fact that we have only processed a small amount of the total data . in this case ,",
    "@xmath86 , where @xmath69 is a set with the indices of the instances contained in the current minibatch .",
    "finally , in sep the computation of the gradients for updating the hyper - parameters is done exactly as in ep .",
    "figure [ fig : fig_ep_vs_sep ] compares among ep , sep and adf @xcite when used to update @xmath25 . in the figure training",
    "is done in batch mode and the update of the hyper - parameters has been omitted since it is exactly the same in either ep , sep or adf . in adf the cavity distribution @xmath87 is simply the posterior approximation @xmath25 , and when @xmath25 is recomputed , the natural parameters of the approximate factors are simply added to the natural parameters of @xmath25 .",
    "adf is a simple baseline in which each data point is _ seen _ by the model several times and hence it underestimates variance @xcite .",
    "we evaluate the performance of the model described before when trained using ep , sep and adf .",
    "* performance on datasets from the uci repository : * first , we consider 7 datasets from the uci repository .",
    "the experimental protocol followed is the same as the one described in @xcite . in these experiments",
    "we consider a different number of inducing points @xmath48 .",
    "namely , @xmath88 , @xmath89 and @xmath90 of the total training instances and the training of all methods is done in batch mode for 250 iterations .",
    "table [ tab : ll_uci ] shows the average negative test log likelihood of each method ( the lower the better ) on the test set .",
    "the best method has been highlighted in boldface .",
    "we note that sep obtains similar and sometimes even better results than ep .",
    "by contrast , adf performs worse , probably because it underestimating the posterior variance . in terms of the average training time",
    "all methods are equal .",
    ".average negative test log likelihood for each method and average training time in seconds . [ cols=\"<,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]",
    "* performance on big datasets : * we carry out experiments when the model is trained using minibatches .",
    "we follow @xcite and consider the mnist dataset , which has 70,000 instances , and the airline delays dataset , which has 2,127,068 data instances ( see @xcite for more details ) . in both cases",
    "the test set has 10,000 instances .",
    "training is done using minibatches of size 200 , which is equal to the number of inducing points @xmath48 . in the case of the mnist dataset",
    "we also report results for batch training ( in the airline dataset batch training is infeasible ) .",
    "figure [ fig : stochastic ] shows the avg .",
    "negative log likelihood obtained on the test set as a function of training time . in the mnist dataset training using minibatches",
    "is much more efficient .",
    "furthermore , in both datasets sep performs very similar to ep .",
    "however , in these experiments adf provides equivalent results to both sep and ep .",
    "furthermore , in the airline dataset both sep and adf provide better results than ep at the early iterations , and improve a simple linear model after just a few seconds .",
    "the reason is that , unlike ep , sep and adf do not initialize the approximate factors to be uniform , which has a significant cost in this dataset .",
    "r0.5    @xmath91 the results obtained in the large datasets contradict the results obtained in the uci datasets in the sense that adf performs similar to ep .",
    "we believe the reason for this is that adf may perform similar to ep only when the model is simple ( small @xmath48 ) and/or when the number of training instances is very large ( large @xmath5 ) . to check that this is the case , we repeat the experiments with the mnist dataset with an increasing number of training instances @xmath5 ( from @xmath92 to @xmath93 ) and with an increasing number of inducing points @xmath48 ( from @xmath94 to @xmath95 ) .",
    "the results obtained are shown in figure [ fig : n_vsm ] , which confirms that adf only performs similar to ep in the scenario described .",
    "by contrast , sep seems to always perform similar to ep .",
    "finally , increasing the model complexity ( @xmath48 ) seems beneficial .",
    "stochastic expectation propagation ( sep ) @xcite can reduce the memory cost of the method recently proposed in @xcite to address large scale gaussian process classification .",
    "such a method uses expectation propagation ( ep ) for training , which stores @xmath9 parameters in memory , where @xmath6 is some small constant and @xmath5 is the training set size",
    ". this cost may be too expensive in the case of very large datasets or complex models .",
    "sep reduces the storage resources needed by a factor of @xmath5 , leading to a memory cost that is @xmath11 .",
    "furthermore , several experiments show that sep provides similar performance results to those of ep .",
    "a simple baseline known as adf may also provide similar results to sep , but only when the number of instances is very large and/or the chosen model is very simple . finally , we note that applying bayesian learning methods at scale makes most sense with large models , and this is precisely the aim of the method described in this extended abstract .    *",
    "acknowledgments : * yl thanks the schlumberger foundation for her faculty for the future phd fellowship .",
    "jmhl acknowledges support from the rafael del pino foundation .",
    "ret thanks epsrc grant # s ep / g050821/1 and ep / l000776/1 .",
    "tb thanks google for funding his european doctoral fellowship .",
    "dhl and jmhl acknowledge support from plan nacional i+d+i , grant tin2013 - 42351-p , and from comunidad autnoma de madrid , grant s2013/ice-2845 casi - cam - cm .",
    "dhl is grateful for using the computational resources of _ centro de computacin cientfica _ at universidad autnoma de madrid .",
    "j.  hensman , a.  matthews , and z.  ghahramani .",
    "scalable variational gaussian process classification . in _ proceedings of the eighteenth international conference on artificial intelligence and statistics _ , 2015 ."
  ],
  "abstract_text": [
    "<S> a method for large scale gaussian process classification has been recently proposed based on expectation propagation ( ep ) . </S>",
    "<S> such a method allows gaussian process classifiers to be trained on very large datasets that were out of the reach of previous deployments of ep and has been shown to be competitive with related techniques based on stochastic variational inference . nevertheless , the memory resources required scale linearly with the dataset size , unlike in variational methods . </S>",
    "<S> this is a severe limitation when the number of instances is very large . </S>",
    "<S> here we show that this problem is avoided when stochastic ep is used to train the model . </S>"
  ]
}