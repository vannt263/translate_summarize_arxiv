{
  "article_text": [
    "the most important aspect of any classifier is its error , @xmath0 , defined as the probability of misclassification , since @xmath0 quantifies the predictive capacity of the classifier .",
    "relative to a classification rule and a given feature - label distribution , the error is a function of the sampling distribution and as such possesses its own distribution , which characterizes the true performance of the classification rule . in practice , the error must be estimated from data by some error estimation rule yielding an estimate , @xmath1 .",
    "if samples are large , then part of the data can be held out for error estimation ; otherwise , the classification and error estimation rules are applied on the same set of training data , which is the situation that concerns us here . like the true error ,",
    "the estimated error is also a function of the sampling distribution .",
    "the performance of the error estimation rule is completely described by its joint distribution , @xmath2 .",
    "three widely - used metrics for performance of an error estimator are the bias , deviation variance , and root - mean - square ( rms ) , given by @xmath3\\,=\\,e[\\hat{\\varepsilon}]-e[\\varepsilon ] \\,,\\;\\;\\ ; \\\\[-0.5ex ] & \\text{var}^{d}[\\hat{\\varepsilon}]\\,=\\,\\text{var}(\\hat{\\varepsilon}% -\\varepsilon ) \\,=\\,\\text{var}(\\varepsilon ) + \\text{var}(\\hat{\\varepsilon})-2% \\text{cov}(\\varepsilon , \\hat{\\varepsilon})\\ , , \\\\[-0.5ex ] & \\text{rms}[\\hat{\\varepsilon}]\\,=\\,\\sqrt{e[(\\varepsilon -\\hat{\\varepsilon}% ) ^{2}]}\\,=\\,\\sqrt{e[\\varepsilon ^{2}]+e[{\\hat{\\varepsilon}}% ^{2}]-2e[\\varepsilon \\hat{\\varepsilon}]}=\\sqrt{\\text{bias}[\\hat{\\varepsilon}% ] ^{2}+\\text{var}^{d}[\\hat{\\varepsilon}]}\\ , , \\end{aligned } \\label{eq - rms}\\]]respectively .",
    "the rms ( square root of mean square error , mse ) is the most important because it quantifies estimation accuracy .",
    "bias requires only the first - order moments , whereas the deviation variance and rms require also the second - order moments .    historically",
    ", analytic study has mainly focused on the first marginal moment of the estimated error for linear discriminant analysis ( lda ) in the gaussian model or for multinomial discrimination  @xcite-@xcite ; however , marginal knowledge does not provide the joint probabilistic knowledge required for assessing estimation accuracy , in particular , the mixed second moment .",
    "recent work has aimed at characterizing joint behavior .",
    "for multinomial discrimination , exact representations of the second - order moments , both marginal and mixed , for the true error and the resubstitution and leave - one - out estimators have been obtained  @xcite . for lda",
    ", the exact joint distributions for both resubstitution and leave - one - out have been found in the univariate gaussian model and approximations have been found in the multivariate model with a common known covariance matrix  zollbragdoug:12,zollbragdoug:10 .",
    "whereas one could utilize the approximate representations to find approximate moments via integration in the multivariate model with a common known covariance matrix , more accurate approximations , including the second - order mixed moment and the rms , can be achieved via asymptotically exact analytic expressions using a double asymptotic approach , where both sample size ( @xmath4 ) and dimensionality ( @xmath5 ) approach infinity at a fixed rate between the two  @xcite .",
    "finite - sample approximations from the double asymptotic method have shown to be quite accurate @xcite .",
    "there is quite a body of work on the use of double asymptotics for the analysis of lda and its related statistics zollanvari , raud:72,deev:70,fuji:00,serd:00,bickel:04 .",
    "raudys and young provide a good review of the literature on the subject @xcite .",
    "although the theoretical underpinning of both  @xcite and the present paper relies on double asymptotic expansions , in which @xmath6 at a proportional rate , our practical interest is in the finite - sample approximations corresponding to the asymptotic expansions . in @xcite , the accuracy of such finite - sample approximations was investigated relative to asymptotic expansions for the expected error of lda in a gaussian model .",
    "several single - asymptotic expansions ( @xmath7 ) were considered , along with double - asymptotic expansions ( @xmath6 ) @xcite .",
    "the results of wyma:90 show that the double - asymptotic approximations are significantly more accurate than the single - asymptotic approximations .",
    "in particular , even with @xmath8 , the double - asymptotic expansions yield excellent approximations  while the others falter .",
    "the aforementioned work is based on the assumption that a sample is drawn from a fixed feature - label distribution @xmath9 , a classifier and error estimate are derived from the sample without using any knowledge concerning @xmath9 , and performance is relative to @xmath9 .",
    "research dating to 1978 , shows that small - sample error estimation under this paradigm tends to be inaccurate .",
    "re - sampling methods such as cross - validation possess large deviation variance and , therefore , large rms @xcite .",
    "scientific content in the context of small - sample classification can be facilitated by prior knowledge @xcite .",
    "there are three possibilities regarding the feature - label distribution : ( 1 ) @xmath9 is known , in which case no data are needed and there is no error estimation issue ; ( 2 ) nothing is known about @xmath9 , there are no known rms bounds , or those that are known are useless for small samples ; and ( 3 ) @xmath9 is known to belong to an uncertainty class of distributions and this knowledge can be used to either bound the rms @xcite or be used in conjunction with the training data to estimate the error of the designed classifier .",
    "if there exists a prior distribution governing the uncertainty class , then in essence we have a distributional model . since virtually nothing can be said about the error estimate in the first two cases , for a classifier to possess scientific content we must begin with a distributional model .",
    "given the need for a distributional model , a natural approach is to find an optimal minimum mean - square - error ( mmse ) error estimator relative to an uncertainty class @xmath10  @xcite .",
    "this results in a bayesian approach with @xmath10 being given a prior distribution , @xmath11 , and the sample @xmath12 being used to construct a posterior distribution , @xmath13 , from which an optimal mmse error estimator , @xmath14 , can be derived .",
    "@xmath15 provides a mathematical framework for both the analysis of any error estimator and the design of estimators with desirable properties or optimal performance .",
    "@xmath13 provides a sample - conditioned distribution on the true classifier error , where randomness in the true error comes from uncertainty in the underlying feature - label distribution ( given @xmath12 ) .",
    "finding the sample - conditioned mse , @xmath16 $ ] , of an mmse error estimator amounts to evaluating the variance of the true error conditioned on the observed sample  @xcite .",
    "@xmath17\\rightarrow 0 $ ] as @xmath18 almost surely in both the discrete and gaussian models provided in  @xcite , where closed form expressions for the sample - conditioned mse are available .",
    "the sample - conditioned mse provides a measure of performance across the uncertainty class @xmath10 for a given sample @xmath12 . as such ,",
    "it involves various sample - conditioned moments for the error estimator : @xmath19 $ ] , @xmath20 $ ] , and @xmath21 $ ] .",
    "one could , on the other hand , consider the mse relative to a fixed feature - label distribution in the uncertainty class and randomness relative to the sampling distribution .",
    "this would yield the feature - label - distribution - conditioned mse , @xmath22 $ ] , and the corresponding moments : @xmath23 $ ] , @xmath24 $ ] , and @xmath25 $ ] . from a classical point of view , the moments given @xmath26 are of interest as they help shed light on the performance of an estimator relative to fixed parameters of class conditional densities .",
    "using this set of moments ( i.e. given @xmath26 ) we are able to compare the performance of the bayesian mmse error estimator to classical estimators of true error such as resubstitution and leave - one - out .    from a global perspective , to evaluate performance across both the uncertainty class and the sampling distribution requires the unconditioned mse , @xmath27 $ ] , and corresponding moments @xmath28 $ ] , @xmath29 $ ] , and @xmath30 $ ] .",
    "while both @xmath31 $ ] and @xmath32 $ ] have been examined via simulation studies in @xcite for discrete and gaussian models , our intention in the present paper is to derive double - asymptotic representations of the feature - labeled conditioned ( given @xmath26 ) and unconditioned mse , along with the corresponding moments of the bayesian mmse error estimator for linear discriminant analysis ( lda ) in the gaussian model .",
    "we make three modeling assumptions . as in many analytic error analysis studies ,",
    "we employ stratified sampling : @xmath33 sample points are selected to constitute the sample @xmath12 in @xmath34 , where given @xmath4 , @xmath35 and @xmath36 are determined , and where @xmath37 and @xmath38 @xmath39  are randomly selected from distributions @xmath40 and @xmath41 , respectively .",
    "@xmath42 possesses a multivariate gaussian distribution @xmath43 , for @xmath44 .",
    "this means that the prior probabilities @xmath45 and @xmath46 for classes 0 and 1 , respectively , can not be estimated from the sample ( see @xcite for a discussion of issues surrounding lack of an estimator for @xmath47 ) .",
    "however , our second assumption is that @xmath47 and @xmath48 are known .",
    "this is a natural assumption for many medical classification problems .",
    "if we desire early or mid - term detection , then we are typically constrained to a small sample for which @xmath49 and @xmath36 are not random but for which @xmath47 and @xmath48 are known ( estimated with extreme accuracy ) on account of a large population of post - mortem examinations .",
    "the third assumption is that there is a known common covariance matrix for the classes , a common assumption in error analysis @xcite .",
    "the common covariance assumption is typical for small samples because it is well - known that lda commonly performs better that quadratic discriminant analysis ( qda ) for small samples , even if the actual covariances are different , owing to the estimation advantage of using the pooled sample covariance matrix . as for the assumption of known covariance ,",
    "this assumption is typical simply owing to the mathematical difficulties of obtaining error representations for unknown covariance ( we know of no unknown - covariance result for second - order representations ) . indeed ,",
    "the natural next step following this paper and @xcite is to address the unknown covariance problem ( although with it being outstanding for almost half a century , it may prove difficult ) .    under our assumptions ,",
    "the _ anderson _ @xmath50 _ statistic _ is defined by @xmath51where @xmath52 and @xmath53 .",
    "the corresponding linear discriminant is defined by @xmath54 if @xmath55 and @xmath56 if @xmath57 , where @xmath58 .",
    "given sample @xmath12 ( and thus @xmath59 and @xmath60 ) , for @xmath44 , the error for @xmath61 is given by @xmath62 , where @xmath63and @xmath64 denotes the cumulative distribution function of a standard normal random variable .",
    "raudys proposed the following approximation to the expected lda classification error @xcite : @xmath65=p(w(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},% \\mathbf{x})\\leq c\\mid \\mathbf{x}\\in \\pi _ { 0})\\,\\eqsim \\,\\phi \\left ( \\frac{% -e_{s_{n}}[w(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{x})\\mid   \\mathbf{x}\\in \\pi _ { 0}]+c}{\\sqrt{\\text{var}_{s_{n}}[w(\\bar{\\mathbf{x}}_{0},% \\bar{\\mathbf{x}}_{1},\\mathbf{x})\\mid \\mathbf{x}\\in \\pi _ { 0}]}}\\right ) \\ , \\end{aligned}%\\]]we provide similar approximations for error - estimation moments and prove asymptotic exactness .",
    "in the bayesian classification framework @xcite , it is assumed that the class-0 an class-1 conditional distributions are parameterized by @xmath66 and @xmath67 , respectively .",
    "therefore , assuming known @xmath68 , the actual feature - label distribution belongs to an uncertainty class parameterized by @xmath69 according to a prior distribution , @xmath15 . given a sample @xmath12 ,",
    "the bayesian mmse error estimator minimizes the mse between the true error of a designed classifier , @xmath61 , and an error estimate ( a function of @xmath12 and @xmath70 ) .",
    "the expectation in the mse is taken over the uncertainty class conditioned on @xmath12 .",
    "specifically , the mmse error estimator is the expected true error , @xmath71 $ ] .",
    "the expectation given the sample is over the posterior density , @xmath13 .",
    "thus , we write the bayesian mmse error estimator as @xmath72 $ ] . to facilitate analytic representations , @xmath73 and @xmath67 are assumed to be independent prior to observing the data .",
    "denote the marginal priors of @xmath74 and @xmath67 by @xmath75 and @xmath76 , respectively , and the corresponding posteriors by @xmath77 and @xmath78 , respectively .",
    "independence is preserved , i.e. , @xmath79 for @xmath80  @xcite .    owing to the posterior independence between @xmath73 and @xmath67 and the fact that @xmath81 is known , the bayesian mmse error estimator can be expressed by  @xcite @xmath82+\\alpha _ { 1}\\mathrm{e}_{\\pi ^{\\ast } } [ \\varepsilon _ { 1}]=\\alpha _ { 0}\\hat{% \\varepsilon}_{0}^{b}+\\alpha _ { 1}\\hat{\\varepsilon}_{1}^{b } ,   \\label{eq : beeu}\\]]where , letting @xmath83 be the parameter space of @xmath84 , @xmath85=\\int_{% \\mathbf{\\theta } _ { i}}\\varepsilon _ { i}(\\theta _ { i})\\pi ^{\\ast } ( \\theta _ { i})d\\theta _ { i}.   \\label{eq : mmse : general}\\]]for known @xmath86 and the prior distribution on @xmath87 assumed to be gaussian with mean @xmath88 and covariance matrix @xmath89 , @xmath90 is given by equation ( 10 ) in @xcite : @xmath91where @xmath92and @xmath93 is a measure of our certainty concerning the prior knowedge  the larger @xmath94 is the more localized the prior distribution is about @xmath88 . letting @xmath95^{t}$ ] , the moments that interest us are of the form @xmath96 $ ] , @xmath97 $ ] , and @xmath98 $ ] , which are used to obtain @xmath99 $ ] , and @xmath100 $ ] , @xmath101 $ ] , and @xmath102 $ ] , which are needed to characterize @xmath103 $ ] .",
    "the raudys - kolmogorov asymptotic conditions @xcite are defined on a sequence of gaussian discrimination problems with a sequence of parameters and sample sizes : @xmath104 , @xmath105 , where the means and the covariance matrix are arbitrary .",
    "the common assumptions for raudys - kolmogorov asymptotics are @xmath106 . for notational simplicity , we denote the limit under these conditions by @xmath107 . in the analysis of classical statistics related to lda",
    "it is commonly assumed that the mahalanobis distance , @xmath108 , is finite and @xmath109 ( see @xcite , p. 4 ) .",
    "this condition assures existence of limits of performance metrics of the relevant statistics @xcite .    to analyze the bayesian mmse error estimator , @xmath90 ,",
    "we modify the sequence of gaussian discrimination problems to : @xmath110 in addition to the previous conditions , we assume that the following limits exist for @xmath111 : @xmath112 , @xmath113 , and @xmath114 , where @xmath115 , @xmath116 , and @xmath117 are some constants to which the limits converge . in @xcite , fairly mild sufficient conditions are given for the existence of these limits .",
    "we refer to all of the aforementioned conditions , along with @xmath118 , as the _ bayesian - kolmogorov asymptotic conditions _",
    "( b.k.a.c ) .",
    "we denote the limit under these conditions by @xmath119 , which means that , for @xmath111 , @xmath120this limit is defined for the case where there is conditioning on a specific value of @xmath121 .",
    "therefore , in this case @xmath122 is not a random variable , and for each @xmath5 , it is a vector of constants .",
    "absent such conditioning , the sequence of discrimination problems and the above limit reduce to @xmath123and @xmath124respectively . for notational simplicity",
    "we assume clarity from the context and do not explicitly differentiate between these conditions .",
    "we denote convergence in probability under bayesian - kolmogorov asymptotic conditions by @xmath125 `` .",
    "@xmath126 '' and @xmath127 \" denote ordinary convergence under bayesian - kolmogorov asymptotic conditions . at no risk of ambiguity",
    ", we henceforth omit the subscript @xmath5  from the parameters and sample sizes in ( [ gkusguk ] ) or ( [ gkusgukll ] ) .",
    "we define @xmath128 and , for ease of notation write @xmath129 as @xmath130 .",
    "there are two special cases : ( 1 ) the square of the mahalanobis distance in the space of the parameters of the unknown class conditional densities , @xmath131 ; and ( 2 ) a measure of distance of prior distributions , @xmath132 , where @xmath133^{t}$ ] .",
    "the conditions in ( [ kaclu ] ) assure existence of @xmath134 , where the @xmath135 s can be any combination of @xmath88 and @xmath136 , @xmath44 .",
    "consistent with our notations , we use @xmath137 , @xmath138 , and @xmath139 to denote the @xmath140 of @xmath141 , @xmath142 , and @xmath143 , respectively .",
    "thus , @xmath144    the ratio @xmath145 is an indicator of complexity for lda ( in fact , any linear classification rule ) : the vc dimension in this case is @xmath146 devrgyorlugo:96 .",
    "therefore , the conditions ( [ kaclu ] ) assure the existence of the asymptotic complexity of the problem .",
    "the ratio @xmath147 is an indicator of relative certainty of prior knowledge to the data : the smaller @xmath148 , the more we rely on the data and less on our prior knowledge .",
    "therefore , the conditions ( [ kaclu ] ) state asymptotic existence of relative certainty . in the following ,",
    "we let @xmath149 , so that @xmath150 .",
    "in this section we use the bayesian - kolmogorov asymptotic conditions to characterize the conditional and unconditional first moment of the bayesian mmse error estimator .      the asymptotic ( in a bayesian - kolmogorov sense )",
    "conditional expectation of the bayesian mmse error estimator is characterized in the following theorem , with the proof presented in the appendix .",
    "note that @xmath154 , @xmath155 , and @xmath156 depend on @xmath157 , but to ease the notation we leave this implicit .",
    "[ thm - m1 ] consider the sequence of gaussian discrimination problems defined by ( [ gkusguk ] ) . then @xmath158=\\phi \\left ( ( -1)^{i}\\;\\frac{-g_{i}^b+c}{\\sqrt{d}}% \\right)\\ , , \\end{aligned } \\label{eq - t1}\\]]so that @xmath159\\,={\\alpha}_0\\phi \\left ( \\frac{-g_{0}^b+c}{\\sqrt{d}}\\right ) + { \\alpha}_1\\phi \\left ( \\frac{g_{1}^b - c}{\\sqrt{d}}\\right)\\ , ,   \\label{csxasas}\\]]where @xmath160 & g_{1}^b=\\frac{-1}{2(1+\\gamma_1)}\\big ( \\gamma_1(\\overline{\\eta } _ { \\mathbf{m}% _ 1,\\boldsymbol{\\mu}_0}-\\overline{\\eta } _ { \\mathbf{m}_1,\\boldsymbol{\\mu}_1})+% \\overline{\\delta}^2_{\\boldsymbol{\\mu}}+(1-\\gamma_1)j_1+(1+\\gamma_1)j_0 \\big ) \\\\[-0.4ex ] & d\\,=\\,\\overline{\\delta}^{2}_{\\boldsymbol{\\mu}}+j_{0}+j _ { 1}\\ , .",
    "\\quad \\blacksquare \\end{aligned } \\label{njzz}\\ ] ]    theorem  [ thm - m1 ] suggests a finite - sample approximation : @xmath161\\,\\eqsim \\phi \\left (   \\frac{-g_{0}^{b , f}+c}{\\sqrt{\\delta _ { \\boldsymbol{\\mu } } ^{2}+\\frac{p}{n_{0}}+% \\frac{p}{n_{1}}}}\\right ) , \\end{aligned } \\label{eq - tdee0s2du}\\]]where @xmath162 is obtained by using the finite - sample parameters of the problem in ( [ njzz ] ) , namely , @xmath163to obtain the corresponding approximation for @xmath164 $ ] , it suffices to use ( [ eq - tdee0s2du ] ) by changing the sign of @xmath165 , exchanging @xmath49 and @xmath36 , @xmath166 and @xmath167 , @xmath168 and @xmath169 , and @xmath170 and @xmath171 in @xmath172 .    to obtain a raudys - type of finite - sample approximation for the expectation of @xmath173 , first note that the gaussian distribution in ( [ qwsaqwsau ] ) can be rewritten as @xmath174where",
    "@xmath175 is independent of @xmath12 , @xmath176 is a multivariate gaussian @xmath177 , and @xmath178taking the expectation of @xmath173 relative to the sampling distribution and then applying the standard normal approximation yields the raudys - type of approximation : @xmath161=p(u_{0}(\\bar{\\mathbf{x% } } _ { 0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\leq c|\\mathbf{z}\\in \\psi _ { 0},% \\boldsymbol{\\mu } ) \\eqsim \\,\\phi \\left ( \\frac{-e_{s_{n},\\mathbf{z}}[u_{0}(% \\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})|\\mathbf{z}\\in \\psi _ { 0},\\boldsymbol{\\mu } ) ] + c}{\\sqrt{\\text{var}_{s_{n},\\mathbf{z}}[u_{0}(\\bar{% \\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})|\\mathbf{z}\\in \\psi _ { 0},% \\boldsymbol{\\mu } ] } } \\right ) . \\end{aligned } \\label{kshbxkabk}\\]]algebraic manipulation yields ( suppl .",
    "section a )    @xmath161\\,\\eqsim \\phi \\left (   \\frac{-g_{0}^{b , r}+c}{\\sqrt{d_{0}^{b , r}}}\\right ) , \\end{aligned } \\label{condraudys}\\ ] ]    where @xmath179 with @xmath162 being presented in ( [ g0f ] ) and @xmath180+\\frac{p}{n_{0}}+\\frac{p}{n_{1}}+\\frac{p}{% n_{0}^{2}(1+\\beta _ { 0 } ) } \\\\ & + \\frac{p}{n_{0}n_{1}(1+\\beta _ { 0})}+\\frac{(1-\\beta _ { 0})^{2}p}{% 2n_{0}^{2}(1+\\beta _ { 0})^{2}}+\\frac{p}{n_{0}n_{1}(1+\\beta _ { 0})^{2}}+\\frac{p% } { 2n_{1}^{2}}. \\end{aligned } \\label{d0f}\\]]the corresponding approximation for @xmath181 $ ] is @xmath182\\,\\eqsim \\phi \\left (   \\frac{g_{1}^{b , r}-c}{\\sqrt{d_{1}^{b , r}}}\\right ) , \\end{aligned}%\\]]where @xmath183 and @xmath184 are obtained by exchanging @xmath49 and @xmath36 , @xmath166 and @xmath167 , @xmath168 and @xmath169 , and @xmath170 and @xmath171 in @xmath185 and @xmath186 , respectively .",
    "it is straightforward to see that @xmath187with @xmath154 being defined in theorem 1 .",
    "therefore , the approximation obtained in ( [ condraudys ] ) is asymptotically exact and ( eq - tdee0s2du ) and ( [ condraudys ] ) are asymptotically equivalent .",
    "we consider the unconditional expectation of @xmath90 under bayesian - kolmogorov asymptotics .",
    "the proof of the following theorem is presented in the appendix .",
    "[ thm - m2 ] consider the sequence of gaussian discrimination problems defined by ( [ gkusgukll ] ) .",
    "then @xmath189=\\phi \\left ( ( -1)^{i}\\;\\frac{-h_{i}+c}{\\sqrt{f}}% \\right ) , \\end{aligned } \\label{eq - t1}\\]]so that @xmath190\\,= { \\alpha}_0\\phi \\left ( \\frac{-h_{0}+c}{\\sqrt{f% } } \\right ) + { \\alpha}_1\\phi \\left ( \\frac{h_{1}-c}{\\sqrt{f}}\\right ) , \\label{csxasas}\\]]where @xmath191 & h_{1}\\,=\\,-\\frac{1}{2}\\left(\\overline{\\delta}^{2}_{\\mathbf{m}}+j_0-j_1+% \\frac{j_0}{\\gamma_0}+\\frac{j_1}{\\gamma_1}\\right ) , \\\\[-1ex ] & f\\,=\\,\\overline{\\delta}^{2}_{\\mathbf{m}}+j _ { 0}+j_{1}+\\frac{j_0}{\\gamma_0}+% \\frac{j_1}{\\gamma_1}\\ , .",
    "\\quad \\blacksquare \\end{aligned } \\label{njzzp}\\ ] ]    theorem [ thm - m2 ] suggests the finite - sample approximation @xmath192 \\eqsim \\phi \\left ( \\frac{-h_{0}^{r}+c}{\\sqrt{\\delta _ { \\mathbf{m}}^{2}+\\frac{p}{n_{0}% } + \\frac{p}{n_{1}}+\\frac{p}{\\nu _ { 0}}+\\frac{p}{\\nu _ { 1}}}}\\right ) , \\end{aligned } \\label{uncondkol}\\]]where @xmath193    from ( [ rewritegaussian ] ) we can get the raudys - type approximation : @xmath194 & = e_{\\boldsymbol{\\mu } % } \\left [ p(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\leq c\\,|\\mathbf{z}\\in \\psi _ { 0},\\boldsymbol{\\mu } ) \\right ] \\eqsim \\phi \\left (   \\frac{-e_{\\boldsymbol{\\mu } , s_{n},\\mathbf{z}}[u_{0}(\\bar{\\mathbf{x}}_{0},% \\bar{\\mathbf{x}}_{1},\\mathbf{z})|\\mathbf{z}\\in \\psi _ { 0})]+c}{\\sqrt{\\text{var% } _ { \\boldsymbol{\\mu } , s_{n},\\mathbf{z}}[u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{% \\mathbf{x}}_{1},\\mathbf{z})|\\mathbf{z}\\in \\psi _ { 0}]}}\\right ) .",
    "\\end{aligned}%\\]]some algebraic manipulations yield ( suppl .",
    "section b ) @xmath194\\,\\eqsim \\phi \\left (   \\frac{-h_{0}^{r}+c}{\\sqrt{f_{0}^{r}}}\\right ) , \\end{aligned } \\label{uncondraudys}\\]]where @xmath195it is straightforward to see that @xmath196with @xmath197 defined in theorem 2 .",
    "hence , the approximation obtained in ( uncondraudys ) is asymptotically exact and both ( [ uncondkol ] ) and ( uncondraudys ) are asymptotically equivalent .",
    "here we employ the bayesian - kolmogorov asymptotic analysis to characterize the second and cross moments with the actual error , and therefore the mse of error estimation .      defining two i.i.d .",
    "random vectors , @xmath175 and @xmath200 , yields the second moment representation @xmath201=e_{s_{n}}[p(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}% ) \\leq c\\,|\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}\\in \\psi _ { 0},% \\boldsymbol{\\mu } )",
    "^{2 } ] \\\\ & = e_{s_{n}}\\bigg[p(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z% } ) \\leq c\\,|\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}\\in \\psi _ { 0},% \\boldsymbol{\\mu } ) p(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z% } ^{\\prime } ) \\leq c\\,|\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}% ^{\\prime } \\in \\psi _ { 0},\\boldsymbol{\\mu } ) \\bigg ] \\\\ & = e_{s_{n}}\\bigg[p\\big(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},% \\mathbf{z})\\leq c\\,,u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z% } ^{\\prime } ) \\leq c\\,|\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}\\in \\psi _",
    "{ 0},\\mathbf{z}^{\\prime } \\in \\psi _ { 0},\\boldsymbol{\\mu } \\big)\\bigg ] \\\\ & = p\\big(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\leq c\\,,u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}^{\\prime } ) \\leq c\\,|\\mathbf{z}\\in \\psi _",
    "{ 0},\\mathbf{z}^{\\prime } \\in \\psi _ { 0},% \\boldsymbol{\\mu } \\big ) , \\end{aligned } \\label{kjxbkasu}\\]]where @xmath175 and @xmath200 are independent of @xmath12 , and @xmath176 is a multivariate gaussian , @xmath202 , and @xmath203 being defined in ( [ ui ] ) .",
    "we have the following theorem , with the proof presented in the appendix .",
    "[ thm - m4 ] for the sequence of gaussian discrimination problems in ( gkusguk ) and for @xmath111 , @xmath204=\\phi \\left((-1)^{i}\\ ; \\frac{% -g_{i}^{b}+c}{\\sqrt{d}}\\right ) \\phi \\left ( ( -1)^{j}\\;\\frac{-g_{j}^{b}+c}{% \\sqrt{d}}\\right ) , \\end{aligned } \\label{eq - t1}\\]]so that @xmath205=\\left",
    "[ { \\alpha}_{0}\\phi \\left ( \\frac{-g_{0}^{b}+c}{\\sqrt{d% } } \\right ) + { \\alpha}_{1}\\phi \\left ( \\frac{g_{1}^{b}-c}{\\sqrt{d}}\\right ) % \\right ] ^{2 } ,   \\label{csxasas}\\]]where @xmath154 , @xmath155 , and @xmath156 are defined in ( [ njzz ] ) .",
    "@xmath206    this theorem suggests the finite - sample approximation @xmath207\\,\\eqsim \\left [ \\phi \\left ( \\frac{-g_{0}^{b , f}+c}{\\sqrt{\\delta _ { \\boldsymbol{\\mu } } ^{2}+% \\frac{p}{n_{0}}+\\frac{p}{n_{1}}}}\\right ) \\right ] ^{2}\\ , , \\end{aligned } \\label{eq - tdee0s2dgg}\\]]which is the square of the approximation ( [ eq - tdee0s2du ] ) . corresponding approximations for @xmath208 $ ] and @xmath209 $ ] are obtained similarly .",
    "similar to the proof of theorem [ thm - m4 ] , we obtain the conditional cross moment of @xmath14 .",
    "[ thm - m5 ] consider the sequence of gaussian discrimination problems in ( [ gkusguk ] ) .",
    "then for @xmath111 , @xmath210=\\phi \\left ( ( -1)^{i}\\;\\frac{% -g_{i}^{b}+c}{\\sqrt{d}}\\right ) \\phi \\left ( ( -1)^{j}\\;\\frac{-g_{j}+c}{\\sqrt{d}% } \\right ) , \\end{aligned } \\label{eq - t1bb}\\]]so that @xmath211=\\sum_{i=0}^{1}\\sum_{j=0}^{1}\\bigg[{% \\alpha } _ { i}{\\alpha } _",
    "{ j}\\phi \\left ( ( -1)^{i}\\;\\frac{-g_{i}^{b}+c}{\\sqrt{d}}% \\right ) \\phi \\left ( ( -1)^{j}\\;\\frac{-g_{j}+c}{\\sqrt{d}}\\right ) \\bigg ] , \\end{aligned } \\label{csxasasbb}\\]]where @xmath212 and @xmath156 are defined in ( [ njzz ] ) and @xmath213 is defined in ( [ njzzpp ] ) .",
    "@xmath206    this theorem suggests the finite - sample approximation @xmath214\\eqsim \\phi \\left ( \\frac{-g_{0}^{b , f}+c}{\\sqrt{\\delta _ { \\boldsymbol{\\mu } } ^{2}+% \\frac{p}{n_{0}}+\\frac{p}{n_{1}}}}\\right ) \\phi \\left ( -\\frac{1}{2}\\frac{% \\delta _ { \\boldsymbol{\\mu } } ^{2}+\\frac{p}{n_{1}}-\\frac{p}{n_{0}}-c}{\\sqrt{% \\delta _ { \\boldsymbol{\\mu } } ^{2}+\\frac{p}{n_{0}}+\\frac{p}{n_{1}}}}\\right ) . \\end{aligned } \\label{bxjbxk}\\]]this is a product of ( [ eq - tdee0s2du ] ) and the finite - sample approximation for @xmath215 $ ]  in zollanvari .",
    "a consequence of theorems  [ thm - m1 ] ,  [ thm - m4 ] , and [ thm - m5 ] is that all the conditional variances and covariances are asymptotically zero : @xmath216hence , the deviation variance is also asymptotically zero , @xmath217 @xmath218=0 $ ] .",
    "hence , defining the conditional bias as    @xmath219=e_{s_{n}}[\\hat{\\varepsilon}% ^{b}-\\varepsilon |\\boldsymbol{\\mu } ] , \\end{aligned}%\\ ] ]    the asymptotic rms reduces to@xmath220=\\lim_{\\substack { \\text{b.k.a.c . }   \\\\ } } |\\text{bias}% _ { c , n}[\\hat{\\varepsilon}^{b}]| . \\end{aligned } \\label{mshhxa}\\]]to express the conditional bias , as proven in @xcite , @xmath221\\,=\\alpha _ { 0}\\phi \\left ( \\frac{-g_{0}+c}{\\sqrt{d}}\\right ) + \\alpha _ { 1}\\phi \\left ( \\frac{g_{1}-c}{\\sqrt{d}}\\right ) ,   \\label{csxasaspp}\\]]where    @xmath222 & g_{1}\\,=\\,-\\frac{1}{2}(\\delta _ { \\boldsymbol{\\mu } } ^{2}+j_{0}-j_{1 } ) , \\\\% [ -1ex ] & d\\,=\\,\\delta _ { \\boldsymbol{\\mu } } ^{2}+j_{0}+j_{1}\\ , . \\end{aligned } \\label{njzzpp}\\ ] ]    it follows from theorem 1 and ( [ csxasaspp ] ) that    @xmath223\\!=\\ ! \\alpha_{0}\\!\\left [ \\phi \\left ( \\frac{-g_{0}^{b}+c}{\\sqrt{d}}% \\right ) \\!-\\!\\phi \\left ( \\frac{-g_{0}+c}{\\sqrt{d}}\\right ) \\right ] \\ ! + \\",
    "! \\alpha _ { 1}\\!\\left [ \\phi \\left ( \\frac{g_{1}^{b}-c}{\\sqrt{d}}\\right ) \\!-\\!\\phi \\left ( \\frac{g_{1}-c}{\\sqrt{d}}\\right )",
    "\\right ] . \\end{aligned } \\label{condbiasb}\\ ] ]    recall that the mmse error estimator is unconditionally unbiased : @xmath224\\,=\\,e_{\\boldsymbol{\\mu } , s_{n}}\\left [ \\hat{% \\varepsilon}^{b}-\\varepsilon \\right ] = 0 $ ] .",
    "we next obtain raudys - type approximations corresponding to theorems 3 and 4 by utilizing the joint distribution of @xmath225 and @xmath226 , defined in ( [ ui ] ) , with @xmath175 and @xmath200 being independently selected from populations @xmath227 or @xmath228 .",
    "we employ the function @xmath229which is the distribution function of a joint bivariate gaussian vector with zero means , unit variances , and correlation coefficient @xmath230 .",
    "note that @xmath231 and @xmath232 .",
    "for simplicity of notation , we write @xmath233 as @xmath234 .",
    "the rectangular - area probabilities involving any jointly gaussian pair of variables @xmath235 can be expressed as @xmath236with @xmath237 $ ] , @xmath238 $ ] , @xmath239 , @xmath240 , and correlation coefficient @xmath241 .    using ( [ kjxbkasu ] ) ,",
    "we obtain the second - order extension of ( kshbxkabk ) by @xmath242=p\\big(u_{0}(% \\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\leq c\\,,u_{0}(\\bar{% \\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}^{\\prime } ) \\leq c\\,|\\mathbf{z}% \\in \\psi _ { 0},\\mathbf{z}^{\\prime } \\in \\psi _ { 0},\\boldsymbol{\\mu } \\big ) \\\\",
    "& \\eqsim \\phi \\bigg(\\frac{-e_{s_{n},\\mathbf{z}}[u_{0}(\\bar{\\mathbf{x}}_{0},% \\bar{\\mathbf{x}}_{1},\\mathbf{z})\\mid \\mathbf{z}\\in \\psi _ { 0},\\boldsymbol{\\mu   } ] + c}{\\sqrt{\\text{var}_{s_{n},\\mathbf{z}}[u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{% \\mathbf{x}}_{1},\\mathbf{z})\\mid \\mathbf{z}\\in \\psi _ { 0},\\boldsymbol{\\mu } ] } } ; % \\frac{\\text{cov}_{s_{n},\\mathbf{z}}[u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x% } } _ { 1},\\mathbf{z}),u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}% ^{\\prime } ) |\\mathbf{z}\\in \\psi _",
    "{ 0},\\mathbf{z}^{\\prime } \\in \\psi _ { 0},% \\boldsymbol{\\mu } ] } { \\text{var}_{s_{n},\\mathbf{z}}[u_{0}(\\bar{\\mathbf{x}}_{0},% \\bar{\\mathbf{x}}_{1},\\mathbf{z})|\\mathbf{z}\\in \\psi _ { 0},\\boldsymbol{\\mu } ] } % \\bigg ) .",
    "\\end{aligned } \\label{eq-2dam112}\\]]using ( [ eq-2dam112 ] ) ,",
    "some algebraic manipulations yield @xmath207\\,\\eqsim \\,\\phi \\left ( \\frac{-g_{0}^{b , r}+c}{\\sqrt{d_{0}^{b , r}}};\\frac{c_{0}^{b , r}}{% d_{0}^{b , r}}\\right ) , \\end{aligned } \\label{eq-2dam1123ss}\\]]with @xmath243 and @xmath185 being presented in ( [ bkshbaa ] ) and ( [ d0f ] ) , respectively , and @xmath244 \\\\ & = \\frac{\\beta _ { 0}}{(1+\\beta _ { 0})^{2}}\\bigg[\\frac{\\eta _ { \\mathbf{m}_{0},% \\boldsymbol{\\mu } _ { 1}}-(1-\\beta _ { 0})\\eta _ { \\mathbf{m}_{0},\\boldsymbol{\\mu } % _ { 0}}-\\delta _ { \\boldsymbol{\\mu } } ^{2}}{n_{0}}+\\frac{(1+\\beta _ { 0})\\eta _ { % \\mathbf{m}_{0},\\boldsymbol{\\mu } _ { 1}}-\\eta _ { \\mathbf{m}_{0},\\boldsymbol{\\mu } % _ { 0}}}{n_{1}}\\bigg]+\\frac{(1-\\beta _ { 0})^{2}p}{2n_{0}^{2}(1+\\beta _ { 0})^{2 } } \\\\ & + \\frac{p}{n_{0}n_{1}(1+\\beta _ { 0})^{2}}+\\frac{p}{2n_{1}^{2}}+\\frac{\\delta _ { \\boldsymbol{\\mu } } ^{2}}{n_{1}(1+\\beta _ { 0})}+\\frac{\\delta _ { \\boldsymbol{% \\mu } } ^{2}}{n_{0}(1+\\beta _ { 0})^{2 } } , \\end{aligned } \\label{c0f}\\]]the proof of ( [ c0f ] ) follows by expanding @xmath245 and @xmath246 from ( [ ui ] ) and then using the set of identities in the proof of ( [ uncondraudys ] ) , i.e. equation ( s.1 ) from suppl .",
    "section b. similarly , @xmath247=p(u_{1}(\\bar{% \\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})>c\\,,u_{1}(\\bar{\\mathbf{x}}% _ { 0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}^{\\prime } ) > c\\,|\\mathbf{z}\\in \\psi _ { 1},% \\mathbf{z}^{\\prime } \\in \\psi _ { 1},\\boldsymbol{\\mu } ) \\eqsim \\phi \\left ( \\frac{% g_{1}^{b , r}-c}{\\sqrt{d_{1}^{b , r}}};\\frac{c_{1}^{b , r}}{d_{1}^{b , r}}\\right ) , \\end{aligned } \\label{eq-2dam1123s}\\]]where @xmath183 , @xmath184 , and @xmath248 are obtained by exchanging @xmath49 and @xmath36 , @xmath166 and @xmath167 , @xmath168 and @xmath169 , and @xmath170 and @xmath171 , in ( [ d0f ] ) , in @xmath172 obtained from ( [ g0f ] ) , and in ( c0f ) , respectively .",
    "having @xmath249 together with ( bcjlak ) shows that ( [ eq-2dam1123ss ] ) is asymptotically exact , that is , asymptotically equivalent to @xmath250 $ ] obtained in theorem 3 .",
    "similarly , it can be shown that @xmath251=p(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\leq c\\,,-u_{1}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}^{\\prime } ) <",
    "-c\\,|\\mathbf{z}\\in \\psi _ { 0},\\mathbf{z}^{\\prime } \\in \\psi _ { 1},% \\boldsymbol{\\mu } ) \\\\ & \\eqsim \\phi \\left ( \\frac{-g_{0}^{b , r}+c}{\\sqrt{d_{0}^{b , r}}},\\frac{% g_{1}^{b , r}-c}{\\sqrt{d_{1}^{b , r}}};\\frac{c_{01}^{b , r}}{\\sqrt{% d_{0}^{b , r}d_{1}^{b , r}}}\\right ) , \\end{aligned } \\label{eq-2dam112u}\\]]where , after some algebraic manipulations we obtain @xmath252 \\\\ & + \\frac{1}{n_{1}(1+\\beta _ { 0})(1+\\beta _ { 1})}\\bigg[\\beta _ { 1}\\eta _ { \\mathbf{% m}_{1},\\boldsymbol{\\mu } _ { 1},\\boldsymbol{\\mu } _ { 1},\\boldsymbol{\\mu } % _ { 0}}-\\beta _ { 0}\\beta _ { 1}\\eta _ { \\mathbf{m}_{1},\\boldsymbol{\\mu } _ { 1},% \\mathbf{m}_{0},\\boldsymbol{\\mu } _ { 1}}+\\beta _ { 0}\\eta _ { \\mathbf{m}_{0},% \\boldsymbol{\\mu } _ { 0},\\boldsymbol{\\mu } _ { 0},\\boldsymbol{\\mu } _ { 1}}+\\beta _ { 0}\\delta _ { \\boldsymbol{\\mu } } ^{2}+\\delta _ { \\boldsymbol{\\mu } } ^{2}\\bigg ] \\\\ & + \\frac{p}{n_{0}n_{1}(1+\\beta _ { 0})(1+\\beta _ { 1})}+\\frac{(1-\\beta _ { 0})p}{% 2n_{0}^{2}(1+\\beta _ { 0})}+\\frac{(1-\\beta _ { 1})p}{2n_{1}^{2}(1+\\beta _ { 1})}. \\end{aligned } \\label{c01f}\\]]suppl .",
    "section c gives the proof of ( [ c01f ] ) .",
    "since @xmath253 , ( [ eq-2dam112u ] ) is asymptotically exact , i.e. ( [ eq-2dam112u ] ) becomes equivalent to the result of theorem 3 .",
    "we obtain the conditional cross moment similarly : @xmath254=p(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\leq c\\,,w(% \\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{x})\\leq c\\,|\\mathbf{z}\\in \\psi _ { 0},\\mathbf{x}\\in \\pi _ { 0},\\boldsymbol{\\mu } ) \\\\ & \\eqsim \\phi \\bigg(\\frac{-e_{s_{n},\\mathbf{z}}[u_{0}(\\bar{\\mathbf{x}}_{0},% \\bar{\\mathbf{x}}_{1},\\mathbf{z})|\\mathbf{z}\\in \\psi _ { 0},\\boldsymbol{\\mu } ] + c% } { \\sqrt{\\text{var}_{s_{n},\\mathbf{z}}[u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{% \\mathbf{x}}_{1},\\mathbf{z})|\\mathbf{z}\\in \\psi _ { 0},\\boldsymbol{\\mu } ] } } , % \\frac{-e_{s_{n},\\mathbf{x}}[w(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},% \\mathbf{x})|\\mathbf{x}\\in \\pi _ { 0},\\boldsymbol{\\mu } ] + c}{\\sqrt{\\text{var}% _ { s_{n},\\mathbf{x}}[w(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{x})|% \\mathbf{x}\\in \\pi _ { 0},\\boldsymbol{\\mu } ] } } ; \\\\ &",
    "\\frac{\\text{cov}_{s_{n},\\mathbf{z},\\mathbf{x}}[u_{0}(\\bar{\\mathbf{x}}_{0},% \\bar{\\mathbf{x}}_{1},\\mathbf{z}),w(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},% \\mathbf{x})|\\mathbf{z}\\in \\psi _ { 0},\\mathbf{x}\\in \\pi _ { 0},\\boldsymbol{\\mu } ] % } { \\sqrt{v_{u_{0}}^{\\text{c}}v_{w}^{\\text{c}}}}\\bigg ) , \\end{aligned } \\label{eq-2dam112tr}\\]]where @xmath255 , \\\\ &",
    "v_{w}^{\\text{c}}=\\text{var}_{s_{n},\\mathbf{x}}[w(\\bar{\\mathbf{x}}_{0},\\bar{% \\mathbf{x}}_{1},\\mathbf{x})|\\mathbf{x}\\in \\pi _ { 0},\\boldsymbol{\\mu } ] , \\end{aligned}%\\]]where superscript ",
    "@xmath256 denotes conditional variance .",
    "algebraic manipulations like those leading to ( [ c0f ] ) yield @xmath214\\eqsim \\phi \\left ( \\frac{-g_{0}^{b , r}+c}{\\sqrt{d_{0}^{b , r}}},\\frac{-g_{0}^{r}+c}{% \\sqrt{d_{0}^{r}}};\\frac{c_{0}^{bt , r}}{\\sqrt{d_{0}^{b , r}d_{0}^{r}}}\\right ) , \\end{aligned } \\label{eq-2dam112gb}\\]]where @xmath257% -\\frac{(1-\\beta _ { 0})p}{2n_{0}^{2}(1+\\beta _ { 0})}+\\frac{p}{2n_{1}^{2 } } , \\end{aligned}%\\]]and @xmath258 and @xmath259 having been obtained previously in equations ( 49 ) and ( 50 ) of @xcite , namely , @xmath260=\\frac{1}{2}\\left ( \\delta _ { \\boldsymbol{\\mu } } ^{2}+\\frac{p}{n_{1}}-\\frac{p}{n_{0}}\\right ) , \\\\ & d_{0}^{r}=\\text{var}_{s_{n},\\mathbf{z}}[w(\\bar{\\mathbf{x}}_{0},\\bar{% \\mathbf{x}}_{1},\\mathbf{x})\\mid \\mathbf{x}\\in \\pi _ { 0},\\boldsymbol{\\mu } % ] = \\delta _ { \\boldsymbol{\\mu } } ^{2}+\\frac{\\delta _ { \\boldsymbol{\\mu } } ^{2}}{% n_{1}}+p\\left ( \\frac{1}{n_{0}}+\\frac{1}{n_{1}}+\\frac{1}{2n_{0}^{2}}+\\frac{1}{% 2n_{1}^{2}}\\right ) . \\end{aligned } \\label{bjsbkjas}\\]]similarly , we can show that @xmath261\\eqsim \\phi \\left ( \\frac{g_{1}^{b , r}-c}{\\sqrt{d_{1}^{b , r}}},\\frac{g_{1}^{r}-c}{% \\sqrt{d_{1}^{r}}};\\frac{c_{1}^{bt , r}}{\\sqrt{d_{1}^{b , r}d_{1}^{r}}}\\right ) , \\end{aligned } \\label{eq-2dam112x}\\]]where @xmath183 and @xmath184 are obtained as in ( [ eq-2dam1123s ] ) , and @xmath262 , @xmath263 , and @xmath264 are obtained by exchanging @xmath49 and @xmath36 in @xmath259 , @xmath265 , and @xmath266 , respectively .",
    "similarly , @xmath267\\eqsim",
    "\\phi \\left ( \\frac{-g_{0}^{b , r}+c}{\\sqrt{d_{0}^{b , r}}},\\frac{g_{1}^{r}-c}{% \\sqrt{d_{1}^{r}}};\\frac{c_{01}^{bt , r}}{\\sqrt{d_{0}^{b , r}d_{1}^{r}}}\\right ) , \\end{aligned } \\label{eq-2dam112gcb}\\]]where @xmath268+\\frac{(1-\\beta _ { 0})p}{2n_{0}^{2}(1+\\beta _ { 0})}-\\frac{p}{2n_{1}^{2 } } , \\end{aligned}%\\]]and @xmath269\\eqsim \\phi \\left ( \\frac{g_{1}^{b , r}-c}{\\sqrt{d_{1}^{b , r}}},\\frac{-g_{0}^{r}+c}{% \\sqrt{d_{0}^{r}}};\\frac{c_{10}^{bt , r}}{\\sqrt{d_{1}^{b , r}d_{0}^{r}}}\\right ) , \\end{aligned}%\\]]where @xmath270 is obtained by exchanging @xmath49 and @xmath36 , @xmath271 and @xmath167 , @xmath168 and @xmath169 , and @xmath272 and @xmath171 in @xmath273 .",
    "we see that @xmath274 , @xmath275 , and @xmath276 .",
    "therefore , from ( [ bcjlak ] ) and the fact that @xmath277 and @xmath278 , we see that expressions ( eq-2dam112 gb ) , ( [ eq-2dam112x ] ) , and ( [ eq-2dam112gcb ] ) , are all asymptotically exact ( compare to theorem 4 ) .      similarly to the way ( [ kjxbkasu ] ) was obtained , we can show that @xmath280 = e_{% \\boldsymbol{\\mu},{s_{n}}}\\big[p(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}% _ { 1},\\mathbf{z})\\leq c\\,|\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}% \\in \\psi _ { 0},\\boldsymbol{\\mu})^{2}\\big ] \\\\ & = e_{\\boldsymbol{\\mu},{s_{n}}}\\bigg[p\\big(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{% \\mathbf{x}}_{1},\\mathbf{z})\\leq c\\,,u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x% } } _ { 1},\\mathbf{z}^{\\prime } ) \\leq c\\,|\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}% _ { 1 } , \\mathbf{z}\\in \\psi _ { 0},\\mathbf{z}^{\\prime }",
    "\\in \\psi _ { 0},\\boldsymbol{% \\mu}\\big)\\bigg ] \\\\",
    "& = p\\big(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\leq c\\,,u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}^{\\prime } ) \\leq c\\,|\\mathbf{z}\\in \\psi _ { 0},\\mathbf{z}^{\\prime } \\in \\psi _ { 0}\\big ) .",
    "\\end{aligned}%\\ ] ]    similarly to the proofs of theorem [ thm - m4 ] and [ thm - m5 ] , we get the following theorems .",
    "[ thm - m6 ] consider the sequence of gaussian discrimination problems in ( [ gkusgukll ] ) .",
    "for @xmath111 , @xmath281=\\phi \\left ( ( -1)^{i}\\,\\frac{% -h_{i}+c}{\\sqrt{f}}\\right ) \\phi \\left ( ( -1)^{j}\\,\\frac{-h_{j}+c}{\\sqrt{f}}% \\right ) , \\end{aligned } \\label{eq - t1gg}\\]]so that @xmath282\\,=\\left [ { \\alpha}_{0}\\phi \\left ( \\frac{-h_{0}+c}{% \\sqrt{f}}\\right ) + { \\alpha}_{1}\\phi \\left ( \\frac{h_{1}-c}{\\sqrt{f}}\\right ) % \\right ] ^{2 } ,   \\label{csxasasgg}\\]]where @xmath197 , @xmath283 , and @xmath9 are defined in ( [ njzzp ] ) .",
    "@xmath206    [ thm - m7 ] consider the sequence of gaussian discrimination problems in ( [ gkusgukll ] ) .",
    "for @xmath111 , @xmath284=\\lim_{\\substack { \\text{b.k.a.c . }",
    "\\\\ } } % e_{\\boldsymbol{\\mu } , s_{n}}[\\hat{\\varepsilon}_{i}^{b}\\hat{\\varepsilon}% _ { j}^{b}]=\\lim_{\\substack { \\text{b.k.a.c . }   \\\\ } } e_{\\boldsymbol{\\mu } % , s_{n}}[\\varepsilon _ { i}\\varepsilon _ { j } ] , \\end{aligned } \\label{eq - t1bbhh}\\]]so that @xmath285=\\sum_{i=0}\\sum_{j=0}\\bigg[{\\alpha } _ { i}{\\alpha}% _ { j}\\phi \\left ( ( -1)^{i}\\,\\frac{h_{i}+c}{\\sqrt{f}}\\right ) \\phi \\left ( ( -1)^{j}\\,\\frac{h_{j}+c}{\\sqrt{f}}\\right ) \\bigg ] , \\end{aligned } \\label{csxasasbbhh}\\]]where @xmath197 , @xmath283 , and @xmath9 are defined in ( [ njzzp ] ) .",
    "@xmath206    theorems [ thm - m6 ] and [ thm - m7 ] suggest the finite - sample approximation : @xmath286\\eqsim e_{\\boldsymbol{\\mu } , s_{n}}[\\hat{\\varepsilon}% _ { 0}^{b}\\varepsilon _ { 0}]\\eqsim e_{\\boldsymbol{\\mu } , s_{n}}[\\varepsilon _ { 0}\\varepsilon _ { 0}]\\eqsim \\left [ \\phi \\left ( -\\frac{1}{2}\\frac { \\delta _ { % \\mathbf{m}}^{2}+\\frac{p}{n_{1}}-\\frac{p}{n_{0}}+\\frac{p}{\\nu _ { 0}}+\\frac{p}{% \\nu _ { 1}}-c}{\\sqrt{\\delta _ { \\mathbf{m}}^{2}+\\frac{p}{n_{0}}+\\frac{p}{n_{1}}+% \\frac{p}{\\nu _ { 0}}+\\frac{p}{\\nu _ { 1}}}}\\right ) \\right ] ^{2}. \\end{aligned } \\label{ksksksas}\\ ] ]",
    "a consequence of theorems  [ thm - m2 ] ,  [ thm - m6 ] , and [ thm - m7 ] is that @xmath287=\\lim_{\\substack { \\text{b.k.a.c . }   \\\\ } } |% \\text{bias}_{u , n}[{\\hat{\\varepsilon}}^{b}]|=\\lim_{\\substack { \\text{b.k.a.c . }   \\\\ } } \\text{var}_{\\boldsymbol{\\mu } , s_{n}}(\\hat{\\varepsilon}^{b})=\\lim   _ { \\substack { \\text{b.k.a.c . }   \\\\ } } \\text{var}_{\\boldsymbol{\\mu } , s_{n}}({% \\varepsilon } ) \\\\ & = \\lim_{\\substack { \\text{b.k.a.c . }   \\\\ } } \\text{cov}_{\\boldsymbol{\\mu } % , s_{n}}(\\varepsilon , \\hat{\\varepsilon}^{b})=\\lim_{\\substack { \\text{b.k.a.c . }   \\\\ } } \\text{rms}_{\\boldsymbol{\\mu } , s_{n}}[\\hat{\\varepsilon}^{b}]=0 . \\end{aligned } \\label{unrms}\\ ] ]    in @xcite , it was shown that @xmath14 is strongly consistent , meaning that @xmath288 almost surely as @xmath18 under rather general conditions , in particular , for the gaussian and discrete models considered in that paper .",
    "it was also shown that @xmath289\\rightarrow 0 $ ] almost surely as @xmath290 under similar conditions . here",
    ", we have shown that @xmath291{\\overset{k}{% \\rightarrow } } 0 $ ] under conditions stated in ( [ kacl2u ] ) .",
    "some researchers refer to conditions of double asymptoticity as comparable \" dimensionality and sample size @xcite .",
    "therefore , one may think of @xmath103{% \\overset{k}{\\rightarrow } } 0 $ ] meaning that @xmath292 $ ] is close to zero for asymptotic and comparable dimensionality , sample size , and certainty parameter .",
    "we now consider raudys - type approximations .",
    "analogous to the approximation used in ( [ eq-2dam112 ] ) , we obtain the unconditional second moment of @xmath293 : @xmath294\\eqsim \\phi % \\bigg(\\frac{-e_{\\boldsymbol{\\mu } , s_{n},\\mathbf{z}}[u_{0}(\\bar{\\mathbf{x}}% _ { 0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\mid \\mathbf{z}\\in \\psi _ { 0}]+c}{\\sqrt{% \\text{var}_{\\boldsymbol{\\mu } , s_{n},\\mathbf{z}}[u_{0}(\\bar{\\mathbf{x}}_{0},% \\bar{\\mathbf{x}}_{1},\\mathbf{z})\\mid \\mathbf{z}\\in \\psi _ { 0 } ] } } ; \\\\ & \\frac{\\text{cov}_{\\boldsymbol{\\mu } , s_{n},\\mathbf{z}}[u_{0}(\\bar{\\mathbf{x}% } _ { 0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}),u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{% \\mathbf{x}}_{1},\\mathbf{z}^{\\prime } ) |\\mathbf{z}\\in \\psi _ { 0},\\mathbf{z}% ^{\\prime } \\in \\psi _ { 0})]}{\\text{var}_{\\boldsymbol{\\mu } , s_{n},\\mathbf{z}% } [ u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})|\\mathbf{z}\\in \\psi _ { 0}]}\\bigg ) .",
    "\\end{aligned } \\label{eq-2dam112bc}\\]]using ( [ eq-2dam112bc ] ) we get    @xmath295\\,=\\,\\phi \\left (   \\frac{-h_{0}^{r}+c}{\\sqrt{f_{0}^{r}}};\\frac{k_{0}^{b , r}}{f_{0}^{r}}\\right ) \\ ,",
    "\\end{aligned } \\label{eq-2dam1123sskk}\\ ] ]    with @xmath296 and @xmath297 given in ( [ jacdbdx ] ) and ( [ f0f ] ) , respectively , and @xmath298 \\\\",
    "& = \\left ( \\frac{1}{n_{0}(1+\\beta _ { 0})^{2}}+\\frac{1}{n_{1}}+\\frac{1}{\\nu _ { 0}(1+\\beta _ { 0})^{2}}+\\frac{1}{\\nu _ { 1}}\\right ) \\delta _ { \\mathbf{m}}^{2}+% \\frac{p}{2n_{0}^{2 } } \\\\ & + \\frac{p}{2\\nu _ { 0}^{2}}-\\frac{p}{n_{0}\\nu _ { 0}}+\\frac{p}{n_{1}\\nu _ { 1}}+% \\frac{p}{2n_{1}^{2}}+\\frac{p}{2\\nu _ { 1}^{2}}+\\frac{p}{n_{0}n_{1}(1+\\beta _ { 0})^{2}}+\\frac{p}{n_{0}\\nu _ { 1}(1+\\beta _ { 0})^{2}}+\\frac{p}{n_{1}\\nu _ { 0}(1+\\beta _ { 0})^{2 } } , \\end{aligned } \\label{k0f}\\]]suppl .",
    "section d presents the proof of ( [ k0f ] ) . in a similar way , @xmath299\\,=\\,\\phi \\left (   \\frac{h_{1}^{r}-c}{\\sqrt{f_{1}^{r}}};\\frac{k_{1}^{b , r}}{f_{1}^{r}}\\right ) , \\end{aligned } \\label{eq-2dam1123sshh}\\]]where @xmath300 , @xmath301 , and @xmath302 are obtained by exchanging @xmath49 and @xmath36 , @xmath166 and @xmath167 , @xmath168 and @xmath303 , and @xmath170 and @xmath171 , in ( [ f0f ] ) , in @xmath304 obtained from ( [ jacdbdx ] ) , and ( [ k0f ] ) , respectively .    having @xmath305 together with ( ncdlnc ) makes ( [ eq-2dam1123sskk ] ) asymptotically exact .",
    "we similarly obtain @xmath306=\\phi \\left ( \\frac{-h_{0}^{r}+c}{\\sqrt{f_{0}^{r}}},\\frac{h_{1}^{r}-c% } { \\sqrt{f_{1}^{r}}};\\frac{k_{01}^{b , r}}{\\sqrt{f_{0}^{r}f_{1}^{r}}}\\right ) , \\end{aligned } \\label{eq-2dam112oo}\\]]where @xmath307suppl .",
    "section e presents the proof of ( [ k01f ] ) . since @xmath308 , ( [ eq-2dam112oo ] ) is asymptotically exact ( compare to theorem 5 ) .",
    "similar to ( [ eq-2dam112tr ] ) and ( [ eq-2dam112 gb ] ) , where we characterized conditional cross moments , we can get the unconditional cross moments as follows : @xmath309\\,=p(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\leq c\\,,w(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{x})\\leq c\\,\\mid   \\mathbf{z}\\in \\psi _ { 0},\\mathbf{x}\\in \\pi _ { 0 } ) \\\\ & \\eqsim \\phi \\bigg(\\frac{-e_{\\boldsymbol{\\mu } , s_{n},\\mathbf{z}}[u_{0}(\\bar{% \\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\mid \\mathbf{z}\\in \\psi _ { 0}]+c}{\\sqrt{\\text{var}_{\\boldsymbol{\\mu } , s_{n},\\mathbf{z}}[u_{0}(\\bar{% \\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\mid \\mathbf{z}\\in \\psi _ { 0}]% } } , \\frac{-e_{\\boldsymbol{\\mu } , s_{n},\\mathbf{x}}[w(\\bar{\\mathbf{x}}_{0},\\bar{% \\mathbf{x}}_{1},\\mathbf{x})\\mid \\mathbf{x}\\in \\pi _ { 0}]+c}{\\sqrt{\\text{var}_{% \\boldsymbol{\\mu } , s_{n},\\mathbf{x}}[w(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}% _ { 1},\\mathbf{x})\\mid \\mathbf{x}\\in \\pi _ { 0 } ] } } ; \\\\ & \\frac{\\text{cov}_{\\boldsymbol{\\mu } , s_{n},\\mathbf{z},\\mathbf{x}}[u_{0}(% \\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}),w(\\bar{\\mathbf{x}}_{0},% \\bar{\\mathbf{x}}_{1},\\mathbf{x})|\\mathbf{z}\\in \\psi _ { 0},\\mathbf{x}\\in \\pi _ { 0}]}{\\sqrt{v_{u_{0}}^{u}v_{w}^{u}}}\\bigg)=\\phi \\left ( \\frac{-h_{0}^{r}+c}{% \\sqrt{f_{0}^{r}}};\\frac{k_{0}^{bt , r}}{f_{0}^{r}}\\right ) , \\end{aligned } \\label{eq-2dam112trss}\\]]where @xmath310 } , \\\\ & v_{w}^{\\text{u}}={\\text{var}_{\\boldsymbol{\\mu } , s_{n},\\mathbf{x}}[w(\\bar{% \\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{x})\\mid \\mathbf{x}\\in \\pi _ { 0}]}% , \\end{aligned}%\\]]the superscript  @xmath311 representing the unconditional variance , @xmath312 and @xmath297 being presented in ( [ jacdbdx ] ) and ( [ f0f ] ) , respectively , and @xmath313the proof of ( [ k0tf ] ) is presented in suppl .",
    "section f. similarly , @xmath314\\eqsim",
    "\\phi \\left ( \\frac{-h_{0}^{r}+c}{\\sqrt{f_{0}^{r}}},\\frac{h_{1}^{r}-c}{\\sqrt{% f_{1}^{r}}};\\frac{k_{01}^{bt , r}}{\\sqrt{f_{0}^{r}f_{1}^{r}}}\\right ) \\ , \\end{aligned } \\label{eq-2dam112trsso}\\]]where , @xmath315see suppl .",
    "section g for the proof of ( [ k01tf ] ) . having @xmath316 and @xmath317 along with ( [ ncdlnc ] ) makes ( [ eq-2dam112trss ] ) and ( eq-2dam112trsso ) asymptotically exact ( compare to theorem 6 ) .      to complete the derivations and obtain the unconditional @xmath319 of estimation ,",
    "we need the conditional and unconditional second moment of the true error . the conditional second moment of the true error can be found from results in @xcite , which for completeness are represented here : @xmath320\\eqsim",
    "\\phi \\left ( \\frac{% -g_{0}^{r}+c}{\\sqrt{d_{0}^{r}}};\\frac{c_{0}^{t , r}}{d_{0}^{r}}\\right ) , \\end{aligned } \\label{eq-2dam112trssotcon}\\]]with @xmath258 and @xmath259 defined in ( [ bjsbkjas ] ) , @xmath321\\eqsim \\phi \\left ( \\frac{% g_{1}^{r}-c}{\\sqrt{d_{1}^{r}}};\\frac{c_{1}^{t , r}}{d_{1}^{r}}\\right ) , \\end{aligned } \\label{eq-2dam112trssotconm}\\]]and @xmath322\\eqsim \\phi \\left ( \\frac{-g_{0}^{r}+c}{\\sqrt{d_{0}^{r}}},\\frac{g_{1}^{r}-c}{\\sqrt{% d_{1}^{r}}};\\frac{c_{01}^{t , r}}{\\sqrt{d_{0}^{r}d_{1}^{r}}}\\right ) , \\end{aligned } \\label{eq-2dam112trssotcona}\\]]where @xmath323similar to obtaining ( [ eq-2dam112trss ] ) , we can show that @xmath324\\eqsim \\phi \\left ( \\frac{% -h_{0}^{r}+c}{\\sqrt{f_{0}^{r}}};\\frac{k_{0}^{t , r}}{f_{0}^{r}}\\right ) , \\end{aligned } \\label{eq-2dam112trssot}\\]]with @xmath296 and @xmath297 given in ( [ jacdbdx ] ) and ( [ f0f ] ) , respectively , and @xmath325similarly , @xmath326\\,=\\,\\phi \\left ( \\frac{% h_{1}^{r}-c}{\\sqrt{f_{1}^{r}}};\\frac{k_{1}^{t , r}}{f_{1}^{r}}\\right ) , \\end{aligned } \\label{eq-2dam1123sshht}\\]]with @xmath327 obtained from @xmath328 by exchanging @xmath49 and @xmath329 , and @xmath166 and @xmath167 .",
    "similarly , @xmath324\\eqsim",
    "\\phi \\left ( \\frac{% -h_{0}^{r}+c}{\\sqrt{f_{0}^{r}}},\\frac{h_{1}^{r}-c}{\\sqrt{f_{1}^{r}}};\\frac{% k_{01}^{t , r}}{\\sqrt{f_{0}^{r}f_{1}^{r}}}\\right ) , \\end{aligned } \\label{eq-2dam112trssott}\\]]with @xmath296 and @xmath297 given in ( [ jacdbdx ] ) and ( [ f0f ] ) , respectively , and @xmath330",
    "in this section we compare the asymptotically exact finite - sample approximations of the first , second and mixed moments to monte carlo estimations in conditional and unconditional scenarios .",
    "the following steps are used to compute the monte carlo estimation :    * 1 .",
    "define a set of hyper - parameters for the gaussian model : @xmath331 , @xmath169 , @xmath166 , , @xmath167 , and @xmath86 .",
    "we let @xmath86 have diagonal elements 1 and off - diagonal elements 0.1 .",
    "@xmath168 and @xmath169 are chosen by fixing @xmath332 ( @xmath333 , which corresponds to bayes error @xmath334 ) . setting @xmath335 and @xmath86 fixes the means @xmath170 and @xmath336 of the class - conditional densities ( we assumed @xmath136 has equal elements and @xmath337 ) .",
    "the priors , @xmath338 and @xmath339 , are defined by choosing a small deviation from @xmath272 and @xmath171 , that is , by setting @xmath340 , where @xmath341 .",
    "2 .   ( unconditional case ) : using @xmath342 and @xmath339 , generate random realizations of @xmath343 and @xmath344 .",
    "3 .   ( conditional case ) : use the values of @xmath170 and @xmath336 obtained from step 1 .",
    "4 .   for fixed @xmath40 and @xmath41 ,",
    "generate a set of training data of size @xmath345 for class @xmath44 .",
    "5 .   using the training sample , design the lda classifier , @xmath61 , using ( [ ldac ] ) .",
    "compute the bayesian mmse error estimator , @xmath14 , using ( [ eq : beeu ] ) and ( [ qwsaqwsau ] ) .",
    "7 .   knowing @xmath343 and @xmath346 ,",
    "find the true error of @xmath61 using ( [ eq : true_erroru ] ) .",
    "repeat steps 3 through 6 , @xmath347 times . 9 .",
    "repeat steps 2 through 7 , @xmath348 times .",
    "in the unconditional case , we set @xmath349 and generate @xmath350 samples . for the conditional case , we set @xmath351 and @xmath352 , the latter because @xmath343 and @xmath353 are set in step 2 .",
    "figure 1 treats raudys - type finite - sample approximations , including the rms .",
    "figure 1(a )  compares the first moments obtained from equations ( [ condraudys ] ) and ( uncondraudys ) .",
    "it presents @xmath354 $ ] and @xmath355 $ ] computed by monte carlo estimation and the analytical expressions .",
    "the label fsa be uncond `` identifies the curve of @xmath356 $ ] , the unconditional expected estimated error obtained from the finite - sample approximation , which according to the basic theory is equal to @xmath357 $ ] .",
    "the labels fsa be cond '' and fsa te cond `` show the curves of @xmath358 $ ] , the conditional expected estimated error , and @xmath359 $ ] , the conditional expected true error , respectively , both obtained using the analytic approximations .",
    "the curves obtained from monte carlo estimation are identified by mc '' labels .",
    "the analytic curves in figure 1(a ) show substantial agreement with the monte carlo approximation .    to obtain the second moments , @xmath360 $ ] and @xmath361 $ ] as defined in ( [ eq - rms ] ) , we use equations ( [ eq-2dam1123ss ] ) , ( [ eq-2dam1123s ] ) , ( [ eq-2dam112u ] ) , ( eq-2dam112 gb ) , ( [ eq-2dam112gcb ] ) , ( [ eq-2dam112trssotcon ] ) , ( eq-2dam112trssotconm ) , ( [ eq-2dam112trssotcona ] ) for the conditional case and ( [ eq-2dam1123sskk ] ) , ( [ eq-2dam1123sshh ] ) , ( eq-2dam112oo ) , ( [ eq-2dam112trss ] ) , ( [ eq-2dam112trsso ] ) , ( eq-2dam112trssot ) , ( [ eq-2dam1123sshht ] ) , ( [ eq-2dam112trssott ] ) for the unconditional case . figures 1(b ) , 1(c ) , and 1(d ) compare the monte carlo estimation to the finite - sample approximations obtained for second / mixed moments , @xmath360 $ ] , and @xmath362 $ ] , respectively .",
    "the labels are interpreted similarly to those in figure 1(a ) , but for the second / mixed moments instead .",
    "for example , mc be@xmath363te uncond \" identifies the mc curve of @xmath364 $ ] . the figures 1(b ) , 1(c ) , and 1(d ) show that the finite - sample approximations for the conditional and unconditional second / mixed moments , variance of deviation , and rms are quite accurate ( close to the mc value ) .",
    "while figure 1 shows the accuracy of raudys - type of finite - sample approximations , figures in the supplementary materials show the the comparison between the finite - sample approximations obtained directly from theorem 1 - 6 , i.e. equations ( 29 ) , ( 57 ) , ( 70 ) , ( 73 ) , ( 76 ) , ( 102 ) , and ( 103 ) , to monte carlo estimation .",
    "equations ( [ g0f ] ) , ( [ d0f ] ) , ( [ c0f ] ) , ( [ c01f ] ) , and ( eq-2dam112gcb ) show that @xmath365 $ ] is a function of 14 variables : @xmath366 . studying a function of this number of variables",
    "is complicated , especially because restricting some variables can constrain others .",
    "we make several simplifying assumptions to reduce the complexity .",
    "we let @xmath367 , @xmath368 and assumed priors are centered at unknown true means , i.e. @xmath369 and @xmath370 .",
    "using these assumptions , @xmath371 $ ] is only a function of @xmath372 , and @xmath142 .",
    "we let @xmath373 $ ] , @xmath374 $ ] , @xmath375 , @xmath376 , which means that the bayes error is @xmath377 or @xmath378 .",
    "figure 2(a ) shows plots of @xmath379 $ ] as a function of @xmath5 , @xmath4 , @xmath380 , and @xmath142 .",
    "these show that for smaller distance between classes , that is , for smaller @xmath142 ( larger bayes error ) , the r@xmath381 is larger , and as the distance between classes increases , the @xmath319 decreases .",
    "furthermore , we see that in situations where very informative priors are available , i.e. @xmath382 and @xmath383 , relying more on data can have a detrimental effect on @xmath319 . indeed , the plots in the top row ( for @xmath384 ) have larger @xmath319 than the plots in the bottom row of the figure ( for @xmath385 ) .",
    "using the rms expressions enables finding the necessary sample size to insure a given @xmath386 $ ] by using the same methodology as developed for the resubstitution and leave - one - out error estimators in @xcite .",
    "the plots in figure 2(a ) ( as well as other unshown plots ) show that , with @xmath387 and @xmath388 , the @xmath389 is a decreasing function of @xmath142 .",
    "therefore , the number of sample points that guarantees @xmath390=\\lim_{\\delta _ { \\boldsymbol{\\mu } } ^{2}\\rightarrow 0}\\text{% rms}_{s_{n}}[\\hat{\\varepsilon}^{b}|\\boldsymbol{\\mu } ] $ ] being less than a predetermined value @xmath391 insures that @xmath392<\\tau , $ ] for any @xmath142 .",
    "let the desired bound be @xmath393 $ ] . from equations ( [ eq-2dam1123ss ] ) , ( [ eq-2dam1123s ] ) , ( [ eq-2dam112u ] ) , ( [ eq-2dam112 gb ] ) , ( eq-2dam112gcb ) , ( [ eq-2dam112trssotcon ] ) , ( [ eq-2dam112trssotconm ] ) , and ( [ eq-2dam112trssotcona ] ) , we can find @xmath394 and increase @xmath4 until @xmath395 .",
    "table 1 ( @xmath396 : conditional ) shows the minimum number of sample points needed to guarantee having a predetermined conditional @xmath397 for the whole range of @xmath142 ( other @xmath398 shown in the supplementary material ) .",
    "a larger dimensionality , a smaller @xmath399 , and a smaller @xmath380 result in a larger necessary sample size needed for having @xmath400 .    turning to the unconditional rms , equations ( [ f0f ] ) , ( [ k0f ] ) , ( k01f ) , ( [ k01tf ] ) , ( [ k01ttf ] ) , and ( [ k01ttft ] ) show that @xmath401 $ ] is a function of 6 variables : @xmath402 .",
    "figure 2(b ) shows plots of @xmath403 $ ] as a function of @xmath5 , @xmath4 , @xmath380 , and @xmath404 , assuming @xmath405 , @xmath406 .",
    "note that setting the values of @xmath4 and @xmath380 fixes the value of @xmath407 in the corresponding expressions for @xmath408 $ ] . due to the complex shape of @xmath409 $ ]",
    ", we consider a large range of @xmath4 and @xmath5 .",
    "the plots show that a smaller distance between prior distributions ( smaller @xmath410 ) corresponds to a larger unconditional @xmath319 of estimation .",
    "in addition , as the distance between classes increases , the @xmath319 decreases .",
    "the plots in figure 2(b ) show that , as @xmath411 increases , @xmath319 decreases .",
    "furthermore , figure 2(b ) ( and other unshown plots ) demonstrate an interesting phenomenon in the shape of the @xmath389 . in regions defined by pairs of @xmath412 , for each @xmath5",
    ", @xmath413 first increases as a function of sample size and then decreases .",
    "we further observe that with fixed @xmath5 , for smaller @xmath380 , this peaking phenomenon \" happens for larger @xmath4 . on the other hand , with fixed @xmath380 , for larger @xmath5",
    ", peaking happens for larger @xmath4 .",
    "these observations are presented in figure 3 , which shows curves obtained by cutting the 3d plots in the left column of fig .",
    "2(b ) at a few dimensions .",
    "this figure shows that , for @xmath414 and @xmath385 , adding more sample points increases @xmath319 abruptly at first to reach a maximum value of @xmath389 at @xmath415 , the point after which the @xmath319 starts to decrease .",
    "one may use the unconditional scenario to determine the the minimum necessary sample size for a desired @xmath416 $ ] .",
    "in fact , this is the more practical way to go because in practice one does not know @xmath157 .",
    "since the unconditional @xmath319 shows a decreasing trend in terms of @xmath404 , we use the previous technique to find the minimum necessary sample size to guarantee a desired unconditional @xmath319 .",
    "table 1 ( @xmath396 : unconditional ) shows the minimum sample size that guarantees @xmath417=\\lim_{\\delta _ { \\mathbf{m}}^{2}\\rightarrow 0}% \\text{rms}_{\\boldsymbol{\\mu } , s_{n}}[\\hat{\\varepsilon}^{b}]$ ] being less than a predetermined value @xmath391 , i.e. insures that @xmath418<\\tau $ ] for any @xmath411 ( other @xmath380 shown in the supplementary material ) .    to examine the accuracy of the required sample size that satisfies @xmath419 for both conditional and unconditional settings , we have performed a set of experiments ( see supplementary material ) .",
    "the results of these experiments confirm the efficacy of table 1 in determining the minimum sample size required to insure the rms is less than a predetermined value @xmath391 .",
    "$ ] -peaking phenomenon as a function of sample size .",
    "these plots are obtained by cutting the 3d plots in the left column of fig .",
    "2(b ) at few dimensionality ( i.e. @xmath420 ) . from top",
    "to bottom the rows correspond to @xmath421 , respectively .",
    "the solid - black curves indicate @xmath422 $ ] computed from the analytical results and the red - dashed curves show the same results computed by means of monte carlo simulations . due to computational burden of estimating the curves by means of monte carlo studies , the simulations are limited to @xmath423 and @xmath424 . ]",
    ".minimum sample size , @xmath4 , ( @xmath425 ) to satisfy @xmath426 .",
    "[ cols= \" > , > , > , > , > , > , > , > \" , ]     [ table : ex1 ]",
    "using realistic assumptions about sample size and dimensionality , standard statistical techniques are generally incapable of estimating the error of a classifier in small - sample classification .",
    "bayesian mmse error estimation facilitates more accurate estimation by incorporating prior knowledge . in this paper , we have characterized two sets of performance metrics for bayesian mmse error estimation in the case of lda in a gaussian model : ( 1 ) the first , second , and cross moments of the estimated and actual errors conditioned on a fixed feature - label distribution , which in turn gives us knowledge of the conditional rms@xmath427 $ ] ; and ( 2 ) the unconditional moments and , therefore , the unconditional rms , rms@xmath428 $ ] .",
    "we set up a series of conditions , called the bayesian - kolmogorov asymptotic conditions , that allow us to characterize the performance metrics of bayesian mmse error estimation in an asymptotic sense .",
    "the bayesian - kolmogorov asymptotic conditions are set up based on the assumption of increasing @xmath4 , @xmath5 , and certainty parameter @xmath429 , with an arbitrary constant limiting ratio between @xmath4 and @xmath5 , and @xmath4 and @xmath430 . to our knowledge , these conditions permit , for the first time , application of kolmogorov - type of asymptotics in a bayesian setting .",
    "the asymptotic expressions proposed in this paper result directly in finite - sample approximations of the performance metrics . improved finite - sample accuracy is achieved via newly proposed raudys - type approximations .",
    "the asymptotic theory is used to prove that these approximations are , in fact , asymptotically exact under the bayesian - kolmogorov asymptotic conditions . using the derived analytical expressions ,",
    "we have examined performance of the bayesian mmse error estimator in relation to feature - label distributions , prior knowledge , sample size , and dimensionality .",
    "we have used the results to determine the minimum sample size guaranteeing a desired level of error estimation accuracy .    as noted in the introduction ,",
    "a natural next step in error estimation theory is to remove the known - covariance condition , but as also noted , this may prove to be difficult .",
    "this work was partially supported by the nih grants 2r25ca090301 ( nutrition , biostatistics , and bioinformatics ) from the national cancer institute .",
    "we would like to thank dr .",
    "lori dalton for her critical review of the manuscript .",
    "we explain this proof in detail as some steps will be used in later proofs .",
    "let @xmath431where @xmath432 is defined in ( [ mnuu ] ) .",
    "then @xmath433for @xmath111 and @xmath434 , define the following random variables :    @xmath435    the variance of @xmath436 given @xmath157 does not depend on @xmath437 . therefore ,",
    "under the bayesian - kolmogorov conditions stated in ( [ kaclu ] ) , @xmath438 and @xmath439 do not appear in the limit .",
    "only @xmath440 matters , which vanishes in the limit as follows : @xmath441=\\mathbf{m}_{i}^{t}(\\frac{\\mathbf{% \\sigma } ^{-1}}{n_{0}}+\\frac{\\mathbf{\\sigma } ^{-1}}{n_{1}})\\mathbf{m}_{i}\\,{% \\overset{k}{\\rightarrow } } \\;\\lim_{\\substack { { n_{0}\\rightarrow \\infty } } } % \\frac{\\overline{\\mathbf{m}_{i}^{t}\\mathbf{\\sigma } ^{-1}\\mathbf{m}_{i}}}{n_{0}% } + \\lim_{\\substack { { n_{1}\\rightarrow \\infty } } } \\frac{\\overline{\\mathbf{m}% _ { i}^{t}\\mathbf{\\sigma } ^{-1}\\mathbf{m}_{i}}}{n_{1}}=0\\ , .",
    "\\end{aligned } \\label{vargj}\\ ] ]    to find the variance of @xmath442 and @xmath443 we can first transform @xmath442 and @xmath443 to quadratic forms and then use the results of @xcite to find the variance of quadratic functions of gaussian random variables .",
    "specifically , from @xcite , for @xmath444 and @xmath445 being a symmetric positive definite matrix , var@xmath446=2tr(\\mathbf{a}\\mathbf{% \\sigma } ) ^{2}+4\\boldsymbol{\\mu } ^{t}\\mathbf{a}\\mathbf{\\sigma } \\mathbf{a}% \\boldsymbol{\\mu } ^{\\prime } $ ] , with @xmath447 being the trace operator",
    ". therefore , after some algebraic manipulations , we obtain @xmath448=2\\frac{p}{n_{i}^{2}}+4\\frac{% \\boldsymbol{\\mu } _ { i}^{t}\\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { i}}{n_{i}}\\;% { \\overset{k}{\\rightarrow } } \\;2\\lim_{\\substack { { n_{i}\\rightarrow \\infty } } } % \\frac{j_{i}}{n_{i}}+4\\lim_{\\substack { { n_{i}\\rightarrow \\infty } } } \\frac{% \\overline{\\boldsymbol{\\mu } _ { i}^{t}\\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { i}% } } { n_{i}}=0\\ , , \\\\ & \\text{var}_{s_{n}}[z_{ij}|\\boldsymbol{\\mu } ] = \\frac{p}{n_{i}n_{j}}+\\frac{% \\boldsymbol{\\mu } _ { i}^{t}\\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { i}}{n_{j}}+% \\frac{\\boldsymbol{\\mu } _ { j}^{t}\\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { j}}{% n_{i}}\\;{\\overset{k}{\\rightarrow } } \\;\\lim_{\\substack { { n_{j}\\rightarrow \\infty } } } \\frac{j_{i}}{n_{j}}+\\lim_{\\substack { { n_{j}\\rightarrow \\infty } } } % \\frac{\\overline{\\boldsymbol{\\mu } _ { i}^{t}\\mathbf{\\sigma } ^{-1}\\boldsymbol{% \\mu } _ { i}}}{n_{j}}+\\lim_{\\substack { { n_{i}\\rightarrow \\infty } } } \\frac{% \\overline{\\boldsymbol{\\mu } _ { j}^{t}\\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { j}% } } { n_{i}}=0\\ , . \\end{aligned } \\label{varg}\\]]from the cauchy - schwarz inequality @xmath449\\!\\leq\\ ! \\sqrt{\\text{var}% [ x]\\text{var}[y]})$ ] , @xmath450{% \\overset{k}{\\rightarrow } } 0 $ ] , @xmath451{\\overset{k}{\\rightarrow } } 0 $ ] , and @xmath452\\;{\\overset{k}{\\rightarrow } } \\;0 $ ] for @xmath453 , @xmath454 , furthermore , @xmath455 and @xmath456 . putting this together and following the same approach for @xmath457 yields var@xmath458{\\overset{k}{% \\rightarrow } } 0 $ ] . in general ( via chebyshev s inequality ) , @xmath459var@xmath460=0 $ ] implies convergence in probability of @xmath461 to @xmath462 $ ] . hence , since var@xmath458{\\overset{k}{% \\rightarrow } } 0 $ ] , for @xmath111 and @xmath434 , @xmath463=(-1)^{i}\\big[\\frac{1}{2}\\left ( \\overline{\\boldsymbol{\\mu } % _ { j}^{t}\\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { j}}+j_{j}\\right ) + \\\\ & \\frac{\\gamma _ { i}(\\overline{\\mathbf{m}_{i}^{t}\\mathbf{\\sigma } ^{-1}% \\boldsymbol{\\mu } _ { i}}-\\overline{\\mathbf{m}_{i}^{t}\\mathbf{\\sigma } ^{-1}% \\boldsymbol{\\mu } _ { j}})}{1+\\gamma _ { i}}+\\frac{1-\\gamma _ { i}}{2(1+\\gamma _ { i})% } \\left ( \\overline{\\boldsymbol{\\mu } _ { i}^{t}\\mathbf{\\sigma } ^{-1}\\boldsymbol{% \\mu } _ { i}}+j_{i}\\right ) -\\overline{\\boldsymbol{\\mu } _ { i}^{t}\\mathbf{\\sigma } % ^{-1}\\boldsymbol{\\mu } _ { j}}\\left ( \\frac{1-\\gamma _ { i}}{2(1+\\gamma _ { i})}+% \\frac{1}{2}\\right ) \\big]{=}g_{i}^{b}. \\end{aligned } \\label{tyty}\\ ] ]",
    "now let @xmath464where @xmath465 .",
    "similar to deriving ( [ varg ] ) via the variance of quadratic forms of gaussian variables , we can show @xmath466=4\\delta _ { \\boldsymbol{% \\mu } } ^{2}\\left ( \\frac{1}{n_{0}}+\\frac{1}{n_{1}}\\right ) + 2p\\left ( \\frac{1}{% n_{0}}+\\frac{1}{n_{1}}\\right ) ^{2}.   \\label{xhabma}\\]]thus , @xmath467=\\left ( \\frac{\\nu _ { i}^{\\ast } + 1}{\\nu _ { i}^{\\ast } } \\right ) ^{2}\\text{var}_{s_{n}}[\\hat{\\delta}% ^{2}|\\boldsymbol{\\mu } ] { \\overset{k}{\\rightarrow } } 0 . \\end{aligned }",
    "\\label{cnjasa}\\]]as before , from chebyshev s inequality it follows that @xmath468{=}d.\\ ] ] by the continuous mapping theorem ( continuous functions preserve convergence in probability ) , @xmath469from ( [ bskaba ] ) we have @xmath470with @xmath471 being the delta function and @xmath472 shows convergence in distribution .",
    "boundedness and continuity of @xmath64 along with ( 2 ) allow one to apply helly - bray lemma @xcite to write    @xmath473=\\underset{\\text{b.k.a.c.}}{\\operatorname{lim}}\\;e_{s_{n}}% \\left [ \\phi \\left((-1)^{i}\\ ; \\frac{-\\hat{g}_{i}^{b}+c}{\\sqrt{\\hat{d}_{i}}}% \\right ) |\\boldsymbol{\\mu } \\right ] \\\\",
    "& = e_{z_i}\\left [ \\phi \\left (   z \\right ) \\right ] = \\phi \\left ( ( -1)^{i}\\;\\frac{-g_{i}^{b}+c}{\\sqrt{d_i}}\\right)=\\underset{\\text{b.k.a.c.}}{\\operatorname{plim}}\\;{\\hat{\\varepsilon}}% _ { i}^{b}|\\boldsymbol{\\mu } .\\text { \\ \\ \\ \\ \\ \\ } \\blacksquare \\end{aligned}%\\ ] ]",
    "we first prove that @xmath474 with @xmath475 defined in ( [ ghat0 ] ) .",
    "to do so we use @xmath476=\\text{var}_{% \\boldsymbol{\\mu } } \\left [ e_{s_{n}}[\\hat{g}_{0}^{b}|\\boldsymbol{\\mu } ] \\right ] + e_{\\boldsymbol{\\mu } } \\left [ \\text{var}_{s_{n}}[\\hat{g}_{0}^{b}|\\boldsymbol{% \\mu } ] \\right ] .",
    "\\end{aligned } \\label{condv}\\]]to compute the first term on the right hand side , we have @xmath477 & = \\,\\frac{\\nu _ { 0}\\mathbf{m}% _ { 0}^{t}}{n_{0}+\\nu _ { 0}}\\mathbf{\\sigma } ^{-1}(\\boldsymbol{\\mu } _ { 0}-% \\boldsymbol{\\mu } _ { 1})+\\frac{n_{0}-\\nu _ { 0}}{2(n_{0}+\\nu _ { 0})}\\left (   \\boldsymbol{\\mu } _ { 0}^{t}\\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { 0}+\\frac{p}{% n_{0}}\\right ) \\\\ & + \\frac{1}{2}\\left ( \\boldsymbol{\\mu } _ { 1}^{t}\\mathbf{\\sigma } ^{-1}% \\boldsymbol{\\mu } _ { 1}+\\frac{p}{n_{1}}\\right ) -\\boldsymbol{\\mu } _ { 0}^{t}% \\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { 1}\\left ( \\frac{n_{0}}{n_{0}+\\nu _ { 0}}% \\right ) .",
    "\\end{aligned}%\\]]for @xmath111 and @xmath434 define the following random variables : @xmath478the variables defined in ( [ fff ] ) can be obtained by replacing @xmath479 s with @xmath136 s in ( [ ggg ] ) and @xmath480 and @xmath481 . replacing @xmath136 with @xmath88 and @xmath345 with @xmath482 in ( [ vargj ] ) and ( [ varg ] ) yields",
    "@xmath483by cauchy - schwarz , @xmath484 , @xmath485 , and @xmath486 .",
    "hence , @xmath487\\right ] { \\overset{k}{\\rightarrow } } \\;0 $ ] .    now consider the second term on the right hand side of ( [ condv ] ) .",
    "the covariance of a function of gaussian random variables can be computed from results of @xcite .",
    "for instance , @xmath488=\\frac{2% } { n_{i}}\\mathbf{a}^{t}\\boldsymbol{\\mu } _",
    "{ i}\\ , .",
    "\\label{kasa}\\]]from ( [ kasa ] ) and the independence of @xmath59 and @xmath489 , @xmath490=\\frac{2}{n_{i}}\\boldsymbol{\\mu } _ { j}^{t}\\mathbf{\\sigma } % ^{-1}\\boldsymbol{\\mu } _ { i},\\;\\;i\\neq j   \\label{kasa1}\\]]via ( [ vargp ] ) , ( [ kasa ] ) , and ( [ kasa1 ] ) , the inner variance in the second term on the right hand side of ( [ condv ] ) is @xmath491=\\frac{(n_{0}-\\nu _ { 0})^{2}p}{2n_{0}^{2}(n_{0}+\\nu _ { 0})^{2}}+\\frac{n_{0}p}{n_{1}(n_{0}+\\nu _ { 0})^{2}}+\\frac{p}{2n_{1}^{2 } } \\\\ & + \\left ( \\frac{(n_{0}-\\nu _ { 0})^{2}}{n_{0}(n_{0}+\\nu _ { 0})^{2}}+\\frac{% n_{0}^{2}}{n_{1}(n_{0}+\\nu _ { 0})^{2}}\\right ) \\boldsymbol{\\mu } _ { 0}^{t}% \\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { 0}-2\\left ( \\frac{n_{0}-\\nu _ { 0}}{% ( n_{0}+\\nu _ { 0})^{2}}+\\frac{n_{0}}{n_{1}(n_{0}+\\nu _ { 0})}\\right ) \\boldsymbol{% \\mu } _ { 0}^{t}\\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { 1 } \\\\ & + 2\\left ( \\frac{\\nu _ { 0}(n_{0}-\\nu _ { 0})}{n_{0}(n_{0}+\\nu _ { 0})^{2}}+\\frac{% \\nu _ { 0}n_{0}}{n_{1}(n_{0}+\\nu _ { 0})^{2}}\\right ) \\mathbf{m}_{0}^{t}\\mathbf{% \\sigma } ^{-1}\\boldsymbol{\\mu } _ { 0}-2\\left ( \\frac{\\nu _ { 0}}{(n_{0}+\\nu _ { 0})^{2}}+\\frac{\\nu _ { 0}}{n_{1}(n_{0}+\\nu _ { 0})}\\right ) \\mathbf{m}_{0}^{t}% \\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { 1}+ \\\\ & \\left ( \\frac{n_{0}}{(n_{0}+\\nu _ { 0})^{2}}+\\frac{1}{n_{1}}\\right )   \\boldsymbol{\\mu } _ { 1}^{t}\\mathbf{\\sigma } ^{-1}\\boldsymbol{\\mu } _ { 1}+\\frac{% \\nu _ { 0}^{2}}{(n_{0}+\\nu _ { 0})^{2}}\\left ( \\frac{1}{n_{0}}+\\frac{1}{n_{1}}% \\right ) \\mathbf{m}_{0}^{t}\\mathbf{\\sigma } ^{-1}\\mathbf{m}_{0}. \\end{aligned } \\label{hdhs}\\]]now , again from the results of @xcite , @xmath492=\\mathbf{m}_{i}^{t}\\mathbf{\\sigma } ^{-1}\\mathbf{m}_{i}+% \\frac{p}{\\nu _ { i}},\\\\ &   e_{\\boldsymbol{\\mu } } [ \\boldsymbol{\\mu } _ { i}^{t}\\mathbf{\\sigma } ^{-1}% \\boldsymbol{\\mu } _ { j}]=\\mathbf{m}_{i}^{t}\\mathbf{\\sigma } ^{-1}\\mathbf{m}% _ { j},i\\neq j. \\end{aligned } \\label{bkashbkau}\\]]from ( [ hdhs ] ) and ( [ bkashbkau ] ) , some algebraic manipulations yield @xmath493\\right ] = \\frac{(n_{0}-\\nu _ { 0})^{2}p}{2n_{0}^{2}(n_{0}+\\nu _ { 0})^{2}}+% \\frac{n_{0}p}{n_{1}(n_{0}+\\nu _ { 0})^{2 } } \\\\ & + \\frac{p}{2n_{1}^{2}}+\\left ( \\frac{(n_{0}-\\nu _ { 0})^{2}}{n_{0}(n_{0}+\\nu _ { 0})^{2}}+\\frac{n_{0}^{2}}{n_{1}(n_{0}+\\nu _ { 0})^{2}}\\right ) \\frac{p}{\\nu _ { 0 } } + \\left ( \\frac{n_{0}}{(n_{0}+\\nu _ { 0})^{2}}+\\frac{1}{n_{1}}\\right )   \\frac{p}{\\nu _ { 1}}+\\left ( \\frac{n_{0}}{(n_{0}+\\nu _ { 0})^{2}}+\\frac{1}{n_{1}}% \\right ) \\delta _ { \\mathbf{m}}^{2}. \\end{aligned } \\label{jeyxn}\\]]from ( [ jeyxn ] ) we see that @xmath494\\big]{\\overset{k}{\\rightarrow } } \\;0 $ ] . in sum , @xmath495{\\overset{k}{% \\rightarrow } } \\;0 $ ] and similar to the use chebyshev s inequality in the proof of theorem [ thm - m1 ] , we get @xmath496{% \\overset{\\delta } { \\;=\\;}}h_{i } ,   \\label{tytyr}\\]]with @xmath497 defined in ( [ njzzp ] ) .    on the other hand , for @xmath498 defined in ( [ dtrue ] ) we can write @xmath499=\\text{var}_{\\boldsymbol{% \\mu } } \\left [ e_{s_{n}}[\\hat{d}_{i}|\\boldsymbol{\\mu } ] \\right ] + e_{\\boldsymbol{% \\mu } } \\left [ \\text{var}_{s_{n}}[\\hat{d}_{i}|\\boldsymbol{\\mu } ] \\right ] . \\end{aligned } \\label{condvd}\\]]from similar expressions as in ( [ bkashbkau ] ) for @xmath500",
    ", we get @xmath501=\\delta _ { \\boldsymbol{\\mu } } ^{2}+\\frac{p}{n_{0}}+\\frac{p}{n_{1}}$ ]",
    ". moreover , @xmath502 $ ] is obtained from ( [ xhabma ] ) by replacing @xmath345 with @xmath94 , and @xmath503 with @xmath411 .",
    "thus , from ( [ dtrue ] ) , @xmath504% \\right ] = \\left ( \\frac{\\nu _ { i}^{\\ast } + 1}{\\nu _ { i}^{\\ast } } \\right ) ^{2}\\bigg[% 4\\delta _ { \\mathbf{m}}^{2}\\left ( \\frac{1}{\\nu _ { 0}}+\\frac{1}{\\nu _ { 1}}\\right ) + 2p\\left ( \\frac{1}{\\nu _ { 0}}+\\frac{1}{\\nu _ { 1}}\\right ) ^{2}\\bigg]{\\overset{k}% { \\rightarrow } } 0 . \\end{aligned }",
    "\\label{cnjasak}\\]]furthermore , since @xmath505=\\delta _ { \\mathbf{m}}^{2}+\\frac{p}{\\nu _ { 0}}+\\frac{p}{\\nu _ { 1}}$ ] , from ( [ cnjasa ] ) , @xmath506% \\right ] & = \\left ( \\frac{\\nu _ { i}^{\\ast } + 1}{\\nu _ { i}^{\\ast } } \\right ) ^{2 } % \\left [ 4(\\delta _ { \\mathbf{m}}^{2}+\\frac{p}{\\nu _ { 0}}+\\frac{p}{\\nu _ { 1}}% ) \\left ( \\frac{1}{n_{0}}+\\frac{1}{n_{1}}\\right ) + 2p\\left ( \\frac{1}{n_{0}}+% \\frac{1}{n_{1}}\\right ) ^{2}\\right ] { \\overset{k}{\\rightarrow } } 0 . \\end{aligned } \\label{mshdh}\\]]hence , @xmath507{\\overset{k}{% \\rightarrow } } 0 $ ] and , similar to ( [ tytyr ] ) , we obtain @xmath508=\\underset{\\text{% b.k.a.c.}}{\\operatorname{lim}}\\;e_{\\boldsymbol{\\mu } , s_{n}}[\\hat{d}_{1}]{\\overset{% \\delta } { \\;=\\;}}f ,   \\label{msanxs}\\]]with @xmath9 defined in ( [ njzzp ] ) . similar to the proof of theorem thm - m1 , by using the continuous mapping theorem and the helly - bray lemma , we can show that @xmath509 = \\underset{\\text{b.k.a.c.}}{\\operatorname{lim}}e_{% \\boldsymbol{\\mu } , s_{n}}\\left [ \\phi \\left((-1)^{i}\\ ; \\frac{{-\\hat{g}}% _ { i}^{b}+c } { \\sqrt{{\\hat{d}}}}\\right ) \\right ] \\\\ & = \\phi \\left ( ( -1)^{i}\\;\\frac{-h_{i}+c}{\\sqrt{f}}\\right ) , \\end{aligned } \\label{mdns}\\]]and the result follows .",
    "we start from @xmath207=e_{s_{n}}\\bigg[p% \\big(u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z})\\leq c\\,,u_{0}(\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}^{\\prime } ) \\leq c\\,|\\bar{\\mathbf{x}}_{0},\\bar{\\mathbf{x}}_{1},\\mathbf{z}\\in \\psi _",
    "{ 0},% \\mathbf{z}^{\\prime } \\in \\psi _ { 0},\\boldsymbol{\\mu } \\big)\\bigg ] , \\end{aligned}%\\]]which was shown in ( [ kjxbkasu ] ) . here",
    "we characterize the conditional probability inside @xmath510 $ ] . from the independence of @xmath175 , @xmath511 , @xmath59 , and @xmath512 ,",
    "@xmath513where here @xmath514 denotes the bivariate gaussian density function and @xmath475 and @xmath515 are defined in ( [ ghat0 ] ) and ( [ dtrue ] ) .",
    "@xmath516=\\left [ \\phi \\left ( \\frac{-\\hat{g}_{0}^{b}+c}{\\sqrt{\\hat{d}}}\\right ) \\right ] ^{2}|% \\boldsymbol{\\mu } .\\]]similar to the proof of theorem [ thm - m1 ] , we get @xmath517=\\underset{\\text{b.k.a.c.}}{\\operatorname{plim}}\\;(% \\hat{\\varepsilon}_{i}^{b})^{2}|\\boldsymbol{\\mu } = \\left ( \\lim_{\\substack {   \\text{b.k.a.c . }   \\\\ } } e_{s_{n}}[\\hat{\\varepsilon}_{i}^{b}|\\boldsymbol{\\mu } % ] \\right ) ^{2}=\\left [ \\phi \\left ( ( -1)^{i}\\;\\frac{-g_{i}^{b}+c}{\\sqrt{d}}% \\right ) \\right ] ^{2}. \\end{aligned}%\\]]similarly , we obtain @xmath518=$ ] @xmath519 , and the results follow .",
    "@xmath206    29 natexlab#1#1[2]#2 , , ( ) .",
    ", , , , ( ) . , , , ( ) .",
    ", , , , ( ) .",
    ", , , , ( ) . , , , , ( ) .",
    ", , , , ( ) . , , ( ) .",
    ", , ( ) . .",
    ", , ( ) . .",
    ", , , ( ) . , , , ( ) .",
    ", , , , , , ( ) .",
    ", , , , ( ) . , , , ( ) .",
    ", , , ( ) .",
    ", , , ( ) . , , , ( ) . , ,",
    "( ) . , , ( ) . , , ( )",
    ", , , , , , .",
    ", , , , , .",
    ", , , ( ) ."
  ],
  "abstract_text": [
    "<S> the most important aspect of any classifier is its error rate , because this quantifies its predictive capacity . </S>",
    "<S> thus , the accuracy of error estimation is critical . </S>",
    "<S> error estimation is problematic in small - sample classifier design because the error must be estimated using the same data from which the classifier has been designed . </S>",
    "<S> use of prior knowledge , in the form of a prior distribution on an uncertainty class of feature - label distributions to which the true , but unknown , feature - distribution belongs , can facilitate accurate error estimation ( in the mean - square sense ) in circumstances where accurate completely model - free error estimation is impossible . </S>",
    "<S> this paper provides analytic asymptotically exact finite - sample approximations for various performance metrics of the resulting bayesian minimum mean - square - error ( mmse ) error estimator in the case of linear discriminant analysis ( lda ) in the multivariate gaussian model . </S>",
    "<S> these performance metrics include the first , second , and cross moments of the bayesian mmse error estimator with the true error of lda , and therefore , the root - mean - square ( rms ) error of the estimator . </S>",
    "<S> we lay down the theoretical groundwork for kolmogorov double - asymptotics in a bayesian setting , which enables us to derive asymptotic expressions of the desired performance metrics . from these </S>",
    "<S> we produce analytic finite - sample approximations and demonstrate their accuracy via numerical examples . </S>",
    "<S> various examples illustrate the behavior of these approximations and their use in determining the necessary sample size to achieve a desired rms . </S>",
    "<S> the supplementary material contains derivations for some equations and added figures .    </S>",
    "<S> [ theorem]corollary [ theorem]lemma    linear discriminant analysis , bayesian minimum mean - square error estimator , double asymptotics , kolmogorov asymptotics , performance metrics , rms </S>"
  ]
}