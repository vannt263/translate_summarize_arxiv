{
  "article_text": [
    "the eu datagrid ( edg ) project @xcite started in january 2001 and is now entering its third and last year of activity .",
    "the goal of the project is the development of a consistent set of services to create a distributed computing grid , as defined in @xcite.to test the system under development , a distributed testbed was created involving at first the five main partners of the project ( cern , infn , in2p3 , pparc , and nikhef ) .",
    "this testbed was then extended to more and more sites , to a total which now exceeds 20 centers all over europe .    since the beginning of the project",
    ", cern has been in a very central position , from development to final deployment of the software on the main testbed . in this paper",
    "we will describe our experience as managers of the cern testbed site .",
    "the current ( as of march 2003 ) release of the edg middle - ware includes a set of services which implement , even if in a non - definitive way , all the basic services needed to create a grid . in this section we will give an overview of these services .      user and node authentication complies with the grid security infrastructure ( gsi ) , as defined in @xcite , and is based on the openssl implementation of the public key infrastructure ( pki ) .",
    "access to resources is authorized through a simple map file which associates each authorized user certificate with a dynamically assigned local account .    the list of authorized certificates is available from a small number of ldap servers , each managed by a different virtual organization ( vo ) . as a consequence ,",
    "the current authorization structure is very coarse grained and can only distinguish either between two individual users or between users belonging to different vo s .",
    "physical access to local resources , i.e. submission of jobs to a local batch system or transfer of a file to / from a local storage server , uses the gatekeeper service provided by the globus @xcite project with the addition of edg specific extensions , and the gsi - enabled gridftp server , also from globus .",
    "storage management is still at a quite early stage of development and includes a file replication service ( gdmp @xcite ) , which at cern is interfaced to castor , the local mss service @xcite , and an ldap - based replica catalog , which will be described later in some detail .",
    "information about available resources is distributed via a hierarchical grid information service ( gis ) , also using ldap as the base protocol .",
    "this is the intelligence of the grid : a resource broker ( rb ) scans job requirements , chooses the best matching site using information from the gis and delivers the job to the corresponding gatekeeper .    to accomplish this task",
    ", the top rb server uses several subordinate services : job submission server ( jss ) job parsing server ( jps ) , condorg @xcite , ...      the logging and bookkeeping ( lb ) service stores a state - transition based history of each job handled by the grid .",
    "monitoring of the correct functioning of each edg service and of the grid as a whole is foreseen for the final version of the edg software but no integrated monitoring system was available at the time of the report .",
    "the same applies for a detailed accounting of resource utilization .",
    "solutions for both services were in operation and varied from site to site .",
    "one of the goals of the edg project is to provide automatic tools to manage large computing centers in the lhc era .",
    "most testbed sites used an automatic node installation and management tool , the local configuration system ( lcfg @xcite ) , which will be described later in this paper .",
    "due to its central position within the project , cern hosts several parallel testbeds , used for different tasks .      on this testbed the latest `` stable '' version of the edg middle - ware",
    "is installed .",
    "this testbed is open for all users participating to the project and is used for several different tasks :    * programmed `` stress tests '' to evaluate the behavior of the middle - ware on a medium scale distributed testbed ; * data production tests by all application groups , including all lhc experiments , bio - medical applications , and earth observation projects ; * demonstrations and tutorials of edg middle - ware functionalities",
    ". incidentally we can note that these activities , targeted at increasing the public visibility of the project , often posed problems to the site managers : given the current maturity level of the software , to avoid putting the demonstration at risk a constant surveillance of the testbed was required for the whole duration of the same and no other activities ( test , normal usage ) could take place .",
    "recently an independent testbed , specific for demonstrations , has been set up .",
    "the number of sites participating to this testbed grew from the original 5 sites to the current 20 and is constantly growing .",
    "cern hosted most of the central services ( the top node of the information system , replica catalogs for two vo s , two of the main rb s , ... ) and connected to the middle - ware a pbs - based batch system with a number of worker nodes which went up to over 100 in periods of intense activity .",
    "as only software versions which passed a preliminary extensive test were installed here , this testbed underwent a relatively small number of software updates , mostly required for security patches , installation of new applications on behalf of the experimental groups , and modifications to the access permissions . on the other hand , being aimed at production tests",
    ", this testbed required a fairly large amount of manpower due to the instability of the software : most of the main services needed a complete restart at least once per day and , at the same time , a lack of management tools and experience made troubleshooting very hard .      on this testbed",
    "all edg software versions are integrated and tested . in order to keep response and update time as short as possible ,",
    "only the five main sites participate to this testbed .    aimed at functionality testing , the number of nodes in this testbed was fairly small compared to the application testbed . at cern",
    "only up to 10 nodes were used for the development testbed .",
    "update activity on this testbed was continuous , often requiring installation of several versions in a single day . to facilitate troubleshooting , developers had direct access to most service nodes . on several occasions",
    "this induced some traceability problems as it was not always easy to get a complete description of what the developer did to fix a bug , especially if this included node configuration changes .",
    "last april , a new testbed was created to perform the ( still ongoing ) integration of version 2.0 of the edg middle - ware .",
    "this testbed is composed of about 20 nodes and has characteristics very similar to the development testbed : continuous deployment of test versions , free access to developers , frequent re - installation of the nodes .    on this testbed ,",
    "the edg middle - ware was first ported to the redhat 7.3 version of linux and to the 2.2.4 version of the globus toolkit , in its vdt 1.1.8 @xcite manifestation .",
    "since the integration phase for edg 2 started , most of the edg manpower and hardware resources at cern were diverted to this testbed so that , at the time of writing , the cern development testbed has been completely dismantled and the application testbed has been reduced to just a storage element ( se ) node providing access to the castor mss .      before a new version of one of the services is allowed to be merged with the rest of the middle - ware on the development or integration testbed , it must be tested for basic functionality in standalone mode . to this end",
    "we set up several `` reduced '' testbeds , including only software components needed to test one of the services at a time .",
    "each of these testbeds consisted of only two or three nodes but there were continuous requests for the creation of topologies centered on any one of the services .      to support all these testbeds",
    ", cern provided an infrastructure based on some standard cern computer center services with the addition of a few edg specific services :    * 5 data servers , with a total of 2.5 tb of mirrored disk storage , offered nfs - based storage capabilities for edg users home directories , shared areas for the batch system ( the gass_cache area described later ) and generic storage for data production tests ; * one of the edg se s was interfaced to the castor mss ; * as many of the edg developers and users could not be provided with a cern afs account , we set up a nis service for all edg users ; * a certification authority ( ca ) @xcite was set up specifically for edg usage and provided personal and host certificates for cern users .",
    "this ca is now evolving toward a cern - wide service . *",
    "an lcfg server , including a private dhcp server , was setup to support node installation and configuration .",
    "the whole testbed infrastructure was then interconnected to the standard cern lan with 100 mbps or , in the case of data servers , 1 gbps ethernet lines .",
    "before march 2002 , no real software release procedure was in place .",
    "this led to several problems mostly related to the tracing of hand - made modifications on the configuration of the nodes and the installation of private executables by the developers . in turn , this resulted in misalignment among the five participating sites and in huge difficulties in collecting a set of software packages and configuration settings to create a consistent version of the edg middle - ware .    in spite of the difficulties and with a lot of hard work from all the people involved",
    ", we were able to converge to the first real release of the edg software , version 1.1.2 , which was then used in some key demonstrations of functionalities in march 2002 .",
    "to improve the situation , a strict release procedure was defined :    * all new rpms are first delivered to the integration team which takes care of inserting them in a new cvs - based tentative release tag ; * this tentative tag is first installed on the cern development testbed and a predefined set of basic tests is applied ; * the five core sites install the same tag and a new set of tests , centered on distributed functionalities , is applied ; * the tag is installed on the application testbed where scalability tests and generalized use can begin .",
    "if at any of these stages the tests fail , the tag is rejected and bug reports are sent back to the relevant developers .    to improve flexibility ,",
    "application software only needed at the final stage of testing to create a realistic environment is not required to follow this procedure and can be installed upon request of the main application groups .",
    "also , basic os security patches and support for new ca s can be applied at need .",
    "thanks to this procedure , the release of new versions of the code proceeded in a much smoother way and had its finest day last november when all edg sites successfully moved from version 1.2.3 to the non - downward - compatible version 1.3.0 in only one day .",
    "one of the key issue to implement the release procedure described in the previous section is the possibility of installing and configuring service nodes located at geographically remote sites according to a predefined and continuously changing set of instructions .    to this end edg",
    "adopted and extended the local configuration system ( lcfg ) developed at the university of edinburgh .",
    "this tool uses a human - readable description of the basic configuration of the linux os and sends it in xml format to an agent on the node for installation .",
    "there a set of scripts , called `` objects '' , use this description to actually modify the system configuration files .",
    "lcfg can easily be extended by providing additional objects which can configure any new functionality or service one may want to add .    after a slow start , more and more objects were created to effectively configure all edg specific services .",
    "today only very few hand configuration steps are still needed to create an edg - enabled site and even these will be soon completely automated .",
    "even if lcfg has proved to be the most valuable instrument to keep the edg release procedure in track , we found a few drawbacks which had to be taken into account before adopting this tool :    * new objects have to be carefully designed in order not to interfere with other objects ; * some basic limitations of the standard objects have to be taken into account , e.g. hard disks can only be partitioned using the four primary partitions ; * no feedback about the installation or update process is sent back to the lcfg server : this required the creation of _ ad hoc _ tools to do some basic checks and a lot of manual work in case of problems ; * lcfg wants to have total control of the machine configuration , from basic os functions to application level .",
    "this means that , in its basic version , lcfg is not suited to install , for example , the edg middle - ware on top of an already installed node .",
    "recently a modified version of lcfg , called lcfglite@xcite , was developed to handle this case ; * due to its structure , lcfg does not cope well with os settings which may change outside of its control .",
    "an example are user passwords : if users change their passwords , lcfg will change them back to a predefined value . to solve this problem ,",
    "we moved all user accounts to a non - lcfg - managed nis server and left only system accounts under local lcfg control ; * as each modification to the node configuration must be first inserted into the configuration description file on the main server , using lcfg to manage nodes used by developers for their first tests might substantially slow down the fast rate of changes . also , if a developer modifies any configuration file by hand , this might be changed back by lcfg , thus introducing a lot of entropy into the process .    to overcome part of these shortcomings",
    ", the edg wp4 group is currently finalizing an alternative tool which will replace lcfg .    in parallel with lcfg , we used the network boot capabilities of recent nic s and the syslinux tool @xcite to bootstrap the initial installation procedure of the nodes directly from the network .",
    "it was then sufficient to start a private dhcp server on the lcfg server to get the whole node installation and configuration automatically accomplished .",
    "to improve the situation even further , we implemented a system to remotely reset nodes using a serial line controlled relay system connected to the reset switch of the motherboards and we used multi serial line boards to collect all consoles to a few central servers , thus allowing a completely web - based control of each node . a report on this project was presented at this conference in @xcite .",
    "after all the described tools were in place and after an initial period of adaptation and tuning , the large number of rapidly changing nodes needed for the edg deployment became much more manageable and our visits to the cern computer center decreased to almost nil .",
    "the complexity and novelty of the edg project made the list of problems encountered in the integration and testing phases particularly long .",
    "here we briefly list some of the main issues which emerged in the process and describe in more detail the problems which have interesting aspects from the system- and site - manager point of view .",
    "as noted previously , many of the middle - ware services were and still are quite fragile , needing frequent restarts .",
    "in addition to this , the fault patterns tend to be very complex , often involving several services at the same time , thus making the troubleshooting process quite hard and the learning curve very steep .",
    "this state of affairs , normal for an r&d project of this size , was worsened by the fact that the overall architectural design of the project concentrated on defining service functionalities and their interaction but often neglected to deal with the resource management aspects of the problem .    as a consequence , we had many problems in dealing with otherwise normal aspects of storage management : adding new disk space was very tricky , as well as moving file around within the available disk space .",
    "also , no tools to handle scratch space on either batch or service nodes were foreseen , and it was very easy to see a whole disk fill up bringing the node , and often with it the whole grid , to a grinding halt .",
    "log files , one of the main tools to perform troubleshooting and system checking , were most of the time hard to interpret , often lacked some fundamental information ( the time tag ! ) , and for a few services did not exist at all .",
    "another aspect which was not sufficiently taken into account in the first development of the software was that of scalability : several services , even if they correctly implemented all basic functionalities , could not scale beyond a minimal configuration",
    ". examples of this could be found in the resource broker , which , due to a known limitation of the condorg system , was not able to handle more than 512 jobs at the same time , and the replica catalog , which had problems if more than o(1000 ) files ( this number depends on the average length of the file name ) were listed as a single collection .",
    "even the information service started malfunctioning as soon as the number of sites connected to the grid increased from the initial five .",
    "parallel to scalability problems , we encountered problems related to more traditional aspects of distributed system programming : all along the development and integration process , besides the usual memory leak bugs , we saw port leaks , file leaks , even i - node leaks .",
    "these problems were often related to lower levels of the software and led to non obvious fault patterns which needed a long time to debug .    a problem which haunted us in the early phases of the project was the lack of control in the packaging process : executables which worked correctly when compiled by hand by developers , suddenly started misbehaving when inserted into a standard rpm package for deployment .",
    "this was most of the time due to differences in libraries on private machines .",
    "to reduce this risk , an auto - build system was created and is now in operation : developers are required to set their packages so that they can be compiled on the official auto - build node and only packages created by this system are accepted for release .    before going into the details of some of the listed problems , we should note that the experience from the first phase of the project was very valuable to define the priorities for the new and final release of the edg software , currently in the integration phase , so that most of the problems and inconsistencies found during the deployment of the first version of the software were addressed and solved , thus improving stability , scalability , and manageability of the whole system .      due to the internal structure of the gram protocol , defined by the globus project to handle job submission and information passing in a grid - enabled batch system , a disk area , known as the gass_cache area , must be shared between the gatekeeper ( the borderline node between the grid and the batch system ) and all the worker nodes of the batch system .",
    "according to the gram protocol , each job which is submitted to the batch system creates a large number ( @xmath0 ) of tiny files in the gass_cache area .",
    "if the job ends in an unclean way , very often these files are not erased .",
    "in addition , due to a bug in the implementation , even if the job ends correctly , the gass_cache area is not completely cleaned .",
    "given the small size of the files , often not exceeding 100 bytes , these two problems create a steady leak of i - nodes on the shared area so that , even if this area appears to be almost empty , the gram protocol suddenly stops working as no more files can be created .",
    "being at the heart of the job submission system of the grid , problems with the gram protocol manifest themselves in a whole set of different and apparently uncorrelated malfunctions of several subsystems : the first few times it took us a long time to find where the problem lay .    even knowing the real source of the problem , fixing it requires a long time to clean up the huge number of leftover i - nodes , time during which the local batch system is not visible from the grid , and the loss of any job running on the batch system at the time .    as there is no easy way to map the internal structure of the gass_cache area to the jobs which are currently running on the batch system , and no tool to do this was provided , either by globus , or by edg , it is not possible to create a clean up daemon which keeps an eye on the shared area and cleans up files which are not longer needed .",
    "as already noted , the first version of the edg middle - ware did not contain a complete and integrated model for grid - wide storage management but only a set of low level tools and services which could be used to partially interface storage servers to the grid .",
    "the principal tools were :    * a gsi - enabled ftp server ; * gdmp @xcite , a replication - on - demand service * a basic replica catalog ( rc ) , with the cited limitations in the number of files it can manage ; * a set of user level commands to copy files to and from a data server , to trigger file replication between data servers , and to register files to the rc ; * a basic interface to mass storage systems like castor and hpss .",
    "a constraint in the way the rc was organized had unforeseen consequences on the possibility of setting up and organizing a storage system which are worth examining in some detail .",
    "the rc is a simple database , in its first implementation based on the openldap @xcite package , which contains a collection of logical file names ( lfn ) , used to uniquely identify a given file , and for each of them one or more physical file names ( pfn ) which point to the physical location of each replica of the logical file . by design",
    ", the pfn was obtained from the lfn by appending it to the path of the grid - enabled area assigned to the corresponding vo on a given storage system .    as an example , let s assume that a simulation job submitted by a physicist belonging to the atlas vo produces a file with lfn prod / feb2003/simu001.fz . if a copy of this file is stored on a se at cern , it will have a pfn like //lxshare0384.cern.ch / flatfiles / atlas / prod/ + feb2003/simu001.fz where :    * lxshare0384.cern.ch is the node name of the se * /flatfiles / atlas is the path to the disk area assigned to atlas    this apparently harmless limitation had heavy implications in the usage of storage systems : for the file replication system to work in a user transparent way within the whole grid , the disk area on each se must either consist of a single big partition , which merges all physical disk systems available to the node , or all se s must have exactly the same disk partition structure .    to understand this ,",
    "assume that at cern the prod / feb2003 path corresponds to a mount point of a 100 gb disk ( the standard organization of cern disk servers ) so that only atlas can use it . at , e.g. , ral the prod",
    "/ feb2003 path might not exist yet , so if a user wants to replicate the file there he / she must first create it . at ral this path will end up on a partition which depends on the disk layout of the local se and which may very well not have enough disk space to hold the replicated file , even if the se itself has plenty of free disk space on other partitions .",
    "this problem was particularly annoying at cern when we tried to set up a central se where the cms collaboration could collect all simulated data produced on the grid during its `` stress test '' . to allow for automatic replication from the other sites",
    ", we had to carefully plan the partition layout on the central se , taking into account how much data would be produced on average at each site participating to the test , and then ask the cms production group to store their data using different paths according to which site the data were being produced at .",
    "the final system worked fine but it was very far from the expected transparent replication of data within the grid .",
    "as explained above , the resource broker ( rb ) is the central intelligence of the grid , taking care of interpreting user requests expressed in the job description language ( jdl ) and mapping them to the available resources using the grid information service .    due to its central role ,",
    "the rb interacts with most of the grid services in many different and complex ways .",
    "a malfunction in the rb is then very visible from the user point of view as jobs are no longer accepted and it can have bad effects on all services with which it interacts .",
    "a lot of effort was put into fixing problems but it is still the most sensitive spot in the grid .    to improve reliability in job submission ,",
    "several rb s have been set up at different sites , thus increasing the scalability of the grid in terms of the maximum number of jobs accepted and giving users some back - up access points to the grid in case the local rb gets stuck with a problem .    due to the large number of low - level services it uses , several problems can show up in the rb , thus affecting the functioning of the whole grid .",
    "one of these is the corruption of the job requests database .",
    "this is related to a non - thread - safe low level library used by the postgres database which , on dual - processor nodes , can lead to data corruption .",
    "when this problem occurs , all rb daemons appear to run correctly but newly submitted jobs end up in a waiting status .",
    "all the daemons must then be stopped , the whole postgres database cleaned up , and then the rb can be restarted . in the process ,",
    "all active jobs being controlled by the rb are lost . during normal usage of the system , this problem occurs on average once per day per rb node .    it must be noted that a solution to this problem is already available and will be deployed with version 2 of the edg middle - ware .",
    "edg testbeds have been in operation for almost two years , always providing continuous and indispensable feed - back to edg developers .",
    "lhc experiments and the other project partners were able to get a first taste of a realistic grid environment in preparation for future large scale deployments .",
    "being one of the most advanced grid projects currently in operation , most of the problems and inadequacies of the edg middle - ware were hard to isolate and to fix , mostly due to lack of previous experience .",
    "several new problems related to resource management in a gridified environment were isolated and are being or will be addressed in the final version of the edg software and related projects like lcg @xcite ."
  ],
  "abstract_text": [
    "<S> in this paper we report on the first two years of running the cern testbed site for the eu datagrid project . </S>",
    "<S> the site consists of about 120 dual - processor pcs distributed over several testbeds used for different purposes : software development , system integration , and application tests . </S>",
    "<S> activities at the site included test productions of montecarlo data for lhc experiments , tutorials and demonstrations of grid technologies , and support for individual users analysis . </S>",
    "<S> this paper focuses on node installation and configuration techniques , service management , user support in a gridified environment , and includes considerations on scalability and security issues and comparisons with `` traditional '' production systems , as seen from the administrator point of view . </S>"
  ]
}