{
  "article_text": [
    "let @xmath0 be a gibbs probability measure on @xmath1 with dimension @xmath2 very big , that is , @xmath3 where @xmath4 is some @xmath5-finite reference measure on @xmath6 .",
    "our purpose is to study the gibbs sampling  a markov chain monte carlo method ( mcmc in short ) for approximating @xmath0 .",
    "in fact , even for the simplest case where @xmath7 , as the denominator contains an exponential number of terms and each of them may be very big or small for high dimension , it is very difficult to model @xmath0 .",
    "let @xmath8 @xmath9 be the regular conditional distribution of @xmath10 knowing @xmath11 under @xmath0 ; and @xmath12 ( product measure ) , where @xmath13 is the dirac measure at the point @xmath14 .",
    "we see that @xmath15 which is a one - dimensional measure , easy to be realized in practice .",
    "the idea of the gibbs sampling consists in approximating @xmath0 via iterations of the one - dimensional conditional distributions @xmath16 .",
    "it is described as follows .",
    "given a starting configuration @xmath17 , let @xmath18 be a non - homogeneous markov chain defined on some probability space @xmath19 , such that @xmath20 and given @xmath21 then @xmath22 for @xmath23 and the conditional law of @xmath24 is @xmath25 .",
    "in other words , the transition probability at step @xmath26 is :    * @xmath27",
    ".    therefore ,    * @xmath28 .    finally , the gibbs sampling is the time - homogeneous markov chain @xmath29 , whose transition probability is @xmath30 .",
    "this mcmc algorithm is known sometimes as _ gibbs sampler _ in the literature ( see winkler  @xcite , chapters  5 and 6 ) .",
    "it is actively used in statistical physics , chemistry , biology and throughout the bayesian statistics ( a sentence taken from @xcite ) .",
    "it was used by zegarlinski @xcite as a tool for proving the logarithmic sobolev inequality for gibbs measures , see also the second named author @xcite for a continuous time mcmc .",
    "our purpose is two - fold :    * the convergence rate of @xmath31 to @xmath0 ; * the concentration inequality for @xmath32 .",
    "question ( 1 ) is a classic subject .",
    "earlier works by meyn and tweedie @xcite and rosenthal are based on the harris ergodicity theorem ( minorization condition together with the drift condition in the non - compact case ) .",
    "quantitative estimates in the harris ergodic theorem are obtained more recently by rosenthal @xcite and hairer and mattingly @xcite .",
    "but as indicated by diaconis , khare and saloff - coste @xcite , theoretical results obtained from the harris theorem are very far ( even too far ) from the convergence rate of numerical simulations in high dimension ( e.g. , @xmath33 ) .",
    "that is why diaconis , khare and saloff - coste @xcite use new methods and tools ( orthogonal polynomials , stochastic monotonicity and coupling ) for obtaining sharp estimates of @xmath34 ( total variation norm ) for several special models in bayesian statistics , with @xmath35 replaced by @xmath36 , a space of two different components .    for the question ( 1 )",
    ", our tool will be the dobrushin interdependence coefficients ( very natural and widely used in statistical physics ) , instead of the minorization condition in the harris theorem or the special tools in @xcite .",
    "our main idea consists in constructing an appropriate coupling well adapted to the dobrushin interdependence coefficients , close to that of marton @xcite .    to the second question",
    ", we will apply the recent theory on transport inequalities ( see marton @xcite , ledoux @xcite , villani @xcite , gozlan and lonard @xcite and references therein ) , and our approach is inspired from marton @xcite and djellout , guillin and wu @xcite for dependent tensorization of transport inequalities .    see @xcite for monte carlo algorithms and diverse applications , and @xcite for concentration inequalities of general mcmc under the positive curvature condition .",
    "this paper is organized as follows .",
    "the main results are stated in the next section , and we prove them in section  [ sec3 ] .",
    "throughout the paper , @xmath6 is a polish space with the borel @xmath5-field @xmath37 , and @xmath38 is a metric on @xmath6 such that @xmath39 is lower semi - continuous on @xmath40 ( so @xmath38 does not necessarily generate the topology of  @xmath6 ) . on the product space we consider the @xmath41-metric @xmath42 if @xmath43 is the discrete metric on @xmath6 , @xmath44 becomes the hamming distance on @xmath35 , a good metric for concentration in high dimension as shown by marton @xcite .",
    "let @xmath45 be the space of probability measures on @xmath6 and @xmath46 @xmath47 is some fixed point ) .",
    "given @xmath48 , the @xmath49-wasserstein distance between @xmath50 is given by @xmath51 where the infimum is taken over all probability measures @xmath4 on @xmath52 such that its marginal distributions are , respectively , @xmath53 and @xmath54 ( _ coupling of @xmath53 and @xmath54 , say _ ) .",
    "when @xmath55 ( _ the discrete metric _ ) , it is well known that @xmath56    recall the kantorovich ",
    "rubinstein duality relation @xcite @xmath57 let @xmath58 be the given regular conditional distribution of @xmath10 knowing @xmath11 .",
    "_ throughout the paper _ , _ we assume that @xmath59 , @xmath60 for all @xmath61 and @xmath62 , where @xmath63 is some fixed point of @xmath35 , and @xmath64 is lipschitzian from @xmath65 to @xmath66 . _",
    "define the matrix of the @xmath38-dobrushin interdependence coefficients @xmath67 @xmath68 obviously @xmath69 . then the well - known dobrushin uniqueness condition ( see @xcite )",
    "is read as @xmath70 or @xmath71    by the triangular inequality for the metric @xmath72 , @xmath73      when @xmath74 are probability measures , the kullback information ( or relative entropy ) of @xmath75 with respect to @xmath0 is defined as @xmath76    we say that the probability measure @xmath0 satisfies the _ @xmath49-transport - entropy inequality _ on @xmath77 with some constant @xmath78 , if @xmath79 to be short , we write @xmath80 for this relation .",
    "this inequality , related to the phenomenon of measure concentration , was introduced and studied by marton @xcite , developed subsequently by talagrand @xcite , bobkov and gtze @xcite , djellout , guillin and wu @xcite and amply explored by ledoux @xcite , villani @xcite and gozlan - lonard @xcite .",
    "let us mention the following bobkov ",
    "gtze s criterion .",
    "[ bg ] a probability measure @xmath0 satisfies the @xmath49-transport - entropy inequality on @xmath77 with constant @xmath78 , that is , @xmath81 , if and only if for any lipschitzian function @xmath82 , @xmath83 is @xmath0-integrable and @xmath84 where @xmath85 . in that case ,",
    "@xmath86    another necessary and sufficient condition for @xmath87 is the gaussian integrability of @xmath0 , see djellout , guillin and wu @xcite . for further results and recent progresses",
    "see gozlan and lonard @xcite .",
    "[ rem21 ] recall also that w.r.t .",
    "the discrete metric @xmath88 , any probability measure @xmath0 satisfies @xmath87 with the sharp constant @xmath89 ( the well known ckp inequality ) .",
    "for any function @xmath90 , let @xmath91 be the lipschitzian coefficient w.r.t .",
    "the @xmath92th coordinate @xmath93 .",
    "it is easy to see that @xmath94    [ convergencerate ] under the dobrushin uniqueness condition , we have :    a.   for any lipschitzian function @xmath95 on @xmath35 and two initial distributions @xmath96 on @xmath35 , @xmath97 where @xmath98 is a coupling of @xmath99 , that is , the law of @xmath100 is @xmath101 for @xmath102 .",
    "b.   in particular for any initial distribution @xmath75 on @xmath35 , @xmath103 where @xmath98 is a coupling of @xmath104 .    by part ( b ) above @xmath0 is the unique invariant measure of @xmath30 under the dobrushin uniqueness condition , and @xmath105 converges exponentially rapidly to @xmath0 in the metric @xmath106 , showing theoretically why the numerical simulations by the gibbs sampling are very rapid .    let us compare theorem [ convergencerate ] with the known results in @xcite on the convergence rate of the gibbs sampling .",
    "at first the convergence rate in those known works is in the total variation norm , not in the metric @xmath106 .",
    "when @xmath38 is the discrete metric , we have by part ( b ) of theorem [ convergencerate ] @xmath107    next , let us explain once again why the minorization condition in the harris theorem does not yield accurate estimates in high dimension ( see diaconis _ et al . _",
    "@xcite for similar discussions based on concrete examples ) .",
    "indeed assume that @xmath6 is finite , then under reasonable assumption on @xmath108 , there are constant @xmath109 and a probability measure @xmath110 such that @xmath111 ( i.e. , almost the best minorization that one can obtain in the dependent case ) .",
    "hence by the doeblin theorem ( the ancestor of the harris theorem ) , @xmath112 so one requires at least an exponential number @xmath113 of steps for the right - hand side becoming small .",
    "our estimate of the convergence rate is much better in high dimension , that is the good point of theorem [ convergencerate ] .",
    "the weak point of theorem [ convergencerate ] is that our result depends on the dobrushin uniqueness condition , even in low dimension .",
    "if @xmath2 is small , the results in @xcite are already good enough . particularly the estimates of diaconis , khare and saloff - coste @xcite for the special space of two different components in bayesian statistics are sharp .",
    "we should indicate that the dobrushin uniqueness condition is quite natural for the exponential convergence of @xmath114 to @xmath0 with the rate @xmath115 independent of @xmath2 as in this theorem , since the dobrushin uniqueness condition is well known to be sharp for the phase transition of mean field models @xcite .",
    "finally , our tool ( dobrushin s interdependence coefficients ) is completely different from those in the known works .",
    "as indicated by a referee , it would be very interesting to investigate the convergence rate problem under the more flexible dobrushin  shlosman analyticity condition ( i.e. , box version of dobrushin uniqueness condition , reference @xcite ) , but in that case we feel that we should change the algorithm : instead of @xmath116 , one uses the conditional distribution @xmath117 of @xmath118 knowing @xmath119 where @xmath120 is a box containing @xmath92 .    a much more classical topic is glauber dynamics associated with the gibbs measures in finite or infinite volume .",
    "we are content here to mention only zegarlinski @xcite , martinelli and olivieri @xcite , and the lecture notes of martinelli @xcite for a great number of references",
    ".    the convergence rate estimate above will be our starting point for computing the mean @xmath121 , that is , to approximate @xmath121 by the empirical mean @xmath122 .",
    "[ concentrationinequalities ] assume @xmath123 and for some constant @xmath124 , @xmath125 ( recall that @xmath126 for the discrete metric @xmath88 . )",
    "then for any lipschitzian function @xmath95 on @xmath1 with @xmath127 , we have :    a.   @xmath128 b.   furthermore if holds , @xmath129\\\\[-8pt ] & & \\quad\\leq\\exp\\biggl\\{-\\frac { t^{2}(1 - 2r_{1})^{2}n}{2c_{1}\\alpha^{2 } n } \\biggr\\}\\qquad \\forall t>0,n\\geq1,\\nonumber\\end{aligned}\\ ] ] where @xmath130    in conclusion under the conditions of this theorem , when @xmath131 , the empirical means @xmath122 will approximate to @xmath121 exponentially rapidly in probability with the speed @xmath132 , with the bias not greater than @xmath133 .",
    "the speed @xmath132 is the correct one , as will be shown in the remark below .",
    "we do not know whether the concentration inequality with the speed @xmath132 still holds under the more natural dobrushin s uniqueness condition @xmath134 .",
    "we know only that @xmath135 does not imply that @xmath30 is contracting in the metric @xmath106 , see the example in remark [ remar3.3 ] .",
    "consider @xmath136 where @xmath137 is @xmath38-lipschitzian with @xmath138 ( the observable of this type is often used in statistical mechanics ) .",
    "since @xmath139 , the inequality ( [ thm2a ] ) implies for all @xmath140 , @xmath141 which is of speed @xmath142 .",
    "let us show that the concentration inequality ( [ thm2a ] ) is sharp .",
    "in fact in the free case , that is , @xmath143 does not depend upon @xmath144 and @xmath92 , and @xmath0 is the product measure @xmath145 . in this case",
    "@xmath146 , in other words @xmath147 is a sequence of independent and identically distributed ( i.i.d . in short ) random variables valued in @xmath35 , of common law @xmath0 .",
    "since @xmath148 in the free case , the concentration inequality ( [ thm2d ] ) is equivalent to the transport inequality for @xmath149 , by gozlan - lonard @xcite .",
    "that shows also the speed @xmath132 in theorem [ concentrationinequalities ] is the correct one .",
    "we explain now why we do not apply directly the nice concentration results of joulin and ollivier @xcite for general mcmc .",
    "in fact under the condition that @xmath150 , we can prove that @xmath151 ( by lemma [ 1-norm ] ) .",
    "in other words the ricci curvature in @xcite is bounded from below by @xmath152 unfortunately we can not show that the ricci curvature @xmath153 is positive in the case where @xmath154 .    if @xmath77 is unbounded , the results of @xcite , theorems 4 and 5 , do not apply here , because their granularity constant @xmath155 explodes .",
    "assume now that @xmath77 is bounded .",
    "if we apply the results ( @xcite , theorems 4 and 5 ) and their notations , their coarse diffusion constant @xmath156 is of order @xmath157 ; and their local dimension @xmath158 is of order @xmath2 ( by lemma [ transine ] below ) , and their granularity constant @xmath159 is of order @xmath2 . setting @xmath160 theorem 4 in @xcite says that if @xmath161 , @xmath162 for all @xmath163 and @xmath164 .",
    "so for small deviation @xmath165 , their result yields the same order gaussian concentration inequality , but for large deviation @xmath165 , their estimate is only exponential , not gaussian as one may expect in this bounded case . in @xcite ,",
    "theorem 5 , they get a same type gaussian - exponential concentration inequality with @xmath166 depending upon the starting point @xmath167 .    anyway the key lemmas in this paper are necessary for applying the results of @xcite to this particular model .    for the gibbs measure @xmath0 on @xmath168 ,",
    "marton @xcite established the talagrand transport inequality @xmath169 on @xmath168 equipped with the euclidean metric , under the dobrushin ",
    "shlosman analyticity type condition .",
    "the second named author @xcite proved @xmath87 for @xmath0 on @xmath35 equipped with the metric @xmath44 , under . but those transport inequalities are for the equilibrium distribution  @xmath0 , not for the gibbs sampling which is a markov chain with @xmath0 as invariant measure .",
    "however our coupling is very close to that of k. marton .",
    "for @xmath170-mixing sequence of dependent random variables , rio @xcite and samson @xcite established accurate concentration inequalities , see also djellout , guillin and wu @xcite and the recent works by paulin @xcite and wintenberger @xcite for generalizations and improvements . in the markov chain case @xmath170-mixing",
    "means the doeblin uniform ergodicity .",
    "if one applies the results in @xcite to the gibbs sampling , one obtains the concentration inequalities with the speed @xmath171 , where @xmath172 when holds with the discrete metric @xmath38 , @xmath173 is actually finite but it is of order @xmath2 by theorem  [ convergencerate ] ( and its remarks ) .",
    "the concentration inequalities so obtained from @xcite are of speed @xmath174 , very far from the correct speed @xmath132 .",
    "when @xmath95 depends on a very small number of variables , since @xmath175 does not reflect the nature of such observable , one can imagine that our concentration inequalities do not yield the correct speed .",
    "in fact in the free case and for @xmath176 , the correct speed must be @xmath177 , not @xmath132 .",
    "for this type of observable , one may use the metric @xmath178 which reflects much better the number of variables in such observable .",
    "the ideas in marton @xcite should be helpful .",
    "that will be another history .",
    "given any two initial distributions @xmath53 and @xmath54 on @xmath1 , we begin by constructing our coupled non - homogeneous markov chain @xmath179 , which is quite close to the coupling by marton @xcite .",
    "let @xmath180 be a coupling of @xmath99 . and given @xmath181 then @xmath182 and @xmath183 where @xmath184 is an optimal coupling of @xmath8 and @xmath185 such that @xmath186 define the partial order on @xmath187 by @xmath188 if and only if @xmath189 .",
    "then , by ( [ dobrushin ] ) , we have for @xmath190 , @xmath191 \\cr \\vdots \\cr \\vdots \\cr \\mathbb{e } \\bigl[d\\bigl(x_{kn+i}^{n},y_{kn+i}^{n } \\bigr)|x_{kn+i-1 } , y_{kn+i-1}\\bigr ] } \\leq b_{i } \\pmatrix { d \\bigl(x_{kn+i-1}^{1},y_{kn+i-1}^{1}\\bigr ) \\cr \\vdots",
    "\\cr \\vdots \\cr d\\bigl(x_{kn+i-1}^{n},y_{kn+i-1}^{n } \\bigr)},\\ ] ] where @xmath192    therefore by iterations , we have @xmath193    let @xmath194 then we have the following lemma .    [ infty - norm ] under , @xmath195 .",
    "we use the probabilistic method . under we",
    "can construct markov chain @xmath196 , taking values in @xmath197 where @xmath198 is an extra point representing the cemetery , and write as follows : @xmath199 where the transition matrix from @xmath200 to @xmath201 is @xmath202 , more precisely for @xmath203 , @xmath204 here @xmath205 if @xmath206 and @xmath207 otherwise ( kronecker s symbol ) .",
    "then @xmath208 for any @xmath61 , when @xmath209 , we have @xmath210 .",
    "therefore , @xmath211 and thus @xmath212 so @xmath213 .      by ( [ matrix ] ) above , markov property and iterations , @xmath214    let @xmath215 , then by lemma [ infty - norm ] @xmath216 now the results of this theorem follow quite easily from this inequality .",
    "in fact ,    \\(a ) for any lipschitzian function @xmath90 , @xmath217 & \\le & r^k \\max_{1\\le i\\le n}{\\mathbb{e}}d\\bigl(z_0^i(1 ) , z_0^i(2)\\bigr ) \\sum_{i=1}^n \\delta_i(f),\\end{aligned}\\ ] ] where the last inequality follows by ( [ maxine ] ) . that is ( [ a1 ] ) .",
    "\\(b ) now for @xmath218 , @xmath219 , as @xmath220 , we have @xmath221 the desired result .",
    "we begin with    [ 1-norm ] if @xmath222 ( i.e. , ) , then for the matrix @xmath223 given in ( [ q ] ) , @xmath224 in particular @xmath225    the last conclusion ( [ q2 ] ) follows from ( [ q1 ] ) and ( [ matrix ] ) .",
    "we show now ( [ q1 ] ) .    by the definition of @xmath226",
    ", it is not difficult to verify for @xmath227 , @xmath228 here we make the convention @xmath229 .",
    "this can be obtained again by the markov chain @xmath230 valued in @xmath231 constructed in lemma [ infty - norm ] .",
    "since @xmath232 for all @xmath92 , @xmath233 : that is the first line in the expression of @xmath223 .",
    "now for @xmath234 , as @xmath235 and if @xmath236 and @xmath237 , then @xmath238 and so @xmath239 this implies the expression of @xmath223 above by induction .",
    "thus for @xmath240 , @xmath241 where the last inequality holds because for fixed @xmath242 and @xmath243 , @xmath244 so the proof of ( [ q1 ] ) is completed .",
    "[ remar3.3 ] let @xmath0 be the gaussian distribution on @xmath245 with mean @xmath207 and the covariance matrix @xmath246 where @xmath247 .",
    "we have @xmath248 ( i.e. , and both hold ) ; and under @xmath249 , @xmath250 and @xmath251 are i.i.d .",
    "gaussian random variables with mean @xmath207 and variance @xmath252 . hence , @xmath253 and since @xmath254 , @xmath255 , @xmath256= \\bigl(r+r^2\\bigr ) \\bigl|x_2-x'_2\\bigr| .",
    "\\ ] ] thus , @xmath257 and the ricci curvature @xmath153 is positive if and only if @xmath258 .",
    "in other words , though we have missed many terms in the proof above , the estimate of @xmath259 can not be qualitatively improved .",
    "[ transine ] assume and , then @xmath260    the proof is similar to the one used by djellout , guillin and wu @xcite , theorem 2.5 .",
    "first for simplicity denote @xmath261 by @xmath30 and note that for @xmath262 , @xmath263and thus @xmath264 for any probability measure @xmath223 on @xmath1 such that @xmath265 , let @xmath266})$ ] be the regular conditional law of @xmath10 knowing @xmath267}$ ] , where @xmath268}=(x^{1},\\ldots , x^{i-1})$ ] , and @xmath266})$ ] the law of @xmath269 for @xmath270 , all under law @xmath223 .",
    "define @xmath271})$ ] similarly but under @xmath30 .",
    "we shall use the kullback information between conditional distributions , @xmath272}\\bigr)=h\\bigl(q_{i}\\bigl ( \\cdot|y^{[1,i-1]}\\bigr)|p_{i}\\bigl(\\cdot|y^{[1,i-1]}\\bigr ) \\bigr)\\ ] ] and exploit the following important identity : @xmath273}\\bigr){\\,\\mathrm{d}}q(y).\\ ] ] the key is to construct an appropriate coupling of @xmath223 and @xmath30 , that is , two random sequences @xmath274}$ ] and @xmath275}$ ] taking values on @xmath1 distributed according to @xmath223 and @xmath30 , respectively , on some probability space @xmath276 .",
    "we define a joint distribution @xmath277},x^{[1,n]})$ ] by induction as follows ( the marton coupling ) .    at first",
    "the law of @xmath278 is the optimal coupling of @xmath279 and @xmath280 @xmath281 .",
    "assume that for some @xmath282},x^{[1,i-1]})= ( y^{[1,i-1]},x^{[1,i-1]})$ ] is given .",
    "then the joint conditional distribution @xmath283}=y^{[1,i-1]},x^{[1,i-1]}=x^{[1,i-1]})$ ] is the optimal coupling of @xmath284})$ ] and @xmath285})$ ] , that is , @xmath286}=y^{[1,i-1]},x^{[1,i-1]}=x^{[1,i-1 ] } \\bigr ) = w_{1,d}\\bigl(q_{i}\\bigl(\\cdot|y^{[1,i-1 ] } \\bigr),p_{i}\\bigl(\\cdot|x^{[1,i-1]}\\bigr)\\bigr).\\ ] ] obviously , @xmath274},x^{[1,n]}$ ] are of law @xmath287 , respectively . by the triangle inequality for the @xmath72 distance , @xmath288}=y^{[1,i-1]},x^{[1,i-1]}=x^{[1,i-1]}\\bigr ) \\\\ & &",
    "\\quad\\leq w_{1,d}\\bigl(q_{i}\\bigl(\\cdot|y^{[1,i-1 ] } \\bigr),p_{i}\\bigl(\\cdot|y^{[1,i-1]}\\bigr)\\bigr)+ w_{1,d } \\bigl(p_{i}\\bigl(\\cdot|y^{[1,i-1]}\\bigr),p_{i}\\bigl ( \\cdot|x^{[1,i-1]}\\bigr)\\bigr ) \\\\ & & \\quad\\leq\\sqrt{2c_{1}h_{i}\\bigl(y^{[1,i-1]}\\bigr)}+\\sum _ { j=1}^{i-1}c_{ij}d \\bigl(x^{j},y^{j}\\bigr).\\end{aligned}\\ ] ] by recurrence on @xmath92",
    ", this entails that @xmath289 for all @xmath61 . taking the average with respect to @xmath290},x^{[1,i-1]})$ ] , summing on @xmath92 and using jessen s inequality",
    ", we have @xmath291})}{n}}+ \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{i-1}c_{ij}\\mathbb { e}d(y^{j},x^{j})}{n } \\\\ & = & \\sqrt{\\frac{2c_{1}h(q|p)}{n}}+\\frac{\\sum_{j=1}^{n-1}\\mathbb { e}d(y^{j},x^{j})\\sum_{i = j+1}^{n}c_{ij}}{n } \\\\ & \\leq&\\sqrt{\\frac{2c_{1}h(q|p)}{n}}+\\frac{r_1 \\sum_{j=1}^{n}\\mathbb{e}d(y^{j},x^{j})}{n}\\end{aligned}\\ ] ] the above inequality gives us @xmath292 that is , @xmath293 .",
    "theorem [ concentrationinequalities ] is based on the following dependent tensorization result of djellout , guillin and wu @xcite .",
    "[ lemdgw ] let @xmath294 be a probability measure on the product space @xmath295 .",
    "for any @xmath296}:=(x_{1},\\ldots , x_{k})$ ] .",
    "let @xmath297})$ ] denote the regular conditional law of @xmath298 given @xmath299}$ ] under @xmath294 for @xmath300 , and @xmath297})$ ] be the distribution of @xmath301 for @xmath302 .",
    "assume that :    1 .",
    "for some metric @xmath38 on @xmath6 , @xmath303 for all @xmath304}\\in e^{k-1}$ ] ; 2 .",
    "there is some constant @xmath305 such that for all real bounded lipschitzian function @xmath306 with @xmath307 , for all @xmath308 , @xmath309}=x_{[1,k ] } \\bigr)-\\mathbb{e}_{\\mathbb{p}}\\bigl(f(x_{k+1},\\ldots , x_{n } )    & & \\quad \\leq sd(x_{k},y_{k}).\\end{aligned}\\ ] ]    then for all function @xmath83 on @xmath310 satisfying @xmath311 , we have @xmath312 equivalently , @xmath313 on @xmath314 with @xmath315    we are now ready to prove theorem [ concentrationinequalities ] .",
    "proof of theorem [ concentrationinequalities ] we will apply lemma [ lemdgw ] with @xmath77 being @xmath316 , and @xmath317 be the law of @xmath318 on @xmath319 .    by ( [ matrixineq ] ) , lemma [ 1-norm ] and the condition that @xmath150 , the constant @xmath173 in lemma [ lemdgw ] is bounded from above by @xmath320    take @xmath321 , then the lipschitzian norm @xmath322 of @xmath83 w.r.t .",
    "the @xmath323 ( for @xmath324 ) is not greater than @xmath325 .",
    "thus by lemmas [ lemdgw ] and [ transine ] , @xmath326 so , by the classic approach , firstly using chebyshev s inequality , and then optimizing over @xmath327 , we obtain the desired part @xmath328 in theorem [ concentrationinequalities ] .    furthermore by theorem [ convergencerate ] , we have @xmath329 thus , we obtain part @xmath330 in theorem [ concentrationinequalities ] from its part ( a ) .",
    "supported in part by thousand talents program of the chinese academy of sciences and le projet anr evol .",
    "we are grateful to the two referees for their suggestions and references , which improve sensitively the presentation of the paper ."
  ],
  "abstract_text": [
    "<S> the objective of this paper is to study the gibbs sampling for computing the mean of observable in very high dimension  a powerful markov chain monte carlo method . under the dobrushin s uniqueness condition </S>",
    "<S> , we establish some explicit and sharp estimate of the exponential convergence rate and prove some gaussian concentration inequalities for the empirical mean . </S>"
  ]
}