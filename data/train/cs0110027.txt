{
  "article_text": [
    "we present a method of constructing and applying a cascade consisting of a left- and a right - sequential finite - state transducer ( fst ) , @xmath0 and @xmath1 , for part - of - speech ( pos ) disambiguation .    in the process of pos",
    "tagging , we first assign every word of a sentence a unique ambiguity class @xmath2 that can be looked up in a lexicon encoded by a sequential fst .",
    "every @xmath2 is denoted by a single symbol , e.g. `` '' , although it represents a set of alternative tags that a given word can occur with .",
    "the sequence of the @xmath2 of all words of one sentence is the input to our fst cascade ( fig .",
    "[ f - sequences ] ) .",
    "it is mapped by @xmath0 , from left to right , to a sequence of reduced ambiguity classes @xmath3 .",
    "every @xmath3 is denoted by a single symbol , although it represents a set of alternative tags .",
    "intuitively , @xmath0 eliminates the less likely tags from @xmath2 , thus creating @xmath3 .",
    "finally , @xmath1 maps the sequence of @xmath3 , from right to left , to an output sequence of single pos tags @xmath4 .",
    "intuitively , @xmath1 selects the most likely @xmath4 from every @xmath3 ( fig .",
    "[ f - sequences ] ) .    .... ...   [ det relpro ] [ adj noun ] [ adj noun verb ] [ verb ]   ... ....    ' '' ''    @xmath5    ' '' ''    ' '' ''    _ mapping left to right _    ' '' ''    @xmath5    .... ...   [ det relpro ]     [ adj ]       [ adj noun ]    [ verb ]   ... ....    ' '' ''    @xmath5    ' '' ''    ' '' ''    ' '' ''    @xmath5    .... ...       det           adj           noun        verb    ... ....    compared to a hidden markov model ( hmm ) @xcite , this fst cascade has the advantage of significantly higher processing speed , but at the cost of slightly lower accuracy .",
    "applications such as information retrieval , where the speed can be more important than accuracy , could benefit from this approach .",
    "although our approach is related to the concept of bimachines @xcite and factorization @xcite , we proceed differently in that we build two sequential fsts directly and not by factorization .",
    "this article is structured as follows .",
    "section  [ s - classes ] describes how the ambiguity classes and reduced ambiguity classes are defined based on a lexicon and a training corpus .",
    "then , section  [ s - probs ] explains how the probabilities of these classes in the context of other classes are calculated .",
    "the construction of @xmath0 and @xmath1 is shown in section  [ s - construct ] .",
    "it makes use of the previously defined classes and their probabilities .",
    "section  [ s - applic ] describes the application of the fsts to an input text , and section  [ s - results ] finally compares the fsts to an hmm tagger , based on experimental data .",
    "instead of dealing with lexical probabilities of individual words @xcite , many pos taggers group words into _ ambiguity classes _ and deal with lexical probabilities of these classes @xcite .",
    "every word belongs to one ambiguity class that is described by the set of all pos tags that the word can occur with .",
    "for example , the class described by \\{noun , verb } includes all words that could be analyzed either as noun or verb depending on the context .",
    "we follow this approach .",
    "some approaches make a more fine - grained word classification @xcite .",
    "words that occur with the same alternative tags , e.g. , noun and verb , can here be assigned different ambiguity classes depending on whether they occur more frequently with one or with the other tag .",
    "although this has proven to increase the accuracy of hmm - based pos disambiguation , it did not significantly improve our method .",
    "after some investigations in this direction , we decided to follow the simpler classification above .",
    "+ before we can build the fst cascade , we have to define ambiguity classes , that will constitute the input alphabet of @xmath0 , and _ reduced ambiguity classes _ , that will form the intermediate alphabet of the cascade , i.e. , the output of @xmath0 and the input of @xmath1 .",
    "ambiguity classes @xmath2 are defined from the training corpus and lexicon , and are each described by a pair consisting of a tag list @xmath6 and a probability vector @xmath7 : @xmath8 }      \\label{e - ac}\\ ] ]    for example : @xmath9 }      \\label{e - ac - exm1}\\ ] ]    which means that the words that belong to @xmath10 are tagged as adj in 29  % , as noun in 60  % , and as verb in 11  % of all cases in the training corpus .    when all @xmath2 are defined , a class - based lexicon , that maps every word to a single class symbol , is constructed from the original tag - based lexicon , that maps every word to a set of alternative tag symbols . in the class - based lexicon , the above @xmath10 ( eq .  [ e - ac - exm1 ] ) could be represented , e.g. , by the symbol `` '' .",
    "+ we describe a reduced ambiguity classes @xmath3 also by a pair consisting of a tag list @xmath11 and a probability vector @xmath12 . intuitively , an @xmath3 can be seen as a @xmath2 where some of the less likely tags have been removed . since at this point",
    "we can not decide which tags are less likely , all possible subclasses of all @xmath2 are considered . to generate a complete set of @xmath3 , all @xmath2 are split into all possible subclasses @xmath13",
    "that are assigned a tag list @xmath14 containing a subset of the tags of @xmath6 , and an ( un - normalized ) probability vector @xmath15 containing only the relevant elements of @xmath7 .",
    "for example , the above @xmath10 ( eq .  [ e - ac - exm1 ] ) is split into seven subclasses @xmath16  : @xmath17 }                  \\nonumber    \\\\",
    "\\hat t(s_{1,1 } ) \\ ; = & \\ ! { \\langle{{\\tt noun},{\\tt verb}}\\rangle }    { \\rule{13ex}{0 mm } }    \\vec p(s_{1,1 } ) & \\ ! = \\ ; { \\left[\\begin{array}{c}{0.60}\\\\ { 0.11}\\end{array}\\right ] }      \\label{e - rc - exm }                 \\\\",
    "\\hat t(s_{1,2 } ) \\ ; = & \\ ! { \\langle{{\\tt adj},{\\tt verb}}\\rangle }    { \\rule{14ex}{0 mm } }    \\vec p(s_{1,2 } ) & \\ ! = \\ ; { \\left[\\begin{array}{c}{0.29}\\\\ { 0.11}\\end{array}\\right ] }                  \\nonumber    \\\\ { \\mit etc . } { \\rule{8ex}{0 mm } } & &    \\nonumber\\end{aligned}\\ ] ]    different @xmath2 can produce a @xmath13 with the same tag list @xmath14 but with different probability vectors @xmath18 ; e.g. , the classes with the tag lists @xmath19 , @xmath20 , and @xmath21 can all produce a subclass with the tag list @xmath22 . to reduce the total number of subclasses , all @xmath13 with the same tag list @xmath14",
    "are clustered , based on the _ centroid method _",
    "@xcite , using the _ vector cosine _ as the similarity measure between clusters @xcite .",
    "each final cluster constitutes a reduced ambiguity class @xmath23 .",
    "if we obtain , e.g. , three @xmath23 with the same tag list @xmath24 but with different ( re - normalized ) probability vectors : @xmath25 }     { \\rule{5ex}{0 mm } } \\vec p(r_2 ) = { \\left[\\begin{array}{c}{0.57}\\\\ { 0.43}\\end{array}\\right ] }     { \\rule{5ex}{0 mm } } \\vec p(r_3 ) = { \\left[\\begin{array}{c}{0.09}\\\\ { 0.91}\\end{array}\\right ] }      \\label{e - ri - probvec - exm}\\ ] ]    we represent them in an fst by three different symbols , e.g. , `` '' , `` '' , and `` '' .",
    "@xmath0 will map a sequence of @xmath2 , from left to right , to a sequence of @xmath3 .",
    "therefore , the construction of @xmath0 requires estimating the most likely @xmath3 in the context of both the current @xmath2 and the previous @xmath26 ( wrt . the current position @xmath27 in a sequence ) . to determine this @xmath3 ,",
    "a probability @xmath28 is estimated for every pos tag @xmath29 in @xmath30 . in the initial position",
    ", @xmath28 depends on the preceding sentence boundary @xmath31 and the current @xmath2 which are assumed to be mutually independent : @xmath32    the latter @xmath33 can be extracted from the probability vector @xmath34 , and @xmath35 and @xmath36 can be estimated from the training corpus .    in another than the initial position , @xmath28 depends on the preceding @xmath26 and the current @xmath2 which are assumed to be mutually independent : @xmath37    the latter @xmath38 is estimated by : @xmath39    where @xmath40 can be estimated from the training corpus , and @xmath41 can be extracted from the probability vector @xmath42 of the preceding @xmath26 .    to evaluate all tags of the current @xmath30 , a list @xmath43 containing pairs @xmath44 of all tags @xmath29 of @xmath2 with their probabilities @xmath28 ( eq.s  [ e - pt1-init ] ,  [ e - pt1-normal ] ) ,",
    "is created : @xmath45    every tag @xmath29 in @xmath46 is compared to the most likely tag @xmath47 in @xmath46 .",
    "if the ratio of their probabilities is below a threshold @xmath48 , @xmath29 is removed from @xmath46  : @xmath49    removing less likely tags leads to a reduced list @xmath50 that is then split into a reduced tag list @xmath51 and a reduced probability vector @xmath52 that jointly describe a reduced ambiguity class @xmath53 . from among all predefined @xmath3 ( cf .",
    "e.g. eq .  [ e - ri - probvec - exm ] ) , we select the one that has the same tag list @xmath11 as the `` ideal '' reduced class @xmath23 and the most similar probability vector @xmath54 according to the cosine measure . this @xmath3 is considered to be the most likely among all predefined @xmath3 in the context of both the current @xmath2 and the previous @xmath26 .",
    "+ @xmath1 will map a sequence of @xmath3 , from right to left , to a sequence of tags @xmath4 .",
    "therefore , the construction of @xmath1 requires estimating the most likely @xmath4 in the context of both the current @xmath3 and the following @xmath55 . to determine this @xmath4 ,",
    "a probability @xmath56 is estimated for every tag @xmath29 of the current @xmath57 . in the final position",
    ", @xmath56 depends on the current @xmath3 and on the following sentence boundary @xmath58 : @xmath59    in another than the final position , @xmath56 depends on the current @xmath3 and the following tag @xmath60 : @xmath61    the latter @xmath36 , @xmath62 , and @xmath63 are estimated from the training corpus , and @xmath64 is extracted from the probability vector @xmath12 .",
    "the @xmath4 with the highest probability @xmath65 is the most likely tag in the context of both the current @xmath3 and the following @xmath55 ( eq.s  [ e - pt2-init ] ,  [ e - pt2-normal ] ) .",
    "the construction of @xmath0 is preceded by defining all @xmath2 and @xmath3 , and estimating their contextual probabilities . in this process ,",
    "all words in the training corpus , that are initially annotated with pos tags , are in addition annotated with ambiguity classes @xmath2 .    in @xmath0",
    ", one state is created for every @xmath3 ( output symbol ) , and is labeled with this @xmath3 ( fig .  [",
    "f - t1]a ) .",
    "an initial state , not corresponding to any @xmath3 , is created in addition . from every state ,",
    "one outgoing arc is created for every @xmath2 ( input symbol ) , and is labeled with this @xmath2 .",
    "the destination of every arc is the state of the most likely @xmath3 in the context of both the current @xmath2 ( arc label ) and the preceding @xmath26 ( source state label ) which is estimated as described above .",
    "all arc labels are then changed from simple symbols @xmath2 to symbol pairs @xmath2:@xmath3 ( mapping @xmath2 to @xmath3 ) that consist of the original arc label and the destination state label .",
    "all state labels are removed ( fig .",
    "[ f - t1]b ) .",
    "those @xmath3 that are unlikely in any context disappear from @xmath0 because the corresponding states have no incomming arcs . @xmath0 accepts any sequence of @xmath2 and maps it , from left to right , to the sequence of the most likely @xmath3 in the given left context .",
    "( a )    ' '' ''     [ f - t1 ] ]    ' '' ''    ( b )    ' '' ''     [ f - t1 ] ]    the construction of @xmath1 is preceded by annotating the training corpus in addition with reduced ambiguity classes @xmath3 , by means of @xmath0 .",
    "the probability vectors @xmath54 of all @xmath3 are then re - estimated .",
    "the contextual probabilities of tags , are estimated only at this point ( eq.s  [ e - pt2-init ] ,  [ e - pt2-normal ] ) .    in @xmath1",
    ", one state is created for every @xmath4 ( output symbol ) , and is labeled with this @xmath4 ( fig .",
    "[ f - t2]a )",
    ". an initial state is added . from every state ,",
    "one outgoing arc is created for every @xmath3 ( input symbol ) that occurs in the output language of @xmath0 , and is labeled with this @xmath3 .",
    "the destination of every arc is the state of the most likely @xmath4 in the context of both the current @xmath3 ( arc label ) and the following @xmath55 ( source state label ) which is estimated as described above .",
    "note , this is the following tag , rather than the preceding , because @xmath1 will be applied from right to left .",
    "all arc labels are then changed into symbol pairs @xmath3:@xmath4 and all state labels are removed ( fig .",
    "[ f - t2]b ) , as was done in @xmath0 .",
    "@xmath1 accepts any sequence of @xmath3 , generated by @xmath0 , and maps it , from right to left , to the sequence of the most likely @xmath4 in the given right context .",
    "[ f - t2 ] ]    ' '' ''    ( a )    ' '' ''     [ f - t2 ] ]    ' '' ''    ( b )    both @xmath0 and @xmath1 are sequential . they can be minimized with standard algorithms . once @xmath0 and @xmath1",
    "are built , the probabilities of all @xmath4 , @xmath3 , and @xmath2 are of no further use .",
    "probabilities do not explicitly occur in the fsts , and are not directly used at run time .",
    "they are , however , `` reflected '' by the structure of the fsts .",
    "our fst tagger uses the above described @xmath0 and @xmath1 , a class - based lexicon , and possibly a guesser to predict the ambiguity classes of unknown words ( possibly based on their suffixes ) .",
    "the lexicon and guesser are also sequential fsts , and map any word that they accept to a single symbol @xmath2 representing an ambiguity class ( fig .",
    "[ f - sequences ] ) .",
    "if a word can not be found in the lexicon , it is analyzed by the guesser .",
    "if this does not provide an analysis either , the word is assigned a special @xmath2 for unknown words that is estimated from the _ m _ most frequent tags of all words that occur only once in the training corpus .",
    "the sequence of the @xmath2 of all words of one sentence is the input to our fst cascade ( fig .",
    "[ f - sequences ] ) .",
    "it is mapped by @xmath0 , from left to right , to a sequence of reduced ambiguity classes @xmath3 .",
    "intuitively , @xmath0 eliminates the less likely tags from @xmath2 , thus creating @xmath3 .",
    "finally , @xmath1 maps the sequence of @xmath3 , from right to left , to an output sequence of single pos tags @xmath4 .",
    "intuitively , @xmath1 selects the most likely @xmath4 from every @xmath57 .",
    "we compared our fst tagger on english , german , and spanish with a commercially available ( foreign ) hmm tagger ( table  [ t - spd - acc - sz ] ) .",
    "the comparison was made on the same non - overlapping training and test corpora for both taggers ( table  [ t - corpora ] ) .",
    "the fst tagger was on average 10 times as fast but slightly less accurate than the hmm tagger ( 45  600 words / sec and 96.97% versus 4  360 words / sec and 97.43% ) . in some applications such as information retrieval",
    "a significant speed increase could be worth the small loss in accuracy .",
    "speed  ( words / sec ) & @xmath66 & 47  600 & 42  200 & 46  900 & 45  600 + & hmm & 4  110 & 3  620 & 5  360 & 4  360 + accuracy  ( % ) & @xmath66 & 96.54 & 96.79 & 97.05 & 96.97 + & hmm & 96.80 & 97.55 & 97.95 & 97.43 +       # states & 615 & 496 & 353 & 488 + # arcs & 209  000 & 197  000 & 96  000 & 167  000 + # tags & 76 & 67 & 56 & 66 + # ambiguity classes & 349 & 448 & 265 & 354 + # reduced ambiguity classes & 724 & 732 & 465 & 640 +      xx church , k.  w.(1988 ) , a stochastic parts program and noun phrase parser for unrestricted text , _ proceedings of the 2nd conference on applied natural language processing ( anlp ) _ , association for computational linguistics , austin , tx , usa , pp .  136143 .",
    "cutting , d. , kupiec , j. , pedersen , j. and sibun , p.(1992 ) , a practical part - of - speech tagger , _ proceedings of the 3rd conference on applied natural language processing ( anlp ) _ , association for computational linguistics , trento , italy , pp .",
    "daelemans , w. , zavrel , j. , berck , p. and gillis , s.(1996 ) , mbt : a memory - based part - of - speech tagger - generator , _ proceedings of the 4th workshop on very large corpora _ , special interest group for linguistic data and corpus - based approaches ( sigdat ) of the acl , copenhagen , denmark , pp",
    ".  1427 .",
    "elgot , c.  c. and mezei , j.  e.(1965 ) , on relations defined by generalized finite automata , _ ibm journal of research and development _ pp .",
    "kupiec , j.  m.(1992 ) , robust part - of - speech tagging using a hidden markov model , _ computer , speech , and language _ * 6*(3 ) ,  225242 .",
    "rabiner , l.  r.(1990 ) , a tutorial on hidden markov models and selected applications in speech recognition , _ in _",
    "a.  waibel and k .- f .",
    "lee ( eds ) , _ readings in speech recognition _ , morgan kaufmann , pp .",
    "romesburg , h.  c.(1989 ) , _ cluster analysis for researchers _",
    ", krieger publishing company , malabar , fl , usa . salton , g. and mcgill , m.  j.(1983 ) , _ introduction to modern information retrieval _ , mcgraw - hill advanced computer science series , mcgraw - hill publishing company , new york , usa . schtzenberger , m.  p.(1961 ) , a remark on finite transducers , _ information and control _ * 4 * ,  185187 .",
    "tzoukermann , e. and radev , d.  r.(1996 ) , using word class for part - of - speech disambiguation , _ proceedings of the 4th workshop on very large corpora _ , special interest group for linguistic data and corpus - based approaches ( sigdat ) of the acl , copenhagen , denmark , pp"
  ],
  "abstract_text": [
    "<S> the article presents a method of constructing and applying a cascade consisting of a left- and a right - sequential finite - state transducer , @xmath0 and @xmath1 , for part - of - speech disambiguation . in the process of pos </S>",
    "<S> tagging , every word is first assigned a unique ambiguity class that represents the set of alternative tags that this word can occur with . </S>",
    "<S> the sequence of the ambiguity classes of all words of one sentence is then mapped by @xmath0 to a sequence of reduced ambiguity classes where some of the less likely tags are removed . </S>",
    "<S> that sequence is finally mapped by @xmath1 to a sequence of single tags . </S>",
    "<S> compared to a hidden markov model tagger , this transducer cascade has the advantage of significantly higher processing speed , but at the cost of slightly lower accuracy . </S>",
    "<S> applications such as information retrieval , where the speed can be more important than accuracy , could benefit from this approach . </S>"
  ]
}