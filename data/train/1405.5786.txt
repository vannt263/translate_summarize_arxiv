{
  "article_text": [
    "empirical likelihood ( el ) is a powerful and currently widely used approach for developing efficient nonparametric statistics .",
    "the el was introduced by owen ( 1988 ) and since then many papers have appeared on this topic making varied contributions to different inferential problems . in this approach , the parameters are usually defined as functionals of the unknown population distribution .    the first purpose of this paper is to introduce a new family of empirical test statistics as an alternative to the likelihood ratio test statistic proposed by qin and lawless ( 1994 ) for testing a simple null hypothesis . as an extension of the empirical likelihood ratio test of qin and lawless ( 1995 ) , a new family of empirical test statistics",
    "is also considered here for composite hypothsis .",
    "this new family of empirical test statistics is based on divergence measures .",
    "consider @xmath0-variate i.i.d .",
    "random vectors @xmath1 with unknown distribution function @xmath2 , and a @xmath3-dimensional parameter , @xmath4 , associated with @xmath2 having finite mean and non - singular covariance matrix .",
    "we assume that all the information about @xmath5 and @xmath2 is available in the form of @xmath6 functionally independent unbiased estimating functions , through the functions @xmath7 , @xmath8 , such that @xmath9   = 0 $ ] . in vector notation",
    ", we have @xmath10 , such that@xmath11 = \\boldsymbol{0}_{r}. \\label{estf}\\ ] ] we shall assume that for each realization @xmath12 of @xmath13 , @xmath14 @xmath15 is a vector - valued function and the @xmath16 matrix @xmath17 exists .",
    "this formulation is as in qin and lawless ( 1994 ) , but a little different from that of owen ( 1988 , 1990 ) .",
    "the essential difference is that owen considered @xmath18 instead of @xmath6 .",
    "for example , by taking into account that @xmath19   = \\theta$ ] , we can adopt steps similar to the method of moments to obtain estimators through the estimating function @xmath20 of a univariate distribution .",
    "in addition , if we assume @xmath21   = 2\\theta^{2}+1 $ ] , the other estimating function is @xmath22 , and in this case we have @xmath23 .",
    "let @xmath24  be a realization of @xmath25 .",
    "the empirical likelihood function is then given by @xmath26 where @xmath27 .",
    "only distributions with an atom of probability at each @xmath28 have non - zero likelihood , and without consideration of estimating functions ,  the empirical likelihood function @xmath29 is seen to be maximized , at @xmath30 , by the empirical distribution function@xmath31 which is associated with the @xmath32-dimensional discrete uniform distribution@xmath33 now , let@xmath34 be an empirical distribution function associated with the probability vector@xmath35 and @xmath36 be the kernel of the empirical log - likelihood function , @xmath37 . if we are interested in maximizing @xmath38 subject to the restrictions defined by the estimating functions based on @xmath39 given by @xmath40 we obtain , by applying the lagrange multipliers method , @xmath41 where @xmath42 is an @xmath43-dimensional vector to be determined by solving the non - linear system of @xmath43 equations,@xmath44 subject to ( [ cond ] ) and ( [ empf2 ] ) . thus , the kernel of the empirical log - likelihood function is@xmath45   .",
    "\\label{elf}\\ ] ]    one of the important results  of qin and lawless ( 1994 ) is that the empirical likelihood ratio test statistic for testing@xmath46 is given by @xmath47 where @xmath48 is the empirical maximum likelihood estimator of the parameter @xmath5 obtained by maximizing @xmath36 in ( [ elf ] ) . in particular ,",
    "if @xmath18 , it can be seen that @xmath49 , @xmath50 and @xmath48 is the solution of the system of @xmath43 equations @xmath51 , where@xmath52 subject to ( [ cond ] ) and ( [ empf2 ] ) .",
    "furthermore , if @xmath18 , @xmath53 and @xmath54 . the asymptotic properties of @xmath48 , when @xmath6 , were studied by qin and lawless ( 1994 ) .",
    "in fact , the asymptotic distribution of @xmath55 in ( [ 1.1 ] ) is chi - square with @xmath3 degrees of freedom .    here , we propose a new family of test statistics for testing the hypotheses in ( [ h ] ) , based on @xmath56-divergence measures , and then derive their asymptotic distribution .",
    "this new family of empirical test statistics is referred to hereafter as empirical @xmath56-divergence test statistics . in section [ sec2 ] , the asymptotic null distribution of the empirical @xmath56-divergence test statistics is derived .",
    "then , two power approximations of the empirical @xmath56-divergence test statistics are presented in section [ sec3 ] .",
    "an illustrative example is presented in section [ example ] . in section [ simulation ] , a monte carlo simulation study is carried out to compare its performance with respect to the empirical likelihood ratio test when confidence intervals are constructed based on the respective statistics for small simple sizes .",
    "the results show that the empirical @xmath56-divergence test statistic is competitive in terms of power when compared to the empirical likelihood ratio test statistic , and moreover is more robust than the empirical likelihood ratio test statistic in the presence of contamination in the data .",
    "in section [ seccomp ] , we propose empirical phi - divergence test statistics for testing a composite null hypothesis and present some asymptotic as well as simulation results to evaluate the performance of these test procedures .",
    "finally , in section [ lastsec ] , we make some concluding remarks .",
    "the kullback - leibler divergence measure between the probability vectors , @xmath57 and @xmath58 , is given by @xmath59 in the sequel , we shall denote it by @xmath60 , since the above expression is the distance , in the sense of the  kullback - leibler divergence , between the distributions functions @xmath61 and @xmath62 . using this notation ,",
    "it is clear that the empirical likelihood ratio test statistic in ( [ 1.1 ] ) can be expressed as @xmath63 or equivalently @xmath64 where @xmath65 with @xmath66 , @xmath67 .",
    "now , we shall denote by @xmath68 the class of all convex functions @xmath69 such that at @xmath70 , @xmath71 , @xmath72 , and at @xmath73 , @xmath74 and @xmath75 . instead of @xmath66 ,",
    "if we consider a function @xmath56 belonging to @xmath68 , we obtain a new family of test statistics for testing ( [ h ] ) given by @xmath76 we will refer to this family of test statistics as empirical @xmath56-divergence test statistics .    for every @xmath77 that is differentiable at @xmath70",
    ", the function @xmath78 also belongs to @xmath68 .",
    "then , we have @xmath79 and @xmath80 has the additional property that @xmath81 since the two divergence measures are equivalent , we can consider the set @xmath68 to be equivalent to the set @xmath82 . in",
    "what follows , we shall assume that @xmath83 .",
    "the statistics in ( [ f1 ] ) have been considered recently by broniatowski and keziou ( 2012 ) to give some empirical test statistics , generalizing an important subfamily of test statistics introduced by baggerly ( 1998 ) .",
    "we present more details in section [ simulation ] in this regard .",
    "the main purpose of this paper is to present a new family of test statistics for testing the hypotheses in ( [ h ] ) based on the @xmath56-divergence measure between @xmath84 and @xmath85 , namely , @xmath86 .",
    "we shall consider the empirical family of @xmath56-divergence test statistics given by @xmath87 where @xmath56 is a function satisfying the same conditions as function @xmath56 used to construct @xmath88 .",
    "observe that ( [ f1 ] ) and ( [ f2 ] ) are equivalent only when @xmath18 .",
    "it is well - known that the family of test statistics based on @xmath56-divergence has some nice and optimal properties for different inferential problems , and especially in relation to robustness ; see pardo ( 2006 ) and basu et al .",
    "in this section , we derive the asymptotic distribution of @xmath89 . for this purpose ,",
    "the asymptotic distribution of the maximum empirical likelihood estimator of the parameter @xmath5 , @xmath90 as well as the asymptotic distribution of @xmath91 are important@xmath92 these asymptotic distributions are given in qin and lawless ( 1994 ) , for example .",
    "under some regularity assumptions ( see lemma 1 and theorem 1 of qin and lawless ( 1994 ) ) , we have@xmath93 where @xmath94  denotes convergence in law and@xmath95",
    "with@xmath96   , \\label{s11}\\\\ \\boldsymbol{s}_{12}\\left (   \\boldsymbol{\\theta}_{0}\\right )    &   = e_{f}\\left [ \\boldsymbol{g}_{\\boldsymbol{x}}(\\boldsymbol{\\theta}_{0})\\right ] , \\quad\\boldsymbol{s}_{21}\\left (   \\boldsymbol{\\theta}_{0}\\right ) = \\boldsymbol{s}_{12}\\left (   \\boldsymbol{\\theta}_{0}\\right )   ^{t}. \\label{s12}\\ ] ] this result is derived from : a ) @xmath97 where @xmath98 is given by ( [ g_bar ] ) .",
    "it is clear from the central limit theorem that @xmath99 and so @xmath100;b ) @xmath101 , where @xmath102  is given by ( [ v2 ] ) and so @xmath103.in addition , @xmath48 and @xmath104 are asymptotically uncorrelated .    [ lem1]the influence function of the empirical maximum likelihood estimator of parameter @xmath5 , @xmath48 , is given by@xmath105    it follows from the expression given in ( [ thetahat ] ) and taking into account the definition of the influence function given in formula ( 20.1 ) of van der vaart ( 2000 , page 292 ) .",
    "the empirical maximum likelihood estimator of parameter @xmath5 , @xmath48 , is obtained maximizing the kernel of empirical log - likelihood function given in ( [ elf ] ) or equivalently minimizing the function @xmath106 , subject to the restrictions given in ( [ res ] ) .",
    "this expression can be written as the @xmath56-divergence measure between the probability vectors @xmath57 and @xmath107 , i.e. @xmath108 with @xmath109 .",
    "therefore,@xmath110 subject to the restrictions given in ( [ res ] ) .",
    "if  we consider a general function @xmath83 , defined in section [ sec2a ] , instead of considering @xmath66 , then we can define the empirical minimum @xmath56-divergence estimator by @xmath111 subject to the restrictions given in ( [ res ] ) .",
    "the empirical exponential tilting estimator ( et ) , considered for instance in schennach ( 2007 ) , is defined by @xmath112 , subject to the restrictions given in ( [ res ] ) .",
    "the et is another member of this family of estimators since @xmath113 with @xmath114 .",
    "the asymptotic properties of @xmath115 and @xmath48 are the same ( for more details , see ragusa ( 2011 ) , broniatowski and keziou ( 2012 ) and schennach ( 2007 ) ) .",
    "the fisher consistence of @xmath115 was established in ragusa ( 2011 )",
    ". therefore , all the asymptotic results obtained for the test statistics considered in this paper are valid replacing @xmath48 by @xmath115 .",
    "the expression given in ( [ if ] ) , for the influence function of the empirical maximum likelihood estimator of parameter @xmath5 , can be found for the empirical minimum @xmath56-divergence estimators , @xmath116 , in proposition 2.3 of toma ( 2013 ) .",
    "hence , all the estimators based on @xmath56-divergence measures , independently of the @xmath56 function , share the same influence function .",
    "let @xmath117 denote any vector or matrix norm .",
    "we shall assume the following regularity conditions :    1 .",
    "@xmath118 in ( [ s11 ] ) is positive definite , and for @xmath119 in ( [ s12 ] ) , @xmath120 .",
    "2 .   there exists a neighbourhood of @xmath121 , in which @xmath122 is bounded by some integrable function .",
    "there exists a neighbourhood of @xmath121 , in which @xmath123 , given in ( [ g ] ) , is continuous and @xmath124 is bounded by some integrable function .",
    "there exists a neighbourhood of @xmath121 in which @xmath125 is continuous and @xmath126 is bounded by some integrable function .    the asymptotic distribution of the empirical @xmath56-divergence test statistics , @xmath127 , is given in the following theorem .",
    "[ th1]under @xmath128 in ( [ h ] ) and the assumptions i)-iv ) above , we have@xmath129    based on the taylor expansions of ( [ f2 ] ) , as well as the asymptotic properties of @xmath48 , given in qin and lawless ( 1994 ) , it holds@xmath130 with @xmath131 , according to ( [ dist ] ) .    there are some measures of divergence which can not be expressed as a @xmath132-divergence measure such as the divergence measures of bhattacharya ( 1943 ) , rnyi ( 1961 ) , and sharma and mittal ( 1977 ) .",
    "however , such measures can be written in the form@xmath133 where @xmath134 is a differentiable increasing function mapping from @xmath135 onto @xmath136 , with @xmath137 , @xmath138 , and @xmath83 . in table",
    "[ t1 ] , these divergence measures are presented , along with the corresponding expressions of @xmath134 and @xmath56 .    0.8pt @xmath139{ccccc}\\hline divergence & \\hspace*{0.5 cm } & $ ] h ( x ) @xmath140 ( x ) @xmath141^{\\frac{b-1}{a-1}}-1\\right\\ }   , \\quad b , a\\neq1 $ } &   & \\multicolumn{1}{l}{$\\frac{x^{a}-a\\left (   x-1\\right ) -1}{a\\left (   a-1\\right )   } , \\quad a\\neq0,1$}\\\\ \\multicolumn{1}{l}{battacharya } &   & \\multicolumn{1}{l}{$-\\log\\left ( -x+1\\right )   $ } &   & \\multicolumn{1}{l}{$-x^{1/2}+\\frac{1}{2}\\left ( x+1\\right )   $ } \\\\\\hline \\end{tabular } \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ ]    in the case of rnyi s divergence , we have @xmath142 and its limit  cases corresponding to @xmath143 and @xmath144 as @xmath145 and @xmath146 respectively .",
    "the @xmath147-divergence measures were introduced in menndez et al .",
    "( 1995 ) and some associated asymptotic results were established in menndez et al .",
    "( 1997 ) .",
    "[ th3]under the assumptions of theorem [ th1 ] , the asymptotic null distribution of the family of empirical test statistics @xmath148 is chi - squared with @xmath3 degrees of freedom .    [ remark]note that we can also consider the family of empirical @xmath147-divergence test statistics given by@xmath149 and their asymptotic distribution is chi - squared with @xmath3 degrees of freedom as well .",
    "test - statistics @xmath150 and @xmath151 , include as particular cases @xmath88 and @xmath127 , taking @xmath152 . in the same way done for @xmath127 in ( [ taylors ] ) , it may be concluded  that all of them have a taylor expansion of second order which does not depend on @xmath56 or @xmath134,@xmath153 with @xmath154 . with regard to their influence function , a second order influence function",
    "have to be considered since the first order one vanishes ( see van der vaart ( 2000 , section 20.1.1 ) ) , and its expression is given by@xmath155 the first and second equality come from the previous second order taylor expansion , the third one from heritier and ronchetti ( 1994 ) and the last one from lemma [ lem1 ] .",
    "it suggests that under the simple null hypothesis , all members of both family of test - statistics have the same infinitesimal robustness .",
    "this is not a strange result , since in lemma 1 of toma ( 2009 ) a similar conclusion has been reached based on power divergence measures for parametric models .",
    "based on the null distribution presented in theorem [ th1 ] , we reject the null hypothesis in ( [ h ] ) in favour of the alternative hypothesis , if @xmath156 , where @xmath157 is the @xmath158-th quantile of the chi - squared distribution with @xmath3 degrees of freedom .    in most cases , the power function of this test procedure can not be derived explicitly . in the following theorem , we present an asymptotic result , which provides an approximation of the power of the test .    [",
    "th5]let @xmath159 be the true parameter value with @xmath160 . under the assumptions of theorem [ th1 ] and @xmath161   < \\infty$ ]",
    ", we have@xmath162 where @xmath163 with @xmath164 and the matrix @xmath165  as defined in ( [ v ] ) .    a first - order taylor expansion gives@xmath166 but , @xmath167 and so @xmath168",
    "thus , the random variables @xmath169 have the same asymptotic distribution , and hence the desired result .",
    "[ rempostth5]it is straightforward to extend theorem [ th5 ] for @xmath147-divergence measures @xmath170@xmath171 , as follows . since @xmath172",
    ", we have @xmath173 and so@xmath174 where@xmath175 @xmath176 and @xmath177  as defined in theorem [ th5 ] .    [ beta1]from theorem [ th5 ]",
    ", we can present a first approximation of the power function , at @xmath160 , of the test based on @xmath56-divergence measure , as@xmath178 with @xmath179 being the standard normal distribution function.if some alternative @xmath160 is the true parameter , then the probability of rejecting @xmath121 with the rejection rule @xmath156 , for fixed significance level @xmath180 , tends to one as @xmath181 .",
    "thus , the test is consistent in the sense of fraser ( 1957 ) .",
    "similarly , from theorem [ th5 ] , we obtain a first approximation of the power function , at @xmath182 , of the test based on @xmath183-divergence measure as @xmath184    to produce some less trivial asymptotic powers that are not all equal to @xmath185 , we can use a pitman - type local analysis , as developed by le cam ( 1960 ) , by confining attention to @xmath186-neighborhoods of the true parameter values . a key tool to get the asymptotic distribution of the statistic @xmath89 under such a contiguous hypothesis is le cam s third lemma , as presented in hjek and sidk ( 1967 ) .",
    "instead of relying on these results , we present in the following theorem a proof which is easy and direct to follow . this proof is based on the results of morales and pardo ( 2001 ) . specifically , we consider the power at contiguous alternatives of the form @xmath187 where @xmath188 is a fixed vector in @xmath189 such that @xmath190 .",
    "[ th1b]under the assumptions of theorem [ th1 ] and @xmath191 in ( [ cont ] ) , we have the asymptotic distribution of the empirical @xmath132-divergence test statistic @xmath127 to be non - central chi - squared with @xmath3 degrees of freedom and non - centrality parameter@xmath192 where @xmath193    we can write@xmath194 under @xmath191 , we have @xmath195 and @xmath196 in theorem [ th1 ] , it has been shown that @xmath197 on the other hand , we have@xmath198 we thus obtain@xmath199 with @xmath200 as in ( [ ncp ] ) .",
    "notice that @xmath201 and @xmath202 have the same asymptotic distribution under the contiguous alternative hypothesis in ( [ cont ] ) .",
    "hence , it is straightforward to extend theorem [ th1b ] for @xmath150 and to conclude that both test statistics , @xmath150 and @xmath127 , have the same asymptotic distribution under the contiguous alternative hypothesis .",
    "[ beta2]using theorem [ th1b ] and the preceding remark , we obtain a second approximation to the power function , at @xmath203 , as @xmath204 where @xmath205 is the distribution function of @xmath206.if we want to approximate the power at the alternative @xmath207 , then we can take @xmath208 .",
    "it should be noted that this approximation is independent of the function @xmath56 in @xmath209 , as well as of the @xmath134 function .",
    "in this section , we make use of a well - known data to illustrate the application of the empirical phi - divergence test statistics for testing a simple null hypothesis . in 1882 ,",
    "the astronomer and mathematician simon newcomb , measured the time required for a light signal to pass from his laboratory on the potomac river to a mirror at the base of the washington monument and back , a distance of @xmath210 meters .",
    "table [ tnewcomb ] contains these measurements from three samples , as deviations from @xmath211 nanoseconds .",
    "for example , for the first observation , @xmath212 represents the @xmath213 nanoseconds that were spent for the light to travel the required @xmath210 meters .",
    "the data contain three samples , of sizes @xmath214 , @xmath214 and @xmath215 , respectively , corresponding to three different days .",
    "they have been analyzed previously by a number of authors including stigler ( 1973 ) and voinov et al .",
    "( 2013 ) .",
    "our interest here is in obtaining confidence intervals for the mean , @xmath216 , of the passage time of light , @xmath217 .",
    "this means that a unique estimating function , @xmath218 , is required  for a univariate random variable ( @xmath219 ) .",
    "note that @xmath220 , and so @xmath221 .",
    "based on the results of theorem [ th3 ] , we shall use the empirical rnyi test statistics@xmath222{ll}\\frac{2n}{a\\left (   a-1\\right )   } \\log{\\textstyle\\sum\\limits_{i=1}^{n } } \\frac{1}{n}\\left (   1+t(\\mu_{0})(x_{i}-\\mu_{0})\\right )   ^{a-1 } , & a\\neq0,1,\\\\ -2{\\textstyle\\sum\\limits_{i=1}^{n } } \\dfrac{\\log\\left (   1+t(\\mu_{0})(x_{i}-\\mu_{0})\\right )   } { 1+t(\\mu_{0})(x_{i}-\\mu_{0 } ) } , & a=0,\\\\ 2{\\textstyle\\sum\\limits_{i=1}^{n } } \\log\\left (   1+t(\\mu_{0})(x_{i}-\\mu_{0})\\right )   & a=1 , \\end{array } \\right .   \\label{renyiex}\\ ] ] where @xmath223 is the solution of the equation @xmath224 subject to @xmath225 and @xmath226 .",
    "2.8pt @xmath139{ccccccccccccccccccccccccccc}\\cline{1 - 21}day 1 & $ ] 28@xmath22726@xmath22733@xmath22724@xmath22734@xmath227 - 44@xmath22727@xmath22716@xmath22740@xmath227 - 2@xmath22829@xmath22722@xmath22724@xmath22721@xmath22725@xmath22730@xmath22723@xmath22729@xmath22731@xmath22719@xmath22924@xmath22720@xmath22736@xmath22732@xmath22736@xmath22728@xmath22725@xmath22721@xmath22728@xmath22729@xmath22837@xmath22725@xmath22728@xmath22726@xmath22730@xmath22732@xmath22736@xmath22726@xmath22730@xmath22722@xmath23036@xmath22723@xmath22727@xmath22727@xmath22728@xmath22727@xmath22731@xmath22727@xmath22726@xmath22733@xmath22826@xmath22732@xmath22732@xmath22724@xmath22739@xmath22728@xmath22724@xmath22725@xmath22732@xmath22725@xmath22729@xmath22827@xmath22728@xmath22729@xmath22716@xmath22723@xmath231    a confidence interval , with confidence level @xmath232 , based on ( [ renyiex ] )  is given by@xmath233 with @xmath234 being the ( right hand ) quantile of order @xmath180 for the @xmath235  distribution . by taking different choices for the parameter @xmath236 as @xmath237",
    ", we obtain different confidence intervals , and this is done for each of the three days .",
    "table [ tci ] summarizes these results obtained at @xmath238 confidence level .",
    "if the confidence interval based on the likelihood ratio test statistic , i.e. , @xmath239 are compared with @xmath240 , we observe that narrower confidence intervals are obtained for @xmath241 , and wider confidence intervals are obtained for @xmath242 .",
    "this behavior is more evident for the first day , due to the presence of some outliers in the sample ; see for example , bernett and lewis ( 1993 ) .",
    "next , we approximate the power function , assuming that @xmath243 is a value very close to @xmath244 . despite the fact that the exact significance may not be approximated well",
    "( since the true value @xmath243 is unknown ) , the power values can provide meaningful information on the quality of performance of the different rnyi test statistics .",
    "2.8pt @xmath139{clcccccc}\\hline & $ ] ci_0.95 ^ -1()@xmath227ci_0.95 ^ -0.5()@xmath227ci_0.95 ^ 0()@xmath228ci_0.95 ^ 0.5()@xmath227ci_0.95 ^ 1()@xmath227ci_0.95 ^ 1.5()@xmath228ci_0.95 ^ 2.5()@xmath245(13.65,27.25)@xmath228(10.85,27.11)@xmath227(12.00,27.24)@xmath227(12.92,27.24)@xmath227(9.46,26.97)@xmath228(6.90,26.60)@xmath246(26.47,30.70)@xmath228(26.40,30.78)@xmath227(26.43,30.74)@xmath227(26.45,30.71)@xmath227(26.35,30.83)@xmath228(26.24,30.97)@xmath247(26.27,29.48)@xmath228(26.00,29.70)@xmath227(26.10,29.61)@xmath227(26.20,29.54)@xmath227(25.88,29.79)@xmath228(25.62,30.01)@xmath248    in order to calculate the power function by using the first approximation@xmath249 we need to obtain @xmath250 in terms of@xmath251{ll}\\frac{1}{a\\left (   a-1\\right )   d_{\\phi_{a}}\\left (   f_{n,\\mu^{\\ast}},f_{n,\\mu_{0}}\\right )   + 1 } , & a\\left (   a-1\\right )   \\neq0,\\\\ 1 , & a\\left (   a-1\\right )   = 0 , \\end{array } \\right . \\\\ &   = \\left (   \\sum\\limits_{i=1}^{n}p_{i}^{a}(\\mu^{\\ast})p_{i}^{1-a}(\\mu _ { 0})\\right )   ^{-1},\\end{aligned}\\]]@xmath252@xmath253{ll}\\dfrac{n}{1-a}{\\displaystyle\\sum\\limits_{i=1}^{n } } \\left [   t^{\\prime}(\\mu^{\\ast})(x_{i}-\\mu^{\\ast})-t(\\mu^{\\ast})\\right ] p_{i}^{a+1}(\\mu^{\\ast})p_{i}^{1-a}(\\mu_{0 } ) , & a\\neq1,\\\\ n{\\displaystyle\\sum\\limits_{i=1}^{n } } \\left [   t^{\\prime}(\\mu^{\\ast})(x_{i}-\\mu^{\\ast})-t(\\mu^{\\ast})\\right ] p_{i}^{2}(\\mu^{\\ast})\\left (   \\log\\frac{p_{i}(\\mu_{0})}{p_{i}(\\mu^{\\ast})}-1\\right )   , & a=1 , \\end{array } \\right.\\]]@xmath254 and@xmath255   .\\ ] ] since @xmath256 is unknown , it can be replaced by a consistent estimator@xmath257 so that@xmath258 where@xmath259@xmath260{ll}-\\frac{1}{a\\left (   a-1\\right )   } \\log\\kappa_{\\text{\\textrm{r\\'{e}nyi}}}^{a}\\left (   \\mu_{0},\\mu^{\\ast}\\right )   , & a\\left (   a-1\\right )   \\neq0,\\\\ \\sum_{i=1}^{n}p_{i}(\\mu_{0})\\log\\frac{p_{i}(\\mu_{0})}{p_{i}(\\mu^{\\ast } ) } , & a=0,\\\\ \\sum_{i=1}^{n}p_{i}(\\mu^{\\ast})\\log\\frac{p_{i}(\\mu^{\\ast})}{p_{i}(\\mu_{0 } ) } , & a=1 . \\end{array } \\right.\\ ] ] in figure [ fig000 ] , the power functions are plotted for the three days , for three different values of the parameter @xmath261 .",
    "we do observe from these plots that the most powerful test statistics are for the case when @xmath242 , which is in fact the case when the confidence intervals are wider .",
    "notice that there are two outliers on day 1 ( @xmath262 and @xmath263 ) and one possible outlier on day 3 ( @xmath264 ) ; see stigler ( 1973 ) and barnett and lewis ( 1994 ) . by dropping these outliers , we reproduced the plots of approximated power functions for days 1 and 3 , and we found that in the new plots the approximated power functions are more similar to that of day 2 . in the presence of outliers ( days 1 and 3 )",
    ", the comparison of shape of the power function for different values provides an interesting information : the power function is flatter for @xmath265 , with @xmath266 in comparison with @xmath144 , @xmath267 .",
    "this tends to be associated with higher capacity of the empirical likelihood ratio test ( @xmath268 , with @xmath144 ) for detecting samples not fulfilling the null hypothesis , but a lack of robustness in the presence of outliers . in section [ simulation2 ]",
    ", a simulation study is carried out to examine this robustness aspect .",
    "@xmath139{cc}\\raisebox{-0cm}{\\includegraphics [ natheight=2.491500 in , natwidth=3.746400 in , height=4.815 cm , width=7.2071 cm ] { fig3.pdf } } & \\raisebox{1.0032in}{\\fbox{\\includegraphics [ natheight=0.677200 in , natwidth=1.152800 in , height=0.7083 in , width=1.1865 in ] { fig1.pdf } } } \\\\{\\includegraphics [ natheight=2.491500 in , natwidth=3.746400 in , height=1.8957 in , width=2.8374 in ] { fig2.pdf } } & { \\includegraphics [ natheight=2.491500 in , natwidth=3.746400 in , height=1.8957 in , width=2.8374 in ] { fig6.pdf } } \\\\{\\includegraphics [ natheight=2.491500 in , natwidth=3.746400 in , height=1.8957 in , width=2.8374 in ] { fig4.pdf } } & { \\includegraphics [ natheight=2.491500 in , natwidth=3.746400 in , height=1.8957 in , width=2.8374 in ] { fig7.pdf } } \\end{tabular } $ ]",
    "in this section , we will pay special attention to a subfamily of @xmath132-divergence measures in ( [ eqdiv ] ) , the so - called power divergence measures ( see cressie and read ( 1984 ) ) for which @xmath269 is given by@xmath270   , \\quad\\text{if } \\lambda(1+\\lambda)\\neq 0,\\label{pd0}\\\\ \\phi_{0}(x )   &   = \\lim_{\\lambda\\rightarrow0}\\phi_{\\lambda}(x)=x\\log x - x+1,\\nonumber\\\\ \\phi_{-1}(x )   &   = \\lim_{\\lambda\\rightarrow-1}\\phi_{\\lambda}(x)=-\\log x+x-1.\\nonumber\\end{aligned}\\ ] ] in this case , ( [ f1 ] ) can be expressed as@xmath271 this family of test statistics include several commonly used tests such as the empirical modified kullback - leibler statistic ( @xmath272 , with @xmath273 ) , the empirical freeman - tukey statistic ( @xmath274 , with @xmath275 ) , the empirical likelihood ratio test statistic ( @xmath272 , with @xmath276 ) , the empirical cressie - read statistic ( @xmath272 , with @xmath277 ) , and the empirical pearson s chi - square statistic ( @xmath272 , with @xmath278 ) .",
    "similarly , ( [ f2 ] ) can be expressed as @xmath279 the @xmath43-dimensional vectors @xmath280 and @xmath104  are obtained by solving ( [ ec ] ) with @xmath121  and @xmath48  being specified .",
    "we now examine , through monte carlo simulations , the performance of the confidence intervals ( cis ) obtained through the empirical power divergence test statistics in ( [ pd1 ] ) and ( [ pd2 ] ) for the choices @xmath281 , when the sample sizes are small , @xmath282 , and the nominal confidence levels are @xmath283 and @xmath238 . even though the asymptotic distribution is equivalent for the different empirical divergence based statistics , in practice we need to work with finite sample sizes and it is therefore important to compare the cis based on @xmath272 and @xmath284 , in different scenarios .",
    "we focus on the coverage probability and average width of the cis of a mean based on @xmath285 simulated samples under the same setting as example 1 of qin and lawless ( 1994 ) . in this case , a sample of univariate ( @xmath219 ) i.i.d .",
    "random variables , @xmath286 , of size @xmath32 was considered , with mean @xmath287 and variance @xmath288 , i.e. , @xmath289 and@xmath290    0.8pt @xmath139{llllccccccc } &   &   &   & \\multicolumn{7}{c}{$\\mathcal{n}(\\theta,\\theta^{2}+1)$ , $ \\theta _ { 0}=0$}\\\\\\hline & \\hspace*{0.35 cm } &   & \\hspace*{0.35 cm } & \\multicolumn{3}{c}{$t_{n}^{\\lambda } ( \\widehat{\\theta}_{e , n},\\theta_{0})$ } & \\hspace*{0.35 cm } & \\multicolumn{3}{c}{$s_{n}^{\\lambda}(\\widehat{\\theta}_{e , n},\\theta_{0})$}\\\\\\cline{4 - 7}\\cline{9 - 11 } &   &   &   & cov ( \\% ) & \\hspace*{0.35 cm } & avw &   & cov ( \\% ) & \\hspace*{0.35 cm } & avw\\\\\\hline &   & \\multicolumn{1}{r}{$\\lambda$ } &   & \\multicolumn{7}{c}{$n=30$}\\\\ $ ] 90%@xmath29195%@xmath29290%@xmath29395%@xmath29490%@xmath29595%@xmath296{llllccccccc } &   &   &   & \\multicolumn{7}{c}{$\\mathcal{n}(\\theta,\\theta^{2}+1)$ , $ \\theta _ { 0}=1$}\\\\\\hline & \\hspace*{0.35 cm } &   & \\hspace*{0.35 cm } & \\multicolumn{3}{c}{$t_{n}^{\\lambda } ( \\widehat{\\theta}_{e , n},\\theta_{0})$ } & \\hspace*{0.35 cm } & \\multicolumn{3}{c}{$s_{n}^{\\lambda}(\\widehat{\\theta}_{e , n},\\theta_{0})$}\\\\\\cline{4 - 7}\\cline{9 - 11 } &   &   &   & cov ( \\% ) & \\hspace*{0.35 cm } & avw &   & cov ( \\% ) & \\hspace*{0.35 cm } & avw\\\\\\hline &   & \\multicolumn{1}{r}{$\\lambda$ } &   & \\multicolumn{7}{c}{$n=30$}\\\\ $ ] 90%@xmath29795%@xmath29890%@xmath29995%@xmath30090%@xmath30195%@xmath302    in the present study , we have considered an additional sample of size @xmath303 , compared to those of qin and lawless ( 1994 ) . for constructing the test statistic ,",
    "an unknown distribution of @xmath304 is assumed , but for this simulation study , normally distributed random variables were taken with @xmath305 .",
    "it is important to mention here that baggerly ( 1998 ) studied the coverage probabilities for the normal distribution , but only for the family of statistics in ( [ pd1 ] ) .",
    "the results of the simulation study are summarized in table [ t2 ] and figure [ fig1 ] for the empirical power divergence test statistics in ( [ pd1 ] ) and ( [ pd2 ] ) for two different samples when the nominal confidence level is @xmath238 . in figure [ fig1 ]",
    ", it can be seen that the confidence intervals based on both statistics , @xmath306  and @xmath307 , with small values of @xmath281 tend to be narrower in width , and this is also seen in the simulation results of table [ t2 ] .",
    "in fact , the simulation study of baggerly showed that @xmath308 had the worst coverage probability , and this is also seen in the present simulation study .",
    "this is in accordance with the results obtained for two specific samples analyzed in figure [ fig1 ] .",
    "the smallest average width of confidence intervals is obtained for @xmath309 , which is the so - called empirical modified likelihood ratio test statistic or empirical minimum discrimination information statistic ( see gokhale and kullback , 1978 ) , and this might explain , when @xmath310 , the low coverage probability as well . often , the coverage probability closest to the nominal level is also obtained for the same test statistics , @xmath309 when @xmath311 but the empirical likelihood ratio test has a closer coverage probability when @xmath310 .",
    "when both characteristics of a ci , viz . , average width and coverage probability , are taken into account , the empirical modified likelihood ratio test statistic , @xmath309 , is a good compromise and it turns out to be the best for @xmath310 in all the cases considered .",
    "finally , another important feature seen in figure [ fig1 ] is that even though the support of the asymptotic distribution of the test statistics is strictly positive , it is possible to find samples for which @xmath306 is negative , while @xmath312 is always strictly positive .",
    "this usually happens when @xmath313  is very close to @xmath314 .",
    "@xmath139{cc}\\multicolumn{2}{c}{{\\fbox{\\includegraphics [ natheight=1.218500 in , natwidth=1.540200 in , height=1.0637 in , width=1.337 in ] { fig9.pdf } } } } \\\\{\\includegraphics [ natheight=2.965400 in , natwidth=4.483200 in , height=2.252 in , width=3.3909 in ] { fig8.pdf } } & { \\includegraphics [ natheight=2.965400 in , natwidth=4.483200 in , height=2.252 in , width=3.3909 in ] { fig12.pdf } } \\\\{\\includegraphics [ natheight=2.965400 in , natwidth=4.483200 in , height=2.252 in , width=3.3909 in ] { fig10.pdf } } & { \\includegraphics [ natheight=2.965400 in , natwidth=4.483200 in , height=2.252 in , width=3.3909 in ] { fig14.pdf } } \\end{tabular } \\ $ ]      we conducted a simulation study using the same design as in the preceding subsection , but by considering @xmath315 of shifted observations in each sample : @xmath267 shifted observations out of @xmath316 in the sample , @xmath317 shifted observations out of @xmath318 , @xmath319 shifted observations out of @xmath320 .",
    "the underlying distribution for shifted observations is taken to be @xmath321 , given that @xmath322 is the true distribution .",
    "therefore , the shifted observations follow @xmath323 distribution when the true distribution is @xmath324 , while they follow @xmath325 distribution when the true distribution is @xmath326 . in table",
    "[ t2b ] , the results of introducing @xmath315 of shifted observations is shown .",
    "our interest is focused on identifying the statistics that are less sensitive ( robust statistics with respect to shifted observations ) as well as those that are quite sensitive ( non - robust statistics with respect to shifted observations ) . as pointed out in remark [ remark ] ,",
    "all the test - statistics have the same infinitesimal robustness , so the differences in robustness could be atributable to the magnitude of the shift .",
    "when @xmath310 , the best statistics with shifted observations are exactly the same as ones without shifted observations , but when the dispersion of the data is higher , i.e. @xmath311 , the closest coverage probability to the nominal level is not for the empirical likelihood ratio test ; we further observe that @xmath306 and @xmath307 not only have the narrowest cis but also the highest coverage probabilities .",
    "this means that the empirical modified likelihood ratio test statistic , @xmath327 , is a robust statistic with respect to shifted observations in comparison to the empirical likelihood ratio test , with respect to the coverage probability and width of cis .",
    "this also agrees with the conclusion obtained from table [ tci ] .",
    "0.8pt @xmath139{llllccccccc } &   &   &   & \\multicolumn{7}{c}{$\\mathcal{n}(\\theta,\\theta^{2}+1)$ , $ \\theta _ { 0}=0$}\\\\\\hline & \\hspace*{0.35 cm } &   & \\hspace*{0.35 cm } & \\multicolumn{3}{c}{$t_{n}^{\\lambda } ( \\widehat{\\theta}_{e , n},\\theta_{0})$ } & \\hspace*{0.35 cm } & \\multicolumn{3}{c}{$s_{n}^{\\lambda}(\\widehat{\\theta}_{e , n},\\theta_{0})$}\\\\\\cline{4 - 7}\\cline{9 - 11 } &   &   &   & cov ( \\% ) & \\hspace*{0.35 cm } & avw &   & cov ( \\% ) & \\hspace*{0.35 cm } & avw\\\\\\hline &   & \\multicolumn{1}{r}{$\\lambda$ } &   & \\multicolumn{7}{c}{$n=30$}\\\\ $ ] 90%@xmath32895%@xmath32990%@xmath33095%@xmath33190%@xmath33295%@xmath333{llllccccccc } &   &   &   & \\multicolumn{7}{c}{$\\mathcal{n}(\\theta,\\theta^{2}+1)$ , $ \\theta _ { 0}=1$}\\\\\\hline & \\hspace*{0.35 cm } &   & \\hspace*{0.35 cm } & \\multicolumn{3}{c}{$t_{n}^{\\lambda } ( \\widehat{\\theta}_{e , n},\\theta_{0})$ } & \\hspace*{0.35 cm } & \\multicolumn{3}{c}{$s_{n}^{\\lambda}(\\widehat{\\theta}_{e , n},\\theta_{0})$}\\\\\\cline{4 - 7}\\cline{9 - 11 } &   &   &   & cov ( \\% ) & \\hspace*{0.35 cm } & avw &   & cov ( \\% ) & \\hspace*{0.35 cm } & avw\\\\\\hline &   & \\multicolumn{1}{r}{$\\lambda$ } &   & \\multicolumn{7}{c}{$n=30$}\\\\ $ ] 90%@xmath33495%@xmath33590%@xmath33695%@xmath33790%@xmath33895%@xmath339",
    "now , let us consider the problem of testing the composite null hypothesis@xmath340 the rest of this section , proceeds as follows . in section [ seccomp1 ] , we introduce the empirical phi - divergence test statistic for the composite null hypothesis in ( [ 2.1.3 ] ) .",
    "section [ seccomp2 ] is devoted to the asymptotic results . in section [ seccomp3 ] , we present some results regarding the power function of the family of empirical test statistics proposed here . in section [ seccomp4 ] ,",
    "a simulation study is carried out to evaluate the performance of the proposed test procedure in comparison to some other tests .      in the following",
    ", we shall assume as in qin and lawless ( 1995 ) , that the unknown parameter vector @xmath5 is defined through @xmath18 estimating functions , given in ( [ estf ] ) .",
    "with regard to ( [ 2.1.3 ] ) , we shall assume that @xmath341 @xmath342 is a vector - valued function such that the @xmath343 matrix @xmath344@xmath345 exists and is continuous at @xmath5 and that @xmath346@xmath344@xmath347 ( @xmath348 ) .",
    "we denote by @xmath349 the empirical restricted maximum likelihood estimator of @xmath5 , obtained by minimizing @xmath350   $ ] , subject to @xmath351 and @xmath352@xmath353 , i.e. , ( [ ec ] ) or@xmath354 where @xmath355 provides @xmath42 in terms of @xmath356 . upon using the lagrange multiplier method , once again",
    ", we have@xmath357 where @xmath358 is a @xmath359-dimensional vector of lagrange multipliers , and differentiating @xmath360 with respect to @xmath5 and @xmath358 , we obtain@xmath361 therefore , @xmath362 is obtained as the solution of ( [ seq2 ] ) and @xmath363 and conditions ( [ cond ] ) and ( [ empf2 ] ) must be also satisfied .",
    "the empirical likelihood ratio test for testing ( [ 2.1.3 ] ) has the expression @xmath364 where @xmath48 is the empirical maximum likelihood estimator of the parameter @xmath5 , defined in section [ sec1 ]  ( case @xmath18 ) , for which @xmath365 .",
    "taking into account the results stated in section [ sec1 ] for @xmath18 , we have @xmath366 , and so it is easy to show that the expression in ( [ lrt2 ] ) can be written as @xmath367 defining a family of empirical phi - divergence test statistics for testing the hypothesis in ( [ 2.1.3 ] ) , we have@xmath368 where the @xmath56-divergence measures are as defined in ( [ eqdiv ] ) . since @xmath366 , both families of empirical phi - divergence test statistics are equivalent and taking into account ( [ f2 ] ) and @xmath369 , we have@xmath370      under the same conditions of theorem [ th1 ]  and the conditions given in section [ seccomp1 ] for @xmath371 , it is assumed that there exists a neighbourhood of @xmath121 in which @xmath372 is bounded by some some constant , we have @xmath373{c}\\widetilde{\\boldsymbol{\\theta}}_{e , n}-\\boldsymbol{\\theta}_{0}\\\\ \\widetilde{\\boldsymbol{\\nu}}_{e , n}\\end{array } \\right )   \\underset{n\\rightarrow\\infty}{\\overset{\\mathcal{l}}{\\rightarrow}}\\mathcal{n}\\left (   \\boldsymbol{0}_{p+q},\\left ( \\begin{array } [ c]{cc}\\boldsymbol{p}\\left (   \\boldsymbol{\\theta}_{0}\\right )   & \\boldsymbol{0}_{p\\times q}\\\\ \\boldsymbol{0}_{q\\times p } & \\boldsymbol{q}\\left (   \\boldsymbol{\\theta}_{0}\\right ) \\end{array } \\right )   \\right )   , \\label{2.5}\\ ] ] where @xmath121 is the true value of the parameter @xmath5 and@xmath374 this result is derived by taking into account , the facts that@xmath375 for a complete proof , one may refer to qin and lawless ( 1995 ) .",
    "[ th2]under @xmath128 in ( [ 2.1.3 ] ) and the assumptions above , we have@xmath376    let us consider the function@xmath377 a second - order taylor expansion of @xmath378 around @xmath379 , with @xmath380 , gives@xmath381 but , @xmath382 and @xmath383 by the strong law of large numbers , @xmath384 , and so ( [ o1 ] ) can be rewritten as@xmath385 consequently,@xmath386 with @xmath387 , which means that ( [ th2r ] ) holds .",
    "assume that @xmath388 is the true value of the unknown parameter so that @xmath389 , and that there exists a @xmath390 such that the restricted empirical maximum likelihood estimator satisfies @xmath391",
    ". then , we have @xmath392{c}\\widehat{\\boldsymbol{\\theta}}_{e , n}\\\\ \\widetilde{\\boldsymbol{\\theta}}_{e , n}\\end{array } \\right )   -\\left ( \\begin{array } [ c]{c}\\boldsymbol{\\theta}^{\\ast}\\\\ \\boldsymbol{\\theta}_{0}\\end{array } \\right )   \\right )   \\underset{n\\rightarrow\\infty}{\\overset{\\mathcal{l}}{\\rightarrow}}\\mathcal{n}\\left (   \\left ( \\begin{array } [ c]{c}\\boldsymbol{0}_{p}\\\\ \\boldsymbol{0}_{p}\\end{array } \\right )   , \\left ( \\begin{array } [ c]{cc}\\boldsymbol{v}\\left (   \\boldsymbol{\\theta}_{0}\\right )   & \\boldsymbol{a}_{12}\\left (   \\boldsymbol{\\theta}^{\\ast},\\boldsymbol{\\theta}_{0}\\right ) \\\\ \\boldsymbol{a}_{12}\\left (   \\boldsymbol{\\theta}^{\\ast},\\boldsymbol{\\theta}_{0}\\right )   ^{t } & \\boldsymbol{p(\\theta}_{0 } ) \\end{array } \\right )   \\right )   , \\label{a2}\\ ] ] where @xmath393 is  a @xmath394 matrix .    [ thth]under the conditions stated above , we have @xmath395 where @xmath396 with @xmath397 given by ( [ tao ] ) and@xmath398    based on theorem [ thth ] , we get an approximation of the power function at @xmath159 as @xmath399 if some alternative @xmath160 is the true parameter , then the probability of rejecting @xmath121 with the rejection rule @xmath400 , for a fixed significance level @xmath180 , tends to one as @xmath401 thus , the test is consistent in the sense of fraser ( 1957 ) .",
    "we may also find an approximation to the power of @xmath402 at an alternative hypothesis close to the null hypothesis .",
    "let @xmath403 be a given alternative , and let @xmath121 be the element in @xmath404 closest to @xmath405 in terms of euclidean distance .",
    "in order to introduce contiguous alternative hypotheses , we may consider a fixed @xmath188 @xmath406 and allow @xmath405 to tend to @xmath121 as @xmath32 increases in the following manner : @xmath407    [ th2b]under the assumptions of theorem [ th2 ] and @xmath191 given in ( [ c1 ] ) , we have@xmath408 where @xmath409 .",
    "if we substitute ( [ o2 ] ) into ( [ est2 ] ) , we obtain@xmath410 since @xmath411 , the taylor expansion @xmath412 yields @xmath413 by following the same steps as in the proof of theorem [ th1b ] , we have that under @xmath191 , @xmath414 .",
    "hence , we obtain @xmath415 where @xmath416 .",
    "we thus obtain@xmath417 which completes the proof .",
    "a second way to consider contiguous alternative hypotheses is to relax the condition @xmath418 defining the null hypothesis @xmath404 .",
    "let @xmath419 be such that @xmath420 .",
    "consider the following sequence @xmath405 of parameters approaching @xmath404:@xmath421 a taylor expansion of @xmath352@xmath422 around @xmath121 yields @xmath423 upon substituting @xmath203 in the previous formula and taking into account that @xmath352@xmath424 , we have@xmath425 then , the equivalence between @xmath426 and @xmath191 is obtained for @xmath427 .",
    "we thus have the following result .    under the contiguous alternative hypothesis in ( [ c2 ] ) ,",
    "we have @xmath428 with @xmath429 and @xmath430 as defined in ( [ bm ] ) .      from a sample @xmath286 of i.i.d .",
    "random variables with @xmath431=\\mu$ ] and @xmath432=\\sigma^{2},$ ] we wish to test that the coefficient of variation is @xmath185 , i.e. , @xmath433  against@xmath434 . in order to make a decision",
    ", we need to obtain the maximum likelihood estimator under the restriction @xmath435 , with @xmath436 .",
    "the estimating equations in this case are @xmath437 , @xmath438 .",
    "if we establish a bijective transformation between @xmath436  and @xmath439 , and are able to obtain the empirical restricted maximum likelihood estimator of @xmath5 , @xmath349 , due to the invariance property , we can obtain the empirical restricted maximum likelihood estimator of @xmath440 , @xmath441 , by taking the inverse of the transformation .",
    "let @xmath442 and  @xmath443 .",
    "then@xmath444 with @xmath445 .",
    "the estimating equations under the new parameterization are @xmath446 in general , we have the system of @xmath447 equations as follows:@xmath448 which are equivalent to @xmath449 for obtaining @xmath450 .",
    "observe that@xmath451 since the sum of the probabilities is 1 , and consequently the third and fourth equations become @xmath452 , @xmath453 , and we also know that @xmath454 .",
    "so , the optimization problem is reduced to simply to@xmath455 the solution @xmath349  must satisfy that the @xmath32 probabilities are not less than zero , i.e. , @xmath456 and if such a solution exists , it is unique ; see qin and lawless ( 1994 ) . in our empirical study",
    ", we solved the system of these two equations by using the nag subroutine in fortran , ` c05pbf ` .    to compare the exact coverage probabilities of the confidence intervals for the coefficient of variation based  on some empirical phi - divergence test statistics ,",
    "a simulation study was conducted separately for continuous and discrete distributions since we found that the rate of convergence to the asymptotic distribution  is much faster for discrete distributions ( poisson ) than for continuous distributions ( normal and student @xmath457 ) .",
    "based on @xmath458 samples of sizes @xmath316 , @xmath318 , @xmath320 , @xmath459 , @xmath460 , @xmath461 from @xmath323 and @xmath462 distributions and sample sizes @xmath463 , @xmath214 , @xmath464 , @xmath316 from poisson , @xmath465 , the so - called power - divergence measures ( see cressie and read ( 1984 ) ) were considered to construct the phi - divergence test statistics , @xmath466 that is , for each @xmath467 , we have a different divergence measure by taking @xmath468 , where @xmath469  is as defined earlier in section [ simulation ] . in addition , the empirical generalized wald test statistic , the empirical generalized score test statistic , and the empirical lagrange multiplier test statistic were also obtained . for this purpose ,",
    "the following auxiliary matrices are necessary:@xmath470 where @xmath471 $ ] and @xmath472   $ ] are unknown .",
    "the matrices @xmath473  and @xmath474  can be replaced by any consistent estimator @xmath475 and also @xmath476 and @xmath477 by@xmath478 the unconstrained maximum likelihood estimators were calculated as the solution of the system of @xmath479 equations:@xmath480 that is , @xmath481 first , the expression of the empirical generalized wald test statistic is obtained as @xmath482 next , the empirical generalized score test statistic is obtained as @xmath483   ^{2}}{\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{4}-8\\widetilde{u}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{3}-2\\widehat{v}\\widetilde{v}+\\widetilde{v}^{2}+8\\widehat{u}\\widetilde{u}\\widetilde{v}+24\\widetilde{u}^{2}\\widehat{v}-8\\widetilde{u}^{2}\\widetilde{v}-32\\widehat{u}\\widetilde{u}^{3}+16\\widetilde{u}^{4}}\\\\ &   = \\frac{n\\left [   \\widehat{v}+\\widetilde{v}-4\\widetilde{u}\\widehat{u}\\right ] ^{2}}{\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{4}-8\\widetilde{u}\\frac{1}{n}\\sum _ { i=1}^{n}x_{i}^{3}-8\\widehat{u}\\widetilde{u}\\widetilde{v}+10\\widehat{v}\\widetilde{v}+\\widetilde{v}^{2}},\\end{aligned}\\ ] ] where the second equality is obtained by taking into account that @xmath484 ; finally , the empirical lagrange multiplier test statistic is obtained as @xmath485    [ c]ccccccccccc@xmath32 & nom . level & @xmath486 & @xmath487 & @xmath488 & @xmath489 & @xmath490 & @xmath491 & @xmath492 & @xmath493 & @xmath494 + 30 & 0.90 & 0.8537 & 0.8615 & 0.8672 & 0.8717 & * 0.8723 * & 0.8683 & 0.8676 & 0.8671 & 0.8689 + 45 & 0.90 & 0.8682 & 0.8745 & 0.8789 & 0.8828 & 0.8836 & 0.8811 & 0.8778 & 0.8780 & * 0.8842 * + 60 & 0.90 & 0.8775 & 0.8824 & 0.8860 & 0.8895 & 0.8908 & 0.8899 & 0.8852 & 0.8854 & * 0.8930 * + 75 & 0.90 & 0.8815 & 0.8856 & 0.8892 & 0.8923 & 0.8932 & 0.8922 & 0.8870 & 0.8880 & * 0.8962 * + 90 & 0.90 & 0.8851 & 0.8892 & 0.8919 & 0.8947 & 0.8955 & 0.8951 & 0.8900 & 0.8900 & * 0.8996 * + 105 & 0.90 & 0.8870 & 0.8905 & 0.8938 & 0.8965 & 0.8976 & 0.8976 & 0.8916 & 0.8917 & * 0.9009 * + 30 & 0.95 & 0.9062 & 0.9143 & 0.9210 & 0.9251 & * 0.9258 * & 0.9211 & 0.9228 & 0.9169 & 0.9149 + 45 & 0.95 & 0.9199 & 0.9265 & 0.9312 & 0.9352 & * 0.9358 * & 0.9321 & 0.9308 & 0.9275 & 0.9285 + 60 & 0.95 & 0.9286 & 0.9342 & 0.9392 & 0.9425 & * 0.9426 * & 0.9401 & 0.9374 & 0.9336 & 0.9360 + 75 & 0.95 & 0.9322 & 0.9379 & 0.9417 & * 0.9446 * & * 0.9446 * & 0.9427 & 0.9399 & 0.9367 & 0.9398 + 90 & 0.95 & 0.9350 & 0.9398 & 0.9432 & 0.9457 & * 0.9460 * & 0.9443 & 0.9415 & 0.9387 & 0.9419 + 105 & 0.95 & 0.9372 & 0.9417 & 0.9449 & 0.9477 & * 0.9480 * & 0.9464 & 0.9428 & 0.9403 & 0.9447 +    @xmath495    [ c]ccccccccccc@xmath32 & nom . level & @xmath486 & @xmath487 & @xmath488 & @xmath489 & @xmath490 & @xmath491 & @xmath492 & @xmath493 & @xmath494 + 30 & 0.90 & 0.8003 & 0.8086 & 0.8145 & 0.8175 & 0.8174 & 0.8130 & * 0.8302 * & 0.8148 & 0.8208 + 45 & 0.90 & 0.8232 & 0.8306 & 0.8356 & 0.8387 & 0.8384 & 0.8343 & * 0.8465 * & 0.8333 & 0.8417 + 60 & 0.90 & 0.8354 & 0.8424 & 0.8473 & 0.8489 & 0.8488 & 0.8446 & * 0.8561 * & 0.8446 & 0.8526 + 75 & 0.90 & 0.8440 & 0.8500 & 0.8541 & 0.8568 & 0.8573 & 0.8535 & * 0.8621 * & 0.8531 & 0.8597 + 90 & 0.90 & 0.8517 & 0.8571 & 0.8610 & 0.8631 & 0.8629 & 0.8586 & * 0.8683 * & 0.8603 & 0.8645 + 105 & 0.90 & 0.8577 & 0.8627 & 0.8657 & 0.8668 & 0.8665 & 0.8621 & * 0.8737 * & 0.8654 & 0.8665 + 30 & 0.95 & 0.8584 & 0.8696 & 0.8776 & 0.8825 & 0.8822 & 0.8740 & * 0.8897 * & 0.8665 & 0.8709 + 45 & 0.95 & 0.8807 & 0.8906 & 0.8976 & 0.9012 & 0.9008 & 0.8938 & * 0.9037 * & 0.8847 & 0.8923 + 60 & 0.95 & 0.8925 & 0.9007 & 0.9074 & 0.9103 & 0.9098 & 0.9026 & * 0.9107 * & 0.8958 & 0.9004 + 75 & 0.95 & 0.9013 & 0.9088 & 0.9140 & 0.9165 & 0.9159 & 0.9087 & * 0.9169 * & 0.9040 & 0.9072 + 90 & 0.95 & 0.9080 & 0.9150 & 0.9199 & 0.9218 & 0.9209 & 0.9146 & * 0.9220 * & 0.9108 & 0.9112 + 105 & 0.95 & 0.9135 & 0.9201 & 0.9234 & 0.9244 & 0.9235 & 0.9160 & * 0.9265 * & 0.9163 & 0.9125 +    @xmath495    [ c]ccccccccccc@xmath32 & nom . level & @xmath486 & @xmath487 & @xmath488 & @xmath489 & @xmath490 & @xmath491 & @xmath492 & @xmath493 & @xmath494 + 15 & 0.90 & 0.8856 & 0.8878 & 0.9010 & 0.9089 & 0.9089 & 0.9071 & 0.8962 & 0.9075 & * 0.9099 * + 20 & 0.90 & 0.8814 & 0.8849 & 0.8875 & 0.8904 & 0.8925 & 0.8923 & * 0.9074 * & 0.8962 & 0.9013 + 25 & 0.90 & 0.8758 & 0.8793 & 0.8874 & 0.8934 & 0.8930 & 0.8923 & * 0.9048 * & 0.8882 & 0.8974 + 30 & 0.90 & 0.8790 & 0.8848 & 0.8861 & 0.8916 & 0.8923 & 0.8901 & * 0.9027 * & 0.8933 & 0.8928 + 15 & 0.95 & 0.9329 & 0.9392 & 0.9529 & 0.9606 & 0.9613 & 0.9639 & 0.9546 & 0.9440 & * 0.9670 * + 20 & 0.95 & 0.9359 & 0.9452 & 0.9486 & 0.9522 & 0.9539 & * 0.9544 * & 0.9501 & 0.9505 & 0.9519 + 25 & 0.95 & 0.9336 & 0.9388 & 0.9409 & 0.9434 & 0.9438 & 0.9442 & * 0.9502 * & 0.9468 & 0.9472 + 30 & 0.95 & 0.9318 & 0.9336 & 0.9406 & 0.9442 & 0.9449 & 0.9446 & * 0.9519 * & 0.9408 & 0.9466 +    @xmath495    focusing on the coefficients of variation of the two continuous distributions , the exact coverage probabilities of the confidence intervals based on the empirical power divergence test statistics with @xmath496 are presented in tables [ t2.1 ] and [ t2.2 ] when the nominal coverage probability based on the asymptotic distribution is either @xmath283 or @xmath238 . in addition , the empirical generalized wald test statistic , the empirical generalized score test statistic and the empirical lagrange multiplier test statistic are also considered for comparative purposes . from these results , we note that the empirical likelihood ratio test is not satisfactory and that among the empirical power divergence test statistics there is a good choice , for the underlying normal distribution , in the empirical chi - squared test statistic ( @xmath278 ) , and for the non - normal underlying distribution in the empirical cressie - read test statistic ( @xmath497 ) , but there is not much difference between their performance . if we consider other test statistics , for the normally distributed observations , with theoretical asymptotic coverage as @xmath238 , the empirical chi - squared test statistic ( @xmath278 ) while for @xmath283 coverage probability , the empirical lagrange multiplier test statistic are seen to be the best ones but there is very little difference with respect to the empirical chi - squared test statistic ( @xmath278 ) .",
    "for the non - normal distribution , the empirical wald test statistic is slightly superior than the empirical cressie - read test statistic ,  while the opposite seems to be the case for the normal distribution . since in practice we do not know the form of the underlying distribution , based on this simulation study",
    ", we would recommend the use of either the empirical cressie - read test statistic  or the empirical wald test statistic . in table",
    "[ t2.3 ] ,  the same study has been carried out but for the case of poisson distribution .",
    "the empirical likelihood ratio test is once again unsatisfactory and that the empirical cressie - read test statistic and the empirical chi - squared test statistic have a better coverage probability close to the nominal level .",
    "further , the empirical wald test statistic seems to be slightly superior than the empirical cressie read test statistic since it has a greater coverage probability in 5 of the 8 cases .",
    "in a non - parametric setting , we have proposed here a broad family of empirical test statistics based on @xmath56-divergence measures , first for a simple null hypothesis and then for a composite null hypothesis . through numerical examples and simulations",
    ", it has been shown that confidence intervals constructed thought the empirical @xmath56-divergence based statistics improve the coverage probability of the empirical likelihood ratio test slightly . however , the most promising advantage of this new family of test statistics is that some members outperform the empirical likelihood ratio test in the presence of some shifted observations in the data .",
    "the approximation of the power function based on a specific sample provides an insight about the most appropriate robust @xmath56-divergence statistic .",
    "these robust statistics tend to yield narrower confidence intervals in comparison to the empirical likelihood ratio test .",
    "the development of these empirical @xmath56-divergence tests in the two - sample and multi - sample situations will be of great interest .",
    "we are currently working in this direction and hope to report these findings in a future paper .",
    "menndez , m. l. , morales , d. , pardo , l. and salicr , m. ( 1995 ) .",
    "asymptotic behavior and statistical applications of divergence measures in multinomial populations : a unified study .",
    "_ statistical papers , _ * 36 , * 1 - 29 .",
    "menndez , m. l. , pardo , j. a. , pardo , l. and pardo , m. c. ( 1997 ) .",
    "asymptotic approximations for the distributions of the @xmath183-divergence goodness - of - fit statistics : applications to rnyi s statistic .",
    "_ kybernetes , _ * 26 , * 442 - 452 ."
  ],
  "abstract_text": [
    "<S> the main purpose of this paper is to introduce first a new family of empirical test statistics for testing a simple null hypothesis when the vector of parameters of interest are defined through a specific set of unbiased estimating functions . </S>",
    "<S> this family of test statistics is based on a distance between two probability vectors , with the first probability vector obtained by maximizing the empirical likelihood on the vector of parameters , and the second vector defined from the fixed vector of parameters under the simple null hypothesis . </S>",
    "<S> the distance considered for this purpose is the phi - divergence measure . </S>",
    "<S> the asymptotic distribution is then derived for this family of test statistics . the proposed methodology is illustrated through the well - known data of newcomb s measurements on the passage time for light . </S>",
    "<S> a simulation study is carried out to compare its performance with respect to the empirical likelihood ratio test when confidence intervals are constructed based on the respective statistics for small sample sizes . </S>",
    "<S> the results suggest that the empirical modified likelihood ratio test statistic  provides a competitive alternative to the empirical likelihood ratio test statistic , and is also more robust than the empirical likelihood ratio test statistic in the presence of contamination in the data . </S>",
    "<S> finally , we propose empirical phi - divergence test statistics for testing a composite null hypothesis and present some asymptotic as well as simulation results to study the performance of these test procedures .    </S>",
    "<S> = 1    * :* 62e20    * :* empirical likelihood , empirical phi - divergence test statistics , influence function , phi - divergence measures , power function , empirical likelihood ratio , empirical modified likelihood ratio . </S>"
  ]
}