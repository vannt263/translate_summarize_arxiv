{
  "article_text": [
    "there are three large classes of neural networks .",
    "one is the class of _ multilayered feedforward networks _ which , through supervised learning , are used to approximate nonlinear functions .",
    "the second is the class of _ relaxation networks _ with symmetric synaptic connections , like the hopfield network . under time evolution",
    "the relaxation networks evolve to fixed points which , in successful applications , are identified with memorized patterns . in the last class one",
    "includes all networks with arbitrary connections which are neither completely feedforward nor symmetric .",
    "they are called _ recurrent networks_.    recurrent networks exhibit a complex variety of temporal behavior and are increasingly being proposed for use in many engineering applications@xcite @xcite @xcite @xcite .",
    "in contrast to feedforward or relaxation networks , the rich dynamical structure of recurrent networks makes them a natural choice to process temporal information . even for the learning of nonlinear functions the feedback structure improves the reproduction of discontinuities or large derivative regions@xcite . also , in some cases , the feedback structure is a way to enhance , through a choice of architecture , the sensitivity of the network to particular features to be detected@xcite .",
    "the learning algorithms used for feedforward networks may be generalized to the recurrent case@xcite @xcite @xcite @xcite .",
    "some of this will be covered by another speaker at this conference .",
    "feedback loops and coexistence of quiescent , oscillatory and chaotic behavior are also present in biological systems and only the recurrent networks are appropriate models for these phenomena@xcite @xcite .",
    "the whole field of recurrent networks , both for engineering and biological applications , is developing in many directions .",
    "their global analysis is rather involved@xcite .",
    "their characterization as dynamical systems is sharpened by a decomposition theorem .",
    "continuous state - continuous time neural networks , as well as many other systems@xcite , may be written in the cohen - grossberg@xcite form @xmath0 dynamical systems of this type have a decomposition property .",
    "define    @xmath1    then we have the following    _ theorem _",
    "@xcite if @xmath2 @xmath3 and the matrix @xmath4 has an inverse then the vector field @xmath5 in eq.([1.1 ] ) decomposes into one gradient and one hamiltonian components , @xmath6 , where @xmath7 and @xmath8 @xmath9 .",
    "@xmath10 and @xmath11 are the components of the riemannian metric and the symplectic form .",
    "_ proof _ : the decomposition follows by direct calculation from ( [ 1.1 ] ) and ( [ 1.2 ] ) .",
    "the conditions on @xmath12 , @xmath13 and @xmath14 insure that @xmath15 is a well defined metric and @xmath16 is non - degenerate .",
    "indeed let @xmath17 be a vector such that @xmath18 .",
    "then @xmath19 would imply @xmath20 @xmath21 .",
    "that @xmath22 is a closed form follows from the fact that @xmath23 depends only on @xmath24 and @xmath25 .",
    "@xmath26    the identification , in the system ( [ 1.1 ] ) , of just one gradient and one hamiltonian component with explicitly known potential and hamiltonian functions , is a considerable simplification as compared to a generic dynamical system .",
    "recall that in the general case , although such a decomposition is possible locally@xcite , explicit functions are not easy to obtain unless one allows for one gradient and @xmath27 hamiltonian components .",
    "notice that the decomposition of the vector field does not decouple the dynamical evolution of the components .",
    "in fact it is the interplay of the dissipative ( gradient ) and the hamiltonian component that leads , for example , to limit cycle behavior .    for the case of symmetric connections @xmath28",
    "one recovers the cohen - grossberg result@xcite that states that a symmetric system of the type of eq.([1.1 ] ) has a lyapunov function @xmath29 of which hopfields@xcite  energy ",
    "function is a particular case . for the symmetric case the existence of a lyapunov function guarantees global asymptotic stability of the dynamics .",
    "however not all vector fields with a lyapunov function are differentially equivalent to a gradient field .",
    "therefore the fact that a gradient vector is actually obtained gives additional information , namely about structural stability of the model .",
    "a necessary condition for structural stability of the gradient vector field is the non - degeneracy of the critical points of @xmath29 , namely @xmath30 at the points",
    "where @xmath31 . in a gradient flow",
    "all orbits approach the critical points as @xmath32 .",
    "if the critical points are non - degenerate then the gradient flow satisfies the conditions defining a morse - smale field , except perhaps the transversality conditions for stable and unstable manifolds of the critical points .",
    "however because morse - smale fields are open and dense in the set of gradient vector fields , any gradient flow with non - degenerate critical points may always be c@xmath33-approximated by a ( structurally stable ) morse - smale gradient field .",
    "therefore given a symmetric model of the type ( [ 1.1 ] ) , the identification of its gradient nature provides a easy way to check its robustness as a physical model .    as an example of the decomposition applied to a biological model",
    "consider the wilson - cowan model of a neural oscillator without refractory periods , in the antisymmetric coupling case considered by most authors @xmath34 with @xmath35 and @xmath36 is the sigmoid function @xmath37 .",
    "changing variables to @xmath38 one obtains @xmath39 with @xmath40 the model is completely described by these functions , the bifurcation sets@xcite , for example , being characterized by @xmath41 for andronov - hopf bifurcations and by @xmath42 for saddle - node bifurcations .",
    "doyne farmer@xcite has shown that there is a common mathematical framework where neural networks , classifier systems , immune networks and autocatalytic reaction networks may be treated in a unified way . the general model in which all these models may be mapped looks like a neural network where ,",
    "in addition to the node state variables ( @xmath24 ) and the connection strengths ( @xmath43 ) , there is also a node parameter ( @xmath44 ) with learning capabilities ( fig.1 ) .",
    "the node parameter represents the possibility of changing , through learning , the nature of the linear or non - linear function @xmath45 at each node . in the simplest case @xmath46",
    "will be simply an intensity parameter .",
    "therefore the degree to which the activity at node @xmath47 influences the activity at other nodes depends not only on the connection strengths ( @xmath43 ) but also on an adaptive node parameter @xmath44 . in some cases , as in the b - cell immune network , the node parameter is the only means to control the relative influence of a node on others , the connection strengths being fixed chemical reaction rates .",
    "we will denote by @xmath24 the output of node @xmath47 .",
    "hebbian learning @xcite is a type of unsupervised learning where a connection strength @xmath48 is reinforced whenever the product @xmath49 is large . as shown by several authors , hebbian",
    "learning extracts the eigenvectors of the correlation matrix q of the input data .",
    "@xmath50 where @xmath51 means the sample average .",
    "if the learning law is local , the lines of the connection matrix @xmath43 all tend to the eigenvector associated to the largest eigenvalue of the correlation matrix . to obtain the other eigenvector directions one needs non - local laws @xcite @xcite @xcite .",
    "approach has the advantage of organizing the connection matrix in such a way that the rows are the eigenvectors associated to the eigenvalues in decreasing order .",
    "it suffers however from slow convergence rates for the lowest eigenvalues .",
    "the methods that have been proposed may , with small modifications , be used both for linear and non - linear networks . however , because the maximum information about a signal @xmath52 , that may be coded directly in the connection matrix @xmath43 , is the principal components decomposition and this may already be obtained with linear units , we will discuss only this case .",
    "the learning rules proposed below@xcite are a generalization of sanger s scheme including a node parameter @xmath44 .",
    "we consider a one - layer feedforward network with as many inputs as outputs ( fig.2 )    and the updating rules proposed for @xmath43 and @xmath44 are : @xmath53 where @xmath54 is the output of node i @xmath55 and @xmath56 and @xmath57 are positive constants that control the learning rate .",
    "as will be shown below the learning dynamics of eqs([2.3 ] ) has the capability to accelerate the convergence rate for the small eigenvalues of @xmath58 . to avoid undesirable fixed points of the dynamics , where this acceleration effect is not obtained , the learning rules ( [ 2.3 ] ) are supplemented by the following prescription :  _ the starting _",
    "s are all positive and are not allowed to decrease below a value ( _ @xmath44_)_@xmath59_. if , at a certain point of the learning process , _ @xmath44",
    "_  hits the lower bound then one makes the replacement _ @xmath60 _  in the line i of the connection matrix _  ( see remark 3 ) .",
    "define the n - dimensional vectors @xmath61    then the following result was proven @xcite :    _ if the time scale of the system ( [ 2.3 ] ) is much slower than the averaging time of the input signal _",
    "@xmath62_x_@xmath63 _  then : _    _ a ) the system ( [ 2.3 ] ) has a stable fixed point such that _",
    "@xmath64    _ and _ @xmath65_. _ @xmath66 _  _ @xmath67_(i=1 ... ,n ) are the eigenvalues of the correlation matrix .",
    "_    _ b ) convergence to the fixed point is sequential in the sense that _ @xmath68 _",
    "is only attracted to the eigenvector described in ( [ 2.6 ] ) if all vectors _ @xmath69 _  for _ @xmath70 _  are already close to their corresponding eigenvector values .",
    "_    remarks :    1 - the matrix @xmath71 of the connection strengths extracts the principal components of the correlation matrix @xmath58 .",
    "the node parameters @xmath44 at their fixed point ( [ 2.6 ] ) extract additional information on the mean value of the data vector @xmath72 and the eigenvalues of @xmath58 . to deal with data with zero mean it is convenient to change the @xmath73updating law to @xmath74    where @xmath75 is a fixed vector .",
    "the stable fixed point is now @xmath76    if one wishes to separate the eigenvalues from the information on the average data @xmath77 one may add another parameter @xmath78 to each node with a learning law @xmath79    which , with the same assumptions about time scales as before , converges to the stable fixed point @xmath80    the convergence rate near the fixed point is @xmath81 . therefore choosing a large",
    "@xmath82 accelerates convergence for the small eigenvalues .    2 - in addition to its role in extracting additional information on the input signal , the node parameters @xmath44 also plays a role in accelerating the convergence to the stable fixed point .",
    "for fixed @xmath83 , the rate of convergence to the fixed point is very slow for the components associated to the smallest eigenvalues .",
    "this is the reason for the convergence problems in sanger s method and one also finds that sometimes the results for the small components are quite misleading . on the other hand to increase @xmath56 does not help because then the time scale of the @xmath43 learning law becomes of the order of the time scale of the data and one obtains large fluctuations in the principal components . with a node parameter and the learning law ( [ 2.3 ] )",
    "the situation is more favorable because for small eigenvalues the effective control parameter ( @xmath84 ) is dynamically amplified .",
    "this accelerates the convergence of the minor components without inducing fluctuations on the principal components .",
    "3- here one examines the effect of the prescription to avoid the fixed points where some @xmath85 .",
    "consider the average evolution of @xmath86 , assuming all the other variables fixed @xmath87 if @xmath88 the stable fixed point is at @xmath89 and if @xmath90 it is at zero ( see fig.3 ) .    if @xmath91 is @xmath92 , @xmath44 moves towards the fixed point f0 at zero but , when it reaches @xmath93 , the change in the sign of the corresponding row @xmath69 in connection matrix changes the dynamics and it is f1 that is now the attracting fixed point .    to illustrate the effect of node parameters in principal component analysis ( pca ) , consider the following two - dimensional @xmath94 signal , where @xmath95 are gaussian distributed variables with zero mean and unit variance : @xmath96 the principal components of the signal and the eigenvalues are given in the following table .",
    "@xmath97    a one - layer network with node parameters as in fig.2 is used to perform pca .",
    "fig.4 shows the data and the principal directions that are obtained using the learning laws ( [ 2.3 ] ) and ( [ 2.13 ] ) .",
    "the parameter values used are @xmath56 = 0.015 , @xmath57 = 0.005 and * r*=(0.002,0.002 ) .",
    "fig.5 shows the convergence of the @xmath71 s to their final values in the learning process .",
    "fig.6 shows the variation of the node parameters .",
    "sanger s original algorithm is recovered by fixing the node parameters to unit values . in this case",
    "the principal components are also extracted , but the convergence of the process is much slower as shown in fig.7 .    with node parameters , improved convergence of the small eigenvalues is to be expected under generic conditions because the rates of convergence are controlled by @xmath84 and @xmath44 is proportional to @xmath98 . in the example",
    "the effect of @xmath99 is further amplified by the fact that , before @xmath100 starts to converge , @xmath101 is large .",
    "hence , in this case at least , the node parameter acts like a variable learning rate for the minor component .",
    "notice that the method does not induce oscillations in the major components as would happen with large @xmath56 values .",
    "besides speeding up the learning process the node parameters also contain information about first moments and the eigenvalues .",
    "node parameters also have a beneficial effect on competitive learning algorithms .",
    "for details refer to @xcite .",
    "the aim of principal component analysis ( pca ) is to extract the eigenvectors of the correlation matrix , from the data .",
    "there are standard neural network algorithms for this purpose@xcite @xcite @xcite @xcite .",
    "however if the process is non - gaussian , pca algorithms or their higher - order generalizations provide only incomplete or misleading information on the statistical properties of the data .",
    "let @xmath24 denote the output of node @xmath47 in a neural network .",
    "hebbian learning@xcite is a type of unsupervised learning where the neural network connection strengths @xmath43 are reinforced whenever the products @xmath102 are large .",
    "the simplest form is @xmath103 hebbian learning extracts the eigenvectors of the correlation matrix @xmath58@xmath104 but , if the learning law is local as in eq.([3.11 ] ) , all the lines of the connection matrix @xmath43 converge to the eigenvector with the largest eigenvalue of the correlation matrix . to obtain other eigenvector directions requires non - local laws .",
    "these principal component analysis ( pca ) algorithms find the characteristic directions of the correlation matrix @xmath105 .",
    "if the data has zero mean ( @xmath106 ) they are the orthogonal directions along which the data has maximum variance .",
    "if the data is gaussian in each channel , it is distributed as a hyperellipsoid and the correlation matrix @xmath58 already contains all the information about statistical properties .",
    "this is because higher order moments of the data may be obtained from the second order moments .",
    "however , if the data is non - gaussian , the pca analysis is not complete and higher order correlations are needed to characterize the statistical properties .",
    "this led some authors@xcite @xcite to propose networks with higher order neurons to obtain the higher order statistical correlations of the data .",
    "an higher order neuron is one that is capable of accepting , in each of its input lines , data from two or more channels at once .",
    "there is then a set of adjustable strengths @xmath107 , @xmath108 being the order of the neuron .",
    "networks with higher order neurons have interesting applications , for example in fitting data to a high - dimensional hypersurface .",
    "however there is a basic weakness in the characterization of the statistical properties of non - gaussian data by higher order moments .",
    "existence of the moments of a distribution function depends on the behavior of this function at infinity and it frequently happens that a distribution has moments up to a certain order , but no higher ones .",
    "a well - behaved probability distribution might even have no moments of order higher than one ( the mean ) .",
    "in addition a sequence of moments does not necessarily determine a probability distribution function uniquely@xcite .",
    "two different distributions may have the same set of moments .",
    "therefore , for non - gaussian data , the pca algorithms or higher order generalizations may lead to misleading results .    as an example",
    "consider the two - dimensional signal shown in fig.8 .",
    "fig.9 shows the evolution of the connection strengths @xmath109 and @xmath110 when this signal is passed through a typical pca algorithm .",
    "large oscillations appear and finally the algorithm overflows .",
    "smaller learning rates do not introduce qualitative modifications in this evolution .",
    "the values may at times appear to stabilize , but large spikes do occur .",
    "the reason is that the seemingly harmless data in fig.8 is generated by a linear combination of a gaussian with the following distribution @xmath111 which has first moment , but no moments of higher order .    to be concerned with non - gaussian processes",
    "is not a pure academic exercise , because in many applications adequate tools are needed to analyze such processes .",
    "for example , processes without higher order moments , in particular those associated with lvy statistics , are prominent in complex processes such as relaxation in glassy materials , chaotic phase diffusion in josephson junctions and turbulent diffusion@xcite @xcite @xcite . moments of an arbitrary probability distribution may not exist .",
    "however , because every bounded and measurable function is integrable with respect to any distribution , the existence of the characteristic function f(@xmath82 ) is always assured@xcite .",
    "@xmath112 @xmath82 and @xmath94 are @xmath113-dimensional vectors , @xmath94 is the data vector and @xmath114 its distribution function .",
    "the characteristic function is a compact and complete characterization of the probability distribution of the signal .",
    "if , in addition , one wishes to describe the time correlations of the stochastic process @xmath115 , the corresponding quantity is the characteristic functional@xcite @xmath116 where @xmath117 is a smooth function and the scalar product is @xmath118 @xmath119 being the probability measure over the sample paths of the process .    in the following",
    "i will describe an algorithm to compute the characteristic function from the data , by a learning process@xcite .",
    "the main idea is that , in the end of the learning process , one has a neural network which is a representation of the characteristic function .",
    "this network is then available to provide all the required information on the probability distribution of the data being analyzed .",
    "suppose we want to learn the characteristic function @xmath120 of a one - dimensional signal @xmath115 in a domain @xmath121 $ ] .",
    "the @xmath82-domain is divided in @xmath113 intervals by a sequence of values @xmath122 and a network is constructed with @xmath123 intermediate layer nodes and an output node ( fig . 10 ) .",
    "the learning parameters in the network are the connection strengths @xmath124 and the node parameters @xmath44 .",
    "the existence of the node parameter means that the output of a node in the intermediate layer is @xmath125 , @xmath126 being a non - linear function .",
    "the use of both connection strengths and node parameters in neural networks makes them equivalent to a wide range of other connectionist systems@xcite and improves their performance in standard applications@xcite .",
    "the learning laws for the network in fig .",
    "10 are : @xmath127 @xmath128 .",
    "the intermediate layer nodes are equipped with a radial basis function @xmath129 where in general one uses @xmath130 for all @xmath47 .",
    "the output is a simple additive node .",
    "the learning constant @xmath131 should be sufficiently small to insure that the learning time is much smaller than the characteristic times of the data @xmath115 .",
    "if this condition is satisfied each node parameter @xmath44 tends to @xmath132 , the real part of the characteristic function @xmath120 for @xmath133 .",
    "the @xmath134 learning law was chosen to minimize the error function @xmath135 one sees that the learning scheme is an hybrid one , in the sense that the node parameter @xmath44 learns , in an unsupervised way , ( the real part of ) the characteristic function @xmath136 and then , by a supervised learning scheme , the @xmath124 s are adjusted to reproduce the @xmath44 value in the output whenever the input is @xmath137 . through the learning law ( [ 3.17 ] ) each node parameter @xmath44 converges to @xmath138 and the interpolating nature of the radial basis functions guarantees that , after training , the network will approximate the real part of the characteristic function for any @xmath82 in the domain [ @xmath139 .",
    "a similar network is constructed for the imaginary part of the characteristic function , where now @xmath140 for higher dimensional data the scheme is similar .",
    "the number of required nodes is @xmath141 for a @xmath142-dimensional data vector @xmath143 .",
    "for example for the 2-dimensional data of fig.8 a set of @xmath144 nodes was used ( fig.11 ) .",
    "each node in the square lattice has two inputs for the two components @xmath145 and @xmath146 of the vector argument of @xmath147 .",
    "the learning laws are , as before @xmath148 the pair ( @xmath149 ) denotes the position of the node in the square lattice and the radial basis function is @xmath150 two networks are used , one for the real part of the characteristic function , and another for the imaginary part with , in eqs.([3.22 ] ) , @xmath151 replaced by @xmath152 .",
    "figs.12 and 13 show the values computed by the algorithm for the real and imaginary parts of the characteristic function corresponding to the two - dimensional signal in fig.8 .    on the left is a plot of the exact characteristic function and on the right the values learned by the network . in this case",
    "we show only the mesh corresponding to the @xmath44 values .",
    "one obtains a 2.0% accuracy for the real part and 4.5% accuracy for the imaginary part .",
    "the convergence of the learning process is fast and the approximation is reasonably good .",
    "notice in particular the slope discontinuity at the origin which reveals the non - existence of a second moment .    for a second example",
    "the data was generated by a weierstrass random walk with probability distribution @xmath153 and b=1.31 , which is a process of the lvy flight type .",
    "the characteristic function , obtained by the network , is shown in fig .",
    "14 .    taking",
    "the @xmath154 of the network output one obtains the scaling exponent 1.49 near @xmath155 , close to the expected fractal dimension of the random walk path ( 1.5 ) .",
    "these examples test the algorithm as a process identifier , in the sense that , after the learning process , the network is a dynamical representation of the characteristic function and may be used to perform all kinds of analysis of the statistics of the data .",
    "freeman and collaborators@xcite @xcite @xcite @xcite @xcite @xcite have extensively studied and modeled the neural activity in the mammalian olfactory system .",
    "their conclusions challenge the idea that pattern recognition in the brain is accomplished as in an attractor neural network@xcite .",
    "pattern recognition in the brain is the process by which external signals arriving at the sense organs are converted into internal meaningful states .",
    "the studies of the excitation patterns in the olfactory bulb of the rabbit lead to the conclusion that , at least in this biological pattern recognition system , there is no evolution towards an equilibrium fixed point nor does it seem to be minimizing an energy function . other interesting conclusions of these biological studies are :    - the main component of the neural activity in the olfactory system is chaotic .",
    "this is also true in other parts of the brain , periodic behavior occurring only in abnormal situations like deep anesthesia , coma , epileptic seizures or in areas of the cortex that have been isolated from the rest of the brain ;    - the low - level chaos that exists in absence of an external stimulus is , in the presence of a signal , replaced by bursts lasting for about 100 ms which have different intensities in different regions of the olfactory bulb .",
    "olfactory pattern recognition manifests itself as a spatially coherent pattern of intensity ;    - the recognition time is very fast , in the sense that the transition between different patterns occurs in times as short as 6 ms .",
    "given the neuron characteristic response times this is clearly incompatible with the global approach to equilibrium of an attractor neural network ;    - the biological measurements that have been performed do not record the action potential of individual neurons , but the local effect of the currents coming out of thousands of cells .",
    "therefore the very existence of measurable activity bursts implies a synchronization of local assemblies of many neurons .",
    "freeman , yao and burke@xcite @xcite model the olfactory system with a set of non - linear coupled differential equations , the coupling being adjusted by means of an input correlation learning scheme .",
    "each variable in the coupled system is assumed to represent the dynamical state of a local assembly of many neurons . based on numerical simulations",
    "they conjecture that olfactory pattern recognition is realized through a multilobe strange attractor .",
    "the system would be , most of the time , in a basal ( low - activity ) state , being excited to one of the higher lobes by the external stimulus .    to compute or even prove the existence of chaotic measures in coupled differential equation systems is a awesome task .",
    "therefore , even if it may be biologically accurate , the analytical model of these authors is difficult to deal with and unsuitable for wide application in technological pattern recognition tasks , although one such application has indeed been attempted by the authors@xcite .",
    "however the idea that efficient pattern recognition may be achieved by a chaotic system , which selects distinct invariant measures according to the class of external stimuli , is quite interesting and deserves further exploration .",
    "inspired by the biological evidence a model has been developed @xcite , which behaves roughly as an olfactory system ( in freeman s sense ) and , at the same time , is easier to describe and control by analytical means . to play the role of the local chaotic assembly of neurons a bernoulli unit",
    "is chosen .",
    "the connection between the units is realized by linear synapses with an input correlation learning law and the external inputs also have adjustable gains , changing as in a biological potentiation mechanism .",
    "this last feature turns out to be useful to enhance the novelty - filter qualities of the system .",
    "the output of the nodes is denoted by @xmath54 and the @xmath24 s are the external inputs .",
    "@xmath156 with @xmath157 are the connection strengths and @xmath158 the input gains .",
    "both @xmath43 and @xmath159 are real numbers in the interval [ 0,1 ] .",
    "the input patterns are zero - one sequences ( @xmath160 ) .",
    "the learning laws for the connection strengths and the input gains are the following :      according to eqs.([3.1a]-[3.3 ] ) , when an input pattern has a one in both the i and j positions , the correlation of the units i and j becomes stronger .",
    "@xmath168 is a constant related to the node dynamics , which the sum of the off - diagonal connections is not allowed to exceed .",
    "@xmath169 is a small parameter that controls the learning speed .",
    "finally the diagonal element @xmath170 is chosen in such a way that the sum of all connections entering each unit adds to one .",
    "in the input gain learning law , @xmath171 ( or @xmath172 ) is the number of times that a one ( or a zero ) has appeared at the input i , up to time t. eq.([3.3 ] ) means that if an input is excited many times , during the learning phase , it becomes more sensitive .",
    "the learning process starts with @xmath175 and @xmath176 for @xmath162 .",
    "each unit has then an independent absolutely continuous invariant measure which is the lebesgue measure in [ @xmath177 and zero outside .",
    "when the @xmath43 ( @xmath162 ) become different from zero but the inputs @xmath178 are still zero , all variables @xmath179 stay in the interval [ @xmath177 because of the convex linear combination of inputs imposed by the normalization of the @xmath43 s . when some inputs @xmath178 are @xmath180 , there is a finite probability for an irregular burst in the interval [ @xmath181 , of some of the node variables , with reinjection into [ @xmath177 whenever the iterate falls on the interval [ @xmath182 .",
    "the bursts in the interval [ @xmath181 , in response to some of the input patterns , is the recognition mechanism of the network .",
    "the basal chaotic dynamics insuring an uniform covering of the interval [ @xmath177 , the timing of the onset of the bursts depends only on the correlation probability and on the clock time of the discrete dynamics .",
    "we understand therefore why a chaos - based network may have a recognition time faster than an attractor network .        in eqs.([3.1a]-[3.1c ] ) , either the node i is not correlated to any other node and then all off - diagonal elements @xmath43 are zero or , as soon as the input patterns begin to correlate the node i with any other node , the off - diagonal elements start to grow and only the second case ( eq.([3.1c ] ) ) needs to be considered .",
    "let the learning gain be small .",
    "then , in first order in @xmath169 , we have @xmath184 for n learning steps , in first order in @xmath169@xmath185 denoting by @xmath186 the probability for the occurrence , in the input patterns , of a one both in the i and the j positions , the above equation has the stationary solution @xmath187    now we establish an equation for the burst probabilities . consider the case where @xmath188 is much smaller than one , that is , the basal chaos is of low intensity . in this case , because of the normalization chosen for @xmath189 , the dynamics inside the interval [ @xmath177 is dominated by @xmath190(mod @xmath188 ) and in the interval [ @xmath181 by @xmath190(mod @xmath191 ) .",
    "hence , to a good approximation , we may assume uniform probability measures for the motion inside each one of the intervals . denoting the interval [ @xmath177 as the state 1 and the interval [ @xmath181 as the state 2 ,",
    "the dynamics of each node is a two - state markov process with transition probabilities between the states corresponding to the probabilities of falling in some subintervals of the intervals [ @xmath177 and [ @xmath181 .",
    "namely , the probability @xmath192 equals the probability of falling in the reinjection interval [ @xmath182 and the probability @xmath193 that of falling near the point @xmath188 at a distance smaller than the off - diagonal excitation .",
    "@xmath194 @xmath195 @xmath196 @xmath197 where we have used the notation @xmath198 for functions truncated to the range [ @xmath199 .",
    "that is , @xmath200 if @xmath201 , @xmath202 if @xmath203 and @xmath204 if @xmath205 .",
    "the sum in the right - hand side of eq.([3.2c ] ) is approximated in probability by @xmath206 where @xmath207 denotes the probability of finding the node j in the state 2 at time t. the probability estimate ( [ 3.5 ] ) , for the outputs @xmath208 , assumes statistical independence of the units .",
    "this hypothesis fails when there are synchronization effects , which are to be expected mainly when a small group of units is strongly correlated .    with the probability estimate for the @xmath209 s and the detailed balance principle it is now possible to write a self - consistent equation for the probability @xmath210 to find an arbitrary node i in the state 2 at time t @xmath211    for each input pattern @xmath212 , one obtains an estimate for @xmath210 solving eq.([3.6 ] ) by iteration .",
    "we find that the solution that is obtained is qualitatively similar to the numerically determined invariant measures , although it tends to overestimate the burst excitation probabilities when they are small .",
    "this may be understood from the synchronization effects between groups of units .",
    "when one unit is not excited ( not in state 2 ) the others tend also not to be excited , hence ( [ 3.5 ] ) overestimates the sum @xmath213 .",
    "we now illustrate how the network behaves as an associator and pattern recognizer .",
    "consider , for display simplicity , a network of four nodes that is exposed during many iterations to the patterns 1000 and 0110 where the first pattern appears twice as much as the second .",
    "after this learning period we have exposed the network to all possible zero - one input patterns for 500 time steps each and observed the network reaction . during the recall experiment no further adjustment of the @xmath43 s is made .",
    "the result is shown in the fig.18 .",
    "the conclusions from this and other simulations is that , according to nature of the learning patterns , the network acts , for the recall input patterns , as a mixture of memory , associator and novelty filter .",
    "for example in fig.18 we see that after having learned the sequences 1000 and 0110 , the network reproduces these patterns as a memory . the pattern 1001",
    "is associated to the pattern 1000 and the pattern 1110 associated to a mixture of the two learned patterns .",
    "by contrast the pattern 0100 is not recognized by the network which acts then as a novelty filter .",
    "fig.19 shows the invariant measures of the system ( expressed in probability per bin ) when the input patterns are 1100 and 0110 .",
    "\\3 . as a pattern recognizer",
    "the network is a mixture of memory , associator and novelty filter .",
    "this however is sensitive to the learning algorithm that is chosen and , for the algorithm that is discussed here , it is sensitive also to value of the parameters @xmath188 and @xmath169 .      in this part one",
    "studies dynamical systems composed of a set of coupled quadratic maps @xmath214 with @xmath215 $ ] , @xmath216 @xmath217 and @xmath218 @xmath219 and @xmath220 .",
    "the value chosen for @xmath221 implies that , in the uncoupled limit ( @xmath222 @xmath162 ) , each unit transforms as a one - dimensional quadratic map in the accumulation point of the feigenbaum period - doubling bifurcation cascade .",
    "this system will be called a feigenbaum network .",
    "the quadratic map at the feigenbaum accumulation point is not in the class of chaotic systems ( in the sense of having positive lyapunov exponents ) however , it shares with them the property of having an infinite number of unstable periodic orbits .",
    "therefore , before the interaction sets in , each elementary map possesses an infinite diversity of potential dynamical behaviors . as we will show later , the interaction between the individual units is able to selectively stabilize some of the previously unstable periodic orbits .",
    "the selection of the periodic orbits that are stabilized depends both on the initial conditions and on the intensity of the interaction coefficients @xmath43 . as a result",
    "feigenbaum networks appear as systems with potential applications in the fields of control of chaos , information processing and as models of self - organization .",
    "control of chaos or of the transition to chaos has been , in recent years , a very active field ( see for example ref.@xcite and references therein ) .",
    "several methods were developed to control the unstable periodic orbits that are embedded within a chaotic attractor .",
    "having a way to select and stabilize at will these orbits we would have a device with infinite storage capacity ( or infinite pattern discrimination capacity ) .",
    "however , an even better control might be achieved if , instead of an infinite number of unstable periodic orbits , the system possesses an infinite number of periodic attractors .",
    "the basins of attraction would evidently be small but the situation is in principle more favorable because the control need not be as sharp as before . as long as the system is kept in a neighborhood of an attractor the uncontrolled dynamics itself stabilizes the orbit .",
    "the creation of systems with infinitely many sinks near an homoclinic tangency was discovered by newhouse @xcite and later studied by several other authors @xcite @xcite @xcite @xcite @xcite . in the newhouse phenomenon",
    "infinitely many attractors may coexist but only for special parameter values , namely for a residual subset of an interval .",
    "another system , different from the newhouse phenomena , which also displays many coexisting periodic attractors is a rotor map with a small amount of dissipation@xcite .",
    "here one shows that for a feigenbaum system with only two units and symmetrical couplings one obtains a system which has an infinite number of sinks for an open set of coupling parameters .",
    "then one also analyzes the behavior of a feigenbaum network in the limit of a very large number of units .",
    "a mean field analysis shows how the interaction between the units may generate distinct periodic orbit patterns throughout the network .          the mechanism leading to the emergence of periodic attractors from a system that , without coupling , has no stable finite - period orbits is the permanence of the unstabilized orbits in a flip bifurcation and the contraction effect introduced by the coupling .",
    "the structure of the basins of attraction is also understood from the same mechanism .",
    "the result is@xcite :            for practical purposes some restrictions have to be put on the range of values that the connection strengths may take . for information processing ( pattern storage and pattern recognition )",
    "it is important to preserve , as much as possible , the dynamical diversity of the system .",
    "that means , for example , that a state with all the units synchronized is undesirable insofar as the effective number of degrees of freedom is drastically reduced . from @xmath231",
    "one sees that instability of the fully synchronized state implies @xmath232 .",
    "therefore , the interesting case is when the off - diagonal connections are sufficiently small to insure that @xmath233    for large @xmath113 , provided there is no large scale synchronization effect , a mean - field analysis might be appropriate , at least to obtain qualitative estimates on the behavior of the network . for the unit @xmath47 the average value @xmath234 acts like a constant and the mean - field dynamics is @xmath235 where @xmath236 and @xmath237 @xmath238 is the effective parameter for the mean - field dynamics of unit @xmath47 . from ( [ f3.3 ] ) and ( [ f3.6 ] ) it follows @xmath239 .",
    "the conclusion is that the effective mean - field dynamics always corresponds to a parameter value below the feigenbaum accumulation point , therefore , one expects the interaction to stabilize the dynamics of each unit in one of the @xmath226- periodic orbits . on the other hand to keep the dynamics inside an interesting region",
    "we require @xmath240 , the period-2 bifurcation point . with the estimate @xmath241 one obtains @xmath242 which",
    ", together with ( [ f3.3 ] ) , defines the interesting range of parameters for @xmath243 .",
    "let , for example , the @xmath43 connections be constructed from an input signal @xmath24 by a correlation learning process @xmath244 the dynamical behavior of the network , at a particular time , will reflect the learning history , that is , the data regularities , in the sense that @xmath156 is being structured by the patterns that occur more frequently in the data . the decay term",
    "@xmath245 insures that the off - diagonal terms remain small and that the network structure is determined by the most frequent recent patterns .",
    "alternatively , instead of the decay term , we might use a normalization method and the connection structure would depend on the weighted effect of all the data .    in the operating mode described above the network acts as a _ signal identifier_. for example",
    "if the signal patterns are random , there is little correlation established and all the units operate near the feigenbaum point . alternatively the learning process may be stopped at a certain time and the network then used as a _ pattern recognizer_. in this latter mode , whenever the pattern \\{@xmath24 } appears , one makes the replacement @xmath246 therefore if @xmath43 was @xmath180 but either @xmath24 or @xmath25 is @xmath247 then @xmath248 .",
    "that is , the correlation between node @xmath47 and @xmath249 disappears and the effect of this connection on the lowering of the periods vanishes .",
    "if both @xmath24 and @xmath25 are one , then @xmath250 and the effect of this connection persists .",
    "suppose however that for all the @xmath156 s different from zero either @xmath24 or @xmath25 are equal to zero .",
    "then the correlations are totally destroyed and the network comes back to the uncorrelated ( nonperiodic behavior ) .",
    "this case is what is called a _",
    "novelty filter_. conversely , by displaying periodic behavior , the network _ recognizes _ the patterns that are similar to those that , in the learning stage , determined its connection structure . recognition and _",
    "association _ of similar patterns is then performed @xcite ."
  ],
  "abstract_text": [
    "<S> a survey is made of several aspects of the dynamics of networks , with special emphasis on unsupervised learning processes , non - gaussian data analysis and pattern recognition in networks with complex nodes . </S>"
  ]
}