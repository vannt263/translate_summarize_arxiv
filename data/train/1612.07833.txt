{
  "article_text": [
    "there has been a great deal of interest in multi - modal artificial intelligence research recently , bringing together the fields of computer vision and natural language processing .",
    "this interest has been fueled in part by the availability of many large - scale image datasets with textual annotations .",
    "several vision+language tasks have been proposed around these datasets  @xcite .",
    "image captioning  @xcite and visual question answering  @xcite have in particular attracted a lot of attention .",
    "the performances on these tasks have been steadily improving , owing much to the wide use of deep learning architectures  @xcite .",
    "a central theme underlying these efforts is the use of natural language to identify how much visual information is perceived and understood by a computer system .",
    "presumably , a system that understands a visual scene well enough ought to be able to describe what the scene is about ( thus `` captioning '' ) or provide correct and visually - grounded answers when queried ( thus `` question - answering '' ) .    in this paper ,",
    "we argue for directly measuring how well the semantic representations of the visual and linguistic modalities align ( in some abstract semantic space ) . for instance , given an image and two captions  a correct one and an incorrect yet - cunningly - similar one  can we both qualitatively and quantitatively measure the extent to which humans can dismiss the incorrect one but computer systems blunder ? arguably , the degree of the modal alignment is a strong indicator of task - specific performance on any vision+language task .",
    "consequentially , computer systems that can learn to maximize and exploit such alignment should outperform those that do not .",
    "we take a two - pronged approach for addressing this issue .",
    "first , we introduce a new and challenging dual machine comprehension ( dmc ) task , in which a computer system must identify the most suitable textual description from several options : one being the target and the others being `` adversarialy''-chosen decoys .",
    "all options are free - form , coherent , and fluent sentences with _ high degrees of semantic similarity _ ( hence , they are `` cunningly similar '' ) .",
    "a successful computer system has to demonstrate comprehension beyond just recognizing `` keywords '' ( or key phrases ) and their corresponding visual concepts ; they must arrive at a coinciding and visually - grounded understanding of various linguistic elements and their dependencies .",
    "what makes the dmc task even more appealing is that it admits an easy - to - compute and well - studied performance metric : the accuracy in detecting the true target among the decoys .",
    "second , we illustrate how solving the dmc task benefits related vision+language tasks . to this end",
    ", we render the dmc task as a classification problem , and incorporate it in a multi - task learning framework for end - to - end training of joint objectives .",
    "our work makes the following contributions : ( 1 ) an effective and extensible algorithm for generating decoys from human - created image captions ( section  [ sec : creation ] ) ; ( 2 ) an instantiation of applying this algorithm to the coco dataset  @xcite , resulting in a large - scale dual machine - comprehension dataset that we make publicly available ( section  [ sec : mcic - coco ] ) ; ( 3 ) a human evaluation on this dataset , which provides an upper - bound on performance ( section  [ sec : human_eval ] ) ; ( 4 ) a benchmark study of baseline and competitive learning approaches ( section  [ sec : results ] ) , which underperform humans by a substantial gap ( about 20% absolute ) ; and ( 5 ) a novel multi - task learning model that simultaneously learns to solve the dmc task and the image captioning task ( sections  [ sec : seq+ffnn ] and [ sec : lambda_gen ] ) .",
    "our empirical study shows that performance on the dmc task positively correlates with performance on the image captioning task .",
    "therefore , besides acting as a standalone benchmark , the new dmc task can be useful in improving other complex vision+language tasks .",
    "both suggest the dmc task as a fruitful direction for future research .",
    "image understanding is a long - standing challenge in computer vision .",
    "there has recently been a great deal of interest in bringing together vision and language understanding . particularly relevant to our work",
    "are image captioning ( ic ) and visual question - answering ( vqa ) . both have instigated a large body of publications ,",
    "a detailed exposition of which is beyond the scope of this paper .",
    "interested readers should refer to two recent surveys  @xcite .    in ic tasks ,",
    "systems attempt to generate a fluent and correct sentence describing an input image .",
    "ic systems are usually evaluated on how well the generated descriptions align with human - created captions ( ground - truth ) .",
    "the language generation model of an ic system plays a crucial role ; it is often trained such that the probabilities of the ground - truth captions are maximized ( mle training ) , though more advanced methods based on techniques borrowed from reinforcement learning have been proposed  @xcite . to provide visual grounding ,",
    "image features are extracted and injected into the language model . note",
    "that language generation models need to both decipher the information encoded in the visual features , and model natural language generation .    in vqa tasks ,",
    "the aim is to answer an input question correctly with respect to a given input image . in many variations of this task ,",
    "answers are limited to single words or a binary response ( `` yes '' or `` no '' )  @xcite .",
    "the visual7w dataset  @xcite contains anaswers in a richer format such as phrases , but limits questions to `` wh-''style ( what , where , who , etc ) . the visual genome dataset  @xcite , on the other hand , can potentially define more complex questions and answers due to its extensive textual annotations .    our dmc task is related but significantly different . in our task ,",
    "systems attempt to discriminate the best caption for an input image from a set of captions  all but one are decoys . arguably , it is a form of vqa task , where the same default ( thus uninformative ) question is asked : _ which of the following sentences best describes this image ?",
    "_ however , unlike current vqa tasks , choosing the correct answer in our task entails a deeper `` understanding '' of the available answers .",
    "thus , to perform well , a computer system needs to understand both complex scenes ( visual understanding ) and complex sentences ( language understanding ) , _ and _ be able to reconcile them .",
    "the dmc task admits a simple classification - based evaluation metric : the accuracy of selecting the true target .",
    "this is a clear advantage over the ic tasks , which often rely on imperfect metrics such as bleu  @xcite , rouge  @xcite , meteor  @xcite , cider  @xcite , or spice  @xcite .",
    "related to our proposal is the work in  @xcite , which frames image captioning as a ranking problem .",
    "while both share the idea of selecting captions from a large set , our framework has some important and distinctive components .",
    "first , we devise an algorithm for smart selection of candidate decoys , with the goal of selecting those that are sufficiently similar to the true targets to be challenging , and yet still be reliably identifiable by human raters .",
    "second , we have conducted a thorough human evaluation in order to establish a performance ceiling , while also quantifying the level to which current learning systems underperform .",
    "lastly , we show that there exists a positive correlation between the performance on the dmc task and the performance on related vision+language tasks by proposing and experimenting with a multi - task learning model .",
    "our work is also substantially different from their more recent work  @xcite , where only one decoy is considered and its generation is either random , or focusing on visual concept similarity ( `` switching people or scenes '' ) instead of our focus on both linguistic surface and paragraph vector embedding similarity .",
    "we propose a new multi - modal machine comprehension task to examine how well visual and textual semantic understanding are aligned . given an image , human evaluators or machines",
    "must accurately identify the best sentence describing the scene from several decoy sentences .",
    "accuracy on this task is defined as the percentage that the true targets are identified .",
    "it seems straightforward to construct a dataset for this task , as there are several existing datasets which are composed of images and their ( multiple ) ground - truth captions , including the popular coco dataset  @xcite .",
    "thus , for any given image , it appears that one just needs to use the captions corresponding to other images as decoys .",
    "however , this nave approach could be overly simplistic as it is provides no control over the properties of the decoys .",
    "specifically , our desideratum is to recruit _ challenging _ decoys that are sufficiently similar to the targets .",
    "however , for a small number of decoys , e.g. 4 - 5 , randomly selected captions could be significantly different from the target .",
    "the resulting dataset would be too `` easy '' to shed any insight on the task .",
    "since we are also interested in human performance on this task , it is thus impractical to increase the number of decoys to raise the difficulty level of the task at the expense of demanding humans to examine tediously and unreliably a large number of decoys . in short ,",
    "we need an _ automatic procedure to reliably create difficult sets of decoy captions _ that are sufficiently similar to the targets .",
    "we describe such a procedure in the following .",
    "while it focuses on identifying decoy captions , the main idea is potentially adaptable to other settings .",
    "the algorithm is flexible in that the  difficulty \" of the dataset can be controlled to some extent through the algorithm s parameters .",
    "the main idea behind our algorithm is to carefully define a `` good decoy '' .",
    "the algorithm exploits recent advances in paragraph vector ( pv ) models  @xcite , while also using linguistic surface analysis to define similarity between two sentences .",
    "due to space limits , we omit a detailed introduction of the pv model .",
    "it suffices to note that the model outputs a continuously - valued embedding for a sentence , a paragraph , or even a document .",
    "the pseudo - code is given in algorithm  [ amcic ] ( the name mc - ic stands for `` machine - comprehension for image & captions '' ) .",
    "as input , the algorithm takes a set @xmath0 of @xmath1 pairs , as those extracted from a variety of publicly - available corpora , including the coco dataset  @xcite .",
    "the output of the algorithm is the set .",
    "@xmath2 + @xmath3 + @xmath4 + @xmath5 +    concretely , the mc - ic algorithm has three main arguments : a dataset @xmath6 where @xmath7 is an image and @xmath8 is its ground - truth caption ; an integer @xmath9 which controls the size of @xmath8 s neighborhood in the embedding space defined by the paragraph vector model ; and a function which is used to score the @xmath9 items in each such neighborhood .",
    "the first two steps of the algorithm tune several hyperparameters .",
    "the first step finds optimal settings for the model given the dataset @xmath0 .",
    "the second finds a weight parameter @xmath10 given , dataset @xmath0 , and the function .",
    "these hyperparameters are dataset - specific .",
    "details are discussed in the next section .",
    "the main body of the algorithm , the outer @xmath11 loop , generates a set of @xmath12 ( 4 here ) decoys for each ground - truth caption .",
    "it accomplishes this by first extracting @xmath9 candidates from the neighborhood of the ground - truth caption , excluding those that belong to the same image . in the inner @xmath11 loop",
    ", it computes the similarity of each candidate to the ground - truth and stores them in a list @xmath13 .",
    "if enough candidates are generated , the list is sorted in descending order of score .",
    "the top @xmath12 captions are marked as `` decoys '' ( * false * ) , while the ground - truth caption is marked as `` target '' ( * true * ) .",
    "the score function @xmath14 is a crucial component of the decoy selection mechanism .",
    "its definition leverages our linguistic intuition by combining linguistic surface similarity , @xmath15 , with the similarity suggested by the embedding model , @xmath16 : @xmath17 where the common argument @xmath18 is omitted .",
    "the higher the similarity score , the more likely that @xmath19 is a good decoy for @xmath20 . note that if the surface similarity is above the threshold @xmath21 , the function returns 0 , flagging that the two captions are too similar to be used as a pair of target and decoy .    in this work , @xmath22",
    "is computed as the bleu score between the inputs  @xcite ( with the brevity penalty set to 1 ) .",
    "the embedding similarity , @xmath23 , is computed as the cosine similarity between the two in the pv embedding space .",
    "[ cols= \" < \" , ]     the results in table  [ table : lambda_gen ] illustrate one of the main points of this paper . that is , the ability to perform the comprehension task ( as measured by the accuracy metric ) positively correlates with the ability to perform other tasks that require machine comprehension , such as caption generation",
    ". at @xmath24 , the model not only has a high accuracy of detecting the ground - truth option , but it also generates its own captions given the input image , with an accuracy measured on at 0.9890 ( dev ) and 0.9380 ( test ) cider scores . on the other hand , at an accuracy level of about 59% ( on test , at @xmath25 ) , the generation performance is at only 0.9010 ( dev ) and 0.8650 ( test ) cider scores .",
    "we note that there is an inherent trade - off between prediction accuracy and generation performance , as seen for @xmath26 values above 4.0 .",
    "this agrees with the intuition that training a model using a loss @xmath27 with a larger @xmath26 means that the ground - truth detection loss ( the first term of the loss in eq.[eq : mloss ] ) may get overwhelmed by the word - generation loss ( the second term ) .",
    "however , our empirical results suggest that there is value in training models with a multi - task setup , in which both the comprehension side as well as the generation side are carefully tuned to maximize performance .",
    "we have proposed and described in detail a new multi - modal machine comprehension task ( dmc ) , combining the challenges of understanding visual scenes and complex language constructs simultaneously .",
    "the underlying hypothesis for this work is that computer systems that can be shown to perform increasingly well on this task will do so by constructing a visually - grounded understanding of various linguistic elements and their dependencies .",
    "this type of work can therefore benefit research in both machine visual understanding and language comprehension .",
    "the architecture that we propose for addressing this combined challenge is a generic multi - task model .",
    "it can be trained end - to - end to display both the ability to choose the most likely text associated with an image ( thus enabling a direct measure of its `` comprehension '' performance ) , as well as the ability to generate a complex description of that image ( thus enabling a direct measure of its performance in an end - to - end complex and meaningful task ) .",
    "the empirical results we present validate the underlying hypothesis of our work , by showing that we can measure the decisions made by such a computer system and validate that improvements in comprehension and generation happen in tandem .",
    "the experiments presented in this work are done training our systems in an end - to - end fashion , starting directly from raw pixels .",
    "we hypothesize that our framework can be fruitfully used to show that incorporating specialized vision systems ( such as object detection , scene recognition , pose detection , etc . )",
    "is beneficial .",
    "more precisely , not only it can lead to a direct and measurable impact on a computer system s ability to perform image understanding , but it can express that understanding in an end - to - end complex task .",
    "m.  abadi , a.  agarwal , p.  barham , e.  brevdo , z.  chen , c.  citro , g.  corrado , a.  davis , j.  dean , m.  devin , s.  ghemawat , i.  goodfellow , a.  harp , g.  irving , m.  isard , y.  jia , r.  jozefowicz , l.  kaiser , m.  kudlur , j.  levenberg , d.  man , r.  monga , s.  moore , d.  murray , c.  olah , m.  schuster , j.  shlens , b.  steiner , i.  sutskever , k.  talwar , p.  tucker , v.  vanhoucke , v.  vasudevan , f.  vigas , o.  vinyals , p.  warden , m.  wattenberg , m.  wicke , y.  yu , and x.  zheng .",
    ": large - scale machine learning on heterogeneous systems , 2015 .",
    "software available from tensorflow.org .",
    "s.  banerjee and a.  lavie . : an automatic metric for mt evaluation with improved correlation with human judgments . in _ proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization _ , 2005 .",
    "r.  bernardi , r.  cakici , d.  elliott , a.  erdem , e.  erdem , n.  ikizler - cinbis , f.  keller , a.  muscat , and b.  plank .",
    "automatic description generation from images : a survey of models , datasets , and evaluation measures .",
    ", 55 , 2016 .",
    "k.  cho , b.  van merrienboer ,  .",
    "glehre , d.  bahdanau , f.  bougares , h.  schwenk , and y.  bengio . learning phrase representations using rnn encoder - decoder for statistical machine translation . in _ proceedings of emnlp ,",
    "october 25 - 29 , 2014 , doha , qatar _ , pages 17241734 , 2014 .",
    "j.  donahue , l.  a. hendricks , s.  guadarrama , m.  rohrbach , s.  venugopalan , k.  saenko , and t.  darrell .",
    "long - term recurrent convolutional networks for visual recognition and description . in _ proc .",
    "of ieee conference on computer vision and pattern recognition ( cvpr ) _ , 2014 .",
    "h.  fang , s.  gupta , f.  iandola , r.  srivastava , l.  deng , p.  dollr , j.  gao , x.  he , m.  mitchell , j.  platt , et  al . from captions to visual concepts and back . in _ proc . of ieee conference on computer vision and pattern recognition ( cvpr ) _ , 2015 .",
    "r.  krishna , y.  zhu , o.  groth , j.  johnson , k.  hata , j.  kravitz , s.  chen , y.  kalantidis , l .- j .",
    "li , d.  a. shamma , m.  bernstein , and l.  fei - fei . : connecting language and vision using crowdsourced dense image annotations . 2016 .",
    "i.  sutskever , o.  vinyals , and q.  v.  v. le .",
    "sequence to sequence learning with neural networks . in _ advances in neural information processing systems 27",
    "_ , pages 31043112 .",
    "curran associates , inc . , 2014 .",
    "k.  xu , j.  ba , r.  kiros , a.  courville , r.  salakhutdinov , r.  zemel , and y.  bengio .",
    "show , attend and tell : neural image caption generation with visual attention . in _ proc .",
    "of the 32nd international conference on machine learning ( icml ) _ , 2015 .",
    "b.  yao and f .- f .",
    "li . modeling mutual context of object and human pose in human - object interaction activities . in _ proceedings of the 2010 ieee conference on computer vision and pattern recognition ( cvpr ) _ , 2010 ."
  ],
  "abstract_text": [
    "<S> we introduce a new multi - modal task for computer systems , posed as a combined vision - language comprehension challenge : identifying the most suitable _ text _ describing a scene , given several similar options . accomplishing </S>",
    "<S> the task entails demonstrating comprehension beyond just recognizing `` keywords '' ( or key - phrases ) and their corresponding visual concepts . </S>",
    "<S> instead , it requires an alignment between the representations of the two modalities that achieves a visually - grounded `` understanding '' of various linguistic elements and their dependencies . </S>",
    "<S> this new task also admits an easy - to - compute and well - studied metric : the accuracy in detecting the true target among the decoys .    </S>",
    "<S> the paper makes several contributions : an effective and extensible mechanism for generating decoys from ( human - created ) image captions ; an instance of applying this mechanism , yielding a large - scale machine comprehension dataset ( based on the coco images and captions ) that we make publicly available ; human evaluation results on this dataset , informing a performance upper - bound ; and several baseline and competitive learning approaches that illustrate the utility of the proposed task and dataset in advancing both image and language comprehension . we also show that , in a multi - task learning setting , the performance on the proposed task is positively correlated with the end - to - end task of image captioning . </S>"
  ]
}