{
  "article_text": [
    "many mathematical models used in science and technology contain parameters for which a direct observation is very difficult .",
    "a good example is subsurface geophysics .",
    "the aim in subsurface geophysics is the reconstruction of subsurface properties such as density and permeability given measurements on the surface.using the laws of physics , these properties can be used as parameters of a forward model mapping them to the measurements which we subsequently call data .",
    "inverting such a relationship is non - trivial and lies in the focus of the area of inverse problems .",
    "classically , these parameters are estimated by minimisation of a regularised least squares functional which is based on the data output mismatch ( tikhonov ) .",
    "the idea of this approach is to use optimisation techniques aiming at parameters that produce nearly the same noiseless output as the given noisy data while being not too irregular .",
    "however , it is difficult to quantify how the noise in the data translates into the uncertainty of the reconstructed parameters for this method .",
    "uncertainty quantification is much more straightforward in the bayesian approach .",
    "the basic idea of the bayesian method is that not all parameter choices are a priori equally likely . instead , the parameters are artificially treated as random variables by modelling their distribution using a priori knowledge .",
    "this distribution is accordingly called the prior . for a specific forward model and",
    "given the distribution of the observational noise , the parameters and the data can be treated as jointly varying random variables . under mild conditions , the prior",
    "can then be updated by conditioning the parameters on the data .",
    "the posterior is one of the main tools for making inference about the parameters .",
    "possible estimates include approximation of the posterior mean or the maximum a posteriori ( map ) estimator .",
    "moreover , it is possible to quantify the uncertainty of the reconstructed parameter by posterior variance or posterior probability of a set around for example an estimate of the parameters under consideration .",
    "the main focus of this article lies on posterior consistency which quantifies the quality of the resulting posterior in a thought experiment . as for any evaluation for an approach to inverse problems , an identical twin experiment is performed , that is for a fixed set of parametersand artificial data is generated .",
    "it is conceivable to expect that , under appropriate conditions , the posterior concentrates around this set of true parameters .",
    "results of this type are called posterior consistency .",
    "it justifies the bayesian method by establishing that this method recovers the true parameters sometimes with a specific rate .",
    "so far , there are only posterior consistency results available for linear forward models and mainly gaussian priors @xcite . in this article",
    ", we prove posterior consistency of nonlinear inverse problems with explicit bounds on the rate . the main idea behind our posterior consistency results is to use stability properties of the deterministic inverse problem to reduce posterior consistency of a nonlinear inverse problem to posterior consistency of a bayesian non - parametric regression problem .",
    "our guiding example is the inverse problem of reconstructing the diffusion coefficient from measurements of the pressure .",
    "more precisely , we assume that the relation between the diffusion coefficient @xmath0 and the pressure @xmath1 satisfies the following partial differential equation ( pde ) with dirichlet boundary conditions @xmath2 where @xmath3 is a bounded smooth domain in @xmath4 for this guiding example the required stability results are due to @xcite .",
    "however , our methods are generally applicable to inverse problems with deterministic stability results .",
    "these are often available in the literature because they are also needed for convergence results of the tikhonov regularisation ( consider for example theorem 10.4 . in @xcite ) .",
    "finally , we complete our reasoning by proving appropriate posterior consistency results for the corresponding bayesian non - parametric regression problem .      in section [ sec : preliminary ] , we both review preliminary material and give a detailed exposition of our main ideas , steps and results . in section [ sec : general - results ] , we provide novel posterior consistency results for bayesian non - parametric regression . in order to evaluate the rate for the regression problem , we compare our rates to those for gaussian priors for which optimal rates are known . these results are needed in order to obtain posterior consistency for the elliptic inverse problem in section [ sec : results - for - elliptic ] . we obtain explicit rates for priors based on a series expansion with uniformly distributed coefficients . in section [ sec : conclusion ] , we draw a conclusion and mention other inverse problems to which this approach is applicable .",
    "the appendix contains a detailed summary of relevant technical tools such as gaussian measures and hilbert scales which are used in the proofs of our main results .",
    "the author would like to thank professor martin hairer , professor andrew stuart , dr .",
    "hendrik weber and sergios agapiou for helpful discussions .",
    "sjv is grateful for the support of an erc scholarship .",
    "our crucial idea for proving posterior consistency for a nonlinear bayesian inverse problem is the use of stability results which allow us to break it down to posterior consistency of a bayesian regression problem . because the proofs are quite technical , it is worth becoming familiar with the outline of our main ideas first .",
    "therefore this section is intended to motivate , review and summarise our investigation of posterior consistency for a nonlinear inverse problem leaving technical details to the sections [ sec : general - results ] and [ sec : results - for - elliptic ] . for the convenience of the reader",
    "we also repeat the relevant material on bayesian inverse problems in section [ sub : reviewbayesian ] without proofs , thus making our exposition self - contained . in section [ sub : posterior - consistency ] , we precisely define posterior consistency in this setting and place it within the literature . * * s**ubsequently , we introduce an elliptic inverse problem as guiding example for which we apply our method using stability results from @xcite .",
    "finally , we conclude our exposition by giving a general abstract theorem of posterior consistency for nonlinear inverse problems with stability results in section [ sub : consistency - through - stability ] .",
    "the key idea of bayesian inverse problems is to model the input @xmath5 of a mathematical model , for example the initial condition of a pde , as random variable with distribution @xmath6 based on a priori knowledge .",
    "this distribution is called the prior which is updated based on the observed data @xmath7 .",
    "the resulting distribution @xmath8 is called posterior and lies in the focus of the bayesian approach .",
    "we assume that the data is modelled as @xmath9 with @xmath10 being the forward operator , a mapping between the hilbert spaces @xmath11 and @xmath12 , and with the observational noise @xmath13 .",
    "the aim of the inverse problem is the reconstruction of @xmath14 given the data @xmath15 .",
    "because @xmath10 might be non - injective and @xmath16 is unknown , the problem is not exactly solvable as stated . * * if the distribution of the noise @xmath13 is known , then @xmath14 and @xmath15 can be treated as jointly varying random variables . under mild assumptions on the prior , the distribution of the noise and the forward operator",
    ", there exists a conditional probability measure on @xmath14 , called the posterior @xmath17 .",
    "it is an update of the prior using the data and models the a posteriori uncertainty .",
    "therefore it can be viewed as the solution to the inverse problem itself . in this way it is possible to obtain different explanations of the data corresponding to different modes of the posterior .    in this article",
    ", we assume that the law of the observational noise @xmath18 is a mean - zero gaussian with covariance @xmath19 . in this case bayes rule",
    "can be generalised for any @xmath10 mapping into a finite dimensional space @xmath12 .",
    "it follows that    @xmath20    by @xmath21 we denote the norm of the cameron - martin space @xmath22 of @xmath23 that is the closure of @xmath12 with respect to @xmath24 ( see [ sec : notation ] for more details ) . a proper derivation of equation ( [ eq : generalposterior ] ) , including the fact that its last line is also valid for functional data , and an appropriate introduction to bayesian inverse problems can be found in @xcite and @xcite .",
    "all in all the bayesian approach can be summarised as @xmath25 \\mbox{noise } & \\noise\\:\\sim\\mathcal{n}(0,\\gamma ) \\\\[10pt ] \\mbox{posterior } & \\frac{d\\mu^{y}}{d\\mu_{0}}\\propto\\exp\\left(-\\frac{1}{2}\\bigl\\vert\\opobs(\\input)\\bigr\\vert_{\\obscov}^{2}+\\left\\langle y,\\opobs(\\input)\\right\\rangle _ { \\obscov}\\right ) .",
    "\\end{array}\\label{eq : bayesinv}\\ ] ] as one can see in this example , the posterior can usually only be expressed implicitly as an unnormalised density with respect to the prior .",
    "thus , in order to estimate the input parameters or perform inference using the posterior , it has to be probed using either    * sampling methods , such as mcmc which aim at generating draws from the posterior or * variational methods for determining the location of an infinitesimal ball with maximal posterior probability .",
    "the second approach is also called the maximum a posteriori probability ( map ) estimator .",
    "it can be viewed as an extension to many classical methods for inverse problems .",
    "for example , it can be linked to the @xmath26-tikhonov regularisation by considering a gaussian prior and noise @xcite .",
    "this relates the choice of norms in the tikhonov regularisation to the choice of the covariance of the prior and the noise .",
    "these regularisation techniques can be justified by convergence results . similarly , inference methods based on the posterior can be justified by posterior consistency , a concept which we introduce in the next section .",
    "as for any approach to inverse problems , the bayesian method can be evaluated by considering an identical twin experiment .",
    "therefore a fixed input @xmath27 , called the _",
    "truth _ , is considered and data is generated using a sequence of forward models @xmath28 which might correspond to the increasing amount of data or diminishing noise .",
    "for each @xmath29 we denote the posterior corresponding to the prior @xmath30 , the noise distribution @xmath31 and the forward operator @xmath32 by @xmath33 . under appropriate assumptions ,",
    "the posterior @xmath34 is well - defined for @xmath35 given by bayes rule in equation ( [ eq : bayesinv ] ) for @xmath30-a.e . @xmath14 and @xmath31-a.e .",
    "@xmath36 ( c.f .",
    "this bayes rule does not give rise to a well - defined measure for arbitrary @xmath7 .",
    "however , we will pose assumptions such that the normalising constant in the bayes rule will be bounded above and below for every @xmath37 belonging to a particular set and @xmath31-a.e .",
    "we will denote these posteriors by @xmath39 .",
    "this sequence of inverse problems is called posterior consistent if the posteriors @xmath40 concentrate around the truth @xmath37 .",
    "we quantify the concentration by the posterior probability assigned to the ball @xmath41 . here",
    "@xmath41 denotes a ball of radius @xmath42 with respect to a metric @xmath43 .    in the following we define this concept precisely and",
    "place it within the literature before closing this section by relating posterior consistency to small ball probabilities for the prior .",
    "* [ def : posterior - consistency]*(analogue to @xcite ) a sequence of bayesian inverse problems @xmath44 is posterior consistent for @xmath37 with rate @xmath45 and with respect to a metric @xmath43 if for @xmath46 there exists a constant @xmath47 and a sequence @xmath48 such that @xmath49 we simply say that @xmath44 is posterior consistent if the above holds for any fixed constant @xmath50 .",
    "two important special cases of this definition are    * posterior consistency in the small noise limit : @xmath51 * posterior consistency in the large data limit : @xmath52    in the above formulation @xmath53 corresponds to different measurements while @xmath54 denotes the law of a random variable . *",
    "*    there exists a variety of results for * * posterior consistency and inconsistency for statistical problems .",
    "two important examples are the identification of a distribution from ( often i.i.d . ) samples or density estimation @xcite .",
    "the former is concerned with considering a prior on a set of probability distributions and the resulting posterior based on @xmath29 samples of one of these probability distributions . in @xcite",
    ", doob proved that if a countable collection of samples almost surely allows the identification of the generating distribution , then the posterior is consistent for almost every probability distribution with respect to the prior .",
    "this very general result is not completely satisfactory because it does not provide a rate and the interest may lie in showing posterior consistency for every possible truth in a certain class .",
    "moreover , some surprisingly simple examples of posterior inconsistency have been provided for example by considering distributions on @xmath55 @xcite .",
    "the necessary bounds for posterior consistency ( c.f . equation ( [ eq : posteriorconsistency ] ) ) can be obtained using the existence of appropriate statistical tests which are due to bounds on entropy numbers .",
    "these methods are used in a series of articles , for example in @xcite .",
    "this idea has also recently been applied to the bayesian approach to linear inverse problems in @xcite .    in general , posterior consistency for infinite dimensional inverse problems",
    "has mostly been studied for linear inverse problems in the small noise limit where the prior is either a sieve prior , a gaussian or a wavelet expansion with uniform distributed coefficients @xcite . except for @xcite",
    ", all these articles exploit the explicit structure of the posterior in the conjugate gaussian setting , that means that we have a gaussian prior as well as a gaussian posterior .",
    "in contrast , we consider general priors , general forward operators and gaussian noise in this article .",
    "usually , the posterior has a density with respect to the prior as in equation ( [ eq : bayesinv ] ) .",
    "however , it is possible to provide examples where both the prior and posterior are gaussian but not absolutely continuous .",
    "this can be achieved using for example proposition 3.3 in @xcite .",
    "subsequently , we assume that the posterior has a density with respect to the prior implying that the posterior probability of a set is zero whenever the prior probability of this set is zero .",
    "therefore it is necessary that @xmath37 is in the support of the prior giving rise to the following definition .",
    "[ ass : nondegeneratedef]the support of a measure @xmath56 in a metric space @xmath57 is given by    @xmath58    it is natural to expect that the posterior consistency rate depends on the behaviour of @xmath59 as @xmath60 .",
    "asymptotics of this type are called small ball probabilities .",
    "we recommend @xcite as a good survey and refer the reader to @xcite for an up - to - date list of references . in this article",
    ", we consider algebraic rates of posterior consistency , that means we take @xmath61 in definition [ def : posterior - consistency ] . in order to establish these rates of posterior consistency , we consider small ball asymptotics of the following form    @xmath62 where @xmath63 and with the notation as in appendix [ sec : notation ] .",
    "both posterior consistency and the contraction rate depend on properties of the prior .",
    "this suggests that we should choose a prior with favourable posterior consistency properties . from a dogmatic point of view",
    "the prior is only supposed to be chosen to match the subjective a priori knowledge . in practice priors",
    "are often picked based on their computational performance whereas some of their parameters are adapted to represent the subjective knowledge .",
    "an example for this is the choice of the base measure and the intensity for a dirichlet process @xcite .",
    "finally , we would like to conclude this section by mentioning that it has been shown in @xcite that posterior consistency is equivalent to the property that the posteriors corresponding to two different priors merge .",
    "the yet unpublished book @xcite contains a more detailed discussion about the justification of posterior consistency studies for dogmatic bayesians .",
    "the aim of this section is to set up the elliptic inverse problem for which we will prove posterior consistency ( c.f .",
    "section [ sub : posterior - consistency ] ) both in the small noise and the large data limit . in a second step",
    "we describe the available stability results and how they can be used to reduce the problem of posterior consistency of a nonlinear inverse problem to that of a linear regression problem .",
    "we end this section by stating a special case of our posterior consistency results in section [ sec : results - for - elliptic ] .",
    "our results do not only apply to this particular elliptic inverse problem but to any nonlinear inverse problem with appropriate stability results ( c.f .",
    "section [ sub : consistency - through - stability ] ) .",
    "however , the results for the elliptic inverse problem are of particular interest because it is used in oil reservoir simulations and the reconstruction of the groundwater flow @xcite .",
    "the forward model corresponding to our elliptic inverse problem is based on the relation between @xmath64 and @xmath14 given by the elliptic pde in [ eq : postconellipticforward ] .",
    "we would like to highlight that the relation between @xmath14 and @xmath64 is nonlinear .",
    "under the following assumptions , the solution operator @xmath65 to the above pde is well - defined @xcite .",
    "( [ ass : ellipticforward]forward conditions ) suppose that    1 .",
    "@xmath3 is compact , satisfies the exterior sphere condition ( see @xcite ) and has a smooth boundary ; 2 .",
    "@xmath66 and f is smooth in @xmath67 ; 3 .",
    "@xmath68 and @xmath69 in equation ( [ eq : postconellipticforward ] ) .",
    "under these assumptions , the regularity results from @xcite yield the following forward stability result .    [",
    "prop : forwardstability]if @xmath70 and @xmath71 satisfy assumption [ ass : ellipticforward ] and are elements of @xmath72 for @xmath73 , then @xmath74    the inverse problem is concerned with the reconstruction of @xmath0 given the data @xmath75 which is related to @xmath1 in the following way .",
    "[ ass : forwardsplitting]the forward operator @xmath10 can be split into a composition of the solution operator @xmath64 and an observation operator @xmath76 , that is @xmath77    the bayesian approach to the elliptic inverse problem ( eip ) summarises as @xmath78 \\mbox{prior } & \\prior\\mbox { on } \\input \\\\[7pt ] \\mbox{data } & \\data=\\opobs_{n}(\\input)+\\noise_{n}=\\mathcal{o}_{n}(\\pressure(\\cdot,\\input))+\\noise_{n},\\,\\noise_{n}\\sim\\mathcal{n}(0,\\gamma_{n } ) \\\\[7pt ] \\mbox{posterior } & \\frac{d\\mu^{n}}{d\\prior}(\\input)\\propto\\exp\\left(-\\frac{1}{2}\\bigl\\vert\\opobs_{n}(\\input)\\bigr\\vert_{\\obscov_{n}}^{2}+\\left\\langle y,\\opobs(\\input)\\right\\rangle _ { \\obscov_{n}}\\right ) .",
    "\\end{array}}\\quad\\left(\\mbox{eip}\\right)\\ ] ] a rigorous bayesian formulation of this inverse problem , with log - gaussian priors and besov priors has been given in @xcite and @xcite respectively .",
    "in @xcite the problem is considered with a prior based on a series expansion with uniformly distributed coefficients ( see section [ sub : posterior - consistency - rate ] ) . in the same article , a generalised polynomial chaos ( gpc )",
    "method is derived in order to approximate posterior expectations .",
    "we consider posterior consistency as set up in definition [ def : posterior - consistency ] in the following cases :    * the small noise limit with @xmath79 corresponding to a functional observation and an additive gaussian random field as noise such that @xmath80 * the large data limit with @xmath81 where @xmath82 are evaluations at @xmath83 . in this case",
    "the data takes the form @xmath84    posterior consistency in both cases are based on a stability result which can be derived by taking @xmath14 as the unknown in equation ( [ eq : postconellipticforward ] ) .",
    "this leads to the following hyperbolic pde @xmath85    imposing assumption [ ass : ellipticforward ] , it has been established that there exists a unique solution @xmath14 to this pde without any additional boundary conditions :    [ prop : richter]suppose @xmath64 arises as a solution to equation ( [ eq : postconellipticforward ] ) with @xmath14 as diffusion coefficient satisfying assumption [ ass : ellipticforward ]",
    ". then equation ( [ eq : hyperbolicpde ] ) is uniquely solvable for any @xmath86 and @xmath14 such that @xmath87 moreover , if @xmath70 and @xmath71 satisfy these assumptions , then    the stability result above and a change of variables ( theorem [ thm : indpendentofdiscription ] ) implies    @xmath88 this statement reduces posterior consistency of the eip  in @xmath89 to posterior consistency of the following bayesian regression problem ( brp ) in @xmath90 @xmath91 \\text{data } & \\data=\\mathcal{o}_{n}(\\pressure)+\\noise_{n},\\,\\noise_{n}\\sim\\mathcal{n}\\left(0,\\gamma_{n}\\right ) \\\\[7pt ] \\text{posterior } & \\frac{d\\tilde{\\mu}^{y_{n}}}{d\\tilde{\\prior}}(\\pressure)\\propto\\exp\\left(-\\frac{1}{2}\\bigl\\vert\\mathcal{o}(\\pressure)\\bigr\\vert_{\\obscov_{n}}^{2}+\\left\\langle y,\\mathcal{o}(\\pressure)\\right\\rangle _ { \\obscov_{n}}\\right ) \\\\[7pt ]   & \\text{with } \\mathcal{o}_{n}=\\text{id}\\text { or } \\mathcal{o}_{n}=\\left(e_{x_{i}}\\right)_{i=1}^{n } \\end{array}}\\quad\\left(\\text{brp}\\right)\\ ] ] where @xmath1 is now treated as an variable , that is the prior and the posterior are now formulated on the pressure space .",
    "moreover , @xmath92 denotes the push forward of the prior under @xmath1 .",
    "note that for @xmath93 the brp  can also be viewed as the simplest linear inverse problem .",
    "the required posterior consistency results for the brp  can be derived from those in section [ sec : general - results ] using interpolation inequalities . in this way",
    "we obtain posterior consistency results in section [ sec : results - for - elliptic ] a special case of which is the following theorem :    suppose that the prior @xmath30 satisfies @xmath94 let the noise be given by @xmath95 . if @xmath96 and @xmath97 , then ( eip ) is posterior consistent for any @xmath98 in the small noise limit with respect to the @xmath99-norm for any @xmath100 .",
    "this approach is not limited to the eip   as the following section shows .      in section [ sub : introelliptic ] , we present our main idea , that is the reduction of the problem of posterior consistency of the eip   to that of the brp .",
    "the main ingredients of this reduction are the stability result that was summarised in proposition [ prop : richter ] and the posterior consistency results for the brp .",
    "this approach is not limited to the eip   but it is applicable to any inverse problem for which appropriate stability results are available .",
    "this is the case for many inverse problems such as the inverse scattering problem in @xcite or the calderon problem in @xcite .",
    "we would like to point out that these stability results are also crucial for proving the convergence of regularisation methods ( see theorem 10.4 in @xcite ) .",
    "suppose @xmath101 with @xmath102 and @xmath103 .",
    "moreover , we assume that    * there exists a stability result of the form @xmath104 * the sequence of bayesian inverse problems @xmath105 is posterior consistent with respect to @xmath106 for all @xmath107 with rate @xmath108 .",
    "then @xmath44 is posterior consistent with respect to @xmath109 for all @xmath110 with rate @xmath111    using the notation of section [ sub : introelliptic ] , we denote the posteriors for the bayesian inverse problems @xmath44 and @xmath105 by @xmath40 and @xmath112 , respectively . then a change of variables ( c.f .",
    "theorem [ thm : indpendentofdiscription ] ) implies @xmath113",
    "as described in the previous section , for many inverse problems posterior consistency can be reduced to posterior consistency of a brp  ( c.f .",
    "section [ sub : consistency - through - stability ] ) using stability results .",
    "thus , with the results obtained in this section we may conclude posterior consistency for apparently harder nonlinear inverse problems .",
    "for the eip  this is achieved by an application of the results in theorem [ thm : notail ] and [ thm : largedata ] . because the derivation of these two results is quite technical",
    ", we first give a summary and we recommend the reader to become familiar with both theorems but to skip the technical details on the first read .",
    "it is classical to model the response as @xmath114 in the following we consider two bayesian regression models with    * @xmath93 and the noise is a gaussian random field that is scaled to zero like @xmath115 or * @xmath81 and @xmath116 corresponding to evaluations of a function with additive i.i.d .",
    "gaussian noise .",
    "these models represent the large data and the small noise limit , respectively . *",
    "*    we prove posterior consistency for both problems under weak assumptions on the prior .",
    "this is necessary because the brps resulting from nonlinear inverse problems are usually only given in an implicit form .",
    "for both cases we are able to obtain a rate assuming appropriate asymptotic lower bounds on the small ball probabilities of the prior around @xmath37 ( see section [ sub : posterior - consistency ] )",
    ". moreover , posterior consistency with respect to stronger norms can be obtained using prior or posterior regularity in combination with interpolation inequalities which is the subject of section [ sub : convergence - in - stronger ] .    for the large data limit , that is @xmath81",
    ", we obtain posterior consistency with respect to the @xmath89-norm in section [ sub : point - wise - observations - in ] .",
    "we assume an almost sure upper bound on a hlder norm for the prior and an additional condition on the locations of the observations .",
    "the latter is justified by construction of a counterexample .",
    "for the small noise limit , that is @xmath93 , we prove posterior consistency with respect to the cameron - martin norm of the noise in section [ sub : the - small - noise ] .",
    "this norm corresponds to the @xmath117-norm in the hilbert scale with respect to the covariance operator @xmath118 .",
    "both the cameron - martin norm and hilbert scales are introduced in [ sec : notation ] . if an appropriate @xmath119-norm is @xmath30-a.s",
    ". bounded , we obtain an explicit rate of posterior consistency .",
    "otherwise , the rate is implicitly given as a low - dimensional optimisation problem . however , the condition for mere posterior consistency takes a simple form .",
    "( see corollary [ cor : postconsmallnorate ] for the case of general noise ) + suppose that the noise is given by @xmath120 and @xmath121 for @xmath122 and @xmath123 .",
    "then the posterior is consistent in @xmath124 for any @xmath125 if @xmath126 and @xmath127 satisfy the following conditions    [ rem : noratecorollary - gaussian]if the prior is gaussian , then the above inequality is satisfied because @xmath128 and the rhs is less than @xmath129 for any @xmath130 .",
    "thus , the only remaining condition is @xmath122 .",
    "[ rem : noratecorollary - logconcave]it is worth pointing out that for the large class of log - concave measures it is known that @xmath131 , for details consult @xcite .    in the statistics literature regression models",
    "are mainly concerned with pointwise observations . despite its name",
    "this is also true for _ functional data analysis _ * * ( see @xcite ) .",
    "however , the regression problem associated with @xmath93 can be viewed as a particular linear inverse problem . as described in the introduction ,",
    "this has been studied for gaussian priors in @xcite and @xcite .",
    "although our focus lies on establishing posterior consistency for general priors and non - linear models , we also obtain rates which in the special case of gaussian priors are close to the optimal rates given in the references above .      in the following",
    "we study posterior consistency for a bayesian regression problem assuming that the data takes values in the hilbert space @xmath132 .",
    "in particular we deal with the regression model @xmath133 with @xmath134 and @xmath16 all being elements of @xmath132 . moreover",
    ", we suppose that the observational noise @xmath13 is a gaussian random field @xmath18 on @xmath132 and we assume that it satisfies the following assumption .",
    "[ ass : noisetracesumm]suppose there is @xmath135 such that @xmath136 is trace - class for all @xmath137 , that is @xmath138    imposing this assumption , it becomes possible to quantify the regularity of the observational noise in terms of the hilbert scale defined with respect to the covariance operator ( c.f . [ sec : notation ] ) .",
    "more precisely , this is possible due to lemma [ lem : regularitynoise ] . from @xcite .",
    "the regression model in equation ( [ eq : postconinvers ] ) is a special case of a general inverse problem as considered in equation ( [ eq : ip ] ) .",
    "hence the corresponding posterior takes the following form ( c.f . equation ( [ eq : bayesinv ] ) ) .",
    "@xmath139    assuming that the data takes values in the hilbert space @xmath132 , equation ( [ eq : brpposterior ] ) can simply be derived by an application of the cameron - martin lemma in combination with the conditioning lemma ( lemma 5.3 in @xcite ) .",
    "we generate data for a fixed truth @xmath37 @xmath140 by changing the normalising constant , we may rewrite the posterior in the following way @xmath141    the normalising constant is bounded above and below for @xmath142 for @xmath31-a.e .",
    "@xmath16 . in fact , this holds under weaker assumptions than needed for our results .",
    "[ lem : brpnormalising]suppose @xmath143 for @xmath144 and @xmath145 .",
    "then the normalising constant in equation ( [ eq : identityposterior ] ) is bounded for @xmath146-a.s . and every @xmath147 above and away form zero .",
    "see [ sec : normal ] .",
    "the expression above suggests that the posterior concentrates in balls around the truth in the cameron - martin norm .",
    "first , we make this fact rigorous for priors which are a.s .",
    "uniformly bounded with respect to the @xmath148-norm . in a second step , we assume that the prior has higher exponential moments .",
    "considering gaussian priors , we show that our rate is close to the optimal rate obtained in @xcite .",
    "the following theorem can be viewed as a preliminary step towards theorem [ thm : postconsmallgeneral ] which contains our most general posterior consistency result for the bayesian regression problem in the small noise limit .",
    "while containing our main ideas , the following result also establishes an explicit rate for posterior consistency which will be used for the eip   in section [ sec : results - for - elliptic ] .",
    "[ thm : notail ] suppose that the noise satisfies assumption [ ass : noisetracesumm ] and @xmath149 for @xmath150 if @xmath151 and then @xmath40 is consistent in @xmath152 . additionally , if the following small ball asymptotic is satisfied @xmath153 then this holds with rate @xmath154 for any @xmath155 with @xmath156 .",
    "our proof is based on the observation that posterior consistency is implied by the existence of a sequence of subsets @xmath157 such that @xmath158 and @xmath159 where @xmath160 .",
    "this implication holds because @xmath161 and thus @xmath162 which together with @xmath158 implies posterior consistency , for details see equation ( [ eq : posteriorconsistency ] ) .",
    "fix @xmath163 .",
    "then @xmath164 with @xmath165 as @xmath166 sufficiently slow .",
    "we notice that lemma [ lem : regularitynoise ] implies that @xmath167 as @xmath166 .",
    "the remainder of the proof will be devoted to showing that equation ( [ eq : ratiolimit ] ) holds .",
    "we bound @xmath168 by smoothing @xmath16 at the expense of @xmath169 @xmath170 interpolating between @xmath152 and @xmath171 for @xmath172 ( c.f .",
    "lemma [ lem : hscaleinterpolation ] ) yields    @xmath173    with @xmath174 .",
    "an application of equation ( [ eq : identityposterior ] ) yields the following upper bound @xmath175\\nonumber\\\\",
    "\\hspace{-0.4 cm } & \\ge \\hspace{-0.1cm}z(n,\\xi)\\exp\\left[-n^{1 - 2\\kappa}\\left[\\frac{\\epsilon\\bigl\\vert a - a^{\\dagger}\\bigr\\vert_{1}}{2}\\right]^{2}-k_{n}n^{\\frac{1}{2}-\\lambda\\kappa}\\left(\\frac{\\epsilon}{2}\\right)^{\\lambda}\\right]\\hspace{-0.1cm}\\prior\\hspace{-0.1cm}\\left[b_{\\frac{\\epsilon}{2}n^{-\\kappa}}^{1}\\left(\\truth\\right)\\right]\\hspace{-0.1cm}.\\label{eq : postconsmallnottaillower}\\end{aligned}\\ ] ] similarly , we obtain the following upper bound    @xmath176    the expression in the exponential in equation ( [ eq : identityposterior ] ) can be rewritten as a function @xmath177 of @xmath178 which is decreasing on @xmath179 . if @xmath180 then @xmath181 for @xmath182 and @xmath29 large enough leading to @xmath183 we now derive sufficient conditions for @xmath184 to be the dominant term in the exponential in the equations ( [ eq : postconsmallnottaillower ] ) and ( [ eq : postconsmallnottailupper ] ) implying equation ( [ eq : ratiolimit ] ) .",
    "this is the case if , in addition to inequality ( [ eq : postconsmallnotailrate1 ] ) , @xmath185 hold .",
    "the first line is equivalent to inequality ( [ eq : postconsmallnotailrate1 ] ) and using inequality ( [ eq : yhmnotailsmallball ] ) the second line is implied by @xmath186 thus , the inequalities ( [ eq : postconsmallnotailrate1 ] ) and ( [ eq : postconsmallnotailrate2 ] ) imply that @xmath187 is the dominant term in the inequalities ( [ eq : postconsmallnottaillower ] ) and ( [ eq : postconsmallnottailupper ] ) establishing equation ( [ eq : ratiolimit ] ) .",
    "letting @xmath188 concludes the proof .",
    "in the following we weaken the assumptions of theorem [ thm : notail ] by assuming that the prior has exponential moments of @xmath189 .",
    "the price we pay is that the algebraic rate of convergence is implicitly given as a low - dimensional optimisation problem .",
    "[ thm : postconsmallgeneral]suppose that the noise satisfies assumption [ ass : noisetracesumm ] , the prior satisfies the small ball asymptotic @xmath190 and @xmath191@xmath192 for @xmath123 and @xmath193 for @xmath144 .",
    "if the following optimisation problem has a solution @xmath194 , then for any @xmath195 the posterior @xmath40 is consistent in @xmath152 for @xmath37 in @xmath171 with rate @xmath196 .",
    "@xmath197 where @xmath198",
    ".    see appendix [ sec : proof - of - theorem ] .",
    "[ rem : postcongenerals]in general , @xmath199 might depend on @xmath172 for @xmath191@xmath192 to hold . therefore the rate might be improved by optimising over different @xmath144 .",
    "whereas the algebraic rate in theorem [ thm : postconsmallgeneral ] is implicit , the following corollary yields a simple condition implying posterior consistency .    [ cor : postconsmallnorate]suppose that the noise satisfies assumption [ ass : noisetracesumm ] , and @xmath191@xmath192 for @xmath123 , @xmath193 and @xmath144 .",
    "if one of the following two conditions holds @xmath200 then @xmath40 is posterior consistent for @xmath37 in @xmath171 .",
    "it follows from the proof of theorem [ thm : postconsmallgeneral ] that we only have to find @xmath201 , @xmath202 , @xmath203 and @xmath172 such that the inequalities ( [ eq : postconsmallrate1 ] ) , ( [ eq : postconsmallrate2 ] ) , ( [ eq : postconsmallrate5 ] ) , ( [ eq : postconsmallrate7 ] ) and ( [ eq : postconsmallrate8 ] ) are satisfied . choosing @xmath201 as large as inequality ( [ eq : postconsmallrate1 ] ) permits , that is @xmath204 , extends the range of solutions of the other inequalities ( ( [ eq : postconsmallrate2 ] ) and ( [ eq : postconsmallrate8 ] ) ) containing @xmath201",
    "similarly , choosing @xmath205 as large as ( [ eq : postconsmallrate2 ] ) permits , that is @xmath206 , extends the range of solutions of inequality ( [ eq : postconsmallrate8 ] ) .",
    "letting @xmath60 in ( [ eq : postconsmallrate8 ] ) yields @xmath207@xmath208    now it is left to perform a case - by - case analysis .",
    "starting from inequality ( [ eq : postconnorate1 ] ) , the first two cases are @xmath209 and @xmath210 . for these cases we have to treat @xmath211 and @xmath212 separately in order to rearrange equation ( [ eq : postconnorate1 ] ) to a quadratic inequality in @xmath1 .",
    "the details are tedious but straightforward algebra .",
    "we would like to point out that the remarks [ rem : noratecorollary - gaussian ] and [ rem : noratecorollary - logconcave ] are also valid for this more general corollary [ cor : postconsmallnorate ] .",
    "in the special case of jointly diagonalisable prior and noise covariance , we evaluate the consistency rate in theorem [ thm : postconsmallgeneral ] by comparing it with the optimal rates obtained in @xcite . by numerically solving the optimisation problem in theorem [ thm : postconsmallgeneral ] , we indicate that our rates are close to the optimal rate .    in the following we first derive a gaussian prior and noise for a regression problem before reformulating our result in this context . in a second step",
    "we reformulate the problem in the notation of @xcite and state the corresponding result .",
    "we conclude this section by an actual comparison between the posterior consistency rate obtained in @xcite and the results of this paper .",
    "we suppose that the prior is gaussian @xmath213 and that the covariance operators @xmath214 of the prior and @xmath118 of the noise are jointly diagonalisable over @xmath215 denoting an orthonormal basis of eigenvectors .",
    "furthermore , we assume that the eigenvalues @xmath216 and @xmath217 of @xmath214 and @xmath118 satisfy @xmath218 respectively .",
    "the inner product of the hilbert scale with respect to @xmath118 can now explicitly be written as @xmath219 moreover , we remark that assumption [ ass : noisetracesumm ] is satisfied with @xmath220 .",
    "the covariance operator @xmath221 of @xmath30 on @xmath171 has eigenvalues @xmath222 which can be seen by denoting @xmath223 and calculating @xmath224   & = \\left\\langle \\priorcov s^{2sr}u , s^{2sr}u\\right\\rangle _ { \\mathcal{h}}=\\left\\langle s^{2sr}\\priorcov u , v\\right\\rangle _ { \\mathcal{h}^{s}}.\\nonumber \\end{aligned}\\ ] ] in order to conclude that @xmath221 is trace - class on @xmath225 , we need to impose that @xmath226 . in this case , we know from example 2 and proposition 3 in section 18 of @xcite that the small balls asymptotic @xmath190 is satisfied for @xmath30 with @xmath227 .",
    "for this problem we adapt theorem [ thm : postconsmallgeneral ] by optimising over @xmath172 in the appropriate range as described in remark [ rem : postcongenerals ] .",
    "moreover , fernique s theorem @xcite for gaussian measures motivates us setting @xmath128 and @xmath227 as discussed above .",
    "[ cor : gaussiancase ] let the prior and the observational noise be specified as in equation ( [ eq : postconsmallgaussprior ] ) and ( [ eq : postconsmallgaussnoise ] ) . if the following optimisation problem has a solution @xmath194 , then for any @xmath195 the posterior @xmath40 is consistent in @xmath152 for @xmath37 in @xmath171 with rate @xmath196 .",
    "@xmath228 @xmath229 where @xmath230 .",
    "we now recast our problem reformulating it in the setting and notation of @xcite .",
    "letting @xmath231 be @xmath132-valued white noise , our problem corresponds to recovering @xmath14 from @xmath232 this problem is equivalent to    @xmath233    where @xmath234",
    "let @xmath235 be an orthonormal basis of eigenvectors of @xmath19 on @xmath132 .",
    "in order to adapt the notation of @xcite , we write @xmath236 and note that @xmath237 will be equivalent to the cameron - martin space which takes the form @xmath238 with orthonormal basis @xmath239 .",
    "moreover , let @xmath240 be defined as    @xmath241 in order to match assumption 3.1 in @xcite , we have to bound the eigenvalues @xmath242 of @xmath243 as follows @xmath244 we determine these eigenvalues by noting that @xmath245 the calculation above yields @xmath246 and thus @xmath247 as in equation ( [ eq : determinecov ] ) , we identify the covariance operator of @xmath30 on @xmath237 through its eigenvalues @xmath248 by theorem 4.1 in @xcite the posterior contraction rate is given by @xmath249 where @xmath250 ( compare equation ( 3.5 ) in @xcite ) and @xmath251 is the regularity of the truth .",
    "as above , we suppose that @xmath252 resulting in @xmath253 in figure [ fig : comparison ] , we use numerical optimisation to compare our rate to the optimal one for @xmath254 with varying @xmath255 .    just considering inequality ( [ eq : postconsmallnoisesmallball ] ) ( essential to our approach since this implies that the cameron - martin term dominates the prior measure c.f .",
    "equation ( [ eq : postconhilbertscaledeltaball ] ) ) yields    @xmath256 which coincides with the rate @xmath257 obtained by solving the optimisation problem in corollary [ cor : gaussiancase ] .",
    "thus , even if we are able to improve our bounds , there is a genuine gap between our rate and the optimal rate in the case of gaussian priors .",
    "the reason for this gap is that theorem [ thm : postconsmallgeneral ] is applicable to any prior satisfying the stated regularity and small ball assumptions .",
    "nevertheless , figure [ fig : comparison ] indicates that the obtained rates are quite close .",
    "in contrast , @xcite is only applicable to gaussian priors for which the gaussian stucture of the prior and the posterior are explicitly used .",
    "posterior consistency rate for the bayesian regression model with the noise and prior given in equations ( [ eq : postconsmallgaussprior ] ) and ( [ eq : postconsmallgaussnoise ] ) .",
    "we denote the rate obtained in @xcite and the one based on corollary [ cor : gaussiancase ] as @xmath258 and @xmath259 , respectively .",
    "we also plot @xmath260 an upper bound on the rate that is obtainable with our method which is based on the small ball asymptotics of the prior.,scaledwidth=79.0% ]      we consider the following non - parametric bayesian regression problem    @xmath261    with @xmath262 , @xmath3 a bounded domain and @xmath263 .",
    "we assume that a prior @xmath30 is supported on @xmath264 resulting in a posterior of the form @xmath265 subsequently , we will prove posterior consistency for this problem for the case @xmath266 $ ]",
    ". however , the same reasoning applies to any bounded domain @xmath267 but the actual posterior consistency rate depends on @xmath43 .    as in the previous section ,",
    "we suppose that the data @xmath268 in equation ( [ eq : largedatamodel ] ) is generated for a fixed truth @xmath37 .",
    "hence @xmath269 in this setup posterior consistency depends on the properties of the prior as well as on the sequence @xmath270 . in the following ,",
    "we discuss appropriate assumptions on both giving rise to theorem [ thm : largedata ] .",
    "moreover , we relate this result with its assumptions to the literature .",
    "[ ass : postcondatapriorass]there exist @xmath271 $ ] and @xmath272 such that @xmath273    as @xmath29 increases , we gain more and more information about the function @xmath14 . in particular , if @xmath270 is dense in @xmath274 $ ] it is even possible to reconstruct the value of @xmath275 from @xmath276 .",
    "more precisely , let @xmath277 $ ] be arbitrary , then there are @xmath278 such that @xmath279    however , we will see that this is not sufficient for posterior consistency .",
    "in fact , we will give an example of posterior inconsistency for this case .",
    "so far , the problem of posterior consistency for this type of regression problems has mainly been investigated for random evaluation points @xmath280 which are known as random covariates .",
    "appropriate results of this type can be found in * @xcite*. an exception is @xcite where posterior consistency without a rate with respect to the @xmath281-norm for deterministic @xmath280 is shown .",
    "this result is obtained under the following assumption .",
    "@xcite[ass : choi ] suppose that there exists a constant @xmath282 such that whenever @xmath283 for @xmath284 there is at least one @xmath285 such that @xmath286 .",
    "the above condition guarantees that the number of observations in each interval satisfies a lower bound proportional to its size .",
    "more precisely , an interval of size @xmath196 has at least order @xmath287 for @xmath29 being large enough .",
    "this can be seen by chopping the interval into intervals of size @xmath288 .",
    "in contrast to @xcite , we are also able to obtain a posterior consistency rate under this assumption .",
    "posterior consistency without a rate can be concluded under the following weaker assumption .",
    "[ ass : equally spaced ] we suppose that for @xmath270 there exists a @xmath289 such that for any @xmath290 $ ] there is an @xmath291 such that @xmath292 where @xmath293 denotes the empirical distribution of @xmath294    [ thm : largedata]suppose that the assumptions [ ass : postcondatapriorass ] and [ ass : equally spaced ] are satisfied .",
    "then @xmath295 is posterior consistent with respect to the @xmath89-norm for any @xmath296 .",
    "moreover , if assumption [ ass : choi ] and the small ball asymptotic @xmath297 are satisfied , then @xmath295 is posterior consistent with respect to the @xmath89-norm with any rate @xmath196 and @xmath298    as in theorem [ thm : notail ] , posterior consistency is implied by @xmath299 for increasing sets @xmath157 such that @xmath300@xmath301 . for notational convenience",
    "we write @xmath302 @xmath303 and we denote by @xmath201 a generic @xmath304 random variable .",
    "this allows us to rewrite the posterior in equation ( [ eq : postconlargedata ] ) as @xmath305 since @xmath306 is finite dimensional , it is easy to see that @xmath307 is bounded from above and below . again , fixing @xmath163 , we only need to consider @xmath308 . thus ,",
    "for @xmath309 we have a lower bound on @xmath310 in order to derive an upper bound on @xmath311 , let @xmath312 be chosen arbitrarily and notice that @xmath313 is decreasing for @xmath314 .",
    "the upper bound on @xmath311 therefore boils down to a lower bound on @xmath315 that is larger than @xmath316 in fact , there is @xmath317 such that @xmath318 . applying hlder continuity yields @xmath319\\ ] ] for @xmath320 .",
    "let @xmath321 be the following index set @xmath322\\right\\ } .\\ ] ] for @xmath29 larger than @xmath323 it follows that @xmath324 if we only consider @xmath280 with @xmath325 , we obtain that @xmath326 which gives rise to the following upper bound    @xmath327.\\label{eq : thmlargedatapostconupper}\\ ] ]    by choosing @xmath328 small enough , we also know that @xmath329 in order to obtain a rate of posterior consistency , we use @xmath330 and hence @xmath331.\\label{eq : postconlargelower}\\end{aligned}\\ ] ] thus , equation ( [ eq : thmlargedatapostconupper ] ) implies that @xmath332 the first term in the exponential in equation ( [ eq : postconlargelower ] ) is dominant over the corresponding term in equation ( [ eq : postconlargeupper ] ) by choosing @xmath333 moreover , the first * * term in the equations ( [ eq : postconlargelower ] ) and ( [ eq : postconlargeupper ] ) is dominant over the other terms respectively if @xmath334 these three inequalities are respectively implied by @xmath335 choosing @xmath336 small enough , we see that @xmath40 is consistent in @xmath89 with any rate @xmath337    the assumptions in the theorem above can be justified because a slight violation leads to the example of posterior inconsistency in the next section .      in this section ,",
    "we construct a counterexample to illustrate that despite the strong assumption [ ass : postcondatapriorass ] it is not sufficient for @xmath338 to be dense in order to establish posterior consistency .",
    "given such a sequence it is always possible to extract a subsequence satisfying assumption [ ass : equally spaced ] .",
    "even though all the other observations can be viewed as additional , we will choose a prior so that the posterior sequence is not consistent .    in the following ,",
    "we choose the prior concentrated on functions @xmath339 which are continuous , satisfy @xmath340 and are linear on @xmath341 $ ] and * * @xmath342 $ ] . by identifying @xmath343 and @xmath344 with the first and second component respectively the following two - dimensional example",
    "can be extended to the setting of equation ( [ eq : largedatamodel ] ) .",
    "this extension is an example of posterior inconsistency with respect to the @xmath345-norm for @xmath346 because any of these norms is equivalent to @xmath347 for an arbitrary norm on @xmath348 .",
    "we consider the following prior on @xmath348 @xmath349 and we choose @xmath350 as truth. the data consists of @xmath29 and @xmath351 with @xmath352 being measurements of the form @xmath353 respectively .",
    "consequently , the posterior takes the form @xmath354 here , posterior consistency of @xmath355 is equivalent to the statement that for any @xmath282 there is @xmath356 such that @xmath357 we will not only show that @xmath355 is posterior inconsistent but also that there is @xmath358 such that for @xmath359 @xmath360 because of @xmath361 , we may proceed as in the proofs of the theorems [ thm : notail ] and [ thm : largedata ] and thus it is enough to construct sets of increasing @xmath362-probability such that on these sets @xmath363   & = & \\frac{\\sum_{k}\\exp\\left(-\\frac{1}{2}\\sum_{j=1}^{n}\\frac{1}{k}-\\frac{2}{k}\\xi_{j}-2k^{2}\\right)}{\\sum_{k}\\exp\\left(-\\frac{1}{2}\\sum_{j=1}^{n}\\frac{1}{4k}-\\frac{1}{\\sqrt{k}}\\xi_{j}-\\frac{1}{2}\\sum_{j=1}^{n^{\\theta}}1 - 2\\tilde{\\xi}_{k}-k^{2}\\right)}\\rightarrow0.\\end{aligned}\\ ] ] the @xmath362-probabilities of @xmath364 and @xmath365 are exponentially small in @xmath29 .",
    "thus , it is enough to consider @xmath366   & \\leq\\max\\left\\ { \\exp\\left(-\\frac{3}{4}\\sqrt{n}\\right),\\exp(-n)\\right\\ } \\rightarrow0\\text { as } n\\rightarrow\\infty.\\end{aligned}\\ ] ] hence we have shown that @xmath40 is not posterior consistent .",
    "this example relies on the prior having strong correlations between its two components .",
    "therefore it seems an interesting question how the assumptions on @xmath30 can be strengthened in order to relax those on @xmath367      we conclude this section by showing that interpolation inequalities can be used in order to strengthen the norm in which the posterior concentrates .",
    "in particular we consider the small noise limit as described in section [ sub : the - small - noise ] .",
    "suppose we know that the posterior concentrates around the truth @xmath37 in the cameron - martin norm @xmath368 . in order to show consistency in @xmath369 ,",
    "we write @xmath370 the posterior probability of the first set is small due to the posterior consistency in @xmath152 .",
    "the posterior probability of the second set is small due to the tails of the prior and the posterior .",
    "obtaining estimates of this type can be done similarly to the steps subsequent to equation ( [ eq : postconhilbertscalesposteriormoment ] ) in the proof of theorem [ thm : postconsmallgeneral ] . using this technique ,",
    "it is also possible to apply the results of this section to the eip  in the next section .",
    "a similar technique based on interpolation inequalities between hlder spaces applies to the large data limit and is also used for the eip .",
    "in section [ sub : introelliptic ] , we introduced the idea of reducing posterior consistency of the eip   to that of the brp . for this example",
    "we demonstrate our method for both the small noise and the large data limit .",
    "we start by giving the proof for the small noise limit in detail before sketching the same steps for the large data limit . *",
    "* we emphasise the case of posterior consistency in the small noise limit because of its analogy with convergence results for regularisation methods .      using theorem [ thm : notail ] from section [ sec : general - results ] to conclude posterior consistency of the eip   ( c.f .",
    "section [ sub : introelliptic ] ) is not entirely straightforward because we have to lift the posterior consistency for the brp   to @xmath90 .",
    "moreover , we have to find appropriate assumptions on the prior @xmath30 so that the push forward prior @xmath92 satisfies the assumptions of the theorem [ thm : notail ] . again",
    ", a rate of posterior consistency is obtained if the prior satisfies appropriate small ball asymptotics . in a second step we verify those for the so - called uniform priors which are based on a series expansion with uniformly distributed coefficients , for details see below or consider @xcite .    in order to formulate assumptions on @xmath30 implying that @xmath92 satisfies the assumptions of theorem [ thm : notail ] , we assume for simplicity that @xmath371 where @xmath372 denotes the laplacian with homogeneous dirichlet conditions . in this case the abstract hilbert scale @xmath171 ( c.f . [",
    "sec : notation ] ) corresponds to the standard sobolev space @xmath373 .",
    "thus , the almost sure bounds in theorem [ thm : notail ] are implied by the appropriate assumptions on the prior and classical results from @xcite .",
    "moreover , the choice @xmath371 also implies that assumption [ ass : noisetracesumm ] holds for @xmath374 .",
    "this is due to the fact that the operator @xmath375 has eigenvalues @xmath376 with @xmath377 ( see section [ sec : notation ] for notation ) where @xmath43 denotes the dimension of the domain @xmath3 .",
    "these results are called weyl asymptotics and further details can be found in @xcite and @xcite .",
    "the following theorem summarises the consequences for the posterior consistency of the eip .",
    "[ thm : ellipticnorate]suppose that the noise is given by @xmath378 and that the prior @xmath30 satisfies @xmath379 if @xmath96 , @xmath380 and @xmath381 , then the eip   is posterior consistent with respect to the @xmath99-norm for any @xmath100 .",
    "additionally , if @xmath382 then the eip   is posterior consistent with respect to the @xmath89-norm with rate @xmath196 for any @xmath383 such that @xmath384    before proving theorem [ thm : ellipticnorate ] , we notice that forward stability results ( as proposition [ prop : forwardstability ] ) can be used to transfer small ball asymptotics from @xmath30 to @xmath385 .",
    "[ lem : pushforwardsmallball]if the prior satisfies the small ball asymptotic @xmath386 then @xmath387    proposition [ prop : forwardstability ] implies that @xmath388 \\text { and } p\\left(b_{\\epsilon}^{c^{\\beta}}\\left(\\truth\\right)\\right ) & \\subseteq & b_{c\\epsilon}^{c^{\\beta+1}}\\left(\\pressure^{\\dagger}\\right).\\end{aligned}\\ ] ] hence the statement follows .",
    "having established lemma [ lem : pushforwardsmallball ] , we are now in the position to prove the main theorem of this section .",
    "subsequently , @xmath47 will denote a generic constant in different contexts that may change form line to line .",
    "we will first prove posterior consistency in @xmath89 before we use an interpolation inequality to bootstrap it to @xmath99 . in order to prove posterior consistency in the @xmath89-norm",
    ", it is enough to show posterior consistency of the brp   in the @xmath90-norm because @xmath389 which follows by an application of proposition [ prop : richter ] and a change of variables ( see theorem [ thm : indpendentofdiscription ] ) . using theorem 6.19 from @xcite",
    ", we may conclude that @xmath390 since @xmath391 , @xmath1 is @xmath392-a.s .",
    "an element of the cameron - martin space of @xmath23 as it corresponds to @xmath124 .",
    "posterior consistency of the brp   with respect to the @xmath124-norm is now implied by theorem [ thm : notail ] .",
    "its conditions are satisfied because @xmath393 with @xmath394    furthermore , proposition [ prop : forwardstability ] and the fact that @xmath381 imply that @xmath395 . in order to bootstrap to posterior consistency in the @xmath90-norm",
    ", we use a generalisation of the sobolev embedding theorem for besov spaces and an interpolation inequality between besov spaces on domains ( for details consult @xcite ) .",
    "we first note that @xmath396 and @xmath397 for @xmath398 .",
    "in particular theorem 4.33 in @xcite implies that @xmath399 for @xmath163 being small .",
    "if @xmath400 , we can conclude posterior consistency in the @xmath90-norm because @xmath401 holds for @xmath336 small enough .",
    "otherwise , we use the interpolation inequality between besov spaces subject of theorem 4.17 in @xcite @xmath402 for @xmath336 small enough and with @xmath403 .",
    "similar to equation ( [ eq : ellipticsmallnoiseenough ] ) , it follows that @xmath404 the equations ( [ eq : ellipticsmallnoiseenough ] ) and ( [ eq : ellipticsmallnoiseinterp ] ) allow us to bootstrap the posterior consistency of @xmath405 to @xmath406 equation ( [ eq : thmnottailnoratestability ] ) implies posterior consistency of @xmath407 in the @xmath89-norm .",
    "similarly , we bootstrap to posterior consistency in @xmath99 for @xmath100 using the same interpolation technique as above .",
    "in order to obtain a rate for posterior consistency , we first note that @xmath408 implies @xmath409 due to lemma [ lem : pushforwardsmallball ] .",
    "now theorem [ thm : notail ] implies posterior consistency of the sequence of posteriors @xmath405 in @xmath124 with any rate @xmath383 such that @xmath410 using the interpolation inequality as above gives rise to posterior consistency for * * @xmath405 in @xmath411 with rate @xmath196 for any @xmath383 such that @xmath384 as above , this implies the same rate of posterior consistency for @xmath40 in @xmath412      in this section , we establish a rate of posterior consistency for the eip   with the so - called uniform prior introduced in @xcite .",
    "this choice of the prior was motivated by the preceding analysis in the uncertainty quantification literature , see for instance @xcite .",
    "it is given by    @xmath413\\label{eq : uniformprior}\\ ] ]    where @xmath414 denotes the law of a random variable .",
    "moreover , we suppose that @xmath415 , @xmath416 and @xmath417 such that @xmath418 in order to obtain a rate for the eip   with this prior , we derive a small ball asymptotic under an appropriate assumption on the decay of @xmath419 .    [",
    "ass : coefficientdecay]there exists @xmath420 such that for all @xmath421 @xmath422    since the series in equation ( [ eq : uniformprior ] ) is absolutely convergent , we assume without loss of generality that @xmath423 is decreasing .",
    "this allows us to use the following classical inequality from approximation theory @xcite @xmath424    [ thm : smallballprior]suppose that @xmath30 is given as in equation ( [ eq : uniformprior ] ) , assumption [ ass : coefficientdecay ] is satisfied with @xmath425 and @xmath426.\\ ] ] then for any @xmath421 @xmath427    we obtain an asymptotic lower bound on the small ball probability by choosing an appropriate subset @xmath428 of @xmath429 .",
    "we denote a generic element of this set by @xmath430 choosing @xmath431 such that @xmath432 , the corresponding terms contribute at most @xmath433 to the difference @xmath434 .",
    "the subset @xmath428 prescribes intervals for @xmath435 @xmath436 such that this contribution is at most @xmath433 , too .",
    "more precisely , let @xmath437 , then equation ( [ eq : summabilityexp ] ) implies @xmath438 for @xmath439 .",
    "let the subset @xmath440 be given by @xmath441 then @xmath442    combining lemma [ thm : smallballprior ] and theorem [ thm : ellipticnorate ] results in the following theorem which characertises posterior consistency for this class of priors .",
    "let the prior @xmath30 be defined as in equation ( [ eq : uniformprior ] ) and let assumption [ ass : coefficientdecay ] be satisfied .",
    "additionally , we assume that @xmath443 , @xmath96 and @xmath444 then the posterior @xmath40 is consistent for any @xmath445\\ ] ] with respect to with rate @xmath446 for any @xmath383 such that @xmath447      in the following we show that the results for the brp   can be transferred to posterior consistency results in the large data limit for the eip .",
    "we consider only the case @xmath448 with @xmath266 $ ] as the general case is similar .",
    "furthermore , assuming that the observations are of the form @xmath449 the sequence of posteriors is given by @xmath450    posterior consistency of the eip   in @xmath89 can then be derived on the basis of theorem [ thm : largedata ] .",
    "suppose that the sequence @xmath451 satisfies assumption [ ass : equally spaced ] , @xmath452 @xmath30-a.s . with @xmath453 and @xmath454",
    "@xmath30-a.s .. if @xmath455 , then the eip   is posterior consistent in the large data limit with respect to @xmath456 for any @xmath457    an application of theorem 6.13 in @xcite yields the existence of @xmath458 so that for all @xmath14 satisfying @xmath459 there is a unique solution @xmath1 such that @xmath460 .",
    "thus , @xmath461 satisfies the assumptions of theorem [ thm : largedata ] implying that @xmath462 is posterior consistent in @xmath412 using the interpolation inequality between @xmath89 and @xmath463 , we also obtain consistency in @xmath90 . as in theorem",
    "[ thm : ellipticnorate ] , proposition [ prop : richter ] can be used in order to conclude posterior consistency of @xmath464 in @xmath89 .",
    "we can bootstrap from @xmath465 to @xmath456 by interpolating between @xmath89 and @xmath466 .",
    "in this article , we have established a novel link between stability results for an inverse problem and posterior consistency for the bayesian approach to it .",
    "we have explicitly shown this link for an elliptic inverse problem ( c.f .",
    "eip ) but the same method is also applicable for the general case .",
    "an instance is electrical impedance tomography ( caldern problem ) for which stability results are available @xcite .",
    "this example would lead to a very slow posterior consistency rate since its stability results are weak .",
    "essentially , we would have to redo all the calculations on a log - scale instead of an algebraic scale .",
    "so far , we need exponential moments of the prior for the bayesian regression of functional response and for pointwise observations ( see also section 4.2.2 in @xcite ) . for this reason it is harder to prove posterior consistency for example for log - gaussian priors .",
    "log - gaussian measures have moments of arbitrary order but no exponential moments .",
    "this is a problem that we would like to pursue further in the future .",
    "we use the following notation for asymptotic inequalities :    let @xmath467 and @xmath468 be sequences in @xmath469 .",
    "we denote by @xmath469 @xmath470 that there are @xmath471 and @xmath472 such that @xmath473 moreover , if @xmath474 , we write @xmath475      in order to measure the smoothness of the noise and samples of the prior , we introduce hilbert scales following @xcite .",
    "let @xmath19 be a self - adjoint , positive - definite , trace - class linear operator with eigensystem @xmath476 .",
    "we know that @xmath477 is a densely defined , unbounded , symmetric and positive - definite operator because @xmath478 we define the _ hilbert scale _ by @xmath479 with @xmath480 for @xmath481 we will denote balls with respect to the @xmath482-norm by @xmath483    moreover , these collection of norms satisfies an interpolation inequality    [ lem : hscaleinterpolation](proposition 8.19 in @xcite ) let @xmath484 then the following interpolation inequality holds @xmath485    our definition here is slightly different from the literature in order to match it to the sobolev spaces for @xmath486      in this section , we set out our notation for some standard results about infinite dimensional gaussian measures which can be found in the following textbooks and lecture notes @xcite .",
    "let @xmath336 be a gaussian measure on a hilbert space @xmath487 .",
    "it is characterised by its _",
    "mean _ given by the bochner integral @xmath488 and the _ covariance operator _",
    "@xmath489 characterised by the relation @xmath490 from this it is clear that the covariance operator is positive - definite and self - adjoint .",
    "moreover , we note that @xmath19 is necessarily trace - class and the gaussian can be expressed through eigenvalues @xmath376 and the corresponding eigenbasis @xmath491 @xmath492 the _ cameron - martin space _ associated with @xmath336 is @xmath493 equipped with the inner product @xmath494 where @xmath495 and @xmath496.this space characterises the support as well as the direction such that @xmath497 where @xmath498 is the translation operator @xmath499 .",
    "we also consider the hilbert scale @xmath500 generated by @xmath336 and the regularity of a draw @xmath501 can be expressed as follows .",
    "[ lem : regularitynoise](@xcite ) imposing assumption [ ass : noisetracesumm ] the following statements hold :    1 .",
    "let @xmath231 be a white noise , then @xmath502 for all @xmath137 .",
    "2 .   let @xmath503 , then @xmath504 @xmath505-a.s .",
    "for every @xmath137 .",
    "the state of a model can be described in several ways . in this section , we present the resulting relationship between two different descriptions of the same model .",
    "[ thm : indpendentofdiscription]suppose @xmath101 with @xmath102 and @xmath506 .",
    "furthermore , assume that the posterior @xmath40(@xmath112 ) is well - defined for the forward operator @xmath507(@xmath508 ) , the prior @xmath509 ( @xmath510 ) and the noise @xmath511 .",
    "it is given by @xmath512 in this case @xmath513 .",
    "it is sufficient to show that both measures agree on all sets @xmath514    @xmath515 by the transformation rule @xmath516",
    "we follow the same steps as in the proof of theorem [ thm : notail ] up to equation ( [ eq : postconnotailinterp ] ) reading    @xmath517    with @xmath174 .",
    "we now separate the product using young s inequality with @xmath518    @xmath519    where , for simplicity of notation , we used @xmath520 .",
    "+      the following lower bound on @xmath521 is based on equation ( [ eq : postconsmallnoisecminner ] ) @xmath522\\right].\\label{eq : postconhilbertscaledeltaball}\\end{aligned}\\ ] ] the term @xmath184 has to be dominant in equation ( [ eq : postconhilbertscaledeltaball ] ) because the same exponent is appearing in equation ( [ eq : postconhilbertscaledeltaballcomplement ] ) except for a larger coefficient .",
    "choosing @xmath523 and substituting the expression for @xmath524 , this is the case if @xmath525 we need small ball probabilities and the exponential moments of @xmath30 in order to obtain explicit sufficient conditions on @xmath383 .",
    "we first note that    @xmath526    equation ( [ eq : postconsmalltmp1 ] ) holds if @xmath527      we bound @xmath529 by @xmath530      we denote by @xmath532 the following supremum    @xmath533 which is finite if @xmath534 the first two summands above can be rewritten as a function @xmath535 of @xmath536 where @xmath537 by considering @xmath538 , we see that @xmath535 is decreasing for @xmath539 .",
    "thus , for    @xmath540    the following inequality holds @xmath541.\\label{eq : postconhilbertscaledeltaballcomplement } \\end{aligned}\\ ] ] then for large @xmath29 , equation ( [ eq : postconhilbertscalenecdecreasing ] ) is implied by @xmath542      in this section , we bound @xmath544 using markov s inequality in combination with the exponential moments of the prior @xmath545 we denote the term appearing in the exponential in the second line by @xmath546 .",
    "it can be bounded similar to the upper bound on * @xmath531 * @xmath547 we denote by @xmath548 an inequality with a multiplicative constant not involving @xmath29 or @xmath383 . in order to get an upper bound for equation ( [ eq : postconhilbertscalesposteriormoment ] ) , we bound the exponential moment by @xmath549 introducing @xmath550 and performing an integration by parts , it follows that @xmath551 + 1d\\prior(\\input)\\\\   & \\lesssim\\int_{0}^{\\infty}g^{\\prime}(r)\\exp\\left(g(r)+\\mathfrak{u}_{t_{0}}\\right)d\\prior\\left(\\left\\vert \\input\\right\\vert _ { s}>r\\right)dr\\\\   & \\lesssim\\int_{0}^{\\infty}g^{\\prime}(r)\\exp\\left(n^{\\frac{1}{2}-\\hilbertscalesyoungs}r^{(1-\\hilbertscalesinter)q}-2fr^{e}\\right)dr.\\end{aligned}\\ ] ] the above can only be expected to be finite if @xmath552 moreover , we assume that @xmath553 since otherwise @xmath554 in order to achieve an upper bound , we split the term in the exponential into @xmath555 and @xmath556 .",
    "the first term is negative whenever @xmath557 for @xmath29 large enough @xmath558 holds . on the interval",
    "@xmath559 $ ] an upper bound @xmath560 on the maximum value of @xmath561 can be derived as follows @xmath562 putting everything together gives rise to @xmath563 for some @xmath0 . using markov s inequality , this yields",
    "@xmath564 again substituting @xmath523 , this is asymptotically smaller than @xmath565 if    @xmath566    collecting the inequalities from above , we see that the results follow by letting @xmath188 .",
    "in order to bound @xmath567 in equation ( [ eq : identityposterior ] ) , we rewrite it as @xmath568 where @xmath569 .",
    "we bound @xmath570 using the cauchy - schwarz inequality @xmath571 the following steps are quite similar to the steps in the proof of the theorems [ thm : notail ] and [ thm : postconsmallgeneral ] .",
    "we treat @xmath572 by smoothing @xmath16 at the expense of @xmath0 @xmath573    we use the interpolation inequality for hilbert scales with @xmath174 ( see lemma [ lem : hscaleinterpolation ] ) and hlder s inequality with @xmath518 to obtain @xmath574 combining these bounds yields @xmath575 the first three terms are bounded in @xmath0 because they are dominated by the first if @xmath576 .",
    "this is implied by choosing @xmath577 note that @xmath578 is @xmath579-a.s .",
    "bounded due to lemma [ lem : regularitynoise ] .",
    "thus @xmath580 is bounded below if @xmath581 .",
    "letting @xmath582 in @xmath583 we see that this is the case for @xmath584 an upper bound on @xmath580 follows from a simple lower bound on @xmath570 on @xmath585 and the prior measure of this set ."
  ],
  "abstract_text": [
    "<S> in the bayesian approach , the a priori knowledge about the input of a mathematical model is described via a probability measure . </S>",
    "<S> the joint distribution of the unknown input and the data is then conditioned , using bayes formula , giving rise to the posterior distribution on the unknown input . in this </S>",
    "<S> setting we prove posterior consistency for nonlinear inverse problems : a sequence of data is considered , with diminishing fluctuations around a single truth and it is then of interest to show that the resulting sequence of posterior measures arising from this sequence of data concentrates around the truth used to generate the data . </S>",
    "<S> posterior consistency justifies the use of the bayesian approach very much in the same way as error bounds and convergence results for regularisation techniques do . as a guiding example </S>",
    "<S> , we consider the inverse problem of reconstructing the diffusion coefficient from noisy observations of the solution to an elliptic pde in divergence form . </S>",
    "<S> this problem is approached by splitting the forward operator into the underlying continuum model and a simpler observation operator based on the output of the model .    in general , </S>",
    "<S> these splittings allow us to conclude posterior consistency provided a deterministic stability result for the underlying inverse problem and a posterior consistency result for the bayesian regression problem with the push - forward prior .    </S>",
    "<S> moreover , we prove posterior consistency for the bayesian regression problem based on the regularity , the tail behaviour and the small ball probabilities of the prior . </S>"
  ]
}