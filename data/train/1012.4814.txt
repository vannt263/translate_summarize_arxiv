{
  "article_text": [
    "we work directly with a classical - quantum channel @xmath7 taking input classical symbols @xmath8 to output quantum states @xmath9 . to recover the case of a classical channel",
    ", one can simply require that the output states @xmath10 be simultaneously diagonalizable .",
    "we note that the restriction to classical channel inputs can be made without loss of generality as we are interested in transmitting classical information , either publicly or privately . in terms of a physical channel accepting quantum inputs ,",
    "this just amounts to fixing the quantum states to be input for given classical value @xmath11 ; here this choice is effectively part of the channel ( this is possible since , in the one - shot treatment adopted here , the channel is only used once ) . on the other hand",
    ", one may regard this choice as part of the encoder , and the only necessary modification of the expression for the capacity ( see theorem  [ thm : capacity ] below ) would be to include an optimization over this choice .    an @xmath12-coding scheme for a classical - quantum channel",
    "consists of an encoder @xmath13 taking classical messages @xmath14 to channel inputs and a decoder @xmath15 taking channel outputs to guesses of the input messages , for which @xmath16 and @xmath17\\leq { \\varepsilon}\\end{aligned}\\ ] ] for all @xmath14 .",
    "in addition , if @xmath7 outputs a bipartite state @xmath18 , of which bob receives the @xmath1 subsystem and an eavesdropper eve the @xmath19 subsystem living in @xmath20 , then an @xmath12-private coding scheme is an encoder - decoder pair as above , with the additional requirement that every message @xmath21 be approximately unknown to the eavesdropper : @xmath22 where @xmath23 need not be the average state , but can be arbitrary . ] .",
    "it is useful to think of the message as being a random variable @xmath24 , taking values in @xmath25 according to the probability distribution @xmath26 .",
    "then the message transmission process is encapsulated by the following sequence of random variables ( markov chain ) .",
    "we use a prime to denote a random variable or quantum system which is meant to be nearly identical to the unprimed version ; in the present context we would like the output @xmath27 to be essentially equal to the input @xmath24 .",
    "@xmath28    given a choice of @xmath29 , the capacity for public communication ( usually just referred to as the classical capacity ) @xmath30 of the channel is simply @xmath31 for the largest @xmath32 in an @xmath12 coding scheme . the private capacity @xmath33 is defined similarly using private coding schemes . here",
    "we prove the following upper and lower bounds on these capacities in terms of the smooth min- and max - entropies , which are defined in the appendix .",
    "[ thm : capacity ] @xmath34 + for all @xmath35 , @xmath36,\\\\   c_{{\\rm pub}}^{{\\varepsilon}}(\\theta)\\leq \\max_{p_x}\\big[&{{h}_{\\min}^{}}(x)-{{h}_{\\max}^{\\sqrt{2{\\varepsilon}}}}(x|y)\\big].\\end{aligned}\\ ] ] for @xmath37 a markov chain , @xmath38,\\\\ c_{{\\rm prv}}^{{\\varepsilon}}(\\theta)\\leq \\max_{p_t , t\\rightarrow x}\\big[&{{h}_{\\min}^{\\sqrt{2{\\varepsilon}}}}(t|z)-{{h}_{\\max}^{\\sqrt{2{\\varepsilon}}}}(t|y)\\big].\\end{aligned}\\ ] ]",
    "the proof of the direct parts proceeds in three steps , successively building up to a construction of an encoder and decoder . the first step is to show that in doing this , we only need to worry about the average transmission error and secrecy of the communication scheme , assuming the inputs are uniformly distributed .",
    "then , we show that protocols for information reconciliation can be adapted to the channel coding scenario , when the input to the channel is uniformly distributed ( not just the messages themselves ) . finally , we show how to mimic any particular channel input distribution from a uniform distribution by using randomness extraction , and how to mimic an input distribution so that the eavesdropper learns nothing about the message using privacy amplification .",
    "schematic of using randomness extraction and information reconciliation ( data compression with side information ) to perform noisy channel communication .",
    "messages @xmath39 are input to the encoder @xmath40 and subsequently to the shaper @xmath41 , which is a randomness extractor run in reverse .",
    "then they are then transmitted over the channel @xmath42 to the receiver , who uses the decoder @xmath43 to construct a guess @xmath44 of the original input . concatenating the shaper and channel",
    "gives a new effective channel @xmath45 , for which an encoder / decoder pair @xmath46 can be constructed by repurposing a compressor / decompressor pair that operates on the joint input - output @xmath47 of the channel .",
    "ultimately , the shaper can instead be regarded as part of the encoder @xmath48 , which is formed by concatenating @xmath40 and @xmath41 . ]",
    "we start by observing that constructing an encoder / decoder pair with low _ average _ error probability on the receiver s end and low average trace distance of eavesdropper outputs suffices to construct an encoder / decoder pair with low error probability and secrecy parameter in the worst case .",
    "[ lem : avetoworst ] given a channel @xmath49 and an encoder / decoder pair @xmath50 , @xmath51 such that @xmath52 and @xmath53 , then there exists an encoder / decoder pair for a subset @xmath54 of size at least @xmath55 such that @xmath56 and @xmath57 for all @xmath58 .    by the markov inequality , a fraction at most one - quarter of @xmath59 have @xmath60 .",
    "similarly , for at most one - quarter does @xmath61 .",
    "thus , there is a subset @xmath62 of half the @xmath63 for which neither statement is true .",
    "restricting the input of @xmath48 to @xmath64 gives the new encoder .",
    "the new decoder is given by altering the old decoder so that outputs @xmath65 are mapped at random to @xmath66 .",
    "[ rem : avetoworst ] if we only require that @xmath67 , then @xmath68 suffices .",
    "now we show that an information reconciliation protocol can be adapted to channel coding , at least when the input to the channel is uniformly or nearly uniformly distributed .",
    "we do this explicitly for the case of linear compression functions and subsequently remark how it can be made more general .",
    "first we need to specify classical - quantum information reconciliation protocols more precisely . given a classical random variable @xmath0 and a quantum system @xmath1 jointly described by the classical - quantum state @xmath69 , an @xmath29-good information reconciliation protocol consists of a compression map @xmath70 taking @xmath0 to another classical random variable @xmath2 and a decompression map @xmath71 taking @xmath2 and states in the system @xmath1 to elements @xmath72 of the input alphabet @xmath73 such that the error probablility @xmath74\\leq { \\varepsilon}$ ] .",
    "if the alphabet @xmath75 forms a linear space , the compression map could be linear , and one speaks of a _",
    "linear compressor_.    [ lem : csicoding ] given a cq channel @xmath76 from uniformly - distributed inputs @xmath77 to arbitrary outputs @xmath1 , suppose the linear compressor and arbitrary decompressor pair @xmath78 form an @xmath29-good information reconciliation protocol for the combined input and output @xmath47 .",
    "then there exists a linear encoder @xmath79 and a decoder @xmath80 for @xmath7 such that the error probability of transmitting a uniformly - distributed message @xmath24 of size @xmath81 is also less than @xmath29 .",
    "start by defining @xmath82 $ ]",
    ". then the information reconciliation error probability can be formulated as @xmath83 where @xmath84 denotes the average of @xmath0 using the distribution @xmath85 and @xmath86 is the uniform distribution . in other words ,",
    "the average error probability is the average over outputs @xmath6 of the average error probability of inputs @xmath87 consistent with a given output . in this expression",
    "we have split the summation over @xmath87 to first a summation over the values of @xmath6 and then for each of these a summation over the @xmath87 for which @xmath88 . in so doing",
    ", we have used the fact that there are @xmath89 preimages for each @xmath6 , which follows from lemma  [ lem : linearpreimage ] ( see appendix ) .",
    "choosing the value of @xmath3 with the lowest error probability @xmath90 enables us to define an encoder and decoder from the compressor and decompressor restricted to this value .",
    "the encoder simply maps @xmath63 to those @xmath91 for which @xmath92 in some fixed order , say lexicographic order . by linearity of the compressor , @xmath93 . the decoder is then defined by taking the output of the decompressor and then applying the inverse of the encoding map to the result @xmath94 or outputting a random @xmath14 when @xmath95 .    the error probability for the encoder / decoder combination for uniformly distributed messages @xmath24 is exactly the same as the error probability for the compressor / decompressor combination , which must be lower than the average by construction .",
    "[ rem : csiepscoding ] if the compressor / decompressor pair has error probability @xmath96 when acting on a nearly uniform input @xmath97 satisfying @xmath98 , then applying the corresponding encoder / decoder to a uniform input gives an error probability of at most @xmath99 by the triangle inequality .",
    "[ rem : generalcomp ] by using lemma  [ lem : arbpreimage ] instead of lemma  [ lem : linearpreimage ] ( see appendix ) , the restriction to linear compression functions can be removed at the cost of reducing the number of messages by a factor @xmath29 and an additional failure probablity @xmath29 .      finally , we need to remove the restriction of uniform inputs to the channel .",
    "this is done by combining the channel with a _ distribution shaper _ , which is a means of mapping a uniform distribution to a chosen distribution .",
    "by running the distribution shaper and then the channel , we obtain a virtual channel which acts again on a ( roughly ) uniformly distributed input .",
    "the distribution shaper can be constructed using a randomness extractor , as follows .",
    "suppose that @xmath100 is a function which produces an @xmath29-good approximation of a uniformly distributed random variable @xmath77 from an input @xmath0 distributed according to @xmath85 , in the sense that @xmath101 .",
    "the extractor defines a joint distribution @xmath102 , and with this we can define a function @xmath103 which is in some sense the inverse of @xmath104 . here",
    "@xmath105 is some additional randomness , and @xmath41 is defined by using @xmath105 to select an @xmath106 from the distribution @xmath107 given the input value @xmath108 .",
    "thus , the output of the shaper is again @xmath0 .",
    "shapers constructed in this manner will be called @xmath29-shapers . moreover ,",
    "if the extractor performs privacy amplification of @xmath0 against some @xmath19 generated from @xmath0 , then the shaper replicates @xmath0 while hiding @xmath97 from the eavesdropper .",
    "this follows because the conditional states relevant to the eavesdropper are the same in both cases .",
    "it may seem strange to additionally require a source of randomness for this purpose , and ideally we would like all the randomness needed to generate @xmath0 to be contained in @xmath97 .",
    "however , the mapping that takes a general @xmath0 to a nearly - uniform distribution @xmath97 may map two values @xmath11 to the same @xmath94 . when that @xmath94 is input to @xmath41 , some randomness is needed to reverse the mapping .",
    "now we can combine these three pieces to establish the direct part of theorem  [ thm : capacity ] .",
    "we do this first for the private capacity and then make some modifications to obtain the lower bound on the classical capacity .",
    "the latter can be obtained as a special case of the former by assuming the channel does not leak anything to an adversary , i.e.  @xmath19 is trivial , but the additional modifications will improve the constants in the bound .    for a given channel @xmath49 and input distribution @xmath85 , we can define a new channel @xmath109 ( with output states @xmath110 ) by concatenating an @xmath111-shaper @xmath41 that generates @xmath0 , built from a privacy amplification extractor , with @xmath7 and regarding @xmath105 as part of the channel .",
    "next , following remark  [ rem : csiepscoding ] we construct an encoder / decoder pair @xmath40/@xmath112 from an @xmath96-good compressor / decompressor for @xmath113 , where @xmath114 is the distribution of the input @xmath97 to the shaper @xmath41 . when input with a uniformly distributed @xmath77 , the error probability averaged over codewords and choices of code is at most @xmath99 , while the average leakage to the eavesdropper is at most @xmath115 . for simplicity , define @xmath116 . by the markov inequality ,",
    "at least three - quarters of the code choices have an average codeword error rate below @xmath117 . by the same reasoning , at least three - quarters of the code choices have an average @xmath118 less than @xmath119 .",
    "therefore , at least half have both properties .    regarding the shaper as part of the encoder instead of part of the channel",
    ", we can define @xmath120 .",
    "applying lemma  [ lem : avetoworst ] , we can then make the further adjustments to @xmath48 and @xmath43 to simultaneously achieve a worst - case error of @xmath121 and worse - case leakage @xmath121 .",
    "finally , we can count how many messages can be reliably sent using the constructed encoder and decoder . from lemma  [ lem : csicoding ]",
    ", we have @xmath122 . inserting the known results for privacy amplification and data compression , theorems  [ thm : pa ] and  [ thm : csi ] in the appendix ,",
    "this becomes @xmath123 where @xmath124 and @xmath125 .",
    "again for simplicity , let @xmath126 . because @xmath97 is a function of @xmath0 via the extractor , the max - entropy can not increase when replacing @xmath0 by @xmath97 .",
    "since we are free to choose any @xmath85 in this argument we therefore have @xmath127.\\end{aligned}\\ ] ] to complete the argument for the private capacity , note that alice could precede the channel with another mapping from @xmath128 to @xmath0 , which she is free to optimize . regarding this as part of the original channel in the above argument",
    "then leads to the desired result .",
    "the direct part for the channel capacity follows by making a few small modifications .",
    "first , the markov inequality is no longer needed to ensure the two conditions of private communication are satisfied . here",
    "there is only one , and certainly there exists an encoding with average codeword error probability less than the average over codes and codewords , @xmath99 .",
    "we then only require remark  [ rem : avetoworst ] rather than lemma  [ lem : avetoworst ] to move to the worst - case error @xmath129 over codewords .",
    "finally , though in principle it is also possible for alice to precede the channel with a @xmath130 mapping , we shall see in the converse that this is not necessary .",
    "note also that in this context the encoder can dispense with the randomness needed to properly simulate @xmath0 and just fix a particular value of the output , for instance the @xmath11 with the largest @xmath131 .",
    "we first prove the converse for the private capacity and then modify the argument to establish the converse for the classical capacity .",
    "given an @xmath12-private coding scheme , the two requirements of the output made by the definition imply that @xmath132 and @xmath133 , the former for any distribution of messages and the latter for the uniform distribution .",
    "the former follows because the trace distance of the pair @xmath134 to @xmath135 is less than @xmath29 and therefore @xmath135 is in the @xmath136-neighborhood of @xmath134 ( see the appendix ; the square root is a consequence of the conversion of the trace to purification distance ) . but @xmath137 and thus @xmath132 .",
    "the latter follows because again the ideal output , in which @xmath19 is independent of the uniformly - distributed @xmath24 and therefore satisfies @xmath138 , is in the @xmath136-neighborhood of the actual pair @xmath139 .",
    "additionally , by the data processing inequality  @xcite , @xmath140 since the decoder generates the guess @xmath27 from @xmath1 .",
    "defining @xmath141 to be the uniform distribution , we have @xmath142\\\\&\\geq \\max_{m\\rightarrow x}\\left[{{h}_{\\min}^{\\sqrt{2{\\varepsilon}}}}(m|z)_{\\bar{p}_m}-{{h}_{\\max}^{\\sqrt{2{\\varepsilon}}}}(m|y)_{\\bar{p}_m}\\right]\\\\ & \\geq \\max_{m\\rightarrow x}\\left[{{h}_{\\min}^{\\sqrt{2{\\varepsilon}}}}(m|z)_{\\bar{p}_m}-{{h}_{\\max}^{\\sqrt{2{\\varepsilon}}}}(m|m')_{\\bar{p}_m}\\right]\\\\ & \\geq n,\\end{aligned}\\ ] ] which is the form we set out to prove .    for the converse of the classical capacity , observe that the encoding function @xmath48 is without loss of generality deterministic and injective .",
    "it might as well be deterministic , since if it used randomness , we could make it deterministic by fixing the randomness to that value with the least probability of error , which can not be worse than the average case . moreover , for this deterministic choice , @xmath48 must be injective , since a collision of two inputs having the same codeword necessarily implies an error . now , using the injectivity of @xmath48 we can define a distribution @xmath143 given a distribution over @xmath24 by simply taking the distribution of @xmath24 on its image in @xmath0 and zero otherwise .",
    "choosing the uniform distribution over @xmath24 and observing that @xmath144 when @xmath24 is uniformly distributed , we obtain @xmath145\\\\&\\geq \\left[{{h}_{\\min}^{}}(x)_{\\bar{p}_x}-{{h}_{\\max}^{\\sqrt{2{\\varepsilon}}}}(x|m')_{\\bar{p}_x}\\right]\\\\ & = \\left[{{h}_{\\min}^{}}(m)-{{h}_{\\max}^{\\sqrt{2{\\varepsilon}}}}(m|m')\\right]\\\\&\\geq n.\\end{aligned}\\ ] ]",
    "in the asymptotic limit of @xmath146 uses of a memoryless channel we recover the known results on the _ rate _ of public or private communication , where the rate of private communication is defined by @xmath147 and the rate of public communication is defined similarly . in general , the rates take the rather ungainly form @xmath148,\\\\ \\label{eq : privrate } r_{{\\rm prv}}(\\theta)&=\\lim_{\\ell\\rightarrow\\infty}\\frac{1}{\\ell}\\max_{p_{t},t\\rightarrow x^\\ell}\\left[h(t|z^{\\otimes \\ell})-h(t|y^{\\otimes \\ell})\\right].\\end{aligned}\\ ] ] here @xmath149 refers to a classical random variable on @xmath150 , while @xmath151 refers to the @xmath32-fold tensor product of the hilbert space @xmath152 and similarly for @xmath19 .",
    "the rate for public communication over quantum channels is known as the hsw theorem , after holevo  @xcite and schumacher and westmoreland  @xcite .",
    "the private rate was proven by devetak  @xcite .",
    "for the special case of classical channel outputs , i.e.  @xmath153 ( and separately @xmath154 ) are all simultaneously diagonalizeable , these reduce to the familiar and simpler form @xmath155,\\label{eq : clasyrate}\\\\ r_{{\\rm prv}}(\\theta_{\\rm cl})&=\\max_{p_t , t\\rightarrow x}\\left[h(t|z)-h(t|y)\\right].\\label{eq : prasyrate}\\end{aligned}\\ ] ] the classical rate is shannon s original noisy channel coding theorem  @xcite .",
    "the private rate was first established by wyner in the specific setting of the wire - tap channel  @xcite , later expanded to arbitrary channels by ahlswede and csiszar  @xcite , and strengthened to the stronger form of security used here ( cf .  eq .",
    "[ eq : secdef ] ) by maurer and wolf  @xcite .",
    "the proof proceeds in two steps .",
    "first we show that such rates are possible using the lower bound on the capacity and applying the asymptotic equipartition property ( aep ) of the conditional min- and max - entropies .",
    "then we show that the rates can not be exceeded by making use of the upper bound on the capacity and bounds on the conditional min- and max - entropy in terms of the conditional von neumann entropy . since the case of public communication follows from that of private communication ,",
    "we only give the argument for the latter .",
    "for the direct part , we begin with the lower bound on the capacity from theorem  [ thm : capacity ] . for @xmath21 uses of the channel @xmath7 ,",
    "this becomes @xmath156.\\end{aligned}\\ ] ] since this is a lower bound , we re free to choose @xmath128 as we like .",
    "we choose @xmath157 to be i.i.d . , each instance @xmath158 separately generating the likewise i.i.d .",
    "@xmath159 via some fixed map @xmath130 .",
    "now we make use of the aep , which states  @xcite @xmath160 and similarly for the conditional max - entropy .",
    "we then obtain @xmath161.\\end{aligned}\\ ] ] finally , we let @xmath162 , and replay the above argument using the superchannel @xmath163 ( @xmath164 independent uses of the channel ) to obtain the desired result .",
    "note that , in contrast to the standard proof technique , here we only need to make a statement about the entropies in the capacity formula , a statement provided by the aep .",
    "importantly , typical sequences , type classes , or the like play no role in the protocol itself , but could be used to establish the aep .",
    "such methods are not necessary ; indeed , the approach of  @xcite is based on properties of rnyi entropies .    to complete the argument , we consider the upper bound on the capacity , and use the bounds on the conditional min- and max - entropies from lemma  [ lem : entropybounds ] , replacing dim@xmath165 with @xmath166 . now",
    "the upper bound on the private capacity becomes @xmath167.\\end{aligned}\\ ] ] dividing through by @xmath32 and taking the limits @xmath146 and @xmath168 ( whose order is now irrelevant ) yields the desired result ( replacing @xmath32 by @xmath164 ) .    when the channel has purely classical outputs , the limit involving @xmath169 ( called regularization ) can be removed . for private communication",
    "we show this explicitly in lemma  [ lem : singlelett ] , which recovers eq .",
    "[ eq : clasyrate ] ; for public communication ( eq .",
    "[ eq : prasyrate ] ) see , e.g.  theorem 4.2.1 in  @xcite .",
    "should the channel produce quantum outputs , regularization is known to be necessary in both cases , private  @xcite and public  @xcite .    finally , we note that the optimization over maps @xmath130 is generally necessary to achieve the optimal rate of private communication , by means of the following example .",
    "suppose @xmath7 is a purely classical channel defined in fig .",
    "[ fig : example ] . to send private messages to @xmath1 ,",
    "clearly one can encode 0 as @xmath170 or @xmath171 randomly and 1 as @xmath172 .",
    "the message can be unambiguously determined from @xmath1 no matter the encoding , but @xmath19 will be completely random for either input , and so this encoding scheme achieves a rate of 1 bit .",
    "eschewing random encoding , the maximum rate of private communication is @xmath173 $ ] . due to the structure of the @xmath174 map ,",
    "the maximum of the first term is one , and this can only occur when 0 and 1 occur with equal probability . but",
    "this implies the second term is nonzero , meaning the overall rate is less than one .",
    "channel demonstrating the need for randomness by the encoder to achieve the private communication capacity .",
    "unmarked arrows denote deterministic maps ; otherwise the probability of a transition is marked . ]",
    "rather than reusing the proof techniques involved in understanding more basic information processing protocols such as privacy amplification and information reconciliation , here we have shown how to construct channel coding protocols from these protocols themselves . moreover , if the underlying protocols are optimal , then so are the channel coding protocols .",
    "this provides an appealing conceptual framework for two - terminal problems in information theory in which one successively builds up to more complicated protocols using simpler elements whose internal workings are not relevant for the present task .",
    "moreover , it is also appealing to see that the two basic primitives are characterized in terms of the two basic entropic quantities , smooth min- and max - entropy , and that these quantities enter the capacity expressions in a way which reflects the protocol construction .    in the setting of quantum information theory",
    "these entropies are dual  @xcite , as are the two primitives  @xcite , meaning only one primitive is needed to construct more and more complicated protocols . as an example , instead of appealing to theorem  [ thm : csi ] for the compressor / decompressor pair needed to establish eq .",
    "[ eq : capacitycount ] in theorem  [ thm : capacity ] , we may rely on    1 .   theorem  [ thm : pa ] , 2 .",
    "the duality of privacy amplification and information reconciliation as shown in  @xcite , and 3 .",
    "a new form of the uncertainty principle derived in  @xcite .    specifically , in the proof of theorem  [ thm : capacity ] we require a linear compressor / decompressor pair operating on the classical - quantum state of @xmath175 , classical in say the @xmath97 basis ( in an abuse of notation ) . by the duality in  @xcite",
    ", such a pair with error probabilty @xmath136 can be constructed from @xmath29-good linear privacy amplification of the conjugate basis @xmath176  , and the size of the compressed output @xmath177 in the optimal case is given by @xmath178 , where @xmath105 is the purification of the original system @xmath179 and @xmath180 .",
    "but from the uncertainty principle of  @xcite we have @xmath181 therefore @xmath182 , and we recover eq .  [ eq : capacitycount ] up to @xmath183 terms .",
    "also can not be substantially smaller , by the lower bound of theorem  [ thm : csi ] ) , and the capacity as calculated here is certainly no smaller than that of eq .",
    "[ eq : capacitycount ] ( up to @xmath183 terms ) . ]",
    "thus we have constructed all - decoupling proofs of the public and private capacities of a quantum channel , in the sense that establishling the capacity now does not rely on directly constructing a decoder for the receiver , as in the proof of theorem  [ thm : csi ] , but rather on decoupling the purifying system , as in the proof of theorem  [ thm : pa ] .",
    "in the asymptotic limit of many independent uses of the channel , we then recover the familiar hsw  @xcite and private capacity  @xcite results .",
    "this derivation of the classical capacity of a quantum channel can thus be seen as the classical - quantum analogue of  @xcite , where the quantum capacity of a quantum channel is derived using a decoupling approach . as with the main proof of theorem  [ thm : capacity ] ,",
    "the decoupling procedure outlined above also suffices to derive shannon s result on the public capacity of classical channels  @xcite , as well as the associated privacy capacity results  @xcite , simply by treating the classical channel in the formalism of quantum information theory .",
    "jmr acknowledges the support of cased ( www.cased.de ) .",
    "rr acknowledges support from the swiss national science foundation ( grant no .",
    "200021 - 119868 ) and the erc ( grant no .",
    "the conditional max - entropy for a state @xmath184 is defined by @xmath185 where the maximization is over positive , normalized states @xmath186 and @xmath187 is the fidelity of @xmath188 and @xmath186 .",
    "dual to the conditional max - entropy is the conditional min - entropy , @xmath189 with @xmath190 .",
    "the two are dual in the sense that @xmath191 for @xmath192 a pure state  @xcite .",
    "the min- and max - entropies derive their names in part from the following relation ( lemma 2 ,  @xcite ) , @xmath193 where @xmath194 for @xmath195 $ ] is the usual conditional von neumann entropy",
    ".    the min- and max - entropies can be _ smoothed _ by considering possibly subnormalized states @xmath196 in the @xmath29-neighborhood of @xmath184 , defined using the purification distance @xmath197 , @xmath198 note that the purification distance is essentially equivalent to the trace distance , due to the bounds @xmath199  @xcite .",
    "the smoothed entropies are then given by @xmath200 furthermore , the dual of @xmath201 is @xmath202 , so that taking the dual and smoothing can be performed in either order  @xcite .",
    "optimal one - shot privacy amplification results are established in  @xcite . using the entropy definitions above , the number of @xmath29-good random bits @xmath203 which can be extracted from the classical random variable @xmath0 against a possibly quantum eavesdropper is bounded by    given a state @xmath204 and @xmath205 such that @xmath206 , [ thm : pa ] @xmath207    meanwhile , optimal one - shot information reconciliation results are given in  @xcite .",
    "the minimum number of bits @xmath208 to which the classical random variable @xmath0 can be compressed and still be recovered using side information @xmath209 at the decoder with error probability less than @xmath29 is bounded by    [ thm : csi ] given a state @xmath210 and @xmath205 such that @xmath206 @xmath211",
    "[ lem : linearpreimage ] let @xmath212 be a linear function .",
    "then @xmath213 for all @xmath214 .",
    "pick an @xmath215 and consider all the @xmath216 such that @xmath217 . forming the differences",
    "@xmath218 , it follows from linearity that @xmath219 for all @xmath220 .",
    "now consider an arbitrary @xmath221 .",
    "clearly @xmath222 for all @xmath220 , so each output value has the same number of preimages .",
    "[ lem : arbpreimage ] let @xmath212 be an arbitrary function and denote by @xmath223 the preimage of an output @xmath224 . for a randomly - chosen output value @xmath224 , @xmath225 with probability at least @xmath226 .",
    "let @xmath0 be a uniform random variable over @xmath75 and @xmath227 . by the min - entropy chain rule  ( lemma 3.1.10 in  @xcite ) we have @xmath228 here",
    "@xmath229 is the size of the support of the distribution @xmath230 , the number of values taking nonzero probability . by the normalization condition for @xmath231 it follows that @xmath232 . and by the definition of @xmath233 , @xmath234 finally , applying the markov inequality to the random variable @xmath235 yields @xmath236\\leq \\frac{{\\varepsilon}|\\mathcal{x}|}{|\\mathcal{y}|}\\sum_{y\\in\\mathcal{y}}p_{y = y}\\frac{1}{|\\mathcal{x}_y|}\\leq { \\varepsilon},\\end{aligned}\\ ] ] which concludes the proof .",
    "let @xmath239 be a state in @xmath240 such that @xmath241 .",
    "then from eq .",
    "[ eq : entropyorder ] we have @xmath242 .",
    "since the purification distance bounds the trace distance , @xmath243 and we can use the continuity of the conditional von neumann entropy  @xcite to establish that @xmath244 , completing the proof for the min - entropy .",
    "an entirely similar argument holds for the max - entropy .",
    "start with the expression @xmath247 from eq .",
    "[ eq : privrate ] . by lemma 4.1 of  @xcite , or direct calculation ,",
    "we can rewrite this as a sum @xmath248 for @xmath249 .",
    "observe that the random variables involved in @xmath250 form the markov chain @xmath251 .",
    "this follows because @xmath252 is a markov chain , where @xmath72 denotes the tuple of @xmath253 random variables omitting @xmath159 , and @xmath254 can be computed from @xmath255 .",
    "but now we can maximize each term over @xmath254 subject to the markov chain condition and obtain the single - letter formula @xmath256\\\\ & \\leq   \\max_{(t , v)}\\ell\\left[h(t|vz)-h(t|vy)\\right],\\end{aligned}\\ ] ] for the markov chain @xmath257 .",
    "since shannon entropies conditioned on @xmath258 are averages of entropies conditioned on specific values @xmath259 , this implies @xmath260.\\end{aligned}\\ ] ] finally , conditioning on @xmath259 preserves the markov chain @xmath261 , since @xmath262 implies @xmath263 . and",
    "because each choice of @xmath258 and @xmath264 induces a conditional distribution @xmath265 , the maximization need only be taken over @xmath128 . using this in eq .",
    "[ eq : privrate ] completes the proof .",
    "u.  maurer and s.  wolf , `` information - theoretic key agreement : from weak to strong secrecy for free , '' ser .",
    "lecture notes on computer science , b.  preneel , ed .",
    "1807.1em plus 0.5em minus 0.4emberlin , heidelberg : springer berlin heidelberg , 2000 .",
    "j.  soriaga and p.  siegel , `` on distribution shaping codes for partial - response channels , '' in _ proceedings of the 41st annual allerton conference on communication , control , and computing _ , monticello ,",
    "illinois , oct .",
    "2003 , pp . 468477 .",
    "j.  m. renes , `` duality of privacy amplification against quantum adversaries and data compression with quantum side information , '' _ proc .",
    "a _ , published online before print dec .",
    "2010 ; doi:10.1098/rspa.2010.0445 m.  tomamichel and r.  renner , `` the uncertainty relation for smooth entropies , '' _ arxiv : 1009.2015 _ , sep . 2010 .",
    "r.  renner and r.  knig , `` universally composable privacy amplification against quantum adversaries , '' in _ second theory of cryptography conference _",
    "lecture notes in computer science , vol . 3378/2005 .",
    "1em plus 0.5em minus 0.4emcambridge , ma : springer , feb . 2005 , pp . 407425 ."
  ],
  "abstract_text": [
    "<S> we show that optimal protocols for noisy channel coding of public or private information over either classical or quantum channels can be directly constructed from two more primitive information - theoretic tools : privacy amplification and information reconciliation , also known as data compression with side information . </S>",
    "<S> we do this in the one - shot scenario of structureless resources , and formulate our results in terms of the smooth min- and max - entropy . in the context of classical information theory , this shows that essentially all two - terminal protocols can be reduced to these two primitives , which are in turn governed by the smooth min- and max - entropies , respectively . in the context of quantum information theory </S>",
    "<S> , the recently - established duality of these two protocols means essentially all two - terminal protocols can be constructed using just a single primitive .    quantum information , channel coding , privacy amplification , information reconciliation </S>",
    "<S> , slepian - wolf coding , smooth entropies    of the major trends in information theory , both classical and quantum , is that a small set of proof techniques can be used to construct a wide variety of protocols . </S>",
    "<S> random coding is as ubiquitous as it is useful in classical information theory , and the method of decoupling increasingly plays a similar role in quantum information theory . instead of reusing proofs , a different approach is to reuse the protocols themselves , building up more complicated protocols by combining simpler ones . the goal is to do this in such a way that the inner workings of the parts do not have to be analyzed to ensure the correct functioning of the overall protocol . </S>",
    "<S> for instance , joint source - channel coding can be accomplished by simply combining a data compressor with a channel coding scheme  @xcite . in the quantum realm </S>",
    "<S> , the `` mother of all '' protocols , a fully - quantum version of the slepian - wolf task , can generate a variety of two terminal protocols involving entanglement when combined with teleportation and dense coding  @xcite .    in this paper </S>",
    "<S> we construct optimal protocols for communication of classical information over noisy channels from two simpler primitives : randomness extraction and information reconciliation , also known as data compression with side information . </S>",
    "<S> the construction works for either classical channels or quantum channels explicitly accepting classical inputs , and by replacing randomness extraction with privacy amplification , we directly obtain a protocol for private communication . </S>",
    "<S> we work in the one - shot scenario of structureless resources , meaning the coding scheme does not rely on repeated uses of a memoryless channel . </S>",
    "<S> rather , the one - shot scenario is considerably broader in approach , encompassing not only channels in the traditional sense of communication ( both with and without memory ) , but also channels as models for the dynamics of a physical system , for which the memoryless assumption would be out of place .    besides adopting a new technique to construct the protocols , </S>",
    "<S> the resulting capacity expressions are novel as well . </S>",
    "<S> we find that the capacities of a channel for one - shot public   or private communication can be characterized in terms of smooth conditional min- and max - entropies , introduced and characterized in  @xcite . </S>",
    "<S> furthermore , these expressions are shown to be essentially tight , up to small additive terms . </S>",
    "<S> appealing to the asymptotic equipartition property ( aep ) for smooth entropies  @xcite allows us to quickly recover the usual capacity expressions in the memoryless case , from shannon s original result on the capacity of the classical channel for public communication  @xcite and the associated capacity for private communication  @xcite , to the capacity of a quantum channel for public classical communication ( known colloquially as the holevo - schumacher - westmoreland , or hsw , theorem )  @xcite as well as for private communication  @xcite . </S>",
    "<S> furthermore , dividing the problem of noisy channel communication into questions of coding and questions of channel properties considerably simplifies the logical arguments and should be of independent pedagogical value .    </S>",
    "<S> one - shot expressions for the capacity of public communication have been derived before . in  </S>",
    "<S> @xcite the one - shot capacity of a classical channel was characterized in terms of smooth min- and max - entropies , while  @xcite derives an expression for the capacity of quantum channels in terms of generalized ( rnyi ) relative entropies following a hypothesis - testing approach . </S>",
    "<S> these results can be seen as generalizations of earlier ( asymptotic ) results based on the information spectrum method  @xcite . </S>",
    "<S> very recently ,  @xcite finds tight bounds on the capacity in terms of a smooth relative entropy quantity again from a hypothesis - testing approach . combining the latter results with those here implies a relation between the smooth relative and conditional entropies .    </S>",
    "<S> both of the primitive tasks used here are designed to manipulate `` static '' resources , in the sense that the goal is to transform randomness shared by distant parties into a different form ( so that the joint distribution of the values held by the parties is close to a given one ) . </S>",
    "<S> this task should be performed using local operations and a limited amount of communication . in particular </S>",
    "<S> , randomness extraction corresponds to the task of generating uniformly - distributed random variables out of non - uniform inputs , while information reconciliation uses classical communication to correlate , or reconcile , a random variable held by one party ( alice ) with that held by another ( bob ) . </S>",
    "<S> one can view the classical data transmitted for this latter task as a compression of alice s random variable , as it can be decompressed by bob with the help of his random variable ( or quantum system ) .    intuitively , it is plausible that the static information reconciliation protocol could be adapted to enable reliable communication over a noisy channel , a `` dynamic '' resource , in the following manner , depicted schematically in figure  [ fig : diagram ] . assuming uniform distribution of the channel inputs , consider the random variables describing the input @xmath0 and output @xmath1 ( where the latter may be quantum in the case of a quantum channel ) . </S>",
    "<S> as described above , information reconciliation enables bob to reconstruct @xmath0 from the compressed version of it , @xmath2 , along with his information @xmath1 . </S>",
    "<S> now suppose alice and bob agree on a particular @xmath3 in advance for the communication task , in that alice restricts her channel inputs @xmath0 to those with compressed output @xmath4 . </S>",
    "<S> upon receipt of @xmath1 , bob can reconstruct @xmath0 by simply reusing the decompressor of the information reconciliation protocol . in this way </S>",
    "<S> , each information reconciliation protocol defines a channel coding scheme : every value @xmath5 specifies a channel code consisting of all the possible inputs which compress to that value @xmath6 . since we assumed uniform distribution of the channel inputs , this coding scheme is generally not optimal . </S>",
    "<S> however , by running a randomness extractor backwards we can create the optimal input distribution from a uniform one , circumventing this problem . </S>",
    "<S> this is similar to a method used by gallager  @xcite and later expanded in  @xcite .    </S>",
    "<S> the remainder of the paper is devoted to making this intuition rigorous . </S>",
    "<S> we begin in the next section by formally specifying the problem and stating the results in theorem  [ thm : capacity ] . </S>",
    "<S> we then move immediately to the proof of the direct part , achievability , in section  [ sec : achieve ] and the converse in section  [ sec : converse ] . in section  [ sec : asympt ] </S>",
    "<S> we show how the usual results may be quickly recovered for the case of very many uses of a memoryless channel . </S>",
    "<S> we conclude in section  [ sec : concl ] by discussing applications of this result and its relation to other work . </S>"
  ]
}