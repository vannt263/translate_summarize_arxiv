{
  "article_text": [
    "norbert wiener , the founder of cybernetics , wrote that it is the `` boundary regions of science which offer the richest opportunities to the qualified investigator .",
    "they are at the same time the most refractory to the accepted techniques of mass attack and the division of labor ''  @xcite .",
    "he went on to explain that `` a proper exploration of these blank spaces on the map of science could only be made by a team of scientists , each a specialist in his own field but each possessing a thoroughly sound and trained acquaintance with the fields of his neighbors ; all in the habit of working together , of knowing one another s intellectual customs , and of recognizing the significance of a colleague s new suggestion before it has taken on a full formal expression .",
    "the mathematician need not have the skill to conduct a physiological experiment , but he must have the skill to understand one , to criticize one , and to suggest one . the physiologist need not be able to prove a certain mathematical theorem , but he must be able to grasp its physiological significance and to tell the mathematician for what he should look . ''    indeed , three giants of science , wiener , von neumann and shannon , realised in the 1940s the need for understanding the brain in terms of the fundamental engineering principles applicable to any computational device : energy , entropy and feedback  @xcite .",
    "this led to the macy conferences ( 19461953 ) which attracted leading scientists from across engineering and the physical and life sciences .",
    "the macy conferences were one of the earliest organised approaches to transdisciplinarity and hailed by some as the most important event in the history of science after world war ii .",
    "they demonstrated the need for , and the initial difficulties in , establishing a common language powerful enough to communicate the intricacies of the relevant fields across the physical and life sciences and engineering .    while their dream was not realised , this was primarily due to insufficient experimental data .",
    "as time marched on , the barrier to bringing together the ever more specialised disciplines grew larger . with tremendous experimental advances",
    "having been made in the past 60 years , it is timely to stand on the shoulders of these giants and resume their quest . with this as motivation , the present article endeavours to whet the appetites of neuroscientists and information theorists alike for learning more of each other s fields .",
    "the human brain is often described as the most complex structure in the known universe  @xcite .",
    "certainly , it is the most efficient signal processing device known . drawing only 20 watts of power ,",
    "the brain significantly outperforms engineered devices at signal processing tasks such as source separation , feature extraction , and speech and image recognition  @xcite .",
    "this is all the more remarkable because signals within the brain propagate very slowly compared with those in a computer .",
    "this suggests that the brain uses a paradigm for signal processing very different from any developed in engineering .",
    "why then should engineering in general and information theory in particular have relevance to understanding the brain ?",
    "the answer lies partially in the fact that engineers study fundamental laws pertinent to any system , including biological ones  @xcite . indeed , john von neumann viewed the brain as a hybrid computer which performs control , communication and computation , and concluded that information theory is therefore essential for understanding its functionality  @xcite .",
    "wiener too recognised that information theory was essential for a deeper understanding of feedback and thus life  @xcite .",
    "anecdotal evidence suggests shannon himself , the father of information theory , may have been partially motivated by how his brain processed `` information '' when performing a complex task such as juggling balls .",
    "a few words on the concept of feedback are in order .",
    "feedback refers to achieving a task , such as keeping a car travelling at a constant speed , by repeatedly measuring the current state , such as the car s speed , and feeding those measurements back and using them to make the requisite changes at the input , such as applying more or less pressure to the accelerator of the car .",
    "feedback is a fundamental concept in engineering because it can militate errors caused by imprecisions and external interference .",
    "the brain too must use feedback to overcome imprecisions  @xcite ; without feedback , we would fall over whenever we attempted to walk . within the sensory pathways",
    "there are tremendous numbers of feedback paths connecting regions of higher - level brain function to regions of lower - level functionality , giving rise to top - down processing theories of the visual pathways and providing a mechanism for selective attention .",
    "although they started out in different disciplines  information theory emerged from communication theory while feedback was studied in control theory  recent years have seen some convergence of feedback and information theory .",
    "a fundamental question is what is the slowest rate at which information must be fed back for the system to work .",
    "scientists have started to consider how fast the brain must be processing information if we are able to walk properly and can move our hand in a straight line even though random external forces are impeding its motion in experiments  @xcite .",
    "this is an example of such convergence of two important theories .    by virtue of being introductory",
    ", the present paper focuses on discussing information theory in `` one - way '' ( i.e. feed - forward exclusively ) biological contexts .",
    "a comprehensive account of the brain in engineering terms would necessarily involve the marriage of information theory and control theory . repeating the words of von neumann",
    ", the brain must be understood in terms of control ( feedback ) , communication ( information theory ) and computation .",
    "it must be recognised that the mathematical discipline of `` information theory '' does not ( and should not ) capture all aspects of how the word `` information '' is used in spoken language .",
    "failing to distinguish the two can lead to errors caused by flawed intuition in one direction , or the inappropriate application of information theory in the other .",
    "information theory was invented in response to practical problems faced by the designers of communication systems such as telephones and data modems .",
    "the basic problem is to find an efficient way of transmitting information from one place to another , whether it be a probe sending information from the moon back to earth or a mobile telephone sending and receiving voice and internet packets .",
    "indeed , consider the problem of one person trying to send a series of messages to another person on the other side of a brick wall ; for simplicity , assume this second person is not allowed to speak or send any other form of message to the first person such as an acknowledgement or request for clarification ( or `` retransmission '' ) .",
    "how should the first person send each message ?",
    "one thing is clear ; the louder the first person shouts , the greater the chance that the second person can understand the message over the background noise ( perhaps the neighbours are mowing their lawns ) . perhaps a little harder to appreciate but equally true in the digital world ,",
    "if the first person were to speak more slowly the second person would have a greater chance of catching every word . the third parameter that can be adjusted",
    "is the level of redundancy .",
    "when we speak with a young child we tend to elaborate and use more words to describe a concept in an attempt to increase the chances of correct reception of the overall message .    in information theory , these parameters are referred to formally as the transmission power , the transmission rate and coding ( or redundancy ) .",
    "the simplest form of coding is to repeat the message two or more times .",
    "this is known as a repetition code .",
    "shannon s pioneering work shattered a long - held belief that with finite power it was impossible to be able to transmit a message in such a way as to _ guarantee _ its _ perfect reception _ even in the presence of noise and other interference . indeed ,",
    "even if i shouted at the top of my voice and repeated myself a hundred times , every so often the interference ( lawn mowers ? ) will prove too great and my message will be lost .",
    "the answer lies in coding ; repetition codes are not particularly good codes .",
    "shannon realised that there exist very clever codes which can ensure that any two messages are so different from each other that the receiver can correctly decide which message was sent despite the interference .",
    "technically , perfect reception requires the receiver to listen forever before deciding which message was sent but the key point is that given any positive but arbitrarily small probability of error ( such as one message being incorrectly received in @xmath0 messages ) then a code can be constructed which achieves this level of performance in finite time , and more importantly , the transmission power does not need to be increased .",
    "increasing transmission power to achieve a particular error rate is grossly inefficient compared with choosing a better code . in the example of one person trying to convey a message to the other person , the secret is to share a codebook beforehand , and a different sequence of sounds , one for each message that may be sent , is written in it .",
    "`` it will rain tomorrow '' might be encoded as ( a segment of ) beethoven s 5th symphony while `` it will be sunny tomorrow '' might be encoded as a hard rock song .",
    "these two encoded messages are `` sufficiently different from each other '' to have very little chance of being confused . more importantly , any small fragment of the two messages are different .",
    "this is how interference is overcome .",
    "because interference itself has limited power ( otherwise the game would not be fair ! ) then even if there are times when the interference is particularly bad , there will be other times when the interference is back to normal and in the long run , there is no confusing beethoven for hard rock .    for the transmission rate to be acceptable",
    "the codebooks would need to contain more than just two messages .",
    "( with two messages , each one encoded by a five - minute song , the transmission rate would be 1 bit per 5 minutes . ) in the same way that the transmission power does not need to go to infinity , the transmission rate need not go to zero .",
    "precisely , shannon discovered a quantity known as the channel capacity .",
    "if the transmission rate is less than the channel capacity then communication with any desired level of accuracy is possible whereas if the transmission rate exceeds the channel capacity then it becomes impossible to have arbitrarily good performance in finite time . as is to be expected , the channel capacity depends on the interference .",
    "the more destructive the interference the lower the channel capacity .",
    "messages are also passed around within an organism .",
    "information gathered by an organism s senses must be communicated for it to have any effect on the organism . within the brain and nervous system",
    ", information is manipulated in at least three different ways :    * information acquisition ( sensory transduction ) ; * communication between spatially separated regions ( information transmission ) ; * memory formation and recall ( information storage  @xcite ) .    in brains ,",
    "each of these are essential for the emergence of broader functions that might be termed ` computation ' .",
    "communication is perhaps most fundamental , since information from the senses needs to be communicated in order for it to have any affect on an organism , while information storage , whether in computers or brain memories , can be viewed as communication from the past to the present or future .",
    "understanding how the brain stores and transmits information is tantamount to understanding the brain as a whole because if we could `` listen in '' to brain messages it would surely be just a matter of time before we understood the computational side , too .",
    "that said , the possibility that the brain does not separate out information transmission from information processing must be considered .",
    "whereas computer architectures have mechanisms known as buses for moving information between different processing units , the brain might take a more efficient distributed approach and simultaneously process and communicate in a nonseparable fashion .",
    "it has been stated that `` computation in the brain always means that information is moved from one place to another ''  @xcite .",
    "a comprehensive understanding of the brain s mechanisms for internal communication will likely form an integral part of more advanced theories about how ` computation ' arises within brain networks .",
    "regardless of how the brain actually processes information , at the end of the day the brain is an input - output system ( we react to what we sense ) and therefore subject to the same laws as any other input - output system .",
    "information theory is therefore relevant to understanding how the brain works , and conversely , it is highly likely that advances in the field of information theory will be made in synergy with new discoveries of the computing paradigms used by the brain  @xcite . indeed , information theory may have to expand to address new neurobiologically relevant questions if it is to be powerful enough to explain all aspects of how the brain manipulates information .    to demonstrate the relevance of even simply thinking in information theoretic terms",
    ", landauer estimated that humans learn information at a rate of about two bits per second  @xcite . taking memory loss into account",
    ", a person will accumulate approximately two billion bits of information in a lifetime ( or approximately 240 mb in computing terms ) .",
    "since our brain has many more synapses than two billion , landauer concludes that `` possibly we should not be looking for models and mechanisms that produce storage economies but rather ones in which marvels are produced by profligate use of capacity . ''",
    "according to  @xcite , biological science asks six kinds of questions about domains ranging from molecules and cells , up to the biosphere :    1 .",
    "how is it built ?",
    "( structures ) 2 .",
    "how does it work ?",
    "( mechanisms ) 3 .   what is it for ? ( functions ) 4 .",
    "what goes wrong ? ( pathologies ) 5 .",
    "how is it fixed ? ( repairs ) 6",
    ".   how did it begin ? ( origins )    utilising information theory in neuroscience is ultimately useful only if it can address one or more of these questions .    in this paper",
    "we advocate that information theory 1 ) can be a useful framework for finding answers to some of these questions ; but 2 ) must be broadened for its theorems to be directly applicable to neuronal networks .",
    "although information manipulation can happen at very different levels of organisation , such as storage of information in genes , or communication at the level of synaptic transmission between cells , or at that of spiking patterns of neurons in a network , in this paper we will be focusing on examples that involve spiked - based communication between neurons .    in making these points ,",
    "it is necessary for us to introduce the most basic and well - known information theoretic concepts in section  [ sec : info ] , before discussing the challenges of applying the theory meaningfully to questions in neuroscience in section  [ s : neuro ] .",
    "then in section  [ sec : neuralcapacity ] we summarise a specific example which illustrates that information theoretic approaches depend critically on different assumptions that could be made about neural systems . finally in section  [ s : conclusion ] we conclude the paper with some closing remarks on the material we cover and briefly summarise recent developments on information theoretic approaches in neuroscience that extend well beyond the classical ideas we present , thereby with increasing relevance to neurobiological systems .",
    "this section briefly explains key concepts from shannon information theory and hints at possible contributions in neuroscience . by _",
    "shannon information theory _ we are referring to a specific sub - part of the broader field of _ information theory_. the latter , by definition , encompasses any _ mathematical theorems about information _ , and therefore is not confined to well - known concepts introduced by shannon , such as entropy and mutual information . as we discuss later , information theory beyond shannon theory may be very important in neuroscience .",
    "shannon s milestone paper  @xcite that founded the field of information theory showed to the world that introducing the right kind of redundancy was the key to moving information from one place to another in an efficient and reliable manner . since information sources such as spoken voice or pdf ( portable document format ) documents generally contain the wrong kind of redundant information , shannon proposed a two - step process :",
    "first remove the existing redundancy by compressing the message to be sent , then introduce the right kind of redundancy for communicating the message through the channel at hand .",
    "these two important concepts are known as `` source coding '' and `` channel coding '' respectively .",
    "they motivate several fundamental questions including determining the maximum amount of compression possible of an information source .",
    "answers to these questions are given in terms of quantities such as _ entropy _ and _ mutual information_. it is important to realise that these quantities were given special names because they serve to answer important questions for a particular class of problems .",
    "it would be a mistake to assume without additional justification that they are applicable or even meaningful beyond the bounds of the original questions for which they serve as the answers to .",
    "see e.g.  @xcite for more discussion",
    ".      living in the digital age , readers will be familiar with compressing files .",
    "zipping up a file to send to a friend is an example of lossless compression . generally ( but not always )",
    "the compressed file will be smaller than the original yet no information has been lost ; the friend can recover the original file by decompressing the compressed file ( fig .",
    "[ fig : sourcecoding ] ) . for compressing music or photos , significantly greater",
    "compression can be achieved by using lossy compression algorithms such as mp3 and jpeg .",
    "as the name suggests , some information is lost  @xcite .",
    "the original can be recovered sufficiently well for a satisfactory compromise to have been reached ; a small amount of quality is sacrificed for a large saving in storage space .",
    "the remainder of this section discusses lossless compression only .",
    "consider the problem of compressing a short message of length 8 bits .",
    "a bit is simply a `` 0 '' or a `` 1 '' so an 8-bit message is a sequence of eight zeros and ones , such as `` 01011101 '' or `` 10101010 '' .",
    "a calculation ( or by writing out all the possibilities if need be , starting with `` 00000000 '' , `` 00000001 '' and continuing until `` 11111111 '' ) shows that there are precisely 256 different 8-bit messages .",
    "compressing a message would mean using fewer than 8 bits to store the message .",
    "a simple enumeration shows that this is impossible as stated ; there are only 128 7-bit messages , not enough to represent all possible 8-bit messages .",
    "how then does a computer compress a file losslessly ?",
    "the secret is that there is often redundancy in the kinds of information that people are interested in .",
    "equivalently , it is generally the case that not all messages have an equal chance of occurring .",
    "for argument s sake , assume that out of the 256 possible messages , there are 15 messages which occur most of the time . to exploit this",
    ", we may decide to use 4 bits to represent each of these messages .",
    "precisely , `` 0000 '' would represent the first message , `` 0001 '' the second , up to `` 1110 '' for the 15th message . to represent any other message",
    ", we would first write down `` 1111 '' to mean `` not one of the 15 ''",
    "and then we would write down the original message using 8 bits .",
    "this means that 15 of the messages can be written down using only 4 bits but the remaining @xmath1 messages now require @xmath2 bits for their storage .",
    "the only way to make this meaningful is to consider repeating this compression exercise many times . if we had to store a very large number @xmath3 of 8-bit messages using this scheme , how many bits will be required ?",
    "assume that @xmath4 out of the @xmath3 messages belong to the set of 15 special messages .",
    "these @xmath4 messages require 4 bits while the remaining @xmath5 messages require 12 bits , or in total , @xmath6 bits are required compared with @xmath7 bits had we not compressed the messages .",
    "provided @xmath4 is sufficiently large , we will have succeeded in compressing the data .",
    "for example , if @xmath8 and @xmath9 then we would require only 6,000 bits rather than the original 8,000 bits .",
    "the conventional way to describe the above scenario is to work with probabilities .",
    "we assume that the messages we are being asked to compress are being generated at random and there is no correlation between the message we are being asked to compress now and the messages we have already compressed .",
    "mathematically , we represent the original sequence of messages by a sequence of independent and identically distributed random variables @xmath10 , each having a probability density @xmath11 . in the above example , each @xmath12 would be an 8-bit message ( or equivalently , a number between 0 and 255 inclusive ) and @xmath11 would be the probability that a particular message @xmath13 is chosen . for concreteness ,",
    "assume that each of the first fifteen messages have a 5% chance of occurrence ( meaning there is a 75% chance of a randomly chosen message being one of these 15 and thereby corresponding with the earlier choice of @xmath8 and @xmath9 ) .",
    "then @xmath14 .",
    "we assume that all other messages each have a probability of @xmath15 of occurring .",
    "the expected number of bits required to compress a single message can then be calculated by summing over @xmath16 the probability that the @xmath16 th message occurs multiplied by the number of bits required to represent the @xmath16 th message .",
    "when most of the probabilities are the same the calculation simplifies . with the values given above , the expected length is calculated to be @xmath17 .",
    "thus , on average , @xmath18 bits would be required to compress @xmath3 messages drawn at random if the above scheme were used .",
    "is there a better compression scheme , one which requires fewer than 6 bits per message on average ?",
    "in fact , what is the best possible ? as elucidated presently , shannon was able to answer these questions .",
    "first though , a technicality needs mentioning .    coding each message separately , as was done above , is inefficient",
    "it is better to concatenate a series of messages and compress them all at once ; this provides more opportunity for better compression through the simple fact that there are more compression schemes to choose from .",
    "( it also alleviates the wasted space caused by otherwise having to use an integer number of bits to represent each message . )    it is therefore quite standard to refer to each @xmath12 as a symbol rather than a message and ask how many bits per symbol on average must be used to compress the infinitely long sequence of independent and identically distributed symbols @xmath19 if each symbol has a probability @xmath11 of occurring .",
    "when @xmath13 is a random variable and its distribution is @xmath11 , its _ entropy _ is defined as @xmath20 ,    \\label{eq : entropy_discrete}\\end{aligned}\\ ] ] where @xmath21 $ ] denotes the expectation with respect to @xmath11 .",
    "the practical operation is a summation when @xmath13 is discrete and an integration when @xmath13 is continuous .",
    "when @xmath22 is the base of logarithm , i.e. @xmath23 , the units of entropy are bits and @xmath24 is precisely the number of bits per symbol required on average to compress an infinitely long sequence of symbols when each symbol has probability @xmath11 of occurring .",
    "it is for this reason that people endeavour to explain entropy as quantifying the `` ambiguity '' or `` uncertainty '' about the random variable @xmath13 .",
    "when @xmath13 has only one possible state ( that must therefore occur with probability @xmath25 ) , there is no ambiguity about @xmath13 and the entropy is @xmath26 .",
    "however if @xmath13 takes one of two states with probability @xmath27 and @xmath28 , respectively , ( @xmath29 ) , entropy is maximised when @xmath30 and @xmath31 .",
    "this is exactly 1 bit and implies that a sequence of equally likely zeros and ones can not be compressed .",
    "note also that if @xmath32 , @xmath33 .    _",
    "shannon s source coding theorem _",
    "states that ( in the limit as the number of symbols goes to infinity ) it is _ possible _ to compress each symbol to @xmath24 bits on average ( and _ impossible _ to do better ) .",
    "it does not however say how to design such a source code .",
    "furthermore , the practical construction of compression and decompression methods is complicated by considerations of algorithmic efficiency ( which affects battery life in portable equipment such as mobile telephones ) and latency ( how long the receiver must wait from the time a symbol is sent until that symbol can be received and decoded ) .",
    "that said , having a target to aim for is extremely useful and entropy provides that target for source compression .",
    "the reader may wish to verify that for the example introduced in this section , the corresponding entropy is @xmath34 bits per symbol .",
    "this represents the best any compression scheme can hope to achieve , and indeed , it is lower than the 6 bits per symbol scheme presented here .",
    "the following example of a binary symmetric channel will be used to add concreteness to the ensuing introduction of _ mutual information _ and _ channel capacity_. let @xmath35 denote a binary sequence which is to be transmitted to another person or device .",
    "it is called the source sequence . the medium through which a message can be sent from one person or device to another",
    "is called the channel .",
    "mathematically , a channel takes a sequence at its input and it generates another sequence at its output . if the channel were ideal , it would simply copy its input to its output and communication would be straightforward .",
    "generally though , the channel is not ideal .",
    "it introduces random errors .",
    "if @xmath36 is the binary input sequence ( which is shorthand notation for @xmath10 ) then the binary output sequence @xmath37 of a binary symmetric channel with error probability @xmath27 is given by the rules that 1 ) for each integer @xmath16 , the output @xmath38 at time @xmath16 depends only on the corresponding input @xmath39 at the same time @xmath16 ; and 2 ) the probability that @xmath40 differs from @xmath39 is @xmath27 . if @xmath41 then on average one in every ten symbols will be corrupted , meaning either a @xmath26 was sent and a @xmath25 was received , or a @xmath25 was sent and a @xmath26 was received .    what sequence @xmath36 should be sent over the channel if the ultimate aim is to send @xmath42 reliably to the receiver , assuming of course that the receiver can process the output @xmath43 of the channel before deciding what it believes the message @xmath42 is ?",
    "this is illustrated in fig .",
    "[ fig : channelcoding ] where the operation of generating @xmath36 from @xmath42 is called ( channel ) encoding and the operation of generating @xmath44 , the receiver s best guess at the original message , is called ( channel ) decoding .        for simplicity ,",
    "often the encoding and decoding processes work on blocks of data .",
    "precisely , the original source sequence @xmath45 is divided up into subsequences of length @xmath4 .",
    "each of these is encoded to a longer binary sequence @xmath13 with length @xmath3 .",
    "for example , a simple @xmath46 , @xmath47 block code would be to add a parity bit ( i.e. a bit that is zero when the sequence has an even number of zeros , and a one when an odd number ) after every two symbols , so : `` @xmath48 '' becomes `` @xmath49 '' ; `` @xmath50 '' becomes `` @xmath51 '' ; `` @xmath52 '' becomes `` @xmath53 '' and `` @xmath54 '' becomes `` @xmath55 . ''",
    "therefore , the sequence `` @xmath56 '' becomes `` @xmath57 '' where the 3rd and 6th bits are the introduced parity bits .",
    "this coded sequence is transmitted through the channel . at the other end",
    ", the receiver reverses the process , converting each block of @xmath3 symbols back into a block of @xmath4 symbols . in this particular case , introducing just a single parity bit does not allow the receiver to have a better guess at what the original message is , but it does allow the receiver to detect if a single bit has been changed .",
    "this is called error detection .",
    "error correction , when the receiver is not only able to detect an error has occurred but can fix the error and therefore recover the original message , requires more redundancy to be introduced , that is , choosing @xmath3 to be larger than @xmath58 .",
    "( if there are too many errors then error correction would fail , but the key point is that the probability that several consecutive bits are wrong is significantly smaller than if a single bit were wrong , therefore a small increase in redundancy allows a substantial increase in reliability . )    the two lengths @xmath4 and @xmath3 together define the `` rate '' of the code , which perhaps is better understood as measuring the decrease in throughput caused by the introduction of redundancy by the encoder .",
    "precisely , in the above example , the rate of the code is @xmath59 , meaning that if the channel can accept encoded symbols at a rate of 1 bit per second then the source symbols must have a rate of only @xmath60 bits per second .",
    "reducing the rate enables more redundancy to be introduced which can be used to increase the chance of the receiver being able to work out what message was sent .",
    "shannon s remarkable observation was that there is a much better way of increasing the chance of correct reception than by decreasing the rate towards zero . for a fixed rate @xmath60 ,",
    "the block size @xmath4 can be increased ( thereby increasing @xmath3 according to the formula @xmath61  ) .",
    "this allows a more sophisticated form of redundancy to be introduced ( but at the price of introducing greater latency ; the receiver must receive @xmath3 symbols before it can work out what the corresponding @xmath4 message symbols were ) .",
    "shannon proved that there exists a rate @xmath62 , called the channel capacity , such that for any rate @xmath60 strictly less than @xmath62 and any desired error rate @xmath63 ( meaning that the probability that the receiver decodes a bit incorrectly is less than @xmath64 , which might be chosen to be @xmath65 or smaller in practice ) , there exists a @xmath4 ( possibly quite large ) and a block encoder and decoder pair such that the receiver can correctly decode each bit of the source message with error probability less than @xmath64 .",
    "this is customarily summarised by saying that error - free communication is possible at rates below the channel capacity . ] .",
    "shannon was able to give a formula for computing the channel capacity @xmath62 .",
    "when this formula ( described below ) is applied to the above example of a binary symmetric channel with probability of error @xmath27 , the channel capacity is found to be @xmath66 , meaning for example that if the channel can transfer one bit per second then the source symbols must arrive slower than @xmath62 bits per second . if @xmath41 then @xmath67 meaning that for every 1,000 source symbols , just over 1,883 encoded symbols are required for reliable communications .",
    "the formula for channel capacity involves a quantity called _ mutual information_. intuitively , the mutual information of the input and the output of the channel measures how much information the output provides about the input ; the more reliable the channel the higher the mutual information .",
    "it is therefore reasonable to expect that the larger the mutual information the greater the channel capacity .",
    "bearing in mind that `` information '' is a very general word and it is therefore not possible to capture all its nuances in a single mathematical definition , it is expedient to return to the idea in the previous section of using asymptotic compressibility as a measure of information .",
    "it turns out that this is the right definition to use when it comes to determining the capacity of a channel ( which in itself is an asymptotic measure ) .",
    "suppose there are two random variables , @xmath13 and @xmath68 , and they are somehow related to each other .",
    "for example , @xmath13 might denote temperature while @xmath68 denotes humidity .",
    "even simpler , @xmath13 might represent the outcome of rolling a 6-sided die while @xmath68 is given the value @xmath69 if the die landed on an even number , or @xmath70 if odd . knowing @xmath68 gives partial information about @xmath13 ; how can we measure how much information @xmath68 tells us about @xmath13 ?",
    "the fact that @xmath68 gives partial information about @xmath13 is reflected in the fact that if @xmath68 is known then @xmath13 can be compressed more than if @xmath68 were not known . in the above example , if @xmath68 were not known then it is impossible to compress @xmath13 because each of the outcomes is equally likely ; we are forced to use one of six possible symbols ( or @xmath71 bits ) to store each sample of @xmath13 ; the entropy of @xmath13 is @xmath72 . if @xmath68 is known though then only one of three possible symbols ( or @xmath73 bits ) needs to be stored ; the _ average conditional entropy _ is @xmath74 .",
    "the additional amount of compression possible , @xmath75 , is called the mutual information and measures the amount of information @xmath68 provides about @xmath13 .",
    "it turns out that mutual information is symmetric  @xmath76  hence there is no need to specify the order of @xmath13 and @xmath68 . in the above example ,",
    "if @xmath13 is known then @xmath68 is known , therefore no additional bits are required to store @xmath68 if @xmath13 is known : @xmath77 . since @xmath78 it is indeed the case that @xmath79 first then compress @xmath68 , or if we were to compress @xmath68 first then compress @xmath13 , we end up either way with having compressed optimally the joint sequence generated by @xmath13 and @xmath68 .",
    "mathematically , @xmath80 from which it follows immediately that @xmath76 . ] .",
    "returning to the channel capacity calculation , assume that a sequence generated by @xmath13 is sent through the channel .",
    "the output sequence is itself generated by a random variable , call it @xmath68 .",
    "if the receiver wants to recover @xmath81 it needs at least an extra @xmath82 bits of information ( for otherwise there would be an even more efficient scheme for compressing @xmath13 than the best possible , a contradiction ) .",
    "looking at it from another angle though , this implies that @xmath75 bits of information have somehow been transmitted successfully with each use of the channel ( since with an extra @xmath82 bits of carefully chosen information it is theoretically possible to recover @xmath13 ) . for the case of the binary symmetric channel with error probability @xmath27",
    ", a reasonably straightforward calculation shows that if the input @xmath13 takes the value @xmath25 with probability @xmath83 and the value @xmath26 with probability @xmath84 then the mutual information of the input @xmath13 and the output @xmath68 is @xmath85 where @xmath86 is the number of bits required to compress a binary sequence taking the value @xmath25 with probability @xmath87 and the value @xmath26 with probability @xmath88 .",
    "although we must have the channel input @xmath13 represent the source message @xmath45 in some way , there is otherwise arbitrary freedom in how to choose @xmath13",
    ". why not choose @xmath13 to maximise the mutual information ?",
    "the largest value @xmath89 can take is @xmath25 ( which occurs when @xmath90  ) .",
    "therefore , the largest number of bits we can ever expect to transmit reliably through the binary symmetric channel is @xmath91 bits per usage of channel .",
    "if @xmath41 then @xmath92 in this case , at most every @xmath93 bits of the source message must be expanded to @xmath25 bit ( since the channel transmits 1 bit per usage ) , or in other words , we must have the rate of the code ( see above ) satisfy @xmath94 .",
    "remarkably , shannon proved that this bound is achievable ; whenever the rate is less than the maximum of the mutual information , ( as close as you like to ) error - free communication is possible and @xmath4 goes to infinity , but did not show `` how to achieve it . '' in order to be close to the bound , we generally need a good error correction code and @xmath3 and @xmath4 must be very large . ] .",
    "it is of interest to note that while here we have considered an example where @xmath13 is a discrete random variable , the most well known case of a channel for which the capacity achieving input distribution is known , is the additive gaussian noise channel , with a power constraint on the input . in this case , the capacity achieving input distribution is in fact continuous , i.e. a gaussian distribution . as we discuss later though , it is far more common for the capacity achieving input to be discrete .",
    "we now summarise and precisely define the important information theoretic terms that we have introduced and discussed above without stating their formal definitions .",
    "each of these are defined mathematically as follows .",
    "we already introduced entropy , in eqn .  .",
    "the average conditional entropy requires a double expectation : @xmath95\\bigr ] .",
    "\\label{eq : condentropy_discrete}\\ ] ] as an aid to intuition , consider a single outcome of the random variable @xmath96 the entropy of @xmath68 given @xmath13 can be calculated from eqn .   by calculating the expectation with respect to the conditional distribution of @xmath68 given @xmath97 .",
    "if this is carried out for all possible outcomes of @xmath13 , the result is a function of @xmath98 .",
    "this function can then be averaged with respect to the distribution of @xmath98 , and by definition , the result is the average conditional entropy , @xmath99 .    as mentioned above",
    ", the mutual information can be expressed as @xmath100 . in",
    "what follows below we write mutual information in a different form based on another entity called _ relative entropy _ or _ kullback - leibler divergence_. this is defined as @xmath101 , %   \\label{eq : relentropy_discrete}\\ ] ] where @xmath11 and @xmath102 are two distributions of the same random variable @xmath13 .",
    "note that the relative entropy is positive and is equal to 0 if @xmath11 is identical to @xmath102 .",
    "mutual information is defined as the relative entropy between the joint distribution of @xmath13 and @xmath68 , and the product of the marginal distributions of @xmath13 and @xmath68 : @xmath103 .",
    "\\end{split }    \\label{eq : mi_discrete}\\end{aligned}\\ ] ] it is straightforward using @xmath104 to obtain the above stated relationships between mutual information and entropy .",
    "the definitions as written here hold for both discrete and continuous distributions of @xmath13 and @xmath68 . in this section",
    "we have considered only a simple discrete case , where @xmath13 and @xmath68 are both binary . in general",
    "they can have any number of states , or be continuous , as is the case below .",
    "in full generality , the channel capacity is defined as @xmath105",
    "in this section , some of the challenges of integrating information theory into neuroscience are touched upon . in particular",
    ", we must make assumptions about the way in which information is represented in the brain , whereas in engineering this is specified by the designer .",
    "ultimately it will be necessary to extend the frontiers of information theory if it is to encompass in its entirety the information processing techniques of neuronal networks .",
    "such an expansion would involve in part the greater integration into information theory of systems and control theory from engineering and the theory of computation from mathematics . whereas engineers aim to keep separate communication circuitry from computation circuitry so",
    "as to simplify the design and analysis of engineered systems , there is no reason why nature should maintain such a separation .",
    "evolution tends to find efficient designs and not necessarily `` simple '' designs",
    ".    it would be counter - productive though to assume that information theory in its current form could not be applied usefully in computational neuroscience .",
    "one place it is immediately applicable is the early sensory pathways where information is primarily flowing in one direction .",
    "considerably extra care must be taken when feedback loops are present .",
    "this is especially the case because ( in experiments ) we have control over the input signal itself and hence can investigate how a known signal is communicated from one neuron to another .",
    "the complication though is that it appears the information is being processed at the same time it is being communicated .",
    "the brain heavily compresses the information it receives from its sensory systems .",
    "since the entropy of a signal determines precisely how much ( lossless ) compression is possible , it sets fundamental limits which must be respected by any system , including biological systems .",
    "it is no surprise then that the estimation of entropy of neural signals based on experimental data is an active research area  @xcite .    in the brain ,",
    "neurons communicate with each other and transfer information .",
    "the primary means of communication are the spikes of each neuron  @xcite , and it is parsimonious to model their occurrences as depending randomly on the neuron s input  @xcite .",
    "thus , neurons communicate through a noisy channel , and mutual information should therefore play an important role in understanding the nervous system and brain    in order to consider a neuron as a communication channel , we need to consider what we mean by `` communication '' in the specific context of biological neurons",
    ". there are several important concepts to consider before we can begin to discuss a specific example of the application of information theory in neuroscience .",
    "a definition of communication requires the existence of a physical medium that allows propagation of energy from one place ( an `` energy source '' ) to another place where that energy has some causal effect ( an `` energy sink '' ) .",
    "we also need to define a means by which some property of the source can be altered in a way that results in an observable difference at the sink after propagation through the channel . in communications engineering theory ,",
    "the energy propagation is called `` transmission , '' the source is known as a `` transmitter '' and the sink as a `` receiver . ''",
    "these concepts are not sufficient for communication",
    ". there also needs to be an `` information source '' that is initially observable at the transmitter s location , but not at the receiver s .",
    "communication requires the transmitter to alter the energy source in a manner that reflects the information source , and that can subsequently be observed at the receiver after propagation .",
    "this conversion from information source to energy source is known as `` modulation . ''",
    "a familiar example where each of these concepts is readily identified is analog am or fm radio transmission , in which recorded sound signals are communicated , and then reproduced via a speaker . in this example , the transmission medium can be a vacuum or air , the propagating energy source is electromagnetic radiation , and the transmitter modulates the electromagnetic waves in a manner that reflects the recorded sound signal .",
    "am is amplitude modulation , and means that a single frequency sinusoidal wave of e - m ( electromagnetic ) radiation has its amplitude changed over time .",
    "fm is frequency modulation , which means the amplitude remains constant , while the carrier frequency is changed over time .",
    "modulation of the energy source can be thought of as a code , since it requires a conversion from one kind of information representation to another .",
    "indeed , in neuroscience , modulation has a more general meaning than in communications engineering , and the conversion from an information source to variations in a parameter of the energy source is instead known as a `` code . '' this is largely in contrast with communications engineering , where `` code '' instead refers to conversion between different representations of the information source prior to transmission at the source , for example `` source coding '' and `` error correction coding . ''",
    "if we wish to consider communication between neurons , we need to identify the transmission medium , the form of energy propagation , and a modulation mechanism . from now on we will use neuroscience terminology , and refer to modulation as the `` code . ''",
    "further , we will refer to the information source as the `` input , '' and the observable effect at the receiver that results from the input as the `` output . ''",
    "although over longer time scales the plasticity of neurons can encode / carry information , in shorter time scales the primary physical medium for communication seems to be the axons of neurons , and the energy propagation is a pulse - like wave of voltage that travels along an axon where it may be received by other neurons at synaptic junctions .",
    "these pulses are known as action potentials , or spikes .",
    "typical cortical neurons transmit spikes to many other neurons , and receive spikes from many neurons .    while there are a number of different `` communication channels '' in neuronal circuitry",
    " including segments of the dendritic tree which carry post - synaptic potentials towards the soma of the cell  we choose to focus on action potentials because it is one of the most important communication mechanisms between two neurons .",
    "the other concept we must also attempt to identify is the way in which spikes are coded ( modulated ) in order to communicate information .",
    "two possibilities are the height and the width of each spike .",
    "however , these are observed to be close to identical in most cases , and do not seem to be information carrying parameters .",
    "instead , it is the interval between spikes ( isi : inter - spike interval ) that is thought to play an important role in carrying information through a neuronal channel .    given this , how do isis represent information ?",
    "in neuroscience there are mainly two different ideas .",
    "one idea is that the isi itself ( see for example  @xcite ) carries information .",
    "this is called `` temporal coding '' ( fig .  [ fig : tc ] ) .",
    "the other is that the number of spikes in a fixed time interval ( see  @xcite ) carries information .",
    "this is called `` rate coding '' ( fig .",
    "[ fig : rc ] ) .",
    "so far we have only stated that an input can be communicated to a receiver output .",
    "if this is a perfectly repeatable process , the rate at which information can be transmitted depends on the rate at which the input is updated , and  in line with section  [ sec : info ]  also depends on the probability distribution of the input , via its entropy .",
    "the transmission is usually not perfect , and noise is introduced .",
    "this fact leads us to consider the information theoretic concepts of mutual information and channel capacity .",
    "information theory is not concerned with the type of modulation .",
    "it requires an abstraction that specifies only what the observable output variable should be .",
    "since we are not designing a system , we must make some guesses about aspects of the input and output for a neuronal communication channel , and then proceed to calculations of mutual information .",
    "therefore , in section  [ sec : neuralcapacity ] where we consider the channel capacity of a neuron model , we necessarily begin by specifically defining the input and output of the channel , and state a model for the channel noise .",
    "in this section we present some of our results on the channel capacity for simple neuron models .",
    "one reason for providing this example , is to illustrate that there is no simple single formula for channel capacity , and hence assumptions about the underlying model are very important .",
    "if these assumptions change , the channel capacity also changes .    as we have seen , channel capacity is the maximum amount of information that can be transferred through a noisy channel in a unit time",
    ". it may be much larger than the actual information transmission rate .",
    "this brings us to a natural question , that is , why do we need to know the capacity ?",
    "channel capacity is something similar to the maximum speed indicated in the speedometer of an automobile .",
    "while you will likely never drive with that speed , the maximum speed is useful because it tells you the potential of the automobile , even though you drive with moderate speed .",
    "channel capacity provides not only the upper limit of the possible information transmission rate , but also describes how good the channel is .",
    "although there is much interest in the quantity in neurophysiology  @xcite , theoretical work is rare  @xcite .",
    "we have obtained some interesting results on the capacity from two different viewpoints .",
    "the details will be given below .",
    "we consider here a single spiking neuron , and assume that the input to the neuron controls the expectation of the neuron s output isis . using the terminology introduced above , the information source modulates the isi .",
    "we introduce channel noise to the picture by assuming that the isi is a gamma - distributed random variable , when the input to the neuron remains constant .",
    "biologically , each cortical neuron receives inputs from a lot of ( pre - synaptic ) neurons and each sensory neuron receives physical stimuli .",
    "the above assumption is to model all the inputs to the neuron as a single parameter @xmath87 .",
    "although this assumption may seem too simple , @xmath87 is a time varying function and is able to represent a a lot of possible functions . in the gamma isi model ,",
    "the expectation of the isi is given by @xmath106 .",
    "because @xmath107 is fixed , @xmath87 is the input to the neuron .    due to refractoriness , a neuron can not fire too fast ; therefore the isi can not be 0 but must be larger than a few milliseconds .",
    "on the other hand , if the isi is too large , it means the neuron is not working .",
    "thus we assume the input to the neuron is trying to control the isi in a fixed range of time .",
    "the average isi , which depends on @xmath87 and @xmath107 , is limited between @xmath108 and @xmath109 , that is , @xmath110 thus , @xmath87 is bounded in @xmath111 .      for a noisy channel ,",
    "one important fundamental problem is to compute the capacity @xmath62 .",
    "another problem is to obtain the capacity achieving distribution .",
    "the family of all the possible distributions @xmath112 of inputs @xmath113 is defined as @xmath114 the mutual information and the capacity depends on the choice of an output variable .",
    "this is called `` coding '' in computational neuroscience , but `` modulation '' is an appropriate term in information theory .",
    "traditionally , two types of modulations have been considered in computational neuroscience .",
    "one is `` temporal coding '' and the other is `` rate coding '' ( see fig .  [",
    "fig : tcandrc ] ) . note that both temporal and rate coding may be used in the brain .",
    "for example , binaural sound localisation needs phase information and temporal coding seems natural while rate coding is appropriate for a motor neuron because muscles react according to the rate .",
    "we provide some results on the capacity of temporal and rate coding in the following .      in temporal coding ,",
    "the received information is @xmath115 . for a @xmath116",
    ", we define the marginal distribution as @xmath117.\\end{aligned}\\ ] ] the mutual information between @xmath115 and @xmath118 is defined as @xmath119    \\biggr].\\end{aligned}\\ ] ] the capacity per spike is defined as @xmath120 this optimisation problem can not be solved analytically .",
    "however , it has been proven that the capacity @xmath121 is achieved by a discrete distribution with a finite number of mass points ( see  @xcite for the details ) .",
    "since the optimal distribution is a discrete distribution with a finite number of mass points , the optimisation problem becomes simple , and we can compute the capacity and the capacity achieving distribution numerically .",
    "figure  [ fig : result_t ] shows the capacity achieving probability distribution for @xmath122 .",
    "the channel capacity @xmath123 is 34.68bps ( bit ber second ) ( see  @xcite for further results ) .",
    "figure  [ fig : result_t ] shows that the capacity is achieved when the input is a discrete memoryless distribution . ] with 3 states .",
    "this does not imply that the brain is using discrete states .",
    "it is more plausible that the brain is using continuous states ; it is likely that the actual information transmission rate in the brain is less than the numerically computed capacity .      in rate coding , a time window",
    "is set and the number of spikes in this interval is counted .",
    "let us denote the interval and the rate as @xmath124 and @xmath60 , respectively , and define the distribution of @xmath60 as @xmath125 .",
    "the form of the distribution of @xmath60 is shown in  @xcite . for @xmath116 ,",
    "let us define the following marginal distribution @xmath126.\\end{aligned}\\ ] ] the mutual information of @xmath60 and @xmath87 is defined as @xmath127    \\biggr ] .",
    "%   \\label{eq : mutual_informr}\\end{aligned}\\ ] ] hence , the capacity per channel use or equivalently per @xmath124 is defined as @xmath128 this optimisation problem can not be solved analytically either , but the capacity @xmath129 has been proven to be achievable by a discrete distribution with a finite number of mass points  @xcite .",
    "figure  [ fig : decode_r-1 ] shows the capacity achieving distribution for @xmath122 . the channel capacity @xmath129is 44.95 bits per second ( see  @xcite for further results ) .",
    "the definition of channel capacity requires a maximisation over all possible input probability distributions .",
    "this definition arose in an engineering context , where a system designer is assumed to have control over the inputs to the channel , but not the channel itself . a different optimisation problem results if the input to the channel is assumed to be fixed , but some control over the channel is possible .",
    "this idea is particularly relevant for studies of biological sensory transduction . in this context , an external stimulus that can not be controlled by the sensing organism must be transduced and encoded into action potentials for communication to the brain .",
    "this stimulus can be thought of as an input to a communication channel .    given internal noise in the transduction mechanisms , the encoded stimulus received by the brain is also noisy . since we introduced mutual information in the context of the channel coding theorem and digital data , and here our channel input is a sensory stimulus , mutual information may not seem relevant .",
    "however there are other reasons why it can be useful to ensure mutual information is as large as possible  @xcite and we therefore are interested in how the channel might be altered to maximise mutual information .",
    "but what can be optimised when the stimulus can not be controlled ?",
    "the only other variable that can alter the mutual information is the conditional distribution of the channel output given its input . in the biological context",
    "this is the distribution of a neural response for a given stimulus .",
    "optimising this distribution means seeking to find the communication channel that it best suited to a fixed stimulus distribution . without some constraints on the form of the conditional distribution , this would not be a meaningful task .",
    "one such constraint is to consider a fixed form for the conditional distribution that has some parameters that can be optimised .",
    "clearly the optimal parameter set may change for different input distributions .",
    "one reason for considering such an optimisation might be to assess whether neuronal mechanisms exist for adaptively altering the conditional distribution to match non - stationary stimuli .",
    "another equally intriguing reason is the idea that evolution might have enabled neural systems to change parameters over eons with the end result that those parameters are information theoretically optimal . in this scenario ,",
    "the underlying fixed form of the probability distribution must be what it is due to unavoidable constraints , or perhaps governed by criteria that are not information theoretic , e.g. energy considerations  @xcite .",
    "there are several potentially important sets of parameters that might be chosen .",
    "however , in other contexts there has been much interest in determining the optimal form of neuronal _ tuning curves _ , and this is our sole focus here . experimentally produced tuning curves are plots of the _ mean _ response of a neural system , as a function of a stimulus parameter  @xcite .",
    "classic examples of a stimulus parameter include the angle of a moving bar of light relative to the receptive field of visual cells , or the sound pressure level of a single frequency ( pure tone ) sound played by a speaker . in these examples the average response as a function of the stimulus defines the tuning curve .",
    "the former kind of tuning curve typically has a bell - shape , meaning that there is a stimulus that produces a maximal response , while more than one stimulus can produce the same lesser response .",
    "the latter kind has a sigmoidal shape , meaning that the mean firing rate monotonically increases with stimulus , and here we focus only on this case .",
    "we therefore wish to find the sigmoidal tuning curve that maximises mutual information , for a given fixed form of a conditional distribution . while this is generally a difficult optimisation problem , a simple solution exists for channels where the capacity achieving input distribution is discrete , like those considered in this paper .",
    "in fact , the mutual information maximising tuning curve can be derived for an arbitrary stimulus distribution , if the capacity achieving input distribution has been calculated first .",
    "the reason for this is explained in the following .",
    "we consider the same noisy channel as section  [ subsec : channel capacity of a single neuron ] , that is the isis are governed by a gamma distribution .",
    "we now make a slight generalisation to the setup of sections  [ subsec : inputs and noise of channel ] and [ subsec : channel capacity of a single neuron ] and consider the expectation of the isis to be governed by a random variable @xmath13 with a known distribution , such that @xmath118 is an arbitrary function of @xmath13 .    we therefore write @xmath130 . for the gamma isi channel",
    ", the tuning curve is defined as the conditional expectation of the response variable ( either @xmath115 or @xmath60 ) given a specified outcome of @xmath13 . for timing coding or rate coding",
    "respectively , we write these expectations as @xmath131=\\kappa f(x)\\end{aligned}\\ ] ] and @xmath132=\\frac{\\delta}{\\kappa f(x)}.\\end{aligned}\\ ] ] since @xmath130 , when the capacity achieving distribution for @xmath118 is discrete with say @xmath133 states , the tuning curve will also consist of @xmath133 unique values .",
    "lets call these @xmath134, .. ,@xmath135 . while this discontinuous tuning curve achieves the largest possible mutual information for the channel , it is not unique ; other tuning curves are equally good .",
    "suppose @xmath13 is a continuous random variable , and that the tuning curve maps large intervals of @xmath13 to the same @xmath133 values @xmath136 , @xmath137 , @xmath138 .",
    "this tuning curve provides the same @xmath133 possibilities for the conditional expected isi . in order for it to provide the same mutual information achieved by the original discrete capacity achieving distribution ,",
    "it is necessary that the probability with which each @xmath139 occurs is the same in both cases .",
    "it has been proven for a special case of the gamma distribution and rate coding that this can be achieved by appropriate choice for the ranges of @xmath13 that are mapped to each @xmath139 .",
    "the resulting optimal discrete tuning curve is then dependent only on the probability density function of @xmath13  @xcite .",
    "consequently , the capacity achieving input distributions derived in section  [ subsec : tuning curves ] can be converted to an optimal tuning curve for any choice of the stimulus distribution .",
    "an example of the capacity achieving tuning curve is shown in figure  [ fig : tuning-1 ] , for the special case of @xmath140 , which means the channel is equivalent to a poisson neuron  @xcite .",
    "the maximum rate is restricted to 30 spikes per input sample .",
    "although such a result holds exactly only for discrete input distributions , similar derivations of information theoretically optimal continuous tuning curves have been made , which hold only in the low noise limit  @xcite .",
    "see  @xcite for related work in the high noise limit .",
    "we have shown our results on neuron channel capacity from two very different viewpoints .",
    "interestingly , both show that the capacity is achieved by a discrete distribution .",
    "the numerically computed capacities are similar to the range indicated by some biologically measured results of sensory neurons  @xcite .",
    "the channel capacity depends on various factors , and we consider some of them below .",
    "we first discuss the input and output of the neuron channel in this subsection .",
    "let us start with the input .",
    "although each neuron receives information from many neurons , we have only considered a single input @xmath87 .",
    "this may seem too simple .",
    "we assumed that the single input @xmath87 summarises all the inputs to the neuron .",
    "moreover , @xmath87 has been assumed to be memoryless and can have any distribution within the support .",
    "considering the biological system , this is far from realistic .",
    "the net input of a neuron may not change quickly , that is , it has memory .",
    "moreover , a neuron s input is a collection of many neurons noisy outputs , therefore , it may follow a particular probability distribution .",
    "this implies that we have computed capacity under less restrictive assumptions , and the biologically achievable rate should be smaller than the capacity obtained in the numerical studies .",
    "a better understanding of the constraints on the input of a neuron would lead to a more accurate calculation of neuronal channel capacity .",
    "next , we discuss the output of a neuron from two viewpoints , decoding and demodulation .    in order to achieve channel capacity , the receiver must act as an optimal decoder , meaning that when @xmath141 is observed , the receiver must compute the posterior distribution of the input @xmath13 as @xmath142 when @xmath98 takes discrete values @xmath143 , it becomes @xmath144 this is a real number for each @xmath145 . in engineering , this type of decoding",
    "is called `` soft decoding . ''",
    "it seems unlikely that a neuronal mechanisms for carrying this out could exist , since a computation of the posterior distribution is necessary .",
    "another standard decoding technique is `` hard decoding , '' that is , only a single value of @xmath145 is chosen by the decoder .",
    "the optimal hard decoder chooses the one which maximises the posterior distribution , that is @xmath146 .",
    "it is possible to implement the optimal hard decoder without requiring the online computation of the posterior distribution .",
    "indeed , the output space ( the space where @xmath141 lies ) can be divided into @xmath147 subspaces in advance , such that the @xmath148 th subspace contains all the points @xmath141 such that @xmath12 has the largest posterior probability given @xmath141 .",
    "therefore , the optimal hard decoder can be implemented by a `` quantisation '' or `` thresholding '' algorithm which simply checks to see which subspace @xmath141 lies in .",
    "such an algorithm is often computationally simpler than computing first the posterior distribution .",
    "when we consider information processing in the brain , a naive soft decoding seems difficult , at least for a single neuron .",
    "the hard decoding idea seems more natural .",
    "however , the information transmission rate of the best hard decoding is less than the best soft decoding .",
    "we should keep this point in mind .",
    "further discussion is found in  @xcite .      under the assumptions made",
    ", we have shown that the capacity of a neuron is achieved by a discrete distribution with a finite number of probability mass points . in information theory , there are many other known types of channels for which channel capacity is achieved by a discrete distribution  @xcite .",
    "for example , although the capacity of an average power constrained additive gaussian white noise channels is achieved by continuously valued gaussian inputs , simply placing a constraint on the maximum amplitude of the channel means capacity is achieved by a discrete distribution  @xcite .",
    "our results do not imply that `` neuron signals are only using discrete levels . '' on the contrary , we believe many neurons are using continuous levels .",
    "what is implied by our results is that those neurons can not achieve the capacity and the actual rate of information transfer is therefore less than the capacity .",
    "another implication is for the measurement of information capacity in neuroscience  @xcite .",
    "our result implies that only a small number of discrete ranges are sufficient for the input distribution to measure the information capacity of neural coding .",
    "information theory provides a way of quantifying whether discrete or continuously distributed signals are better for any given communication channel corrupted by random fluctuations .",
    "many neuroscience studies quantify information using shannon s famous information _ capacity _ formula relating mutual information to signal - to - noise ratio . this formula is correct only for gaussian additive noise channels  @xcite  it relies on many assumptions , and if any are false it can significantly under- or over - estimate the true capacity  @xcite .",
    "guessing the channel model is not straightforward . in particular , feedback is prevalent in many parts of the brain , which makes it much more difficult to relate changes in responses to inputs .",
    "one important example where it is known that the circuitry is solely feedforward is that of retinal cells  @xcite .",
    "this example has been used to demonstrate that it is possible to rule out certain guesses of neural codes . because this is based on optimal bayesian decoding of a forced binary choice",
    ", it would be interesting to extend  @xcite beyond the binary limitation to that where the signals being coded may have many possibilities    what can we say about more complicated situations ?",
    "for example , information processing of cortical neurons are not strictly feedforward and the information is shared by many neurons . if we assume every neuron is performing the same computation , and each neuron encodes and decodes information in the same way , it seems possible to extend our results .",
    "however , when different neurons encode information differently , the problem becomes very different .",
    "understanding how the brain works in information theoretic terms is one of the grand challenges of this century .",
    "most information theory research to date has been predicated on an engineering viewpoint .",
    "the main thrust has been to _ design _ compression / decompression methods that compress source information to the limits given by its entropy ( source coding ) , or to _ design _ error correction schemes and encoding / decoding methods that allow communication close to capacity .",
    "on the other hand , the goal of neuroscience is instead to _ understand _ information processing in the brain .",
    "this does not mean that information theory has no place in computational neuroscience .",
    "information theory provides a way to _ measure _ information and to understand the _ limits _ of compression ( namely , entropy ) and communication ( namely , mutual information and channel capacity ) .",
    "a common `` information theoretic '' method in computational neuroscience is to obtain quantitative estimates of the mutual information between observed sets of data . since various methods for the ( difficult ) problem of accurately estimating mutual information exist ,",
    "the bigger difficulty is with using the result to say something about how the system works .",
    "indeed , the actual goal of computational neuroscience is that of `` system identification , '' as engineers would call it .    if the brain uses spikes to transmit information ( which on faster time - scales appears to be the case ) then understanding the neural code  how the brain encodes information before sending it across the `` channel ''  is tantamount to understanding how the brain works . indeed ,",
    "if we would listen in to the messages as they are sent from one neuron to another , it would be relatively straightforward to determine what each neuron is doing .",
    "although system identification is not the forte of traditional information theory  @xcite , this is merely for historical reasons . with computational neuroscience as a main motivator",
    ", we predict that the next decade will see the expansion of information theory to include more powerful techniques for system identification , and possibly even an integration of control , computation and information theory into a unified framework .",
    "some recent information theoretic approaches in neuroscience that go beyond standard shannon theory are summarized in the following list .    * while it is traditional in engineering to separately code an information source , and then _ channel code _ it for communication across a channel ,",
    "it has been shown for some simple but instructive examples , that non - separation of these two aspects can achieve an optimal communication system with a vastly reduced complexity compared to separation  @xcite .",
    "this fact is potentially important for neurobiological systems , where separation mechanisms seem implausible . *",
    "many studies have investigated whether the brain might have mechanisms for implementing bayesian algorithms during decision making , prediction and pattern recognition  @xcite . *",
    "the possibility of analog cortical error correcting codes has been proposed by  @xcite .",
    "* one limitation of shannon theory is that its measures say nothing about directionality or causality .",
    "however , directed information theory has also been developed  @xcite and has recently been applied in neuroscience  @xcite . *",
    "the relationship between control , information theory and thermodynamics has been discussed by  @xcite .",
    "summarising our own modest contribution , we carefully came up with a simple neuron channel model based on biological evidence and computed the channel capacity of this model .",
    "interestingly , it was proved that the channel capacity is achieved by a discrete distribution .",
    "mark d. mcdonnell s contribution was supported by the australian research council ( arc ) under arc grants dp1093425  ( including an australian research fellowship ) , rn0459498 and dp0770747 .",
    "jonathan h. manton wishes to acknowledge several interesting and thought - provoking conversations with professor iven mareels which facilitated the writing of parts of this paper .",
    "jm s contribution was supported in part by the australian research council .",
    "etienne burdet , rieko osu , david  w. franklin , theodore  e. milner , and mitsuo kawato .",
    "the central nervous system stabilizes unstable dynamics by learning optimal impedance .",
    "_ nature _ ,",
    "414:0 446449 , november 2001 .",
    "stefano panzeri , riccardo senatore , marcelo  a. montemurro , and rasmus  s. petersen . correcting for the sampling bias problem in spike train information measures .",
    "_ j. neurophysiol _ , 98:0 10641072 , 2007 .",
    "a.  l. jacobs , g.  fridman , r.  m. douglas , n.  m. alam , p.  e. latham , g.  t. prusky , and s.  nirenberg .",
    "ruling out and ruling in neural codes . _",
    "proceedings of the national academy of sciences of the usa _ , early edition:0 16 , 2009 ."
  ],
  "abstract_text": [
    "<S> this paper introduces several fundamental concepts in information theory from the perspective of their origins in engineering . </S>",
    "<S> understanding such concepts is important in neuroscience for two reasons . </S>",
    "<S> simply applying formulae from information theory without understanding the assumptions behind their definitions can lead to erroneous results and conclusions . </S>",
    "<S> furthermore , this century will see a convergence of information theory and neuroscience ; information theory will expand its foundations to incorporate more comprehensively biological processes thereby helping reveal how neuronal networks achieve their remarkable information processing abilities . </S>"
  ]
}