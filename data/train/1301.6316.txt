{
  "article_text": [
    "in order to understand complex data , hierarchical feature extraction strategy has been used [ 1 ] .",
    "one best known algorithm is deep belief network ( dbn ) introduced in 2006 [ 2 ] . with the success of training deep architectures ,",
    "several variants of deep learning have been introduced [ 3 ] .",
    "although these multi - layered algorithms take hierarchical approaches in feature extraction and provide efficient solution to complex problems , they do not provide us the relationships of features in form of hierarchies that are learned throughout the hierarchical structure . in this paper",
    ", we propose a hierarchical data representation model , hierarchical multi - layer non - negative matrix factorization .",
    "( similar approach has been introduced in [ 4 ] . )",
    "we extend a variant of nmf algorithm [ 5 ] , nsnmf [ 6 ] into several layers for hierarchical learning . here",
    ", we demonstrate intuitive feature hierarchies present in the data set by learning relationships between features across layers .",
    "we also prove that instead of one step learning , hierarchical approach learns more meaningful and helpful features , which leads to better distributed representations , and results in better performance in classification and reconstruction for small number of features , which guarantees reduced loss of performance , even when representing data in small dimensions .",
    "proposed network is constructed by stacking nsnmf [ 6 ] into several layers .",
    "non - smooth non - negative matrix factorization ( nsnmf ) is a variant of nmf that restricts sparsity constraint .",
    "basic nmf decomposes non - negative input data * x * into non - negative * w * and * h * , which are features and corresponding coefficients or data representation respectively .",
    "it aims to reduce error between original data * x * and its reconstruction * wh * : @xmath0    to apply sparsity constraint to standard nmf , a sparsity matrix * s * is introduced in [ 6 ] : @xmath1 @xmath2 is number of features , and @xmath3 is parameter for smoothing effect , in range of 0 to 1 . *",
    "i*(k ) is identity matrix of size k x k , and * ones*(k ) is a matrix of size k x k with all components of 1s .",
    "we smooth a matrix by multiplying it with * s*. the closer @xmath3 is to 1 , more smoothing effect is applied . during alternative update ,",
    "we smooth * h * matrix by multiplying * s * and * h * during iterations as * h = sh*. to compensate the loss of sparsity , * w * becomes sparse .",
    "the proposed hierarchical multi - layer nmf structure comprise of several layers of unit algorithm .",
    "we first train each layer separately . we process outcome of each layer",
    "@xmath4 to get @xmath5 .",
    "@xmath6 , where @xmath7 , f(@xmath8 ) is nonlinear function , and @xmath9 denotes index of layer , @xmath10 .",
    "the superscript of each term denotes layer index .",
    "processed data representation of @xmath5 is used as input to next layer . using nsnmf ,",
    "@xmath5 is decomposed into @xmath11 and @xmath12:@xmath13 .",
    "then , we use outcome of separate training as initialization , and train the whole network jointly . the cost function for joint training",
    "is described : @xmath14 , where @xmath15 is the reconstruction of @xmath16 , which can be computed via back propagation of errors from the last layer to the @xmath17 layer : @xmath18 , ... , @xmath19 , where @xmath20 .",
    "@xmath21 is inverse nonlinear function .",
    "( more details on the actual update computation is described in appendix [ aa ] ) .",
    "after training until the last layer , final data representation @xmath22 is acquired .",
    "this is the activation information of complex features , which is the integration of features throughout the layers , @xmath23 .",
    "for more detailed explanation , refer to the pseudo - code for the training procedure in appendix [ pseudo - code ] .",
    "we applied our proposed network to document database .",
    "reuters-21578 collection , distribution 1.0  as database .",
    "we sorted top 10 categories from modapte split , conducted pre - processing of removing stop - words , and reduced dimension to 1000 .",
    "there are 5786 and 2587 document samples for training data , and test data .",
    "we constructed two - layered network with number of hidden neurons as 160 .",
    "we observed how concepts form hierarchies in document data in figure [ fig:8 ] ( a ) .",
    "first , second , and third @xmath24 features contain words related to oil production ( exploration , pipeline , production , industry ) , oil contract ( contract , purchase , barrel , prices ) , and oil refinery processing ( refinery , reserves , pipeline , petroleum ) , respectively .",
    "these sub - class topic features are combined together and develop into one broader topic oil. with this combination relationship of features , we can figure out that those three seemingly independent features can be re - categorized under the same broader topic .",
    "( the concept hierarchy learned in reuters : sub - categories of oil production , contract , and refinery processing exist under oil category . )",
    "furthermore we analyzed reconstruction and classification performance as shown in figure [ fig:99 ] ( a ) . the proposed hierarchical feature extraction method results in much better classification and reconstruction , especially for small number of features , compared to extracting features at one step .",
    "this proves the efficiency and effectiveness of our proposed approach in learning of features .",
    "we also applied our network to handwritten digit image mnist .",
    "the final data representation @xmath25 displayed distinct activation patterns for samples of the different classes , as a result of successful learning of feature hierarchy , which determines the combination of low level features in forming of distinct class features . in figure [ fig:99 ] ( b ) , the reconstruction error and classification performance also demonstrate better performance of our proposed method in small number of dimensions . in figure [ fig:10 ] ( a )",
    ", we can observe sparser and clear reconstruction of our proposed network .",
    "the fisher discriminant values of final data representation of the shallow network and our proposed network were 0.51 and 0.61 respectively .",
    "we can infer that proposed network learns more meaningful and helpful features so that it results in better distributed ( clustered ) representation of data .",
    "we can also check this via the visualization of @xmath22 to 2-d domain shown in figure [ fig:10 ] ( b ) .",
    "in this paper , we proposed a hierarchical data representation model , hierarchical multi - layer nmf by stacking nsnmf into several layers .",
    "we demonstrated hierarchical approach in learning of the features .",
    "there are mainly two findings of our research .",
    "taking hierarchical learning by stacking nmfs : 1.reveals intuitive feature hierarchies ( subcategories ) by learning feature relationships throughout the layers , and 2.learns more meaningful features compared to one - step learning .",
    "( as a result , our proposed method results in much better classification and reconstruction performance , provided small number of dimensions for data representation . )",
    "we expect our proposed method to be applied to various types of data for discovering underlying feature hierarchies and at the same time , maintain reconstruction and classification performance even with small number of features for data representation .",
    "continued from section [ multi - layer architecture ] , the actual computation is done as described in  .",
    "[ eq : update1 ] @xmath26    here , @xmath27 .",
    "@xmath15 is the reconstruction of @xmath16 , which can be computed via back propagation of errors from the last layer to the @xmath17 layer as shown in  .",
    "@xmath28 @xmath29 is a matrix of column - wise mean of @xmath4 , and @xmath21 is inverse nonlinear function .",
    "% % separate training of layers in extending mode randomly initialize @xmath30 and @xmath4 @xmath31 @xmath32 @xmath33 @xmath34 @xmath6 % % joint training the whole network use @xmath30 and @xmath4 , and use @xmath29 acquired from above @xmath35 @xmath36 which can be written in full length as : @xmath37 @xmath38 @xmath39 which can be written in full length as : @xmath40 @xmath41 @xmath42 @xmath43 @xmath44 @xmath45"
  ],
  "abstract_text": [
    "<S> in this paper , we propose a data representation model that demonstrates hierarchical feature learning using nsnmf . </S>",
    "<S> we extend unit algorithm into several layers . experiments with </S>",
    "<S> document and image data successfully discovered feature hierarchies . </S>",
    "<S> we also prove that proposed method results in much better classification and reconstruction performance , especially for small number of features . </S>"
  ]
}