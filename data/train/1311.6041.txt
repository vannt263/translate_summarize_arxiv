{
  "article_text": [
    "one of the most important stages in many areas of engineering and applied sciences is modeling and the use of optimization techniques to increase the quality and performance of products or processes . generally in literature",
    "the term _ optimization _ is related to ( the output of ) a mathematical technique or algorithm used to identify the extreme value of an arbitrary objective function ( _ fitness _ ) through the manipulation of a known set of variables and subject to a set of constrains . more technically , a maximization problem with an explicit objective can in general be expressed in the following mathematical form : finding the value    @xmath0    where @xmath1 is a give vector in a generic multidimensional space @xmath2 and @xmath3 is a function of the vector @xmath1 and @xmath4is a ( discrete or continuous , but here the focus will be on continuous ) subset of the multidimensional real euclidean space . from now on we will refer to @xmath2 as the _ _ search space _ _ ] .",
    "in most real - world engineering optimization problems , no analytical expression exists for accurately evaluating the response of a candidate solution .",
    "sometimes the fitness consists just in the possibility to observe different sets of pairs of input and output from a computational simulation or an experiment .",
    "this is the * _ black - box scenario _ * that will be considered here .",
    "further , in what follows we will refer to as * _ hard optimization problems _ * those with a fitness that manifest a combination , to a varying degree , of the following elements :    * * high number of independent input variables .",
    "* the large number of candidate solutions to an optimization problem makes it computationally very hard to be attacked by evolutionary algorithms because the number of candidate solutions grows exponentially with increasing dimensionality .",
    "this fact , which is frequently named _ the curse of dimensionality _ , is well known by practitioners that have to handle problems with hundreds of variables @xcite .",
    "* * difficult fitness landscape . * in landscape surface with weak ( low ) causality , small changes in the candidate solutions often lead to large changes in the objective values , i.e. ruggedness @xcite .",
    "a fitness landscape is rugged if there are many local optima concentrated in any constrained region of the space . at the opposite side , even _",
    "neutral _ landscape where the optimum is a narrow peak in between regions with similar fitness values can be very hard to optimize since no guiding information can be extracted from neutral areas .",
    "* * computational expensive fitness evaluation .",
    "* simulation can be very time consuming in the order of many minutes or even hours for each single fitness call .    in hard black - box optimization",
    "many issues are at stake .",
    "some good reviews are @xcite . in fig 1 are presented some practical hints derived from literature .",
    "it s worth noting that increasing sample size as countermeasure for difficult landscapes compete with the need of reducing the number of fitness calls in computational expensive problems .",
    "further , a number of engineering design tasks as well in other context are modeled as multi - objective problems and this makes the optimization process even harder but this case will not be considered here . here",
    "the focus will be mainly on the following issues : the practical meaning of the * _ no free lunch theorem _ * ( nflt ) and on its natural connection with the bayesian framework .",
    "this choice is due to the fact that they both represent a critical link between optimization theory and optimization practice .",
    "black - box optimization ( easy or hard ) is the reign of metaheuristics .",
    "a metaheuristic describes the way an optimization method decides which part of the search space to explore in the next step . in general every optimization algorithm",
    "can be abstractly considered as a sampling process acting on the search space .",
    "it starts with a set of values , and then step after step it generates new samples according to some specified mechanism based on current samples and the fitness values :    @xmath5    where @xmath6 is a random variable and index @xmath7 is the iteration counter .",
    "so differences among different optimization methods come from the specific mechanism an algorithm uses to generate and accept new samples and in so doing alternating an exploration ( global ) phase with a exploitation ( local ) phase .",
    "for example just to mention one class , _ nature - inspired _ algorithms , like the well known genetic algorithm @xcite , are based on the idea of mimicking some natural phenomena that leads to the maximization of some defined quantity .",
    "they can be population - based or not .",
    "other examples are : particle swarm , ant colonies , simulated annealing and many other families @xcite .",
    "however , practice shows that any successful application depends on careful tuning of operators , parameters , and problem - dependent features .",
    "potentially there are an infinity of possible optimization problems , one for any possible fitness function . at the same time",
    "there are an infinity of thinkable optimization methods , one for every possible exploration - exploitation trade - off combination .",
    "the choice of the correct metaheuristics for a given class of problem is a crucial theme that leads to take into account the role played by the nflt in terms of its theoretical and practical relevance .",
    "we will start with the usual formal statement of the nflt for optimization @xcite .",
    "wolpert and macready s result considered a finite search space @xmath8 and the space of all the possible objective functions @xmath9 defined on it called @xmath10 .",
    "they defined with @xmath11 the conditional probability of finding a value @xmath12 given a function @xmath13 , after @xmath14 iteration with algorithm @xmath15 .",
    "this can be seen a performance measure of the algorithm , its ability to locate a given function value after a given amount of iterations . under some quite general conditions the theorem states that",
    ", for any pair of algorithms @xmath16 and @xmath17 :    @xmath18    where the sum is carried out over the set of all the possible function @xmath19 .    according to the most common understanding the nflt implies that there is no optimization method superior to others for all possible optimization problems . for some function",
    "@xmath16 will be able to locate the maximum faster than @xmath17 ; for some other function it will be the opposite .",
    "averaging over the whole space @xmath19 the performance will be the same .",
    "equivalently it is possible to say that over @xmath19 no algorithm will perform better than pure random search .",
    "wolpert and macready adopted a probabilistic framework .",
    "their result holds if we assume a uniform distribution over @xmath19 , i.e. any functional form is uniformly admissible . to understand the practical implication of this theorem for black - box optimization problem let s re - state it adopting a different perspective .",
    "substantially the theorem states that    * nflt - _",
    "no induction _ form * _ with no prior knowledge about the function @xmath20 , in a situation where any functional form is uniformly admissible , the information provided by the value of the function in some points of the domain will not say anything about the value of the function in other regions of the its domain .",
    "_    this interpretation of the nflt is pictured in fig .",
    "so in this scenario the information collected with the data sample is not helpful in guiding the search in which direction is better to explore next . in this sense , averaging over the set of all the possible functions , every algorithm performs the same .",
    "of course every function has its own structure , the problem if is when prior knowledge about the functional form is not available because no rationale can guide an optimization strategy i.e. to decide which optimization method to use , which set of parameters , and so on .",
    "the lesson that has to be learned from nflt is in the implications for a rational optimization strategy able to tackle hard optimization problems .",
    "it is clear now that for the practitioner the correct question is not _ which algorithm i have to use _ but _ what is the geometry of the problem fitness _ : the optimization problem becomes an inferential problem as it will be clear in the discussion below .",
    "knowing the structure of the fitness landscape makes ( theoretically ) possible to properly tune an algorithm in terms of a trade - off between local search and global search .",
    "it is also true that general results about which class of algorithms is better suited to which kind of problems class is still far to come even if many studies are going on . as we will see below ,",
    "bayesian probability theory will appear to be the natural foundational framework for metaheuristics .    in the nflt scenario , where nothing is known in literature about the structure of the problem at hand ,",
    "practitioners tend to decide the optimization method according their background knowledge , practical availability of code , simulation software and so on . in a different scenario where something is known about the function , like a lower bound on the function value or some information about the response landscape",
    ", this information must be used to tailor the algorithm and the optimization strategy accordingly .",
    "knowledge of the objective function structure is a key to adjust effective algorithms in terms of a better trade - off between exploration and exploitation .",
    "summarizing , from the discussion of the nflt , black - box optimization involves two main ingredients : input - output data and some prior knowledge .",
    "this leads naturally to the next section where the connection with the bayesian framework will be explored .",
    "our interpretation of the nflt shows strong connection with the problem of _ underdetermination _ ( fig .",
    "3 ) . in the black - box context",
    "only a finite sample data set concerning the functional response is available : @xmath21.in this case locating the optimum becomes a pure inferential problem that leads naturally to a bayesian framework .",
    "the rationale of bayesian probability theory is briefly summarized here in the context of optimization .",
    "the problem is to construct a model of the function we need to optimize . in the bayesian framework",
    "there are two ingredients : a data set and a prior .",
    "a prior distribution @xmath22 over the space of functions is combined with the likelihood @xmath23 to generate a posterior : @xmath24    that takes into account the information given by the data set @xmath25 . in term of modeling a function ,",
    "if we assume as a prior every possible function ( that is equivalent to say that there is no prior ) we are exactly in the nflt case : no useful inferential information can be extracted from the data set .",
    "if the prior can be restricted to a proper class of function the inference about the model realizing the data set will be more accurate . as said about the implications of the nflt ,",
    "if it is possible to restrict the problem to a given sub - class of function , a proper optimization algorithm choice can produce good results .",
    "* for an optimization method , given @xmath25 , to have indications about where promising areas for the optimum can be located is equivalent to say that a ( posterior ) refined model of the fitness has been inferred in bayesian terms*.      +   +   + & + & non - uniform over @xmath19 & with no prior & + no @xmath15 better than & some @xmath15 better than & no valid inference & inference can be + pure random search & pure random search & for the model & made about the model + in locating @xmath26&in locating @xmath26 & realizing @xmath25 & realizing @xmath25 +    as it is now clear , _ there is a strong connection between the nflt and the bayesian framework _ at the point that they can be considered two sides of the same coin .",
    "this is summarized in the table 1 .",
    "the nflt assumes a uniform distribution over the space of possible functions . in the bayesian terminology",
    "this is equivalent to say that there is no prior knowledge about the fitness : every functional form is admitted . according to the implication of the nflt",
    ", it is essential to restrict the class of possible function ( in the nflt terminology ) to a proper subclass in order to tailor an effective optimization strategy .",
    "this restricted class of function represents the prior that appears in the bayesian formula .",
    "so the secret of a successful black - box optimization , assuming a _ representative _ sample @xmath25 , lies in the possibility of narrowing the prior .",
    "the discussion above about the interpretation of nflt in terms of the bayesian approach ( and vice - versa ) can be useful in two ways : to use the nflt for understanding surrogate - based optimization techniques and , on the other side , to use a bayesian framework for understanding the metaheuristics working logic . in the context of hard optimization",
    "there is a large literature about methods for fitness approximation ( also called _ meta - models _ or _ surrogates _ ) as ways to generate functional models that are computationally more efficient .",
    "all of them _ assume _ , seldom implicitly , a prior in terms of a restricted class of modeling function .",
    "functional surrogate models can be algebraic representations of the true problem functions .",
    "the most popular ones are polynomials , in a method often known in the statistical literature as _ response surface methodology_. several related methods are now commonly used for fitness approximation : the kriging model , radial - basis - function networks and the support vector machines ( for a general reference , see @xcite ) .",
    "after the model has been developed some classical derivative - based or derivative - free methods can be applied @xcite . _",
    "bayesian optimization _ is briefly discussed here as a tool that is getting relevance in optimization practice .",
    "this technique conceptually combines surrogate estimation with optimum localization ( for a good introduction see @xcite ) .",
    "it uses the bayesian rationale to infer about a starting fitness model .",
    "the second step is to infill new points of the search space in those regions where a maximum improvement is expected according to the current best value and the predictive variance ( fig .",
    "3 ) . then the bayesian procedure is updated . usually in literature",
    "the prior is supposed to be a realization of a * gaussian process * that means a distribution over the space of function modeled as a multivariate gaussian distribution , so one can think the gaussian process as defining a distribution over the space of functions :    @xmath27    meaning that the function @xmath13 is distributed as a gp with mean function @xmath28 and covariance function @xmath29 .",
    "a common choice for the covariance function is defined by :    @xmath30    where @xmath31 represents a family of hyper - parameters that have to be properly estimated .",
    "represent the next point to explore since maximum improvement is expected.,scaledwidth=47.0% ]    the power of bayesian optimization philosophy briefly sketch out above lies in its ability to tackle optimization problems with a limited number of fitness calls and this is very important for computational expensive simulations . again",
    ", the key point in the bayesian rationale is given by the choice of the prior , i.e the non uniform distribution over over @xmath19 in the nflt terminology .",
    "the choice of the prior as a _ gaussian process",
    "_ implies that the fitness is supposed to be in the class of smooth well - behaving functions and this can be an arbitrary hypothesis . for objective functions with discontinuities or jumps this prior",
    "will not be able to provide very good results .",
    "further , the construction of the surrogate model is essentially an exercise of design space exploration exactly in the same way any optimization algorithm result in essence is .",
    "if the dimension of the search space is high ( more than 100 variables ) the ability to sample promising regions becomes weaker and weaker : bayesian optimization can not avoid the curse of dimensionality .    from the discussion above it is now clear how a bayesian framework can be used to shed light on the main metaheuristics operating principles .",
    "all metaheuristics _ assume _ a model class ( the bayesian prior ) of the objective function during the search and combine it with the sampled points at a given iteration to direct the search in the most promising directions .",
    "for example _ genetic algorithms _ , as well as other optimizers , assume ( and they work well if there is ) _ strong causality _ : small causes produce small effects . for function with low causality",
    "they tend to suffer @xcite . in metaheuristics",
    "the prior about the fitness is implicitly define by the search operators used ( _ crossover _ , _ mutation _ , etc . ) and the set of internal working parameters .",
    "many automated self - adapting strategies able to change the internal parameters values according to partial results obtained during the search process have been studied in recent years @xcite .",
    "what these ( so called _ smart _ ) optimization algorithms do is to incorporate a more explicit bayesian operating logic .",
    "they generate a posterior model using the sampled points collected at a given iteration and some assumptions about the fitness structure ( prior ) .",
    "iteration after iteration , they adjust the set of internal parameters to better tailor the fitness model and to foster the search in more promising regions ( according to the _ posterior _ model ) .    just for mentioning one ,",
    "covariance matrix adaptation evolutionary strategies ( cma - es ) is one of the most famous in the field of continuous global optimization @xcite .",
    "again , the main idea of this algorithm is to use the information collected during the iterations in terms of sampled points to generate _ on - the - fly _ a model of the function to optimize by assuming a local second - order functional structure .",
    "this again shows the strong relationship between optimum location , model determination and the prior knowledge about the fitness geometry over which the optimization process is supposed to be carried out .",
    "the internal fitness prior of a metaheuristic defines and constrains the exploration - exploitation trade - off an so the overall capabilities of the search process .",
    "summarizing , the ability of every optimization method is connected to the following points : a ) how much the adopted prior model fits the geometry of the problem at hand and b ) on the other side , the possibility to access to a _ good _ sample @xmath21 of @xmath32 .",
    "this can be a problem for computational expensive functions and/or for high - dimensional search spaces . as we have said .",
    "the curse of dimensionality affects the ability to generate a valid posterior since with increasing dimensions the needed number of sampled points able to say something usefully in the inferential process increases exponentially .",
    "summarizing the discussion above :      * in general , knowledge about the fitness landscape is the key for a better optimization in terms of the possibility to better tune the trade - off between local search ( exploration ) and global search ( exploitation ) .    *",
    "the nflt and bayesian theory can be seen as two faces of the same coin . at an abstract level ,",
    "the bayesian approach is the natural framework on which to base an effective black - box optimization strategy .        *",
    "the curse of dimensionality affects the ability to search of every optimization method : in high - dimensional search spaces the number of sampled points needed for the inference process increases dramatically .",
    "s. shan , g. gary wang , survey of modeling and optimization strategies to solve high - dimensional design problems with computationally - expensive black - box functions structural and multidisciplinary optimization , volume 41 , issue 2 , pp 219 - 241 .",
    "march 2010 .",
    "e. brochu , v. m. cora , n. de freitas : a tutorial on bayesian optimization of expensive cost functions , with application to active user modeling and hierarchical reinforcement learning .",
    "corr abs/1012.2599 , 2010 ."
  ],
  "abstract_text": [
    "<S> challenging optimization problems , which elude acceptable solution via conventional calculus methods , arise commonly in different areas of industrial design and practice . </S>",
    "<S> hard optimization problems are those who manifest the following behavior : a ) high number of independent input variables ; b ) very complex or irregular multi - modal fitness ; c ) computational expensive fitness evaluation </S>",
    "<S> . this paper will focus on some theoretical issues that have strong implications for practice . </S>",
    "<S> i will stress how an interpretation of the no free lunch theorem leads naturally to a general bayesian optimization framework . </S>",
    "<S> the choice of a prior over the space of functions is a critical and inevitable step in every black - box optimization . </S>"
  ]
}