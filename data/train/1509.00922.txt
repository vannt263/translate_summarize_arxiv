{
  "article_text": [
    "in certain applications , there is no natural likelihood function that describes the relationship between the observable data and unknown parameters . instead , this relationship is defined via an optimization problem , i.e. , the true parameter is defined as the minimizer of a suitable risk ( expected loss ) function . for example , in machine learning applications , rather than making model assumptions , it is more common to define the true parameter as the minimizer a sort of mean squared error , perhaps subject to a penalty for complexity . in these cases where no likelihood is available ,",
    "the standard bayesian methodology can not be used to construct a posterior distribution for the parameter . however",
    ", an empirical version of the risk function can be used as a substitute for the negative log - likelihood , and the posterior distribution obtained by following bayes s rule is called the _",
    "gibbs posterior_. @xcite recently showed that the gibbs posterior is the appropriate generalization of the standard bayesian posterior , in terms of coherent updating of information , in the absence of a likelihood .",
    "theoretical and computational work on gibbs posteriors can be found in @xcite , @xcite , @xcite , and @xcite , and @xcite use a gibbs posterior in a medical statistics application .",
    "one of the most natural applications of the gibbs posterior is in quantile regression ( e.g. , * ? ? ?",
    "* ) where , typically , no distributional assumptions are made , only that the quantiles of the response variable are written as parameter - dependent functions of a set of predictor variables .",
    "the gibbs posterior based on the so - called `` check loss '' or , equivalently , a bayesian posterior based on a misspecified asymmetric laplace distribution likelihood , has shown to have good theoretical and empirical performance ; see , e.g. , @xcite and @xcite .",
    "the challenge in implementation of the gibbs posterior is that it depends on an unspecified scale ( or inverse temperature ) parameter .",
    "the role played by this parameter is to weight the information in the data relative to that in prior and , consequently , the finite - sample performance of the gibbs posterior is highly sensitive to the choice of this scale parameter .",
    "a number of ideas for choosing this scale parameter have been presented in the literature .",
    "for example , @xcite proposes using cross - validation and @xcite propose a hierarchical bayes approach and an expected loss matching approach .",
    "these approaches are reasonable , but an arguably more natural idea would be to select a particular feature one would like the gibbs posterior to have , and then choose the scale parameter so that this feature is attained . since a goal of using a bayesian approach is to have easy access to interval estimates , we propose to choose the scale parameter in such a way that the gibbs posterior credible region is calibrated in a frequentist sense .",
    "our jumping off point is the observation that the scale parameter controls the spread of the gibbs posterior , so there ought to be a degree of spread that is most appropriate for calibrating the credible regions .",
    "we formulate this value of the scale parameter as the solution of a particular equation that involves the underlying data - generating distribution and we propose a gibbs posterior scaling ( gps ) algorithm for solving this equation numerically , combining various monte carlo , bootstrap , and stochastic approximation techniques .",
    "properties of the scale parameter selected by the gps algorithm are explored both numerically and theoretically .",
    "the key point is that the gps algorithm aims to choose the scale to calibrate the gibbs posterior credible regions , so one can be sure that the coverage probability of the corresponding credible region will be at the nominal level , at least approximately . as an application",
    ", we consider the quantile regression problem where , despite being able to construct likelihood - free bayesian methods , calibrating the credible intervals remains an important open problem .",
    "our simulation study shows that the gibbs posterior scaled by the gps algorithm gives credible intervals which are exactly calibrated and more efficient  returns intervals with shorter average length  than existing bayesian - like methods .",
    "the remainder of the paper is organized as follows .",
    "section  2 sets our notation , defines the gibbs posterior , and states our general convergence rate result .",
    "we also give an illustrative example that make two important points : first , the gibbs posterior avoids the dangers faced by using naively misspecified bayesian posteriors ; second , even though the gibbs posterior is safe , it can be inefficient if the scale is not properly chosen .",
    "the gps algorithm is presented in section  3 along with some properties of the scale it selects .",
    "section  4 contains the quantile regression application , and section  5 makes some concluding remarks .",
    "technical proofs are deferred to the appendix .",
    "suppose we have data @xmath0 iid from a common marginal distribution @xmath1 on @xmath2 ; here each @xmath3 could be a vector or a response  predictor variable pair .",
    "the iid context is only for simplicity , situations more general than iid can be handled similarly .",
    "let @xmath4 be the empirical measure and use de  finetti s notion for expectation : in particular , @xmath5 and @xmath6 .",
    "in addition , suppose we are given a loss function @xmath7 , indexed by a ( possibly infinite - dimensional ) parameter @xmath8 ; squared error @xmath9 is perhaps the simplest example .",
    "define the risk @xmath10 and the empirical risk @xmath11 .",
    "the quantity of interest is the minimizer @xmath12 of the risk @xmath13 so , of course , @xmath12 depends on @xmath1 .",
    "given a prior @xmath14 distribution defined on @xmath15 , the gibbs posterior is defined as @xmath16 where @xmath17 is a unspecified scale ( or inverse temperature ) parameter .",
    "the dependence of the gibbs posterior on the data @xmath18 will be suppressed throughout ; also , when the scale parameter is taken to be fixed , we will write @xmath19 .    at a high level ,",
    "gibbs posteriors offer a sort of robustness to probability model misspecification because they do not require the user to specify a likelihood  the user can construct a posterior distribution using only a prior and the given empirical risk function . besides this sort of robustness , it is perhaps not obvious that the gibbs posterior will have any other desirable properties .",
    "the following theorem shows that a standard set of sufficient conditions ( theorem  5.52 in * ? ? ?",
    "* ) for finding the m - estimator convergence rate , along with a mild local condition on the prior , are also sufficient for identifying the gibbs posterior convergence rate .    before stating the theorem",
    ", we need a bit more notation .",
    "let @xmath20 be a distance measure defined on the parameter space @xmath15 .",
    "next , for @xmath1 and @xmath21 defined before , let @xmath22 be the empirical process .",
    "[ thm : gibbs ] assume that for fixed constants @xmath23 and @xmath24 , for every @xmath25 , and for every sufficiently small @xmath26 , @xmath27 for the prior distribution @xmath14 on @xmath15 , assume that @xmath28 where @xmath29 for some @xmath30 .",
    "let @xmath31 .",
    "then the gibbs posterior @xmath32 satisfies @xmath33 as @xmath34 , where @xmath35 for any diverging sequence @xmath36 .",
    "see appendix  [ proofs : gibbs ] .",
    "the implication of the theorem is that the gibbs posterior inherits the convergence rate of the m - estimator corresponding to the given loss function for any suitable prior .",
    "moreover , the rate does not depend on the choice of scale @xmath37 .",
    "the condition on the prior is mild .",
    "for example , in the case of finite - dimensional @xmath38 it holds for any prior with a continuous density bounded away from zero in a neighborhood of @xmath12 . to see this , note that by condition , @xmath39 ; this also highlights the connection between conditions and .",
    "chapter  19 in @xcite gives a number of helpful examples for checking condition ; an important special case , covered by van der vaart s corollary  5.53 , is applicable when @xmath40 is lipschitz .",
    "it should be noted that theorem  [ thm : gibbs ] is just one of a variety of results that could be proved concerning the convergence of the gibbs posterior . in particular , in the case of finite - dimensional @xmath38 and smooth @xmath7 , an argument for gibbs posterior normality could be made along the lines of theorem  1.4.2 in @xcite .",
    "then , the gibbs posterior is approximately normal with mean equal to the m - estimator @xmath41 and variance @xmath42 where @xmath43 is the second derivative matrix of @xmath44 .",
    "if one compares this statement to the asymptotic normality of the m - estimator in theorem  5.23 in @xcite it can be seen that the gibbs posterior and the m - estimator do not have the same asymptotic variance .",
    "this difference has consequences for calibration which will be discussed in section  [ ss : omegan ] .",
    "consider the following classification problem .",
    "given data @xmath45 , @xmath46 with covariates @xmath47 , @xmath48 , the goal is to find a hyperplane in @xmath49 that best separates the two groups corresponding to @xmath50 and @xmath51 .",
    "machine learning approaches like support vector machines can be used to find an optimal separating hyperplane , but these approaches do not say anything about uncertainty in the hyperplane parameters .",
    "bootstrapping is one option for constructing confidence intervals for the classifier parameters .",
    "additionally , the support vector machine requires specification of a ",
    "cost \" parameter which is usually done by cross - validation .",
    "the gibbs model is a reasonable alternative to the support vector machine , since it is straightforward to obtain interval estimates from the posterior distribution .",
    "let @xmath52 be the value of the @xmath53 covariate in @xmath54 and let @xmath55 denote the @xmath56 vector consisting of @xmath54 with the @xmath53 covariate removed . since the goal would be to minimize the misclassification error of the classifier , it is natural to set the empirical risk to be the empirical misclassification error : @xmath57 , \\quad \\theta = ( \\theta_0 , \\theta_1),\\ ] ] where @xmath58 is the scalar intercept parameter and @xmath59 is the @xmath60-dimensional vector of slope parameters . with the empirical risk in place , the gibbs posterior distribution in can be readily obtained .",
    "an alternative to the gibbs posterior is to follow a standard bayesian approach but with a misspecified likelihood .",
    "logistic regression is a commonly - used tool for classification ( * ? ? ?",
    "4.4 ) , so a natural idea might be to construct a bayes model using the logistic regression likelihood function .",
    "that is , for @xmath61 as above , write @xmath62 where @xmath63 and @xmath64^{-1}$ ] .",
    "despite this being a seemingly natural approach , there are cases where this can give very misleading results .    for a quick illustration",
    ", we compare the gibbs ( @xmath65 ) and bayes posteriors described above in a simple example with @xmath66 . in this case , we take @xmath67 where @xmath68 then , given @xmath54 , we take @xmath69 , where @xmath70 is the normal distribution function with mean zero and standard deviation @xmath71 .",
    "the model defines the separation of two groups along the line with slope @xmath72 and intercept @xmath73 in the @xmath74-plane .",
    "figure  [ fig : posts1 ] shows posterior samples from the gibbs and bayes models for two sample sizes : @xmath75 and @xmath76 .",
    "the plots reveal that the bayesian logistic model is inconsistent , concentrating far from the point @xmath77 .",
    "the gibbs model , on the other hand , appears to be consistent , concentrating near the true parameter values .",
    "we expect that there would be other situations where the bayesian logistic regression model would be able to identify the true parameter values .",
    "the problem is that it is not clear how to distinguish those situations where the bayesian model works from those , like the one presented here , where it does not work .",
    "as figure  [ fig : posts1 ] shows , the gibbs posterior with @xmath78 concentrates tightly near the true parameter values .",
    "the rate result in theorem  [ thm : gibbs ] indicates that @xmath37 does not affect the large - sample performance , but it can make a substantial difference in the finite - samples .",
    "figure  [ fig : posts2 ] shows ( smoothed ) posterior densities based on @xmath78 and @xmath79 and it is apparent that the gibbs posterior is less concentrated for smaller @xmath37 .     in the linear classification problem ;",
    "true parameter marked with a dotted line.,scaledwidth=92.0% ]    it is unclear what scale should be used in order to ensure that credible intervals derived from the gibbs posterior are calibrated , i.e. , attain their nominal coverage probability . ignoring the scale parameter or , equivalently , assuming @xmath65",
    "typically will not calibrate the gibbs posterior for any sample size .",
    "for instance , in the classification example above , with @xmath65 , 95% credible intervals have only 85% coverage for @xmath75 and only @xmath80 coverage for @xmath76 .",
    "so it appears that the calibration problem does not dissipate as the sample size increases .",
    "therefore , there is a need to consider a proper scaling of the gibbs posterior in order to calibrate its credible intervals .",
    "as mentioned in section  [ s : intro ] , our proposed method for selecting the scale parameter @xmath37 is to use it to calibrate gibbs posterior credible regions . to this end , define the coverage probability function @xmath81 where @xmath82 is a @xmath83 gibbs posterior credible region , e.g. , a highest posterior density region .",
    "then calibration requires that @xmath37 be such that @xmath84 i.e. , that the @xmath85% gibbs posterior credible region is also a @xmath85% confidence region .",
    "this sort of bayesian frequency calibration property is desirable , as evidenced by the large body of work on the subject ( e.g. , * ? ? ?",
    "* ) . for genuine posterior distributions ,",
    "the likelihood is fixed and the prior is flexible , so frequency calibration is typically achieved by constructing a probability - matching prior . as described below , in the gibbs posterior context , the flexibility of the additional scale parameter facilitates calibration for any prior .",
    "that is , the prior can be chosen by any means and then the scale that satisfies will depend ( implicitly ) on the chosen prior .",
    "it is worth pointing out that our goal is more ambitious than the usual asymptotic calibration property considered for probability - matching priors .",
    "indeed , our goal is to select @xmath37 to calibrate the credible intervals for the _ given @xmath25 _ , not just as @xmath34 .",
    "the numerical results presented below suggest that this higher goal is , in fact , attainable .      to build up our intuition , start by assuming that @xmath1 and , therefore , @xmath12 are known ; later we will switch to the more realistic case of unknown @xmath1 .",
    "even in this overly simplified case , it is generally not possible to solve for @xmath37 in explicitly .",
    "so , numerical methods are required . as a basic starting point ,",
    "suppose we can sample from the gibbs posterior @xmath86 for any given @xmath37 .",
    "standard markov chain monte carlo methods , such as metropolis  hastings , can be used for this . from this posterior sample , the gibbs credible region @xmath87 can be evaluated . since we have assumed that @xmath1 is known , this process can be repeated for many different data sets sampled from @xmath1 and the coverage probability @xmath88 , for fixed @xmath37 , can be approximated by a simple monte carlo estimate , @xmath89 .",
    "since this can be evaluated , at least approximately , for every @xmath37 , the equation can be solved numerically .",
    "this is the essence of the gibbs posterior scaling ( gps ) algorithm .",
    "the rough statement of the gps algorithm , in particular the method for adjusting @xmath37 , can be modified for efficiency .",
    "we do not have to evaluate the coverage probability on a fine grid of points in a wide interval to solve the equation .",
    "a more efficient strategy is to use stochastic approximation , an iterative root - finding algorithm similar to newton s method ; see , for example , @xcite , @xcite , and @xcite .",
    "it creates a sequence @xmath90 by iterating according to the rule @xmath91 where @xmath92 is a non - stochastic sequence such that @xmath93 and @xmath94 , e.g. , @xmath95 for @xmath96 $ ] . in section  [ s : qreg ]",
    "we use @xmath97    the above process for solving for @xmath37 was outlined with the assumption that @xmath1 is known , which is not realistic . for the case where @xmath1 is unknown , the algorithm changes in two ways .",
    "first , since it is not possible to sample @xmath18 from @xmath1 , we replace simulation from @xmath1 with simulation from @xmath21 , i.e. , we sample with replacement from the observed data @xmath18 ; let @xmath98 denote a sample from @xmath21 .",
    "second , since we also do not know @xmath12 , we can not check if a given credible region @xmath82 covers @xmath12 .",
    "instead , we replace @xmath12 with @xmath99 , the m - estimator corresponding to minimizing the empirical risk , @xmath44 , or a bootstrap bias - corrected version thereof , and compute the probability that @xmath100",
    ". then we replace the coverage probability @xmath88 with an empirical version , @xmath101 and the proposal is to set @xmath102 and accept the solution to as an approximate solution to . of course",
    ", we often can not evaluate exactly , so we can replace this by a monte carlo / bootstrap approximation , denoted by @xmath103 .",
    "the same stochastic approximation technique discussed above for the known-@xmath1 case can be used to solve with @xmath1 is replaced by @xmath21 .",
    "the following collection of steps to solve this equation is what we call the _ gps algorithm_.    1 .",
    "take @xmath104 bootstrap samples @xmath105 of size @xmath25 from @xmath21 .",
    "initialize @xmath106 and use , e.g. , the metropolis ",
    "hastings method to get @xmath106-gibbs posterior samples of size @xmath107 for each bootstrap sample @xmath108 , @xmath109 .",
    "3 .   for @xmath110 , repeat",
    "construct credible regions @xmath111 for each @xmath109 and evaluate the approximate empirical coverage @xmath112 as in .",
    "update @xmath113 to @xmath114 according to the stochastic approximation rule . 3 .",
    "sample from the gibbs posterior using the updated scale @xmath114 using , for instance , metropolis - hastings mcmc .",
    "4 .   if the empirical coverage @xmath112 is within a tolerance @xmath115 of @xmath116 , then stop and return @xmath114 as the output @xmath117 of the gps algorithm ; otherwise , set @xmath118 and go back to ( a ) .",
    "the reader will notice that the proposed gps algorithm involves nested loops , which might suggest that these computations are expensive .",
    "however , the gps algorithm is relatively inexpensive .",
    "for example , in the quantile regression problem in section  [ s : qreg ] , with a two - dimensional parameter , sample size @xmath75 , @xmath119 bootstrap resamples , and posterior sample size @xmath120 , the gps algorithm took less than 1 minute to converge on a mac laptop computer with a 2.3 ghz intel core i7 processor .",
    "r code to implement the gps algorithm in the quantile regression example of section  [ ] is available at www.math.uic.edu/~rgmartin .",
    "there are other approaches to scaling the gibbs posterior , as discussed in section  [ s : intro ] , but we should emphasize again that these can not guarantee calibration of the corresponding gibbs posterior credible regions . since the gps algorithm is designed specifically for the purpose of calibration , some additional computational investment should be expected . in our opinion , getting exact calibration ( see section  [ s : qreg ] ) is worth the cost .      here",
    "we want to consider how the solution @xmath117 of the gps algorithm will behave and if scaling the gibbs posterior by a sequence will disrupt its desirable properties , e.g. , theorem  [ thm : gibbs ] . to start , consider a very simple example of estimating the mean of a distribution @xmath1 based on iid samples @xmath18 .",
    "it is reasonable to take the loss @xmath9 so , if the prior @xmath14 is flat , then the corresponding gibbs posterior is @xmath121 where @xmath122 is the sample mean . in this case , it is clear that the gibbs posterior @xmath86 is a normal distribution with mean @xmath122 and variance @xmath123 and the 95% credible interval is @xmath124 . if it happens that @xmath1 is a normal distribution with mean @xmath12 and variance @xmath125 , then the gibbs posterior 95% credible interval is calibrated if and only if @xmath126 .",
    "we expect the output of the gps algorithm to be close to this value . to check this , we carried out a small simulation study .",
    "we take 1000 samples of size @xmath75 , 500 , and 1000 from a standard normal distribution . in this case",
    ", we expect our gps solution to be close to the ideal value @xmath79 . indeed , the average scale parameters obtained by the gps algorithm are 0.58 , 0.55 , and 0.49 for sample size @xmath127 , 500 , and 1000 , respectively .",
    "as expected , the gps algorithm also succeeds in calibrating gibbs posterior credible intervals with coverage at 95% for each @xmath25 .",
    "this confirms that , at least in this simple normal example , the gps algorithm is doing the right thing .",
    "our numerical results in section  [ s : qreg ] suggest that this quality performance is more general .",
    "other problems featuring a smooth loss @xmath40 are basically the same as the above normal mean example . in such problems , one should expect @xmath117 to converge to a positive number bounded away from zero .",
    "as mentioned in section  [ sect : gibbs_asymp ] , the gibbs posterior is asymptotically normal with mean @xmath41 and variance @xmath128 . on the other hand ,",
    "the m - estimator @xmath41 is asymptotically normal with mean @xmath12 and variance @xmath129 where @xmath130 with @xmath131 the derivative of @xmath132 . since @xmath133 in @xmath1-probability ,",
    "then gibbs posterior credible intervals can be calibrated by proper choice of @xmath37 if @xmath134 is proportional to the identity matrix .",
    "it is not obvious that this last statement holds generally , but as a consequence of the proof of proposition  [ lem : qrgibbs ] in appendix  [ proofs : lemma ] it can be seen that @xmath135 indeed holds for quantile regression , which is examined in section  [ s : qreg ] .",
    "furthermore , the above normal approximation for the gibbs posterior appears to hold in the quantile regression numerical examples in section  [ s : qreg ] even though the loss is not as smooth as required by the argument outlined in @xcite .    in the case of non - smooth loss @xmath136",
    "it is less obvious how @xmath117 should behave .",
    "for a non - smooth loss example , in which misclassification error is the loss function , @xcite show in numerical experiments that @xmath117 behaves like the vanishing sequence @xmath137 .",
    "as long as @xmath117 does not vanish too quickly , the convergence in theorem  [ thm : gibbs ] will not be affected .",
    "[ cor : vanishing ] for @xmath138 satisfying the assumptions of theorem  [ thm : gibbs ] , write @xmath139 .",
    "then the conclusion of theorem  [ thm : gibbs ] holds if the empirical risk function @xmath44 is scaled by a sequence @xmath117 that vanishes no faster than @xmath140 .",
    "the result follows from the proof of theorem  [ thm : gibbs ] , so we omit the details .    in summary",
    ", we have that the gps output @xmath117 , as a function of @xmath25 , is expected to converge to a non - zero value for smooth loss functions and to zero for non - smooth loss functions . in either case , by proposition  [ cor : vanishing ] , we expect that the desirable convergence properties of the gibbs posterior will not be disrupted by the use of a sequence of scales .",
    "moreover , scaling the gibbs posterior based on the gps algorithm does , indeed , calibrate the corresponding credible regions .",
    "quantile regression , like ordinary regression , seeks to explain some distributional feature of response variable using covariates . in the ordinary case ,",
    "the target feature is the mean , but in quantile regression the feature of interest is one or more quantiles . in particular , for fixed @xmath141 , we are interested in the @xmath142 quantile of the response @xmath143 , given the covariates @xmath144 , expressed as @xmath145 where dimension @xmath146 represents an intercept and @xmath147 covariates . in this formula , the vector @xmath38 depends on @xmath148 but , for notational simplicity , we will omit this dependence .",
    "this model specifies no parametric form for the conditional distribution of @xmath149 given @xmath74 .",
    "inference on the quantile regression coefficient @xmath38 may be carried out using asymptotic approximations , such as theorem  4.1 in @xcite , or by using the bootstrap as in @xcite .",
    "a bayesian approach would also be attractive , however , no distributional form for the conditional distribution is given in , so there is no likelihood function .",
    "some recent work has proposed the use of various forms of working likelihoods : dirichlet process mixture models @xcite , infinite normal mixtures @xcite , variations on the laplace distribution @xcite , and empirical likelihood @xcite .",
    "this list is far from exhaustive .",
    "the gibbs model is similar to other bayesian models in that the unknown true likelihood is replaced with a surrogate expression , but in this case , the surrogate is not a likelihood , but rather an expression related to the appropriate loss function .",
    "the natural empirical risk function for quantile regression for the @xmath142 quantile is @xmath150 where @xmath151 , @xmath152 , are the observations , and @xmath153 is the indicator function . as usual , the gibbs posterior can be written @xmath154 , for some prior distribution @xmath14 . when written this way ,",
    "the gibbs posterior is the same as the bayesian model using a ( misspecified ) asymmetric laplace likelihood ; see @xcite and @xcite . the only difference , in fact , is that the gibbs posterior offers freedom in the choice of scaling parameter .",
    "another approach to improving inference based on the asymmetric laplace likelihood is given in @xcite .    before exploring the performance of the gibbs posterior , scaled by our gps algorithm",
    ", we note that in the case of the quantile regression model in the conditions of theorem  [ thm : gibbs ] can be verified with @xmath155 and @xmath156 by using corollary  5.53 in @xcite , confirming the expected @xmath157 convergence rate .",
    "[ asp : one ] the marginal distribution @xmath158 of @xmath74 , which is free of unknown parameters , is such that @xmath159 exists and is positive definite .",
    "[ asp : two ] the conditional distribution @xmath149 , given @xmath160 , has at least one finite moment and admits a continuous density @xmath161 such that @xmath162 is bounded away from zero for @xmath158-almost all @xmath163 .",
    "[ lem : qrgibbs ] consider iid data @xmath164 , @xmath152 , under the model given in , with fixed @xmath141 , and suppose that assumptions  [ asp : one ] and [ asp : two ] hold . if @xmath165 is the true value , then @xmath166 as @xmath34 , where @xmath167 and @xmath36 is any diverging sequence , for any prior @xmath14 with continuous density bounded away from zero on a neighborhood of @xmath12 .",
    "see appendix  [ proofs : lemma ] .",
    "proposition  [ lem : qrgibbs ] confirms that the gibbs posterior shares the same @xmath157 convergence rate of the m - estimator presented in ( * ? ? ?",
    "* theorem  4.1 ) .",
    "our proof is short and straightforward for the case of iid observations .",
    "a similar result for the case of independent but not iid observations is given in @xcite .    despite the desirable rate result for the gibbs posterior , developing a bayesian - like",
    "approach with calibrated credible intervals in this quantile regression problem remains an important challenge .",
    "it turns out that our gps algorithm can successfully scale the gibbs posterior such that the credible intervals are exactly , or at least approximately , calibrated for all @xmath25 , without loss of efficiency in terms of interval lengths . to demonstrate this",
    ", we revisit a simulation example presented in @xcite . for @xmath168 ,",
    "the model they consider is @xmath169 where @xmath170 , @xmath171 , @xmath172 , and @xmath173 .",
    "note that assumptions  [ asp : one ] and [ asp : two ] hold under these settings .",
    "for this model , yang and he showed numerically that their proposed bayesian empirical likelihood approach ( `` bel.s '' ) produced credible intervals with approximate coverage near the nominal 95% level .",
    "moreover , compared to the bayesian method with misspecified asymmetric laplace likelihood ( `` bdl '' ) or , equivalently , our gibbs posterior with @xmath37 chosen by averaging residuals , their method is shown to be more efficient in terms of interval length . the results for these methods are presented in table  [ table : qreg ] , along with the results from the gibbs posterior intervals scaled by the gps algorithm .",
    "there are two key observations to be made .",
    "first , the gps method calibrates the credible intervals to have exact 95% coverage across the range of @xmath25 , while the other methods tend to over - cover .",
    "second , our gps - driven credible intervals tend to be considerably shorter than those of the other methods for all @xmath25 , especially for @xmath75 .",
    "all three methods have a @xmath157 convergence rate so , for large @xmath25 , we can not expect to see substantial differences between the various methods . therefore , the small-@xmath25 case should be the most important and , at least in this case , our gibbs posterior intervals , scaled by the gps algorithm , are clearly the best .",
    ".comparison of @xmath174 posterior credible intervals of the median regression parameters from five methods : bel.s ; bdl ; normal , the confidence interval computed using the asymptotic normality of the m - estimator ; @xmath175 , the gibbs posterior with @xmath37 fixed equal to @xmath176 ; and gps .",
    "coverage probability and average interval lengths are computed over @xmath177 simulated data sets .",
    "[ cols=\"<,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]     as mentioned in section  [ ss : omegan ] , we expect that in smooth models like quantile regression the gps algorithm will produce a scaling @xmath37 that neither diverges to infinity nor vanishes .",
    "figure  [ fig : omega_n ] shows that the @xmath37 values returned by gps in the above simulations are indeed bounded , fluctuating around @xmath72 and getting a bit tighter as @xmath25 increases .",
    "the apparently good performance of the gibbs posterior with @xmath178 is merely a coincidence .",
    "for example , if the conditional variance of @xmath149 , given @xmath74 , is changed to @xmath72 in the above example , then using @xmath78 will perform poorly : the @xmath174 credible intervals are too wide , with coverage of about @xmath179 for both parameters at each sample size .",
    "solutions found by the gps algorithm for each sample size @xmath180 , @xmath181 , and @xmath182 in @xmath177 simulations.,scaledwidth=45.0% ]    the above simulations show that gps performs well in the quantile regression example by beating both the empirical likelihood method termed bel.s and another method of choosing the scaling in bdl .",
    "but , considering that in smooth models we expect @xmath37 to account for the difference in asymptotic variance between the gibbs posterior and the m - estimator , it is reasonable to ask if we need gps at all or if we can instead get by with a fixed value of @xmath37 based on these asymptotic variances .",
    "we have not shown that the normal approximation given in section  [ sect : gibbs_asymp ] applies to quantile regression we note that the credible intervals match the normal intervals using variance @xmath183 very closely in simulations .",
    "calculation of this gibbs posterior asymptotic variance and the m - estimator asymptotic variance @xmath184 shows that @xmath185 so that we take @xmath186 in an attempt to calibrate posterior credible intervals with a fixed scaling .",
    "table  [ table : qreg ] shows that gps is still better than using a fixed scale based on asymptotic normality , especially at smaller sample sizes where the normal approximation is less justifiable .",
    "the presence of an extra parameter in the gibbs posterior presents a challenge to its implementation and may dissuade practitioners from using the method .",
    "this paper shows that the added flexibility of the scale parameter can be used as an advantage .",
    "indeed , we have demonstrated that the proposed gps algorithm allows the user to choose the scale parameter to calibrate gibbs posterior credible sets .",
    "the addition of the gps algorithm gives gibbs posteriors an edge over the ordinary bayesian posterior when the model may be misspecified .",
    "bayesian posteriors have a probability - matching property only under certain conditions with special priors .",
    "the gps algorithm , on the other hand , calibrates credible intervals by modifying the loss function without changing the prior .",
    "this means that the prior can be used for its true purpose : to incorporate prior information about the quantity of interest .",
    "calibrated gibbs posterior models are not only robust to misspecification , but can be highly accurate . in the application shown , the gps algorithm performs much better than current bayesian quantile regression methods ; in fact , it performs nearly as well as the model using the true likelihood .",
    "the focus here and in typical presentations of gibbs posteriors is in cases where there is no likelihood function available to carry out a proper bayesian analysis .",
    "a point that is generally overlooked , however , is that one might be interested in using a gibbs posterior even in cases where a likelihood is available .",
    "a reason for this might be that the statistical model involves a number of nuisance parameters : with a gibbs posterior , one can avoid specification of prior distributions for these nuisance parameters and , furthermore , computational resources can be saved by avoiding posterior computations on these unnecessary dimensions .",
    "so , we think that gibbs posteriors have some untapped potential along these lines , providing even further motivation for the gps algorithm for scaling .    the gps algorithm is a promising first step towards scaling the gibbs posterior , but further work is needed .",
    "first , the techniques employed in our implementation of the gps algorithm are perhaps too simple to be efficient in more complex problems .",
    "for example , more sophisticated versions of the resampling strategy may make the approximations more accurate or faster to compute .",
    "for instance , we use metropolis  hastings mcmc in all of our examples , but sampling from variational approximations to the posterior are a possible alternative @xcite .",
    "alternatively , mcmc could be used to sample the gibbs posterior for the initial value of @xmath37 and then sampling importance resampling could be used to resample from the posterior samples as @xmath37 is updated .",
    "it would also be interesting to explore if any of the steps in the gps algorithm could be done in parallel to speed up the computations .",
    "second , the examples shown in this paper consist of problems involving only a few parameters , so efforts are needed to make the posterior sampling as well as scaling more efficient .",
    "this work is partially supported by the u.  s.  army research offices , award # w911nf-15 - 1 - 0154 .",
    "here , it will be convenient to rewrite the posterior distribution in as @xmath187 where the dependence on the data has been suppressed for notational convenience . for simplicity",
    ", it is assumed @xmath78 , but any constant @xmath188 or suitably vanishing @xmath189 will do ; see proposition  [ cor : vanishing ] .",
    "then the goal is to obtain appropriate bounds on the numerator and on the denominator .",
    "for the denominator , we work along the lines of proposition  1 in @xcite . though our argument here is similar to theirs , note that our neighborhood in is of a different form , in that it does not involve a second moment .",
    "start with @xmath190 add and subtract @xmath191 in the exponent to get @xmath192 we can further bound @xmath193 by restricting the domain of integration to a subset containing parameter values @xmath38 that are close to @xmath12 in the appropriate way . for the sequence @xmath194 defined in the theorem statement ,",
    "let @xmath195 next , define @xmath196 where we abuse notation and think of @xmath197 as the empirical process evaluated at the given @xmath198 . for a fixed @xmath199 , write @xmath200 for the slice of @xmath201 corresponding to @xmath198 ;",
    "for fixed @xmath8 , define @xmath202 similarly . by and",
    "markov s inequality , we have that @xmath203 for our choice of @xmath204 we have @xmath205 , so @xmath206 . by definition of @xmath207 and @xmath208 , for @xmath18 the observations , we now have @xmath209e^{-2nt_n}.\\end{aligned}\\ ] ] taking expectation with respect to the distribution @xmath210 of @xmath18 and applying fubini s theorem gives @xmath211 & = \\int\\int i(s_n \\cap w_n(x^n ) ) \\ , \\pi(d\\theta ) \\ ,",
    "p^n(dx^n ) \\\\ & = \\int\\int i(s_n)i(w_n ) \\ , \\pi(d\\theta ) \\ , p^n(dx^n)\\\\ & = \\int i(s_n ) p^n\\{w_n(\\theta)\\ } \\ , \\pi(d\\theta)\\\\ & \\lesssim \\frac{1}{\\sqrt{n}t_n^{1-\\beta/\\alpha}}\\pi(s_n)\\\\ & \\lesssim \\frac{1}{\\sqrt{n}}\\rightarrow 0,\\end{aligned}\\ ] ] where the last line follows from the condition on @xmath212 . from this and",
    "markov s inequality again it follows that , with probability converging to @xmath72 , @xmath213      finally , put together the in - probability bounds on the numerator and denominator : @xmath217 for some @xmath218 .",
    "since we have chosen @xmath219 we have that @xmath220 and , since @xmath221 , the bound vanishes , completing the proof .",
    "we can use corollary  5.53 in @xcite to show that @xmath155 and @xmath156 , yielding the @xmath157 rate of convergence . suppress the dependence on @xmath148 and refer to a generic parameter as @xmath38 and the true parameter minimizing the risk @xmath13 as @xmath12 .",
    "first , we claim that the loss function @xmath222 for the quantile regression problem satisfies a lipschitz property , i.e. , that @xmath223 this follows from looking at each of the three cases@xmath224 , @xmath225 , and @xmath226and an application of the cauchy  schwartz inequality .",
    "the lipschitz constant , @xmath227 , which depends only on @xmath163 in this case , satisfies @xmath228 by assumption  [ asp : one ] . according to @xcite ,",
    "corollary  19.35 , this implies that condition holds with @xmath229 .",
    "next , we want to check that the risk function @xmath13 admits a suitable second - order taylor approximation at @xmath12 . towards this , write @xmath230 \\ , g(dx),\\ ] ] where @xmath158 is the marginal distribution of @xmath74 , defined on the space @xmath2 , and @xmath231 is the conditional distribution function of @xmath149 , given @xmath160 , and @xmath232 is the corresponding density function .",
    "differentiation with respect to @xmath38 gives @xmath233 differentiating under the integral is permissible by the continuity and moment conditions imposed .",
    "since @xmath234 , we have a second - order taylor approximation : @xmath235 based on our assumptions , we have that @xmath236 exists and is positive definite which , according to @xcite , page  76 , implies with @xmath237 .    finally , by theorem  [ thm : gibbs ] , we can conclude that the rate of convergence for the gibbs posterior is @xmath238 , where @xmath239 .",
    "therefore , we have proved the claimed @xmath157 rate as long as the prior distribution for @xmath38 has a density function which is continuous and bounded away from 0 in a neighborhood of @xmath12 ."
  ],
  "abstract_text": [
    "<S> in some applications , the relationship between the observable data and unknown parameters is described via a loss function rather than likelihood . in such cases , </S>",
    "<S> the standard bayesian methodology can not be used , but a gibbs posterior distribution can be constructed by appropriately using the loss in place of a likelihood . </S>",
    "<S> although misspecified by definition , gibbs posteriors have a number of desirable properties ; for example , we show that the gibbs posterior converges at the same rate as the corresponding m - estimator . </S>",
    "<S> inference based on the gibbs posterior is not straightforward , however , because the finite - sample performance is highly sensitive to the scale of the loss function . to meet this challenge </S>",
    "<S> , we propose a gibbs posterior scaling ( gps ) algorithm that adaptively selects the scaling in order to calibrate the corresponding gibbs posterior credible regions . in the important quantile regression problem </S>",
    "<S> , we show numerically that our gibbs posterior credible intervals , with scale selected by the gps algorithm , are exactly calibrated and are more efficient than those obtained via other bayesian - like methods .    _ keywords and phrases : _ bayesian inference ; calibration ; gps algorithm ; likelihood - free ; m - estimation ; stochastic approximation . </S>"
  ]
}