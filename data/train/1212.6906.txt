{
  "article_text": [
    "let @xmath2 be independent random vectors in @xmath3 , with each @xmath4 having coordinates denoted by @xmath5 , that is , @xmath6 .",
    "suppose that each @xmath7 is centered , namely @xmath8=0 $ ] , and has a finite covariance matrix @xmath9 $ ] .",
    "consider the rescaled sum : @xmath10 our goal is to obtain a distributional approximation for the statistic @xmath11 defined as the maximum coordinate of vector @xmath12 : @xmath13 the distribution of @xmath11 is of interest in many applications .",
    "when @xmath0 is fixed , this distribution can be approximated by the classical central limit theorem ( clt ) applied to @xmath12 .",
    "however , in modern applications ( cf .",
    "@xcite ) , @xmath0 is often comparable or even larger than @xmath1 , and the classical clt does not apply in such cases .",
    "this paper provides a tractable approximation to the distribution of @xmath11 when @xmath0 can be large and possibly much larger than @xmath1 .",
    "the _ first _ main result of the paper is the gaussian approximation result ( gar ) , which bounds the kolmogorov distance between the distributions of @xmath11 and its gaussian analog @xmath14 .",
    "specifically , let @xmath15 be independent centered gaussian random vectors in @xmath3 such that each @xmath16 has the same covariance matrix as @xmath7 : @xmath17)$ ] .",
    "consider the rescaled sum of these vectors : @xmath18 vector @xmath19 is the gaussian analog of @xmath12 in the sense of sharing the same mean and covariance matrix , namely @xmath20 = { { \\mathrm{e}}}[y ] = 0 $ ] and @xmath21 = { { \\mathrm{e}}}[yy ' ] =   n^{-1}\\sum_{i=1}^n { { \\mathrm{e}}}[x_i x_i'].$ ] we then define the gaussian analog @xmath14 of @xmath11 as the maximum coordinate of vector @xmath19 : @xmath22 we show that , under suitable moment assumptions , as @xmath23 and possibly @xmath24 , @xmath25 where constants @xmath26 and @xmath27 are independent of @xmath1 .",
    "importantly , in ( [ eq : main result ] ) , @xmath0 can be large in comparison to @xmath1 and be as large as @xmath28 for some @xmath29 .",
    "for example , if @xmath5 are uniformly bounded ( namely , @xmath30 for some constant @xmath31 for all @xmath32 and @xmath33 ) the kolmogorov distance @xmath34 converges to zero at a polynomial rate whenever @xmath35 at a polynomial rate .",
    "we obtain similar results when @xmath5 are sub - exponential and even non - sub - exponential under suitable moment assumptions .",
    "figure [ fig : two deviations ] illustrates the result ( [ eq : main result ] ) in a non - sub - exponential example , which is motivated by the analysis of the dantzig selector of @xcite in non - gaussian settings ( see section [ sec : dantzig ] ) .",
    "[ fig : two deviations ]   and @xmath14 in the example motivated by the problem of selecting the penalty level of the dantzig selector . here",
    "@xmath5 are generated as @xmath36 with @xmath37 ( a @xmath38-distribution with four degrees of freedom ) , and @xmath39 are non - stochastic ( simulated once using @xmath40 $ ] distribution independently across @xmath32 and @xmath33 ) .",
    "the dashed line is 45@xmath41 .",
    "the distributions of @xmath11 and @xmath14 are close , as ( qualitatively ) predicted by the gar derived in the paper .",
    "the quality of the gaussian approximation is particularly good for the tail probabilities , which is most relevant for practical applications.,title=\"fig:\",width=384 ]    the proof of the gaussian approximation result ( [ eq : main result ] ) builds on a number of technical tools such as slepian s smart path interpolation ( which is related to the solution of stein s partial differential equation ; see appendix [ sec : slepian - stein note ] of the supplementary material ( sm ; @xcite ) ) , stein s leave - one - out method , approximation of maxima by the smooth potentials ( related to  free energy \" in spin glasses ) and using some fine or subtle properties of such approximation , and exponential inequalities for self - normalized sums .",
    "see , for example , @xcite for introduction and prior uses of some of these tools .",
    "the proof also critically relies on the anti - concentration and comparison bounds of maxima of gaussian vectors derived in @xcite and restated in this paper as lemmas [ lem : anticoncentration ] and [ lemma : distances gaussian to gaussian ] .",
    "our new gaussian approximation theorem has the following innovative features .",
    "first , we provide a general result that establishes that maxima of sums of random vectors can be approximated in distribution by the maxima of sums of gaussian random vectors when @xmath42 and especially when @xmath0 is of order @xmath28 for some @xmath26 .",
    "the existing techniques can also lead to results of the form ( [ eq : main result ] ) when @xmath24 , but under much stronger conditions on @xmath0 requiring @xmath43 ; see example 17 ( section 10 ) in @xcite .",
    "some high - dimensional cases where @xmath0 can be of order @xmath28 can also be handled via hungarian couplings , extreme value theory or other methods , though special structure is required ( for a detailed review , see section [ sec : literature review ] of the sm @xcite ) .",
    "second , our gaussian approximation theorem covers cases where @xmath44 does not have a limit distribution as @xmath23 and @xmath24 . in some cases , after a suitable normalization , @xmath44 could have an extreme value distribution as a limit distribution , but the approximation to an extreme value distribution requires some restrictions on the dependency structure among the coordinates in @xmath7 .",
    "our result does not limit the dependency structure .",
    "we also emphasize that our theorem specifically covers cases where the process @xmath45 is not asymptotically donsker ( i.e. , ca nt be embedded into a path of an empirical process that is donsker ) .",
    "otherwise , our result would follow from the classical functional central limit theorems for empirical processes , as in @xcite .",
    "third , the quality of approximation in ( [ eq : main result ] ) is of polynomial order in @xmath1 , which is better than the logarithmic in @xmath1 quality that we could obtain in some ( though not all ) applications using the approximation of the distribution of @xmath44 by an extreme value distribution ( see @xcite ) .",
    "note that the result ( [ eq : main result ] ) is immediately useful for inference with statistic @xmath11 , even though @xmath46 needs not converge itself to a well - behaved distribution function . indeed ,",
    "if the covariance matrix @xmath47 $ ] is known , then @xmath48-quantile of @xmath14 , can be computed numerically , and we have latexmath:[\\[\\label{eq : inference }     the _ second _ main result of the paper establishes validity of the multiplier ( or wild ) bootstrap for estimating quantiles of @xmath14 when the covariance matrix @xmath47 $ ] is unknown .",
    "specifically , we define the gaussian - symmetrized version @xmath50 of @xmath11 by multiplying @xmath7 with i.i.d .",
    "standard gaussian random variables @xmath51 : @xmath52 we show that the conditional quantiles of @xmath50 given data @xmath53 are able to consistently estimate the quantiles of @xmath14 and hence those of @xmath11 ( where the notion of consistency used is the one that guarantees asymptotically valid inference ) . here",
    "the primary factor driving the bootstrap estimation error is the maximum difference between the empirical and population covariance matrices : @xmath54 ) \\right |,\\ ] ] which can converge to zero even when @xmath0 is much larger than @xmath1 . for example , when @xmath5 are uniformly bounded , the multiplier bootstrap is valid for inference if @xmath35 .",
    "earlier related results on bootstrap in the `` @xmath55 but @xmath56 '' regime were obtained in @xcite ; interesting results on inference on the mean vector of high - dimensional random vectors when @xmath42 based on concentration inequalities and symmetrization are obtained in @xcite , albeit the approach and results are quite different from those given here .",
    "in particular , in @xcite , either gaussianity or symmetry in distribution is imposed on the data .",
    "the key motivating example of our analysis is the analysis of construction of one - sided or two - sided uniform confidence band for high - dimensional means under non - gaussian assumptions .",
    "this requires estimation of a high quantile of the maximum of sample means .",
    "we give two concrete applications .",
    "one application deals with high - dimensional sparse regression model . in this model , @xcite and @xcite",
    "assume gaussian errors to analyze the dantzig selector , where the high - dimensional means enter the constraint in the problem .",
    "our results show that gaussianity is not necessary and the sharp , gaussian - like , conclusions hold approximately , with just the fourth moment of the regression errors being bounded .",
    "moreover , our approximation allows to take into account correlations among the regressors .",
    "this leads to a better choice of the penalty level and tighter bounds on performance than those that had been available previously .",
    "in another example we apply our results in the multiple hypothesis testing via the step - down method of @xcite . in the sm",
    "@xcite we also provide an application to adaptive specification testing . in either case the number of hypotheses to be tested or the number of moment restrictions to be tested can be much larger than the sample size .",
    "lastly , in a companion work ( @xcite ) , we derive the strong coupling for suprema of general empirical processes based on the methods developed here and maximal inequalities .",
    "these results represent a useful complement to the results based on the hungarian coupling developed by @xcite for the entire empirical process and have applications to inference in nonparametric problems such as construction of uniform confidence bands and testing qualitative hypotheses ( see , e.g. , @xcite , @xcite , and @xcite ) .      in section [ sec : gaus vs nongaus ] , we give the results on gaussian approximation , and in section [ sec : multiplier bootstrap ] on the multiplier bootstrap . in sections [ sec :",
    "dantzig ] and [ sub : mht ] , we develop applications to the dantzig selector and multiple testing .",
    "appendices [ sec : auxiliary lemmas]-[sec : multiplier bootstrap proofs ] contain proofs for each of these sections , with appendix [ sec : auxiliary lemmas ] stating auxiliary tools and lemmas . due to the space limitation , we put additional results and proofs into the sm @xcite .",
    "in particular , appendix [ sub : ast ] of the sm provides additional application to adaptive specification testing .",
    "results of monte carlo simulations are presented in appendix [ sec : monte carlo ] of the sm .      in",
    "what follows , unless otherwise stated , we will assume that @xmath57 . in making asymptotic statements",
    ", we assume that @xmath23 with understanding that @xmath0 depends on @xmath1 and possibly @xmath55 as @xmath23 .",
    "constants @xmath58 are understood to be independent of @xmath1 . throughout the paper ,",
    "@xmath59 $ ] denotes the average over index @xmath60 , that is , it simply abbreviates the notation @xmath61 $ ] . for example , @xmath62 $ ] @xmath63 @xmath64 .",
    "in addition , @xmath65={{\\mathbb{e}_n}}[{{\\mathrm{e}}}[\\cdot]]$ ] .",
    "for example , @xmath66 $ ] @xmath63 @xmath67 $ ] .",
    "for @xmath68 , @xmath69 denotes the transpose of @xmath70 . for a function @xmath71 , we write @xmath72 for nonnegative integer @xmath73 ; for a function @xmath74 , we write @xmath75 for @xmath76 , where @xmath77 .",
    "we denote by @xmath78 the class of @xmath73 times continuously differentiable functions from @xmath79 to itself , and denote by @xmath80 the class of all functions @xmath81 such that @xmath82 for @xmath83 .",
    "we write @xmath84 if @xmath85 is smaller than or equal to @xmath86 up to a universal positive constant . for @xmath87 , we write @xmath88 . for two sets @xmath89 and @xmath90 , @xmath91 denotes their symmetric difference , that is , @xmath92 .",
    "the purpose of this section is to compare and bound the difference between the expectations and distribution functions of the non - gaussian to gaussian maxima : @xmath93 where vector @xmath12 is defined in equation ( [ eq : define x ] ) and @xmath19 in equation ( [ eq : define y ] ) . here and in what follows , without loss of generality",
    ", we will assume that @xmath94 and @xmath95 are independent . in order to derive the main result of this section",
    ", we shall employ slepian interpolation , stein s leave - one - out method , a truncation method combined with self - normalization , as well as some fine properties of the smooth max function ( such as  stability \" ) .",
    "( the relative complexity of the approach is justified in comment [ comment : warmup ] below . )",
    "the following bounds on moments will be used in stating the bounds in gaussian approximations : @xmath96)^{1/k}.\\ ] ]    the problem of comparing distributions of maxima is of intrinsic difficulty since the maximum function @xmath97 is non - differentiable . to circumvent the problem",
    ", we use a smooth approximation of the maximum function . for @xmath98 , consider the function : @xmath99 where @xmath100 is the smoothing parameter that controls the level of approximation ( we call this function the `` smooth max function '' ) . an elementary calculation shows that for all @xmath101 , @xmath102 this smooth max function arises in the definition of  free energy \" in spin glasses ; see , for example , @xcite .",
    "some important properties of this function , such as stability , are derived in the appendix .",
    "given a threshold level @xmath103 , we define a truncated version of @xmath5 by @xmath104)^{1/2 } \\right \\ } - { { \\mathrm{e}}}\\left [ x_{ij } 1   \\left \\{|x_{ij}| { \\leqslant}u ( { \\bar { { \\mathrm{e}}}}[x_{ij}^2])^{1/2 } \\right \\ }   \\right ] .",
    "\\label{def : truncated x}\\ ] ] let @xmath105 be the infimum , which is attained , over all numbers @xmath106 such that @xmath107)^{1/2}\\right\\}\\right ] { \\leqslant}\\varphi^{2 } { \\bar { { \\mathrm{e}}}}[x_{ij}^{2 } ] .",
    "\\label{def : truncation varphi}\\ ] ] note that the function @xmath108 is right - continuous ; it measures the impact of truncation on second moments .",
    "define @xmath109 as the infimum over all numbers @xmath110 such that @xmath111)^{1/2 } , 1 { \\leqslant}i { \\leqslant}n , 1 { \\leqslant}j { \\leqslant}p \\right ) { \\geqslant}1-\\gamma.\\ ] ] also define @xmath112 and @xmath113 by the corresponding quantities for the analogue gaussian case , namely with @xmath114 replaced by @xmath115 in the above definitions . throughout the paper we use the following quantities : @xmath116 also , in what follows , for a smooth function @xmath117 , write @xmath118 the following theorem is the main building block toward deriving a result of the form ( [ eq : main result ] ) .    [",
    "theorem : comparison non - gaussian ] let @xmath119 and @xmath120 be such that @xmath121 and @xmath122 .",
    "then for every @xmath123 , @xmath124| \\lesssim d_{n}(g,\\beta , u,\\gamma)$ ] , so that @xmath125| \\lesssim d_{n}(g,\\beta , u,\\gamma ) + \\beta^{-1 } g_1 \\log p,\\end{aligned}\\ ] ] where @xmath126    we will also invoke the following lemma , which is proved in @xcite .    [",
    "lem : anticoncentration ] ( a ) let @xmath127 be jointly gaussian random variables with @xmath128=0 $ ] and @xmath129   > 0 $ ] for all @xmath130 , and let @xmath131 $ ] .",
    "let @xmath132 and @xmath133 .",
    "then for every @xmath134 , @xmath135 where @xmath136 is a constant depending only on @xmath137 and @xmath138 . when @xmath139 are all equal , @xmath140 on the right side can be replaced by @xmath141 .",
    "( b ) furthermore , the worst case bound is obtained by bounding @xmath142 by @xmath143 .    by theorem [ theorem :",
    "comparison non - gaussian ] and lemma [ lem : anticoncentration ] , we can obtain a bound on the kolmogorov distance , @xmath34 , between the distribution functions of @xmath44 and @xmath144 , which is the main theorem of this section .",
    "[ cor : gaussian to nongaussian ks 2 ] suppose that there are some constants @xmath145 such that @xmath146 { \\leqslant}c_{1}$ ] for all @xmath147 .",
    "then for every @xmath120 , @xmath148 where @xmath136 is a constant that depends on @xmath149 and @xmath150 only .    [ comment : relaxed ] the condition that @xmath66{\\geqslant}c_1 $ ] for _ all _ @xmath151 can not be removed in general",
    "however , this condition becomes redundant , if there is at least a nontrivial fraction of components @xmath5 s of vector @xmath7 with variance bounded away from zero and all pairwise correlations bounded away from 1 : for some @xmath152 , @xmath153{\\geqslant}c_1 ,   \\ \\ \\frac{|{\\bar { { \\mathrm{e}}}}[x_{ij}x_{ik } ] | } { \\sqrt{{\\bar { { \\mathrm{e}}}}[x_{ij}^2 ] } \\sqrt{{\\bar { { \\mathrm{e}}}}[x_{ik}^2 ] } } { \\leqslant}1- \\nu ' , \\ \\",
    "\\forall   ( k , j ) \\in j \\times j : k \\neq j,\\ ] ] where @xmath154 and @xmath155 are some constants independent of @xmath1 or @xmath0 .",
    "section [ sec : low variance ] of the sm @xcite contains formal results under this condition .",
    "in applications , it is useful to have explicit bounds on the upper function @xmath156 . to this end , let @xmath157 be a _ young - orlicz modulus _",
    ", that is , a convex and strictly increasing function with @xmath158 .",
    "denote by @xmath159 the inverse function of @xmath160 .",
    "standard examples include the power function @xmath161 with inverse @xmath162 and the exponential function @xmath163 with inverse @xmath164 .",
    "these functions describe how many moments the random variables have ; for example , a random variable @xmath165 has finite @xmath166th moment if @xmath167 < \\infty$ ] , and is sub - exponential if @xmath168 < \\infty$ ] for some @xmath27 .",
    "we refer to @xcite , chapter 2.2 , for further details .",
    "[ lem : bound on u ] let @xmath157 be a young - orlicz modulus , and let @xmath169 and @xmath170 be constants such that @xmath171)^{1/2 } { \\leqslant}b$ ] for all @xmath172 , and @xmath173 { \\leqslant}1 $ ] . then under the condition of theorem [ cor : gaussian to nongaussian",
    "ks 2 ] , @xmath174 where @xmath136 is a constant that depends on @xmath149 and @xmath150 only .    in applications , parameters",
    "@xmath90 and @xmath175 ( with @xmath176 and @xmath177 as well ) are allowed to increase with @xmath1 .",
    "the size of these parameters and the choice of the young - orlicz modulus are case - specific .",
    "the purpose of this subsection is to obtain bounds on @xmath34 for various leading examples frequently encountered in applications .",
    "we are concerned with simple conditions under which @xmath34 decays polynomially in @xmath1 .",
    "let @xmath178 and @xmath31 be some constants , and let @xmath179 be a sequence of constants .",
    "we allow for the case where @xmath180 as @xmath23 .",
    "we shall first consider applications where one of the following conditions is satisfied _ uniformly in _",
    "@xmath60 and @xmath130 :    * @xmath181 { \\leqslant}c_{1}$ ] and @xmath182 + { { \\mathrm{e}}}[\\exp(|x_{ij}|/b_n ) ] { \\leqslant}4 $ ] ; * @xmath181 { \\leqslant}c_{1}$ ] and @xmath183 + { { \\mathrm{e } } } [ ( \\max_{1{\\leqslant}j { \\leqslant}p } |x_{ij}| / b_{n})^4 ] { \\leqslant}4 $ ] .",
    "as a rather special case , condition ( e.1 ) covers vectors @xmath4 made up from sub - exponential random variables , that is , @xmath184 { \\geqslant}c_{1 } \\text { and } { { \\mathrm{e}}}[\\exp ( |x_{ij}|/c_{1 } ) ]   { \\leqslant}2\\ ] ] ( set @xmath185 ) , which in turn includes , as a special case , vectors @xmath4 made up from sub - gaussian random variables . condition ( e.1 ) also covers the case when @xmath186 for all @xmath32 and @xmath33 , where @xmath187 may increase with @xmath1 .",
    "condition ( e.2 ) is weaker than ( e.1 ) in that it restricts only the growth of the fourth moments but stronger than ( e.1 ) in that it restricts the growth of @xmath188 .",
    "we shall also consider regression applications where one of the following conditions is satisfied _ uniformly in _",
    "@xmath60 and @xmath130 :    *  @xmath189 , where @xmath39 are non - stochastic with @xmath190 , @xmath191=1 $ ] , and @xmath192=0 $ ] , @xmath193 { \\geqslant}c_{1}$ ] , and @xmath194 { \\leqslant}2 $ ] ; or *  @xmath189 , where @xmath39 are non - stochastic with @xmath190 , @xmath191=1 $ ] , and @xmath192=0 $ ] , @xmath193 { \\geqslant}c_{1}$ ] , and @xmath195 { \\leqslant}c_1 $ ] .    conditions ( e.3 ) and ( e.4 ) cover examples that arise in high - dimensional regression , for example , @xcite , which we shall revisit later in the paper",
    ". typically , @xmath196 s are independent of @xmath33 ( i.e. , @xmath197 and hence @xmath195 { \\leqslant}c_1 $ ] in condition ( e.4 ) reduces to @xmath198 { \\leqslant}c_1 $ ] .",
    "interestingly , these examples are also connected to spin glasses , see , for example , @xcite and @xcite ( @xmath39 can be interpreted as generalized products of  spins \" and @xmath199 as their random  interactions \" ) .",
    "note that conditions ( e.3 ) and ( e.4 ) are special cases of conditions ( e.1 ) and ( e.2 ) but we state ( e.3 ) and ( e.4 ) explicitly because these conditions are useful in applications .",
    "+    [ cor : central limit theorem ] suppose that there exist constants @xmath200 and @xmath201 such that one of the following conditions is satisfied : ( i ) ( e.1 ) or ( e.3 ) holds and @xmath202 or ( ii ) ( e.2 ) or ( e.4 ) holds and @xmath203 . then there exist constants @xmath26 and @xmath136 depending only on @xmath204 , and @xmath205 such that @xmath206    this corollary follows relatively directly from theorem [ cor : gaussian to nongaussian ks 2 ] with help of lemma [ lem : bound on u ] .",
    "moreover , from lemma [ lem : bound on u ] , it is routine to find other conditions that lead to the conclusion of corollary [ cor : central limit theorem ] .",
    "[ comment : warmup ] we note in section [ sec : elementary gar ] of the sm @xcite , that it is possible to derive the following result by a much simpler proof :    suppose that there are some constants @xmath178 and @xmath31 such that @xmath207 { \\leqslant}c_{1}$ ] for all @xmath147 .",
    "then there exists a constant @xmath136 depending only on @xmath208 and @xmath209 such that @xmath210)^{1/4},\\ ] ] where @xmath211 .",
    "this simple ( though apparently new , at this level of generality ) result follows from the classical lindeberg s argument previously given in chatterjee @xcite ( in the special context of a spin - glass setting like ( e.4 ) with @xmath212 ) in combination with lemma [ lem : anticoncentration ] and standard kernel smoothing of indicator functions . in the sm @xcite",
    ", we provide the proof using slepian - stein methods , which a reader wishing to see a simple exposition ( before reading a much more involved proof of the main results ) may find helpful .",
    "the bound here is only useful in some limited cases , for example , in ( e.3 ) or ( e.4 ) when @xmath213 . when @xmath214 , the simple methods fail , requiring a more delicate argument .",
    "note that in applications @xmath187 typically grows at a fractional power of @xmath1 , see , for example , @xcite and @xcite , and so the limitation is rather major , and was the principal motivation for our whole paper .",
    "the proofs of the main results in this section rely on the following lemma .",
    "let @xmath215 and @xmath19 be centered gaussian random vectors in @xmath3 with covariance matrices @xmath216 and @xmath217 , respectively . the following lemma compares the distribution functions of @xmath218 in terms of @xmath0 and @xmath219    [ lemma : distances gaussian to gaussian ] suppose that there are some constants @xmath145 such that @xmath220 for all @xmath147 .",
    "then there exists a constant @xmath136 depending only on @xmath208 and @xmath209 such that @xmath221    the result is derived in @xcite , and extends that of @xcite who gave an explicit error in sudakov - fernique comparison of expectations of maxima of gaussian random vectors .",
    "suppose that we have a dataset @xmath94 consisting of @xmath1 independent centered random vectors @xmath7 in @xmath3 .",
    "in this section , we are interested in approximating quantiles of @xmath222 using the multiplier bootstrap method .",
    "specifically , let @xmath223 be a sequence of i.i.d .",
    "@xmath224 variables independent of @xmath94 , and let @xmath225 then we define the multiplier bootstrap estimator of the @xmath226-quantile of @xmath11 as the conditional @xmath226-quantile of @xmath50 given @xmath94 , that is , @xmath227 where @xmath228 is the probability measure induced by the multiplier variables @xmath229 holding @xmath94 fixed ( i.e. , @xmath230 ) .",
    "the multiplier bootstrap theorem below provides a non - asymptotic bound on the bootstrap estimation error .    before presenting the theorem",
    ", we first give a simple useful lemma that is helpful in the proof of the theorem and in power analysis in applications .",
    "define @xmath231 where @xmath232 and @xmath233 is a sequence of independent @xmath234)$ ] vectors .",
    "recall that @xmath235-{\\bar { { \\mathrm{e}}}}[x_{ij } x_{ik } ] \\right |$ ] .",
    "[ lem : quantile conditional to unconditional ] suppose that there are some constants @xmath145 such that @xmath146{\\leqslant}c_{1}$ ] for all @xmath151 .",
    "then for every @xmath236 , @xmath237 where , for @xmath238 denoting a constant depending only on @xmath208 and @xmath209 , @xmath239    recall that @xmath240 we are now in position to state the first main theorem of this section .",
    "[ thm : multiplier bootrstrap i ] suppose that for some constants @xmath145 , we have @xmath146{\\leqslant}c_{1}$ ] for all @xmath151 . then for every @xmath241 , @xmath242 where @xmath243 is defined in lemma [ lem : quantile conditional to unconditional ] .",
    "in addition , @xmath244    theorem [ thm : multiplier bootrstrap i ] provides a useful result for the case where the statistics are maxima of exact averages .",
    "there are many applications , however , where the relevant statistics arise as maxima of approximate averages .",
    "the following result shows that the theorem continues to apply if the approximation error of the relevant statistic by a maximum of an exact average can be suitably controlled . specifically , suppose that a statistic of interest , say @xmath245 which may not be of the form ( [ average ] ) , can be approximated by @xmath11 of the form ( [ average ] ) , and that the multiplier bootstrap is performed on a statistic @xmath246 , which may be different from ( [ average - multiplier ] ) but still can be approximated by @xmath50 of the form ( [ average - multiplier ] ) .",
    "we require the approximation to hold in the following sense : there exist @xmath247 and @xmath248 , depending on @xmath1 ( and typically @xmath249 as @xmath23 ) , such that @xmath250 we use the @xmath226-quantile of @xmath246 , computed conditional on @xmath94 : @xmath251 as an estimate of the @xmath226-quantile of @xmath252 .",
    "[ lem : quantile approximated to exact ] suppose that condition ( [ eq : conditional quantiles ] ) is satisfied .",
    "then for every @xmath253 , @xmath254    the next result provides a bound on the bootstrap estimation error .",
    "[ thm : multiplier bootrstrap ii ] suppose that , for some constants",
    "@xmath145 , we have @xmath146{\\leqslant}c_{1}$ ] for all @xmath151 .",
    "moreover , suppose that ( [ eq : statistic approximation ] ) and ( [ eq : conditional quantiles ] ) hold .",
    "then for every @xmath241 , @xmath255 where @xmath243 is defined in lemma [ lem : quantile conditional to unconditional ] , and @xmath256 depends only on @xmath208 and @xmath209 .",
    "in addition , @xmath257    in this paper , we focus on the gaussian multiplier bootstrap ( which is a form of wild bootstrap ) .",
    "this is because other exchangeable bootstrap methods are asymptotically equivalent to this bootstrap .",
    "for example , consider the empirical ( or efron s ) bootstrap which approximates the distribution of @xmath44 by the conditional distribution of @xmath258)/\\sqrt{n}$ ] where @xmath259 are i.i.d",
    ". draws from the empirical distribution of @xmath2 .",
    "we show in section [ sec : nonparametric bootstrap ] of the sm @xcite , that the empirical bootstrap is asymptotically equivalent to the gaussian multiplier bootstrap , by virtue of theorem [ cor : gaussian to nongaussian ks 2 ] ( applied conditionally on the data ) .",
    "the validity of the empirical bootstrap then follows from the validity of the gaussian multiplier method .",
    "the result is demonstrated under a simplified condition . a detailed analysis of more sophisticated conditions , and the validity of more general exchangeably weighted bootstraps ( see @xcite ) in the current",
    "setting , will be pursued in future work .",
    "here we revisit the examples in section [ sub : examples of applications gar ] and see how the multiplier bootstrap works for these leading examples .",
    "let , as before , @xmath260 and @xmath238 be some constants , and let @xmath179 be a sequence of constants .",
    "recall conditions ( e.1)-(e.4 ) in section [ sub : examples of applications gar ] .",
    "the next corollary shows that the multiplier bootstrap is valid with a polynomial rate of accuracy for the significance level under weak conditions .",
    "[ cor : multiplier bootstrap examples ] suppose that conditions ( [ eq : statistic approximation ] ) and ( [ eq : conditional quantiles ] ) hold with @xmath261 . moreover ,",
    "suppose that one of the following conditions is satisfied : ( i ) ( e.1 ) or ( e.3 ) holds and @xmath202 or ( ii ) ( e.2 ) or ( e.4 ) holds and @xmath203 .",
    "then there exist constants @xmath26 and @xmath27 depending only on @xmath262 , and @xmath205 such that @xmath263 in addition , @xmath264 .",
    "the purpose of this section is to demonstrate the case with which the gar and the multiplier bootstrap theorem given in corollaries [ cor : central limit theorem ] and [ cor : multiplier bootstrap examples ] can be applied in important problems , dealing with a high - dimensional inference and estimation .",
    "we consider the dantzig selector previously studied in the path - breaking works of @xcite , @xcite , @xcite in the gaussian setting and of @xcite in a sub - exponential setting . here",
    "we consider the non - gaussian case , where the errors have only four bounded moments , and derive the performance bounds that are approximately as sharp as in the gaussian model .",
    "we consider both homoscedastic and heteroscedastic models .",
    "let @xmath265 be a sample of independent observations where @xmath266 is a non - stochastic vector of regressors .",
    "we consider the model @xmath267=0 , \\",
    "i=1,\\dots , n , \\",
    "{ { \\mathbb{e}_n}}[z_{ij}^2]=1 ,   \\ j=1,\\dots ,",
    "p,\\ ] ] where @xmath16 is a random scalar dependent variable , and the regressors are normalized in such a way that @xmath191=1 $ ] . here",
    "we consider the homoscedastic case : @xmath268 = \\sigma^2 , \\ i=1,\\dots , n,\\ ] ] where @xmath269 is assumed to be known ( for simplicity ) .",
    "we allow @xmath0 to be substantially larger than @xmath1 .",
    "it is well known that a condition that gives a good performance for the dantzig selector is that @xmath270 is sparse , namely @xmath271 ( although this assumption will not be invoked below explicitly ) .",
    "the aim is to estimate the vector @xmath270 in some semi - norms of interest : @xmath272 , where the label @xmath273 is the name of a norm of interest .",
    "for example , given an estimator @xmath274 the prediction semi - norm for @xmath275 is @xmath276},\\ ] ] or the @xmath33th component seminorm for @xmath277 is @xmath278 and so on .",
    "the dantzig selector is the estimator defined by @xmath279|{\\leqslant}{\\lambda},\\ ] ] where @xmath280 is the @xmath281-norm .",
    "an ideal choice of the penalty level @xmath282 is meant to ensure that @xmath283|{\\leqslant}{\\lambda}\\ ] ] with a prescribed confidence level @xmath284 ( where @xmath226 is a number close to zero . )",
    "hence we would like to set penalty level @xmath285 equal to @xmath286 ( note that @xmath287 are treated as fixed ) .",
    "indeed , this penalty would take into account the correlation amongst the regressors , thereby adapting the performance of the estimator to the design condition .",
    "we can approximate this quantity using the gaussian approximations derived in section 2 . specifically , let @xmath288|,\\ ] ] where @xmath289 are i.i.d .",
    "@xmath224 random variables independent of the data .",
    "we then estimate @xmath290 by @xmath291 note that we can calculate @xmath292 numerically with any specified precision by the simulation .",
    "( in a gaussian model , design - adaptive penalty level @xmath292 was proposed in @xcite , but its extension to non - gaussian cases was not available up to now ) .",
    "an alternative choice of the penalty level is given by @xmath293 which is the canonical choice ; see @xcite and @xcite .",
    "note that canonical choice @xmath294 disregards the correlation amongst the regressors , and is therefore more conservative than @xmath292 . indeed , by the union bound , we see that @xmath295    our first result below shows that the _ either _ of the two penalty choices , @xmath296 or @xmath297 , are approximately valid under non - gaussian noise  under the mild moment assumption @xmath298 { \\leqslant}\\text{const}.$ ] replacing the canonical gaussian noise assumption . to derive this result",
    "we apply our gar to @xmath11 to establish that the difference between distribution functions of @xmath11 and @xmath14 approaches zero at polynomial speed .",
    "indeed @xmath11 can be represented as a maximum of averages , @xmath299 , for @xmath300 where @xmath301 denotes the transpose of @xmath302 .    to derive the bound on estimation error @xmath303 in a seminorm of interest",
    ", we employ the following identifiability factor : @xmath304|}{\\| \\delta \\|_i } : \\delta\\in \\mathcal{r}(\\beta ) ,   \\| \\delta\\|_i \\neq 0 \\right \\},\\ ] ] where @xmath305 is the restricted set ; @xmath306 is defined as @xmath307 if @xmath308 ( this happens if @xmath309 ) .",
    "the factors summarize the impact of sparsity of true parameter value @xmath270 and the design on the identifiability of @xmath270 with respect to the norm @xmath310 .",
    "the identifiability factors @xmath306 depend on the true parameter value @xmath270 .",
    "these factors represent a modest generalization of the cone invertibility factors and sensitivity characteristics defined in @xcite and @xcite , which are known to be quite general .",
    "the difference is the use of a norm of interest @xmath272 instead of the @xmath311 norms and the use of smaller ( non - conic ) restricted set @xmath312 in the definition .",
    "it is useful to note for later comparisons that in the case of prediction norm @xmath313 and under the exact sparsity assumption @xmath314 , we have @xmath315 where @xmath316 is the restricted eigenvalue defined in @xcite .    next we state bounds on the estimation error for the dantzig selector @xmath317 with canonical penalty level @xmath318 and the dantzig selector @xmath319 with design - adaptive penalty level @xmath320    [ thm : dantzig estimator ] suppose that there are some constants @xmath321 and @xmath322 , and a sequence @xmath179 of constants such that for all @xmath60 and @xmath130 : ( i ) @xmath323 ; ( ii ) @xmath191= 1 $ ] ; ( iii ) @xmath324 = \\sigma^2 $ ] ; ( iv ) @xmath325{\\leqslant}c_{1}$ ] ; and ( v ) @xmath326 . then there exist constants @xmath26 and @xmath27 depending only on @xmath327 and @xmath269 such that , with probability at least @xmath328 , for either @xmath329 or @xmath141 , @xmath330    the most important feature of this result is that it provides gaussian - like conclusions ( as explained below ) in a model with non - gaussian noise , having only four bounded moments . however , the probabilistic guarantee is not @xmath284 as , for example , in @xcite , but rather @xmath331 , which reflects the cost of non - gaussianity ( along with more stringent side conditions ) . in what follows we discuss details of this result . note that the bound above holds for any semi - norm of interest @xmath310 .",
    "the use of the design - adaptive penalty level implies a better performance guarantee for @xmath319 over @xmath317 .",
    "indeed , we have @xmath332 for example , in some designs , we can have @xmath333| = o_{{{\\mathrm{p}}}}(1)$ ] , so that @xmath334 , whereas @xmath335 . thus , the performance guarantee provided by @xmath319 can be much better than that of @xmath317 .    to compare to the previous results obtained for the gaussian settings , let us focus on the prediction norm and on estimator @xmath319 with penalty level @xmath296 .",
    "suppose that the true value @xmath270 is sparse , namely @xmath314 . in this case , with probability at least @xmath331 , @xmath336",
    "where the last bound is the same as in @xcite , theorem 7.1 , obtained for the gaussian case .",
    "we recover the same ( or tighter ) upper bound without making the gaussianity assumption on the errors .",
    "however , the probabilistic guarantee is not @xmath284 as in @xcite , but rather @xmath337 , which together with side conditions is the cost of non - gaussianity .",
    "unrelated to the main theme of this paper , we can see from ( [ eq : boundd ] ) that there is some tightening of the performance bound due to the use of the identifiability factor @xmath338 in place of the restricted eigenvalue @xmath316 ; for example , if @xmath339 and @xmath340 and the two regressors are identical , then @xmath341 , whereas @xmath342 .",
    "there is also some tightening due to the use of @xmath292 instead of @xmath294 as penalty level , as mentioned above .",
    "we consider the same model as above , except now the assumption on the error becomes @xmath343   { \\leqslant}\\sigma^2 , \\ \\",
    "i=1,\\dots , n,\\ ] ] that is , @xmath269 is the upper bound on the conditional variance , and we assume that this bound is known ( for simplicity ) . as before , ideally we would like to set penalty level @xmath285 equal to @xmath286 ( where @xmath11 is defined above , and we note that @xmath287 are treated as fixed ) .",
    "the gar applies as before , namely the difference of the distribution functions of @xmath11 and its gaussian",
    "analogue @xmath14 converges to zero . in this case",
    ", the gaussian analogue can be represented as @xmath344|.\\ ] ] unlike in the homoscedastic case , the covariance structure is no longer known , since @xmath345 are unknown and we can no longer calculate the quantiles of @xmath14 .",
    "however , we can estimate them using the following multiplier bootstrap procedure .",
    "first , we estimate the residuals @xmath346 obtained from a preliminary dantzig selector @xmath317 with the conservative penalty level @xmath347 , where @xmath269 is the upper bound on the error variance assumed to be known . let @xmath223 be a sequence of i.i.d .",
    "standard gaussian random variables , and let @xmath348|.\\ ] ] then we estimate @xmath349 by @xmath350 defined conditional on data @xmath265 . note that @xmath351 can be calculated numerically with any specified precision by the simulation .",
    "then we apply program ( [ eq : dantzig estimator ] ) with @xmath352 to obtain @xmath319 .",
    "[ thm : dantzig estimator 2 ] suppose that there are some constants @xmath353 and @xmath322 , and a sequence @xmath179 of constants such that for all @xmath60 and @xmath130 : ( i ) @xmath323 ; ( ii ) @xmath191= 1 $ ] ; ( iii ) @xmath354 { \\leqslant}\\sigma^2 $ ] ; ( iv ) @xmath325{\\leqslant}c_{1}$ ] ; ( v ) @xmath326 ; and ( vi ) @xmath355 .",
    "then there exist constants @xmath26 and @xmath27 depending only on @xmath356 and @xmath357 such that , with probability at least @xmath358 where @xmath359 , we have @xmath360 moreover , with probability at least @xmath361 , @xmath362 where @xmath363 ; where @xmath364 .    the result above contains a practical test of joint significance of all regressors , that is , a test of the hypothesis that @xmath365 , with the exact asymptotic size @xmath226 .    under conditions of the either of preceding two theorems , the test , that rejects the null hypothesis @xmath366 if @xmath367 , has size equal to @xmath368 .    to see this note that under the null hypothesis of @xmath365 , @xmath369 satisfies the constraint in ( [ eq : dantzig estimator ] ) with probability @xmath370 , by construction of @xmath282 ; hence @xmath371 with exactly this probability .",
    "appendix [ sub : ast ] of the sm @xcite generalizes this to a more general test , which tests @xmath365 in the regression model @xmath372 , where @xmath373 s are a small set of variables , whose coefficients are not known and need to be estimated . the test orthogonalizes each @xmath5 with respect to @xmath373 by partialling out linearly the effect of @xmath373 on @xmath5 .",
    "the result similar to that in the corollary continues to hold .    following gautier and tsybakov @xcite , the bounds given in the preceding theorems",
    "can be used for scheffe - type ( simultaneous ) inference on all components of @xmath369 .    under the conditions of either of the two preceding theorems , a @xmath370-confidence rectangle for @xmath369",
    "is given by the region @xmath374 , where @xmath375 .",
    "$ ]    we note that @xmath376 if @xmath377 = 0 $ ] for all @xmath378 .",
    "therefore , in the orthogonal model of donoho and johnstone , where @xmath377 = 0 $ ] for all pairs",
    "@xmath379 , we have that @xmath376 for all @xmath130 , so that @xmath380 $ ] , which gives a practical simultaneous @xmath370 confidence rectangle for @xmath270 . in non - orthogonal designs ,",
    "we can rely on @xcite s tractable linear programming algorithms for computing lower bounds on @xmath306 for various norms @xmath273 of interest ; see also @xcite .",
    "there are many interesting applications where the results given above apply .",
    "there are , for example , interesting works by @xcite and @xcite that consider related estimators that minimize a convex penalty subject to the multiresolution screening constraints . in the context of the regression problem studied above , such estimators may be defined as : @xmath381|{\\leqslant}{\\lambda},\\ ] ] where @xmath382 is a convex penalty , and the constraint is used for multiresolution screening .",
    "for example , the lasso estimator is nested by the above formulation by using @xmath383 , and the previous dantzig selector by using @xmath384 ; the estimators can be interpreted as a point in confidence set for @xmath270 , which lies closest to zero under @xmath382-discrepancy ( see references cited above for both of these points ) .",
    "our results on choosing @xmath282 apply to this class of estimators , and the previous analysis also applies by redefining the identifiability factor @xmath306 relative to the new restricted set @xmath385 ; where @xmath306 is defined as @xmath307 if @xmath308 .",
    "in this section , we study the problem of multiple hypothesis testing in the framework of multiple means or , more generally , approximate means .",
    "the latter possibility allows us to cover the case of testing multiple coefficients in multiple regressions , which is often required in empirical studies ; see , for example , @xcite .",
    "we combine a general stepdown procedure described in @xcite with the multiplier bootstrap developed in this paper .",
    "in contrast with @xcite , our results do not require weak convergence arguments , and , thus , can be applied to models with an increasing number of means .",
    "notably , the number of means can be large in comparison with the sample size .",
    "let @xmath386 be a vector of parameters of interest .",
    "we are interested in simultaneously testing the set of null hypotheses @xmath387 against the alternatives @xmath388 for @xmath76 where @xmath389 .",
    "suppose that the estimator @xmath390 is available that has an approximately linear form : @xmath391 where @xmath392 are independent zero - mean random vectors in @xmath393 , the influence functions , and @xmath394 are linearization errors that are small in the sense required by condition ( m ) below .",
    "vectors @xmath392 need not be directly observable . instead , some estimators @xmath395 of influence functions @xmath392 are available , which will be used in the bootstrap simulations .",
    "we refer to this framework as testing multiple approximate means .",
    "this framework covers the case of testing multiple means with @xmath396 .",
    "more generally , this framework also covers the case of multiple linear and non - linear m - regressions ; see , for example , @xcite for explicit conditions giving rise to linearizaton ( [ linearize ] ) .",
    "the detailed exposition of how the case of multiple linear regressions fits into this framework can be found in @xcite .",
    "note also that this framework implicitly covers the case of testing equalities ( @xmath397 ) because equalities can be rewritten as pairs of inequalities .",
    "we are interested in a procedure with the strong control of the family - wise error rate . in other words ,",
    "we seek a procedure that would reject at least one true null hypothesis with probability not greater than @xmath398 uniformly over a large class of data - generating processes and , in particular , uniformly over the set of true null hypotheses .",
    "more formally , let @xmath399 be a set of all data generating processes , and @xmath400 be the true process .",
    "each null hypothesis @xmath401 is equivalent to @xmath402 for some subset @xmath403 of @xmath399 .",
    "let @xmath404 and for @xmath405 denote @xmath406 where @xmath407 .",
    "the strong control of the family - wise error rate means @xmath408 where @xmath409 denotes the probability distribution under the data - generating process @xmath400 .",
    "this setting is clearly of interest in many empirical studies",
    ".    for @xmath76 , denote @xmath410 .",
    "the stepdown procedure of @xcite is described as follows . for a subset @xmath405 ,",
    "let @xmath411 be some estimator of the @xmath412-quantile of @xmath413 .",
    "on the first step , let @xmath414 .",
    "reject all hypotheses @xmath401 satisfying @xmath415 .",
    "if no null hypothesis is rejected , then stop .",
    "if some @xmath401 are rejected , let @xmath416 be the set of all null hypotheses that were not rejected on the first step . on step @xmath417 ,",
    "let @xmath418 be the subset of null hypotheses that were not rejected up to step @xmath419 .",
    "reject all hypotheses @xmath401 , @xmath420 , satisfying @xmath421 .",
    "if no null hypothesis is rejected , then stop .",
    "if some @xmath401 are rejected , let @xmath422 be the subset of all null hypotheses among @xmath420 that were not rejected .",
    "proceed in this way until the algorithm stops .",
    "romano and wolf @xcite proved the following result .",
    "suppose that @xmath411 satisfy @xmath423 then inequality ( [ eq : strong control ] ) holds if the stepdown procedure is used .",
    "indeed , let @xmath424 be the set of true null hypotheses .",
    "suppose that the procedure rejects at least one of these hypotheses .",
    "let @xmath419 be the step when the procedure rejected a true null hypothesis for the first time , and let @xmath425 be this hypothesis . clearly , we have @xmath426 .",
    "so , @xmath427 combining this chain of inequalities with ( [ eq : conditions mht ] ) yields ( [ eq : strong control ] ) .    to obtain suitable @xmath411 that satisfy inequalities ( [ eq : critical value property 1 ] ) and ( [ eq : conditions mht ] ) above",
    ", we can use the multiplier bootstrap method .",
    "let @xmath229 be an i.i.d .",
    "sequence of @xmath224 random variables that are independent of the data .",
    "let @xmath411 be the conditional @xmath412-quantile of @xmath428 given @xmath429 .    to prove that so defined critical values @xmath411 satisfy inequalities ( [ eq : critical value property 1 ] ) and ( [ eq : conditions mht ] ) , the following two quantities play a key role : @xmath430.\\ ] ] we will assume the following regularity condition ,    * there are positive constants @xmath431 and @xmath432 : ( i ) @xmath433 @xmath434 @xmath435 and ( ii ) @xmath436 .",
    "in addition , one of the following conditions is satisfied : ( iii ) ( e.1 ) or ( e.3 ) holds and @xmath202 or ( iv ) ( e.2 ) or",
    "( e.4 ) holds and @xmath203 .",
    "[ thm : mht ] suppose that ( m ) is satisfied uniformly over a class of data - generating processes @xmath399 .",
    "then the stepdown procedure with the multiplier bootstrap critical values @xmath411 given above satisfy ( [ eq : strong control ] ) for this @xmath399 with @xmath437 strengthened to @xmath438 for some constants @xmath29 and @xmath136 depending only on @xmath439 , and @xmath432 .",
    "let us consider the simple case of testing multiple means . in this case",
    ", @xmath440 $ ] and @xmath441 $ ] , where @xmath442 are i.i.d .",
    "vectors , so that the influence functions are @xmath443 $ ] , and the remainder is zero , @xmath396 .",
    "the influence functions @xmath4 are not directly observable , though easily estimable by demeaning , @xmath444 $ ] for all @xmath32 and @xmath33 .",
    "it is instructive to see the implications of theorem [ thm : mht ] in this simple setting .",
    "condition ( i ) of assumption ( m ) holds trivially in this case .",
    "condition ( ii ) of assumption ( m ) follows from lemma [ lem : symmetrization inequality 1 ] under conditions ( iii ) or ( iv ) of assumption ( m ) .",
    "therefore , theorem [ thm : mht ] applies provided that @xmath445 { \\leqslant}\\bar \\sigma^2 $ ] , @xmath446 for arbitrarily small @xmath447 and , for example , either ( a ) @xmath448 { \\leqslant}2 $ ] ( condition ( e.1 ) ) or ( b ) @xmath449 { \\leqslant}c_{1}$ ] ( condition ( e.2 ) ) .",
    "hence , the theorem implies that the gaussian multiplier bootstrap as described above leads to a testing procedure with the strong control of the family - wise error rate for the multiple hypothesis testing problem of which the _ logarithm _ of the number of hypotheses is nearly of order @xmath450 .",
    "note here that no assumption that limits the dependence between @xmath451 or the distribution of @xmath4 is made .",
    "previously , @xcite proved strong control of the family - wise error rate for the rademacher multiplier bootstrap with some adjustment factors assuming that @xmath4 s are gaussian with unknown covariance structure .",
    "the question on how large @xmath0 can be was studied in @xcite but from a conservative perspective .",
    "the motivation there is to know how fast @xmath0 can grow to maintain the size of the simultaneous test when we calculate critical values ( conservatively ) ignoring the dependency among @xmath38-statistics @xmath452 and assuming that @xmath452 were distributed as , say , @xmath224 .",
    "this framework is conservative in that correlation amongst statistics is dealt away by independence , namely by idk procedures .",
    "in contrast , our approach takes into account the correlation amongst statistics and hence is asymptotically exact , that is , asymptotically non - conservative .",
    "the following lemma , which is derived in @xcite , is a useful variation of standard maximal inequalities .",
    "[ lem : symmetrization inequality 1 ] let @xmath2 be independent random vectors in @xmath3 with @xmath453 .",
    "let @xmath454 and @xmath455 $ ] .",
    "then @xmath456-{\\bar { { \\mathrm{e}}}}[x_{ij}]| \\right ] \\lesssim   \\sigma\\sqrt { ( \\log p)/n } + \\sqrt{{{\\mathrm{e}}}[m^2]}(\\log p)/n.\\ ] ]    see @xcite , lemma 8 .",
    "we will use the following properties of the smooth max function .",
    "[ lemma : smooth max property ] for every @xmath457 , @xmath458 where , for @xmath459 , @xmath460 moreover , @xmath461    the first property was noted in @xcite .",
    "the other properties follow from repeated application of the chain rule .",
    "[ lemma : lipschitz f ] for every @xmath462 and @xmath463 , we have @xmath464 .",
    "the proof follows from the fact that @xmath465 with @xmath466 and @xmath467 .",
    "we will also use the following properties of @xmath468 .",
    "we assume @xmath469 in lemmas [ lemma : second deriv m]-[lemma : switching property ] below .",
    "[ lemma : second deriv m ] for every @xmath457 , @xmath470 where @xmath471 , @xmath472 and @xmath473 are defined in lemma [ lemma : smooth max property ] , and @xmath474 denotes evaluation at @xmath70 , including evaluation of @xmath475 at @xmath70 .",
    "the proof follows from repeated application of the chain rule and by the properties noted in lemma [ lemma : smooth max property ] .",
    "[ lemma : bounds on derivatives of m ] for every @xmath457 , @xmath476 where @xmath477 moreover , @xmath478    the lemma follows from a direct calculation .    the following lemma plays a critical role .",
    "[ lemma : switching property ] for every @xmath463 , @xmath479 with @xmath480 , @xmath481 $ ] , and every @xmath457 , we have @xmath482    observe that @xmath483 similarly , @xmath484 . since @xmath485 and @xmath486 are finite sums of products of terms such as @xmath471 , @xmath487 , @xmath488 , @xmath489 , the claim of the lemma follows .",
    "the proof of theorem [ theorem : comparison non - gaussian ] uses the following properties of the truncation operation .",
    "define @xmath490 and @xmath491 @xmath492 @xmath493 @xmath494 , where  tilde \" denotes the truncation operation defined in section 2 .",
    "the following lemma also covers the special case where @xmath495 .",
    "the property ( d ) is a consequence of sub - gaussian inequality of @xcite , theorem 2.16 , for self - normalized sums .",
    "[ lemma : truncation ] for every @xmath496 and @xmath497 , ( a ) @xmath498)^{1/q } { \\leqslant}2 ( { \\bar { { \\mathrm{e}}}}[|x_{ij}|^q])^{1/q}$ ] ; ( b ) @xmath499 { \\leqslant}(3/2 ) ( { \\bar { { \\mathrm{e}}}}[x_{ij}^2 ] + { \\bar { { \\mathrm{e}}}}[x^2_{ik } ] ) \\varphi(u)$ ] ; ( c ) @xmath500)^{1/2 } \\ } ] ) ^{2 } ]   { \\leqslant}{\\bar { { \\mathrm{e}}}}[x_{ij}^2 ] \\varphi^{2 } ( u)$ ] .",
    "moreover , for a given @xmath120 , let @xmath122 where @xmath156 is defined in section [ sec : gaus vs nongaus ] . then : ( d ) with probability at least @xmath501 , for all @xmath130 , @xmath502 } \\varphi(u )   \\sqrt{2 \\log ( p/\\gamma)}.\\ ] ]    see section [ sec : additional proofs ] of sm @xcite .",
    "the second claim of the theorem follows from property ( [ eq : smooth max property ] ) of the smooth max function .",
    "hence we shall prove the first claim .",
    "the proof strategy is similar to the proof of lemma [ thm : warmup comparison ] .",
    "however , to control effectively the third order terms in the leave - one - out expansions we shall use truncation and replace @xmath12 and @xmath19 by their truncated versions @xmath491 and @xmath503 , defined as follows : let @xmath504 , where @xmath505 was defined before the statement of the theorem , and define the truncated version of @xmath12 as @xmath506 .",
    "also let @xmath507)^{1/2 } \\right\\ } , \\",
    "\\tilde y   = \\frac{1}{\\sqrt{n } } \\sum_{i=1}^n \\tilde y_{i}.\\ ] ] note that by the symmetry of the distribution of @xmath508 , @xmath509 = 0 $ ] .",
    "recall that we are assuming that sequences @xmath510 and @xmath233 are independent .",
    "the proof consists of four steps .",
    "step 1 will show that we can replace @xmath12 by @xmath491 and @xmath19 by @xmath503 .",
    "step 2 will bound the difference of the expectations of the relevant functions of @xmath491 and @xmath503 .",
    "this is the main step of the proof .",
    "steps 3 and 4 will carry out supporting calculations .",
    "the steps of the proof will also call on various technical lemmas collected in appendix [ sec : auxiliary lemmas ] .    * step 1 . *",
    "let @xmath511 .",
    "the main goal is to bound @xmath512 $ ] .",
    "define @xmath513 where @xmath514 . by lemma [ lemma : truncation ]",
    ", we have @xmath515 { \\geqslant}1- 10 \\gamma$ ] .",
    "observe that by lemma [ lemma : lipschitz f ] , @xmath516 so that @xmath517 hence @xmath518| \\lesssim   n^{-1/2 }   ( g_3   +   g_2 \\beta + g_1   \\beta^2)m_3 ^ 3 + ( g_2 +   \\beta g_1 ) m_2 ^ 2 \\varphi(u).\\ ] ]    we define the slepian interpolation @xmath519 between @xmath503 and @xmath520 , stein s leave - one - out version @xmath521 of @xmath519 , and other useful terms : @xmath522 we have by taylor s theorem , @xmath523 = \\frac{1}{2 } \\sum_{j=1}^p \\sum_{i=1}^n \\int_0 ^ 1 { { \\mathrm{e}}}[\\partial_{j } m(z(t ) ) \\dot z_{ij}(t ) ]   dt = \\frac{1}{2}(i+ii+iii),\\ ] ] where @xmath524dt , \\\\ ii & = \\sum_{j , k=1}^p \\sum_{i=1}^n \\int_0 ^ 1 { { \\mathrm{e } } } [ \\partial_{j } \\partial_k   m(z^{(i)}(t ) )    \\dot z_{ij}(t ) z_{ik}(t ) ] dt , \\\\ iii & =   \\sum_{j , k , l=1}^p \\sum_{i=1}^n \\int_0 ^ 1 \\int_0 ^ 1 ( 1-\\tau ) { { \\mathrm{e } } } [   \\partial_{j } \\partial_k \\partial_l m ( z^{(i)}(t ) + \\tau z_{i}(t ) ) \\dot z_{ij}(t ) z_{ik}(t ) z_{il}(t ) ] d \\tau dt.\\end{aligned}\\ ] ] by independence of @xmath525 and @xmath526 together with the fact that @xmath527 = 0 $ ] , we have @xmath528 .",
    "moreover , in steps 3 and 4 below , we will show that @xmath529 ) by independence of @xmath521 and @xmath530 , @xmath531 \\cdot   | { { \\mathrm{e } } } [ \\dot z_{ij}(t ) z_{ik}(t ) ] | dt \\\\ & { \\leqslant}\\sum_{j , k=1}^p \\sum_{i=1}^n   \\int_0 ^ 1   { { \\mathrm{e}}}[u_{jk } ( z^{(i)}(t ) ) ] \\ \\cdot   |{{\\mathrm{e}}}[\\dot z_{ij}(t ) z_{ik}(t ) ] | dt,\\end{aligned}\\ ] ] where the last step follows from lemma [ lemma : bounds on derivatives of m ] . since @xmath532 , so that @xmath533 ( which is satisfied by the assumption @xmath534 ) , by lemmas [ lemma : switching property ] and [ lemma : bounds on derivatives of m ] , the last expression is bounded up to an absolute constant by @xmath535 \\cdot   |{{\\mathrm{e}}}[\\dot z_{ij}(t ) z_{ik}(t ) ] | dt \\\\ & =   \\int_0 ^ 1   \\left   \\ { \\sum_{j , k=1}^p { { \\mathrm{e}}}[u_{jk } ( z(t ) ) ] \\right \\ } \\max_{1{\\leqslant}j , k{\\leqslant}p}\\sum_{i=1}^n    |{{\\mathrm{e}}}[\\dot z_{ij}(t ) z_{ik}(t ) ] | dt \\\\ & \\lesssim   ( g_2 + g_1 \\beta ) \\int_0 ^ 1 \\max_{1{\\leqslant}j , k{\\leqslant}p}\\sum_{i=1}^n    |{{\\mathrm{e}}}[\\dot z_{ij}(t ) z_{ik}(t ) ] | dt.\\end{aligned}\\ ] ] observe that since @xmath536 = { { \\mathrm{e } } } [ y_{ij } y_{ik } ] $ ] , we have that @xmath537 = n^{-1 }   { { \\mathrm{e } } } [ \\tilde{x}_{ij } \\tilde{x}_{ik } - \\tilde{y}_{ij } \\tilde{y}_{ik } ] = n^{-1 } { { \\mathrm{e } } } [ \\tilde{x}_{ij } \\tilde{x}_{ik }   - x_{ij}x_{ik } ] + n^{-1 } { { \\mathrm{e } } } [ y_{ij } y_{ik } - \\tilde{y}_{ij } \\tilde{y}_{ik}],$ ] so that by lemma [ lemma : truncation ] ( b ) , @xmath538 | { \\leqslant}\\bar { { \\mathrm{e}}}[| \\tilde{x}_{ij } \\tilde{x}_{ik }   - x_{ij}x_{ik}| ] + \\bar { { \\mathrm{e } } } [ | y_{ij } y_{ik } - \\tilde{y}_{ij } \\tilde{y}_{ik } | ] \\lesssim ( { \\bar { { \\mathrm{e}}}}[x_{ij}^2 ] + { \\bar { { \\mathrm{e}}}}[x_{ik}^2 ] ) \\varphi(u ) \\lesssim m_{2}^{2 }   \\varphi(u ) .",
    "$ ] therefore , we conclude that @xmath539 + * step 4 . *",
    "( bound on @xmath540 ) observe that @xmath541 dt \\notag \\\\ & = _ { ( 3 ) } \\sum_{j , k , l=1}^p \\sum_{i=1}^n \\int_0 ^ 1   { { \\mathrm{e } } } [ u_{jkl } ( z^{(i)}(t ) ) ] \\cdot { { \\mathrm{e } } } [ |\\dot z_{ij}(t ) z_{ik}(t ) z_{il}(t)| ] dt , \\label{eq : last}\\end{aligned}\\ ] ] where ( 1 ) follows from @xmath542 ( see lemma [ lemma : bounds on derivatives of m ] ) , ( 2 ) from lemma [ lemma : switching property ] , ( 3 ) from independence of @xmath521 and @xmath543",
    ". moreover , the last expression is bounded as follows : @xmath544 \\cdot { { \\mathrm{e } } } [ |\\dot z_{ij}(t ) z_{ik}(t ) z_{il}(t)| ] dt    \\\\ & = _ { ( 5 ) } \\sum_{j , k , l=1}^p   \\int_0 ^ 1   { { \\mathrm{e } } } [ u_{jkl } ( z(t ) ) ] \\cdot n { \\bar { { \\mathrm{e } } } } [   |\\dot z_{ij}(t ) z_{ik}(t ) z_{il}(t)| ] dt\\\\ & { \\leqslant}_{(6 ) }    \\int_0 ^ 1 \\left ( \\sum_{j , k , l=1}^p   { { \\mathrm{e } } } [ u_{jkl } ( z(t ) ) ]   \\right ) \\max_{1{\\leqslant}j , k , l { \\leqslant}p } n { \\bar { { \\mathrm{e } } } } [   |\\dot z_{ij}(t ) z_{ik}(t ) z_{il}(t)| ] dt \\\\ & \\lesssim_{(7 ) }    ( g_3   +   g_2 \\beta + g_1   \\beta^2 )    \\int_0 ^ 1 \\max_{1{\\leqslant}j , k , l { \\leqslant}p }   n { \\bar { { \\mathrm{e } } } } [   |\\dot z_{ij}(t ) z_{ik}(t ) z_{il}(t)| ] dt,\\end{aligned}\\ ] ] where ( 4 ) follows from lemma [ lemma : switching property ] , ( 5 ) from definition of @xmath545 , ( 6 ) from a trivial inequality , ( 7 ) from lemma [ lemma : bounds on derivatives of m ]",
    ". we have to bound the integral on the last line .",
    "let @xmath546 , and observe that @xmath547dt    \\\\ & = \\int_0 ^ 1 \\omega(t ) \\max_{1{\\leqslant}j , k , l { \\leqslant}p }   n { \\bar { { \\mathrm{e } } } } [   |(\\dot z_{ij}(t)/\\omega(t ) )   z_{ik}(t ) z_{il}(t)| ] dt \\\\ & { \\leqslant}n \\int_0 ^ 1 \\omega(t ) \\max_{1{\\leqslant}j , k , l { \\leqslant}p } { \\left(}{\\bar { { \\mathrm{e}}}}[|\\dot z_{ij}(t)/\\omega(t)|^3 ] { \\bar { { \\mathrm{e}}}}[| z_{ik}(t ) |^3 ] { \\bar { { \\mathrm{e}}}}[| z_{il}(t ) |^3 ] { \\right)}^{1/3 } dt,\\end{aligned}\\ ] ] where the last inequality is by hlder . the last term is further bounded as @xmath548 \\\\ & \\lesssim_{(2 ) }   n^{-1/2 }    \\max_{1{\\leqslant}j { \\leqslant}p } ( { \\bar { { \\mathrm{e}}}}[|\\tilde x_{ij}|^{3 } ] + { \\bar { { \\mathrm{e}}}}[|\\tilde y_{ij}|^{3 } ] ) \\\\ & \\lesssim_{(3 ) } n^{-1/2 }    \\max_{1{\\leqslant}j { \\leqslant}p } ( { \\bar { { \\mathrm{e}}}}[| x_{ij}|^{3 } ] + { \\bar { { \\mathrm{e}}}}[|y_{ij}|^{3 } ] ) \\\\ & \\lesssim_{(4 ) }   n^{-1/2 }    \\max_{1 { \\leqslant}j { \\leqslant}p }   { \\bar { { \\mathrm{e}}}}[| x_{ij } |^{3}],\\end{aligned}\\ ] ] where ( 1 ) follows from the fact that : @xmath549 , @xmath550 , and the product of terms @xmath551^{1/3}$ ] , @xmath552^{1/3}$ ] and @xmath553^{1/3}$ ] is trivially bounded by @xmath554 @xmath555;$ ] ( 2 ) follows from @xmath556 , ( 3 ) from lemma [ lemma : truncation ] ( a ) , and ( 4 ) from the normality of @xmath508 with @xmath557 = { { \\mathrm{e}}}[x^2_{ij } ] $ ] , so that @xmath558 \\lesssim ( { { \\mathrm{e}}}[y_{ij}^2])^{3/2 } =   ( { { \\mathrm{e}}}[|x_{ij}^2|])^{3/2 } { \\leqslant}{{\\mathrm{e}}}[|x_{ij}|^{3}]$ ] . this completes the overall proof .",
    "see appendix [ proof of cor : gaussian to nongaussian ks 2 ] of the sm @xcite .      since @xmath559 { \\geqslant}c_{1}$ ] by assumption , we have @xmath560)^{1/2 } \\ } { \\leqslant}1 \\ { | x_{ij } | > c^{1/2}_{1 } u \\}.$ ] by markov s inequality and the condition of the lemma , we have @xmath561)^{1/2 } , \\ \\text{for some $ ( i , j)$ } \\right ) { \\leqslant}{\\textstyle   \\sum}_{i=1}^{n } { { \\mathrm{p}}}\\left ( \\max_{1 { \\leqslant}j { \\leqslant}p } | x_{ij } | > c^{1/2}_1u \\right ) \\\\ & \\quad { \\leqslant}{\\textstyle   \\sum}_{i=1}^{n } { { \\mathrm{p}}}\\left ( h(\\max_{1 { \\leqslant}j { \\leqslant}p } | x_{ij } |/d ) >",
    "h(c^{1/2}_1u / d ) \\right ) { \\leqslant}n / h(c^{1/2}_1u / d ) .\\end{aligned}\\ ] ] this implies @xmath562 . for @xmath563 , by @xmath564)$ ] with @xmath565 { \\leqslant}b^{2}$ ] , we have @xmath566 \\lesssim 1 $ ] .",
    "hence @xmath567)^{1/2 } , \\ \\text{for some $ ( i ,",
    "j)$ } \\right ) { \\leqslant}{\\textstyle   \\sum}_{i=1}^{n } { \\textstyle   \\sum}_{j=1}^{p}{{\\mathrm{p } } } (   | y_{ij } | > c^{1/2}_1u ) \\\\ & \\quad { \\leqslant}{\\textstyle   \\sum}_{i=1}^{n } { \\textstyle   \\sum}_{j=1}^{p}{{\\mathrm{p } } } (   | y_{ij } |/(2b ) >",
    "c^{1/2}_1u/(2b ) ) \\lesssim np \\exp ( - c_1u^{2}/(4b^{2})).\\end{aligned}\\ ] ] therefore , @xmath568 where @xmath569 depends only on @xmath208 .",
    "since conditions ( e.3 ) and ( e.4 ) are special cases of ( e.1 ) and ( e.2 ) , it suffices to prove the result under conditions ( e.1 ) and ( e.2 ) only .",
    "the proof consists of two steps .    * step 1 .",
    "* in this step , in each case of conditions ( e.1 ) and ( e.2 ) , we shall compute the following bounds on moments @xmath176 and @xmath570 and parameters @xmath90 and @xmath175 in lemma [ lem : bound on u ] with specific choice of @xmath160 :    *  @xmath571 , @xmath572 , @xmath573 ; *  @xmath574 , @xmath575 ;    here @xmath569 is a ( sufficiently large ) constant that depends only on @xmath208 and @xmath209 .",
    "the bounds on @xmath576 , @xmath176 and @xmath177 follow from elementary computations using hlder s inequality .",
    "the bounds on @xmath175 follow from an elementary application of lemma 2.2.2 in @xcite .",
    "for brevity , we omit the detail .",
    "* step 2 . * in all cases , there are sufficiently small constants @xmath577 and @xmath578 , and a sufficiently large constant @xmath579 , depending only on @xmath580 such that , with @xmath581 , @xmath582 hence taking @xmath583 , we conclude from theorem [ cor : gaussian to nongaussian ks 2 ] and lemma [ lem : bound on u ] that @xmath584 where @xmath136 depends only on @xmath585 .        recall that @xmath586-{\\bar { { \\mathrm{e}}}}[x_{ij}x_{ik}]|$ ] . by lemma [ lemma : distances gaussian to gaussian ] , on the event @xmath587 , we have @xmath588 for all @xmath589 , and so on this event @xmath590 implying the first claim .",
    "the second claim follows similarly .      by equation ( [ eq : conditional quantiles ] ) ,",
    "the probability of the event @xmath591 is at least @xmath592 . on this event ,",
    "@xmath593 implying that @xmath594 .",
    "the second claim of the lemma follows similarly .",
    "for @xmath241 , let @xmath595 as defined in lemma [ lem : quantile conditional to unconditional ] . to prove the first inequality , note that @xmath596 where ( 1 ) follows from lemma [ lem : quantile conditional to unconditional ] , ( 2 ) follows from the definition of @xmath34 , and ( 3 ) follows from the fact that @xmath14 has no point masses .",
    "the first inequality follows .",
    "the second inequality follows from the first inequality and the definition of @xmath34 .      for @xmath241 ,",
    "let @xmath595 with @xmath201 as in lemma [ lem : quantile conditional to unconditional ] .",
    "in addition , let @xmath597 and @xmath598 . to prove the first inequality , note that @xmath599 where @xmath600 depends on @xmath149 and @xmath150 only and where ( 1 ) follows from equation ( [ eq : statistic approximation ] ) and lemmas [ lem : quantile conditional to unconditional ] and [ lem : quantile approximated to exact ] , ( 2 ) follows from the definition of @xmath34 , and ( 3 ) follows from lemma [ lem : anticoncentration ] and the fact that @xmath14 has no point masses .",
    "the first inequality follows .",
    "the second inequality follows from the first inequality and the definition of @xmath34 .      since conditions ( e.3 ) and ( e.4 ) are special cases of ( e.1 ) and ( e.2 ) , it suffices to prove the result under conditions ( e.1 ) and ( e.2 ) only .",
    "the proof of this corollary relies on :    [ lem : technical2 ] recall conditions ( e.1)-(e.2 ) in section [ sub : examples of applications gar ]",
    ". then @xmath601 { \\leqslant}c \\times \\begin{cases } \\sqrt{\\frac{b_{n}^{2}\\log p}{n } } \\bigvee \\frac{b_{n}^{2}(\\log ( pn))^{2}(\\log p)}{n } , & \\text{under ( e.1 ) } , \\\\ \\sqrt{\\frac{b_{n}^{2}\\log p}{n } } \\bigvee \\frac{b_{n}^{2}(\\log p)}{\\sqrt{n } } , & \\text{under ( e.2 ) } , \\end{cases}\\ ] ] where @xmath136 depends only on @xmath208 and @xmath209 that appear in ( e.1)-(e.2 ) .    by lemma [ lem : symmetrization inequality 1 ] and hlder s inequality",
    ", we have @xmath601 \\lesssim    m_{4}^{2}\\sqrt{(\\log p)/n }   + ( { { \\mathrm{e } } } [ \\max_{i , j } | x_{ij } |^{4 } ] ) ^{1/2}(\\log p)/n.\\ ] ] the conclusion of the lemma follows from elementary calculations with help of lemma 2.2.2 in @xcite .    to prove the first inequality , we make use of theorem [ thm : multiplier bootrstrap ii ] .",
    "let @xmath602 and @xmath27 denote generic constants depending only on @xmath585 , and their values may change from place to place . by corollary [ cor : central limit theorem ] , in all cases , @xmath603 .",
    "moreover , @xmath604 implies that @xmath605 ( recall @xmath57 ) , and hence @xmath606 . also , @xmath607 by assumption .",
    "let @xmath608)^{1/2}/\\log p$ ] .",
    "by lemma [ lem : technical2 ] , @xmath609(\\log p)^2{\\leqslant}cn^{-c}$ ] .",
    "therefore , @xmath610 ( with possibly different @xmath611 ) .",
    "in addition , by markov s inequality , @xmath612/\\vartheta{\\leqslant}cn^{-c}$ ] .",
    "hence , by theorem [ thm : multiplier bootrstrap ii ] , the first inequality follows .",
    "the second inequality follows from the first inequality and the fact that @xmath613 as shown above .",
    "the authors would like to express their appreciation to l.h.y .",
    "chen , david gamarnik , qi - man shao , vladimir koltchinskii , enno mammen , axel munk , steve portnoy , adrian rllin , azeem shaikh , and larry wasserman for enlightening discussions .",
    "we thank the editors and referees for the comments of the highest quality that have lead to substantial improvements .",
    "99 alquier , p. and hebiri , m. ( 2011 ) .",
    "generalization of @xmath281 constraints for high dimensional regression problems .",
    "_ statist .",
    "lett . _ * 81 * 1760 - 1765 .",
    "anderson , m. ( 2008 ) .",
    "multiple inference and gender differences in the effects of early intervention : a reevaluation of the abecedarian , perry preschool , and early training projects .",
    "_ jasa _ * 103 * 1481 - 1495 .",
    "arlot , s. , blanchard , g. and roquain , e. ( 2010a ) . some non - asymptotic results on resampling in high dimension",
    "i : confidence regions .",
    "statist . _",
    "* 38 * 51 - 82 .",
    "arlot , s. , blanchard , g. and roquain , e. ( 2010b ) . some non - asymptotic results on resampling in high dimension",
    "ii : multiple tests .",
    "statist . _",
    "* 38 * 83 - 99 .",
    "belloni , a. and chernozhukov , v. ( 2009 ) .",
    "least squares after model selection in high - dimensional sparse models .",
    "_ bernoulli _ * 19 * 521 - 547 .",
    "bickel , p. , ritov , y. and tsybakov , a. ( 2009 ) .",
    "simultaneous analysis of lasso and dantzig selector .",
    "statist . _",
    "* 37 * 1705 - 1732 .",
    "bretagnolle , j. and massart , p. ( 1989 ) .",
    "hungarian construction from the non asymptotic viewpoint .",
    "* 17 * 239 - 256 .",
    "bhlmann , p. and van de geer , s. ( 2011 ) .",
    "_ statistics for high - dimensional data : methods , theory and applications_. springer .",
    "cands , e.j . and",
    "tao , t. ( 2007 ) .",
    "the dantzig selector : statistical estimation when @xmath0 is much larger than @xmath1 .",
    "statist . _",
    "* 35 * 2313 - 2351 .",
    "chatterjee , s. ( 2005a ) . a simple invariance theorem .",
    "arxiv : math/0508213 .",
    "chatterjee , s. ( 2005b ) .",
    "an error bound in the sudakov - fernique inequality .",
    "arxiv : math/0510424 .",
    "chen , l. , goldstein , l. and shao , q .- m .",
    "_ normal approximation by stein s method_. springer .",
    "chernozhukov , v. , chetverikov , d. and kato , k. ( 2012a ) .",
    "central limit theorems and multiplier bootstrap when @xmath0 is much larger than @xmath1 .",
    "chernozhukov , v. , chetverikov , d. and kato , k. ( 2012b ) .",
    "gaussian approximation of suprema of empirical processes .",
    "chernozhukov , v. , chetverikov , d. and kato , k. ( 2012c ) .",
    "comparison and anti - concentration bounds for maxima of gaussian random vectors .",
    "chernozhukov , v. , chetverikov , d. , and kato , k. ( 2013 ) .",
    "supplement to `` gaussian approximations and multiplier bootstrap for maxima of sums of high - dimensional random vectors . ''",
    "chetverikov , d. ( 2011 ) . adaptive test of conditional moment inequalities .",
    "chetverikov , d. ( 2012 ) .",
    "testing regression monotonicity in econometric models . arxiv:1212.6757 .",
    "de la pea , v. , lai , t. and shao , q .- m .",
    "_ self - normalized processes : limit theory and statistical applications_. springer .",
    "dudley , r.m .",
    "_ uniform central limit theorems_. cambridge university press .",
    "dumbgen , l. and spokoiny , v. ( 2001 ) .",
    "multiscale testing of qualitative hypotheses .",
    "_ * 29 * 124 - 152 .",
    "fan , j. , hall , p. and yao , q. ( 2007 ) . to how many simultaneous hypothesis tests can normal , student s t or bootstrap calibration be applied .",
    "_ j. amer .",
    "assoc . _ * 102 * 1282 - 1288 .",
    "frick , k. , marnitz , p. and munk , a. ( 2012 ) .",
    "shape - constrained regularization by statistical multiresolution for inverse problems : asymptotic analysis .",
    "_ inverse problems _ * 28 * 065006 .",
    "gautier , e. and tsybakov , a. ( 2011 ) .",
    "high - dimensional istrumental variables regression and confidence sets .",
    "arxiv : 1105.2454 .",
    "gin , e. and nickl , r. ( 2010 ) .",
    "confidence bands in density estimation .",
    "_ * 38 * 1122 - 1170 .",
    "he , x. , and shao , q - m .",
    "( 2000 ) . on parameters of increasing dimensions .",
    "_ journal of multivariate analysis _ * 73 * 120 - 135 .",
    "juditsky , a. and a. nemirovski .",
    "( 2011 ) . on verifiable sufficient conditions for sparse signal recovery via @xmath281 minimization . _",
    "b _ * 127 * 57 - 88 .",
    "koltchinskii , v.i .",
    "komls - major - tusndy approximation for the general empirical process and haar expansions of classes of functions .",
    "_ j. theoret .",
    "probab . _ * 7 * 73 - 118 .",
    "koltchinskii , v. ( 2009 ) .",
    "the dantzig selector and sparsity oracle inequalities .",
    "_ bernoulli _ * 15 * 799 - 828 .",
    "komls , j. , major , p. , and tusndy , g. ( 1975 ) .",
    "an approximation for partial sums of independent rv s and the sample df i. _ z. warhsch .",
    "gabiete _ * 32 * 111 - 131 .",
    "leadbetter , m. , lindgren , g. and rootzn , h. ( 1983 ) .",
    "_ extremes and related properties of random sequences and processes_. springer .",
    "mammen , e. ( 1993 ) . bootstrap and wild bootstrap for high dimensional linear models .",
    "statist . _",
    "* 21 * 255 - 285 .",
    "panchenko , d. ( 2013 ) . _ the sherrington - kirkpatrick model_. springer .",
    "pollard , d. ( 2002 ) . _",
    "a user s guide to measure theoretic probability_. cambridge university press .",
    "praestgaard , j. and wellner , j.a .",
    "exchangeably weighted bootstraps of the general empirical processes .",
    "* 21 * 2053 - 2086 .",
    "rio , e. ( 1994 ) .",
    "local invariance principles and their application to density estimation .",
    "_ probab . theory related fields _ * 98 * 21 - 45 .",
    "rllin , a. ( 2011 ) .",
    "stein s method in high dimensions with applications , _ ann .",
    "h. poincar probab .",
    "_ * 49 * , 529 - 549 .",
    "romano , j. , and wolf , m. ( 2005 ) .",
    "exact and approximate stepdown methods for multiple hypothesis testing . _ j. amer .",
    "_ * 100 * 94 - 108 .",
    "slepian , d. ( 1962 ) . the one - sided barrier problem for gaussian noise .",
    "_ bell syst .",
    "j. _ * 41 * 463 - 501 .",
    "stein , c. ( 1981 ) .",
    "estimation of the mean of a multivariate normal distribution .",
    "_ ann . statist . _ * 9 * 1135 - 1151 .",
    "talagrand , m. ( 2003 ) . _",
    "spin glasses : a challenge for mathematicians_. springer .",
    "van der vaart , a.w . and wellner , j.a .",
    "_ weak convergence and empirical processes : with applications to statistics_. springer .",
    "ye , f. and zhang , c. ( 2010 ) .",
    "rate minimaxity of the lasso and dantzig selector for the @xmath614 loss in @xmath615 balls .",
    "_ j. mach .",
    "res . _ * 11 * 3519 - 3540 .",
    "* supplement to `` gaussian approximations and multiplier bootstrap for maxima of sums of high - dimensional random vectors '' * +   +    by v. chernozhukov , d. chetverikov , and k. kato +   + _ mit , ucla , and university of tokyo _ +   +    supplementary material",
    "i +   + * deferred proofs for results from main text * +",
    "claim ( a ) . define @xmath616)^{1/2 }",
    "\\}$ ] , and observe that @xmath617 ) ^{1/q } & { \\leqslant } ( { \\bar { { \\mathrm{e}}}}[|x_{ij } i_{ij}|^q ] ) ^{1/q } +    ( { { \\mathbb{e}_n } } [ | { { \\mathrm{e } } } [   x_{ij } i_{ij }   ] |^{q}])^{1/q } \\\\ & { \\leqslant } ( { \\bar { { \\mathrm{e}}}}[|x_{ij } i_{ij}|^q ] ) ^{1/q }   + ( { \\bar { { \\mathrm{e}}}}[|x_{ij } i_{ij}|^q ] ) ^{1/q } { \\leqslant}2 ( { \\bar { { \\mathrm{e}}}}[|x_{ij}|^q ] ) ^{1/q}.\\end{aligned}\\ ] ]    claim ( b ) . observe that @xmath618 & { \\leqslant}{\\bar { { \\mathrm{e}}}}[|   ( \\tilde x_{ij}-   x_{ij } ) \\tilde x_{ik } | ]   + { \\bar { { \\mathrm{e}}}}[| x_{ij } ( \\tilde x_{ik } - x_{ik } ) | ] \\\\ & { \\leqslant}\\sqrt{{\\bar { { \\mathrm{e}}}}[(\\tilde x_{ij } -x_{ij } ) ^2 ] } \\sqrt { { \\bar { { \\mathrm{e}}}}[\\tilde x^2_{ik } ] } +    \\sqrt{{\\bar { { \\mathrm{e}}}}[(\\tilde x_{ik } -x_{ik } ) ^2 ] } \\sqrt { { \\bar { { \\mathrm{e}}}}[x^2_{ij } ] } \\\\ & { \\leqslant}2 \\varphi(u ) \\sqrt { { \\bar { { \\mathrm{e}}}}[x^2_{ij } ] } \\sqrt { { \\bar { { \\mathrm{e}}}}[x^2_{ik } ] } +     \\varphi(u ) \\sqrt { { \\bar { { \\mathrm{e}}}}[x^2_{ik } ] } \\sqrt { { \\bar { { \\mathrm{e}}}}[x^2_{ij } ] }     \\\\ & { \\leqslant}(3/2 ) \\varphi(u ) ( { \\bar { { \\mathrm{e}}}}[x^2_{ij } ] +   { \\bar { { \\mathrm{e}}}}[x^2_{ik}]),\\end{aligned}\\ ] ] where the first inequality follows from the triangle inequality , the second from the cauchy - schwarz inequality , the third from the definition of @xmath619 together with claim ( a ) , and the last from inequality @xmath620 .    claim ( c ) .",
    "this follows from the cauchy - schwarz inequality and the definition of @xmath619 .",
    "claim ( d ) . we shall use the following lemma .",
    "[ lemma : sn ] let @xmath621 be independent real - valued random variables such that @xmath622=0 $ ] and @xmath623 < \\infty$ ] for all @xmath60 .",
    "let @xmath624 .",
    "then for every @xmath625 , @xmath626 where @xmath627 $ ] and @xmath628 .",
    "see @xcite , theorem 2.16 .",
    "define @xmath629 } + \\sqrt { { { \\mathbb{e}_n}}[(x_{ij } - \\tilde x_{ij})^2 ] } .\\ ] ] then by lemma [ lemma : sn ] and the union bound , with probability at least @xmath630 , @xmath631 by claim ( c ) , for @xmath122 , with probability at least @xmath632 , for all @xmath130 , @xmath633 } + \\sqrt{{{\\mathbb{e}_n } } [ ( { { \\mathrm{e}}}[x_{ij}1\\{|x_{ij}|>u({\\bar { { \\mathrm{e}}}}[x_{ij}^2])^{1/2}\\}])^{2 } ] } \\\\   { \\leqslant}5 \\sqrt",
    "{ { \\bar { { \\mathrm{e}}}}[x_{ij}^{2 } ] } \\varphi(u).\\end{gathered}\\ ] ] the last two assertions imply claim ( d ) .",
    "since @xmath634 is bounded from below and above by positive constants , we may normalize @xmath635 , without loss of generality . in this proof ,",
    "let @xmath136 denote a generic constant depending only on @xmath208 and @xmath209 , and its value may change from place to place",
    ".    for given @xmath120 , denote @xmath636 and let @xmath637 define @xmath638 and @xmath639 .",
    "then @xmath640 and the choice of @xmath270 trivially obeys @xmath641 .",
    "so , by theorem [ theorem : comparison non - gaussian ] and using the argument as that in the proof of corollary [ cor : gaussian to nongaussian ks 1 ] , for every @xmath642 and any @xmath643 , we have @xmath644 .",
    "\\label{key to 1}\\end{aligned}\\ ] ]    * step 1 . *",
    "we claim that we can take @xmath645 for all @xmath646 . since @xmath559 { \\geqslant}c_{1}$ ] , we have @xmath647)^{1/2 } \\ } { \\leqslant}1 \\ { | x_{ij } | > c^{1/2}_{1 } u \\}.$ ] hence @xmath648)^{1/2 } \\ } ] { \\leqslant}\\bar { { \\mathrm{e } } } [ x_{ij}^{2 } 1\\ {   | x_{ij } | > c^{1/2}_{1 } u\\ } ] \\\\ & \\quad { \\leqslant}\\bar { { \\mathrm{e } } } [ x_{ij}^{4 } 1\\ {   | x_{ij } | > c^{1/2}_1 u\\ } ] /(c_1u^2 )    { \\leqslant}\\bar { { \\mathrm{e } } } [ x_{ij}^{4}]/(c_{1 } u^{2 } ) { \\leqslant}m_4 ^ 4/(c_{1 } u^{2}).\\end{aligned}\\ ] ] this implies @xmath649 . for @xmath650 ,",
    "note that @xmath651={{\\mathbb{e}_n}}[{{\\mathrm{e}}}[y_{ij}^4 ] ] = 3 { { \\mathbb{e}_n}}[({{\\mathrm{e } } } [ y_{ij}^{2}])^2 ] = 3 { { \\mathbb{e}_n}}[({{\\mathrm{e } } } [ x_{ij}^{2}])^2]{\\leqslant}3{{\\mathbb{e}_n}}[{{\\mathrm{e } } } [ x_{ij}^{4 } ] ] = 3{\\bar { { \\mathrm{e } } } } [ x_{ij}^{4}],\\ ] ] and hence @xmath652 as well .",
    "this implies the claim of this step .    * step 2 .",
    "* we shall bound the right side of ( [ key to 1 ] ) by suitably choosing @xmath653 depending on the range of @xmath654 . in order to set up this choice",
    "we define @xmath655 by the following equation : @xmath656 we then take @xmath657 we note that for @xmath658 , @xmath659 that is , when @xmath658 the smoothing parameter @xmath653 is smaller than when @xmath660 .    using these choices of parameters @xmath270 and @xmath653 and elementary calculations ( which will be done in step 3 below )",
    ", we conclude from ( [ key to 1 ] ) that whether @xmath658 or @xmath661 , @xmath662 the bound in the theorem follows from this inequality .",
    "* step 3 . * ( computation of the bound on @xmath34 ) .",
    "note that since @xmath663 , we only had to consider the case where @xmath664 since otherwise the inequality is trivial by taking , say , @xmath665 .",
    "since @xmath666 and @xmath667 , we have @xmath668 also note that @xmath669 , and so @xmath670 .",
    "therefore , @xmath671 in addition , note that @xmath672 and @xmath673 under either case .",
    "this implies that @xmath674 and @xmath675 .",
    "+ using these inequalities , we can compute the bounds claimed above .    *",
    "( a)*. bounding @xmath34 when @xmath661 .",
    "then @xmath676 where we have used step 1 and the fact that @xmath677 the claimed bound on @xmath34 now follows .",
    "* ( b)*. bounding @xmath34 when @xmath678 .",
    "since @xmath653 is smaller than in case ( a ) , by the calculations in step ( a ) @xmath679 moreover , using definition of @xmath653 , @xmath680 , definition of @xmath681 , we have @xmath682 analogously and using @xmath664 , we have @xmath683 this completes the proof .",
    "the proof proceeds in three steps . in the proof @xmath684 denotes @xmath685 with @xmath73 either @xmath686 or @xmath141",
    ".    * step 1 . * here we show that there exist some constants @xmath29 and @xmath27 ( depending only @xmath327 and @xmath269 ) such that @xmath687 with @xmath688 .",
    "we first note that @xmath689 $ ] , where @xmath300 .",
    "application of corollary [ cor : central limit theorem]-(ii ) gives @xmath690 where @xmath29 and @xmath136 are constants depending only on @xmath327 and @xmath269 .",
    "the claim follows since @xmath691 , which holds because @xmath692 , and @xmath693 ( by the union bound @xmath694 ) .",
    "* step 2 . *",
    "we claim that with probability @xmath695 , @xmath696 obeys : @xmath697 | { \\leqslant}2 \\lambda.\\ ] ] indeed , by definition of @xmath274 , @xmath698 | { \\leqslant}\\lambda , $ ] which by the triangle inequality implies @xmath699 | { \\leqslant}t_0 + \\lambda .",
    "$ ] the claim follows from step 1 .",
    "* step 3 . * by step 1 , with probability @xmath695 , the true value @xmath270 obeys the constraint in optimization problem ( [ eq : dantzig estimator ] ) in the main text , in which case by definition of @xmath274 , @xmath700 .",
    "therefore , with the same probability , @xmath701 by definition of @xmath306 we have that with the same probability , @xmath702 |.\\ ] ] combining this inequality with step 2 gives the claim of the theorem .",
    "the proof has four steps . in the proof",
    ", we let @xmath703 for sufficiently small @xmath26 and sufficiently large @xmath136 depending only on @xmath704 , where @xmath705 and @xmath706 ( and hence @xmath707 ) may change from place to place .    *",
    "* the same argument as in the previous proof applies to @xmath317 with @xmath708 , where now @xmath269 is the upper bound on @xmath709 $ ] .",
    "thus , we conclude that with probability at least @xmath710 , @xmath711    * step 1 . *",
    "we claim that with probability at least @xmath710 , @xmath712{\\right)}^{1/2 } { \\leqslant}b_n   \\frac{2 c_0(1 - 1/n)}{\\sqrt{n } \\kappa_{\\operatorname{pr}}(\\beta)}= : \\iota_n.\\ ] ] application of hlder s inequality and identity @xmath713 gives @xmath712{\\right)}^{1/2 } { \\leqslant}b_n ( { { \\mathbb{e}_n}}[z_{i } ' ( { \\widehat}\\beta^{(0 ) } - \\beta)]^2)^{1/2 } { \\leqslant}b_n   \\| { \\widehat}\\beta^{(0 ) } - \\beta\\|_{\\operatorname{pr}}.\\ ] ] the claim follows from step 0 .",
    "* step 2 . * in this step , we apply corollary [ cor : multiplier bootstrap examples]-(ii ) to @xmath714 , \\ w = \\sqrt{n}\\max_{1{\\leqslant}j { \\leqslant}2p } { { \\mathbb{e}_n}}[\\tilde z_{ij}{\\widehat}{{\\varepsilon}}_{i}e_{i } ] , \\",
    "\\text{and } \\\\ & w_0 = \\sqrt{n}\\max_{1{\\leqslant}j { \\leqslant}2p } { { \\mathbb{e}_n}}[\\tilde z_{ij}{\\varepsilon}_{i}e_{i}],\\end{aligned}\\ ] ] where @xmath715 , to conclude that uniformly in @xmath236 @xmath716 to show applicability of corollary [ cor : multiplier bootstrap examples]-(ii ) , we note that for any @xmath717 , @xmath718/\\zeta_1 { \\leqslant}\\sqrt{n}{{\\mathrm{e}}}_e\\left[\\max_{1{\\leqslant}j   { \\leqslant}p}|{{\\mathbb{e}_n}}[z_{ij}({\\widehat}{{\\varepsilon}}_{i}-{\\varepsilon}_{i})e_{i}]|\\right]/\\zeta_1 \\\\ & \\lesssim \\sqrt{\\log p } \\max_{1{\\leqslant}j { \\leqslant}p}({{\\mathbb{e}_n}}[z_{ij}^2({\\widehat}{{\\varepsilon}}_{i}-{\\varepsilon}_{i})^2])^{1/2}/\\zeta_1,\\end{aligned}\\ ] ] where the third inequality is due to pisier s inequality .",
    "the last quantity is bounded by @xmath719 with probability @xmath720 by step 1 .    since @xmath721 by assumption ( vi ) of the theorem",
    ", we can take @xmath722 in such a way that @xmath723 and @xmath724 .",
    "then all the conditions of corollary [ cor : multiplier bootstrap examples]-(ii ) with so defined @xmath722 and @xmath725 are satisfied , and hence application of the corollary gives that uniformly in @xmath236 , latexmath:[\\[\\label{equation meta }    which implies the claim of this step .",
    "* step 3 . * in this step we claim that with probability at least @xmath710 , @xmath727 combining step 2 and lemma [ lem : quantile approximated to exact ] gives that with probability at least @xmath592 , @xmath728 , where @xmath722 and @xmath729 are chosen as in step 2 . in addition , lemma [ lem : quantile conditional to unconditional ] shows that @xmath730 .",
    "finally , lemma [ lem : anticoncentration ] yields @xmath731 .",
    "combining these bounds gives the claim of this step .",
    "* step 4 . * given ( [ eq : given ] ) , the rest of the proof is identical to steps 2 - 3 in the proof of theorem [ thm : dantzig estimator ] with @xmath732 .",
    "the result follows for @xmath733 .",
    "the multiplier bootstrap critical values @xmath411 clearly satisfy @xmath734 whenever @xmath735 , so inequality ( [ eq : critical value property 1 ] ) in the main text is satisfied .",
    "therefore , it suffices to prove ( [ eq : conditions mht ] ) in the main text .",
    "let @xmath424 denote the set of true null hypotheses .",
    "then for any @xmath736 , @xmath737 therefore , for all @xmath736 , we can and will assume that @xmath738 .    for @xmath739 ,",
    "define @xmath740 in addition , define @xmath741 to prove ( [ eq : conditions mht ] ) , we will apply corollary [ cor : multiplier bootstrap examples ] . by assumption ,",
    "either ( i ) or ( ii ) of corollary [ cor : multiplier bootstrap examples ] holds .",
    "therefore , it remains to verify conditions in equations ( [ eq : statistic approximation ] ) and ( [ eq : conditional quantiles ] ) in the main text with @xmath742 for some @xmath29 and @xmath136 uniformly over all @xmath743 .",
    "set @xmath744 and @xmath745 for sufficiently small @xmath29 and large @xmath136 depending on @xmath439 , and @xmath432 only .",
    "note that @xmath742 . also note that @xmath746 for all @xmath743 .",
    "therefore , it follows from assumption ( i ) that @xmath747 for all @xmath743 , i.e. condition in equation ( [ eq : statistic approximation ] ) holds uniformly over all @xmath743 .",
    "further , note that @xmath748 conditional on @xmath114 and @xmath749 is distributed as @xmath750)$ ] random variable and @xmath751 for all @xmath743 .",
    "therefore , @xmath752{\\leqslant}(c/2)\\sqrt{\\delta_2\\log p}$ ] , and so it follows from borell inequality and assumption ( ii ) that @xmath753 for all @xmath743 , i.e. condition in equation ( [ eq : conditional quantiles ] ) holds uniformly over all @xmath743 .",
    "this completes the proof by applying corollary [ cor : multiplier bootstrap examples ] .",
    "in this section , we present results of monte carlo simulations that illustrate our theoretical results on dantzig selector given in section [ sec : dantzig ] .",
    "we consider gaussian and non - gaussian noise with homoscedasticity and heteroscedasticity .",
    "we study 3 types of dantzig selector depending on the choice of the penalty level : canonical , ideal ( based on gaussian approximation , gar ) , and multiplier bootstrap ( mb ) .",
    "we consider the following regression model : @xmath754 where observations are independent across @xmath32 , @xmath755 is a scalar dependent variable , @xmath302 is a @xmath0-dimensional vector of covariates , and @xmath756 is noise .",
    "the first component of @xmath302 equals 1 in all experiments ( an intercept ) .",
    "other @xmath757 components are simulated as follows : first , we simulate a vector @xmath758 from the gaussian distribution with zero mean so that @xmath759=1 $ ] for all @xmath760 and @xmath761=\\rho$ ] for all @xmath762 with @xmath763 ; second , we set @xmath764)^{1/2}$ ] ( equicorrelated design ) . depending on the experiment , we set @xmath765 , @xmath766 , @xmath767 , or @xmath768 . we simulate @xmath769 where depending on the experiment , @xmath770 or @xmath771 and @xmath772 is taken either from @xmath224 distribution ( gaussian noise ) or from t - distribution with 5 degrees of freedom normalized to have variance 1 ( non - gaussian noise ) . to investigate the effect of heteroscedasticity on the properties of different estimators , we set @xmath773 where @xmath774 is either 0 ( homoscedastic case ) or 1 ( heteroscedastic case ) .",
    ".results of monte carlo experiments for prediction error .",
    "non - gaussian noise . [ cols=\"^,^,^,^,^,^ \" , ]     tables 1 and 2 present results on prediction error of dantzig selector for the case of non - gaussian and gaussian noise , respectively .",
    "prediction error is defined as @xmath775}\\ ] ] where @xmath776 is the dantzig selector ; see section [ sec : dantzig ] for the definition of the dantzig selector .",
    "recall that implementing the dantzig selector requires selecting the penalty level @xmath282 .",
    "both tables show results for 3 different choices of the penalty level .",
    "canonical penalty is @xmath777 where @xmath778 , the upper bound on the variance of @xmath756 s .",
    "ideal ( based on gaussian approximation , gar ) penalty is @xmath779 , the conditional @xmath412 quantile of @xmath14 given @xmath780 where @xmath781|\\ ] ] where @xmath782 independently across @xmath32 .",
    "finally , multiplier bootstrap ( mb ) penalty is defined as follows .",
    "first , we calculate the dantzig selector with the canonical choice of the penalty level , @xmath776 , and select regressors corresponding to non - zero components of @xmath776 .",
    "second , we run the ols regression of @xmath755 on the set of selected regressors , and take residuals from this regression , @xmath783 .",
    "then the multiplier bootstrap penalty level is @xmath784 , the conditional @xmath412 quantile of @xmath785 given @xmath786 where @xmath787|\\ ] ] where @xmath782 independently across @xmath32 .",
    "the results show that the gar penalty always yields smaller prediction error than that of the canonical penalty .",
    "moreover , as predicted by the theory , gar penalty works especially good in comparison with the canonical penalty in heteroscedastic case and/or in the case with high correlation between regressors ( high @xmath34 ) .",
    "in addition , in most cases , the results for the mb penalty are similar to those for the gar penalty . in particular , the mb penalty in most cases yields smaller prediction error than that of the canonical penalty . finally , the gar penalty in most cases is slightly better than the mb penalty .",
    "note , however , that when heteroscedasticity function @xmath788 is unknown , the gar penalty becomes infeasible but the mb penalty is feasible given that the upper bound on the variance of @xmath756 s exists .",
    "supplementary material ii +   + * additional results and discussions * +",
    "to keep the notation simple , consider a random vector @xmath12 in @xmath3 and a standard normal vector @xmath19 in @xmath3 .",
    "we are interested in bounding @xmath789 - { { \\mathrm{e}}}[g(y)],\\ ] ] over some collection of test functions @xmath790 . without loss of generality ,",
    "suppose that @xmath19 and @xmath12 are independent .",
    "consider stein s partial differential equation : @xmath791   = \\triangle h(x ) -   x'\\nabla h(x)\\ ] ] where @xmath792 and @xmath793 refer to the laplacian and the gradient of @xmath794 .",
    "it is well known , e.g. @xcite and @xcite , that an explicit solution for @xmath160 in this equation is given by @xmath795 - { { \\mathrm{e}}}[g(y ) ] \\right ] dt,\\ ] ] so that @xmath789 - { { \\mathrm{e}}}[g(y ) ] = { { \\mathrm{e}}}[\\triangle h(x ) -   x ' \\nabla h(x)].\\ ] ] the stein type method for normal approximation bounds the right side for @xmath790 .    next ,",
    "let us consider the slepian smart path interpolation : @xmath796 then we have @xmath789 - { { \\mathrm{e}}}[g(y ) ] = { { \\mathrm{e}}}\\left [ \\int_0 ^ 1 \\frac{1}{2 }   \\nabla g(z(t ) ) ' \\left ( \\frac{x}{\\sqrt{t } } - \\frac{y}{\\sqrt{1-t}}\\right ) \\right ] dt.\\ ] ] the slepian type method , as used in our paper , bounds the right side for @xmath790 .",
    "we also refer the reader to @xcite for a related discussion and interesting results ( see in particular lemma 2.1 in @xcite ) .",
    "elementary calculations and integration by parts yield the following observation .",
    "[ compare slepian vs stein ] suppose that @xmath797 is a @xmath798-function with uniformly bounded derivatives up to order two",
    ". then @xmath799 =   -{{\\mathrm{e}}}[x ' \\nabla h(x ) ] \\intertext{and } ii & : = { { \\mathrm{e}}}\\left [ \\int_0 ^ 1 \\frac{1}{2 } \\nabla g(z(t ) ) ' \\left ( \\frac{y}{\\sqrt{1-t}}\\right ) \\right ] = -{{\\mathrm{e}}}[\\triangle h(x)].\\end{aligned}\\ ] ]    hence the slepian and stein methods both show that difference between @xmath273 and @xmath800 is small or approaches zero under suitable conditions on @xmath12 ; therefore , they are very similar in spirit , if not identical .",
    "the details of treating terms may be different from application to application ; see more on this in @xcite .    by definition of @xmath160",
    ", we have @xmath801   =    { { \\mathrm{e}}}\\left [ x ' \\int_0 ^ 1 \\frac{1}{2 t }   \\nabla g ( z(t ) )   \\sqrt{t } dt\\right ] =   { { \\mathrm{e}}}\\left [ \\int_0 ^ 1   \\nabla g(z(t ) ) ' \\frac{x}{2 \\sqrt{t } }   dt \\right].\\end{aligned}\\ ] ] on the other hand , by definition of @xmath160 and stein s identity ( lemma [ lemma : talagrand ] ) , @xmath802 =   { { \\mathrm{e}}}\\left [ \\frac{1}{2 } \\int_0 ^ 1 \\triangle g ( z(t ) )     dt \\right ]   =    { { \\mathrm{e}}}\\left [ \\frac{1}{2 } \\int_0 ^ 1 \\nabla g(z(t ) ) ' \\left ( \\frac{y}{\\sqrt{1-t}}\\right ) dt \\right].\\end{aligned}\\ ] ] this completes the proof .",
    "[ lemma : talagrand ] let @xmath803 be a centered gaussian random vector in @xmath3 .",
    "let @xmath804 be a @xmath805-function such that @xmath806 < \\infty$ ] for all @xmath130 .",
    "then for every @xmath130 , @xmath807=\\sum_{k=1}^p{{\\mathrm{e}}}[w_{j } w_{k } ] { { \\mathrm{e}}}[\\partial_k f ( w)].\\ ] ]    see section a.6 of @xcite , and also @xcite .",
    "this section can be helpful to the reader wishing to see how slepian - stein methods can be used to prove a simple gaussian approximation ( whose applicability is limited however . ) we start with the following elementary lemma .",
    "[ thm : warmup comparison ] for every @xmath469 and @xmath808 , @xmath809| \\lesssim n^{-1/2 } ( g_3+g_2 \\beta + g_1\\beta^2 ) { \\bar { { \\mathrm{e}}}}[s^3_i ] , \\\\ \\intertext{and hence } & |{{\\mathrm{e}}}[g(t_{0})-",
    "g(z_{0})]| \\lesssim n^{-1/2}(g_3+g_2 \\beta + g_1\\beta^2 ) { \\bar { { \\mathrm{e}}}}[s^3_i ]   + \\beta^{-1}g_1\\log p.\\end{aligned}\\ ] ]    the optimal value of the last bound is given by taking the minimum over @xmath270 .",
    "we postpone choices of @xmath270 to the proof of the subsequent corollary , leaving ourselves more flexibility in optimizing bounds in the corollary .",
    "the bound above per se seems new , though it is merely a simple extension of results in @xcite , who obtained the bound for the case with @xmath12 having a special structure like in our example ( e.4 ) , related to spin glasses , using classical lindeberg s method .",
    "we give a proof using a variant of slepian - stein method , since this is the tool we end up using to prove our main results , as the lindeberg s method , in its pure form , did not yield the same sharp results .",
    "our proof is related but rather different in details from the more abstract / general arguments based on stein triplets given in @xcite ( lemma 2.1 ) , but given for the special case of data @xmath114 with coordinates @xmath4 s that @xmath810-valued , in contrast to the @xmath811-valued case treated here . @xcite re - analyzed @xcite s setup under local dependence and gave a number of other interesting applications .",
    "the next result states a bound on the kolmogorov distance between distributions of @xmath44 and @xmath144 .",
    "the result follows from lemma [ thm : warmup comparison ] and the anti - concentration inequality for maxima of gaussian random variables stated in lemma [ lem : anticoncentration ] .",
    "note that this result was not included in either @xcite or @xcite for the cases that they have analyzed .",
    "[ cor : gaussian to nongaussian ks 1 ] suppose that there are some constants @xmath178 and @xmath31 such that @xmath207 { \\leqslant}c_{1}$ ] for all @xmath147 .",
    "then there exists a constant @xmath136 depending only on @xmath208 and @xmath209 such that @xmath812)^{1/4}.   \\end{aligned}\\ ] ]    theorem [ thm : warmup comparison ] and corollary [ cor : gaussian to nongaussian ks 1 ] imply that the error of approximating the maximum coordinate in the sum of independent random vectors by its gaussian analogue depends on @xmath0 ( possibly ) only through @xmath813 .",
    "this is the main qualitative feature of all the results in this paper .",
    "both lemma [ thm : warmup comparison ] and corollary [ cor : gaussian to nongaussian ks 1 ] and all the results in this paper do not limit the dependence among the coordinates in @xmath7 .    while lemma [ thm : warmup comparison ] and corollary [ cor : gaussian to nongaussian ks 1 ] convey an important qualitative aspect of the problem and admit easy - to - grasp proofs , an important disadvantage of these results is that the bounds depend on @xmath814 $ ] .",
    "when @xmath814 $ ] increases with @xmath1 , for example when @xmath815 for all @xmath32 and @xmath33 and @xmath187 grows with @xmath1 , the simple bound above may be too poor , and can be improved considerably using several inputs .",
    "we derive in theorem [ theorem : comparison non - gaussian ] in the main text a bound that can be much better in the latter scenario .",
    "the improvement there comes at a cost of more involved statements and proofs .    without loss of generality",
    ", we are assuming that sequences @xmath510 and @xmath233 are independent . for @xmath816 $ ]",
    ", we consider the slepian interpolation between @xmath19 and @xmath12 : @xmath817 we shall also employ stein s leave - one - out expansions : @xmath818 let @xmath819 $ ] for @xmath820 .",
    "then by taylor s theorem , @xmath821 & = \\psi ( 1 ) - \\psi ( 0 ) = \\int_0 ^ 1 \\psi'(t ) dt \\\\ & = \\frac{1}{2 } \\sum_{j=1}^{p}\\sum_{i=1}^{n}\\int_0 ^ 1{{\\mathrm{e}}}[\\partial_{j}m(z(t ) ) \\dot z_{ij}(t)]dt=\\frac{1}{2 } ( i + ii + iii),\\end{aligned}\\ ] ] where @xmath822 dt , \\\\ ii & = \\sum_{j , k=1}^{p } \\sum_{i=1}^{n } \\int_0 ^ 1 { { \\mathrm{e } } } [ \\partial_{j } \\partial_k   m(z^{(i)}(t ) )    \\dot z_{ij}(t ) z_{ik}(t ) ] dt , \\\\ iii & = \\sum_{j , k , l=1}^{p } \\sum_{i=1}^{n } \\int_0 ^ 1 \\int_0 ^ 1 ( 1-\\tau ) { { \\mathrm{e } } } [ \\partial_{j } \\partial_k \\partial_l m ( z^{(i)}(t ) + \\tau z_{i}(t ) ) \\dot z_{ij}(t ) z_{ik}(t ) z_{il}(t ) ] d \\tau dt.\\end{aligned}\\ ] ] note that random vector @xmath521 is independent of @xmath823 , and @xmath527 = 0 $ ] .",
    "hence we have @xmath528 ; moreover , since @xmath824=n^{-1}{{\\mathrm{e}}}[x_{ij}x_{ik}-y_{ij}y_{ik}]=0 $ ] by construction of @xmath115 , we also have @xmath825 .",
    "consider the third term @xmath540 .",
    "we have that @xmath826,\\end{aligned}\\ ] ] where ( 1 ) follows from @xmath827 holding by lemma [ lemma : bounds on derivatives of m ] , and ( 2 ) is shown below .",
    "the first claim of the theorem now follows .",
    "the second claim follows directly from property ( [ eq : smooth max property ] ) in the main text of the smooth max function .",
    "it remains to show ( 2 ) .",
    "define @xmath546 and note , @xmath828dt    \\\\ & = \\int_0 ^ 1 \\omega(t ) n { \\bar { { \\mathrm{e}}}}\\left [   \\max_{1{\\leqslant}j , k , l { \\leqslant}p }   |\\dot z_{ij}(t)/\\omega(t ) )   z_{ik}(t ) z_{il}(t)| \\right ] dt \\\\ & { \\leqslant}n \\int_0 ^ 1 \\omega(t ) { \\left(}{\\bar { { \\mathrm{e } } } } [   \\max_{1{\\leqslant}j { \\leqslant}p } | \\dot z_{ij}(t)/\\omega(t)|^3 ] { \\bar { { \\mathrm{e } } } } [   \\max_{1{\\leqslant}j { \\leqslant}p } | z_{ij}(t ) |^3 ] { \\bar { { \\mathrm{e } } } } [ \\max_{1{\\leqslant}j { \\leqslant}p } | z_{ij}(t ) |^3 ] { \\right)}^{1/3 } dt \\\\ & { \\leqslant}n^{-1/2 } \\left \\ { \\int_0 ^ 1 \\omega(t ) dt \\right \\ } { \\bar { { \\mathrm{e}}}}\\left [ \\max_{1{\\leqslant}j { \\leqslant}p }   { \\left(}| x_{ij}| + | y_{ij}|{\\right)}^3\\right ] \\end{aligned}\\ ] ] where the first inequality follows from hlder s inequality , and the second from the fact that @xmath829 , @xmath830 . finally we note that @xmath556 , so inequality ( 2 ) follows .",
    "this completes the overall proof .    in this proof ,",
    "let @xmath136 denote a generic constant depending only on @xmath208 and @xmath209 , and its value may change from place to place . for @xmath100",
    ", define @xmath831 .",
    "recall that @xmath832 . consider and fix a @xmath833-function @xmath834 $ ] such that @xmath835 for @xmath836 and @xmath837 for @xmath838 .",
    "fix any @xmath839 , and define @xmath840 . for this function @xmath841 , @xmath842 and @xmath843 .",
    "observe now that @xmath844 \\\\ & { \\leqslant}{{\\mathrm{e}}}[g(f_\\beta(y ) ) ] + c(\\psi^3+\\beta\\psi^2+\\beta^2\\psi)(n^{-1/2}{\\bar { { \\mathrm{e}}}}[s_i^3 ] ) \\\\ & { \\leqslant}{{\\mathrm{p}}}(f_\\beta(y ) { \\leqslant}t+e_\\beta+\\psi^{-1 } ) + c(\\psi^3+\\beta\\psi^2+\\beta^2\\psi)(n^{-1/2}{\\bar { { \\mathrm{e}}}}[s_i^3 ] ) \\\\ & { \\leqslant}{{\\mathrm{p}}}(z_{0 } { \\leqslant}t+e_\\beta+\\psi^{-1 } ) + c(\\psi^3+\\beta\\psi^2+\\beta^2\\psi)(n^{-1/2}{\\bar { { \\mathrm{e}}}}[s_i^3]).\\end{aligned}\\ ] ] where the first inequality follows from ( [ eq : smooth max property ] ) , the second from construction of @xmath841 , the third from theorem [ thm : warmup comparison ] , and the fourth from construction of @xmath841 , and the last from ( [ eq : smooth max property ] ) . the remaining step is to compare @xmath845 with @xmath46 and this is where lemma [ lem : anticoncentration ] plays its role . by lemma [ lem : anticoncentration ] , @xmath846 by which we have @xmath847 ) + ( e_{\\beta}+\\psi^{-1})\\sqrt{1 \\vee \\log ( p \\psi ) }   ] .\\ ] ] we have to minimize the right side with respect to @xmath270 and @xmath653 .",
    "it is reasonable to choose @xmath270 in such a way that @xmath848 and @xmath849 are balanced , i.e. , @xmath850 . with this @xmath270 ,",
    "the bracket on the right side is bounded from above by @xmath851 )   + \\psi^{-1 } \\sqrt{1 \\vee \\log ( p \\psi)}],\\ ] ] which is approximately minimized by @xmath852)^{-1/4}$ ] . with this @xmath653 , @xmath853)^{-1/4 } { \\leqslant}c n^{1/8}$ ]",
    "( recall that @xmath57 ) , and hence @xmath854 .",
    "therefore , @xmath855)^{1/4 } ( \\log ( pn))^{7/8}.\\ ] ] this gives one half of the claim .",
    "the other half follows similarly .",
    "the purpose of this section is to provide results without an assumption that @xmath66>c$ ] for all @xmath151 and some constant @xmath29 .      in this subsection",
    ", we use the same setup and notation as those in section [ sec : gaus vs nongaus ] .",
    "in particular , @xmath392 is a sequence of independent centered random vectors in @xmath393 , @xmath856 is a sequence of independent centered gaussian random vectors such that @xmath857={{\\mathrm{e}}}[x_ix_i^\\prime]$ ] , @xmath858 where @xmath859 , @xmath860 where @xmath861 , and @xmath862 in addition , denote @xmath863^{1/k}}{{\\bar { { \\mathrm{e}}}}[x_{ij}^2]^{1/2}}\\text { and } \\ell_n:=\\log(pn/\\gamma).\\ ] ] we will impose the following condition :    * there exists @xmath152 such that @xmath864 and for all @xmath865 with @xmath763 , @xmath66{\\geqslant}c_1 $ ] and @xmath866|{\\leqslant}(1-\\nu^\\prime)({\\bar { { \\mathrm{e}}}}[x_{ij}^2]{\\bar { { \\mathrm{e}}}}[x_{ik}^2])^{1/2}$ ] for some strictly positive constants @xmath867 , and @xmath149 independent of @xmath1 .",
    "[ thm : main_result1_extended ] suppose that condition ( sm ) holds .",
    "in addition , suppose that there is some constant @xmath868 such that @xmath66 { \\leqslant}c_{1}$ ] for @xmath147 . then for every @xmath120 , @xmath869 where @xmath611 are constants that depend only on @xmath870 and @xmath150 .",
    "theorem [ thm : main_result1_extended ] has the following applications .",
    "let @xmath31 be some constant that is independent of @xmath1 , and let @xmath179 be a sequence of constants .",
    "we allow for the case where @xmath180 as @xmath23 .",
    "we will assume that one of the following conditions is satisfied _ uniformly in _",
    "@xmath60 and @xmath130 :    * @xmath871 { \\leqslant}c_{1}$ ] and @xmath872 { \\leqslant}2 $ ] ; * @xmath871 { \\leqslant}c_{1}$ ] and @xmath873 { \\leqslant}2 $ ] .",
    "[ cor : central limit theorem extended ] suppose that there exist constants @xmath200 and @xmath201 such that one of the following conditions is satisfied : ( i ) ( e.5 ) holds and @xmath202 or ( ii ) ( e.6 ) holds and @xmath203 .",
    "in addition , suppose that condition ( sm ) holds and @xmath874 for some constants @xmath875 and @xmath600 .",
    "then there exist constants @xmath26 and @xmath136 depending only on @xmath876 , and @xmath877 such that @xmath206      in this subsection , we use the same setup and notation as those in section [ sec : multiplier bootstrap ] .",
    "in particular , in addition to the notation used above , we assume that random variables @xmath252 and @xmath785 satisfy conditions ( [ eq : statistic approximation ] ) and ( [ eq : conditional quantiles ] ) in the main text , respectively , where @xmath878 and @xmath879 depend on @xmath1 and where @xmath50 appearing in condition ( [ eq : conditional quantiles ] ) is defined in equation ( [ average - multiplier ] ) in the main text .",
    "recall that @xmath880-{\\bar { { \\mathrm{e}}}}[x_{ij}]|$ ] .",
    "[ thm : multiplier bootstrap ] suppose that there is some constant @xmath868 such that @xmath881{\\leqslant}c_1 $ ] for all @xmath151 .",
    "in addition , suppose that condition ( sm ) holds .",
    "moreover , suppose that conditions ( [ eq : statistic approximation ] ) and ( [ eq : conditional quantiles ] ) are satisfied .",
    "then for every @xmath241 , @xmath882 where @xmath883 and @xmath611 depend only on @xmath884 and @xmath209 .",
    "in addition , @xmath885    [ cor : multiplier bootstrap ] suppose that there exist constants @xmath886 such that conditions ( [ eq : statistic approximation ] ) and ( [ eq : conditional quantiles ] ) hold with @xmath261 .",
    "moreover , suppose that one of the following conditions is satisfied : ( i ) ( e.5 ) holds and @xmath202 or ( ii ) ( e.6 ) holds and @xmath203 .",
    "finally , suppose that condition ( sm ) holds and @xmath874 for some constants @xmath875 and @xmath600 .",
    "then there exist constants @xmath26 and @xmath27 depending only on @xmath887 , and @xmath877 such that @xmath263 in addition , @xmath264 .",
    "the proofs rely on the following auxiliary lemmas , whose proofs will be given below .",
    "[ lem : anticoncentration_extended ] ( a ) let @xmath127 be jointly gaussian random variables with @xmath128=0 $ ] and @xmath129 $ ] for all @xmath130 .",
    "let @xmath888 $ ] and @xmath889 .",
    "assume that @xmath890 for some @xmath891 .",
    "then for every @xmath134 , @xmath892 where @xmath611 are some constants depending only on @xmath149 and @xmath138 .",
    "( b ) furthermore , the worst case bound is obtained by bounding @xmath893 by @xmath894 .",
    "[ lem : gaussian comparison extended ] let @xmath215 and @xmath19 be centered gaussian random vectors in @xmath393 with covariance matrices @xmath895 and @xmath896 , respectively .",
    "let @xmath897 .",
    "suppose that there are some constants @xmath898 such that @xmath899{\\leqslant}c_1 $ ] for all @xmath151 and @xmath900{\\geqslant}c_1\\sqrt{\\log p}$ ] .",
    "then there exist constants @xmath29 and @xmath136 depending only on @xmath149 and @xmath150 such that @xmath901    it follows from theorem 2.3.16 in @xcite that condition ( sm ) implies that @xmath902{\\geqslant}c\\sqrt{\\log p}$ ] for some @xmath29 that depends only on @xmath867 , and @xmath149 .",
    "therefore , using the argument like that in the proof of theorem [ cor : gaussian to nongaussian ks 2 ] with an application of lemma [ lem : anticoncentration_extended ] instead of lemma [ lem : anticoncentration ] , we obtain @xmath903 \\label{key to}\\end{aligned}\\ ] ] where all notation is taken from the proof of theorem [ cor : gaussian to nongaussian ks 2 ] .",
    "recall that @xmath904 is any function satisfying @xmath905 for all @xmath103 and @xmath906 .",
    "to bound @xmath105 , we have @xmath907)^{1/2 } \\ } ] & { \\leqslant}{\\bar { { \\mathrm{e}}}}[x_{ij}^4]/(u^2{\\bar { { \\mathrm{e}}}}[x_{ij}^2])\\\\ & = { \\bar { { \\mathrm{e}}}}[x_{ij}^4]/(u^2{\\bar { { \\mathrm{e}}}}[x_{ij}^2]^2){\\bar { { \\mathrm{e}}}}[x_{ij}^2]{\\leqslant}(m_{4,2}^4/u^2){\\bar { { \\mathrm{e}}}}[x_{ij}^2].\\end{aligned}\\ ] ] this implies that @xmath908 . to bound @xmath112 , note that @xmath909{\\leqslant}3{\\bar { { \\mathrm{e}}}}[x_{ij}^4]$ ] , which was shown in the proof of theorem [ cor : gaussian to nongaussian ks 2 ] .",
    "in addition , @xmath910={\\bar { { \\mathrm{e}}}}[x_{ij}^2]$ ] . therefore , @xmath911 .",
    "hence , we can set @xmath912 for all @xmath103 .",
    "the rest of the proof is the same as that for theorem [ cor : gaussian to nongaussian ks 2 ] with @xmath177 replaced by @xmath913 .",
    "note that in both cases , @xmath914 and @xmath915{\\leqslant}m_{3,2}^3\\max_{1{\\leqslant}j{\\leqslant}p}{\\bar { { \\mathrm{e}}}}[x_{ij}^2]^{3/2}{\\leqslant}cm_{3,2}^3{\\leqslant}cb_n.\\ ] ] therefore , the claim of the corollary follows from theorem [ cor : central limit theorem extended ] by the same argument as that leading to corollary [ cor : central limit theorem ] from theorem [ cor : gaussian to nongaussian ks 2 ] .",
    "the proof is the same as that for theorem [ thm : multiplier bootrstrap ii ] with lemmas [ lem : anticoncentration_extended ] and [ lem : gaussian comparison extended ] replacing lemmas [ lem : anticoncentration ] and [ lemma : distances gaussian to gaussian ] .    since @xmath916 ,",
    "both under ( e.5 ) and under ( e.6 ) we have @xmath917 .",
    ". then conditions ( [ eq : statistic approximation ] ) and ( [ eq : conditional quantiles ] ) hold with @xmath919 replacing @xmath920 and @xmath921 .",
    "further , since @xmath874 , we have @xmath922 .",
    "let @xmath923)^{1/2}/\\log p)\\vee n^{-1}$ ] . then @xmath924 .",
    "in addition , if @xmath925 , then @xmath926 .",
    "finally , @xmath927^{1/2}{\\leqslant}m_{4,2}^2\\max_{1{\\leqslant}j{\\leqslant}p}{\\bar { { \\mathrm{e}}}}[x_{ij}^2]{\\leqslant}cm_{4,2}^2{\\leqslant}cb_n.\\ ] ] the rest of the proof is similar to that for corollary [ cor : multiplier bootstrap examples ] .    in the proof",
    ", several constants will be introduced .",
    "all of these constants are implicitly assumed to depend only on @xmath149 and @xmath138 .",
    "we choose @xmath29 such that @xmath928 .",
    "fix @xmath929 .",
    "it suffices to consider the case @xmath930 .",
    "let @xmath931 for sufficiently small @xmath200 to be chosen below .",
    "note that @xmath932 .",
    "so , if @xmath933 , ( [ eq : anticoncentration_extended ] ) holds trivially by selecting sufficiently large @xmath706 .",
    "consider the case @xmath934 .",
    "assume that @xmath935 .",
    "then @xmath936 where the last inequality follows from borell s inequality .",
    "so , ( [ eq : anticoncentration_extended ] ) holds by selecting sufficiently large @xmath706",
    ".    now assume that @xmath937 .",
    "then @xmath938 where the last inequality follows from borell s inequality .",
    "so , ( [ eq : anticoncentration_extended ] ) holds by selecting sufficiently large @xmath706 .    finally , assume that @xmath939 then @xmath940 where @xmath941 and @xmath942 .",
    "consider @xmath273 .",
    "we have @xmath943 where ( 1 ) holds by assumption , ( 2 ) follows from the definition of @xmath705 , and ( 3 ) holds because @xmath930 .",
    "in addition , there exists @xmath201 such that @xmath944{\\leqslant}c_2c_2b_p$ ] , and there exist @xmath600 such that @xmath945 , so that @xmath946 .",
    "we choose @xmath447 so that @xmath947 , @xmath948 , and @xmath949 . then @xmath950 , @xmath951 , and @xmath944{\\leqslant}b_p/4 $ ] . also recall that @xmath934 .",
    "therefore , @xmath952 & { \\geqslant}b_p/2-\\bar{\\sigma}\\sqrt{2\\log(\\bar{\\sigma}/\\varsigma)}\\\\ & { \\geqslant}\\bar{\\sigma}\\sqrt{2\\log(\\bar{\\sigma}/\\varsigma ) } { \\geqslant}\\underline{\\sigma}\\sqrt{2\\log(\\underline{\\sigma}/\\varsigma)}.\\end{aligned}\\ ] ] so , borell s inequality yields @xmath953 because @xmath954 for all @xmath955 .",
    "consider @xmath800 .",
    "it is proved in @xcite that @xmath956 where @xmath957 $ ] .",
    "see , in particular , equation ( 16 ) in that paper .",
    "note that @xmath958",
    ". therefore , ( [ eq : standard_anticoncentration ] ) combined with our restriction on @xmath70 yields @xmath959 where in the second line we used the facts that @xmath934 and @xmath950 by assumption and by construction , respectively . now ( [ eq : anticoncentration_extended ] ) holds by selecting sufficiently large @xmath136 , and using the fact that @xmath960 .",
    "this completes the proof .",
    "the proof is the same as that for theorem 2 in @xcite with lemma [ lem : anticoncentration_extended ] replacing lemma [ lem : anticoncentration ] .",
    "in this section , we study the validity of the empirical ( or efron s ) bootstrap in approximating the distribution of @xmath44 in the simple case where @xmath5 s are uniformly bounded ( the bound can increase with @xmath1 ) .",
    "moreover , we consider here the asymptotics where @xmath23 and possibly @xmath24 .",
    "recall the setup in section [ sec : gaus vs nongaus ] : let @xmath2 be independent centered random vectors in @xmath3 and define @xmath961 the empirical bootstrap procedure is described as follows .",
    "let @xmath962 be i.i.d",
    ". draws from the empirical distribution of @xmath2 .",
    "conditional on @xmath94 , @xmath962 are i.i.d . with mean @xmath963 $ ] and covariance matrix @xmath964)(x_{i}-{{\\mathbb{e}_n } } [ x_{i } ] ) ' ] $ ] .",
    "define @xmath965).\\ ] ] the empirical bootstrap approximates the distribution of @xmath44 by the conditional distribution @xmath966 given @xmath94 .",
    "recall @xmath967 where @xmath51 are i.i.d .",
    "@xmath224 random variables independent of @xmath94",
    ". we shall here compare the conditional distribution of @xmath966 to that of @xmath968 .",
    "[ prop : efron ] suppose that there exists constants @xmath969 and a sequence @xmath179 of constants such that @xmath970 { \\leqslant}c_{1}$ ] for all @xmath130 and @xmath971 for all @xmath60 and @xmath130 .",
    "then provided that @xmath972 , with probability @xmath973 , @xmath974    this theorem shows the asymptotic equivalence of the empirical and gaussian multiplier bootstraps .",
    "the validity of the empirical bootstrap ( in the form similar to that in theorem [ thm : multiplier bootrstrap i ] ) follows relatively directly from the validity of the gaussian multiplier bootstrap .",
    "the proof consists of three steps .    *",
    "step 1*. we first show that with probability @xmath973 , @xmath975)^{2 } ] { \\leqslant}2 c_{1}$ ] for all @xmath130 . by lemma [ lem : symmetrization inequality 1 ] , @xmath976 | \\right ] \\lesssim \\sqrt{c_{1 }",
    "( \\log p)/n } + b_{n}(\\log p)/n = o((\\log p)^{-1/2 } ) , \\\\ & { { \\mathrm{e}}}\\left [ \\max_{1 { \\leqslant}j { \\leqslant}p } |{{\\mathbb{e}_n}}[x_{ij}^{2 } ] - { \\bar { { \\mathrm{e } } } } [ x_{ij}^{2 } ] | \\right ] \\lesssim \\sqrt{c_{1}b_{n}^{2 } ( \\log p)/n } + b_{n}^{2}(\\log p)/n = o(1),\\end{aligned}\\ ] ] so that uniformly in @xmath130 , @xmath977)^{2 } ] - { \\bar { { \\mathrm{e } } } } [ x_{ij}^{2 } ] | = o_{{{\\mathrm{p}}}}(1)$ ] , which implies the desired assertion .    *",
    "step 2*. define @xmath978).\\ ] ] we show that with probability @xmath973 , @xmath979 conditional on @xmath94 , @xmath980 $ ] are independent centered random vector in @xmath3 with covariance matrix @xmath981)(x_{i } - { { \\mathbb{e}_n}}[x_{i } ] ) ' ] $ ] . hence conditional on @xmath94 , we can apply corollary [ cor : central limit theorem ] to @xmath966 to deduce ( [ conditionalgar ] ) .    *",
    "step 3*. we show that with probability @xmath973 , @xmath982 by definition , we have @xmath983 | \\times   \\left | \\frac{1}{\\sqrt{n } } \\sum_{i=1}^{n } e_{i }   \\right | = o_{{{\\mathrm{p}}}}((\\log p)^{-1/2}).\\ ] ] hence using the anti - concentration inequality together with step 1 , we deduce the desired assertion .",
    "the conclusion of theorem [ prop : efron ] follows from combining steps 1 - 3 .",
    "we first point out that our gaussian approximation result ( [ eq : main result ] ) can be viewed as a version of multivariate central limit theorem , which is concerned with conditions under which @xmath984 uniformly in a collection of sets @xmath89 , typically _ all _ convex sets .",
    "recall that @xmath985 where @xmath2 are independent centered random vectors in @xmath3 with possibly @xmath24 , and @xmath986 @xmath987 independent random vectors with @xmath988)$ ] .",
    "in fact , the result ( [ eq : main result ] ) in the main text can be rewritten as @xmath989 where @xmath990 .",
    "hence , our paper contributes to the literature on multivariate central limit theorems with growing number of dimensions ( see , among others , @xcite ) .",
    "these papers are concerned with results of the form ( [ eq : mclt ] ) , but either explicitly or implicitly require the condition that @xmath991 for some @xmath26 ( when specialized to a setting like our setup ) .",
    "results in these papers rely on the anti - concentration results for gaussian random vectors on the @xmath277-expansions of boundaries of arbitrary convex sets @xmath89 ( see @xcite ) .",
    "we restrict our attention to the class of sets of the form @xmath992 in ( [ eq : mclt2 ] ) .",
    "these sets have a special structure that allows us to deal with the case where @xmath42 : in particular , concentration of measure on the @xmath277-expansion of boundary of @xmath992 is at most of order @xmath993\\ ] ] for gaussian random vectors with unit variance ( and separable gaussian processes more generally ) , as shown in @xcite ( see also lemma [ lem : anticoncentration ] )",
    ".    there is large literature on bounding the difference : @xmath994 , in particular the recent work includes @xcite . any such bounds lead to gaussian approximations , though the structure of @xmath995 s plays an important role in limiting the scope of this approximation .",
    "two methods in the literature that turned out most fruitful for deriving gaussian approximation results in high dimensional settings ( @xmath996 as @xmath997 in our context ) are those of lindeberg and stein .",
    "the history of the lindeberg method dates back to lindeberg s original proof of the central limit theorem ( @xcite ) , which has been revived in the recent literature .",
    "we refer to the introduction of @xcite for a brief history on the lindeberg s method ; see also @xcite .",
    "the recent development on stein s method when @xmath7 s are multivariate can be found in @xcite .",
    "see also @xcite for a comprehensive overview of different methods .",
    "in contrast to these papers , our paper analyzes a rather particular , yet important case @xmath998 , with @xmath117 ( progressively less ) smooth function and @xmath999 , and in our case , self - normalized truncation , some fine properties of the smooth potential @xmath475 , maximal fourth order moments of variables and of their envelops , play a critical role ( as we comment further below ) , and so our main results can not be ( immediately ) deduced from these prior results ( nor do we attempt to follow this route ) .    using the lindeberg s method and the smoothing technique of bentkus @xcite",
    ", @xcite derived in their theorem 5 a gaussian approximation result that is of similar form to our _ simple _ ( non - main ) gaussian approximation result presented in section [ sec : elementary gar ] of the sm .",
    "however , in @xcite , the anti - concentration property is _ assumed _ ( see equation ( 1.4 ) in their paper ) ; in our notation , their assumption on anti - concentration says that there exists a constant @xmath706 independent of @xmath1 and @xmath0 such that @xmath1000 this assumption is useful for the analysis of the donsker case , but does not apply in our ( non - donsker ) cases .",
    "in fact , it rules out the simple case where @xmath127 are independent ( i.e. , the coordinates in @xmath7 are uncorrelated ) or @xmath127 are weakly dependent ( i.e. , the coordinates in @xmath7 are weakly correlated ) . in addition , it is worth pointing out that the use of bentkus s @xcite smoothing , which is readily established for this smoothing method . ] instead of the smoothing by potentials from spin glasses used here , does not lead to optimal results in our case , since very subtle properties ( stability property noted in lemma [ lemma : switching property ] ) of potentials play an important role in our proofs , and in particular , is crucial for getting a reasonable exponent in the dependence on @xmath813 .",
    "chatterjee @xcite , who also used the lindeberg method , analyzed a spin - glass example like our example ( e.4 ) and also derived a result similar to our _ simple _ ( non - main ) gaussian approximation result presented in section [ sec : elementary gar ] of the sm , where @xmath1001 , where @xmath1002 are fixed and @xmath1003 are i.i.d @xmath810-valued and centered with bounded third moment .",
    "we note @xcite only provided a result for smooth functionals , but the extension to non - smooth cases follows from our lemma [ lem : anticoncentration ] along with standard kernel smoothing .",
    "in fact , all of our paper is inspired by chatterjee s work , and an early version employed ( combinatorial ) lindeberg s method .",
    "we discuss the limitations of lindeberg s approach ( in the canonical form ) in the main text , where we motivate the use of a combination of slepian - stein method in conjunction with self - normalized truncation and subtle properties of the potential function that approximates the maximum function .",
    "generalization of results of chatterjee to the case where @xmath810-valued @xmath1003 are locally dependent are given in @xcite , who uses slepian - stein methods for proofs and gave a result similar to our _ simple _ ( non - main ) gaussian approximation result presented in section [ sec : elementary gar ] of the sm .",
    "like @xcite , we also use a version of slepian - stein methods , but the proof details ( as well as results and applications ) are quite different , since we instead analyze the case where the data @xmath114 are @xmath393-valued ( instead of @xmath79-valued ) independent vectors , and since we have to perform truncation ( to get good dependencies on the size of the envelopes , @xmath1004 ) and use subtle properties of the potential function to get our main results ( to get good dependencies on @xmath813 ) .    using an interesting modification of the lindeberg method , @xcite obtained an invariance principle for a sequence of sub - gaussian @xmath810-valued random variables @xmath1003 ( instead of @xmath811-valued case consider here ) .",
    "specifically , they looked at the large - sample probability of @xmath1003 hitting a polytope formed by @xmath0 half - spaces , which is on the whole a different problem than studied here ( though tools are insightful , e.g. the novel use of results developed by nazarov @xcite ) .",
    "these results have no intersection with our results , except for a special case of  sub - exponential regression / spin - glass example \" ( e.3 ) , if we further require in that example , that @xmath1005 for all @xmath151 , that @xmath756 s are sub - gaussian , that @xmath1006=0 $ ] , and that @xmath1007 .",
    "all of these conditions and especially the last one are substantively more restrictive than what is obtained for the example ( e.3 ) in our corollary [ cor : central limit theorem ] .",
    "finally , we note that when @xmath4 s are identically distributed in addition to being independent , the theory of strong approximations and , in particular , hungarian coupling can also be used to obtain results like that in ( [ eq : main result ] ) in the main text under conditions permitting @xmath42 ; see , for example , theorem 3.1 in koltchinskii @xcite and rio @xcite .",
    "however , in order for this theory to work , @xmath4 have to be well approximable in a haar basis when considered as functions on the underlying probability space ",
    "e.g. , @xmath1008 , where @xmath1009 should have a total variation norm with respect to @xmath1010 ( where @xmath1011 is fixed ) that does not grow too quickly to enable the expansion in the haar basis .",
    "this technique has been proven fruitful in many applications , but this requires a radically different structure than what our leading applications impose ; instead our results , based upon slepian - stein methods , are more readily applicable in these settings ( instead of controlling total variation bounds , they rely on control of maxima moments and moments of envelopes of @xmath1012 ) . for further theoretical comparisons of the two methods in the context of strong approximations of suprema of non - donsker emprical processes by those of gaussian processes , in the classical kernel and series smoothing examples , we refer to our companion work @xcite ( there is no winner in terms of guaranteed rates of approximation , though side conditions seem to be weaker for the slepian - stein type methods ; in particular hungarian couplings often impose the boundedness conditions , e.g. @xmath1013 ) .    supplementary material iii +   + * additional application * +",
    "in this section , we study the problem of adaptive specification testing .",
    "let @xmath1014 be a sample of independent random pairs where @xmath16 is a scalar dependent random variable , and @xmath1015 is a vector of non - stochastic covariates .",
    "the null hypothesis , @xmath1016 , is that there exists @xmath1017 such that @xmath1018=v_{i}'\\beta;\\ , i=1,\\dots , n.\\label{eq : null}\\ ] ] the alternative hypothesis , @xmath1019 , is that there is no @xmath270 satisfying ( [ eq : null ] ) .",
    "we allow for triangular array asymptotics so that everything in the model may depend on @xmath1 . for brevity , however , we omit index @xmath1 .",
    "let @xmath1020 $ ] , @xmath1021 .",
    "then @xmath1022=0 $ ] , and under @xmath1016 , @xmath1023 . to test @xmath1016 ,",
    "consider a set of test functions @xmath1024 , @xmath76 .",
    "let @xmath1025 .",
    "we choose test functions so that @xmath1026=0 $ ] and @xmath1027=1 $ ] for all @xmath76 . in our analysis , @xmath0 may be higher or even much higher than @xmath1 .",
    "let @xmath1028)^{-1}({{\\mathbb{e}_n}}[v_{i}y_{i}])$ ] be an ols estimator of @xmath270 , and let @xmath1029 be corresponding residuals .",
    "our test statistic is @xmath1030}}.\\ ] ] the test rejects @xmath1016 if @xmath252 is significantly large .",
    "note that since @xmath1026=0 $ ] , we have @xmath1031 therefore , under @xmath1016 , @xmath1032}}.\\ ] ] this suggests that we can use the multiplier bootstrap to obtain a critical value for the test .",
    "more precisely , let @xmath223 be a sequence of independent @xmath224 random variables that are independent of the data , and let @xmath1033}}.\\ ] ] the multiplier bootstrap critical value @xmath351 is the conditional @xmath412-quantile of @xmath785 given the data . to prove the validity of multiplier bootstrap",
    ", we will impose the following condition :    * there are some constants @xmath1034 , and a sequence @xmath179 of constants such that for all @xmath60 , @xmath130 , @xmath1035 : ( i ) @xmath323 ; ( ii ) @xmath191= 1 $ ] ; ( iii ) @xmath1036 { \\leqslant}\\bar \\sigma^2 $ ] ; ( iv ) @xmath1037 ; ( v ) @xmath1038 ; and ( vi ) the minimum eigenvalue of @xmath1039 $ ] is bounded from below by @xmath208 .",
    "the literature on specification testing is large . in particular",
    ", @xcite and @xcite developed adaptive tests that are suitable for inference in @xmath1048-norm .",
    "in contrast , our test is most suitable for inference in @xmath1049-norm .",
    "an advantage of our procedure is that selecting a wide class of test functions leads to a test that can effectively adapt to a wide range of alternatives , including those that can not be well - approximated by hlder - continuous functions .",
    "we only consider case ( a ) .",
    "the proof for case ( b ) is similar and hence omitted . in this proof ,",
    "let @xmath1050 denote generic positive constants depending only on @xmath1051 and their values may change from place to place .",
    "let @xmath1052 } }   \\ \\text{and } \\",
    "w_0:=\\max_{1{\\leqslant}j{\\leqslant}p}\\frac{|\\sum_{i=1}^nz_{ij}{\\varepsilon}_{i}e_{i}/\\sqrt{n}|}{\\sqrt{{{\\mathbb{e}_n}}[z_{ij}^2\\sigma^2_{i}]}}.\\ ] ] we make use of corollary [ cor : multiplier bootstrap examples]-(ii ) . to this end , we shall verify conditions ( [ eq : statistic approximation ] ) and ( [ eq : conditional quantiles ] ) in section [ sec : multiplier bootstrap ] of the main text , which will be separately done in steps 1 and 2 , respectively .      by corollary [ cor : central limit theorem]-(ii ) , we have @xmath1055 uniformly in @xmath839 . by the gaussian concentration inequality ( proposition a.2.1",
    "* ) , for every @xmath1056 , we have @xmath1057 + c t   \\right ) { \\leqslant}e^{-t^{2}}.\\ ] ] since @xmath1058 { \\leqslant}c \\sqrt{\\log p}$ ] , we conclude that @xmath1059 moreover , @xmath1060 & = { { \\mathbb{e}_n}}[z_{ij}^2({\\widehat}{{\\varepsilon}}_{i}-{\\varepsilon}_{i})^2]+{{\\mathbb{e}_n}}[z_{ij}^2({\\varepsilon}_{i}^2-\\sigma_{i}^2)]+2{{\\mathbb{e}_n}}[z_{ij}^2{\\varepsilon}_{i}({\\widehat}{{\\varepsilon}}_{i}-{\\varepsilon}_{i } ) ] \\\\ & = : i_{j } + ii_{j } + iii_{j}.\\end{aligned}\\ ] ]    consider @xmath1061 .",
    "we have @xmath1062\\vert^2,\\ ] ] where ( 1 ) follows from assumption s-(ii ) , ( 2 ) from s-(iv ) and s-(v ) , and ( 3 ) from s-(vi ) . since @xmath1063\\vert^2 ] { \\leqslant}c / n$ ] , by markov s inequality , for every @xmath1056 , @xmath1064 > t",
    "\\right ) { \\leqslant}c/(nt).\\ ] ]      consider @xmath1067 .",
    "we have @xmath1068|{\\leqslant}2\\vert{{\\mathbb{e}_n}}[z_{ij}^2{\\varepsilon}_{i}v_{i}]\\vert\\vert{\\widehat}{\\beta}-\\beta\\vert$ ] . hence @xmath1069| >",
    "t \\right ) \\notag \\\\ & { \\leqslant}{{\\mathrm{p}}}\\left(\\max_{1{\\leqslant}j{\\leqslant}p}\\vert { { \\mathbb{e}_n}}[z_{ij}^2{\\varepsilon}_{i}v_{i}]\\vert > t",
    "\\right)+ { { \\mathrm{p } } } ( \\| { \\widehat}{\\beta } - \\beta \\| > 1 ) \\notag \\\\ & { \\leqslant}c [ b_{n}^2(\\log p)/(\\sqrt{n}t ) + 1/n ] .\\label{eq : step1 - 4}\\end{aligned}\\ ] ]    by ( [ eq : step1 - 2])-([eq : step1 - 4 ] ) , we have @xmath1070 | >",
    "t \\right ) { \\leqslant}c [   b_{n}^2(\\log p)/(\\sqrt{n}t ) + 1/(nt ) + 1/n ] .",
    "\\label{eq : step1 - 5}\\ ] ] in particular , @xmath1070 | > \\underline{\\sigma}^{2}/2 \\right ) { \\leqslant}cn^{-c}.\\ ] ] since @xmath1071 { \\geqslant}\\underline{\\sigma}^{2 } > 0 $ ] ( which is guaranteed by s-(iii ) and s-(ii ) ) , on the event @xmath1072 | { \\leqslant}\\underline{\\sigma}^{2}/2 $ ] , we have @xmath1073 { \\geqslant}\\min_{1 { \\leqslant}j { \\leqslant}p } { { \\mathbb{e}_n } } [ z_{ij}^{2 } \\sigma_{i}^{2 } ] - \\underline{\\sigma}^{2}/2 { \\geqslant}\\underline{\\sigma}^{2}/2,\\ ] ] and hence @xmath1074}-\\sqrt{{{\\mathbb{e}_n } } [ z_{ij}^{2 } { \\widehat}{{\\varepsilon}}_{i}^{2 } ] } \\right | \\times",
    "t_{0 } \\\\ & { \\leqslant}c \\max_{1 { \\leqslant}j { \\leqslant}p }   | { { \\mathbb{e}_n } } [ z_{ij}^{2 } \\sigma_{i}^{2 } ] -{{\\mathbb{e}_n } } [ z_{ij}^{2 } { \\widehat}{{\\varepsilon}}_{i}^{2 } ] | \\times t_{0},\\end{aligned}\\ ] ] where the last step uses the simple fact that @xmath1075 ) and ( [ eq : step1 - 5 ] ) , for every @xmath1056 , @xmath1076.\\ ] ] by choosing @xmath1077 with sufficiently small @xmath1078 , we obtain the claim of this step .      for @xmath1079 , consider the event @xmath1080|{\\leqslant}t , \\max_{1{\\leqslant}i{\\leqslant}p}({\\widehat}{{\\varepsilon}}_{i}-{\\varepsilon}_{i})^2{\\leqslant}t^{2 } \\right \\}.\\ ] ] by calculations in step 1 , @xmath1081 $ ] .",
    "we shall show that , on this event , @xmath1082 for ( [ step2 - 1 ] ) , by the gaussian concentration inequality , for every @xmath1083 , @xmath1084 + cs   \\right ) { \\leqslant}e^{-s^{2}}.\\ ] ] where we have used the fact @xmath1085={{\\mathbb{e}_n}}[z_{ij}^2\\sigma_{i}^2]+{{\\mathbb{e}_n}}[z_{ij}^2({\\widehat}{{\\varepsilon}}_{i}^2-\\sigma_{i}^2)]{\\leqslant}\\bar{\\sigma}^{2}+t { \\leqslant}\\bar{\\sigma}^{2 } + \\underline{\\sigma}^{2}/2 $ ] on the event @xmath1086 . here",
    "$ ] means the expectation with respect to @xmath223 conditional on @xmath1088 .",
    "moreover , on the event @xmath1086 , @xmath1089 { \\leqslant}c \\sqrt{\\log p}.\\ ] ] hence by choosing @xmath1090 , we obtain ( [ step2 - 1 ] ) .",
    "inequality ( [ step2 - 2 ] ) follows similarly , by noting that @xmath1091)^{1/2}{\\leqslant}\\max_{1{\\leqslant}i{\\leqslant}n}|{\\widehat}{{\\varepsilon}}_{i}-{\\varepsilon}_{i}|{\\leqslant}t$ ] on the event @xmath1086 .",
    "define @xmath1092}}.\\ ] ] note that @xmath1071 { \\geqslant}\\underline{\\sigma}^{2}$ ] . since on the event @xmath1086 , @xmath1093| { \\leqslant}t { \\leqslant}\\underline{\\sigma}^{2}/2 $ ] , in view of step 1 , on this event",
    ", we have @xmath1094 therefore , by ( [ step2 - 1 ] ) and ( [ step2 - 2 ] ) , on the event @xmath1086 , we have @xmath1095 by choosing @xmath1096 with sufficiently small @xmath26 , we obtain the claim of this step .    * step 3 .",
    "* steps 1 and 2 verified conditions ( [ eq : statistic approximation ] ) and ( [ eq : conditional quantiles ] ) in section [ sec : multiplier bootstrap ] of the main text . theorem [ thm : ast ] case ( a ) follows from corollary [ cor : multiplier bootstrap examples]-(ii ) .",
    "99 asriev , a.v . and rotar , v.i .",
    "( 1989 ) . on the convergence rate in the infinite - dimensional central limit theorem for probabilities of hitting parallelepipeds .",
    "_ theory of probability and its applications _ * 30 * 691 - 701 .",
    "ball , k. ( 1993 ) .",
    "the reverse isoperimetric problem for gaussian measure .",
    "_ discrete comput .",
    "_ * 10 * 411 - 420 .",
    "bentkus , v. ( 1990 ) .",
    "smooth approximations of the norm and differentiable functions with bounded support in banach space @xmath1097 .",
    "math . journal _ * 30 * 223 - 230 .",
    "bentkus , v. ( 2003 ) . on the dependence of the berry - esseen bound on dimension .",
    "_ j. statist . plann .",
    "_ * 113 * 385 - 402 .",
    "bentkus , v. , gtze , f. , paulauskas , v. , and rackauskus , l. ( 2000 ) .",
    "the accuracy of gaussian approximation in banach spaces . in : _",
    "limit theorems of probability theory _ ( eds .",
    "y.v . prokholov and v. statuleviius ) , springer , pp . 25 - 111 .",
    "chatterjee , s. ( 2005a ) . a simple invariance theorem .",
    "arxiv : math/0508213 .",
    "chatterjee , s. ( 2006 ) .",
    "a generalization of lindeberg s principle .",
    "probab . _ * 34 * 2061 - 2076 .",
    "chatterjee , s. and meckes , e. ( 2008 ) .",
    "multivariate normal approximation using exchangeable pairs .",
    "_ alea lat .",
    "j. probab .",
    "* 4 * 257 - 283 .",
    "chen , l. , and fang , x. ( 2011 ) .",
    "multivariate normal approximation by stein s method : the concentration inequality approach .",
    "chernozhukov , v. , chetverikov , d. and kato , k. ( 2012a ) .",
    "gaussian approximation of suprema of empirical processes .",
    "chernozhukov , v. , chetverikov , d. and kato , k. ( 2012b ) .",
    "comparison and anti - concentration bounds for maxima of gaussian random vectors .",
    "submitted to _ probab .",
    "theory related fields_. de la pea , v. , lai , t. and shao , q .- m .",
    "_ self - normalized processes : limit theory and statistical applications_. springer .",
    "dudley , r.m .",
    "_ uniform central limit theorems_. cambridge university press .",
    "goldstein , l. , and rinott , y. ( 1996 ) multivariate normal approximations by stein s method and size bias couplings . _ j. appl .",
    "probab . _ * 33 * 1 - 17 .",
    "gtze , f. ( 1991 ) . on the rate of convergence in the multivariate clt .",
    "_ * 19 * 724 - 739 .",
    "guerre , e. and lavergne , p. ( 2005 ) .",
    "data - driven rate - optimal specification testing in regression models . _ ann .",
    "statist . _",
    "* 33 * 840 - 870 .",
    "harsha , p. , klivans , a. , and meka , r. ( 2012 ) .",
    "an invariance principle for polytopes .",
    "_ journal of the acm _ * 59*. horowitz , j. l. and spokoiny , v.g .",
    "( 2001 ) . an adaptive , rate - optimal test of a parametric mean - regression model against a nonparametric alternative .",
    "_ econometrica _ * 69 * 599 - 631 .",
    "koltchinskii , v.i .",
    "komls - major - tusndy approximation for the general empirical process and haar expansions of classes of functions .",
    "_ j. theoret .",
    "probab . _ * 7 * 73 - 118 .",
    "lindeberg , j. w. ( 1922 ) .",
    "eine neue herleitung des exponentialgesetzes in der wahrscheinlichkeitsrechnung .",
    "z. _ * 15 * 211 - 225 .",
    "nagaev , s. ( 1976 ) .",
    "an estimate of the remainder term in the multidimensional central limit theorem .",
    "third japan - ussr symp .",
    "theory_. lecture notes in math .",
    "419 - 438 .",
    "norvaisa , r. and paulauskas , v. ( 1991 ) . rate of convergence in the central limit theorem for empirical processes .",
    "_ journal of theoretical probability . _ * 4 * 511 - 534 .",
    "nazarov , f. ( 2003 ) . on the maximal perimeter of a convex set in @xmath1098 with respect to a gaussian measure . in : _ geometric aspects of functional analysis _ , lecture notes in mathematics vol .",
    "1807/2003 , springer , pp .",
    "169 - 187 .",
    "portnoy , s. ( 1986 ) . on the central limit theorem in @xmath3",
    "when @xmath55 .",
    "theory related fields _ * 73 * 571 - 583 .",
    "reinert , g. and rllin , a. ( 2009 ) .",
    "multivariate normal approximation with stein s method of exchangeable pairs under a general linearity condition .",
    "probab . _ * 37 * 2150 - 2173 .",
    "rio , e. ( 1994 ) .",
    "local invariance principles and their application to density estimation .",
    "theory related fields _ * 98 * 21 - 45 .",
    "rllin , a. ( 2011 ) .",
    "stein s method in high dimensions with applications , _ ann .",
    ". h. poincar probab .",
    "_ * 49 * , 529 - 549 .",
    "stein , c. ( 1981 ) .",
    "estimation of the mean of a multivariate normal distribution . _ ann .",
    "_ * 9 * 1135 - 1151 .",
    "talagrand , m. ( 2003 ) .",
    "_ spin glasses : a challenge for mathematicians_. springer .",
    "van der vaart , a.w . and wellner , j.a .",
    "_ weak convergence and empirical processes : with applications to statistics_. springer ."
  ],
  "abstract_text": [
    "<S> we derive a gaussian approximation result for the maximum of a sum of high dimensional random vectors . specifically , we establish conditions under which the distribution of the maximum is approximated by that of the maximum of a sum of the gaussian random vectors with the same covariance matrices as the original vectors . </S>",
    "<S> this result applies when the dimension of random vectors ( @xmath0 ) is large compared to the sample size ( @xmath1 ) ; in fact , @xmath0 can be much larger than @xmath1 , without restricting correlations of the coordinates of these vectors . </S>",
    "<S> we also show that the distribution of the maximum of a sum of the random vectors with unknown covariance matrices can be consistently estimated by the distribution of the maximum of a sum of the conditional gaussian random vectors obtained by multiplying the original vectors with i.i.d . </S>",
    "<S> gaussian multipliers . </S>",
    "<S> this is the gaussian multiplier ( or wild ) bootstrap procedure . here </S>",
    "<S> too , @xmath0 can be large or even much larger than @xmath1 . </S>",
    "<S> these distributional approximations , either gaussian or conditional gaussian , yield a high - quality approximation to the distribution of the original maximum , often with approximation error decreasing polynomially in the sample size , and hence are of interest in many applications . </S>",
    "<S> we demonstrate how our gaussian approximations and the multiplier bootstrap can be used for modern high dimensional estimation , multiple hypothesis testing , and adaptive specification testing . </S>",
    "<S> all these results contain non - asymptotic bounds on approximation errors .    </S>"
  ]
}