{
  "article_text": [
    "sensor networks are useful in applications , for the environment and habitat monitoring , disaster recovery , agricultural monitoring , health administration , and target - tracking @xcite , where human intervention is not possible",
    ". being unattended these networks are more prone to frequent topology changes due to a random node or link failures .",
    "such scenarios demand low overhead , more robustness and fault tolerant algorithms .",
    "thus topology - based algorithms are not useful for these networks as they involve high overhead for maintenance of topology information and recovery mechanisms for critical points of failure @xcite . on the other hand ,",
    "stateless algorithms like the random walk - based algorithm we present in this paper , are reliant only on local information , have low overhead and do not suffer from the problem of critical point failure ( e.g. cluster heads  @xcite or nodes close to root of data gathering tree @xcite ) as all nodes of the network play similar role in the algorithm ( see e.g.  @xcite ) .",
    "these algorithms are simple , fault tolerant and scalable , thus more suitable for unattended sensor networks . despite all these advantages random walk - based algorithms are not often preferred for data collection since they tend to have higher latency compared to routing - based methods which rely on global topology information and are vulnerable to changes in topology but are optimized for low latency .",
    "however , some sensor network applications particularly those involving monitoring of habitat , environment or agriculture like intel s wireless vineyard , great duck island project , zebranet project , and many others ( see @xcite and references therein ) can compromise on latency but require uninterrupted continuous data collection . motivated by these examples , we propose a simple decentralized random walk based algorithm [ alg : main_algo ] , for data collection which requires no configuration and thus has zero setup cost .",
    "we consider a multi - hop sensor network where each node is equipped with a queue ( we use the term buffer and queue interchangeably ) which helps in store and forward process of data .",
    "each node gathers data at a fixed rate from its surroundings and stores it in its buffer along with other data packets that may be generated earlier or received from neighboring nodes . at each time - step",
    ", each node picks a random data packet from its buffer and forwards it to a randomly chosen neighbor .",
    "[ alg : main_algo ] algorithm does not require any global information and is truly distributed .",
    "we study the movement of data packets in [ alg : main_algo ] by modeling it as a random walk on the graph representing the underlying network .",
    "we provide theoretical bounds on the latency and throughput for [ alg : main_algo ] .",
    "though our latency bound is not comparable to that of routing based algorithms , we observe that it is no more than logarithmic factor higher than worst case latency of routing based algorithms .",
    "we study the throughput of the network and provide a lower bound in terms of the spectral gap of the transition matrix of a metropolis random walk on the graph and an upper bound in terms of the edge expansion of the graph .",
    "we also demonstrate the tightness of our throughput bounds through simulations for random @xmath0-regular graphs and random geometric graphs . through simulation",
    "we compare the [ alg : main_algo ] algorithm with directed diffusion @xcite which is known for its high throughput and show that although directed diffusion achieves better throughput than our algorithm , its advantage over [ alg : main_algo ] is bounded and depends linearly on the degree of the network .",
    "[ [ our - approach ] ] our approach + + + + + + + + + + + +    given a sensor network , we assume that each node except a designated node called _ sink _ is sensing the environment . specifically , each node senses data as independent bernoulli process with some _ stable _ rate and relay it to the _",
    "sink_. stable rate ensures that all data is successfully collected at the sink ( we will define it more formally in section  [ subsec : performance_metrics ] ) . in our model , we assume that the network is connected , but we do not expect any node to know anything about the network except the identity of its neighbors ( the nodes with which it can directly communicate ) . we also assume that time is slotted and nodes communicate with each other at the start of every time slot .",
    "however , this assumption can be easily removed and does not affect our results .    in [ alg : main_algo ] algorithm at the start of any time slot",
    ", a node picks up a data packet from its queue uniformly at random and forwards it to a neighbor who is also chosen uniformly at random .",
    "we allow a node to transmit only one packet to one of its neighbors , but , it can receive multiple packets from its neighbors .",
    "this is known as transmitter gossip constraint @xcite and has been used in literature @xcite for energy conservation and to prevent data implosion @xcite .",
    "the movement of any data packet in such setting can be seen as the _ random walk _ of the data packet on the graph",
    ". however , presence of other packets in the buffer of a given node results in delaying of the given packet , thereby , making analysis of random walk harder .",
    "so , to analyze it we need to ensure that the expected queue size is bounded by 1 for all nodes at all time steps ( discussed in detail in section  [ subsec : proposed_algorithm ] ) . for regular networks such",
    "constraint is easily satisfied but for ensuring it for general graphs we use a modified version of random walk known as metropolis chain wherein each node is allowed to accept or reject a data packet from its neighbor with some probability .",
    "this modified version of random walk is also helpful in achieving load balancing @xcite among the sensor nodes .",
    "we define a parameter , _ collection time _ , which is the time taken for the sink to receive data packets from all the nodes and hence , represents the latency in collecting all data .",
    "we also define the _ throughput of network _ which is the rate at which data is received at the sink .",
    "we analyze these parameters by studying the movement of data packets as the random walk on the graph representing the underlying network . for a single round of data collection scenario ,",
    "we study @xmath1 random walks moving in the network ( where @xmath1 is the number of nodes in the network ) and determine the collection time in terms of graph parameters like maximum , minimum degrees and worst - case hitting time .",
    "similarly , for a stable data arrival rate , we find the average data collection time over @xmath2 rounds of data packets sensed by each node .    as each node in [ alg : main_algo ] algorithm makes transmission decisions based on its immediate neighborhood ( local information ) , it does not require any global information about the network topology or position of the sink node .",
    "the algorithm requires zero configuration before it starts data transmission and is immune to topology changes during its execution .",
    "the major drawback of our work is that our transmission model allows simultaneous transmission and reception and also allows for a node to receive more than one packet at a time , thereby bypassing the question of interference which is critical to sensor networks built on wireless nodes .",
    "this effectively means that at first sight , it appears that our results are valid only for wired sensor networks .",
    "however , this is not so . as we discuss in section  [ sec : conclusion_future_work ] , we feel that our mathematical techniques can be adapted to a setting where a node can either receive or send in one slot and where a node can receive from exactly one neighbor in a given slot .",
    "we feel that the bounds we prove will not change qualitatively in such a setting .",
    "[ [ our - contributions ] ] our contributions + + + + + + + + + + + + + + + + +    1 .",
    "we propose a simple , low overhead , fault - tolerant decentralized random walk based algorithm , [ alg : main_algo ] , to collect data packets from all the nodes at the designated node _",
    "sink_. 2 .",
    "we give an upper bound on the latency for [ alg : main_algo ] and show that the data rate which determines the network throughput is lower bounded by the spectral gap of [ alg : main_algo ] s transition matrix which is proportional to the spectral gap of the network for regular graphs .",
    "we study through simulation how close our throughput bounds are to the observed data rate and also compare the optimal throughput of our method to the throughput obtained by the well - known routing - based data collection method directed diffusion .",
    "[ [ organization ] ] organization + + + + + + + + + + + +    following a survey of the literature in section  [ sec : related_work ] , we formalize our network setting and data collection model in section  [ sec : model_algorithm ]",
    ". we also present the [ alg : main_algo ] in section  [ sec : model_algorithm ] .",
    "we define our performance metrics and main results in section  [ sec : metrics_analysis ] . in section [ sec : data_collection_time_rate ] we prove our main theorems followed by a discussion about the results and some examples .",
    "some empirical results are presented in section [ sec : empirical_results ] along with simulations for analyzing our rate bounds and comparison with directed diffusion method .",
    "we conclude the work in section  [ sec : conclusion_future_work ] with a discussion for possible future work .",
    "[ [ data - collection - algorithms ] ] data collection algorithms + + + + + + + + + + + + + + + + + + + + + + + + + +    data collection in a sensor network is a well - studied field , and several algorithms have been proposed for it in the literature .",
    "we briefly discuss some of them and compare with [ alg : main_algo ] algorithm .",
    "one category of such algorithms is location - based like gpsr @xcite which uses some routing information at every node generated either using global or geographical information of the network . in a typical sensor network ,",
    "routing information at any node becomes invalid frequently ( due to movement or failure of nodes ) , hence these algorithms are not very efficient in practice . on the other hand , in [ alg : main_algo ] every node needs only to know about its neighborhood , thus is more robust and less vulnerable to node failures and network changes .",
    "another class of algorithms is based on hierarchical structures like clusters in leach @xcite or chain in pegasis @xcite .",
    "these algorithms also require time to learn some global information of the network and set up the cluster heads or chain of sensor nodes .",
    "also , in clustering protocols @xcite , over a period of time cluster heads become the bottleneck for the propagation of data packets in the network and such solutions not scalable .",
    "the [ alg : main_algo ] is decentralized in the sense that each node transmits data only to its neighbors and there is no centralized node to govern these transmissions .",
    "thus , [ alg : main_algo ] is truly distributed and scalable .",
    "data correlation @xcite and coding techniques @xcite have also been used for data collection .",
    "we do not use any coding technique in our algorithm and so we need low configured nodes which are just capable of storing and forwarding data . however , our network model is similar to kamra et al .",
    "@xcite as they use only local information in their algorithms and have a single sink for data collection .",
    "closest to the [ alg : main_algo ] algorithm is the technique used in gossip algorithms for data dissemination and rumour spreading @xcite . in these algorithms , at any time step , a node selects one of its neighbor uniformly at random and communicates some data to it .",
    "every node performs certain computation on the received data ( along with its data ) and transmits the output to a randomly chosen neighbor in the next time slot .",
    "we use a similar approach to communication in our setting , but unlike gossip algorithms , our aim is to collect all the data packets at the sink node . in [ alg : main_algo ] algorithm the nodes do not perform any computation on the data and just forward the data packets which they receive . most of the literature in gossip algorithm setting computes functions like the average , sum or separable functions @xcite .",
    "we are interested in collecting all the data packets which can be seen as _",
    "identity function _ on the data .",
    "moreover , we use push mechanism for spreading information rather than other variants like pull , push - pull as done by demers et al .",
    "@xcite and karp et al .",
    "@xcite .",
    "[ [ random - walk - based - algorithms ] ] random walk based algorithms + + + + + + + + + + + + + + + + + + + + + + + + + + + +    models based on simple random walks have been used in literature for query processing @xcite , to model routing for data gathering @xcite and for opportunistic forwarding @xcite , but no analysis has been done for finding the average data collection time or resulting throughput explicitly . in a different context than ours , neely @xcite showed that when independent markov processes modulated arrival processes , the average network delay grows at most logarithmically in the number of nodes in the network .",
    "our latency ( or data collection time ) bounds are similar to these results .",
    "biased random walks have also been explored to improvise various results .",
    "one such biased random walk wherein priority is given to unvisited neighbors has been used by avin et al .",
    "@xcite to improve their query processing results and partial cover time bounds .",
    "they suggest future improvement _ super bias _ @xcite which is similar to the metropolis algorithm that we have used . regarding the inherent problem of latency in random walks , they suggest performing parallel random walks as part of their future work@xcite . in [ alg : main_algo ] , we consider data packets are performing parallel random walks which reduce the latency considerably .",
    "[ [ network - throughput - and - data - rate ] ] network throughput and data rate + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    data arrival rate determines the frequency at which nodes in the network are collecting data from their surroundings , hence the network throughput .",
    "it is a critical performance parameter of the sensor network .",
    "the throughput capacity of the network in different contexts has been thoroughly studied @xcite following the pioneering work by gupta and kumar @xcite .",
    "many protocols have been proposed which offer high data rates , most popular among them are directed diffusion @xcite and its variants like rumor routing @xcite , energy - aware routing and others ( see @xcite and references therein ) .",
    "all these offer high data rates along a reinforced path but suffer from overhead in setting up and maintaining such path .",
    "also , these are only suitable for query - driven models and not useful for applications which require continuous data gathering @xcite .",
    "the stable data rate in terms of different underlying graph primitives has been found under varying contexts in literature .",
    "banerjee et al .",
    "@xcite determine the maximum data or refresh rate for a network computing _ fully - multiplexible _ functions in terms of min - mincut of the graph .",
    "their result is a natural upper bound on our rate results as we simply collect data without any aggregation .",
    "dependence of rate on the maximum degree of sensor network organized as a tree has been found by incel et al .",
    "@xcite in their setting .",
    "we also find a sufficient bound on the rate in terms of graph parameters like maximum and minimum degrees .",
    "in this section we discuss our network setting , and data collection model .",
    "we present the [ alg : main_algo ] algorithm in detail here .      [ [ sensor - node - capabilities ] ] sensor node capabilities + + + + + + + + + + + + + + + + + + + + + + + +    we consider a connected multi - hop sensor network comprising a large number of sensor nodes deployed over a given geographical area .",
    "each node of the network has a sensor which senses the environment , a transceiver which is used to send and receive data from other network nodes , and some storage in which data sensed from the environment and received from neighbouring nodes can be stored .",
    "the network is deployed with zero configuration i.e. no node is aware of the sink position or global network routes . moreover , each sensor is provided with a standard pseudo - random number generator , which is used for generating random numbers to choose among the neighbors . the ability of sensor nodes to choose random numbers has been exploited a lot in literature , be it in data - centric protocols like acquire , gradient - based routing , and energy - aware routing or hierarchical protocols like leach ( see @xcite and references therein ) .",
    "[ [ transceiver - assumptions ] ] transceiver assumptions + + + + + + + + + + + + + + + + + + + + + + +    each node is configured to forward data to one of its neighbor chosen uniformly at random , but it can receive data from multiple neighbors at any time step .",
    "this is known as transmitter gossip constraint @xcite .",
    "it has been widely used in literature @xcite as it results in slow energy dissipation and also prevents data implosion by maintaining only a single copy of data at any node @xcite .",
    "multi - packet reception and simultaneous transmission - reception are easily ensured for wired networks due to the physical connections , but , for wireless networks , these are not possible due to interference .",
    "however , the main ideas underlying our proofs do not change even if we consider more realistic assumptions like allowing either transmission or reception of a single packet in a given time slot ( see section  [ sec : conclusion_future_work ] ) and the analysis we present in this paper can serve as a benchmark for those scenarios .",
    "[ [ data - generation - and - collection ] ] data generation and collection + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we assume that sensor nodes generate new data as required , e.g. , a temperature sensor may be configured to generate a new reading when the change over the prior reading is at least a certain threshold value .",
    "this data is then relayed through the network to one of the nodes that is designated as the data sink and is responsible for collecting data that reaches it .",
    "depending on the application , we assume the sink can then relay the data to some decision making or processing unit using _ data mules _ to monitor events , perform local computation or configure local and global actuators @xcite .",
    "[ [ network - setup - and - routing ] ] network setup and routing + + + + + + + + + + + + + + + + + + + + + + + + +    we do not use any centralized algorithm to create connections among the nodes . at deployment time , sensors opportunistically make connections with every other sensor that they can directly communicate with . in wired networks , such nodes ( neighbors ) are the directly connected sensor nodes , and in wireless networks , these are nodes which lie within the transmission range of the given node .",
    "our algorithm requires knowledge of the number of neighbours ( the degree in graph - theoretic terms ) of each of its neighbours so when the initial phase of making connections ends , each node exchanges this information with each of its neighbours . as nodes may become unavailable frequently , due to failures or enter into sleep mode ( like in wireless sensor networks ) , nodes need to perform these handshakes periodically to ensure updated degree information of their neighbors ( discussed in detail in section  [ subsec : graph_data_coll_model ] )",
    "hence , our scheme for network creation offers a basic communication mechanism without incurring any computational overhead .",
    "moreover , no localization algorithm is required for establishing the multi - hop communications .",
    "[ [ network - model ] ] network model + + + + + + + + + + + + +    we model the multi - hop sensor network by an undirected graph @xmath3 , where @xmath4 is the set of @xmath1 sensor nodes with one sink , @xmath5 , and @xmath6 is the set of edges .",
    "there is an edge @xmath7 between nodes @xmath8 , if @xmath9 @xmath10 can directly communicate with each other .",
    "the neighborhood of a node @xmath11 is the set of all nodes @xmath12 which can communicate with it , denoted by @xmath13 , i.e. , @xmath14 the degree of @xmath11 is @xmath15 we denote the maximum and minimum degree among all nodes in the network by @xmath16 and @xmath17 respectively .    [ [ time - model ] ] time model + + + + + + + + + +    we consider a synchronous time model wherein time is slotted across all nodes in the network and nodes communicate with each other at the start of every time slot .",
    "our results do not depend on synchronization and can be adapted to the asynchronous setting as well but in this paper we present the synchronous setting for ease of presentation .    [",
    "[ data - generation - model ] ] data generation model + + + + + + + + + + + + + + + + + + + + +    we model the data generation process at each node as a stochastic arrival process in discrete time that is bernoulli with parameter @xmath18 and independent of the arrivals taking place at all other nodes , i.e. at each time slot @xmath19 each node generates a new data packet with probability @xmath18 independent of all other nodes .",
    "the sink has no data arrival taking place .",
    "[ [ store - and - forward - model ] ] store and forward model + + + + + + + + + + + + + + + + + + + + + + +    at any time slot @xmath19 , due to the enforced transmitter gossip constraint , each node can send only a single data packet to a chosen neighbor , but each node can receive multiple data packets simultaneously from its neighbors .",
    "we also allow a node to send and receive at the same time .",
    "we have discussed the implications of this assumption in section  [ sec : intro ] and will further discuss how to remove this assumption in section  [ sec : conclusion_future_work ] .    at every time step ,",
    "each node maintains a queue of packets , either generated at the node itself or received from neighbours , which have not been forwarded yet .",
    "we denote the number of data packets in the queue of node @xmath11 , also referred to as the queue size , at the start of slot @xmath19 by @xmath20 and let @xmath21 $ ] be the expected queue size . note",
    "that this expectation is over the random arrivals and the random choices made by our algorithm which will be explained further in the subsequent section .      in our proposed algorithm [ alg : main_algo ] , at any time slot @xmath19 each node chooses a data packet from its queue and transmits it to a randomly chosen neighbor . the movement of a data packet in the network can be seen as a random walk on the graph @xmath22 , however the delay caused by the presence of other packets in the buffer of a given node makes this random walk an inhomogeneous markov chain which is harder to analyze .",
    "as we will see below it is possible to analyze this inhomogeneous chain but to do so we need to fulfill a technical requirement : we need to ensure that the expected queue size @xmath23 is bounded by 1 for all nodes at all time steps .",
    "this requirement is easily fulfilled by the simple random walk on a @xmath0-regular graph but for general graphs we need a slight modification that uses the notion of a metropolis chain ( * ? ? ?",
    "* section 3.2 ) .    as we see in the formal definition of algorithm  [ alg : main_algo",
    "] , this modification is as follows : having selected a packet to forward from it s buffer , a node @xmath11 picks a neighbour , say @xmath10 , uniformly at random from its @xmath24 neighbours , but it actually forwards the packet to @xmath10 with probability @xmath25 .",
    "formally , the transition probability from node @xmath11 to @xmath10 in the metropolis chain is given by @xmath26 } = \\begin{cases } \\frac{1}{{\\mbox{deg}(u_i ) } } \\left [ \\frac{{\\mbox{deg}(u_i)}}{{\\mbox{deg}(u_j ) } } \\wedge 1 \\right ] & ~\\mbox{if } u_i \\neq u_j , \\\\ 1 - \\sum\\limits_{u_k : u_k \\neq u_i } \\frac{1}{{\\mbox{deg}(u_i ) } } \\left [ \\frac{{\\mbox{deg}(u_i)}}{{\\mbox{deg}(u_k ) } } \\wedge 1 \\right ] & ~\\mbox{if } u_i = u_j .",
    "\\end{cases } \\label{eq : metropolis_final}\\ ] ] in effect what this means is that if @xmath10 has lower degree than @xmath11 , the packet is always forwarded but if it has higher degree then it may not be forwarded .",
    "intuitively we see that this prevents the packets from concentrating at high degree nodes , and , in fact , this selection of parameters ensures that the metropolis random walk has a uniform stationary distribution ( as opposed to the simple random walk in which the stationary probability of being at a node is proportional to the degree of the node ) .",
    "metropolis - collect    [ alg : main_algo ]    node @xmath27 , which has neighborhood @xmath28 , degree @xmath24 , non - empty queue at time slot @xmath19 i.e. @xmath29 and knows updated values of @xmath30 due to regular updates from @xmath10 , handshakes done at the time of deployment and then at periodic intervals .",
    "node @xmath11 picks a packet from its queue with probability @xmath31 picks a neighbor @xmath10 from @xmath28 with probability @xmath32 .",
    "node @xmath11 sends the data packet to node @xmath10 , which accepts packet with probability @xmath33 .",
    "node @xmath10 sends ack to node @xmath11 if it accepts packet , otherwise sends nak .",
    "both ack and nak are piggybacked with updated degree of @xmath10 .",
    "node @xmath11 on receiving ack from @xmath10 deletes data packet sent to node @xmath10 from its queue and does nothing if it receives nak .    at the end of the time slot ,",
    "if node @xmath10 accepts the data packet and sends an ack then the data packet gets stored in @xmath10 s queue and is deleted from @xmath11 s queue .",
    "otherwise , @xmath10 sends a nak and none of the queues are changed .",
    "[ [ expected - queue - size - equation ] ] expected queue size equation + + + + + + + + + + + + + + + + + + + + + + + + + + + +    since the expected queue size plays a key role in our analysis , we present it separately here . for [ alg : main_algo ] with transition probability @xmath34}$ ] , running on a network in which each node is generating data at rate @xmath18 as discussed in section  [ subsec : graph_data_coll_model ] , the expected queue size at node @xmath35 at time slot @xmath36 , is @xmath37 }   + \\sum\\limits_{u_j : u_j\\sim u_i}\\mu_t ( u_j){\\mathcal{p } [ u_j , u_i ] } + \\beta \\label{eq : queue_eq_with_lambda}\\ ] ]    [ [ information - maintenance ] ] information maintenance + + + + + + + + + + + + + + + + + + + + + + +    sensor nodes obtain the degree information of their neighbors using handshakes done at the time of deployment and then at periodic intervals .",
    "these intervals are fixed based on the application at hand and deployment scenario . to increase the interval duration and avoid frequent handshakes",
    "each node can piggyback its degree information while sending ack or nak .",
    "piggybacking ensures updated information in between the intervals while incurring no extra overhead .",
    "in this section , we first define our performance metrics and then present our main results .",
    "any analysis of a data collection method must focus on latency and throughput .",
    "we make these notions precise in this section .    to address latency we first define a simpler data generation model : the _ single round model_. in the single round model each node ( apart from the sink ) has a single data packet available at time @xmath38 .",
    "no further data appears .",
    "we define the _ single round collection time _",
    "@xmath39 to be the number of time slots taken for all the data packets to reach the sink .",
    "however , since the actual model we consider is of regular data generation at a rate @xmath18 , we need a notion of latency in this setting as well . for this ,",
    "let us identify different rounds of data generated .",
    "starting time from @xmath38 , we number the data packets generated at any node starting from 1 .",
    "we say that _ round @xmath40 _ of data comprises all the @xmath41 data packets that are numbered @xmath40 .",
    "the _ data collection time _ of the first @xmath2 rounds of data , @xmath42 , is defined as , @xmath43 where , @xmath5 denotes the sink and @xmath44 is the random variable denoting the position of @xmath45 round data packet of node @xmath11 at the start of time slot @xmath19 .",
    "[ def : collection_time_k ] the average data collection time for the network is defined by @xmath46    turning to throughput we note that if the data arrival rate @xmath18 is very high [ alg : main_algo ] will not be able to successfully move the data to the sink since the buffers will keep growing , i.e. , the queues at the buffers will become unstable .",
    "so , first we define a _",
    "stable data rate _ for any algorithm to be a value of @xmath18 that ensures @xmath47 , i.e. a data arrival rate is called stable if the average collection time is finite .",
    "we note that in [ alg : main_algo ] since no node is allowed to transmit more than 1 packet in any slot , all stable data rates satisfy @xmath48",
    ". our main theorem , theorem  [ thm : rate_bounds ] , will give a lower bound on the stable data rate achievable by [ alg : main_algo ] and also give a general upper bound on stable data rates for any algorithm . as expected the notion of throughput",
    "is closely related to the notion stable data rate .",
    "[ def : throughput ] given a stable data rate @xmath18 , i.e. a data rate such that the average data collection time of [ alg : main_algo ] , @xmath49 , is finite , the _ network throughput _ is defined as the rate at which data is received by the sink . in other words , if we have @xmath50 data sources ( @xmath51 ) and a stable data rate @xmath18 in a network , the network throughput is @xmath52 .",
    "[ thm : single_tcol ] given a graph @xmath3 with @xmath53 nodes , with maximum degree @xmath16 and minimum degree @xmath17 and worst - case hitting time of metropolis random walk on @xmath22 , @xmath54 denoted by @xmath55 , the single round data collection time @xmath39 for [ alg : main_algo ] is @xmath56 with probability at least @xmath57 .",
    "moreover @xmath58 where @xmath59 is the worst - case hitting time of simple random walk on @xmath22 .",
    "we note that @xmath59 is a natural lower bound on data collection in the sense that even if data had to be moved from just a single node to the sink , the time take would be @xmath59 .",
    "theorem  [ thm : single_tcol ] shows that the time taken to collect a single round of data from all but one of the nodes of the network is just a logarithmic factor higher for @xmath0-regular networks or for networks that have low variability between the maximum and minimum degree .",
    "we present the proof of this theorem in detail in section  [ sec : data_collection_time_rate ] .",
    "for the average data collection time we have the following result :    [ thm : avg_data_coll ] given a graph @xmath22 representing the underlying network where each node except the sink receives independent bernoulli arrivals with rate @xmath18 , then , if @xmath49 is finite for [ alg : main_algo ] , i.e. @xmath18 is a stable data rate then @xmath49 is @xmath60 with probability at least @xmath57 .",
    "the result of theorem  [ thm : avg_data_coll ] is not surprising in the sense that since [ alg : main_algo ] works by ensuring that the expected buffer size is bounded , i.e. the queues are stable , we expect that the data being generated is cleared in a time inversely proportional to the rate in which it is generated .",
    "detailed proof of theorem  [ thm : avg_data_coll ] is presented in next section .",
    "finally we turn to the main result on throughput .",
    "[ thm : rate_bounds ] given a graph @xmath3 with @xmath53 nodes and each node except the sink @xmath5 having independent bernoulli data arrivals with rate @xmath18 , [ alg : main_algo ] is able to achieve a finite average data collection time @xmath49 for @xmath18 such that @xmath61 where @xmath62 is the second largest eigenvalue of transition matrix @xmath63 of graph @xmath22 with transition probability @xmath64}$ ] given by [ alg : main_algo ] .",
    "the corresponding network throughput is @xmath65 . in particular , for @xmath0-regular graphs",
    "@xmath66 where @xmath67 is the second largest eigenvalue of adjacency matrix of @xmath22 .",
    "the lower bound on throughput is shown to be related to the following natural upper bound on any data collection algorithm . in order to present this generalized upper bound ,",
    "we first need to define a few terms . for any vertex subset @xmath68",
    "we define its edge boundary as @xmath69 .",
    "now , for all @xmath70 we define a constant @xmath71 . and @xmath72 .",
    "note @xmath73 where @xmath74 is _ edge expansion _ of graph @xmath22 @xcite .",
    "[ pro : rate_upper_bound ] given a graph @xmath3 with @xmath53 nodes and each node except the sink @xmath5 having independent bernoulli data arrivals with rate @xmath18 , no data collection algorithm is able to achieve a finite average data collection time @xmath49 for @xmath18 such that @xmath75 where @xmath76 is the degree of the sink , @xmath77 is a constant and @xmath78 is at most @xmath79 , edge expansion of graph @xmath22 .",
    "we now present the proof of theorem  [ thm : single_tcol ] that gives an upper bound on the time taken to collect a single round of data .",
    "this proof overcomes a significant difficulty : the fact that the markov chain induced by [ alg : main_algo ] on any data arrival pattern , even the single round arrival pattern where all data is available at @xmath38 and no new data arrives , is _ not _ time homogeneous .",
    "the proof proceeds by first viewing each packets trajectory to the sink as a separate random walk .",
    "now , given a packet @xmath80 and let @xmath81 be its position at the start of time slot @xmath19 . consider the subset of time slots when [ alg : main_algo ] chooses @xmath80 for transmission out of the buffer at node @xmath82 , and the node actually moves ( i.e. is not rejected by its neighbour ) .",
    "let us call this subset @xmath83 .",
    "the key observation is that the process @xmath84 is a time homogeneous random walk on @xmath22 .",
    "we first bound the time taken for @xmath85 such walks to reach the sink and then , in step 2 , bound the delay between successive time steps in each @xmath83 to reach the result .",
    "let the hitting time of a random walk of a data packet from a node @xmath86 to the sink for [ alg : main_algo ] be defined as @xmath87 .",
    "so , for a node @xmath86 we ve @xmath88 \\leq { \\hat{t}_{\\mbox{\\scriptsize hit}}}$ ] , where @xmath89 is the worst - case hitting time of metropolis random walk starting from any node of graph . by markov",
    "s inequality @xmath90 } \\leq \\frac{1}{2}$ ] .",
    "the probability of a random walk not hitting the sink in @xmath91 times the twice worst - case hitting time is @xmath92 } \\leq \\frac{1}{2^w}$ ] .",
    "now , for @xmath93 we get @xmath94 } \\leq \\frac{1}{n^2 } \\label{eq : single_round_hit}\\ ] ]    thus , with probability at least @xmath95 , the number of time slots taken by the data packet from the node @xmath11 to hit the sink @xmath5 are @xmath96 .",
    "this analysis is done assuming there is only one packet in the queue of any node at any time .",
    "now , we analyze the queueing delay due to more than one packets in the queue @xmath97 .",
    "we will prove that no data packet is delayed by more than @xmath98 time slots with probability at least @xmath95 due to the queues at the nodes .",
    "any packet @xmath99 gets delayed at time slot @xmath19 , due to queue at a node @xmath9 because it is not picked for transmission in that slot among all the packets in the queue .",
    "thus , the probability of @xmath99 being delayed by @xmath11 at time @xmath19 is , @xmath100 } & = \\sum_{w=2}^{n } \\frac{w-1}{w } { \\mathbb{p}\\left [ \\mathcal{q}_t(u_i ) = w \\right]}\\\\ & = \\frac{1}{2 } \\sum_{w=2}^{n } ( w-1 ) { \\mathbb{p}\\left [ \\mathcal{q}_t(u_i ) = w \\right ] } \\\\    & \\leq   \\frac{1}{2}\\mathbb{e}[\\mathcal{q}_t(u_i)].\\end{aligned}\\ ] ] we know , initially every node except the sink has one data packet in its queue so , expected queue size @xmath101\\leq 1 $ ] .",
    "now let us assume @xmath102\\leq 1 $ ] holds true for any time slot @xmath19 . we know that for [ alg : main_algo ] @xmath37 }   + \\sum\\limits_{u_j",
    ": u_j\\sim u_i}\\mu_t ( u_j){\\mathcal{p } [ u_j , u_i ] } \\ ] ]    and so , since the outgoing probability to any neighbour @xmath103 , @xmath104}$ ] , is equal to the incoming probability @xmath105 } = 1/\\max\\{\\deg(u),\\deg(v)\\}$ ] , by induction we have @xmath106 \\leq 1 $ ] .",
    "hence , @xmath107 } \\leq \\frac{1}{2}. \\label{eq : prob_xj_delayed}\\ ] ] let @xmath108 be a random variable which is 1 if packet @xmath99 is delayed in time slot @xmath50 and 0 otherwise .",
    "then , @xmath109 \\leq \\frac{1}{2}.$ ] let @xmath110 be a random variable which denotes the total delay incurred by a packet @xmath99 in @xmath19 time slots .",
    "so , @xmath111 \\leq \\frac{t}{2}.$ ] let @xmath112 .",
    "then , @xmath113 = \\mathbb{e}[y_j^t ] - \\frac{t}{2 } \\leq 0.$ ] since @xmath114 and @xmath115 = z_j^{t-1 } + \\mathbb{e}[h_j^t ] - \\frac{1}{2 } \\leq z_j^{t-1},$ ] we see that the sequence @xmath116 is a supermartingale .",
    "now , we know without any delays number of time slots taken by random walk of a data packet to hit the sink @xmath5 is @xmath96 with probability at least @xmath95 . now , using azuma s inequality we show that the probability of exceeding that time by @xmath98 is very low .",
    "let @xmath117 then , @xmath118 } & \\leq \\exp \\left ( -\\frac{(2 \\sqrt{30{\\hat{t}_{\\mbox{\\scriptsize hit}}}}\\log n)^2}{2\\sum_{i=1}^{t}1 } \\right)\\nonumber\\\\    & \\leq \\exp \\left ( - 2 \\log n \\right ) \\leq \\frac{1}{n^2 }   \\label{eq : single_round_z_event_delay}\\end{aligned}\\ ] ] thus , with probability at least @xmath95 , @xmath119 @xmath120 @xmath121 .",
    "so , @xmath122 . let @xmath123 represent the non - delayed slots so from eq .",
    "we have with probability at least @xmath95 , @xmath124 .",
    "as , @xmath125 this proves that none of the data packet s hitting event is delayed by more than @xmath98 time slots with probability at least @xmath95 due to the queues at each node .",
    "now , for the @xmath126 data packet , we have _ two events _ : ( 1 ) @xmath127 and ( 2 ) @xmath128 .",
    "we have shown that @xmath129 } \\leq 1/n^2 $ ] and @xmath130 } \\leq 1/n^2 $ ] .",
    "this means that @xmath131 } \\leq 2/n^2 $ ] .",
    "so , for all @xmath1 data packets we need to find @xmath132 .",
    "so , @xmath133 } \\leq \\frac{2}{n}.\\ ] ]    so , from equations and , all @xmath1 walks take @xmath134 time slots to hit the sink with probability at least @xmath135",
    ".    moreover , since the minimum acceptance probability of any transition in the metropolis chain is @xmath136 , the expected holding time at any state due to rejections is at most @xmath137 and so @xmath58 where @xmath59 is the worst - case hitting time of simple random walk on @xmath22 .",
    "this proves the theorem .",
    "consider our network setting where we have independent bernoulli data arrivals with stable rate @xmath18 given by theorem [ thm : rate_bounds ] and proposition  [ pro : rate_upper_bound ] which ensure finite average data collection time @xmath49 .",
    "for each round of data arrival coming at the rate @xmath18 we have @xmath1 random walks of data packets moving on graph @xmath22 representing the underlying network .",
    "now we find the average data collection time for @xmath2 such rounds of data arrival .    for a given graph @xmath22 where each node except the sink receives independent bernoulli arrivals with stable rate @xmath18 ( see theorem [ thm : rate_bounds ] and proposition  [ pro : rate_upper_bound ] ) which ensures finite average data collection time @xmath49 .",
    "recall each data round generated has total of @xmath138 data packets and each round has its appearance and clearance time in the network . instead of finding such times individually for each round",
    ", we will proceed our analysis by finding the maximum time by which @xmath2 rounds of data arrival have happened and then after this time , we find the clearance time assuming all @xmath139 packets have appeared .",
    "let @xmath140 be the appearance time of @xmath2 rounds of data arrival on each node using [ alg : main_algo ] algorithm for communication .",
    "let @xmath141 be a hypothetical node where we assume packets reside before arriving at the network nodes , then , @xmath142 and @xmath143 , we have @xmath144 .",
    "if the @xmath45 data item appears at node @xmath11 at time @xmath145 then @xmath146 for all @xmath147 and @xmath148 .",
    "with this notation we can define appearance time as : @xmath149 .",
    "so , @xmath140 is the earliest time when @xmath2 packets have appeared at each node .",
    "now , we prove a high probability bound on the appearance time @xmath140 .",
    "let @xmath150 be the event that a source node @xmath151 did not receive @xmath2 arrivals in time @xmath19 .",
    "consider , @xmath152 and @xmath153 .",
    "so , we have @xmath154 } = \\sum_{i = 1}^{k}\\binom{t}{k - 1 } \\beta^{k - i } ( 1 - \\beta)^{t - ( k - i ) } \\label{eq : prob_eq_event_a}\\ ] ] as , @xmath152 , @xmath155 also , @xmath156 , so equation can be written as , @xmath154 } \\leq k\\binom{t}{k}(1 - \\beta)^t\\ ] ] using , @xmath157 and let @xmath158 for @xmath159 , above equation can be rewritten as , @xmath154 } \\leq k ( ec ( 1 - \\beta)^c)^k\\ ] ] so , we can easily find @xmath160 such that @xmath161 . note that the value of @xmath160 will be only @xmath18 dependent as @xmath162 for some constant @xmath163 ( for finding such value of @xmath160 see lemma [ lem : appearnce_time_c ] of appendix [ app : value_of_c ] ) .",
    "so , for a node @xmath151 , @xmath164 } \\leq k\\bigg(\\dfrac{2}{3}\\bigg)^k\\ ] ] since , all random walks of data packets from all @xmath1 nodes are independent of each other , considering worse case analysis of all nodes we have , @xmath165 } \\leq nk\\bigg(\\dfrac{2}{3}\\bigg)^k \\label{eq : whp_eq of_event_a}\\ ] ] so , from equation with high probability the appearance time of @xmath2 rounds of data arrival in a stable network is at most @xmath166 where @xmath162 for some constant @xmath163 , so , @xmath167 note that after @xmath140 time all nodes have @xmath2 arrivals .",
    "let @xmath168 be the clearance time of random walks when @xmath2 data packets have arrived at all the nodes .",
    "we know from theorem  [ thm : single_tcol ] the clearance time of random walks assuming each node has only one data packet is @xmath169 .",
    "also , because of the appearance time of rounds , random walks across the data rounds are independent , so with probability at least @xmath170 , @xmath171 recall , @xmath42 is the collection time of @xmath2 rounds of data arrival and so , we can write @xmath172 now , using the results from eq.s and , we have @xmath173 so , with probability at least @xmath170 the average data collection time of @xmath2 rounds is , @xmath174 let @xmath175 be the time required by a random walk to cover all the states ( see chapter 11 @xcite ) .",
    "recall the definition of @xmath176 let @xmath177 be states for which @xmath178 , thus any walk starting at @xmath179 must have visited @xmath180 by the time all states are covered , so we have @xmath181 for a connected graph like the given graph @xmath22 it is not possible for the random walk to assign non - zero probability to a vertex it has not yet visited , so we can conclude that @xmath182 , where @xmath183 is the mixing time of graph ( see section 4.5 @xcite ) . we know",
    "@xmath184{n(n - 1)}}$ ] ( by theorem [ thm : rate_bounds ] ) and @xmath185 ( theorem 12.3 @xcite ) . using above results in eq .",
    "we prove theorem  [ thm : avg_data_coll ] .      in the previous section",
    ", we proved latency bounds for [ alg : main_algo ] where we considered that each node except the sink receives data at rate @xmath18 .",
    "so , for our setting recall from section  [ subsec : performance_metrics ] to successfully collect all data we need to ensure this rate is stable i.e. average collection time @xmath49 is finite .",
    "so , in this section , we prove bounds on such rate and the corresponding throughput as two are closely related . in particular , we prove the lower bound for [ alg : main_algo ] and a general upper bound for any data collection algorithm .",
    "we also generalize the results for @xmath50 data sources and find a sufficient bound on rate .    for our given graph @xmath3 with @xmath1 nodes out of which one",
    "is sink @xmath5 , let data arrive as independent bernoulli arrivals with rate @xmath18 at the source nodes ( all nodes except sink ) .",
    "now , for [ alg : main_algo ] we find the bound for stable data rate which ensures finite average collection time @xmath49 such that it successfully collect all data .    for the data arrival rate @xmath18 at each node ( except sink ) and transition matrix @xmath63 of graph @xmath22 for the [ alg : main_algo ] algorithm , let @xmath186 be an @xmath1 element row vector representing the steady state queues of nodes ( as discussed in section  [ subsec : proposed_algorithm ] ) in graph i.e. for nodes @xmath187 we have , @xmath188 $ ] respectively .",
    "this is defined assuming that sink collects all data it receives and has no notion of maintaining queue .",
    "let @xmath189 be another @xmath1 element row vector for nodes in graph such that @xmath190 $ ] .",
    "let @xmath191 be the usual @xmath192 identity matrix and @xmath193 $ ] be an @xmath1 element all one row vector .",
    "the steady state queue equations at nodes can be written in vector form as @xmath194 let @xmath195 be a distribution on @xmath4 satisfying @xmath196 , then @xmath195 is said to be a stationary distribution .",
    "for the purpose of this proof we consider @xmath195 to be uniform i.e. @xmath197 for every @xmath198 .",
    "let the transition matrix @xmath63 be reversible with respect to the stationary distribution @xmath195 i.e. @xmath199 . the usual inner product on the vector space @xmath200 is given by @xmath201 .",
    "we define another inner product on @xmath200 that we will use in this proof @xmath202 . from lemma",
    "12.2 @xcite , the inner product space @xmath203 has an orthonormal basis of real - valued eigenfunctions @xmath204 corresponding to real eigenvalues @xmath205 . writing the vector @xmath186 in terms of the eigenvectors",
    ", we have @xmath206 this gives us that @xmath207 from the perron - frobenius theorem ( see e.g.  ( * ? ? ?",
    "* chap 9 ) ) and lemma 12.1 of @xcite , we know @xmath208 .",
    "so , we have @xmath209 note , that @xmath210 form an orthonormal basis so , @xmath211 .",
    "hence we have @xmath212 the eigenfunction @xmath213 corresponding to the eigenvalue 1 can be taken to be a constant vector * 1 * ( see lemma 12.2  @xcite ) , so @xmath214 .",
    "also , @xmath215 .",
    "so , using these results in eq .",
    "we have @xmath216 where , @xmath217 is the expected queue size .",
    "now , taking the square of norm of eq . and using eq . , we have @xmath218 also ,",
    "@xmath219 now , again taking the square of norm of eq . and",
    "using eq.s and , we have @xmath220 to get a lower bound for @xmath18 we need a lower bound on @xmath221 . for this",
    "we first consider two nodes whose queue size we know precisely ( 1 ) the sink , @xmath5 , which has @xmath222 , and ( 2 ) a node @xmath223 , such that @xmath224 . while we do nt know precisely which node is @xmath223 , we know that such a node exists , because if it did nt then we could always raise the value of @xmath18 , i.e. the fact that @xmath18 is maximum implies that some node s expected queue size is 1 .",
    "we note that the contribution of @xmath225 and @xmath223 to the rhs of  ( ) is @xmath226 since for any @xmath227 $ ] , @xmath228 takes a minimum value of @xmath229 at @xmath230 .    noting that , since the arrival rate @xmath18 is stable , at steady state the expected number of packets entering the sink at any time is @xmath231 } = ( n-1)\\beta$ ] , and observing that @xmath232 } \\leq 1 $ ] , we conclude that the sink has at least one neighbour , call it @xmath233 , that has @xmath234 .",
    "let us observe here that @xmath235 since the system is stable and so at any point in time after steady state is reached the total number of packets that have been generated but not yet sunk is @xmath65 in expectation .",
    "hence we know that @xmath236 and so , @xmath237    similarly , since the expected number of packets coming into @xmath238 is exactly @xmath239 at steady state , and since @xmath240 } \\leq 1 $ ] , there is a neighbour of @xmath223 , we call it @xmath241 , such that @xmath242 . now , noting that since the average rate at which the sink can take in packets is at most 1 ( because @xmath243 } \\leq 1 $ ] ) , therefore @xmath244 . this means that @xmath245 which is larger that @xmath235 .",
    "so , we can say that @xmath246 .",
    "so , in order to derive a lower bound on the contribution of @xmath233 and @xmath241 to the rhs of we need to minimize @xmath247 . using calculus and the fact that @xmath248 we get @xmath249 putting and back into , we get @xmath250 using the variance bound in eq . , we have the lower bound on rate as @xmath251{n(n - 1)}}. \\label{eq : lambda_lower}\\ ] ] for @xmath0-regular graph , spectral gap is @xmath252 where @xmath67 is the second largest eigenvalue of its adjacency matrix @xcite . also , it is easy to check that @xmath253 where @xmath62 is the corresponding eigenvalue of transition matrix @xmath63 of random walk on given graph for [ alg : main_algo ] .",
    "so , for @xmath0-regular graphs in particular we have , @xmath251{n(n - 1 ) } } = \\frac{d - \\lambda_2(a)}{d~\\sqrt[]{n(n - 1 ) } } \\label{eq : lambda_lower_regular}\\ ] ] this proves the lower bound on rate @xmath18 .",
    "since , the network throughput is the rate at which the sink receives the data , so for our network with @xmath85 data sources each receiving data at stable rate @xmath18 , corresponding network throughput is @xmath65 .    for our given graph @xmath3 with @xmath1 nodes , with each node except the sink @xmath5 receiving data as independent bernoulli arrivals with rate @xmath18 , we find the maximum value of data arrival rate @xmath18 such that any data collection algorithm can achieve a finite average data collection time @xmath49 .    we know , for any data collection algorithm given a set @xmath68 , where @xmath254 the maximum data flow that can move out of this set is the flow across the boundary , @xmath255 also , for set @xmath256 we have @xmath257 , , where @xmath76 is the degree of the sink .",
    "now , from eq .",
    "[ eq : flow_boundary_gen ] @xmath258 .",
    "so , for any data collection algorithm upper bound on stable rate @xmath18 is given by @xmath259    [ [ discussion - about - network - throughput - upper - bound - for - algmain_algo ] ] discussion about network throughput upper bound for [ alg : main_algo ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in particular , if we consider [ alg : main_algo ] algorithm with transition matrix @xmath63 .",
    "in this algorithm , instead of deterministically sending a data packet along any edge we send it with some probability given by @xmath260 $ ] .",
    "now , for any vertex @xmath261 , we define its measure as , @xmath262}$ ] .",
    "similarly , for any @xmath68 we define the measure @xmath263 and we define its edge boundary as @xmath69 .",
    "thus , @xmath264}$ ] .",
    "now , we define constants @xmath265 and @xmath266 where @xmath267 is the _ cheeger s constant _ for the random walk on the graph @xmath22 .",
    "now , for @xmath268 , @xmath269}}{n - 1 } \\label{eq : h_hatg_sink_rate}\\ ] ]    we know , for any given set @xmath68 , where @xmath254 the maximum data flow that can move out of this set is the flow across the boundary , @xmath270 from eq .",
    ", flow out of set @xmath271 can also be written as , @xmath272 the fundamental necessary bound on the value of @xmath18 is the accepting rate of the sink , so we have @xmath273}$ ] . using eq . in the above equation we have , @xmath274}\\nonumber\\\\ ( n - 1)\\beta & \\leq { \\mathcal{p } [ u,{u_{\\mbox{\\scriptsize s } } } ] } - ( h(u ) - \\beta)|u|\\label{eq : h(u)_lambda_diff}\\end{aligned}\\ ] ] using eq . in eq . , we have @xmath275 , so @xmath276}}{n - 1 } \\label{eq : lambda_sink_alone}\\ ] ] so , the bottleneck for rate can be sink itself or some other far - off node . hence , the maximum value of stable data rate i.e. rate ensuring finite average data collection time @xmath49 for [ alg : main_algo ] is given by , @xmath277}}{n-1}\\big\\ } \\label{eq : lambda_upper}\\ ] ]    now , if we compare the general upper bound for any data collection algorithm ( eq .  ) with [ alg : main_algo ] s upper bound ( eq .  ) , since @xmath278}=1 $ ] and for regular graphs @xmath279 , so we always achieve a rate which is at least a factor @xmath0 less than any other data collection algorithm .",
    "this result is clear from the simulations ( figure  [ fig : graph5_opt_ratio_dd_our_reg ] ) where we compare [ alg : main_algo ] with directed diffusion method .",
    "similarly , for non - regular graphs since @xmath278}\\leq 1 $ ] and @xmath280 where @xmath281 is the minimum degree of the graph @xmath22 , so we are at least a factor @xmath281 less than others .",
    "so , in order to achieve data collection in low configured networks using [ alg : main_algo ] we need to compromise on rate by certain factor .",
    "next , we generalize our results for a network of @xmath1 nodes with only @xmath50 source nodes , one sink and rest as relay nodes .",
    "[ lem : rate_bound_k_sources ] for a given graph @xmath282 with @xmath283 nodes and source set @xmath284 with @xmath285 data sources , each receiving data as independent bernoulli arrivals with stable data arrival rate @xmath18 ( which ensures finite average data collection time @xmath49 ) is given by @xmath286 where @xmath62 is the second largest eigenvalue of transition matrix @xmath287 of graph @xmath288 given by [ alg : main_algo ] .",
    "the corresponding network throughput is @xmath65 .",
    "in particular , for @xmath0-regular graphs @xmath289 where @xmath290 is the second largest eigenvalue of adjacency matrix of @xmath288 .    for a given graph @xmath282 , let , the source set be @xmath291 such that @xmath292 , @xmath18 be the data arrival rate and @xmath293 be the transition matrix of given graph for the [ alg : main_algo ] algorithm .",
    "we consider steady state queue vector @xmath186 and @xmath189 vector same as defined in proof of theorem [ thm : rate_bounds ] .",
    "let , @xmath191 be the usual identity matrix , so only row vector @xmath294 needs to be redefined .",
    "let the @xmath1 element row vector @xmath294 be defined as follows @xmath295 so , steady state queue equations at nodes is given by , @xmath296 now , using the analysis similar to step  1 of the proof of theorem  [ thm : rate_bounds ] , we have @xmath297 but , now based on new @xmath294 , we have @xmath298 using variance bound ( eq . ) and eq.s and in eq .",
    ", we have the lower bound on stable rate which ensures finite average data collection time @xmath49 , @xmath251{m(m + 1 ) } } \\label{eq : lambda_lower_k_src}\\ ] ] for @xmath0-regular graph , we have @xmath299 where @xmath290 is the second largest eigenvalue of its adjacency matrix @xmath300 and @xmath62 is the corresponding eigenvalue of transition matrix @xmath287 of random walk on given graph for [ alg : main_algo ] .",
    "so , similar to theorem  [ thm : rate_bounds ] proof spectral gap for graph is @xmath301 .",
    "so , we can write , @xmath251{m(m + 1 ) } } = \\frac{d - \\lambda_2(\\bar{a})}{d~\\sqrt[]{m(m + 1)}}\\ ] ] hence , the corresponding network throughput is @xmath52 .",
    "similar to eq .",
    ", for graph @xmath288 with @xmath50 data sources upper bound on stable rate @xmath18 for [ alg : main_algo ] with transition matrix @xmath287 is given by @xmath302    next , we find a sufficient bound on rate i.e. @xmath303 network stability is ensured , in terms of basic graph parameters . since , in our network at every time slot , every node except the sink keeps on receiving and forwarding data to their neighbors whereas , sink keeps on receiving data but does not transmit any data towards its neighbors , thereby creating some space in queues of such nodes . this created space propagates among the other nodes and thus can be exploited to receive new arrivals at the nodes .",
    "proof of this lemma is done by analyzing such spaces in queues based on expected queue equation at each node using breadth - first search analysis ( bfs ) where interconnection between nodes is ignored .",
    "[ lem : lambda_sufficient_bound ] for a given graph @xmath22 with data arriving at nodes with rate @xmath18 , sufficient bound on rate which ensures network stability such that the average collection time @xmath49 is finite , is given by , @xmath304 where @xmath17 , @xmath16 are minimum and maximum degrees of nodes and @xmath305 is the diameter of graph @xmath22 .",
    "for our given graph @xmath22 at every time step @xmath306 , with new arrivals coming at rate @xmath18 queues at each node @xmath307 is updated as follows , @xmath308 }   + \\sum_{u_j : u_j\\sim u_i}\\mu_t ( u_j){\\mathcal{p } [ u_j , u_i ] } + \\beta \\label{eq : exp_queue_eq_with_lambda}\\ ] ] we know , for analysis of general graphs we maintain bounded queue size at each node @xmath307 i.e. @xmath309 .",
    "suppose , @xmath310 where , @xmath311 then , for @xmath309 , @xmath18 should be strictly less than @xmath312 to ensure bounded expected queue size constraint .",
    "now , to find the arrival rate @xmath18 such that stability of network i.e. finite average data collection time is ensured we will analyse the space created by sink at the queues of its neighbors which is eventually propagated to the queues of subsequent nodes .",
    "so , eq . can be written as , @xmath313 } + \\sum_{u_j : u_j\\sim u_i}{\\mathcal{p } [ u_j , u_i ] } + \\beta \\label{eq : simplified_exp_queue_eq_with_lambda}\\ ] ] now , for level 1 nodes ( nodes at the distance one from sink ) eq .",
    "can be written as , @xmath314 }   + \\sum_{v : v\\sim u_1}{\\mathcal{p } [ v , u_1 ] } - { \\mathcal{p } [ { u_{\\mbox{\\scriptsize s}}},u_1 ] }   + \\beta\\nonumber\\\\ & \\leq 1 - { \\mathcal{p } [ { u_{\\mbox{\\scriptsize s}}},u_1 ] }   + \\beta \\label{eq : level1_node_eq}\\end{aligned}\\ ] ]    as sink does not forward any data to level 1 node , we subtract the probability of sink sending data to level 1 node .",
    "for level 2 nodes we have , @xmath315 }   + \\sum_{v : v\\sim u_2}{\\mathcal{p } [ v , u_2 ] } + \\beta\\\\ & \\leq 1 - \\sum_{v : v\\sim u_2}{\\mathcal{p } [ u_2,v ] }   + \\sum_{v : v\\sim u_2}{\\mathcal{p } [ v , u_2 ] } - { \\mathcal{p } [ u_1,u_2 ] } \\\\ & + { \\mathcal{p } [ u_1,u_2 ] } ( 1 - { \\mathcal{p } [ { u_{\\mbox{\\scriptsize s}}},u_1 ] }   + \\beta ) + \\beta\\\\ & \\leq 1 - { \\mathcal{p } [ u_1,u_2 ] } + { \\mathcal{p } [ u_1,u_2 ] } ( 1 - { \\mathcal{p } [ { u_{\\mbox{\\scriptsize s}}},u_1 ] }   + \\beta)\\\\ & \\leq 1 - { \\mathcal{p } [ u_1,u_2 ] } { \\mathcal{p } [ { u_{\\mbox{\\scriptsize s}}},u_1 ] } + \\beta { \\mathcal{p } [ u_1,u_2 ] } + \\beta\\end{aligned}\\ ] ] in the above equation , we first subtract the contribution to arrival rate by node @xmath316 with queue size less than one then we add its contribution with its actual queue size as given by .    for level 3 nodes we have , @xmath317 }   + \\sum_{v : v\\sim u_3}{\\mathcal{p } [ v , u_3 ] } + \\beta\\\\ & \\leq 1 - { \\mathcal{p } [ u_2,u_3 ] } { \\mathcal{p } [ u_1,u_2 ] } { \\mathcal{p } [ { u_{\\mbox{\\scriptsize s}}},u_1 ] } + \\beta ( 1 + { \\mathcal{p } [ u_2,u_3 ] } + { \\mathcal{p } [ u_2,u_3 ] } { \\mathcal{p } [ u_1,u_2 ] } ) \\end{aligned}\\ ] ] similarly , for nodes at level @xmath305 we have , @xmath318}\\cdots{\\mathcal{p } [ { u_{\\mbox{\\scriptsize s}}},u_1 ] } \\big)\\nonumber \\\\ & + \\beta \\big ( 1 + { \\mathcal{p } [ u_{d-1},u_d ] } + { \\mathcal{p } [ u_{d-1},u_d ] } { \\mathcal{p } [ u_{d-2},u_{d-1 } ] } \\nonumber \\\\ & + \\cdots\\ +   \\big({\\mathcal{p } [ u_{d-1},u_d ] } \\cdots{\\mathcal{p } [ u_1,u_2 ] } \\big)\\big ) \\label{eq : leveld_eq}\\end{aligned}\\ ] ] now , we know @xmath319 so using eq .",
    "we get , @xmath320}\\cdots{\\mathcal{p } [ { u_{\\mbox{\\scriptsize s}}},u_1 ] } \\big ) + \\beta \\big ( 1 + { \\mathcal{p } [ u_{d-1},u_d ] } + \\cdots \\\\ +   \\big({\\mathcal{p } [ u_{d-1},u_d ] } \\cdots{\\mathcal{p } [ u_1,u_2 ] } \\big)\\big ) \\leq 1\\\\ \\beta \\leq \\dfrac{\\big({\\mathcal{p } [ u_{d-1},u_d ] } \\cdots{\\mathcal{p } [ { u_{\\mbox{\\scriptsize s}}},u_1 ] } \\big)}{\\big ( 1 + { \\mathcal{p } [ u_{d-1},u_d ] } + \\cdots\\ +   \\big({\\mathcal{p } [ u_{d-1},u_d ] } \\cdots{\\mathcal{p } [ u_1,u_2 ] } \\big)\\big)}\\end{aligned}\\ ] ] for our modified random walk , we ve @xmath321 } = \\frac{1}{{\\mbox{deg}(u_i ) } } \\left [ \\frac{{\\mbox{deg}(u_i)}}{{\\mbox{deg}(u_j ) } } \\wedge 1 \\right]$ ] .",
    "so , to get a sufficient bound on @xmath18 we have , @xmath322 in worse case , the distance from sink @xmath305 can be the diameter of graph , hence the result .",
    "we find that the rate at which [ alg : main_algo ] algorithm can collect data is lower bounded by the spectral gap of random walk s transition matrix which is proportional to the spectral gap of the network for regular graphs .",
    "the rate for any data collection algorithm is upper bounded by the edge expansion of graph or the accepting rate of sink depending on whichever is minimum .",
    "we also find the corresponding network throughput with a stable data arrival rate @xmath18 which falls within these bounds .",
    "the cost incurred to achieve such throughput in a _ zero configuration _ operation mode is not too high .",
    "from theorem  [ thm : single_tcol ] we can see that our latency for a single round of data collection is just logarithmic factor higher than worst case latency @xmath59 , as we have bounded degree graphs for practical networks .",
    "also , from the proof of theorem  [ thm : avg_data_coll ] we get that as the number of rounds increase , their average collection time depends only on ( inversely proportional to ) data arrival rate @xmath18 .",
    "this is evident from the fact that , once @xmath18 is fixed , network stability is ensured , so data keeps on flowing freely in the network and gets successfully collected at the sink .",
    "so , the only dependence for such collection time will be the appearance time factor @xmath18 .",
    ".rate bounds for various graphs [ cols=\"<,<,>\",options=\"header \" , ]      let @xmath22 be a @xmath1 node even cycle or ring graph .",
    "so we know , for the given graph second largest eigenvalue of transition matrix for [ alg : main_algo ] is @xmath323 ( see section 12.3 @xcite,@xcite ) .",
    "so , the lower bound on stable rate given by theorem  [ thm : rate_bounds ] is @xmath184{n(n - 1 ) } } =   o\\big(\\frac{1}{n^3}\\big)$ ]    now , for cycle graph we have , for all nodes @xmath8 , @xmath321 } = { \\mathcal{p } [ u_j , u_i ] } = 1/2 $ ] , so @xmath324 = 1 $ ] . also , vertex set @xmath256 minimizes the value of @xmath325 , so @xmath326}/n-1 $ ] for any @xmath327 .",
    "so , from eq .",
    "upper bound on stable rate is @xmath328}{n - 1 } = \\frac{1}{n-1}$ ] .    to validate our bounds we find the stable data rate using first principles .",
    "using steady state queue equation for immediate neighbors of sink @xmath5 we have , for node @xmath316 : @xmath329 and for node @xmath330 : @xmath331 .",
    "similarly , for any general node @xmath11 which is not a neighbor of sink gives @xmath332 using steady state equations of node @xmath316 , @xmath330 and general node @xmath333 , we get @xmath334 now , using general node equation for nodes @xmath335 to @xmath336 and results from successor nodes we will obtain steady state equations in terms of @xmath18 and @xmath337 . now , adding all such steady state equations of node @xmath316 to @xmath336 , we have : @xmath338 , so @xmath339    similarly , repeating the above procedure for nodes @xmath330 to @xmath340 , we have : @xmath341 , so @xmath342 now , adding eq.s and , we have : @xmath343 . using eq .   and the fact that for [ alg : main_algo ] , we have @xmath344 ( see proof of theorem  [ thm : single_tcol ] ) in above equation we get , @xmath345 .",
    "hence , for cycle graph actual rate falls within our proposed bounds , i.e. , neither of the bounds are tight .",
    "the situation is similar for the line .",
    "now , let @xmath22 be a @xmath1 node star graph with sink @xmath5 at center .",
    "now , for [ alg : main_algo ] algorithm with transition matrix @xmath63 , it is easy to check that second eigenvalue of given graph @xmath62 is @xmath346 .",
    "so , from theorem  [ thm : rate_bounds ] lower bound on stable data rate is @xmath184{n(n - 1 ) } } =   \\dfrac{1}{\\sqrt[]{n(n-1)^3}}$ ] .",
    "also , we know for the given graph transition probability @xmath347 } = { \\mathcal{p } [ { u_{\\mbox{\\scriptsize s}}},u ] } = 1/{\\mbox{deg}({u_{\\mbox{\\scriptsize s}}})}$ ] for all @xmath348 , so @xmath324 = 1 $ ] . for given graph , we also have a vertex set @xmath256 for which @xmath325 is minimum , so @xmath326}/n-1 $ ] for any @xmath327 . using this result in eq .",
    "we get the upper bound for data arrival rate at nodes as , @xmath328}{n-1 } = \\frac{1}{n-1}$ ] .",
    "now , to validate our rate bounds we again use the first principle method for finding stable rate of graph .",
    "now , any node @xmath11 other than sink only sends data and has no arrival as sink does nt transmit any data , so steady state queue equation for node @xmath86 with @xmath347 } = { \\mathcal{p } [ { u_{\\mbox{\\scriptsize s}}},u ] } = 1/{\\mbox{deg}({u_{\\mbox{\\scriptsize s}}})}=1/n-1 $ ] is : @xmath349 . as , for [ alg : main_algo ] , we have @xmath350 ( see proof of theorem  [ thm : single_tcol ] ) so @xmath351 . hence , for star graph with sink at center upper bound on rate",
    "is tight .",
    "now let @xmath22 be a random geometric graph where @xmath1 nodes are thrown uniformly at random into a unit disc , we choose transmission radius r such that @xmath352 where @xmath353{\\frac{\\log n}{n}}$ ] is the critical radius .",
    "this ensures that the graph remains connected with high probability @xcite .",
    "the second eigenvalue corresponding to such graph for [ alg : main_algo ] ( gossip setting ) is @xmath354@xcite .",
    "so , from theorem  [ thm : rate_bounds ] lower bound on the stable data arrival rate @xmath18 is @xmath251{n(n - 1 ) } } =   o\\big(\\frac{\\log n}{n^2}\\big)\\ ] ]    now , we know @xmath355 and @xmath356 for rgg ( see theorem 1.4 @xcite ) .",
    "so , for any algorithm ( not only [ alg : main_algo ] ) , with transition probability @xmath34}$ ] , we have @xmath324 \\leq 1 $ ] and from proposition  [ pro : rate_upper_bound ] and eq .",
    "upper bound for data arrival rate at nodes is given by , @xmath357}{n - 1 } \\leq \\frac{1}{n-1 }   = o\\big(\\frac{1}{n-1}\\big)\\ ] ] finding the actual rate from first principles like we did for other examples is difficult , so we skip it here , but we see in section  [ sec : empirical_results ] through simulation that it appears the optimal rate is closer to the upper bound than the lower bound in this case .",
    "in this section we present our rate bounds for some commonly used graphs ( table  [ table : rate_bounds ] ) and discuss our simulation results .",
    "our simulations have been written in c++ and done on a core i7 - 4790 , quad - core 3.6 ghz , 16 gb ram machine . to avoid any inaccuracies due to randomness ,",
    "all results have been plotted after taking an average from three different simulation runs for each instance .",
    "we ran all the simulations for 100000 time slots to capture the packet transmission scenario and each simulation run took less than 2 minutes to execute . in our simulation results ,",
    "we first analyze our algorithm s performance for random geometric graph and @xmath0-regular random graph and then compare our algorithm with the directed diffusion , which is known for high data rate .    random geometric graph ( rgg )",
    "is considered as the most appropriate model for sensor network @xcite as most of its applications require a random deployment of nodes .",
    "we perform our simulations on rggs where nodes are thrown uniformly at random into a unit disc .",
    "we have taken care to use a radius that is large enough to ensure connectivity of the network @xcite . to further validate our results",
    "we perform simulations on @xmath0-regular random graphs .    for all our simulations we consider independent bernoulli data arrivals with parameter @xmath18 at all nodes .",
    "so , each node generates a data packet for each round and by the end of the simulation , different nodes may have generated different rounds of data .",
    "we equip each node with a queue which is used to store data packets which are not forwarded in given time slot .",
    "there is no priority given to any particular round s data packet in any of our simulations .",
    "we choose a node at the center as the sink .",
    "our algorithm , [ alg : main_algo ] s performance is independent of sink location , centrally located sink is chosen to provide the best working environment for directed diffusion with which we compare our results . at every step ,",
    "the sink absorbs any packet it receives .",
    "we say that a particular round of data is sunk when sink has received all the data packets of that round .",
    "as the data arrival rate @xmath18 increases queues at nodes start filling up and _ coupon collector problem _ @xcite steps in .",
    "hence , the probability of data packets of earlier rounds being chosen for transmission is reduced , as a result of which the number of rounds sunk starts decreasing after particular rate resulting in infinite average collection time .",
    "so , our _ stable data rate _ @xmath18 , as defined in section  [ subsec : performance_metrics ] is the rate which ensures finite average collection time @xmath49 i.e. the maximum rate after which the number of rounds sunk starts decreasing .",
    "we also define _ stability region _ as the range of values of the data rate which ensures network stability .",
    "[ alg : main_algo ] s stability region lies within the bounds defined by theorem [ thm : rate_bounds ] and proposition [ pro : rate_upper_bound ] .",
    "we compute the upper and lower bound for @xmath18 using theorem  [ thm : rate_bounds ] for both rgg and d - regular graph with degree @xmath358 for [ alg : main_algo ] algorithm .",
    "we also find the stable data rate @xmath18 as defined in section [ subsec : simulation_setup ] and plot the rate versus number of nodes in graph .",
    "figure  [ fig : graph1_rate_bounds_rgg ] and figure  [ fig : graph2_rate_bound_reg5 ] validate our rate bounds as the stable rate lies within our bounds for both the graphs .      in directed diffusion @xcite , based on the query and corresponding reply links between neighbors , sink reinforces an optimal low - delay path and each node randomly selects a packet from its queue and forwards it along the reinforced path .",
    "it is evident from figure [ fig : graph3_rate_comp_rgg ] and figure [ fig : graph4_rate_comp_reg500 ] that though [ alg : main_algo ] s stability region is much smaller and corresponding throughput is lower but within [ alg : main_algo ] s stability region ( indicated on graph between our lower and upper bound ) its performance is comparable to that of directed diffusion .",
    "also , in directed diffusion , every node deterministically forwards a data packet in each time slot , so @xmath258 whereas , in [ alg : main_algo ] @xmath359}}{n-1}$ ] . since , in practical scenarios we have bounded degree networks advantage of directed diffusion over us is bounded . figure",
    "[ fig : graph5_opt_ratio_dd_our_reg ] shows that the performance difference between two algorithms is directly proportional to degree of network . as the degree of nodes decreases , the ratio of the stable rate of [ alg : main_algo ] and that of directed diffusion also decreases .",
    "= 500,scaledwidth=40.0% ]",
    "in this paper , we propose a simple decentralized and fault - tolerant metropolis random walk based algorithm for data collection in a multi - hop sensor network .",
    "this algorithm is low configuration based and incurs no routing overhead .",
    "we analyze its latency and throughput under the assumption that each sensor node except the sink generates data as independent bernoulli arrivals .",
    "though our latency result is not comparable to routing based methods , it is just a logarithmic factor higher than the hitting time of a random walk on the network which is comparable to the worst case latency .",
    "we find bounds on network throughput given a stable data arrival rate that ensures finite average collection time .",
    "in particular , we find rate is lower bounded by the spectral gap of transition matrix at the heart of our algorithm",
    ". this lower bound is proportional to network s spectral gap for regular graphs .",
    "we also find a generalized upper bound for any data collection algorithm .",
    "we also generalize the rate result for @xmath50 data sources and find a high probability bound on the average collection time of data arrival rounds and conclude that it is inversely proportional to the stable data arrival rate .",
    "then , using simulations we compare our generalized algorithm with application - specific directed diffusion method and find that though our throughput is lower , the performance difference between two methods is proportional to the degree of network and also our performance is comparable to directed diffusion method in our stability region .    [ [ incorporating - interference - and - the - full - duplex - question ] ] incorporating interference and the full - duplex question + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as mentioned in section  [ sec : intro ] , our model of the sensor network assumes simultaneous transmission - reception and multi - packet reception for each node in a given time slot .",
    "while these assumptions may be valid for full - duplex wired networks , they are not reasonable for wireless networks .",
    "however , it is important to note that if we were to move into an aloha - like situation where we allow for transmissions to fail due to the collision , both these assumptions are no longer required .",
    "modeling this in a probabilistic setting is not difficult , and it is easy to see it will lead to an increase in latency which will grow with the degree of the network .",
    "there will be a small drop in throughput as well since the probability of reducing the queue ( buffer ) size ( through successful transmission ) will decrease .",
    "both these changes can be incorporated in our model without changing the essential ideas underlying the proofs of our main theorems and so we omit them here .    as part of future work , analyzing [ alg : main_algo ] by considering branching random walks ( currently we have a branching factor of 1 ) for data collection is an interesting line of work .",
    "we can also perform a similar analysis and mathematical abstractions for more sophisticated methods that use network coding like growth codes @xcite to provide robust data collection in failure - prone sensor networks .",
    "we believe that our results form a baseline analytical framework that can be used to analyze average collection time in various settings .",
    "+   + * acknowledgment*. the authors would like to thank naveen garg for some very useful discussions .",
    "10    a.  a. abbasi and m.  younis .",
    "a survey on clustering algorithms for wireless sensor networks .",
    ", 30(14):28262841 , 2007 .",
    "k.  akkaya and m.  younis .",
    "a survey on routing protocols for wireless sensor networks .",
    "3(3):325349 , 2005 .    i.  f. akyildiz , w.  su , y.  sankarasubramaniam , and e.  cayirci .",
    "wireless sensor networks : a survey . , 38(4):393422 , 2002 .",
    "n.  alon , c.  avin , m.  koucky , g.  kozma , z.  lotker , and m.  r. tuttle .",
    "many random walks are faster than one . , 20(04):481502 , 2011 .    c.  avin and c.  brito .",
    "efficient and robust query processing in dynamic environments using random walk techniques . in _ proc . of the 3rd intl .",
    "symposium on information processing in sensor networks _ , ipsn 04 , pages 277286 , new york , ny , usa , 2004 .",
    "s.  banerjee , p.  gupta , and s.  shakkottai . towards a queueing - based framework for in - network function computation .",
    ", 72(3):219250 , 2012 .",
    "r.  j. barton and r.  zheng .",
    "order - optimal data aggregation in wireless sensor networks using cooperative time - reversal communication . in _ proc . of the 40th annual conf . on information sciences and systems _ , ciss 06 , pages 10501055 , march 2006 .",
    "s.  boyd , a.  ghosh , b.  prabhakar , and d.  shah .",
    "gossip algorithms : design , analysis and applications . in _ proc .",
    "ieee 24th annual joint conf . of the ieee computer and communications societies _ , volume  3 of _",
    "infocom 05 _ , pages 16531664 vol .",
    "ieee , march 2005 .",
    "d.  braginsky and d.  estrin .",
    "rumor routing algorithm for sensor networks . in _ proc . of the 1st acm",
    "workshop on wireless sensor networks and applications _ , wsna 02 , pages 2231 , new york , ny , usa , 2002 .",
    "chau and p.  basu . exact analysis of latency of stateless opportunistic forwarding . in _ proc .",
    "ieee 28th annual joint conf . of the ieee computer and communications societies _ , infocom 09 , pages 828836 .",
    "ieee , 2009 .    f.  r.  k. chung . . , ( 92):214 , 1994 .",
    "r.  cristescu , b.  beferull - lozano , and m.  vetterli . on network",
    "correlated data gathering . in _ proc .",
    "ieee 23th annual joint conf . of the ieee computer and communications societies _ , volume  4 of _",
    "infocom 04 _ , pages 25712582 vol.4 .",
    "ieee , march 2004 .",
    "a.  demers , d.  greene , c.  hauser , w.  irish , j.  larson , s.  shenker , h.  sturgis , d.  swinehart , and d.  terry . epidemic algorithms for replicated database maintenance . in _ proc . of the sixth annual acm symposium on principles of distributed computing _ , podc 87 , pages 112 .",
    "acm , 1987 .",
    "e.  j. duarte - melo and m.  liu .",
    "data - gathering wireless sensor networks : organization and capacity . , 43(4):519  537 , 2003 . wireless sensor networks .",
    "a.  giridhar and p.  r. kumar .",
    "computing and communicating functions over sensor networks . , 23(4):755764 , april 2005 .",
    "p.  gupta and p.  r. kumar .",
    "the capacity of wireless networks .",
    ", 46(2):388404 , mar 2000 .",
    "w.  r. heinzelman , a.  chandrakasan , and h.  balakrishnan .",
    "energy - efficient communication protocol for wireless microsensor networks . in _",
    "system sciences , 2000.proc . of the 33rd annual hawaii intl . conf . on",
    "_ , hicss 00 , pages 10pp .",
    "ieee , 2000 .",
    "w.  r. heinzelman , j.  kulik , and h.  balakrishnan .",
    "adaptive protocols for information dissemination in wireless sensor networks . in _ proc . of the 5th annual acm / ieee intl . conf . on mobile computing and networking _ , mobicom 99 , pages 174185 , new york , ny , usa , 1999 .",
    "s.  ikeda , i.  kubo , and m.  yamashita .",
    "the hitting and cover times of random walks on finite graphs using local degree information .",
    ", 410(1):94100 , 2009 .",
    "o.  d. incel and b.  krishnamachari . enhancing the data collection rate of tree - based aggregation in wireless sensor networks . in _",
    "2008 5th annual ieee communications society conf . on sensor , mesh and ad hoc communications and networks _ , secon 08 , pages 569577 .",
    "ieee , 2008 .    c.  intanagonwiwat , r.  govindan , d.  estrin , j.  heidemann , and f.  silva .",
    "directed diffusion for wireless sensor networking .",
    ", 11(1):216 , feb .",
    "s.  k. iyer and d.  thacker .",
    "nonuniform random geometric graphs with location - dependent radii . , pages 20482066 , 2012 .",
    "a.  kamra , v.  misra , j.  feldman , and d.  rubenstein .",
    "growth codes : maximizing sensor network data persistence . in _ proc . of the 2006 conf .",
    "on applications , technologies , architectures , and protocols for computer communications _ , sigcomm 06 , pages 255266 , new york , ny , usa , 2006 .",
    "b.  karp and h.  t. kung .",
    "gpsr : greedy perimeter stateless routing for wireless networks . in _ proc . of the 6th annual intl .",
    "conf . on mobile computing and networking _ , mobicom 00 ,",
    "pages 243254 , new york , ny , usa , 2000 .",
    "r.  karp , c.  schindelhauer , s.  shenker , and b.  vocking .",
    "randomized rumor spreading . in _",
    "foundations of computer science , 2000.proc .. 41st annual symposium on _ , focs 00 , pages 565574 .",
    "ieee , 2000 .",
    "s.  kashyap , s.  deb , k.  v.  m. naidu , r.  rastogi , and a.  srinivasan .",
    "efficient gossip - based aggregate computation .",
    "in _ proc . of the twenty - fifth acm sigmod - sigact - sigart symposium on principles of database systems",
    "_ , pods 06 , pages 308317 , new york , ny , usa , 2006 .",
    "h.  kenniche and v.  ravelomananana .",
    "random geometric graphs as model of wireless sensor networks . in _ computer and automation engineering ( iccae ) , 2010 the 2nd intl .",
    "conf . on _ , volume  4 , pages 103107 .",
    "ieee , 2010 .",
    "d.  a. levin , y.  peres , and e.  l. wilmer . .",
    "american mathematical soc . , 2009 .",
    "s.  lindsey and c.  s. raghavendra .",
    "pegasis : power - efficient gathering in sensor information systems . in _",
    "aerospace conf .",
    "proc . , 2002 .",
    "ieee _ , volume  3 of _ aeroconf 02 _ , pages 31125 .",
    "ieee , 2002 .",
    "i.  mabrouki , x.  lagrange , and g.  froc .",
    "random walk based routing protocol for wireless sensor networks . in _ proc . of the 2nd intl",
    ". conf . on performance evaluation methodologies and tools _ , valuetools 07 , page  71 .",
    "icst ( institute for computer sciences , social - informatics and telecommunications engineering ) , 2007 .",
    "t.  moscibroda .",
    "the worst - case capacity of wireless sensor networks . in _",
    "2007 6th intl .",
    "symposium on information processing in sensor networks _ , ipsn 07 , pages 110 , april 2007 .",
    "d.  mosk - aoyama and d.  shah .",
    "computing separable functions via gossip . in _ proc .",
    "of the twenty - fifth annual acm symposium on principles of distributed computing _ , podc 06 , pages 113122 .",
    "acm , 2006 .",
    "m.  j. neely .",
    "delay analysis for maximal scheduling with flow control in wireless networks with bursty traffic .",
    ", 17(4):11461159 , aug .",
    "m.  penrose . .",
    "number  5 .",
    "oxford university press , 2003 .",
    "d.  puccinelli and m.  haenggi .",
    "wireless sensor networks : applications and challenges of ubiquitous sensing . , 5(3):1931 , 2005 .",
    "d.  shah .",
    "network gossip algorithms . , pages 36733676 , 2009 .",
    "s.  sternberg . .",
    "courier corporation , 2010 .",
    "[ lem : appearnce_time_c ] for a constant @xmath360 , value of @xmath160 satisfying the inequality @xmath361 depends only on factor @xmath18 .",
    "let @xmath362 be a constant , we will find a value of @xmath160 which satisfies the inequality @xmath363 above inequality can be rewritten as , @xmath364 since , @xmath365 for @xmath366 , so , @xmath367 and we can write eq . as , @xmath368 we know , if @xmath369 and @xmath370 then , @xmath371 implies @xmath369 .",
    "so , value of @xmath160 which satisfies eq .",
    "will also satisfy eq .. now , solving eq .",
    ", @xmath372 for @xmath373 , @xmath374 .",
    "so , we need a solution to @xmath375 such that the condition @xmath373 is satisfied .",
    "we have , @xmath376 @xmath377 where @xmath163 is a constant . since @xmath378 , eq .",
    "clearly satisfies the required condition and is only @xmath18 dependent ."
  ],
  "abstract_text": [
    "<S> we analyze a decentralized random - walk based algorithm for data collection at the sink in a multi - hop sensor network . </S>",
    "<S> our algorithm , metropolis - collect , which involves data packets being passed to random neighbors in the network according to a simple metropolis random - walk mechanism , requires no configuration and incurs no routing overhead . to analyze this method , we model the data generation process as independent bernoulli arrivals at all nodes except the sink . </S>",
    "<S> we analyze both latency and throughput in this setting , providing a theoretical upper bound for latency and a theoretical lower bound for throughput . while our random walk - based method is not comparable to routing - based methods in terms of latency , we prove that it is theoretically no more than a logarithmic factor higher than the worst case latency . </S>",
    "<S> the main contribution of our paper , however , is the throughput result : we show that the rate at which our algorithm can collect data is lower bounded by the spectral gap of the random walk s transition matrix , which is , in turn , proportional to the spectral gap of the network for regular graphs . through simulations </S>",
    "<S> we compare our rate bounds to the routing - based directed diffusion method , showing that although our method achieves a lower throughput ( while incurring no setup cost ) , the difference in the two methods is directly proportional to the degree of the network .    * _ keywords : _ * data collection , throughput , random walk , sensor networks </S>"
  ]
}