{
  "article_text": [
    "there is a stochastic multigrid method and a deterministic one . the stochastic version is used to compute high dimensional integrals in euclidean quantum field theory or statistical mechanics by a monte carlo method which uses updates at different length scales  @xcite .",
    "the deterministic version  @xcite solves discretized partial differential equations .",
    "one hopes to use both of them in simulations of lattice qcd , for updating the gauge fields and for computing fermion propagators in given gauge fields . in either case",
    "the aim is to beat critical slowing down ( csd ) in nearly critical systems .",
    "our notation is as follows : @xmath0 denotes a given `` fundamental '' lattice @xmath1 of spacing @xmath2 .",
    "coarser ( block ) lattices of increasing spacings @xmath3 are denoted @xmath4 . typically , we chose @xmath5 , and a single point as the last layer @xmath6 .",
    "interpolation operators @xmath7 are introduced to transfer functions on coarser lattices into functions on finer lattices , while restriction operators @xmath8 transfer functions from finer to coarser lattices ( `` variational coarsening '' ) .",
    "a crucial problem is how to define and exhibit smooth functions in the disordered context , i.e. when translation symmetry is strongly violated .",
    "other possible applications besides gauge theories are low lying states of spin glasses , the shape of a lightning , waves on fractal lattices ( with bond percolation ) , or the localization of low lying electronic states in amorphous materials .    in the case of deterministic mg",
    ", one wants to solve a discretized elliptic differential equation on @xmath1 : d_0 ^ 0 = f^0  .",
    "[ de ] it might have arisen from an eigenvalue equation @xmath9 by inverse iteration .",
    "if @xmath10 has a small eigenvalue , then local relaxation algorithms suffer from csd .",
    "after some relaxation sweeps on @xmath1 one gets an approximate solution @xmath11 whose error @xmath12 is not necessarily small but is _ smooth ( on length scale @xmath2 ) .",
    "the unknown error @xmath13 satisfies the equation d_0 e^0 = r^0 .",
    "[ dee ] with the residual @xmath14 .",
    "given that @xmath13 is smooth , it can be obtained by smooth interpolation of a suitable function @xmath15 on @xmath16 , e^0 = ^1 e^1  .",
    "[ intp0 ] that is , @xmath17 with @xmath18 which depends smoothly on @xmath19 .",
    "now define a restriction operator @xmath20 such that @xmath21 .",
    "then  ( [ intp0 ] ) can be inverted , @xmath22 applying @xmath23 to both sides of  ( [ dee ] ) yields an equation for @xmath24 , d_1 e^1 = r^1  , [ cgrideq]with @xmath25 and the effective operator @xmath26 . given @xmath24 , one obtains @xmath13 from  ( [ intp0 ] ) , and @xmath27 is an improved solution of  ( [ de ] ) .",
    "thus , _ the problem has been reduced to an equation on the lattice @xmath28 which has fewer points . if necessary , one repeats the procedure , moving to @xmath29 etc .",
    "the procedure stops , because an equation on a `` lattice '' @xmath6 with only a single point is easy to solve .",
    "_ _    the iterated interpolation @xmath30 } \\equiv \\a^1 \\a^2 \\dots \\a^j$ ] from @xmath31 to @xmath1 should yield functions on @xmath0 which are _ smooth on length scale @xmath32 , i.e. which change little over a distance @xmath32 ( in the ordered case ) . for reasons of practicality , one must require @xmath33 unless @xmath19 is near @xmath34 . _",
    "a successful mg scheme , whether deterministic or stochastic , needs smooth interpolation kernels @xmath35 .",
    "thus we may ask : _ which functions are smooth in the disordered situation , for instance in an external gauge field ? _    a ( gauge covariant ) naive answer is _ ( _ , _ ) = ( , - ) _ 0 ( , ) ( with discretized covariant derivatives @xmath36 ) . by definition ,",
    "the lowest eigenvalue @xmath37 of the negative covariant laplacian @xmath38 is not small for disordered gauge fields .",
    "( it is positive and vanishes only for pure gauges . )",
    "therefore there are no smooth functions in this case .",
    "nevertheless there is an answer to the question , assuming a fundamental differential operator @xmath10 is specified by the problem ( in the stochastic case , the hamiltonian often provides @xmath10 ) :    _ a function @xmath39 on @xmath40 is smooth on length scale @xmath41 when @xmath42 in units @xmath43 . _    we found that a deterministic multigrid which employs interpolation kernels @xmath44}$ ] from @xmath45 to the fundamental lattice @xmath1 which are smooth in this sense , works for arbitrarily disordered gauge fields .",
    "when there are no smooth functions in this sense at length scale @xmath2 , then @xmath10 has no low eigenvalue , and there is no csd and no need for mg .",
    "the above answer appears natural , and the `` projective mg '' of  @xcite is in its spirit . but to obtain kernels @xmath44}_{zx}$ ] which are smooth _ on length scale @xmath32 , one needs approximate solutions of eigenvalue equations d_0 ^[0j]_zx = _",
    "0(x ) ^[0j]_zx [ eva ] since @xmath44 } $ ] is required to vanish for @xmath19 outside a neighbourhood of @xmath46 , the problem involves dirichlet boundary conditions . for large @xmath47 , @xmath44 }",
    "$ ] will have a large support .",
    "if there is no degeneracy in the lowest eigenvalue , one can use inverse iteration combined with standard relaxation algorithms for the resulting inhomogeneous equation .",
    "but this and other standard methods will suffer from csd again .",
    "moreover , in the standard multigrid setup , one uses basic interpolation kernels @xmath48 which interpolate from one grid @xmath49 to the next finer one . in this case ^[0j ] = ^1 ^2 ",
    "^j  , [ factor ] and  ( [ eva ] ) becomes a very complicated set of nonlinear conditions .",
    "possible solutions are _",
    "( i ) : :    replace  ( [ eva ] ) by minimality of a cost functional ( cp . later ) .",
    "use    neural algorithms to find kernels @xmath7 which minimize    it .",
    "this is still under study .",
    "( ii ) : :    give up factorization  ( [ factor ] ) and determine independent kernels    @xmath44}$ ] as solutions of  ( [ eva ] ) by multigrid    iteration .",
    "_ this is done successively for @xmath50    one uses already determined kernels @xmath51}$ ] with    @xmath52 for updating @xmath44}$ ] .",
    "_ we found that    this works very well - cp .",
    "[ performance ] .    method ( ii ) will need of order @xmath53 storage space and @xmath54 computational work for a @xmath55-dimensional system of linear extension @xmath56 .",
    "any iteration to solve  ( [ de ] ) amounts to updating steps of the form ^0 ^ 0 = ^0 +  f^0  .",
    "[ iterlin ] with the iteration matrix @xmath57 whose norm governs the convergence , and @xmath58 . if @xmath59 , the iteration converges with a relaxation time @xmath60 .",
    "_ parameters in the algorithm - such as operators @xmath61 , @xmath62 and @xmath63 - are _ optimal if the cost functional @xmath64 is at its minimum .",
    "_ _    as an example , consider a twogrid iteration in which a standard relaxation sweep on @xmath1 with iteration matrix @xmath65 is followed by exact solution of the coarse grid equation  ( [ cgrideq ] ) .",
    "the second step leads to an updating with some iteration matrix @xmath66 , and @xmath67 .",
    "therefore one may estimate @xmath68 with @xmath69 ( fine grid relaxation smoothens the error but does not converge fast - therefore @xmath70 is suppressed whereas @xmath71 is not much smaller than @xmath72 ) and try to optimize the parameters above by minimizing @xmath73 : using the trace norm , @xmath74 , one finds e_1 = volume^-1 _ z , w^0",
    "|_zw|^2 with @xmath75 . prescribing @xmath23 , and determining @xmath76 and @xmath77 by minimizing @xmath73 yields what we call the `` ideal interpolation kernel '' @xmath18 for a given restriction map @xmath20 .",
    "since it has exponential tails instead of vanishing for @xmath19 outside a neighbourhood of @xmath46 , it is impractical for production runs , though  @xcite .",
    "a feed - forward artificial neural network ( ann )  @xcite can perform the computations to solve  ( [ dee ] ) by mg relaxation .",
    "the nodes ( `` neurons '' ) of the nmg are identified with points of the mg as shown in fig .",
    "[ figur1 ] .",
    "the resulting nmg consists of two copies of the same mg , except that the last layer @xmath78 is not duplicated . in the standard mg approach , the basic interpolation kernels @xmath7 interpolate from one layer @xmath45 to the preceding one , @xmath79 .",
    "each node is connected to some of the nodes in the preceding layer . in the upper half ,",
    "the connection strength from @xmath80 to @xmath81 is @xmath61 . in the lower half ,",
    "node @xmath82 is connected to @xmath80 with strength @xmath83 .",
    "in addition there is a connection of strength @xmath84 between the two nodes which represent the same point @xmath19 in @xmath31 ( @xmath85 ) .    according to hebb s hypothesis of synaptical learning",
    ", a biological neural network learns by adjusting the strength of its synaptical connections .",
    "the network receives as input an approximate solution @xmath39 of  ( 2.1 ) , from which the residual @xmath86 is then determined .",
    "it computes as output an improved solution @xmath87 .",
    "the desired output ( `` target '' ) is @xmath88 .",
    "@xmath89 is a linear function of @xmath90 . except on the bottom layer",
    ", each node receives as input a weighted sum of the output of those nodes below it in the diagram to which it is connected .",
    "the weights are given by the connection strengths .",
    "our neurons are linear because our problem is linear .",
    "the output of each neuron is a linear function of the input .",
    "the result of the computation is = ( _ 0  d_0 ^ -1 + _ k1 ^[0k ]  _ k   d_k^-1  r^[k0 ] )  r^0 where @xmath91}=r^k ... r^2r^1 $ ] and @xmath51}= \\a^1\\a^2 ... \\a^k$ ] . the operators @xmath92 and @xmath93 , and the damping parameters @xmath94 @xmath95 are not needed separately since they only enter in the combination @xmath96 .",
    "the fundamental differential operator @xmath97 and its diagonal part @xmath98 are furnished as part of the problem .",
    "the connection strengths ( `` synaptical strengths '' ) @xmath61 , @xmath83 ( and possibly @xmath99 ) need to be found by a learning process in such a way that the actual output is as close as possible to the desired output . in supervised learning  @xcite , pairs @xmath100 ( `` training patterns '' )",
    "are presented to an ann . given input @xmath101 , the actual output @xmath102 is compared to the target @xmath103 , and the connection strengths are adjusted in such a way that the cost functional @xmath104 gets minimized .",
    "an iterative procedure to achieve this minimization is called _ learning rule .",
    "_    taking for the sequence @xmath101 a complete orthonormal system of functions on @xmath1 , in the limit @xmath105 , the target is @xmath106 for any input , and the output @xmath107 by  ( [ iterlin ] ) .",
    "the learning rule for the resulting cost functional e= _  ^^2= tr  _",
    "^2 is our previous optimality condition for multigrid relaxation in sect .",
    "[ criteria ] .",
    "the variant ( ii ) in sect .  [ smoothness ] involves a slightly different nmg . instead of the connections between neighbouring layers of the multigrid , we now have connections from @xmath1 to @xmath108 with strength @xmath109}_{xz}$ ] , and from @xmath31 to @xmath1 with strength @xmath44}_{zx}$ ] .",
    "if we adopt variational coarsening , all connection strengths are determined by interpolation kernels @xmath51}$ ] which have to be learned .",
    "the damping factors @xmath110 were set to @xmath72 and @xmath111 is the diagonal part of @xmath112 as before , with d_k = ^[0k]d_0 ^[0k ]  . the learning rule ( ii )",
    "requires a process of `` hard thinking '' by the nmg .",
    "nodes which have learned their lesson already - i.e. which have their connection strengths fixed - are used to instruct the rest of the neural net , adjusting the strengths of the next layer of nodes in the nmg .",
    "a variant of this algorithm was tested in 2 dimensions , using su(2)-gauge fields which were equilibrated with standard wilson action at various values of @xmath113 , and @xmath114 .",
    "@xmath37 is the lowest eigenvalue of the covariant laplacian @xmath38 , and @xmath115 .",
    "conventional relaxation algorithms for solving  ( [ de ] ) suffer from csd for such @xmath10 , for any volume and small @xmath116 .",
    "it turned out that it was not necessary to find accurate solutions of the eigenvalue equation for the interpolation kernels @xmath44}$ ] .",
    "an approximation @xmath44}_{zx}$ ] to @xmath117 was computed by multigrid iteration .",
    "it does not depend on @xmath116 .",
    "updating @xmath39 at @xmath118 changes @xmath39 by _",
    "z= ^[0j]_zxd_j , x^-1r^j_x  ,   r^j=^[0j]r^0 .",
    "the convergence rate ( in units of mg iterations ) of the @xmath119-iteration is shown in fig .",
    "[ figur2 ] for @xmath120 .",
    "one mg iteration involved one sweep ( in checkerboard fashion ) through each mg layer , starting with @xmath121 .",
    "99 g. mack , in : nonperturbative quantum field theory , eds .",
    "g. t hooft et al .",
    "( plenum , new york 1989 ) .",
    "j. goodman and a.d .",
    "sokal , phys .",
    "d 40 ( 1989 ) 2035 .",
    "w. hackbusch , multi - grid methods and applications ( springer - verlag , berlin , 1985 ) .",
    "brower , k. moriarty , c. rebbi and e. vicari , nucl .",
    "b 20 ( 1991 ) 89 .",
    "a. hulsebos , j. smit and j.c .",
    "vink , nucl .",
    "b 368 ( 1992 ) 379 .",
    "t. kalkreuter , phys .",
    "b 276 ( 1992 ) 485 .",
    "j. hertz , a. krogh and r.g.palmer , introduction to the theory of neural computation ( addison - wesley , redwood city ca , 1991 ) ."
  ],
  "abstract_text": [
    "<S> we present evidence that multigrid ( mg ) works for wave equations in disordered systems , e.g. in the presence of gauge fields , no matter how strong the disorder . </S>",
    "<S> we introduce a `` neural computations '' point of view into large scale simulations : first , the system must learn how to do the simulations efficiently , then do the simulation ( fast ) . </S>",
    "<S> the method can also be used to provide smooth interpolation kernels which are needed in multigrid monte carlo updates .    </S>",
    "<S> h  # 1 # 1 </S>"
  ]
}