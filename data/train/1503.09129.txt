{
  "article_text": [
    "the importance of neuronal spike - timing in neural and cognitive information processing has been indicated at in a variety of studies @xcite . for example , in the olfactory system the precision of spike - timing has been associated with accurate odor - classifications @xcite , and populations of auditory neurons are known to signal input features by the relative timing of spikes @xcite . however , an understanding of how the brain learns to reliably associate specific input patterns with desired spike responses through synaptic strength modifications remains a significant challenge .    from experimental observations ,",
    "is widely believed to underpin learning in the brain @xcite , which can induce either long- or short - term potentiation in synapses based on local variables such as the relative timing of spikes , voltage and firing frequency @xcite .",
    "this closely follows hebb s postulate for associative learning : ` cells that fire together , wire together ' @xcite .",
    "drawing on this as inspiration , a variety of supervised learning rules have been proposed that can train either single- or multilayer networks to generate desired output spike patterns in response to spatio - temporal spiking input patterns ( for a recent review , see @xcite ) .    with respect to single - layer networks , the learning rule introduced by ponulak et al .",
    "@xcite , , is a notable example of how can be applied in teaching a neuron to generate desired output spikes ; from assuming an appropriate functional dependence of synaptic weight changes on the relative timing between actual and target output spikes , for example an exponential decay @xcite , the dissimilarity between an actual and target output spike train can be minimized by an analogous method to the widrow - hoff learning rule .",
    "practical advantages of include being independent of the neuron model implementation , and its rapid convergence when learning to perform arbitrary input - output spike pattern associations .",
    "however , although retains a high degree of biological plausibility through its incorporation of , it still remains a heuristically derived learning rule , and therefore can not be assumed to provide optimal solutions .",
    "an alternative and more rigorous formulation of using gradient descent has been proposed by sporea & grning @xcite .    a further supervised rule , proposed by pfister et al .",
    "@xcite , instead takes a statistical approach that optimizes by gradient ascent the likelihood of generating desired output spike times . in this case , a probabilistic spiking neuron model is used to provide a smooth functional dependence of output spike times with respect to network parameters . from simulations ,",
    "the authors demonstrated a resemblance of the learning rule to experimentally observed , and demonstrated its applicability on an example ` detection ' learning task where input patterns were identified by the responses of readout neurons ; in previous work , we have further demonstrated the high capacity achievable with this this rule when training networks to encode multiple input patterns by specific output spike patterns @xcite .",
    "an important advantage of this method are its general applicability to a wide range of learning paradigms : from supervised @xcite to reinforcement @xcite learning .",
    "furthermore , it has been shown that a unique global maximum in the log - likelihood of generating a desired target spike pattern can be found when taking gradient ascent for a single - layer network @xcite . despite this",
    ", there still exists little work that has aimed at exploring its performance when encoding for a large number input spike patterns , with a reasonable number of spikes , by the precise timing of output spikes .",
    "additional single - layer learning rules have been developed for spiking neural network @xcite , many of which have used either an objective error function that is minimized by gradient descent or an analog of the widrow - hoff learning rule .",
    "key examples include the tempotron @xcite , which has shown a strong capability in learning binary classifications of input patterns , and the chronotron @xcite , that can learn to classify a large number of input patterns into multiple categories by the precise timing of output spikes .",
    "comparatively , the majority of research has focused on training single- rather than multilayer networks .",
    "existing work that has examined networks containing hidden spiking neurons include spikeprop proposed by bohte et al",
    ". @xcite , multilayer by sporea & grning @xcite and the recurrent network learning rules formulated by brea et al .",
    "@xcite and rezende & gerstner @xcite .",
    "learning rules for spiking networks have proven to be a challenge to formulate , and especially given the discontinuous nature of neuronal spike - timing .",
    "a typical solution has been to assume a linear dependence of a neuron s spike - timing on presynaptic inputs around its firing threshold , such that small changes in its input with respect to synaptic weights shifts the timing of an output spike . however , such an approach has the disadvantage of constraining the learning rate to a small value @xcite .",
    "an alternative approach has instead treated a spiking neuron as a stochastically firing unit , where spikes are distributed according to an underlying instantaneous firing rate , that in turn has a smooth dependence on network parameters ; for example , in multilayer a linear poisson neuron model was used as a substitute for deterministic spiking neurons in each layer , during its derivation @xcite .",
    "multilayer learning rules have demonstrated success on several benchmark classification tests , including the linearly nonseparable xor computation and iris dataset @xcite that can not otherwise be solved by single - layer networks .",
    "however , aside from the work of @xcite , no attempts have been made in establishing the performance of a multilayer spiking network when learning to perform a large number of input - output spike pattern mappings ; it is likely that the presence of more than one layer can enhance the storage capacity of the network , by increasing the number of spiking neurons that can perform computations on network inputs .",
    "progress in this area has been hindered by the complexity that arises from applying learning rules to multilayer spiking networks .",
    "much of the previous work examining the performance of both single- and multilayer learning rules for spiking networks have considered simplified coding schemes .",
    "for example , both spikeprop and the chronotron have used the latency of single output spikes to encode for different input patterns , and the tempotron used a binary spike / no - spike output code to discriminate between two classes of inputs .",
    "ideally , for spiking networks a fully temporal coding scheme would be taken advantage of such that input patterns were encoded by the precise timing of multiple output spikes .",
    "we have previously indicated the advantages of using a fully temporal code in @xcite , and in particular found that multiple , rather than single output spikes , increased the reliability of classifications .",
    "most learning rules have been applied to networks containing just a single output neuron .",
    "biologically , however , it is well known that populations of neurons encode for similar patterns of activity , such that the detrimental impact of synaptic noise on neural processing can be eliminated @xcite . in a series of notable studies",
    "@xcite , groups of spiking neurons receiving shared input patterns were simulated to mimic such a population - based coding scheme : with the key result that the speed of learning increased with the population size .",
    "such studies were devised in the framework of reinforcement learning and typically used a spike / no - spike or latency code to classify input patterns ; hence , it would be of interest to investigate populations of spiking neurons utilizing a fully temporal code with multiple output spikes .    here",
    "we derive a supervised learning rule for a multilayer network of spiking neurons which is capable of encoding input spike patterns by the precise timings of multiple output spikes .",
    "our rule extends the single - layer learning rule of pfister et al .",
    "@xcite to multiple layers by combining the method of stochastic gradient ascent with backpropagation .",
    "we demonstrate the efficacy of the proposed learning rule on a wide variety of learning tasks : both in terms of the accuracy of input pattern classifications and the time taken to converge in learning .",
    "we find the learning rule can encode for a large of number of input patterns , comparing favourably with previous multilayer learning rules , and results in increased classification accuracy when classifying inputs by the timings of multiple rather than single output spikes .",
    "the learning rule is further applied to multilayer networks containing multiple output neurons , where we measure the dependence of the performance on the specific network setup when mapping between spatio - temporal spike patterns .",
    "finally , we propose a biologically plausible implementation of the multilayer learning rule , and predict the underlying neural mechanisms that might guide the learning of desired target output spike trains .",
    "our multilayer learning rule differs from those proposed by brea et al .",
    "@xcite and rezende & gerstner @xcite , which have instead taken gradient descent on the kl - divergence in a supervised and reinforcement setting respectively .",
    "the novelty of our paper comes from the application of backpropagation , and its indicated high performance when encoding for a large number of input spike patterns as multiple and precisely timed output spikes .",
    "we introduce our learning rule for a feedforward network of spiking neurons containing a hidden layer . the performance of the proposed learning rule is then examined on a variety of benchmark tests : first for multilayer networks containing a single output neuron as the readout , and secondly for multilayer networks containing multiple output neurons . with respect to single - output networks , learning tasks include : measuring the resilience of the network to noise , solving the xor computation , a comparison over specific network setups , the storage capacity of the network and its ability to generalize on a synthetic dataset . for multiple - output networks ,",
    "the performance of the learning rule is tested on mapping between multiple input - output spatio - temporal spike patterns , and the ratio of hidden to output neurons required to attain reliable input classifications .",
    "finally , we present an alternative and more biologically plausible formulation of backpropagated learning , and compare its performance against that of our derived rule for both single- and multiple - output multilayer networks .      [ [ neuron - model . ] ] neuron model .",
    "+ + + + + + + + + + + + +    we consider a postsynaptic neuron , indexed @xmath0 , that receives its input from other presynaptic neurons @xmath1 .",
    "if the postsynaptic neuron generates a list of spikes @xmath2 in response to the presynaptic spike pattern @xmath3 , then its membrane potential at time @xmath4 is defined by the @xcite : @xmath5 where @xmath6 is the synaptic weight between neurons @xmath1 and @xmath0 , and both @xmath7 and @xmath8 denote a convolution between a spike train and a kernel @xmath9 and reset kernel @xmath10 respectively ( methods ) .",
    "a spike train is given as a sum of dirac @xmath11 functions : @xmath12 , and a convolution is defined by @xmath13 in our analysis we implement a stochastic neuron model , such that postsynaptic spikes are distributed according to an instantaneous firing rate : @xmath14 \\;,\\ ] ] where @xmath15 $ ] is a monotonically increasing function of the neuron s membrane potential ( see also eq .",
    "[ eq : exp_rate ] ) .    [ [ supervised - learning . ] ] supervised learning .",
    "+ + + + + + + + + + + + + + + + + + + +    the learning rule is derived for a fully connected feedforward network containing a single hidden layer .",
    "input layer neurons just present spike patterns to the network , while both hidden and output neurons are free to perform computations on their respective inputs .",
    "input layer neurons are indexed as @xmath16 , hidden neurons @xmath17 and output neurons @xmath18 .    both hidden and output neurons have their spikes distributed according to eq .",
    "[ eq : firing_density ] ; the advantage of implementing a stochastic neuron model is that it allows for the determination of the likelihood for generating a specific output spike pattern .",
    "hence , if the likelihood of generating a list of target output spikes @xmath19 in response to @xmath20 is @xmath21 , then the likelihood of generating a spatio - temporal target pattern @xmath22 is given by the product @xmath23 with log - likelihood ( methods ) : @xmath24 where @xmath25 , @xmath26 the duration over which @xmath20 is presented and @xmath27 the output firing rate .",
    "we aim to maximize the log - likelihood of generating a target output spike pattern by taking gradient ascent with respect to synaptic weights in the network . for clarity ,",
    "we just consider a network containing a single hidden layer , although our technique can straightforwardly be extended to include multiple hidden layers .    from taking gradient ascent on eq .",
    "[ eq : log_likelihood ] ( methods ) , the output layer weight update rule is determined as @xmath28 where @xmath29 is the output learning rate and @xmath30 an output neuron error signal .",
    "this error signal measures the dissimilarity between a target response @xmath31 and the actual output activity @xmath27 , that is given by @xmath32 \\;,\\ ] ] where @xmath33 is a parameter that controls the variability of output spike times ( eq .",
    "[ eq : exp_rate ] ) . from the above",
    ", we find positive values for @xmath30 signal the timings of desired output spikes , while negative values signal erroneous output activity .",
    "the above learning rule was originally derived by pfister et al .",
    "@xcite for a single - layer network , that has been found to well approximate the functional form of observed experimentally in @xcite .",
    "an example of a weight update taking place in the output layer is shown in fig .",
    "[ fig1 ] .",
    ", in response to an input pattern lasting duration @xmath26 , where hidden spike times are indicated by vertical lines .",
    "note that thicker lines indicate phasic bursting , that evoke stronger responses in output layer neurons .",
    "the right panel shows the membrane potential of an output neuron , that responds to stimulation from hidden layer neurons . in this example , the output neuron must learn to generate spikes at the times indicated by the dotted lines .",
    "_ bottom row : _ the left panel is the evoked at output layer neurons due to hidden neuron spikes , that is defined by eq .",
    "[ eq : convolution ] .",
    "the right panel is the candidate weight change between the hidden and output neurons shown in this example , that depends on both the hidden - evoked and the accuracy of the output activity , according to eq .",
    "[ eq : output_rule ] .",
    "note the depressions in @xmath34 correspond to the timings of actual output spikes , that are slightly too early with respect their targets , while the increases take place at the timings of target output spikes , subject to the output error signal @xmath30 ( eq . [ eq : backprop_error ] ) .",
    "in this case , the final update @xmath34 at time @xmath26 is positive , that demonstrates the causal role of the hidden neuron spikes in eliciting accurate output spike times . ]    by taking gradient ascent on eq .",
    "[ eq : log_likelihood ] and using the technique of backpropagation ( methods ) , the hidden layer weight update rule is found as @xmath35 \\ast \\epsilon)(t ) \\mathrm{d}t \\;,\\ ] ] where @xmath36 is the hidden learning rate , @xmath37 a parameter that controls hidden neuron spiking variability and @xmath38 $ ] denotes a double convolution ( eq . [ eq : double_convolution ] ) .",
    "an example of a weight update taking place in the hidden layer is shown in fig .",
    "[ fig2 ] . given the dependence of weight updates on the availability of hidden neuron spike ,",
    "it is necessary that a degree of variable activity persists in the hidden layer : an absence of hidden activity would otherwise prevent updates from taking place and result in stagnated learning . to this end",
    ", hidden weights are additively modified through synaptic scaling ( methods ) .    .",
    "_ top row : _ the left panel shows an input neuron spike train , that is distributed according to a poisson process .",
    "the middle panel shows the membrane potential of a selected hidden neuron , which is partly stimulated by input spikes from the preceding panel .",
    "the right panel shows the membrane potential of an output neuron , with target output spike - timings indicated by dotted lines . _",
    "bottom row : _ shown along this row are a series of synaptic traces , that have a functional dependence on the last panel .",
    "the left panel is the evoked at hidden layer neurons , due to the input spike train above .",
    "the middle panel is a double convolution ( eq . [ eq : double_convolution ] ) , that captures the correlation between input and hidden neuron spike - timing . in this case",
    ", only input and hidden neuron spikes up to the first half of @xmath26 are correlated .",
    "the right panel shows the progression of the candidate weight change @xmath39 between the input and hidden neurons selected in this example , that depends on the synaptic trace shown in the middle panel and the accuracy of output spikes , according to eq .",
    "[ eq : hidden_rule ] . in this case , the final update @xmath39 effected at time @xmath26 is positive : demonstrating the causal role of the input spike train in driving an accurate output spike train . ]",
    "the multilayer learning rule was tested in simulations of networks of stochastic neurons , that performed temporally precise input - output spike pattern mappings . in all simulations ,",
    "input patterns were represented by the firing times of @xmath40 input layer neurons , where an input pattern consisted of a poisson spike train at each input neuron with a mean firing rate of ( methods ) .",
    "input patterns were presented episodically to the network in no particular order , and weight changes were applied at the end of each episode . depending on the learning task ,",
    "a variable number @xmath41 of hidden neurons were implemented in the network to establish the dependence of the performance on the hidden layer size .",
    "here we first present results from simulations of a multilayer network containing a single output neuron as its readout , and then extend our analysis to include a network containing multiple output neurons .",
    "for each experiment , a more detailed description of the network setup can be found in the methods section .",
    "the performance of the learning rule is demonstrated by training a multilayer network to perform generic input - output spike pattern mappings .",
    "we first focus on the relatively simple task of performing a single input - output mapping , and then extend our analysis to more complex multiple input - output mappings that are subject to noise .",
    "[ [ single - input - output - mapping . ] ] single input - output mapping .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    a multilayer network was trained to map between an input pattern and a target output spike train .",
    "the network contained 10 hidden neurons and a single output neuron , which was tasked with learning the timings of five target output spikes .",
    "an illustration of the network setup is shown in fig .",
    "[ fig3 ] , along with example spike rasters depicting input , hidden and output neuron spiking activity over a typical simulation run .    in this example , we examined a hidden neuron that contributed strongly to the responses of the output neuron close to the target spike times : ( fig .",
    "[ fig3]b ) . from this hidden neuron spike raster , highly variable spike times",
    "were observed over the first 200 episodes , that subsequently fine - tuned themselves to the timings of target output spikes ; this initial phase of variable activity demonstrated a form of stochastic exploration by the network , during which time desirable hidden spike patterns were discovered by the network which contributed to accurate output spike times .",
    "as learning progressed , hidden neurons generated bursts of spikes around the timings of target output spikes , such that the likelihood of evoking accurate output responses was increased . in this simulation",
    ", the majority of hidden layer neurons contributed to driving accurate output spiking responses , hence the load imposed on the network in the form of hidden synaptic modifications was more evenly distributed amongst them .    from the output",
    "spike raster ( fig .",
    "[ fig3]c ) it is clear that every target output spike was learnt successfully , and within just 100 episodes .",
    "however , because a stochastic rather than a deterministic neuron model was implemented , a small degree of variation in the timings of output spikes about their respective targets was apparent . despite this , the network still generated output responses to a sufficiently high level of accuracy , that is supported by the measure ( defined in eq .",
    "[ eq : vrd ] ) with a final average value @xmath42 ( fig .",
    "[ fig3]e ) . for an impression of this value",
    ", a distance of 0.55 corresponds to a typical time shift of between paired actual and target output spikes .",
    "input neurons , @xmath43 hidden neurons and a single output neuron .",
    "the input pattern was repeatedly presented to the network over 1000 episodes , where each episode lasted duration @xmath44 .",
    "the target output spike train contained five spikes at times : .",
    "( a ) a spike raster of the input pattern that was presented to the network on each episode .",
    "( b ) the activity of a hidden neuron with each episode , that contributed strongly to the firing times of the output neuron .",
    "( c ) the activity of the output , where the five target output spike times are indicated by crosses .",
    "( d ) an illustration of the multilayer network setup .",
    "( e ) the evolution of the distance between the actual output and target output spike trains of the network , given as a moving average of the @xmath45 with each episode ( methods ) and taken over 100 independent simulation runs .",
    "the shaded region shows the standard deviation . ]",
    "[ [ synaptic - weight - distributions . ] ] synaptic weight distributions .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    shown in fig .",
    "[ fig4 ] is an example of the evolution of both hidden and output synaptic weights with the number of learning episodes and their final distribution , that corresponds to the previous experimental setup . in the left panel ( fig .",
    "[ fig4]a ) , the weights on the hidden neuron can be seen to diverge continuously during learning , with almost twice as many positive as negative weights by the final episode .",
    "this contrasts with the evolution of the weights on the output neuron ( fig .",
    "[ fig4]b , left panel ) , which attained rapid convergence during learning .",
    "we note that in our implementation output weights were confined to positive values , while hidden weights had no such restriction ( methods ) ; preliminary simulations indicated that negative output weight values for a single output neuron had little impact on its performance .",
    "at the end of learning , hidden weights closely followed a gaussian distribution ( fig .",
    "[ fig4]a , right panel ) and output weights a positively skewed distribution ( fig .",
    "[ fig4]b , right panel ) , with coefficients of variation @xmath46 and @xmath47 in the magnitude of hidden and output weight values respectively .",
    "hence , in terms of the absolute value , hidden weights were more widely dispersed than output weights by a factor of just over four .    .",
    "* _ left column : _ an example of the evolution of synaptic weights with the number of learning episodes .",
    "_ right column : _ the final distribution of synaptic weights @xmath48 after 1000 learning episodes .",
    "the panels in row ( a ) correspond to hidden layer weights @xmath49 : the left panel shows the evolution of the first 20 weights on the hidden neuron shown in fig .",
    "[ fig3 ] , and the right panel shows the final distribution over all hidden layer weights .",
    "the panels in row ( b ) show output neuron weights @xmath6 : the left panel shows the evolution of all 10 weights on the output neuron shown in fig .",
    "[ fig3 ] , and the right panel shows the final distribution of output weights . for both panels showing the final distribution of weights , 100 independent simulation runs were taken . ]",
    "[ [ multiple - input - output - mappings - with - noise . ] ] multiple input - output mappings with noise .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we next tested the performance of the multilayer network when learning to map between 10 input - output spike pattern pairs and the impact of input noise on learning .",
    "in this case , each input pattern was identified by a unique target output spike time .",
    "the network contained @xmath50 hidden neurons and a single output neuron . in this experiment ,",
    "we introduced two new measures : the time shift @xmath51 and the performance @xmath52 . the time shift was taken as a moving average of the absolute difference between matching actual and target output spikes : @xmath53 with each episode , which was computed only for instances when exactly one actual output spike was generated which provided a correct input classification .",
    "the measure @xmath52 was taken as a moving average of the network classification performance ( methods ) .",
    "the time shift @xmath51 shared the same averaging window as for @xmath52 , and its motivation came from providing a more physical perspective of the spike train dissimilarity measure @xmath54 .",
    "the performance @xmath52 measured the accuracy of network classifications based on a temporal code , as described in the methods section .",
    "as shown in fig .",
    "[ fig5 ] learning took place over @xmath55 episodes to ensure convergence , where an input - output pattern pair was randomly selected and presented to the network on each episode .",
    "noise was introduced to the network by jittering the timing of each input spike according to a gaussian distribution at the start of every episode , with a standard deviation or amplitude that ranged in value from between .    from the row of panels in fig .",
    "[ fig5]a we found that noiseless input patterns resulted in the most accurate output spike times , providing a final distance of @xmath56 and a typical time shift of . by comparison , introducing amplitude of input jitter ( fig .",
    "[ fig5]b ) gave a final distance of @xmath57 and resulted in output spikes shifted by , thereby reducing the temporal precision of output spikes by a factor of five . in terms of the accuracy of input classifications",
    ", noiseless inputs resulted in a high performance level of , which dropped to with the addition of amplitude of input jitter .",
    "input noise increased the time taken to converge in learning , taking and episodes for noiseless and noisy ( jitter ) inputs respectively .",
    "hidden neurons and a single output neuron .",
    "each input pattern was associated with a unique target output spike .",
    "_ left column : _ the between actual and target output spike trains . _ middle column : _ the time shift between matching actual and target output spikes .",
    "_ right column : _ the performance @xmath58 of the network ( methods ) , when recognizing input patterns by the timing of an output spike .",
    "( a ) learning in the absence of any input noise , and ( b ) learning with intermediate input noise .",
    "input noise was simulated by adding jitter to the timings of input spikes on each episode , where jitter with an amplitude of was used in ( b ) .",
    "( c ) averaged values after @xmath55 learning episodes , as a function of the input jitter amplitude . in all panels , each value was averaged over 20 independent runs , and error runs show the standard deviation .",
    "]    the panels in fig .",
    "[ fig5]c summarise results obtained for amplitude of input jitter , which show a smooth decrease in the network performance with the degree of input noise .",
    "however , even for up to amplitude of input jitter output spikes still fell within of their targets and inputs were classified correctly at least of the time .",
    "this remains well above the chance performance level of , thereby demonstrating the robustness of the multilayer network to strong input noise .",
    "the learning rule has proven capable of training a multilayer network to perform generic input - output spike pattern mappings , and in particular when applied for inputs subject to a high level of noise .",
    "we have also indicated the necessity of both active and variable hidden neuronal spiking to ensure convergence of the learning rule , which was supported through synaptic scaling of hidden weights .",
    "next , we examine in more detail the advantages of introducing a hidden layer , and compare the performance of our multilayer learning rule against that for a single - layer network .      in this section",
    "we compare the performance of multi- and single - layer networks as applied to an example classification task , and when performing an increasing number of arbitrary input - output spike pattern mappings .",
    "the aim is to support the validity of our multilayer learning rule as an efficient neural classifier .",
    "[ [ the - xor - computation . ] ] the xor computation .",
    "+ + + + + + + + + + + + + + + + + + + +    the learning rule was applied to solving the exclusive - or ( xor ) computation , that is a non - trivial classification task .",
    "this is considered a standard benchmark for neural network training , given that a hidden layer is necessary for its solution @xcite .",
    "an xor computation maps two binary inputs to a single binary output as follows : @xmath59 , @xmath60 , @xmath61 and @xmath62 . to represent binary values as spike patterns",
    ", we used a similar setup to that in @xcite .",
    "for the inputs , each binary value was encoded by a set of 50 poisson spike trains with a mean firing rate of , predetermined at the start of each simulation run ; hence , paired binary input values were represented by spike patterns over two groups of 50 neurons . for the output",
    "a latency coding scheme was used , where the binary values 0 and 1 corresponded to late / early output neuron spike - timings of and respectively . in our simulations we considered multi- and singlelayer networks : both networks contained 100 input neurons and a single output neuron , and the multilayer network contained 10 hidden neurons . for single - layer networks ,",
    "[ eq : output_rule ] was applied to updating input - output weights .",
    "binary inputs were presented to the network episodically in a random order .",
    "a correct classification of an input was made when an actual output spike train was closest to its target output as measured by the .    from fig .",
    "[ fig6]a it can be seen that the multilayer network was successful at learning the xor computation within 1000 episodes , with a final accuracy approaching .",
    "the single - layer network , however , maintained an accuracy around that is consistent with chance level . it is further apparent from fig .",
    "[ fig6]b that the multilayer network was capable of separating the two classes , such that output spike responses for each input class matched their respective targets .",
    "in contrast , the single layer network generated erroneous output spikes in response to both input classes , which is indicative of its failure to discriminate between the two classes .",
    "hence , these results support the necessity of including a hidden layer in a spiking network when solving the linearly non - separable xor computation .",
    "input neurons and a single output neuron .",
    "the multilayer network contained @xmath43 hidden neurons .",
    "the two binary variables were encoded as predetermined spike patterns over two populations of input neurons , each of size 50 .",
    "the latency of an output spike coded for the binary value 1 ( early spiking ) or 0 ( late spiking ) .",
    "( a ) evolution of the classification accuracy for multi- and single layer networks , averaged over 20 independent runs .",
    "( b ) output spike rasters for multilayer ( left panel ) and single layer ( right panel ) networks , taken over the final 60 episodes on an example run .",
    "black dots correspond to responses from inputs \\{0 , 1 } and \\{1 , 0 } , and grey dots correspond to responses from inputs \\{0 , 0 } and \\{1 , 1}. target spike times are indicated as crosses for each class of input .",
    "results were averaged over 20 independent runs . ]    [ [ multiple - input - output - mappings . ] ] multiple input - output mappings .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the performance of various network setups when learning to map between an increasing number of input - output spike patterns was tested .",
    "specifically , the performance of three different network setups were examined : a ` free ' multilayer network , a ` fixed ' multilayer network and a single - layer network .",
    "both free and fixed multilayer networks contained 10 hidden neurons and a single output neuron , but differed from each other by their restriction on hidden weight updates : a free multilayer network was allowed changes in the hidden weights during learning by eq .",
    "[ eq : hidden_rule ] , while hidden weights were not allowed to change in the fixed multilayer network other than through synaptic scaling .",
    "the single - layer network lacked a hidden layer and contained a single output neuron . for a more direct comparison , both multi- and single - layer networks contained 100 input neurons .    shown in fig .",
    "[ fig7 ] is the dependence of the network performance on the number of input patterns @xmath63 , up to a maximum of 40 , where each input pattern was associated with a unique target output spike . from the left panel ,",
    "it is clear that both the free multilayer and single - layer networks outperformed the fixed multilayer network over the entire range of input patterns considered ; for example , after learning 40 inputs the performance values were , , and for free , single and fixed respectively .",
    "the performance of the fixed multilayer network remained consistently low over the entire range of inputs considered , with a maximum value of for just two inputs .",
    "hence , it is apparent that allowing hidden weight updates to take place is necessary in training a multilayer network to perform input - output pattern mappings . from comparing the free multilayer and single - layer networks",
    ", it can be seen that the performance of the single - layer network was greatest for less than 12 inputs ; however , for a greater number of inputs the performance of the free multilayer network dominated over the single - layer network . over the entire range of inputs considered ,",
    "the performance of the free multilayer network remained around , and showed no indication of decreasing .",
    "are free to be updated according to eq .",
    "[ eq : hidden_rule ] , and the red curve is a multilayer network with fixed hidden weights .",
    "the green curve corresponds to a single - layer network with no hidden layer .",
    "each network contained a single output neuron .",
    "left panel error bars show the standard deviation , and right panel error bars show the ; the convergence measure was subject to high variance in most cases , therefore just the average number of episodes taken to converge in learning was considered , and not its distribution .",
    "results were averaged over 20 independent runs . ]    shown in the right panel are the number of episodes taken for each network to converge in learning ( methods ) , as a function of the number of inputs .",
    "it can be seen that the convergence time for a multilayer network increased with the number of inputs , and was an order of magnitude larger for free in comparison with fixed when learning 40 inputs .",
    "the difference in convergence time between the free and fixed multilayer networks was attributed to the increased performance of the free multilayer network : a larger number of episodes was necessary to reach an increased performance value .",
    "finally , the convergence time for a single - layer network decreased when learning more than 18 inputs , which coincided with a rapid drop in its performance level .",
    "to summarise , these results are supportive of multilayer over single - layer learning , and importantly when linearly non - separable classifications are performed for which the presence of a hidden layer is essential . in order for single - layer networks to remain competitive with multilayer networks",
    "when mapping between a large number of patterns it would be necessary to scale up the number of input layer neurons , although clearly this would be disadvantageous when more sparse input representations are desired .",
    "an important consideration when training any neural network is the maximum amount of information it can memorize . therefore , we measured the dependence of the performance on the number of input patterns that were presented to a multilayer network , that extends the previous experiment in fig .",
    "[ fig7 ] . given our implementation of a multilayer network",
    ", we also explored the dependence of the performance on the hidden layer size .",
    "finally , the dependence of the performance on the number of target output spikes used to identify inputs was tested .",
    "the aim was to establish the relationship between the hidden layer size and the number of target output spikes that could be supported , and how this impacted on the network capacity .    , and the number of target output spikes @xmath64 .",
    "* in all cases , the network contained one output neuron . in this experiment ,",
    "input patterns @xmath63 were equally assigned between @xmath65 classes .",
    "_ left : _ the performance as a function of the number of input patterns , for @xmath43 ( a ) , @xmath66 ( b ) and @xmath67 ( c ) hidden neurons . in each panel ,",
    "different colour curves correspond to the number of target output spikes belonging to each class .",
    "_ right : _ the number of episodes to convergence in learning .",
    "results were averaged over 20 independent runs . ]    in this experiment , the network was tasked with classifying an increasing number of input patterns @xmath63 into @xmath65 different classes .",
    "an equal number of input patterns were assigned to each class , and all inputs belonging to the same class were identified by a unique target output spike train containing between 1 and 10 spikes . in terms of the network setup , the network contained either 10 , 20 or 30 hidden neurons and a single output neuron as the readout .    fig .",
    "[ fig8 ] shows the multilayer performance as a function of the number of input patterns and the number of target output spikes @xmath64 identifying each class of input . from comparing results between the different hidden layer sizes , a larger number of hidden neurons",
    "was found to support more target output spikes at a given level of performance .",
    "for example , 10 hidden neurons resulted in decreased performance when trained on more than a single output spike , for more than 60 input patterns ( fig .",
    "[ fig8]a ) , while 30 hidden neurons resulted in increased performance when trained on at least five output spikes , over the entire range of input patterns considered ( fig .",
    "[ fig8]c ) . furthermore , from a closer inspection of fig .",
    "[ fig8 ] , it can be seen that over a small region of input patterns @xmath68 the network performance approached when trained on multiple rather than single output spikes , which was more pronounced for a larger number of hidden neurons . to give an indicator of the network s capacity ,",
    "the maximum number of input patterns learnt at a performance greater than was around 100 , 150 and 200 for 10 , 20 and 30 hidden neurons respectively .    in terms of the time taken by the network to perform input classifications ,",
    "the number of episodes increased with both the number of hidden neurons and number of output spikes : taking up to longer for 30 over 10 hidden neurons when trained on 200 input patterns and 10 target output spikes .",
    "a decreased number of episodes taken to converge in learning was generally indicative of the networks inability to learn all input patterns .    in fig .",
    "[ fig9 ] we show in more detail the dependence of the multilayer performance on the number of target output spikes , for different hidden layer sizes .",
    "this figure corresponds to the same setup as used in fig .",
    "[ fig8 ] when classifying 150 input patterns into 10 classes . as found previously ,",
    "a larger number of hidden neurons supported more target output spikes ; for example , in the case of 30 hidden neurons , the performance reached a maximum value when trained on around four target output spikes . for just 10 hidden neurons ,",
    "however , the performance was at its maximum value when trained on just a single target output spike , and decreased as the number of spikes increased . also evident is an increase in the time taken to learn all inputs for more than 10 hidden neurons and an increasing number of spikes .     that were equally assigned between @xmath65 classes , that corresponds to fig .",
    "_ left : _ the performance as a function of the number of target output spikes , for @xmath43 , @xmath66 and @xmath67 hidden neurons .",
    "_ right : _ the number of episodes to convergence in learning .",
    "results were averaged over 20 independent runs . ]    from these experiments it is evident that an increase in the hidden layer size provides more capacity to the network , and is supportive of multiple - spike target output trains for more reliable input classifications .",
    "these results can be attributed to the internal representations of input patterns afforded by hidden layer neurons , such that class discriminations can be performed at an early stage before being processed by the readout .",
    "qualitatively , it was observed from spike rasters that individual hidden neurons selectively responded to certain input patterns , and only contributed to generating a fraction of the total number of target output spikes . from this , it is apparent that neurons in the hidden layer are capable of distributing the synaptic load imposed upon them , as previously found for the experiment in fig .",
    "[ fig3 ] when performing single input - output mappings .",
    "the ability of the network to generalize from stored patterns to similar , new input patterns was tested .",
    "here we built on the earlier experiment ( c.f . fig .",
    "[ fig5 ] ) which examined the impact of noise on mapping between multiple input - output pattern pairs , by instead considering a more realistic data set that contained several classes of input patterns .",
    "the network was tasked with identifying similar inputs belonging to the same class by the timings of output spikes .",
    "we devised a synthetic data set that was inspired by mohemmed et al .",
    "specifically , the accuracy ( or classification performance ) of the network was tested on a generated dataset that consisted of both training and testing patterns , where the aim of the network was to learn to classify patterns between 10 classes . in generating the training patterns ,",
    "a single reference spike pattern was randomly created for each class . each of the 10 reference patterns were then duplicated 15 times , where every duplicate was subsequently jittered according to a gaussian distribution with a noise amplitude between 2 and . hence , a total of 150 training patterns were generated . in the same way ,",
    "25 testing patterns were generated for each class , giving a total of 250 testing patterns .",
    "each class of training and testing patterns were associated with a unique target output spike train , that contained between 1 and 5 spikes . on this task ,",
    "only training patterns were used to train the network , and testing patterns were used to test the ability of the network to generalise .",
    "the network contained 20 hidden neurons and a single output neuron .",
    "hidden neurons and a single output neuron .",
    "the number of training patterns was 150 , and the number of testing patterns 250 .",
    "both training and testing patterns were equally assigned between 10 classes .",
    "_ left : _ the testing and training accuracy as a function of the amplitude of input jitter in the generated dataset , for 5 and 10 target output spikes per class .",
    "_ right : _ the testing and training accuracy as a function of the number of target output spikes , at amplitude input jitter .",
    "to ensure convergence in learning , the number of training episodes was 75000 .",
    "results were averaged over 20 independent runs . ]    shown in fig .",
    "[ fig10 ] is the network accuracy as a function of the noise amplitude , used to generate input patterns at initialization , and the number of target output spikes .",
    "as can be expected , a high degree of noise presented a greater challenge to the network , given that the network had to learn to generalize well in order to accurately classify unseen patterns during the testing phase . despite this , the network still managed to classify testing patterns at least of the time at noise .",
    "furthermore , it is clear that multiple target output spikes led to more accurate classifications in comparison with a single target output spike , giving an increase of almost at noise . from the right panel , a smooth increase in the accuracy with the number of target output spikes at noise can be seen , along with a reduction in the standard deviation ; the accuracy of one target output spike was , compared with for five output spikes .",
    "however , the difference in the accuracy between single and multiple target output spikes became minimal as the noise amplitude approached .",
    "the network is able to generalize well to similar input patterns , and especially when classifications are performed using multiple target output spikes .",
    "two key reasons explain the increase in accuracy with the number of target spikes .",
    "the first relates to the redundancy inherent in multi - spike based classifications : even if an actual output spike train can not match its target in terms of the number of spikes , an accurate classification can still be performed if those spikes which remain are close enough to their respective targets .",
    "the second reason comes from the larger separation between classes as the number of target output spikes increases : class discriminations made by the network are less prone to error from fluctuating output responses .",
    "our learning rules support weight updates in a multilayer network containing more than one output neuron , therefore , we tested the performance of the network when learning to map between spatio - temporal input and output patterns . here a spatio - temporal output pattern consisted of a unique target spike train at each output neuron , and were combined to identify specific input classes ( methods ) .",
    "[ [ single - input - output - mapping.-1 ] ] single input - output mapping .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    first , we considered a mapping between a single input - output pattern pair , where the network was tasked with learning a target spatio - temporal output pattern in response to a single , fixed input pattern . in this experiment ,",
    "the network contained 20 hidden neurons and three output neurons , where each output was assigned a single , unique target spike time . for multiple output neurons , output weights were allowed to change sign during learning ( methods ) .",
    "[ fig11 ] shows an example of a single simulation run , that depicts hidden ( fig . [",
    "fig11]a ) and output ( fig .",
    "[ fig11]b ) neuron spike rasters towards the end of learning . out of the 20 hidden neurons implemented in the network ,",
    "three were selected for demonstrative purposes that contributed intensely to the target output timings . from fig .",
    "[ fig11]a it can be seen that the selected hidden neurons generated stereotypical spike patterns , and particularly around the timings of target output spikes where phasic bursting was observed . in response to hidden layer activity , each output neuron demonstrated a successful learning of their respective target timing ( fig . [ fig11]b ) and to a good degree of temporal accuracy , that is indicated by a final of ( fig .",
    "[ fig11]d ) with a corresponding time shift of at each output .",
    "furthermore , the network learnt to distribute the synaptic load between hidden layer neurons , such that hidden spiking activity became more diverse .",
    "this is supported by a heatmap of the output weight matrix shown in fig .",
    "[ fig11]c , corresponding to the same simulation in panels a and b , which demonstrates a high variance in the synaptic strength between hidden and output neurons .",
    "hidden neurons and @xmath69 output neurons .",
    "each output neuron was assigned a unique target spike for the input , at times : for the first , second and third output neurons respectively .",
    "learning took place over 1000 episodes .",
    "( a ) example hidden neuron spike rasters , and ( b ) output neuron spike rasters , shown over the final 60 learning episodes .",
    "each panel in row ( a ) shows a hidden neuron that contributes strongly to the output neuron response shown in the panel below . in ( b ) , each panel indicates the target spike time of an output with a cross .",
    "the left , middle and right panels show the activity of the first , second and third output neurons respectively .",
    "( c ) heatmap of output layer weights @xmath6 after 1000 learning episodes .",
    "the intensity corresponds to the strength of synaptic weights . for reference , the left ,",
    "middle and right panels in ( a ) show the activity of hidden neuron numbers 15 , 9 and 2 respectively .",
    "( d ) the evolution of the , averaged over 40 independent runs . ]",
    "[ [ dependence - on - the - hidden - layer - size . ] ] dependence on the hidden layer size .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we next explored the performance of the network when input patterns were classified by spatio - temporal output patterns . in this experiment",
    ", a total of 50 input patterns were equally assigned between 10 classes , such that all five patterns belonging to the same class were identified by a unique , target spatio - temporal output pattern .",
    "target output patterns were randomly predetermined for each class of input . to increase the separation between classes , target output spike trains assigned to each output neuron differed from each other by a of at least @xmath70 for @xmath71 output spikes , similarly as for a network containing a single output neuron .",
    "a correct input classification was made when the between an actual and desired target output pattern assumed a minimum value ( methods ) . in measuring the relationship between the performance and network setup , an increasing fractional number @xmath72 of hidden to output neurons",
    "was implemented , for a fixed number of output neurons : @xmath73 or 30 .    from fig .",
    "[ fig12]a it is clear that an increase in the fractional number of hidden - output neurons increased the performance of the network , with the performance approaching for between @xmath74 .",
    "furthermore , there was a dependence of the performance on the number of output neurons ; for example , at a fixed fractional number @xmath75 the performance values were close to for 10 , 20 and 30 output neurons respectively .",
    "hence , it was apparent that a larger number of output neurons increased the separation between classes , while a sufficiently large number of hidden neurons provided support to the output layer during learning .",
    "there was also a trend for fewer @xmath72 needed to reach a performance level of as the number of output neurons increased . in terms of the time taken to converge in learning",
    ", a maximum was found at the first value of @xmath72 for which the performance first approached .     of hidden neurons , and @xmath73 and @xmath76 output neurons .",
    "@xmath77 input patterns were equally assigned between @xmath65 classes , where all patterns belonging to the same class were identified by a unique target output spike pattern .",
    "( a ) _ left : _ the performance as a function of the ratio of hidden to output neurons . _",
    "right : _ the number of episodes to convergence in learning .",
    "( b ) _ left : _ the minimum ratio of hidden to output neurons required to achieve performance , as a function of the number of target spikes at each output neuron .",
    "_ right : _ the number of episodes taken to reach performance .",
    "results were averaged over 10 independent runs . ]",
    "the dependence of the performance on the number of target output spikes was examined . in this case , the absolute number of output neurons was fixed at 10 , and the number of target output spikes taken over the range : @xmath71 . as before ,",
    "the network was tasked with classifying 50 input patterns between 10 classes , with five input patterns assigned to each class .",
    "[ fig12]b shows the minimum fractional number @xmath72 of hidden to output neurons needed by the network to attain performance , as a function of the number of target output spikes .",
    "an increase in the minimum value of @xmath72 with the number of target output spikes was found , that showed an indication of levelling off between 8 and 10 output spikes . in terms of the time",
    "taken to converge in learning , a small increase of @xmath78 when learning 10 spikes compared with one spike was measured .",
    "when learning a single target output spike , the number of episodes to convergence was measured as in fig . [ fig12]b and close to in fig .",
    "[ fig12]a ; this apparent discrepancy can be attributed to the more strict criterion used in fig .",
    "[ fig12]b , that rejected trials where the performance failed to reach by the end of learning .",
    "an important consideration when designing any multilayer network is the hidden layer size , and whether it is sufficient to allow for reasonably accurate input classifications during learning . from the above experiments we have quantified the ratio of hidden to output neurons required by the network to allow for accurate classifications to be made , and in particular for the more general case of fully spatio - temporal based output encodings . to the best of our knowledge ,",
    "this is the first attempt that has aimed to characterise a multilayer network setup for spiking neurons when classifying inputs based on spatio - temporal output patterns .",
    "the technique of backpropagation is commonly associated with poor biological plausibility , an issue that has been challenged in @xcite . to address this",
    ", we propose an alternative and more biologically plausible implementation of our multilayer learning rule .",
    "[ [ reformulation - of - backpropagation . ] ] reformulation of backpropagation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as the learning rule currently stands , output weight updates ( eq . [ eq : output_rule ] ) can be considered biologically plausible , given that updates have a dependence on locally available pre- and postsynaptic activity variables at each synapse .",
    "it is , however , more realistic to effect output weight changes online , for example : @xmath79 the supervisory error signals @xmath30 are specific to each output neuron , and it is reasonable to suppose that desired postsynaptic activity is provided by an ` activity template ' external to the network @xcite , that is a reference output spike pattern originating in another network .",
    "this idea is illustrated in the schematic shown in fig .",
    "[ fig13 ] .",
    "hidden weight updates ( eq . [ eq : hidden_rule ] ) , on the other hand , are more difficult to justify biologically .",
    "although locality at each input - hidden synapse is satisfied in the causal , double convolution term @xmath38 \\ast \\epsilon)$ ] , there is also a non - local dependence on a summation over hidden - specific error signals @xmath80 .",
    "it is unclear by which mechanism the strengths of output weights might be communicated back through the network , and further how these weights would then combine with specific error signals to inform synaptic updates .    to provide a biologically plausible reformulation of backpropagated learning ,",
    "it is therefore necessary to make a few heuristic assumptions regarding the network structure and plasticity .",
    "specifically , we assume output weights are positively valued at initialization and share the same magnitude , and are constrained to positive values during learning . as a result ,",
    "the dependence of hidden weight updates on the values of output weights can be neglected , thereby providing the modified hidden weight update rule : @xmath81 \\ast \\epsilon)(t ) \\;,\\ ] ] where the summation term @xmath82 is now a linear combination of output error signals .",
    "the schematic shown in fig .",
    "[ fig13 ] illustrates this process , and indicates the shared dependence of input - hidden synaptic plasticity changes on multiple output error signals .",
    "we examine the backpropagated error signals @xmath30 , that are shared between all input - hidden synapses .",
    "biologically , it is plausible that a neuromodulator might perform this function , and particularly given the evidence that neuromodulators can influence both the magnitude and direction of synaptic plasticity changes triggered by @xcite .",
    "it is further known that the firing activity of dopaminergic neurons can encode a form of error signal @xcite , and influence cortiocostriatal plasticity by regulating the concentration of dopamine surrounding each synapse @xcite . in light of this , and adopting the method used in @xcite as applied to reinforcement learning , previously instantaneous output error signals @xmath30 used for hidden and output weight updates are instead substituted for concentration - like variables @xmath83 , which evolve according to : @xmath84 \\;,\\ ] ] with a decay time constant @xmath85 . in the above , we have also substituted the instantaneous firing rate of an output neuron for its spike train ( c.f .",
    "[ eq : error_signal ] ) that is transmittable distally .",
    "[ [ performance - of - biological - backpropagation . ] ] performance of biological backpropagation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the performance of the biological implementation of backpropagation ( bio - backprop ) , using eqs .",
    "[ eq : output_rule_bio ] and [ eq : hidden_rule_bio ] , was compared against that of the regular multilayer learning rule ( backprop ) . for bio - backprop",
    ", output error signals @xmath30 were substituted for the filtered signal @xmath83 defined in eq .",
    "[ eq : error_filtered ] .",
    "both learning rules were applied to either a multilayer network containing a single output neuron as the readout , or a multilayer network containing multiple output neurons . in both cases ,",
    "networks were trained to classify input patterns by the timings of single output spikes .",
    "the single - output network contained 10 hidden neurons , and was tasked with classifying an increasing number of input patterns into 10 classes ( c.f .",
    "experiment of fig .",
    "[ fig8 ] ) . as shown in fig .",
    "[ fig14]a , little difference was found in the performance between the bio - backprop and backprop learning rules for less than 80 input patterns .",
    "however , as the number of input patterns increased there was a small performance difference in favour of backprop , approaching by 200 input patterns . in terms of the convergence time , bio - backprop was consistently slower than backprop , taking at least 1.5 times the number of episodes needed by backprop to complete learning .    .",
    "the network contained 10 hidden neurons and a single output neuron .",
    "results were averaged over 20 independent runs .",
    "( b ) the performance of bio - backprop as a function of the ratio of hidden to output neurons , based on the experiment in fig .",
    "[ fig12 ] .",
    "the network contained an increasing number @xmath41 of hidden neurons , and @xmath86 output neurons .",
    "a fixed number of input patterns @xmath77 was used . for reference , results from the regular backpropagation learning rule on the same learning task ( labelled backprop )",
    "is included ( fig .",
    "[ fig12 ] ) .",
    "results were averaged over 10 independent runs . ]",
    "we next considered a multiple - output network containing @xmath86 output neurons and an increasing number @xmath41 of hidden neurons .",
    "the network was tasked with classifying 50 input patterns into 10 classes ( c.f .",
    "experiment of fig .",
    "[ fig12 ] ) . from fig .",
    "[ fig14]b , a marked difference in the performance favouring backprop over bio - backprop was apparent ; while it took just two times the number of hidden to output neurons for backprop to reach a performance level , it took bio - backprop at least ten times the number of hidden to output neurons to reach the same performance level .",
    "in other words , bio - backprop needed five times the number of hidden neurons as backprop to accurately classify all 50 input patterns presented to the network .",
    "with respect to the convergence time , bio - backprop took almost 1.5 times as many episodes as backprop to reach a performance level , using @xmath87 and @xmath88 for bio - backprop and backprop respectively .    from these results , it is clear that the heuristic bio - backprop rule can maintain a similar level of performance to the analytical backprop rule for networks containing a single output neuron .",
    "however , for networks containing more than one output neuron the performance of bio - backprop lagged behind that of backprop ; a reasonable performance level for bio - backprop could only be recovered by a large increase in the number of hidden neurons .",
    "furthermore , weight distributions ( not shown ) indicated a reduced ability of networks trained with bio - backprop to effectively distribute the synaptic load between hidden layer neurons : individual hidden neurons either contributed intensely or weakly to the activity over all output neurons .",
    "ideally , individual hidden neurons would instead be selected to contribute to specific output responses , which was evidenced for the backprop rule ( c.f .",
    "[ fig11]c ) .    despite some limitations , bio - backprop still proved to be a capable learning rule , and maintained a performance level well above the chance level of in most cases .",
    "as such , bio - backprop represents an alternative to our analytical backprop rule when increased biological plausibility is desired .",
    "in this paper we have presented a multilayer learning rule for a spiking network containing a hidden layer that combines gradient ascent with the technique of backpropagation , and demonstrated its high performance when encoding a large number of input patterns by the precise timings of multiple output spikes .",
    "we have further provided an alternative and more biologically plausible implementation of our learning rule , and discussed the underlying neural mechanisms which might support a form of backpropagated learning in the nervous system .",
    "our approach complements the recurrent network algorithms proposed in @xcite .",
    "in our analysis we used the escape noise neuron model defined in @xcite , which has been shown to closely approximate the variable firing activity of neurons _ in - vivo _ @xcite .",
    "our choice of neuron model was primarily motivated by its general applicability in a wide range of learning paradigms , including supervised @xcite and reinforcement @xcite learning .",
    "a key advantage of implementing escape noise neurons comes from being able to determine the probability of generating a specific output spike pattern @xcite , which can then form the basis of a suitable objective function .",
    "here we took the approach of maximizing the log - likelihood of generating a desired output spike pattern in a multilayer network through a combination of gradient ascent and backpropagation , that is an extension of the single - layer learning rule proposed by @xcite to multilayer networks .",
    "output weight updates result from a product of locally available pre- and postsynaptic activity terms , that bears a resemblance to hebbian - like learning ; the presynaptic term originates from filtered hidden neuron spike trains as a , and the postsynaptic term an output error signal that guides the direction and magnitude of weight changes .",
    "hidden weight updates , however , appear as a three - factor rule : s due to input spikes are combined with hidden spike trains , to then be modulated by a linear combination of backpropagated error signals to allow for hidden weight changes .    from training multilayer networks to map between input - output spiking patterns",
    ", it proved necessary to represent input patterns with a sufficiently high degree of spiking activity at each input neuron ; sparse representations otherwise led to decreased performance .",
    "this requirement is apparent from an examination of the hidden layer weight update rule , which has a dependence on hidden neuron spike trains : a lack of input - driven hidden layer activity prevented weight updates from taking place , thereby resulting in diminished learning .",
    "previous multilayer learning rules @xcite have faced a similar challenge in effectively presenting input patterns to the network , but instead took the approach of introducing multiple synaptic connections with varying conduction delays between neurons of neighbouring layers : also termed subconnections .",
    "for example , one presynaptic spike would evoke multiple s at each of its postsynaptic targets through the many subconnections available , thereby driving a sufficient level of hidden layer activity .",
    "combining sparsely encoded input patterns with multiple subconnections represents a plausible alternative to the method we employed in this paper , and could be advantageous when applied to real world datasets such as iris , for which sparse representations of inputs can be ideal @xcite .",
    "we were motivated to introduce synaptic scaling to the network to maintain an optimal range of hidden firing rates @xcite , a process that has been observed in biological networks @xcite . aside from effecting the firing rate , the introduction of synaptic scaling also has side benefits : such as removing the networks over - reliance on the initial values of synaptic weights @xcite , a critical issue that was identified in @xcite .",
    "an important contribution of our paper is the large number of pattern encodings that can be performed by our learning rule : in comparison with multilayer @xcite , trained on a similar network setup , our learning rule was capable of at least @xmath89 as many pattern encodings at a performance level ( fig .",
    "[ fig8 ] ) .",
    "furthermore , we believe our encoding method better took advantage of spike - timing than most alternative methods @xcite ; for example , the tempotron @xcite can only classify input patterns into two classes using a spike / no - spike coding scheme , and the experiments run for the chronotron @xcite and span @xcite required precisely matched output - target spike pairs , which could result in overlearning and impact negatively on generalizing to new input patterns .",
    "in particular , we found our encoding method allowed for increased performance when using multiple target output spikes , and especially for a larger number of hidden layer neurons .",
    "multiple output spikes also allowed the network to better generalize to new input patterns .",
    "backpropagated learning is commonly considered to lack biological plausibility for two key reasons : the first being mechanistic , in the sense that synapses are used bidirectionally to communicate activity variables both forwards and backwards throughout the network ; the second reason is cognitively , since a fully specified target signal must be provided to the network during learning @xcite .    in this paper",
    "we have addressed the first implausibility , by reformulating our hidden layer weight update rule to depend on just local input - hidden synaptic activity variables and a global , linear combination of output - specific error signals ( fig .",
    "[ fig13 ] ) .",
    "hence , our interpretation relies on the presence of error signals that are diffusely available across a large number of synapse .",
    "biologically , this role might be fulfilled by a neuromodulator such as dopamine , which is known to act as an error - correcting signal by modulating synaptic plasticity changes in the cortico - striatal junction during behavioural learning tasks @xcite .    like most existing learning rules for spiking networks",
    "we have assumed the presence of a supervisory signal , which is used to perform on - line comparisons between actual and target output spike patterns during learning . although this might be deemed cognitively implausible , it is possible that such a signal originates from an external network that acts as an ` activity template ' @xcite , which can explain functional plasticity changes in neurons encoding for auditory stimuli in the barn owl @xcite . however",
    ", more recently , reward - modulated learning rules for spiking networks have emerged as a more plausible alternative to supervised learning @xcite , which instead provide summary feedback on the correctness of network responses .",
    "it has been shown in @xcite and in our previous work @xcite how reward - modulation can be applied to learning precise output spiking patterns .",
    "[ [ conclusions - and - future - work . ] ] conclusions and future work .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    in principle , our multilayer learning rule follows from those previously introduced in @xcite which have adapted backpropagaton for use in feedforward spiking networks . through several benchmark tests in this paper",
    "we have indicated the high performance of our learning rule , thereby lending support to its practical deployment as an efficient neural classifier .",
    "we have further highlighted the advantages of using a fully temporal code based on multiple output spike - timings to reliably encode for input patterns .",
    "finally , to address the biological shortcomings of backpropagated learning , we presented a heuristic reformulation of our learning rule which we argue can be considered biologically plausible .",
    "the framework in which we have developed our learning rule is general , and has found applications in the areas of both supervised and reinforcement learning for feedforward and recurrent network structures .",
    "it is therefore natural to assume that our supervised rule might have a reinforcement analogue , that instead uses a delayed feedback signal to indicate the overall ` correctness ' of network responses during learning . in grning @xcite ,",
    "it has been shown how backpropagation can be reimplemented as a cognitively more plausible reinforcement learning scheme , but for rate - coded neurons ; future work could attempt to relate such a technique to our own rule for spiking neurons , with the intent of supporting a biological backpropagation rule .",
    "we start by considering a single postsynaptic neuron @xmath0 in the network that receives input from @xmath90 presynaptic neurons .",
    "the list of presynaptic spikes due to neuron @xmath1 , up to time @xmath4 , is @xmath91 , where @xmath92 is always the last spike before @xmath4 .",
    "if the postsynaptic neuron @xmath0 generates the list of output spikes @xmath93 in response to the presynaptic pattern @xmath3 , then its membrane potential at time @xmath4 is defined by the @xcite : @xmath94 where @xmath6 is the synaptic weight between neurons @xmath0 and @xmath1 , and @xmath95 and @xmath96 are the presynaptic and postsynaptic spike trains respectively , with a spike train defined in terms of a sum of dirac @xmath11 functions : @xmath97 . @xmath98 and @xmath99 are the and reset kernels respectively , taken as : @xmath100\\ , \\theta(s)\\;\\;\\ ; \\mathrm{and}\\;\\;\\ ; \\kappa(s ) = \\kappa_0 \\mathrm{e}^{-s/ \\tau_m}\\ , \\theta(s ) \\;,\\ ] ] where @xmath101 and @xmath102 are scaling constants , @xmath103 the membrane time constant , @xmath104 the synaptic rise time and @xmath105 the heaviside step function .",
    "neuronal spike events are generated by a point process with stochastic intensity @xmath106 , that is the instantaneous firing rate of a postsynaptic neuron , where the probability of generating a spike at time @xmath4 over a small time interval @xmath107 is given by @xmath108 .",
    "the firing rate has a nonlinear dependence on the postsynaptic neuron s membrane potential , that in turn depends on both its presynaptic input and the postsynaptic neuron s firing history : @xmath109 $ ] . here",
    ", we take an exponential dependence of the firing rate on the distance between the membrane potential and firing threshold @xmath110 @xcite : @xmath111 = \\rho_0 \\exp \\left ( \\frac { u-\\vartheta } { \\delta u } \\right ) \\;,\\ ] ] with the instantaneous firing rate at threshold @xmath112 and @xmath113 . the smoothness of the threshold was set to @xmath114 for output layer neurons and @xmath115 for hidden layer neurons . in the limit @xmath116",
    "the deterministic model can be recovered @xcite .",
    "our choice of @xmath117 was motivated by the need for increased variation in hidden neuron spiking for learning to succeed , as indicated by preliminary results .",
    "taking an exponential dependence of the firing rate on the membrane potential represents one choice for distributing output spikes ; alternative functional dependencies exist , such as the arrhenius & current model @xcite , which we have previously applied to learning temporally precise spiking patterns in @xcite .",
    "we initially derive weight update rules for the connections between the hidden and output layers , as originally shown by pfister et al .",
    "we then extend our analysis to include weight updates between the input and hidden layers using backpropagation , that is our novel contribution of a multilayer learning rule in a network of spiking neurons . in our notation ,",
    "input layer neurons are indexed as @xmath118 , hidden neurons @xmath1 and output neurons @xmath0 .",
    "[ [ objective - function . ] ] objective function .",
    "+ + + + + + + + + + + + + + + + + + +    implementing stochastic spiking neurons allows us to determine the likelihood of generating a prescribed target spike train .",
    "specifically , the probability density of an output neuron @xmath0 generating a list of target output spikes @xmath119 in response to a hidden spike pattern @xmath20 is given by @xcite : @xmath120 where @xmath121 and @xmath26 is the duration over which @xmath20 is presented .",
    "it is noted that output neuron activity implicitly depends on variable hidden layer activity through the functional dependence @xmath122 $ ] ( c.f .",
    "[ eq : potential ] and [ eq : exp_rate ] ) . for more than one output neuron ,",
    "the probability density of generating a spatio - temporal target output pattern @xmath22 is given by @xmath123 taking the logarithm of eq .",
    "[ eq : pdf_pattern ] provides us with an objective function , that is a smooth function of the network parameters : @xmath124 by gradient ascent we seek to optimize the above function , through adjusting plastic parameters in the network . here",
    ", we focus on changing the values of synaptic weights , although other network parameters such as conduction delays might also be trained .    [ [ output - weight - updates . ] ] output weight updates .",
    "+ + + + + + + + + + + + + + + + + + + + + +    taking the positive gradient of the log - likelihood ( eq . [ eq : log_pdf_pattern ] ) provides us with the direction of weight updates for neurons in the output layer , such that the expectation of generating a target output pattern @xmath125 is increased , i.e. : @xmath126 where @xmath29 is the output layer learning rate .",
    "the derivative of the log - likelihood can be found as @xmath127 ( \\mathcal{y}_h \\ast \\epsilon ) ( t ) \\mathrm{d}t \\;,\\ ] ] where @xmath128 , and @xmath7 is the convolution of the hidden spike train @xmath129 with the kernel @xmath130 that is defined in eq .",
    "[ eq : convolution ] . given our choice of an exponential dependence for the firing rate on the membrane potential , defined in eq .",
    "[ eq : exp_rate ] , it follows that @xmath131 hence combining eqs .",
    "[ eq : w_oh_update1 ] , [ eq : do_log_pdf_pattern ] and [ eq : d_frac_exp_rate ] provides the output layer weight update rule : @xmath132\\ , ( \\mathcal{y}_h \\ast \\epsilon ) ( t ) \\mathrm{d}t \\;.\\ ] ] we define the backpropagated error signal @xmath30 for the @xmath133 output neuron as @xmath134 \\;,\\ ] ] that is substituted into eq .",
    "[ eq : w_oh_update2 ] for compactness : @xmath135 the above supervised learning rule was originally derived by pfister et al .",
    "@xcite for a single - layer network , that optimizes the log - likelihood of generating a desired postsynaptic spike train in response to a given input pattern .",
    "[ [ hidden - weight - updates . ] ] hidden weight updates .",
    "+ + + + + + + + + + + + + + + + + + + + + +    continuing through to the hidden layer , weights between input and hidden layer neurons are updated according to @xmath136 where @xmath36 is the hidden layer learning rate . using eq .",
    "[ eq : log_pdf_pattern ] , and by making use of the chain rule , the gradient of the log - likelihood with respect to hidden layer weights can be expressed as @xmath137 \\mathrm{d}t \\nonumber \\\\ & = \\sum_o \\int_0^t \\frac { \\rho_o'(t|\\mathbf{y } , z_o ) } { \\rho_o(t|\\mathbf{y } , z_o ) } \\left [ \\mathcal{z}_o^{\\mathrm{ref}}(t ) - \\rho_o(t|\\mathbf{y } , z_o ) \\right ] \\frac{\\partial u_o(t|\\mathbf{y } , z_o)}{\\partial w_{hi } } \\mathrm{d}t \\ ; .\\end{aligned}\\ ] ] using eqs .",
    "[ eq : d_frac_exp_rate ] and [ eq : backprop_error ] , the above can be compacted : @xmath138 the membrane potential of an output layer neuron has a dependence on the firing activity of neurons in the hidden layer according to eq .",
    "[ eq : potential ] , hence the second term on the right - hand side of eq .",
    "[ eq : dh_log_pdf_pattern2 ] can be rewritten as @xmath139 weights changes take place on a time scale of @xmath140 , therefore the gradient of the convolution @xmath7 can be well approximated by @xmath141 the spike train @xmath129 is a discontinuous random variable with no smooth dependence on network parameters , leaving the gradient @xmath142 difficult to solve analytically .",
    "therefore , applying the technique used in @xcite , we heuristically make the substitution @xmath143 , that is the expectation of the hidden spike train @xmath129 conditioned on the input pattern @xmath144 .",
    "the expectation of @xmath129 has a smooth dependence on network parameters , and its gradient is given by : @xmath145 where we have used the relation @xmath146 , the integral runs over all possible lists of spikes @xmath147 up to time @xmath148 and @xmath149 is a spike train .",
    "@xmath150 is the probability density of the list of hidden spikes @xmath151 being equal to @xmath152 , conditioned on @xmath144 .",
    "the probability density or likelihood of a hidden neuron generating a list of spikes @xmath152 up to @xmath148 in response to @xmath144 is defined similarly to equation [ eq : pdf_single ] : @xmath153 and the gradient of the log - likelihood : @xmath154 ( \\mathcal{x}_i \\ast \\epsilon ) ( t)\\ ,   \\mathrm{d}s \\;.\\ ] ] hence , eq .",
    "[ eq : dh_log_pdf_pattern5 ] becomes : @xmath155 ( \\mathcal{x}_i \\ast \\epsilon ) ( t)\\ ,   \\mathrm{d}s",
    "\\right ) \\mathrm{d}q \\;,\\ ] ] such that a spike generated by the neuron at time @xmath148 depends not only on recent input spikes , but also on its own entire spiking history @xmath152 through the integration between times @xmath156 and @xmath148 .",
    "the above can be simplified if we choose to neglect the neuron s firing history by taking the last hidden spike time @xmath157 as given , that allows the substitution @xmath158 for the expectation of the hidden spike train @xmath129 conditioned on both the input @xmath144 and @xmath159 @xcite . in this case",
    ", neglecting the neuron s firing history is not an unreasonable choice , given that the gradient of @xmath129 is convolved by the exponential kernel @xmath9 in eq .",
    "[ eq : dh_log_pdf_pattern4 ] that already captures the recent firing history of the neuron .",
    "hence , the gradient of the hidden spike train in eq .",
    "[ eq : dh_log_pdf_pattern4 ] can instead be expressed in terms of the value of a spike train @xmath160 at each point in time : @xmath161 where we have used the identity @xmath162 and @xmath163 is the dirac distribution as a function of a last spike @xmath164 . using eqs .",
    "[ eq : potential ] and [ eq : exp_rate ] we find : @xmath165 on each learning episode , our best estimate for the expected gradient comes from considering the current observation of @xmath151 given @xmath144 ; hence , the expectation can be dropped and the above can be combined with eqs .",
    "[ eq : dh_log_pdf_pattern3 ] and [ eq : dh_log_pdf_pattern4 ] to give @xmath166",
    "\\ast \\epsilon)(t ) \\;,\\ ] ] where we have defined a double convolution as : @xmath167 \\ast \\epsilon)(t ) \\equiv \\int_0^t \\mathcal{y}_h(t ' ) \\left [ \\int_0^{t ' } \\mathcal{x}_i(t '' ) \\epsilon(t'-t '' ) \\mathrm{d}t '' \\right ] \\epsilon(t - t ' ) \\mathrm{d}t ' \\;.\\ ] ] finally , combining eq .",
    "[ eq : dh_log_pdf_pattern10 ] with eqs .",
    "[ eq : dh_log_pdf_pattern ] and [ eq : dh_log_pdf_pattern2 ] provides the hidden layer weight update rule : @xmath168 \\ast \\epsilon)(t ) \\mathrm{d}t \\;.\\ ] ]      for hidden layer weight updates to take place , a degree of background hidden neuron spiking is necessary during learning",
    ". this condition can be satisfied if we apply synaptic scaling to hidden layer weights , that has previously been shown to maintain a homeostatic firing rate and introduce competition between afferent connections @xcite .",
    "therefore , in addition to eq . [ eq : w_hi_update ] ,",
    "hidden weights are modified at the end of each learning episode by the scaling rule : @xmath169 where @xmath170 is the scaling strength , @xmath171 the actual firing rate of the @xmath172 hidden neuron and @xmath173 and @xmath174 the maximum and minimum reference firing rates respectively .",
    "the above drives the firing rate of each hidden neuron to remain within the range @xmath175 , that makes the network less sensitive to its initial state and prevents extremes in the firing activity of hidden neurons @xcite .",
    "input patterns were presented to the network by @xmath40 input layer neurons , where each input neuron contained an independent poisson spike train with a mean firing rate of .",
    "a relative refractory period with a time constant of was simulated when generating each spike train for biological realism .",
    "a random realization of each input pattern was used , for a total of @xmath63 different patterns .",
    "learning took place on an episodic basis , where each episode corresponded to the presentation of an input pattern to the network lasting duration @xmath44 .",
    "the order in which input patterns were presented was random . unless otherwise stated , simulations were run over @xmath176 episodes to ensure a sufficient amount of time for the network to learn the desired number of inputs .",
    "hence , on average , each input pattern was presented 1000 times .",
    "every input pattern was associated with a target output pattern , and multiple inputs belonging to the same class shared the same target output . a target output pattern consisted of a predetermined spike train at each output neuron , and target",
    "spike trains contained the same number of spikes @xmath177 at each output that depended on the learning task .",
    "target spike trains were initialized by randomly selecting each target spike time @xmath178 from a uniform distribution over the interval @xmath179 , with an interspike separation of at least to avoid conflicted output responses during learning . a minimum target spike - timing of 40",
    "was taken given the evidence that values @xmath180 led to reduced performance @xcite .    at each output neuron",
    ", target spike trains differed from each other by a minimum distance of @xmath181 to ensure each input class was assigned a unique target response , and to reduce crosstalk during learning . the minimum distance scaled with the number of target output spikes , which increased the separation between classes . for our definition of @xmath182 and choice of @xmath26 ,",
    "a maximum of @xmath183 classes identified by a single target output spike was supported , and more correspondingly for multiple target output spikes .",
    "networks were trained to classify input patterns by the timing of output spikes , such that multiple inputs belonging to the same class shared the same target output .",
    "target outputs were randomly set at the start of each simulation , and networks were trained to assign @xmath63 input patterns between @xmath184 classes . for each class , a target output contained between one and ten spikes , depending on the learning task .",
    "instead of relying on precisely matched actual and target output spike trains to classify inputs , we instead allowed for sufficiently accurate output spike trains , that were closer to their desired targets in comparison with any other potential target . in discriminating between different classes of input patterns we used the @xcite , that is a metric for the temporal distance between two spike trains .    from considering a list of spikes @xmath185 ,",
    "the is computed by first performing a convolution over a spike train @xmath186 with an exponential function : @xmath187 where we set the coincidence time constant @xmath188 .",
    "hence , using the above , we can obtain @xmath189 and @xmath190 from @xmath191 and @xmath192 , that are the actual and target output spike trains of an output neuron @xmath0 respectively .",
    "the can then be determined from the definition @xcite : @xmath193 ^ 2 \\mathrm{d}t \\;.\\ ] ] using eq .",
    "[ eq : vrd ] , the between an output generated by the network and every potential target output is computed , giving the list of distances @xmath194 for a total of @xmath184 different classes . a correct classification of the input is then made if its target class label @xmath195 matches the index of the minimum distance , that is if @xmath196 for @xmath197 .    for a network containing more than one output neuron , responses consist of spatio - temporal output patterns @xmath198 with corresponding target outputs @xmath199 . to compute the distance between two spatio - temporal spike patterns ,",
    "the is summed over every output neuron : @xmath200 ^ 2 \\mathrm{d}t \\;,\\ ] ] and is determined with respect to each class . similarly to a network containing a single output neuron , a correct classification of an input is made if its target class label matches the index of the minimum spatio - temporal distance .",
    "the classification performance of the network was taken as an exponential moving average @xmath52 up to the @xmath201 episode , given by @xmath202 . on each episode , the performance either took a value of @xmath203 for a correct input classification or otherwise @xmath204 ( c.f .",
    "pattern recognition ) .",
    "the timing parameter was taken as @xmath205 , which corresponded to an averaging window of @xmath206 for a total of @xmath63 input patterns .",
    "the was also taken as a moving average @xmath45 , with the same averaging window as used for @xmath52 .",
    "a moving average of each measure was necessary , given our choice of a stochastic neuron model that gave rise to fluctuating network responses between episodes .",
    "in our simulations we measured the number of episodes taken for the network to converge in learning , that was defined in terms of its classification performance @xmath52 . specifically , given a total of @xmath207 learning episodes , we considered that learning had converged on the @xmath201 episode for the first value @xmath208 , by which point the network performance fell within of its final value . in the case of the network failing to learn any input patterns , with @xmath209 ,",
    "the number of episodes to convergence was taken as 0 .      in all simulations",
    ", we used a fixed number @xmath40 of input neurons and a variable number @xmath41 of hidden neurons .",
    "depending on the learning task , either a single output neuron or multiple output neurons determined the response of the network to presented input patterns .",
    "the simulation time step was set to @xmath210 .",
    "[ [ multilayer - networks . ] ] multilayer networks .",
    "+ + + + + + + + + + + + + + + + + + + +    in all simulations of a multilayer network , hidden layer synaptic weights were initialized by independently selecting each value from a uniform distribution over the range : @xmath211 , that gave rise to an initial hidden neuron firing rate of @xmath212 .",
    "output weights were initialized depending on the number of output neurons . during learning",
    ", hidden weights were constrained to the range : @xmath213 , and were free to take either positive or negative values . to increase the number of eligible synapses available to the network , and to increase the diversity of hidden neuron spiking , axonal conduction delays were introduced between the input and hidden layers @xcite .",
    "conduction delays were selected from a uniform distribution over the range @xmath214 $ ] and rounded to the nearest , where @xmath215 was the conduction delay between the @xmath216 and @xmath172 input and hidden neurons respectively .",
    "hence , a conduction delay @xmath215 resulted in a evoked at @xmath1 due to an input spike @xmath217 with an effective time course of @xmath218 ( c.f .",
    "[ eq : kernels ] ) .",
    "conduction delays were neglected between the hidden and output layers . hidden and output layer learning rates were set to @xmath219 and @xmath220",
    "respectively , where it was indicated through preliminary simulations that the dependence of @xmath36 on the number of output neurons @xmath221 and number of target output spikes @xmath64 dominated over the number of input patterns @xmath63 . both @xmath36 and @xmath29 depended on the number of afferent synapses : @xmath222 and @xmath41 respectively .",
    "[ [ single - outputs . ] ] single outputs .",
    "+ + + + + + + + + + + + + + +    in simulations of a multilayer network with a single output neuron , initial values of output synaptic weights were all set to the same value @xmath223 that drove the output firing rate to @xmath224 .",
    "each initial value of @xmath6 was identical to allow equal contributions from every hidden layer neuron at the start of learning . during learning ,",
    "output weights were constrained to the range @xmath225 ; the lower bound of 0.01 was enforced to enable hidden weight updates to keep taking place , given that updates depended on output weight values according to @xmath226 ( eq . [ eq : w_hi_update ] ) .",
    "values of @xmath6 were positive and prevented from changing sign during learning to ensure sufficient excitatory drive in the output neuron from the hidden layer .",
    "preliminary simulations indicated that constraining output weights to positive values for a single output neuron had no adverse impact on learning .",
    "[ [ multiple - outputs . ] ] multiple outputs .",
    "+ + + + + + + + + + + + + + + + +    in simulations of a multilayer network with multiple output neurons , output synaptic weights were initialized by independently selecting each value from a uniform distribution over the range @xmath227 , that drove the firing rate of each output neuron to @xmath224 .",
    "randomizing output weights was necessary to increase the diversity between output responses , which improved learning in the initial stages of each simulation run .",
    "output weights were constrained to the range @xmath228 , and were allowed to change sign during learning .",
    "[ [ single - layer - networks . ] ] single - layer networks .",
    "+ + + + + + + + + + + + + + + + + + + + + +    in simulations of a single - layer network , synaptic weights were initialized by independently selecting each value from a uniform distribution over the range @xmath229 , that gave rise to an initial output firing rate of @xmath224 .",
    "the learning rate was set to @xmath230 and weights were constrained to the range @xmath228 , where the values of weights were allowed to change sign during learning . for a closer comparison , the model and parameter set used to generate output spikes in the single - layer network matched those used to generate output spikes in the multilayer network .",
    "bg was funded by epsrc grant ep / j500562/1 .",
    "ag and is were funded by the human brain project ( hbp ) .",
    "gardner b , grning a ( 2013 ) learning temporally precise spiking patterns through reward modulated spike - timing - dependent plasticity . in : artificial neural networks and machine learning ",
    "icann 2013 , springer .",
    ". 256263 ."
  ],
  "abstract_text": [
    "<S> information encoding in the nervous system is supported through the precise spike - timings of neurons ; however , an understanding of the underlying processes by which such representations are formed in the first place remains unclear . </S>",
    "<S> here we examine how networks of spiking neurons can learn to encode for input patterns using a fully temporal coding scheme . to this end </S>",
    "<S> , we introduce a learning rule for spiking networks containing hidden neurons which optimizes the likelihood of generating desired output spiking patterns . </S>",
    "<S> we show the proposed learning rule allows for a large number of accurate input - output spike pattern mappings to be learnt , which outperforms other existing learning rules for spiking neural networks : both in the number of mappings that can be learnt as well as the complexity of spike train encodings that can be utilised . </S>",
    "<S> the learning rule is successful even in the presence of input noise , is demonstrated to solve the linearly non - separable xor computation and generalizes well on an example dataset . </S>",
    "<S> we further present a biologically plausible implementation of backpropagated learning in multilayer spiking networks , and discuss the neural mechanisms that might underlie its function . </S>",
    "<S> our approach contributes both to a systematic understanding of how pattern encodings might take place in the nervous system , and a learning rule that displays strong technical capability . </S>"
  ]
}