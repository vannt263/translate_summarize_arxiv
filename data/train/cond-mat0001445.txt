{
  "article_text": [
    "the authors thank g. nardulli , for valuable discussions , and p. blonda , g. pasquariello , g. satalino ( iesi - cnr ) for providing the landsat data set .                                    while applying _",
    "superparamagnetic clustering _",
    ", we followed the prescription @xcite to connect each point to the neighbor with which it has the highest correlation , so as to capture points lying on the periphery of clusters .",
    "performances by our method , reported in the text , are obtained without adopting a similar processing stage .",
    "5(color ) : first two principal components of the landsat data - set ( see the text ) . comparing the variances along the six principal axis , it turns out that also the third and fourth principal components are relevant to this data - set .",
    "fig . 6 : hierarchical structure of the landsat data - set as it has been found by our algorithm ; the @xmath40 values at which the clusters split can be read on the axis at the bottom .",
    "this results have been obtained using @xmath55 , however values in the range @xmath94 give similar results ."
  ],
  "abstract_text": [
    "<S> a new approach to clustering , based on the physical properties of inhomogeneous coupled chaotic maps , is presented . </S>",
    "<S> a chaotic map is assigned to each data - point and short range couplings are introduced . </S>",
    "<S> the stationary regime of the system corresponds to a macroscopic attractor independent of the initial conditions . </S>",
    "<S> the mutual information between pairs of maps serves to partition the data set in clusters , without prior assumptions about the structure of the underlying distribution of the data . </S>",
    "<S> experiments on simulated and real data sets show the effectiveness of the proposed algorithm .    </S>",
    "<S> the clustering problem consists of partitioning @xmath0 given points into k groups ( clusters ) so that two points belonging to the same group are , in some sense , more similar than two that belong to different ones @xcite ; it has applications in several fields such as pattern recognition @xcite , learning @xcite and astrophysics @xcite . </S>",
    "<S> data points are specified either in terms of their coordinates in a d - dimensional space or , alternatively , by means of an @xmath1 `` distance matrix '' whose elements measure the dissimilarity of pairs of data points . </S>",
    "<S> this problem is inherently ill - posed , i.e. any data set can be clustered in drastically different ways , with no clear criterion to prefer one clustering over another . </S>",
    "<S> the most important sources of ambiguity are the choice of the number of clusters and the fact that a satisfactory clustering of data depends on the desired _ </S>",
    "<S> resolution_.    when prior knowledge of the clusters structure is available ( e.g. , each cluster can be represented by a multivariate gaussian distribution ) , parametric approaches can be used so that prior information is incorporated in a global criterion , thus converting clustering onto an optimization problem . </S>",
    "<S> examples of parametric clustering algorithms are variance minimization @xcite and maximum likelihood @xcite . in many cases of interest </S>",
    "<S> , however , there is no a priori knowledge about the data structure . </S>",
    "<S> then it is more natural to adopt nonparametric approaches , which make fewer assumptions about the model and therefore are suitable to handle a wider variety of clustering problems . </S>",
    "<S> usually these methods employ a local criterion to build clusters by utilizing local structure of the data ( e.g. , by identifying high - density regions in the data space ) @xcite .    </S>",
    "<S> a very interesting nonparametric approach for clustering , based on the physical properties of an inhomogeneous potts model , has been recently proposed @xcite and has proven to perform better than other nonparametric methods . </S>",
    "<S> the central feature of this method , called _ superparamagnetic clustering _ ( spc ) , is to change the similarity index of the problem from the interpoint distance to the spin - spin correlation function of the statistical model ; the temperature of the potts model controls the resolution at which data are clustered .    in the present work </S>",
    "<S> we propose a new nonparametric method based on the physical properties of inhomogeneous coupled chaotic maps . </S>",
    "<S> we assign a map to each data point and introduce couplings , between pairs of maps , whose strength is a decreasing function of their distance . the mutual information between pairs of maps , in the stationary regime , is then used as the similarity index for clustering the data set .    systems of diffusively coupled chaotic maps , living on regular lattices , have been extensively studied ; for large coupling strength they exhibit non - trivial collective behavior , i.e. long - range order emerging out of local chaos @xcite . </S>",
    "<S> globally coupled chaotic maps , a mean field extension of coupled map lattices , have also been considered and their rich variety of behaviors has been outlined @xcite ; it has been shown that mutual synchronization of chaotic oscillations is a robust property displayed by globally coupled maps and clusters of synchronized maps appear in the stationary regime . in a recent paper @xcite randomly coupled maps were studied and the formation of dynamical clusters of _ almost _ synchronized maps was observed . </S>",
    "<S> here we associate a system of chaotic maps to a given data set so that the architecture of the network bias the formation of clusters of _ almost _ synchronized maps to correspond to high density regions in the data set . </S>",
    "<S> let us introduce coupled chaotic maps on finite size inhomogeneous lattices . </S>",
    "<S> given a set of @xmath0 points @xmath2 in a d - dimensional space , we assign a real variable @xmath3 $ ] to each point and define pair - interactions @xmath4 ^ 2 /2a^2)}$ ] , where @xmath5 is the local length scale . </S>",
    "<S> the time evolution of the system is given by : @xmath6 where @xmath7 , and we choose the logistic map @xmath8 . </S>",
    "<S> we note that the equivalent dynamics to ( [ eq:1 ] ) in terms of variables @xmath9 is @xmath10 this form may be more familiar for researchers in neural networks , @xmath11 playing the role of a nonmonotonic transfer function @xcite </S>",
    "<S> . a detailed analysis of the behavior of this class of models will be given elsewhere @xcite ; here we only describe some properties which will be useful for clustering purposes .    </S>",
    "<S> the stationary regime of eqs.([eq:1 ] ) corresponds to a macroscopic attractor which is independent of the initial conditions . to study the correlation properties of the system </S>",
    "<S> , we consider the mutual information @xmath12 , between variables @xmath13 and @xmath14 , whose definition is the following @xcite . if the state of element @xmath15 is @xmath16 then it will be assigned a value 1 , otherwise it will be assigned 0 : this generates a sequence of bits , in a certain time interval , which allows the calculation of the boltzmann entropy @xmath17 for the @xmath15th map . </S>",
    "<S> in a similar way the joint entropy @xmath18 is calculated for each pair of maps and finally the mutual information is given by @xmath19 . </S>",
    "<S> the mutual information is a good measure of correlations @xcite and it is practically precision independent , due to the rough coarse graining of the dynamics . if maps @xmath15 and @xmath20 evolve independently then @xmath21 ; if the two maps are exactly synchronized then the mutual information achieves its maximum value , in the present case @xmath22 , due to our choice of @xmath11 .    </S>",
    "<S> let us now describe our simulations of large systems ( up to @xmath23 ) randomly generated with uniform density @xmath24 in dimension d. the average mutual information between two maps at distance @xmath25 obeys the following scaling form : @xmath26 where @xmath27 is a scaling function which depends on @xmath28 but it is independent of @xmath0 and @xmath24 , provided that @xmath5 is much less than the linear size of the system . in fig.1 </S>",
    "<S> we show the scaling function for @xmath29 , @xmath30 and @xmath31 ; we see that full synchronization is never achieved even for very close pairs of maps , indeed for @xmath25 close to zero @xmath27 is less than @xmath22 . at large distances @xmath27 </S>",
    "<S> tends to a non - vanishing value , i.e. the system is characterized by long range correlations . </S>",
    "<S> moreover the asymptotic value @xmath32 increases with the dimension @xmath28 ; in the limit @xmath33 the system becomes a mean - field model and it can be expected that in this limit the system fully synchronizes . </S>",
    "<S> now we give the definition of @xmath34-nearest neighboring sites for our lattices : sites @xmath15 and @xmath20 are nearest neighbors if and only if @xmath20 is one of the @xmath34 nearest points of @xmath15 and @xmath15 is one of the @xmath34 nearest points of @xmath20 . </S>",
    "<S> the typical distance between two nearest neighbors obviously depends on the density @xmath24 . due to the scaling law ( [ eq:3 ] ) , it follows that , at fixed @xmath5 , the typical amount of mutual information between nearest neighboring maps depends monotonically on the density @xmath24 . </S>",
    "<S> let us now turn to consider a real data set , made of regions with different densities : we find that the mutual information between two neighboring maps , in this case , depends on the local density around the pair . </S>",
    "<S> in particular it is small in low - density regions . </S>",
    "<S> our algorithm employs the contextual character of the mutual information for clustering the data set .    </S>",
    "<S> now we describe our method . </S>",
    "<S> the value of @xmath5 is fixed as the average distance of @xmath34-nearest neighbors pairs of points in the whole system ( our results are quite insensitive to the particular value of @xmath34 ) . </S>",
    "<S> we remark that everything done so far can be easily implemented in the case when instead of providing the @xmath35 for all data we have an @xmath1 `` distance matrix '' . for the sake of computational convenience , we keep only interactions of a map with a limited number of maps , those whose distance is less than @xmath36 , and set all other @xmath37 to zero . </S>",
    "<S> starting from a random initial configuration of @xmath38 , eqs.([eq:1 ] ) are iterated until the system attains its stationary regime ; the mutual information is then evaluated for pairs of maps . </S>",
    "<S> the clusters are identified in the two following steps . </S>",
    "<S> ( 1 ) a link is set between all pairs of data points such that @xmath39 , where @xmath40 is a threshold . </S>",
    "<S> ( 2 ) data clusters are identified as the linked components of the graphs obtained in step 1 .    </S>",
    "<S> the value of @xmath40 controls the resolution at which the data set is clustered ; by repeating the two steps above described for an increasing sequence of @xmath40 values , hierarchical clustering of the data is obtained .    </S>",
    "<S> the following toy problem illustrates how the proposed algorithm works . </S>",
    "<S> fig . </S>",
    "<S> 2 contains two dense regions of @xmath41 and @xmath42 points on a dilute background of @xmath43 points . in fig.3 </S>",
    "<S> we show the frequency distribution of ( a ) distances between neighboring points and ( b ) mutual information between neighboring points . </S>",
    "<S> the peak around @xmath44 , in fig . </S>",
    "<S> 3b , corresponds to points in the background . in fig . </S>",
    "<S> 4 we show the size of the three biggest clusters , found by our algorithm , versus @xmath40 . for @xmath45 </S>",
    "<S> the algorithm identifies a single big cluster of about 2400 points , the remaining @xmath46 points are distributed among @xmath47 clusters of size smaller than @xmath48 . for @xmath49 two big clusters , corresponding to the two dense regions , </S>",
    "<S> are identified ; these clusters consist of @xmath50 and @xmath51 points respectively , while the remaining @xmath52 points are distributed in @xmath53 clusters of size smaller than @xmath48 . as @xmath40 increases above @xmath54 the biggest clusters break into smaller and smaller clusters . </S>",
    "<S> as can be observed from fig.4 , the stability of the largest clusters ( existence of a plateau ) is a clear indication of the optimal partition among the whole hierarchy yielded by our algorithm . </S>",
    "<S> the results above described correspond to @xmath55 , however values in the range @xmath56 give similar results .    </S>",
    "<S> now we turn to consider a real data - set extracted from landsat thematic mapper ( tm ) images . </S>",
    "<S> we analyze data taken from a satellite image of an area in the southern of italy consisting of @xmath57 pixels each of which is represented by six spectral bands . </S>",
    "<S> the ground truth was determined by means of visual interpretation of areal photos followed by site visits . </S>",
    "<S> the area study includes seven landcover classes : ( a ) _ coniferous reafforestation _ , @xmath58 points ; ( b ) _ bare soil _ , @xmath59 points ; ( c ) _ urban areas _ , @xmath60 points ; ( d ) _ vineyards _ , @xmath61 points ; ( e ) _ cropland _ , @xmath62 points ; ( f ) _ pasture _ , @xmath63 points ; ( g ) _ olives groves _ , @xmath64 points . in fig . </S>",
    "<S> 5 the first two principal components of the data - set are shown : this problem is characterized by clusters of different size and density . in spite of these difficulties , </S>",
    "<S> our algorithm succeeds in resolving the data - structure , as it is shown in fig . </S>",
    "<S> 6 . for @xmath65 </S>",
    "<S> our algorithm identifies a single big cluster ; at @xmath66 this cluster splits in two clusters , one corresponding to class a and the other corresponding to data - points of the other six classes . by successive transitions , all the seven classes separate . </S>",
    "<S> both the six clusters partition and the seven clusters one appear stable : prior knowledge is needed , in this case , to select the correct partition of the data set . in the range @xmath67 </S>",
    "<S> seven clusters , consisting of @xmath58 , @xmath68 , @xmath69 , @xmath70 , @xmath71 , @xmath72 , @xmath73 points respectively , are stable ; the remaining @xmath74 points are distributed among @xmath75 small clusters of size smaller than @xmath76 . </S>",
    "<S> hence @xmath77 of data is classified ; the purity of the classification ( percentage of correctly classified points ) is @xmath78 . </S>",
    "<S> as @xmath40 is further increased , these clusters break into smaller and smaller parts ( the cluster which breaks first is the one corresponding to class d ) . </S>",
    "<S> it is worth mentioning that an unsupervised exploration of the underlying structure in a data set ( like the one provided by the proposed method ) make easier the design of a supervised classifier for the same problem ( see @xcite ) .    </S>",
    "<S> it is clear that the proposed algorithm has similarities with spc method , indeed both methods associate a physical system to data - set points and employ a physical correlation ( spin - spin correlation @xcite or mutual information ) as the similarity index . </S>",
    "<S> we apply spc to the landsat data - set and obtain the same hierarchical structure of data as the one from our method ; the best performance corresponds to seven clusters of @xmath79 , @xmath80 , @xmath81 , @xmath82 , @xmath83 , @xmath84 , @xmath85 points respectively : @xmath86 of data are classified with @xmath77 purity @xcite . </S>",
    "<S> hence , as far as the data - set at hand is concerned , our algorithm classifies more points than spc with almost the same purity . </S>",
    "<S> our algorithm has the following computational advantage over spc : the hierarchical structure of data is obtained by a simple thresholding at each value of @xmath40 , while spc requires a monte carlo at each value of the temperature . </S>",
    "<S> this reduces the computational time by orders of magnitude . on the other hand </S>",
    "<S> , spc provides a supplementary indicator , the susceptibility @xcite , which may be helpful to detect the optimal partition of the data - set .    </S>",
    "<S> some remarks are in order . </S>",
    "<S> we have also clusterized data using the average distance between maps @xmath87 , in the stationary regime , as the dissimilarity index : the results were less stable than those obtained by use of the mutual information . </S>",
    "<S> our choice of the logistic map @xmath88 , with @xmath89 , is due to the circumstance that the corresponding invariant measure is symmetric around @xmath90 , so that the mutual information can , in principle , achieve its maximum value @xmath91 ; other maps with symmetric invariant measure work as well , while choosing maps with non symmetric measure would reduce the allowed range of values for the mutual information . </S>",
    "<S> finally we wish to reemphasize the aspects we consider as the main advantages of our algorithm : its simplicity , the physical system to be simulated being described by simple deterministic equations ( [ eq:1 ] ) , and its general applicability , no _ a priori _ knowledge of clusters structure is to be assumed . </S>",
    "<S> applications of our algorithm to other real problems will be presented in a forthcoming paper @xcite . </S>"
  ]
}