{
  "article_text": [
    "nowadays , many engineering applications can be posed as convex quadratic problems ( qp ) .",
    "several important applications that can be modeled in this framework such us model predictive control for a dynamical linear system @xcite and its dual called moving horizon estimation @xcite , dc optimal power flow problem for a power system @xcite , linear inverse problems arising in many branches of science @xcite or network utility maximization problems @xcite have attracted great attention lately . since the computational power has increased by many orders in the last decade ,",
    "highly efficient and reliable numerical optimization algorithms have been developed for solving the optimization problems arising from these applications in very short time .",
    "for example , these hardware and numerical recent advances made it possible to solve linear predictive control problems of nontrivial sizes within the range of microseconds and even on hardware platforms with limited computational power and memory @xcite .",
    "the theoretical foundation of quadratic programming dates back to the work by frank & wolfe @xcite . after the publication of the paper @xcite many numerical algorithms",
    "have been developed in the literature that exploit efficiently the structure arising in this class of problems .",
    "basically , we can identify three popular classes of algorithms to solve quadratic programs : active set methods , interior point methods and ( dual ) first order methods .",
    "_ active set methods _ are based on the observation that quadratic problems with equality constraints are equivalent to solving a linear system .",
    "thus , the iterations in these methods are based on solving a linear system and updating the active set ( the term active set refers to the subset of constraints that are satisfied as equalities by the current estimate of the solution ) .",
    "active set general purpose solvers are adequate for small - to - medium scale quadratic problems , since the numerical complexity per iteration is cubic in the dimension of the problem .",
    "matlab s _ quadprog _ function implements a primal active set method .",
    "dual active set methods are available in the codes @xcite .",
    "_ interior point methods _ remove the inequality constraints from the problem formulation using a barrier term in the objective function for penalizing the constraint violations .",
    "usually a logarithmic barrier terms is used and the resulting equality constrained nonlinear convex problem is solved by the newton method . since the iteration complexity grows also cubically with the dimension , interior - point solvers are also the standard for small - to - medium scale qps .",
    "however , structure exploiting interior point solvers have been also developed for particular large - scale applications : e.g. several solvers exploit the sparse structure of the quadratic problem arising in predictive control ( cvxgen @xcite , forces @xcite ) .",
    "a parallel interior point code that exploits special structures in the hessian of large - scale structured quadratic programs have been developed in  @xcite .    _",
    "first order methods _ use only gradient information at each iterate by computing a step towards the solution of the unconstrained problem and then projecting this step onto the feasible set .",
    "augmented lagrangian algorithms for solving general nonconvex problems are presented in the software package lancelot @xcite .",
    "for convex qps with simple constraints we can use primal first order methods for solving the quadratic program as in @xcite . in this case",
    "the main computational effort per iteration consists of a matrix - vector product . when the projection on the primal feasible set is hard to compute , an alternative to primal first order methods is to use the lagrangian relaxation to handle the complicated constraints and then to apply dual first order algorithms for solving the dual .",
    "the computational complexity certification of first order methods for solving the ( augmented ) lagrangian dual of general convex problems is studied e.g. in @xcite and of quadratic problems is studied in @xcite . in these methods the main computational effort consists of solving at each iteration a lagrangian qp problem with simple constraints for a given multiplier , which allows us to determine the value of the dual gradient for that multiplier , and then update the dual variables using matrix - vector products .",
    "for example , the toolbox fiordos @xcite auto - generates code for primal or dual fast gradient methods as proposed in @xcite .",
    "the algorithm in @xcite dualizes only the inequality constraints of the qp and assumes available a solver for linear systems that is able to solve the lagrangian inner problem .",
    "however , both implementations @xcite do not consider the important aspect that the lagrangian inner problem can not be solved exactly in practice .",
    "the effect of inexact computations in dual gradient values on the convergence of dual first order methods has been analyzed in detail in @xcite .",
    "moreover , most of these papers generate approximate primal solutions through averaging @xcite . on the other hand , in practice",
    "usually the last primal iterate is employed , since in practice these methods converge faster in the primal last iterate than in a primal average sequence .",
    "these issues motivate our work  here .    _",
    "contributions_. in this paper we analyze the computational complexity of several ( augmented ) dual first order methods implemented in duquad for solving convex quadratic problems . contrary to most of the results from the literature @xcite , our approach allows us to use inexact dual gradient information ( i.e. it allows to solve the ( augmented ) lagrangian inner problem approximately ) and therefore is able to tackle more general quadratic convex problems and to solve practical applications .",
    "another important feature of our approach is that we provide also complexity results for the primal latest iterate , while in much of the previous literature convergence rates in an average of primal iterates are given @xcite .",
    "we derive in a unified framework the computational complexity of the dual and augmented dual ( fast ) gradient methods in terms of primal suboptimality and feasibility violation using inexact dual gradients and two types of approximate primal solutions : the last primal iterate and an average of primal iterates . from our knowledge",
    "this paper is the first where both approaches , dual and augmented dual first order methods , are analyzed uniformly .",
    "these algorithms are also implemented in the efficient programming language c in duquad , and optimized for low iteration complexity and low memory footprint .",
    "the toolbox has a dynamic matlab interface which make the process of testing , comparing , and analyzing the algorithms simple .",
    "the algorithms are implemented using only basic arithmetic and logical operations and thus are suitable to run on low cost hardware .",
    "the main computational bottleneck in the methods implemented in duquad is the matrix - vector product .",
    "therefore , this toolbox can be used for solving either qps on hardware with limited resources or sparse qps with large dimension .",
    "_ contents_. the paper is organized as follows . in section [ sec_pf ]",
    "we describe the optimization problem that we solve in duquad . in section [ sec_duquad ]",
    "we describe the the main theoretical aspects that duquad is based on , while in section [ numerical_tests ] we present some numerical results obtained with duquad .    _",
    "notation_. for @xmath0 denote the scalar product by @xmath1 and the euclidean norm by @xmath2 .",
    "further , @xmath3_x$ ] denotes the projection of @xmath4 onto convex set @xmath5 and @xmath6_x\\|$ ] its distance . for a matrix",
    "@xmath7 we use the notation @xmath8 for the spectral norm .",
    "in duquad we consider a general convex quadratic problem ( qp ) in the form : @xmath9 where @xmath10 is a convex quadratic function with the hessian @xmath11 , @xmath12 , @xmath13 is a simple compact convex set , i.e. a box @xmath14 $ ] , and @xmath15 is either the cone @xmath16 or the cone @xmath17 . note that our formulation allows to incorporate in the qp either linear inequality constraints @xmath18 ( arising e.g. in sparse formulation of predictive control and network utility maximization ) or linear equality constraints @xmath19 ( arising e.g. in condensed formulation of predictive control and dc optimal power flow ) .",
    "in fact the user can define linear constraints of the form : @xmath20 and depending on the values for @xmath21 and @xmath22 we have linear inequalities or equalities . throughout the paper",
    "we assume that there exists a finite optimal lagrange multiplier @xmath23 for the qp and it is difficult to project on the feasible set of problem  : @xmath24 therefore , solving the primal problem approximately with primal first order methods is numerically difficult and thus we usually use ( augmented ) dual first order methods for finding an approximate solution for . by moving the complicating constraints @xmath25 into the cost via lagrange multipliers",
    "we define the ( augmented ) dual function : @xmath26 where @xmath27 denotes the ( augmented ) lagrangian w.r.t .",
    "the complicating constraints @xmath28 , i.e. : @xmath29 where the regularization parameter @xmath30 .",
    "we denote @xmath31 and observe that : @xmath32_{{\\mathcal{k } } } & \\text{if } \\;\\ ; \\rho>0 \\\\                   0 & \\text{if } \\;\\ ; \\rho=0 .",
    "\\end{cases}\\ ] ] using this observation in the formulation , we obtain : @xmath33 in order to tackle general convex quadratic programs , in duquad we consider the following two options :    * case 1 * : if @xmath34 , i.e. @xmath35 has the smallest eigenvalue @xmath36 , then we consider @xmath37 and recover the ordinary lagrangian function .    *",
    "case 2 * : if @xmath11 , i.e. @xmath35 has the smallest eigenvalue @xmath38 , then we consider @xmath39 and recover the augmented lagrangian function .    our formulation of the ( augmented ) lagrangian and the previous two cases allow us to thereat in a unified framework both approaches , dual and augmented dual first order methods , for general convex qps .",
    "we denote by @xmath40 the optimal solution of the _ inner problem _ with simple constraints @xmath41 : @xmath42 note that for both cases described above the ( augmented ) dual function is differentiable everywhere .",
    "moreover , the gradient of the ( augmented ) dual function @xmath43 is @xmath44-lipschitz continuous and given by @xcite : @xmath45 for all @xmath46 .",
    "since the dual function has lipschitz continuous gradient , we can derive bounds on @xmath47 in terms of a linear and a quadratic model ( the so - called _ descent lemma _ ) @xcite : @xmath48 - d_\\rho(\\lambda ) \\leq \\frac{l_{\\text{d}}}{2 } \\|\\mu - \\lambda\\|^2   \\quad \\forall \\lambda , \\mu \\in   { \\mathbb{r}^}p.\\end{aligned}\\ ] ] descent lemma is essential in proving convergence rate for first order methods @xcite . since we assume the existence of a finite optimal lagrange multiplier for , strong duality holds and thus the _ outer problem",
    "_ is smooth and satisfies : @xmath49 where @xmath50 note that , in general , the smooth ( augmented ) dual problem is not a qp , but has simple constraints .",
    "we denote a primal optimal solution by @xmath51 and a dual optimal solution by @xmath23 .",
    "we introduce @xmath52 as the set of optimal solutions of the smooth dual problem and define for some @xmath53 the following finite quantity : @xmath54 in the next section we present a general first order algorithm for convex optimization with simple constraints that is used frequently in our toolbox .      in this section",
    "we present a framework for first order methods generating an approximate solution for a smooth convex problem with simple constraints in the form : @xmath55 where @xmath56 is a convex function and @xmath5 is a simple convex set ( i.e. the projection on this set is easy ) .",
    "additionally , we assume that @xmath57 has lipschitz continuous gradient with constant @xmath58 and is strongly convex with constant @xmath59 .",
    "this general framework covers important particular algorithms @xcite : e.g. gradient algorithm , fast gradient algorithm for smooth problems , or fast gradient algorithm for problems with smooth and strongly convex objective function .",
    "thus , we will analyze the iteration complexity of the following general first order method that updates two sequences @xmath60 as follows :    where @xmath61 is the parameter of the method and we choose it in an appropriate way depending on the properties of function @xmath57 .",
    "more precisely , @xmath61 can be updated as follows :    * gm * : in the gradient method @xmath62 , where @xmath63 for all @xmath64 .",
    "this is equivalent with @xmath65 for all @xmath64 . in this case @xmath66 and thus we have the classical gradient update : @xmath67_x$ ] .    *",
    "fgm * : in the fast gradient method for smooth convex problems @xmath68 , where @xmath69 and @xmath70 . in this case we get a particular version of nesterov s accelerated scheme @xcite that updates two sequences @xmath71 and has been analyzed in detail in @xcite .",
    "* * fgm**@xmath72 : in fast gradient algorithm for smooth convex problems with strongly convex objective function , with constant @xmath73 , we choose @xmath74 for all @xmath64 . in this case",
    "we get a particular version of nesterov s accelerated scheme @xcite that also updates two sequences @xmath71 .",
    "the convergence rate of algorithm * fom*(@xmath75 ) in terms of function values is given in the next lemma :    @xcite [ lemma_sublin_dfg ] for smooth convex problem assume that the objective function @xmath57 is strongly convex with constant @xmath76 and has lipschitz continuous gradient with constant @xmath58 .",
    "then , the sequences @xmath77 generated by algorithm * fom*(@xmath75 ) satisfy : @xmath78 where @xmath79 , with @xmath80 the optimal set of , and @xmath81 is defined as follows : @xmath82    thus , algorithm * fom * has linear convergence provided that @xmath83 .",
    "otherwise , it has sublinear convergence .",
    "in this section we describe an inexact dual ( augmented ) first order framework implemented in duquad , a solver able to find an approximate solution for the quadratic program . for a given accuracy @xmath84",
    ", @xmath85 is called an @xmath86-_primal solution _ for problem if the following inequalities hold : @xmath87    the main function in duquad is the one implementing the general algorithm * fom*. note that if the feasible set @xmath88 of is simple , then we can call directly * fom*(@xmath89 ) in order to obtain an approximate solution for . however , in general the projection on @xmath90 is as difficult as solving the original problem . in this case",
    "we resort to the ( augmented ) dual formulation for finding an @xmath86-primal solution for the original qp .",
    "the main idea in duquad is based on the following observation : from  we observe that for computing the gradient value of the dual function in some multiplier @xmath91 , we need to solve exactly the inner problem ; despite the fact that , in some cases , the ( augmented ) lagrangian @xmath92 is quadratic and the feasible set @xmath93 is simple in , this inner problem generally can not be solved exactly .",
    "therefore , the main iteration in duquad consists of two steps :    * step 1 * : for a given inner accuracy @xmath94 and a multiplier @xmath95 solve approximately the inner problem with accuracy @xmath94 to obtain an approximate solution @xmath96 instead of the exact solution @xmath97 , i.e. : @xmath98 in duquad , we obtain an approximate solution @xmath96 using the algorithm * fom*(@xmath99 ) .",
    "from , lemma [ lemma_sublin_dfg ] we can estimate tightly the number of iterations that we need to perform in order to get an @xmath94-solution @xmath96 for : the lipschitz constant is @xmath100 , the strong convexity constant is @xmath101 ( provided that e.g. @xmath19 ) and @xmath102 ( the diameter of the box set @xmath93 ) .",
    "then , the number of iterations that we need to perform for computing @xmath96 satisfying can be obtained from .    *",
    "step 2 * : once an @xmath94-solution @xmath96 for was found , we update at the outer stage the lagrange multipliers using again algorithm * fom*(@xmath103 ) . note that for updating the lagrange multipliers we use instead of the true value of the dual gradient @xmath104 , an approximate value given by : @xmath105 .    in @xcite it has been proved separately , for dual and augmented dual first order methods , that using an appropriate value for @xmath94 ( depending on the desired accuracy @xmath86 that we want to solve the qp ) we can still preserve the convergence rates of algorithm * fom*(@xmath103 ) given in lemma [ lemma_sublin_dfg ] , although we use inexact dual gradients . in the sequel ,",
    "we derive in a unified framework the computational complexity of the dual and augmented dual ( fast ) gradient methods . from our knowledge , this is the first time when both approaches , dual and augmented dual first order methods , are analyzed uniformly .",
    "first , we show that by introducing inexact values for the dual function and for its gradient given by the following expressions : @xmath106 then we have a similar descent relation as in given in the following lemma :    given @xmath107 such that holds , then based on the definitions we get the following inequalities : @xmath108 - d_\\rho(\\lambda ) \\leq\\ !",
    "l_{\\text{d } } \\|\\mu -   \\lambda\\|^2 + \\ !",
    "2 { \\epsilon_{\\text{in}}}\\quad \\forall \\lambda,\\mu \\!\\in\\ ! { \\mathbb{r}^}p.\\end{aligned}\\ ] ]    from the definition of @xmath109 , and it can be derived : @xmath110 which proves the first inequality . in order to prove the second inequality ,",
    "let @xmath111 be a fixed primal point such that @xmath112 .",
    "then , we note that the nonnegative function @xmath113 has lipschitz gradient with constant @xmath114 and thus we have @xcite : @xmath115 taking now @xmath116 and using , then we obtain : @xmath117 furthermore , combining with and we have : @xmath118 using the relation @xmath119 we have : @xmath120 which shows the second inequality of our lemma .",
    "this lemma will play a major role in proving rate of convergence for the methods presented in this paper .",
    "note that in @xmath94 enters linearly , while in @xcite @xmath94 enters quadratically in the context of augmented lagrangian and thus in the sequel we will get better convergence estimates than those in the previous papers . in conclusion , for solving the dual problem in duquad we use the following inexact ( augmented ) dual first order algorithm :    recall that @xmath121 satisfying the inner criterion and @xmath122 .",
    "moreover , @xmath61 is chosen as follows :    * * dgm * : in ( augmented ) dual gradient method @xmath62 , where @xmath63 for all @xmath64 , or equivalently @xmath65 for all  @xmath64 , i.e. the ordinary gradient algorithm . * * dfgm * : in ( augmented ) dual fast gradient method @xmath68 , where @xmath123 and @xmath70 , i.e. a variant of nesterov s accelerated scheme .",
    "therefore , in duquad we can solve the smooth ( augmented ) dual problem either with dual gradient method * dgm * ( @xmath124 ) or with dual fast gradient method * dfgm * ( @xmath61 is updated based on @xmath125 ) .",
    "recall that for computing @xmath126 in duquad we use algorithm * fom*(@xmath127 ) ( see the discussion of step 1 ) . when applied to inner subproblem , algorithm * fom*(@xmath127 ) will converge linearly provided that @xmath128 .",
    "moreover , when applying algorithm * fom*(@xmath127 ) we use warm start : i.e. we start our iteration from previous computed @xmath129 . combining the inexact descent relation with lemma [ lemma_sublin_dfg ]",
    "we obtain the following convergence rate for the general algorithm * dfom*(@xmath130 ) in terms of dual function values of :    @xcite [ lemma_sublin_dfg_inexact ] for the smooth ( augmented ) dual problem the dual sequences @xmath131 generated by algorithm * dfom*(@xmath130 ) satisfy the following convergence estimate on dual suboptimality : @xmath132 where recall @xmath133 and @xmath81 is defined as in .",
    "note that in ( * ? ? ?",
    "* theorem 2 ) , the convergence rate of * dgm * scheme is provided in the average dual iterate @xmath134 and not in the last dual iterate @xmath135 .",
    "however , for a uniform treatment in theorem [ lemma_sublin_dfg_inexact ] we redefine the dual final point ( the dual last iterate @xmath135 when some stopping criterion is satisfied ) as follows : @xmath136_{{\\mathcal{k}}_d}$ ] .",
    "we now show how to choose the inner accuracy @xmath94 in duquad . from theorem [ lemma_sublin_dfg_inexact ]",
    "we conclude that in order to get @xmath86-dual suboptimality , i.e. @xmath137 , the inner accuracy @xmath94 and the number of outer iteration @xmath138 ( i.e. number of updates of lagrange multipliers ) have to be chosen as follows : @xmath139 indeed , by enforcing each term of the right hand side of to be smaller than @xmath140 we obtain first the bound on the number of the outer iterations @xmath138 . by replacing this bound into the expression of @xmath94",
    ", we also obtain how to choose @xmath94 , i.e the estimates .",
    "we conclude that the inner qp has to be solved with higher accuracy in dual fast gradient algorithm * dfgm * than in dual gradient algorithm * dgm*. this shows that dual gradient algorithm * dgm * is robust to inexact ( augmented ) dual first order information , while dual fast gradient algorithm * dfgm * is sensitive to inexact computations ( see also fig .",
    "[ dfgm_sensitivity ] ) . in duquad",
    "the user can choose either algorithm * dfgm * or algorithm * dgm * for solving the ( augmented ) dual problem and he can also choose the inner accuracy @xmath94 for solving the inner problem ( in the toolbox the default values for @xmath94 are taken of the same order as in ) .",
    "for a strongly convex qp with @xmath141.,title=\"fig:\",scaledwidth=52.0%,height=170 ]   for a strongly convex qp with @xmath141.,title=\"fig:\",scaledwidth=52.0%,height=170 ]",
    "it is natural to investigate how to recover an @xmath86-primal solution for the original qp . since dual suboptimality",
    "is given in the last dual iterate @xmath135 , it is natural to consider as an approximate primal solution the last primal iterate generated by algorithm * dfom * in @xmath135 , i.e. : @xmath142 note that the last primal iterate @xmath143 coincides with @xmath121 for algorithm * dgm*. however , for algorithm * dfgm * these two sequences are different , i.e. @xmath144",
    ". we will show below that the last primal iterate @xmath145 is an @xmath146-primal solution for the original qp , provided that @xmath137 .",
    "we can also construct an approximate primal solution based on an average of all previous primal iterates generated by algorithm * dfom * , i.e. : @xmath147 recall that @xmath148 in algorithm * dgm * and @xmath149 is updated according to the rule @xmath150 and @xmath70 in algorithm * dfgm*. in the sequel , we prove that the average of primal iterates sequence @xmath151 is an @xmath86-primal solution for the original qp , provided that @xmath137 .    before proving primal rate of convergence for algorithm * dfom",
    "* we derive a bound on @xmath152 , with @xmath135 generated by algorithm * dfom * , bound that will be used in the proofs of our convergence results . in the case of * dgm * , using its particular iteration , for any @xmath153 , we have : @xmath154 taking now @xmath155 and using an inductive argument , we get : @xmath156 on the other hand , for the scheme * dfgm * , we introduce the notation @xmath157 and present an auxiliary result :    @xcite [ th_tseng_2 ] let @xmath158 be generated by algorithm * dfom*(@xmath159 ) with @xmath123 , then for any lagrange multiplier @xmath46 we have : @xmath160 for all @xmath161 , where @xmath162 .    using this result and a similar reasoning as in @xcite",
    "we obtain the same relation for the scheme * dfgm*. moreover , for simplicity , in the sequel we also assume @xmath163 . in the next two sections we derive rate of convergence results of algorithm *",
    "dfom * in both primal sequences , the primal last iterate and an average of primal iterates .      in this section",
    "we present rate of convergence results for the algorithm * dfom * , in terms of primal suboptimality and infeasibility for the last primal iterate @xmath164 defined in , provided that the relations hold .",
    "let @xmath165 be some desired accuracy and @xmath166 be the primal last iterate sequence generated by algorithm * dfom*(@xmath167 ) using the inner accuracy from . then , after @xmath138 number of outer iterations given in",
    ", @xmath168 is @xmath146-primal optimal for the original qp .    using the lipschitz property of the gradient of @xmath169",
    ", it is known that the following inequality holds @xcite : @xmath170 taking @xmath171 and using the optimality condition @xmath172 for all @xmath173 , we further have : @xmath174 considering @xmath175 and observing that @xmath176 we obtain a link between the primal feasibility and dual suboptimality : @xmath177    provided that @xmath178 and using @xmath94 as in , we obtain : @xmath179 secondly , we find a link between the primal and dual suboptimality .",
    "indeed , we have for all @xmath180 : @xmath181_{{\\mathcal{k } } } \\right \\rangle.\\end{aligned}\\ ] ] further , using the cauchy - schwartz inequality , we derive : @xmath182 on the other hand , from the concavity of @xmath169 we obtain : @xmath183 taking @xmath184 and @xmath94 as in , based on and on the implicit assumption that @xmath185 , we observe that @xmath186 for both schemes * dgm * and * dfgm*. therefore , implies : @xmath187 as a conclusion , from and the previous inequality , we get the bound : @xmath188 which implies @xmath189 . using this fact and",
    "the feasibility bound , which also implies @xmath190 , we finally conclude that the last primal iterate @xmath191 is @xmath146-primal optimal .",
    "we can also prove linear convergence for algorithm * dfom * provided that @xmath192 ( i.e. the objective function is smooth and strongly convex ) and @xmath193 ( i.e. the inner problem is unconstrained ) . in this case",
    "we can show that the dual problem satisfies an error bound property @xcite .",
    "under these settings * dfom * is converging linearly ( see @xcite for more details ) .",
    "further , we analyze the convergence of the algorithmic framework * dfom * in the average of primal iterates @xmath195 defined in .",
    "since we consider different primal average iterates for the schemes * dgm * and * dfgm * , we analyze separately the convergence of these methods in @xmath195 .",
    "let @xmath165 be some desired accuracy and @xmath194 be the primal average iterate given in , generated by algorithm * dgm * , i.e. algorithm * dfom*(@xmath167 ) with @xmath196 for all @xmath161 , using the inner accuracy from . then , after @xmath138 number of outer iterations given in",
    ", @xmath197 is @xmath86-primal optimal for the original qp .",
    "first , we derive sublinear estimates for primal infeasibility for the average primal sequence @xmath195 ( recall that in this case @xmath198 ) .",
    "given the definition of @xmath199 in algorithm * dfom*(@xmath200 ) with @xmath201 , we get : @xmath202_{\\mathcal{k}_d }   \\quad \\forall j \\geq 0.\\ ] ] subtracting @xmath203 from both sides , adding up the above inequality for @xmath204 to @xmath205 , we obtain : @xmath206_{\\mathcal{k}_d } -\\lambda^j \\right\\| = \\frac{1}{k+1}{\\lvert\\lambda^{k+1 } - \\lambda^0\\rvert } .\\end{aligned}\\ ] ] if we denote @xmath207_{\\mathcal{k}_d } $ ] , then we observe that @xmath208 .",
    "thus , we have @xmath209 . using the definition of @xmath210",
    ", we obtain : @xmath211 using @xmath212 and the bound for the values @xmath94 and @xmath184 from in the previous inequality , we get : @xmath213    it remains to estimate the primal suboptimality .",
    "first , to bound below @xmath214 we proceed as follows : @xmath215_{\\mathcal{k } } \\rangle \\nonumber\\\\ & \\le f(\\hat{u}^k_{\\epsilon } ) + { \\lvert\\lambda^*\\rvert } { \\lvertg\\hat{u}^k_{\\epsilon}+g -   \\left[g\\hat{u}^k_{\\epsilon } + g\\right]_{\\mathcal{k}}\\rvert}\\nonumber\\\\ & = f(\\hat{u}^k_{\\epsilon } ) + r_d \\ ;   \\text{dist}_{\\mathcal{k } } \\left(g\\hat{u}^k_{\\epsilon } + g\\right ) .",
    "\\end{aligned}\\ ] ] combining the last inequality with , we obtain : @xmath216 secondly , we observe the following facts : for any @xmath217 , @xmath218 and the following identity holds : @xmath219 based on previous discussion , and , we derive that @xmath220 taking now @xmath221 , @xmath184 and using an inductive argument , we obtain : @xmath222 provided that @xmath163 . from , and",
    ", we obtain that the average primal iterate @xmath197 is @xmath86-primal optimal .",
    "further , we analyze the primal convergence rate of algorithm * dfgm * in the average primal iterate @xmath194 :    let @xmath165 be some desired accuracy and @xmath194 be the primal average iterate given in , generated by algorithm * dfgm * , i.e. algorithm * dfom*(@xmath167 ) with @xmath223 for all @xmath224 , using the inner accuracy from . then , after @xmath138 number of outer iterations given in , @xmath197 is @xmath86-primal optimal for the original qp .",
    "recall that we have defined @xmath225 .",
    "then , it follows : @xmath226 for any @xmath227 we denote @xmath228 and thus we have @xmath229_{\\mathcal{k}_d}$ ] . in these settings",
    ", we have the following relations : @xmath230_{\\mathcal{k}_d})\\right ) \\nonumber\\\\ & = \\theta_j \\left(\\left [ \\mu^{j } + \\frac{1}{2l_{\\text{d } } } \\bar{\\nabla } d_{\\rho}(\\mu^{j } ) \\right]_{\\mathcal{k}_d } - \\lambda^{j}\\right ) \\nonumber\\\\ & =   \\theta_{j}(\\lambda^{j } - \\mu^{j})\\nonumber\\\\ & = \\theta_{j}(\\lambda^{j } - \\lambda^{j-1 } ) + ( \\theta_{j-1 } -1 ) ( \\lambda^{j-2 } - \\lambda^{j-1})\\nonumber\\\\ & = \\underbrace{\\lambda^{j-1 } + \\theta_{j}(\\lambda^{j } - \\lambda^{j-1})}_{=l^{j } } - \\underbrace{(\\lambda^{j-2 } + \\theta_{j-1}(\\lambda^{j-1 } - \\lambda^{j-2}))}_{=l^{j-1}}.\\end{aligned}\\ ] ] for simplicity consider @xmath231 and @xmath232 . adding up the above equality for @xmath233 to @xmath234 , multiplying by @xmath235 and observing that @xmath236_{\\mathcal{k}_d } \\in \\mathcal{k}$ ] for all @xmath237 , we obtain : @xmath238_{\\mathcal{k}_d } ) \\right)\\right\\| \\\\ & =   \\left\\| \\sum\\limits_{j=0}^k   \\frac{\\theta_j}{s_k } \\left ( \\bar{\\nabla } d_{\\rho}(\\mu^j ) - 2 l_{\\text{d}}(z^j-[z^j]_{\\mathcal{k}_d } ) \\right)\\right\\| \\\\ & \\overset{\\eqref{feasibility_aux1}}{= } \\frac{l_\\text{d}}{s_k}{\\lvertl^{k}-l^0\\rvert } \\le \\frac{4l_\\text{d}}{(k+1)^2 } { \\lvertl^k -l^{0}\\rvert}.\\end{aligned}\\ ] ] taking @xmath239 in lemma [ th_tseng_2 ] and using that the two terms @xmath240 and @xmath241 are positive , we get : @xmath242    thus , we can further bound the primal infeasibility as follows : @xmath243 therefore , using @xmath138 and @xmath94 from , it can be derived that : @xmath244 further , we derive sublinear estimates for primal suboptimality .",
    "first , note the following relations : @xmath245 summing on the history and using the convexity of @xmath246 , we get : @xmath247 using in lemma [ th_tseng_2 ] , and dropping the term @xmath248 , we have : @xmath249 moreover , we have that : @xmath250 now , by choosing the lagrange multiplier @xmath251 and @xmath184 in , we have : @xmath252    on the other hand , we have : @xmath253_{\\mathcal{k } } \\rangle\\nonumber\\\\ & \\le f(\\hat{u}^k_{\\epsilon } ) + r_d \\ ; \\text{dist}_{{\\mathcal{k}}}(g \\hat{u}^k_{\\epsilon } + g).\\end{aligned}\\ ] ] taking @xmath254 and @xmath94 from , and using , we obtain : @xmath255 finally , from , and , we get that the primal average sequence @xmath197 is @xmath86 primal optimal .    in conclusion , in duquad we generate two approximate primal solutions @xmath145 and @xmath151 for each algorithm * dgm * and * dfgm*. from previous discussion it can be seen that theoretically , the average of primal iterates sequence @xmath151 has a better behavior than the last iterate sequence @xmath145 . on the other hand , from our practical experience",
    "( see also section [ numerical_tests ] ) we have observed that usually dual first order methods are converging faster in the primal last iterate than in a primal average sequence .",
    "moreover , from our unified analysis we can conclude that for both approaches , ordinary dual with @xmath256 and augmented dual with @xmath11 , the rates of convergence of algorithm * dfom * are the same .",
    "in this section we derive the total computational complexity of the algorithmic framework * dfom*. without lose of generality , we make the assumptions : @xmath257 however , if any of these assumptions does not hold , then our result are still valid with minor changes in constants .",
    "now , we are ready to derive the total number of iterations for * dfom * , i.e. the total number of projections on the set @xmath93 and of matrix - vector multiplications @xmath258 and @xmath259 .    [ in : th_last ] let @xmath165 be some desired accuracy and the inner accuracy @xmath94 and the number of outer iterations @xmath138 be as in . by setting @xmath260 and assuming that the primal iterate @xmath126 is obtained by running the algorithm * fom*(@xmath261 ) , then @xmath164 ( @xmath194 ) is @xmath146 ( @xmath86 ) primal optimal after a total number of projections on the set @xmath93 and of matrix - vector multiplications @xmath258 and @xmath259 given by : @xmath262    from lemma [ lemma_sublin_dfg ] we have that the inner problem ( i.e. finding the primal iterate @xmath126 ) for a given @xmath263 can be solved in sublinear ( linear ) time using algorithm * fom*(@xmath264 ) , provided that the inner problem has smooth ( strongly ) convex objective function , i.e. @xmath265 has @xmath266 ( @xmath267 ) .",
    "more precisely , from lemma [ lemma_sublin_dfg ] , it follows that , regardless if we apply algorithms * dfgm * or  * dgm * , we need to perform the following number of inner iterations for finding the primal iterate @xmath126 for a given @xmath263 : @xmath268 combining these estimates with the expressions for the inner accuracy @xmath94 , we obtain , in the first case @xmath269 , the following inner complexity estimates : @xmath270 multiplying @xmath271 with the number of outer iterations @xmath138 from and minimizing the product @xmath272 over the smoothing parameter @xmath273 ( recall that @xmath100 and @xmath274 ) , we obtain the following optimal computational complexity estimate ( number of projections on the set @xmath93 and evaluations of @xmath258 and @xmath259 ) : @xmath275 which is attained for the optimal parameter choice : @xmath276    using the same reasoning for the second case when @xmath277 , we observe that the value @xmath278 is also optimal for this case in the following sense : the difference between the estimates obtained with the exact optimal @xmath273 and the value @xmath279 are only minor changes in constants .",
    "therefore , when @xmath277 , the total computational complexity ( number of projections on the set @xmath93 and evaluations of @xmath258 and @xmath259 ) is : @xmath280    in conclusion , the last primal iterate @xmath164 is @xmath146-primal optimal after @xmath281 ( @xmath282 ) total number of projections on the set @xmath93 and of matrix - vector multiplications @xmath258 and @xmath259 , provided that @xmath269 ( @xmath277 ) .",
    "similarly , the average of primal iterate @xmath194 is @xmath86-primal optimal after @xmath281 ( @xmath282 ) total number of projections on the set @xmath93 and of matrix - vector multiplications @xmath258 and @xmath259 , provided that @xmath269 ( @xmath277 ) .",
    "moreover , the optimal choice for the parameter @xmath273 is of order @xmath283 , provided that @xmath38 .",
    "let us analyze now the computational cost per inner and outer iteration for algorithm * dfom*(@xmath284 ) for solving approximately the original qp :    * inner iteration * : when we solve the inner problem with the nesterov s algorithm * fom*(@xmath285 ) , the main computational effort is done in computing the gradient of the augmented lagrangian @xmath286 defined in , which e.g. has the form : @xmath287 in duquad these matrix - vector operations are implemented efficiently in c ( matrices that do not change along iterations are computed once and only @xmath288 is computed at each outer iteration ) .",
    "the cost for computing @xmath289 for general qps is @xmath290 .",
    "however , when the matrices @xmath35 and @xmath291 are sparse ( e.g. network utility maximization problem ) the cost @xmath290 can be reduced substantially .",
    "the other operations in algorithm * fom*(@xmath285 ) are just vector operations and thus they are of order @xmath292 .",
    "thus , the dominant operation at the inner stage is the matrix - vector product .",
    "* outer iteration * : when solving the outer ( dual ) problem with algorithm * dfom*(@xmath284 ) , the main computational effort is done in computing the inexact gradient of the dual function : @xmath293 the cost for computing @xmath294 for general qps is @xmath295 .",
    "however , when the matrix @xmath291 is sparse , this cost can be reduced .",
    "the other operations in algorithm * dfom*(@xmath130 ) are of order @xmath296 .",
    "thus the dominant operation at the outer stage is also the matrix - vector product .",
    "[ fig : gprof_n150_dfgm_case1 ] displays the result of profiling the code with gprof . in this simulation , a standard qp with inequality constraints and dimensions @xmath297 and @xmath298",
    "was solved by algorithm * dfgm*. the profiling summary is listed in the order of the time spent in each file .",
    "this figure shows that almost all the time for executing the program is spent in the library module _ math - functions.c_. furthermore , _ mtx - vec - mul _ is by far the dominating function in this list .",
    "this function is multiplying a matrix with a vector , which is defined as a special type of matrix multiplication .        in conclusion ,",
    "in duquad the main operations are the matrix - vector products .",
    "therefore , duquad is adequate for solving qp problems on hardware with limited resources and capabilities , since it does not require any solver for linear systems or other complicating operations , while most of the existing solvers for qps from the literature implementing e.g. active set or interior point methods require the capability of solving linear systems . on the other hand",
    ", duquad can be also used for solving large - scale sparse qp problems since the iterations are very cheap in this case ( only sparse matrix - vector products ) .",
    "duquad is mainly intended for small to medium size , dense qp problems , but it is of course also possible to use duquad to solve ( sparse ) qp instances of large dimension .",
    "the duquad software package is available for download from : +   + and distributed under general public license to allow linking against proprietary codes .",
    "proceed to the menu point `` software '' to obtain a zipped archive of the most current version of duquad . the users manual and extensive source code documentation are available here as well .",
    "+ an overview of the workflow in duquad is illustrated in fig .",
    "[ fig : duquad_workflow ] .",
    "a qp problem is constructed using a matlab script called _",
    "test.m_. then , the function _",
    "duquad.m _ is called with the problem as input and is regarded as a prepossessing stage for the online optimization .",
    "the binary mex file is called , with the original problem and the extra info as input .",
    "main.c _ file of the c - code includes the mex framework and is able to convert the matlab data into c format .",
    "furthermore , the converted data gets bundled into a c struct and passed as input to the algorithm that solves the  problem .",
    "we plot in fig .",
    "[ fig : qp_cpu ] the average cpu time for several solvers , obtained by solving @xmath299 random qp s with equality constraints ( @xmath11 and @xmath19 ) for each dimension @xmath300 , with an accuracy @xmath141 and the stopping criteria @xmath301 and @xmath302 less than the accuracy @xmath86 . in both algorithms * dgm * and * dfgm * we consider the average of iterates @xmath303 .",
    "since @xmath304 , we have chosen @xmath305 . in the case of algorithm * dgm * , at each outer iteration the inner problem is solved with accuracy @xmath306 . for the algorithm * dfgm",
    "* we consider two scenarios : in the first one , the inner problem is solved with accuracy @xmath307 , while in the second one we use the theoretic inner accuracy .",
    "we observe a good behavior of algorithm * dfgm * , comparable to cplex and gurobi .",
    "we plot in fig .",
    "[ fig : comparison_dfo ] the number of iterations of algorithms * dgm * and * dfgm * in the primal last and average iterates for @xmath308 random qps with inequality constraints ( @xmath34 and @xmath18 ) of variable dimension ranging from @xmath309 to @xmath310 .",
    "we choose the accuracy @xmath141 and the stopping criteria was @xmath311 and @xmath312 less than the accuracy @xmath86 . from this figure",
    "we observe that the number of iterations are not varying much for different test cases and also that the number of iterations are mildly dependent on problem s dimension .",
    "finally , we observe that dual first order methods perform usually better in the primal last iterate than in the average of primal iterates .",
    "-0.1 cm , @xmath18 ) for * dgm * and * dfgm * in primal last / average of iterates for different test cases of the same dimension ( left ) and of variable dimension  ( right ) .",
    ", title=\"fig:\",scaledwidth=105.0%,height=188 ]              a. domahidi , a. zgraggen , m. zeilinger , m. morari and c. jones , _ efficient interior point methods for multistage problems arising in receding horizon control _ , ieee conference decision and control , 668674 , 2012 .",
    "j. jerez , k. ling , g. constantinides and e. kerrigan , _ model predictive control for deeply pipelined field - programmable gate array implementation : algorithms and circuitry _ , iet control theory and applications , 6(8):10291041 , 2012 .",
    "v.  nedelcu , i.  necoara and q.  tran dinh , _ computational complexity of inexact gradient augmented lagrangian methods : application to constrained mpc _ , siam journal control and optimization , 52(5):31093134 , 2014 .",
    "i. necoara , l. ferranti and t. keviczky , _ an adaptive constraint tightening approach to linear mpc based on approximation algorithms for optimization _ , optimal control applications and methods , doi : 10.1002/oca.2121 , 1 - 19 , 2015 .",
    "s. richter , m. morari and c. n. jones , _ towards computational complexity certification for constrained mpc based on lagrange relaxation and the fast gradient method _ , ieee conference decision and control , 52235229 , 2011 .                r.d .",
    "zimmerman , c.e .",
    "murillo - sanchez and r.j .",
    "thomas , _ matpower : steady - state operations , planning , and analysis tools for power systems research and education _ , ieee transactions on power systems , 26(1):1219 , 2011 ."
  ],
  "abstract_text": [
    "<S> in this paper we present the solver duquad specialized for solving general convex quadratic problems arising in many engineering applications . </S>",
    "<S> when it is difficult to project on the primal feasible set , we use the ( augmented ) lagrangian relaxation to handle the complicated constraints and then , we apply dual first order algorithms based on inexact dual gradient information for solving the corresponding dual problem . </S>",
    "<S> the iteration complexity analysis is based on two types of approximate primal solutions : the primal last iterate and an average of primal iterates . </S>",
    "<S> we provide computational complexity estimates on the primal suboptimality and feasibility violation of the generated approximate primal solutions . </S>",
    "<S> then , these algorithms are implemented in the programming language c in duquad , and optimized for low iteration complexity and low memory footprint . </S>",
    "<S> duquad has a dynamic matlab interface which make the process of testing , comparing , and analyzing the algorithms simple . </S>",
    "<S> the algorithms are implemented using only basic arithmetic and logical operations and are suitable to run on low cost hardware . </S>",
    "<S> it is shown that if an approximate solution is sufficient for a given application , there exists problems where some of the implemented algorithms obtain the solution faster than state - of - the - art commercial solvers . </S>"
  ]
}