{
  "article_text": [
    "in the classical shannon ",
    "theoretic approach to cryptology ( see , e.g. , @xcite,@xcite,@xcite and references therein ) , two assumptions are traditionally made .",
    "the first is that the reconstruction of the decrypted plaintext source at the legitimate receiver is distortion  free ( or almost distortion  free ) , and the second , which is related , is that the encryption and the decryption units share identical copies of the same key .",
    "yamamoto @xcite has relaxed the first assumption and extended the theory of shannon secrecy systems into a rate  distortion scenario , allowing lossy reconstruction at the legtimate receiver .    in this correspondence , we examine also the second assumption . referring to fig .  1",
    ", we consider the case where the key is delivered to the legitimate receiver across a channel , which is cryptographically secure , but has limited capacity . for this setting",
    ", we characterize the achievable region in the space of three figures of merit : the security level ( measured in terms of the equivocation ) , the compressibility of the cryptogram , and the distortion associated with the reconstruction of the plaintext source .",
    "one conceptually simple approach to handle such a situation would be to apply a reliable channel code to the encryption key bits , at a rate below the capacity of the channel , and thereby obtain , with high probability , the exact copy of the transmitted key bits at the receiver side . with this approach ,",
    "however , the effective key rate , and hence the security level in terms of the equivocation , is limited by the channel capacity .",
    "the question that naturally arises at this point , especially in the lossy reconstruction scenario , is whether this is the best one can do .    to sharpen the question ,",
    "let us even assume that there is an unlimited reservoir of random key bits at the transmitter side , denoted @xmath0 , @xmath1 , @xmath2 .",
    "then , perhaps one might wish to use more key rate ( somewhat above capacity ) for encryption and thereby increase the security of the cryptogram at the expense of some distortion at the reconstruction , due to the unavoidable mismatch between the encryption and decryption keys . to explore this point ,",
    "let us consider a few speculative strategies .    in the first strategy",
    ", one sends the key bits @xmath3 across the channel uncodedly ( assuming , for simplicity , that the channel has a binary input  output alphabet ) . referring to fig .  1 , let us take then @xmath4 and @xmath5 , @xmath2 . in this case , the noisy version of the key , obtained at the receiver side , @xmath6 , is of course somewhat different from the original key .",
    "however , since only lossy reconstruction of the plaintext is required at the receiver side , it may seem conceivable that a reasonably small difference between the keys at both ends could be managable and thus cause a reasonably small distortion in the reconstruction .",
    "this is relatively easy to have if the encryption of the source precedes compression , as proposed in @xcite : one may apply , for example , a certain memoryless mapping from the key bit stream into a stream of symbols @xmath7 taking ( two of the ) values in the alphabet of plaintext source , @xmath8 . then assuming that @xmath8 is a commutative group endowed with an addition operation @xmath9 ( e.g. , addition modulo the alphabet size ) , one can create the enctypted sequence @xmath10 , @xmath2 and then compress the block @xmath11 with @xmath12 as side information at the receiver , using a slepian ",
    "wolf encoder @xcite in the lossless case , or a wyner ",
    "ziv code @xcite in the lossy case .",
    "assuming , for simplicity , lossless compression , then upon decompressing the source at the receiver side and obtaining @xmath13 ( which is with high probability equal to @xmath11 ) , one ` subtracts ' the noisy version of the key and obtain ( with high probability ) the reconstruction @xmath14 , @xmath2 , where @xmath15 is the corresponding noisy version of @xmath16 .",
    "now , since @xmath17 , for all @xmath18 , then for a difference distortion measure @xmath19 , the distortion between @xmath20 and its reconstruction @xmath21 is identical to the distortion between the original key @xmath16 and its noisy version @xmath15 .",
    "a somewhat more sophisticated version of this scheme generates @xmath7 from the key bits using a simulator of a certain ( memoryless ) process ( see , e.g. , @xcite and references therein ) , and then applies a good source  channel code to encode @xmath22 across the channel . the reconstructed version at the receiver side , @xmath23 ,",
    "would then have the minimum possible distortion relative to @xmath22 , given by the distortion ",
    "rate function of @xmath24 computed at the channel capacity , and therefore so would be also the distortion between @xmath25 and @xmath26 .",
    "moreover , there is an additional degree of freedom with regard to the choice of the probability law of @xmath24 for trading off between the security , which is given by the entropy rate of @xmath24 , and the distortion , i.e. , distortion ",
    "rate function of @xmath24 computed at the channel capacity .",
    "another solution strategy may be based on the following point : note that for the purpose of reliable transmission and decoding of the key bits across the channel , the cryptogram ( denoted by @xmath27 in fig .  1 ) , which is a function of these key bits as well , may serve as useful side information at the decoder , unless it is statistically independent of these bits .",
    "thus , one would speculate that it might be wise to allow some dependence between @xmath27 and @xmath3 and thus sacrifice some compression performance at the benefit gaining performance in communicating the key across the channel .",
    "let us assume that the bits of the key string @xmath28 are xored ( added modulo 2 ) with the bits of the compressed version of the source . then , if the compression algorithm is designed in such a way such the bits of the compressed version of @xmath29 are _ not _ symmetric , then @xmath27 is correlated to @xmath30 , and so @xmath27 can be viewed as a noisy version of @xmath30 , which was transmitted uncodedly across a `` parallel channel '' . in such a case ,",
    "we can then think of the key bits as being encoded using a _ systematic _",
    "code across the combined channel whose outputs are @xmath27 and @xmath31 and the effective rate of this code is smaller than that over the original channel depicted in fig .  1 . another way to look at this is the following : the key string @xmath30 can be compressed by a slepian  wolf encoder given @xmath27 ( as side information at the decoder ) before being channel coded , thus increasing the effective capacity by a factor given by the reciprocal of the conditional entropy of the key given the cryptogram .    we show in this correspondence that none of the ideas raised in the last four paragraphs , nor any other creative idea one may have , can work better than the first strategy we mentioned earlier , which is the following : at the lower part of the encoder of fig .  1 ( the `` key encoder '' ) , use a good channel code at rate below capacity , whose role is to reliably transmit a certain amount of key bits . at the upper block of the encoder of fig .  1 , first compress @xmath29 by an optimal rate",
    " distortion code to obtain @xmath32 bits , where @xmath33 is the rate ",
    "distortion function of @xmath29 , and then encrypt the compressed bitstream with the same bits that are fed into the channel code . at the receiver , first decode the key bits from the channel output , and then use them to decrypt and decompress the source .",
    "the result on the optimality of this scheme has a flavor similar to that of the classical source  channel separation theorem in three aspects : ( i ) there is a complete decoupling between source coding ( for @xmath29 ) and channel coding ( for the key bits ) from the operative point of view as well as from the viewpoint of code design ( unlike in the other strategies described above ) , ( ii ) the best possible strategy of controlling the distortion is only via rate  distortion coding , and ( iii ) the necessary and sufficient condition for perfect secrecy is @xmath34 , which is of the same form as the source  channel separation theorem .",
    "the outline of this correspondence is as follows . in section 2 ,",
    "we define notation conventions and give a formal definition of the problem . in section 3 , we state and",
    "prove the main result , and in section 4 , we discuss a few variations and extensions .",
    "we begin by establishing some notation conventions . throughout this paper , scalar random variables ( rv s )",
    "will be denoted by capital letters , their sample values will be denoted by the respective lower case letters , and their alphabets will be denoted by the respective calligraphic letters .",
    "a similar convention will apply to random vectors and their sample values , which will be denoted with same symbols superscripted by the dimension .",
    "thus , for example , @xmath29 ( @xmath35  positive integer ) will denote a random @xmath35-vector @xmath36 , and @xmath37 is a specific vector value in @xmath38 , the @xmath35-th cartesian power of @xmath8 .",
    "sources and channels will be denoted generically by the letter @xmath39 , subscripted by the name of the rv and its conditioning , if applicable , e.g. , @xmath40 is the probability function of @xmath41 at the point @xmath42 , @xmath43 is the conditional probability of @xmath44 given @xmath45 , and so on . whenever clear from the context , these subscripts will be omitted .",
    "information theoretic quantities like entropies and mutual informations will be denoted following the usual conventions of the information theory literature , e.g. , @xmath46 , @xmath47 , and so on .",
    "for single ",
    "letter information quantities ( i.e. , when @xmath48 or @xmath49 ) , subscripts will be omitted , e.g. , @xmath50 will be denoted by @xmath51 , similarly , @xmath52 will be denoted by @xmath53 , and so on .",
    "we now turn to the formal description of the model and the problem setting , as described in the introduction , and referring to fig .  1 .",
    "a source @xmath54 , generates a sequence of independent copies , @xmath55 of a finite  alphabet rv , @xmath56 , whose entropy is @xmath57 . at the same time and independently , a discrete memoryless channel ( dmc ) @xmath58 receives input symbols @xmath59 with coordinates taking values in a finite alphabet @xmath60 , and produces output symbols @xmath61 with coordinates taking values in a finite alphabet @xmath62 , according to a conditional probability law given by the product of single ",
    "letter transition probabilities @xmath63 . the relative rate between the operation of the channel @xmath58 and that of the source is @xmath64 channel symbols per source symbol .",
    "this means that while the source generates a block of @xmath35 symbols , say , @xmath65 , according to the above mentioned probability law , the channel conveys @xmath66 transmissions , is a positive integer . ]",
    "i.e. , it receives a channel input block of length @xmath67 , @xmath68 , and outputs another block of the same length @xmath69 according to the above described conditional probability law .",
    "let @xmath70 denote the channel capacity .",
    "in addition to the source @xmath54 and the channel @xmath58 , yet another source , @xmath71 , henceforth referred to as the _ key source _ , generates an infinite sequence of i.i.d .",
    "purely random bits , @xmath0 , independently of the source @xmath55 . the operation rate of the key source relative to the source @xmath54 ( and the channel @xmath58 ) will be immaterial , i.e. , we will assume that the reservoir of key bits , for every finite period of time , is sufficiently large so that it is effectively unlimited .    a _ block code for joint coding and encryption _ with parameters @xmath67 and @xmath72 , consists of three mappings .",
    "the first mapping is the compressor ",
    "encrypter @xmath73 , where @xmath74 , @xmath75 being the compression rate . upon receiving a source vector @xmath76 and a key sequence @xmath77 , this mapping produces a binary cryptogram @xmath78 according to @xmath79 .",
    "the second mapping is the key ",
    "encoder @xmath80 , which produces a channel input vector @xmath81 according to @xmath82 .",
    "finally , the third mapping is the decoder @xmath83 , where @xmath84 is the reproduction alphabet . upon receiving a cryptogram @xmath85 and a channel output vector @xmath86",
    ", the decoder produces a reproduction vector according to @xmath87 .",
    "let @xmath88 denote a single ",
    "letter distortion measure between source symbols and the reproduction symbols , and let the distortion between the vectors , @xmath76 and @xmath89 , be defined additively across the corresponding components , as usual .",
    "we will assume that @xmath90 is bounded , i.e. , @xmath91 .",
    "let @xmath33 denote the rate ",
    "distortion function of the source @xmath54 with respect to @xmath90 .",
    "an @xmath92 code for joint coding and encryption is a block code with parameters @xmath67 and @xmath64 , as above , which also satisfies the following requirements :    * the expected distortion between the source and the reproduction satisfies @xmath93 * the rate of the cryptogram satisfies @xmath94 * the equivocation of the source satisfies @xmath95    for a given @xmath64 , a triple @xmath96 is said to be _ achievable _ if for every @xmath97 , there is a sufficiently large @xmath67 for which @xmath98 block codes for joint coding and encryption exist .",
    "our purpose , in this paper is to characterize the _ achievable region _ of triples @xmath96 , i.e. , the set of all achievable triples @xmath96 .",
    "our main coding theorem is the following :    a triple @xmath96 is achievable if and only if the following conditions are both satisfied :    * @xmath99_+$ ] , where @xmath100_+{\\stackrel{\\delta } { = } } \\max\\{a,0\\}$ ] . * @xmath101 .",
    "it should be noted that for a given @xmath102 , there is no conflict ( or interaction ) between maximizing @xmath103 and minimizing @xmath104 : as is well known , @xmath104 is lower bounded by @xmath33 even if there is no security requirement , but on the other hand , even in the presence of the highest possible security level requirement , of @xmath105 , the compression ratio @xmath33 is still achievable @xcite . by the same token , and as will be evident from the proof , @xmath103 is upper bounded by @xmath105 even if there is no compressibility requirement , yet it remains achievable even if the compression ratio of @xmath33 is required .",
    "the remaining part of this section is devoted to the proof of theorem 1 .",
    "_ we begin with the converse part .",
    "let an @xmath98 block code for joint coding and encryption be given .",
    "now , since @xmath106_+=\\min\\{h(u),h(u)-r(d)+\\lambda c\\},\\ ] ] we have to prove that both @xmath107 and @xmath108 .",
    "the first bound is trivial since @xmath109 where the first inequality is by definition of an @xmath98 block code for joint coding and encryption .",
    "the inequality @xmath107 now follows from the arbitrariness of @xmath97 . as for the second bound",
    ", we have @xmath110+nc,\\end{aligned}\\ ] ] where the second line is a standard identity , the third is because @xmath111 is a function of @xmath112 , the fourth is because conditioning reduces entropy ( used thrice ) , the fifth is due to the fact that @xmath113 is a markov chain , the sixth is due to the memorylessness of the source and the fact that @xmath114 ( which is also convex ) , and the last line is due to the memorylessness of the channel and the fact that @xmath70 .",
    "again , dividing by @xmath35 , and using the arbitrariness of @xmath97 as well as the continuity of @xmath33 , we get the second bound on @xmath103 , and so , the necessity of condition ( a ) follows .",
    "the proof of the necessity of condition ( b ) is similar to the proof of the converse to the ordinary rate  distortion coding theorem , except that the presence of @xmath31 ( which is independent of @xmath29 ) at the decoder has to be taken into account : @xmath115\\nonumber\\\\ & \\ge&\\sum_{i=1}^n[h(u_i)-h(u_i|w^m , y^n)]\\nonumber\\\\ & = & \\sum_{i=1}^ni(u_i;w^m , y^n)\\nonumber\\\\ & \\ge&\\sum_{i=1}^ni(u_i;v_i)\\nonumber\\\\ & \\ge&nr(d+\\epsilon),\\end{aligned}\\ ] ] where the first line is by definition of an @xmath98 block code for joint coding and encryption , the second , third , fourth and sixth are standard identities and inequalities ,",
    "the fifth is based on the memorylessness of the source and its independence of @xmath31 , the seventh is based on the data processing inequality and the fact that @xmath21 is a function of @xmath116 , and the last inequality is again by the informational definition of @xmath33 and its convexity .",
    "taking again @xmath117 to zero , this completes the proof of the converse part of theorem 1 .    as for the direct part ,",
    "consider the following ( conceptually ) simple coding scheme . for",
    "a given arbirarily small @xmath97 , let @xmath118\\}$ ] and let @xmath119 be given by a channel code whose error probability is below some @xmath120 , provided that @xmath67 is sufficiently large .",
    "since the rate of this code never exceeds @xmath121 , such a channel code exists by the classical channel coding theorem . as for @xmath122 , first apply a rate  distortion code for @xmath29 , whose rate is @xmath123 , and then encrypt @xmath124 of the resulting @xmath125 $ ] bits by @xmath126 ( using the ordinary bit  by ",
    "bit xor ) . as for the equivocation",
    ", we have @xmath127+h(w^m|u^n)\\nonumber\\\\ & = & nh(u)-n[r(d)+\\epsilon]+\\ell\\nonumber\\\\ & = & nh(u)-n[r(d)+\\epsilon]+\\min\\{n(c-\\epsilon),n[r(d)+\\epsilon]\\}\\nonumber\\\\ & \\ge&n\\left(h(u)-[r(d)-\\lambda c]_+-2\\epsilon\\max\\{1,\\lambda\\}\\right),\\end{aligned}\\ ] ] where the first inequality follows from the fact that the rate  distortion code is at rate @xmath128 , and the following equality is due to the fact that @xmath124 bits of the compressed bit string are encrypted . at the decoder , first , the @xmath124 key bits @xmath126 are decoded , and then the decoded key bits @xmath129 are used to decrypt @xmath85 and then use the rate  distortion decoder to produce @xmath130 . with probability at least @xmath131 , the decoded key bits",
    "@xmath129 agree with the original ones @xmath126 and then @xmath85 is decrypted correctly to produce the appropriate reproduction vector @xmath130 within distortion @xmath102 . at the event of erroneous decoding of @xmath126 , the distortion",
    "can only be bounded by @xmath132 , but this should be weighed by the probability of error , which is upper bounded by @xmath133 , and hence contributes only an arbitrarily small additional distortion .",
    "this completes the proof of theorem 1 .",
    "in this section , we discuss a few variations and extensions of the model considered .",
    "we have already mentioned in the introduction that theorem 1 has the spirit of a separation theorem , from several points of view . among them",
    "is the immediate observation that perfect security ( in the sense that @xmath134 ) can be achieved if and only if @xmath135 , an inequality of the very same form as that of the classical joint source  channel separation theorem . in this context",
    ", we should also point out that it is straightforward to extend our setup to a situation of ordinary joint source  channel coding , corresponding to the case where the cryptogram @xmath85 needs to be transmitted via a noisy channel , independent of the key distribution channel .",
    "the only modification to theorem 1 would be to replace @xmath104 in part ( b ) by the capacity of the main channel .",
    "thus , we have a two  fold separation theorem .",
    "suppose that the compressibility of the cryptogram is not an issue , in other words , @xmath104 is immaterial and we are only interested in the tradeoff between @xmath102 and @xmath103 . in this case , there exist situations where optimal performance can be achieved using very simple coding systems , similiarly to the well  known special cases , where this can be done in the context of classical joint source  channel coding ( see , e.g. , @xcite ) .",
    "let us suppose , for example , that @xmath136 , @xmath137 , and that the distortion measure @xmath90 is a difference distortion measure , i.e. , @xmath138 for a well  defined subtraction operation ( cf .  the corresponding discussion in the introduction ) .",
    "suppose further that @xmath54 , which is the uniform distribution over @xmath8 , is the capacity  achieving input for the channel @xmath58 and that @xmath58 in turn achieves the rate ",
    "distortion function of @xmath54 at distortion level @xmath102 , i.e. , @xmath139 . for example",
    ", @xmath54 may be the bss and @xmath58 may be the bsc with crossover probability @xmath102",
    ". then one can easily achieve perfect secrecy , @xmath140 , at the minimum possible distortion , i.e. , @xmath141 ( @xmath142 being the distortion  rate function of @xmath41 ) in the following manner , which is similar to one of the strategies discussed in the introduction : let @xmath7 be a simulated memoryless process , generated from @xmath3 , with the same ( uniform ) distribution as @xmath55 .",
    "note that when @xmath143 is a power of 2 , this is very easy to implement since @xmath41 is uniform . for encryption ,",
    "let @xmath144 . then , obviously , @xmath145 since @xmath20 and @xmath16 are uniformly distributed and independent , and so , perfect secrecy is guaranteed . as for the key transmission ,",
    "let us send @xmath24 uncodedly across the channel , i.e. , @xmath146 . since @xmath58 achieves the rate ",
    "distortion of @xmath25 , and hence also that of @xmath24 , then the channel output @xmath147 will have distortion @xmath102 relative to @xmath24 . at the the decoder",
    ", we simply apply the equation @xmath148 . since @xmath149 , then @xmath150 .",
    "thus , optimal performance is achieved using a very simple system once we have an independent copy of @xmath25 as a key .",
    "another point regarding the case where @xmath104 is immaterial , is the following : it turns out that part ( a ) of theorem 1 ( both the necessity and the sufficiency ) would still apply even if we broaden the scope to a wider class of encoders that allow both @xmath81 and @xmath85 to depend on both @xmath151 and @xmath152 .",
    "this means that @xmath153 is redefined as @xmath154 , and so , @xmath155 .",
    "the direct part would use the same scheme as before . as for the converse part ,",
    "note that eq .",
    "( [ 2nd ] ) is general enough to allow this setup .",
    "the conclusion then is that if only @xmath102 and @xmath103 are the figure of merits of interest , then a good key code @xmath153 need not really use its accessibility to @xmath151 .",
    "the situation becomes somewhat more involved when the compressibility is brought back into the picture , because then the encoder has two paths through which it can pass descriptions of the source .",
    "note that if @xmath135 , the encoder can transmit the entire description via the key distribution channel , without using the main channel at all , thus @xmath156 .",
    "consider the case where one is interested not only to guarantee a certain security level @xmath103 with regard to the original source , but also to guarantee a security level @xmath157 with regard to the reproduction @xmath111 .",
    "this makes sense because it is actually @xmath111 the part of the information that is communicated to the legitimate receiver and thus has to be protected ( see also @xcite ) . to derive necessary conditions for securing @xmath111 at level @xmath157",
    ", we consider two chains of inequalities .",
    "the first is the following : @xmath158 where @xmath159 is random variable taking values in the set @xmath160 with the uniform distribution and @xmath161 .",
    "thus , our first necessary condition for security level @xmath157 is that there exists a random variable @xmath162 with alphabet @xmath84 ( jointly distributed with @xmath41 ) such that @xmath163 .",
    "the second chain of inequalities is as follows : @xmath164 where @xmath165 is random variable taking values in the set @xmath166 with the uniform distribution and @xmath167 .",
    "the second equality is due to the fact that @xmath111 is a function of @xmath112 and so @xmath168 .",
    "thus , another necessary condition is the existence of random variable @xmath169 at the output of the channel @xmath58 ( which means the existence of a channel input variable @xmath170 that induces @xmath169 via @xmath58 ) such that @xmath171 .",
    "the combination of the two necessary conditions then gives @xmath172 .",
    "a restatement of the necessity part of theorem 1 would then be the following : if @xmath173 is achievable then there exist a channel @xmath174 and a source @xmath175 such that the following conditions are simultaneously satisfied :      note that in contrast to theorem 1 , we are no longer taking the minimum of @xmath180 to obtain @xmath33 , nor do we take the maximum of @xmath53 to obtain @xmath181 .",
    "the reason is that such optimizations might be in partial conflict with the need to achieve large values of @xmath182 and @xmath183 in order to meet condition ( b ) . thus , there are more complicated compromises in the choice of @xmath170 and @xmath162 when the tradeoff involves the additional parameter @xmath157 .",
    "the achievability of this set of conditions remains open in general .",
    "however , for the special case where the channel @xmath58 is deterministic , that is , @xmath169 is a deterministic function of @xmath170 , and so @xmath184 , the achievability scheme is essentially the same as before ( but with general choices of @xmath175 and @xmath174 ) as long as the required security @xmath157 does not exceed the level @xmath185 . if it is higher , and if @xmath186 exceeds @xmath180 the additional key bits beyond @xmath187 ( but not more than @xmath182 ) conveyed by the channel can be used to control the ( secret ) choice of the rate  distortion codebook among up to @xmath188 distinct codebooks that exist ( cf .  @xcite,@xcite ) and thereby achieve the extra security needed with regard to @xmath111 .",
    "note that here , the separation principle no longer holds as before , in the strong meaning of this term , because now , the choice of @xmath175 and @xmath174 involves compromises where there is an interaction between the source coding of @xmath29 and the channel coding of @xmath3 .",
    "finally , consider the scenario of the previous subsection , where in addition , there is noiseless feedback from the channel output to the transmitter . in this case , it is clear too how to secure @xmath111 to the level of @xmath189 , and it is also clear that this value can not be further improved upon . here , the encoder and the decoder simply share identical copies of @xmath147 as a common key at both ends , and there is no longer use for the original key , @xmath190 . by the same token , in this case , the equivocation of @xmath29 can be enhanced to the level of @xmath191_+$ ] , but not more . thus , although feedback does not increase the capacity of a dmc , it certainly improves its effectiveness when this channel serves for key delivery .      in our derivations",
    "this far , we have limited ourselves to finite alphabet sources and channels , primarily for reasons of convenience .",
    "theorem 1 extends quite straightforwardly to the continuous alphabet case as well .",
    "one comment is in order , however : in the continuous alphabet case , it no longer makes sense to measure equivocation in terms of conditional ( differential ) entropy , which can be negative .",
    "it still makes sense , nonetheless , to define it by the complementary quantity - the mutual information , @xmath192 , which is always non  negative .",
    "thus , part ( a ) of theorem 1 would be restated to assert that @xmath193_+$ ] is an achievable lower bound to @xmath194 ."
  ],
  "abstract_text": [
    "<S> we consider the shannon cipher system in a setting where the secret key is delivered to the legitimate receiver via a channel with limited capacity . for this setting </S>",
    "<S> , we characterize the achievable region in the space of three figures of merit : the security ( measured in terms of the equivocation ) , the compressibility of the cryptogram , and the distortion associated with the reconstruction of the plaintext source . </S>",
    "<S> although lossy reconstruction of the plaintext does not rule out the option that the ( noisy ) decryption key would differ , to a certain extent , from the encryption key , we show , nevertheless , that the best strategy is to strive for perfect match between the two keys , by applying reliable channel coding to the key bits , and to control the distortion solely via rate  distortion coding of the plaintext source before the encryption . in this sense , </S>",
    "<S> our result has a flavor similar to that of the classical source  channel separation theorem . </S>",
    "<S> some variations and extensions of this model are discussed as well .    </S>",
    "<S> * index terms : * shannon cipher system , key distribution , encryption , cryptography , source  channel separation .    </S>",
    "<S> department of electrical engineering + technion - israel institute of technology + haifa 32000 , israel + merhav@ee.technion.ac.il </S>"
  ]
}