{
  "article_text": [
    "analytics tasks involve preprocessing ( etl , restructuring , cleaning ) that are typically expressed using relational algebra - based languages as well as numerical tasks ( machine learning , optimization , signal processing ) that are typically expressed using linear algebra operations @xcite .",
    "the distinction between these two programming styles is increasingly blurred : machine learning applications are implemented using ra - oriented interfaces using , for example , spark , sometimes with extensions for special cases @xcite .",
    "the relevant tasks involve both programming styles , but systems tend to favor one or the other",
    ". an la - oriented system may emphasize matrix operations in the programming interface but require awkward gymnastics with column and row indices to implement even simple spj queries . a relational system , in contrast , obscures matrix properties suitable for optimization and algorithm selection .",
    "some systems allow explicit transformation of relations into matrices and vice versa , exposing a different set of programming idioms for each data type @xcite .",
    "a common programming environment where both styles can be used interchangeably is desirable , as is being explored by a number of systems and libraries , including myria @xcite , spark @xcite , and more .",
    "these systems emphasize mixed - programming syntax but assume more conventional internal computational models , many based on ra .",
    "the benefit of this approach is that conventional ra properties and rewrites are easy to exploit for optimization .",
    "the problem with this approach is that la properties and rewrites are obscured , or entirely inexpressible .",
    "we find it desirable to use theorems from both la and ra when reasoning about queries .",
    "for example , the fact that the inner product @xmath0 is symmetric suggests an immediate optimization : only produce its upper triangle rather than also computing its lower triangle redundantly .",
    "although this optimization is possible to implement ( and prove ) using ra , it is far from obvious , and no rdbms applies this optimization .",
    "we seek a new set of abstractions to facilitate the use of similar theorem - oriented optimizations .",
    "we propose a lean algebra of three operators , lara , that subsumes the operators and rules of both la and ra .",
    "we keep the number of operators to a minimum to simplify an initial implementation on a number of backend systems , and to simplify reasoning during optimization by avoiding large numbers of special cases for relatively simple concepts ( pushing selections , etc . ) . as with other systems for big data processing @xcite",
    ", lara operators are parameterized by rich user - defined functions , and the properties of these functions are involved in optimization . however , lara emphasizes a more restricted semiring structure to capture the properties of vector space algebra as opposed to emphasizing the `` free - for - all '' udf approach other systems emphasize .",
    "we also propose a physical algebra , plara , that allows reasoning about low - level optimizations such as operator fusion , elimination of unnecessary writes , and shared scans .",
    "finally we show how the physical algebra can be implemented efficiently in a distributed system using only a single efficient primitive : range scans over partitioned sorted maps .",
    "we find this primitive to be nearly universally supported across rdbms , nosql systems , linear algebra libraries , file - based systems , and others .",
    "the object of lara is the _ associative table _ , a data structure capturing core properties of relations , tensors , and key - values .",
    "the operators of lara are ext ( flatmap ) , join ( horizontal concatenation ) , and union ( vertical concatenation ) .",
    "our contributions are :    1 .   a minimal logical algebra , lara , to unify la and ra .",
    "a physical algebra over sorted partitioned maps that exposes low - level optimization opportunities .",
    "3 .   a system , laradb , implementing the lara abstractions on the apache accumulo database .",
    "an evaluation showing that on representative tasks , laradb is competitive with a hand - coded mr implementation at scale , and far faster at small scales .",
    "an evaluation of laradb on a more complex sensor validation task that demonstrates the kinds of optimizations exposed by the lara abstractions .",
    "mapreduce promoted a minimalist approach to distributed programming , but had no real capabilities for reasoning over and optimizing the resulting programs . as a result",
    ", a spate of sql - on - hadoop projects emerged in the first few years . since that early period ,",
    "a number of projects have proposed more refined approaches that balance flexibility for the programmer and optimizability by the system .",
    "fegaras et al .",
    "superimpose three operations atop mapreduce : cmap ( abbreviation of concat - map , which implements flatmap ) , groupby , and join @xcite .",
    "these operators expose optimization opportunities , but the authors do not explore the relationship between these operations and linear algebra .",
    "elgama et al proposed a variety of sum - product optimizations and operator fusion techniques for systemml called spoof by representing matrix equivalences in ra and applying relational optimizations  @xcite .",
    "this approach is similar to our own but focuses on developing a single , tightly coupled system involving heavy use of code generation , as opposed to our goal of providing a general abstraction that can be naturally implemented in many contexts .",
    "kunft et al described a vision for optimization involving both linear and relational algebra equivalences , but did not describe an implementation of the ideas  @xcite .",
    "our paper was inspired in part by this work ; we use an adaptation of their running example in our experiments .",
    "palkar et al proposed weld , a common runtime that replaces the runtime of libraries like spark , pandas , and numpy in order to optimize within and across them @xcite .",
    "weld s algebraic basis is an adaptation of map and reduce termed `` loops and builders '' .",
    "we designed lara with one step more structure by differentiating `` horizontal '' from `` vertical '' group - by , in order to obtain greater reasoning capability .",
    "marker et al built a cost - based optimizer , dxter , and applied it to the task of tensor contractions in mpi environments @xcite . though dxter lays a foundation for ra - style reasoning , they focused solely on the domain of dense la .",
    "crotty et al developed tupleware , a cluster programming environment emphasizing code generation and stateful analytics , but the emphasis is on low - level programming idioms rather than marrying logical and physical abstractions  @xcite .",
    "rheinlander et al proposed a logical optimizer for udf - centric dataflows called sofa  @xcite .",
    "sofa emphasizes properties of udfs to facilitate optimizations .",
    "our approach is complementary , but we focus on properties that allow reasoning about la , specifically semiring structures .",
    "semirings are the main focus of associative arrays , a data structure generalizing la s sparse matrices whose operations were shown to subsume ra @xcite . lara and associative arrays share design choices such as sparsity and pluggable @xmath1 and @xmath2 . ultimately , lara compromises between associative arrays ( heavily la - based ) and relations ( heavily ra - based ) .",
    "lattices were also posed as a basis for ra via two operators : natural join and inner union @xcite .",
    "the relational lattice is a special case of lara , especially w.r.t .",
    "section [ sprops ] s distributive laws , when lara is relaxed to allow tables with infinite support and restricted to only use key attributes .",
    "our hypothesis underlying lara is that the objects of ra and la  relations , scalars , vectors , matrices , and tensors  can be recast into a rectangular representation of multidimensional key - value data .",
    "ra and la operate on `` rectangular blocks '' of key - value records by either applying a function `` record - wise '' ( think selection , projection , flatmap , function application ) , by combining blocks `` horizontally '' ( think cartesian product , tensor product , relational joins ) , or by combining blocks `` vertically '' ( think relational union , aggregation , tensor contraction ) .",
    "these operations are composable ; matrix multiply , for example , is a horizontal operation followed by a vertical one . by expressing these kernels directly , rather than their particular instantiation in ra or la",
    ", we aim to reason about ra and la tasks uniformly .",
    "we call our new representation an _ associative table_.    figure [ fpa ] shows an example table consisting of environmental sensor data .",
    "it has two keys  time and measurement class  that map to measurement values .",
    "the table can be thought of as a lookup function : given a key , return the value it maps to , or the default value ( @xmath3 , in this table ) if the key has no mapping .",
    "default values allow us to model sparse and dense matrices uniformly , for example , in which case we would use numeric 0 as the default value .    [ cols=\"^,^,^\",options=\"header \" , ]     [ [ tuples - and - associative - tables ] ] tuples and associative tables + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the notation @xmath4 $ ] defines @xmath5 as a tuple of three elements named @xmath6 , @xmath7 , and @xmath8 .",
    "all tuples have names associated with their values . writing tuples",
    "@xmath9 side - by - side denotes their concatenation .",
    "we use @xmath10 to denote tuple projection onto names @xmath11 ; for example , @xmath12 $ ] .",
    "an associative table @xmath13 is a total function from key attributes @xmath14 to value attributes @xmath15 with default values @xmath16 .",
    "the _ support _ of @xmath17 is the set of keys that map to non - default values .",
    "associative tables always have finite support .",
    "the expression @xmath18 retrieves the @xmath15 associated with @xmath14 if @xmath14 is in @xmath17 s support , or default values @xmath16 if @xmath19 is not in @xmath17 s support .",
    "[ [ union ] ] union + + + + +    as the `` vertical concatenation '' of tables , the expression    union @xmath20 by @xmath21    creates an associative table over the shared key attributes of @xmath17 and @xmath22 , aggregating values that map to the same key .",
    "union takes a tuple of user - defined @xmath1 functions , one for each value attribute in @xmath17 and @xmath22 , to sum colliding values .",
    "collisions are keys from @xmath17 and @xmath22 s support that match on their common key attributes .",
    "the collisions are summed via _ structural recursion _",
    "@xcite , a strategy to sum values pair - wise until a single value is obtained . for this paper",
    "we assume each @xmath1 is associative and commutative , which implies that we can sum values in any order .",
    "in general we can relax this assumption when @xmath17 and @xmath22 s keys have a total order .",
    "we require that each @xmath1 have @xmath17 and @xmath22 s default value as its identity : @xmath23 , which forces @xmath17 and @xmath22 to have the same 0 .",
    "this requirement ensures that union has the same result independent of whether default values are stored in @xmath17 or @xmath22 ; extra default values merely add extra 0s .",
    "often a lara expression takes the union of a table @xmath17 with an empty table @xmath24 , i.e. , a table with key attributes @xmath14 and empty support .",
    "such a union aggregates @xmath17 onto the key attributes @xmath14 . for this common case",
    "we use the shorthand    agg @xmath17 on @xmath14 by @xmath25    [ [ join ] ] join + + + +    as the `` horizontal concatenation '' of tables , the expression    join @xmath20 by @xmath26    creates an associative table over the union of @xmath17 and @xmath22 s keys .",
    "join forms the cartesian product of @xmath17 and @xmath22 s values that match on their keys in common , and multiplies them .",
    "a tuple of @xmath2 functions , one for each value attribute present in both @xmath17 and @xmath22 , multiply their values .",
    "the default value of join is the multiplication of @xmath17 and @xmath22 s default values .",
    "we require that each @xmath2 have @xmath17 and @xmath22 s default values as its annihilators : @xmath27 . as with union",
    ", this requirement ensures that join is independent of whether default values are stored in @xmath17 or @xmath22 ; extra default values merely multiply extra 0s .",
    "[ [ ext ] ] ext + + +    also known as `` flatmap '' , ext is the extension '' over `` @xmath28 '' to emphasize that ext can extend a table s keys .",
    "it also indicates monadic bind that is monotonic on key types .",
    "] of a function @xmath29 on tuples to a lara operator on tables written    ext @xmath17 by @xmath29    the @xmath29 in ext is a user - defined function that returns a new associative table for every tuple . the keys in the table returned by @xmath29 append to @xmath17 s keys in the result .",
    "the values returned by @xmath29 replace @xmath17 s values in the result .",
    "we demonstrate this process with an example from our running sensor example .",
    "take the ext from figure [ fl ] line 3 .",
    "the action of this ext s @xmath29 on the second row of table @xmath17 is @xmath30 ) =   \\text { \\begin{tabular}{c|cc } $ t'$ & $ v$ & $ cnt$ \\\\ \\hline 460 & 55.2 & 1 \\end{tabular}}\\end{aligned}\\ ] ] we require that @xmath29 satisfy two properties to be used in an ext .",
    "the tables produced by @xmath29 must be valid tables with finite support , and specifically when passed default values , @xmath29 must produce a table with empty support .",
    "these requirements guarantee that the result of ext has finite support , and they offer independence from storing default values in @xmath17 ; default values do not produce support in ext s result .",
    "a common special case of ext produces no additional key attributes .",
    "we call out this behavior with the expression    map @xmath17 by @xmath29    to illustrate how map relates to ext , we show how the map in figure [ fl ] line 2 would be written as an ext : @xmath31 } { } \\\\ \\equiv\\quad & { { \\textsc{ext}}}{}\\ ; a \\;{{\\textsc{by}}}{}\\ ; \\text { \\begin{tabular}{c|c }   & $ v$   \\\\ \\hline ( ) & if ( $ 460 \\leq",
    "t \\leq 860 $ ) $ v$ else $ \\bot$ \\end{tabular}}\\end{aligned}\\ ] ]    another common special case renames keys or values .",
    "renaming is crucial for the correct application of join and union , whose semantics depend on the common and distinct names of their input s keys and values .",
    "we write rename as    rename @xmath17 from @xmath32 to @xmath33    when @xmath32 is a value attribute , renaming is straightforward ; a map function @xmath34 ) = [ y\\colon v]$ ] ( holding other value attributes constant ) performs the renaming .",
    "when @xmath32 is a key , the expression is shorthand for an ext that adds a new @xmath33 key and an agg that removes the old @xmath32 key .",
    "the union does not incur aggregation because collisions can not occur .    promoting a value to a key",
    "is a simple ext , and demoting a key to a value is an agg that may incur aggregation .",
    "[ [ formal - definitions ] ] formal definitions + + + + + + + + + + + + + + + + + +    we now present a more concise algebraic syntax for the lara operators that is useful for writing identities and proving theorems .",
    "we encourage the reader to use the cobol - style syntax when writing scripts .",
    "suppose we have associative tables @xmath17 and @xmath22 with types @xmath35 i.e. , where @xmath7 and @xmath36 are keys and values common to @xmath17 and @xmath22 , and @xmath37 and @xmath33 are keys and values unique to @xmath17 or @xmath22 . though we write these as individual attributes , the definitions hold when these are tuples ( e.g. @xmath38 ) . in the case of @xmath2 and join ,",
    "the common value attribute @xmath36 is allowed to differ in type and default value between @xmath17 and @xmath22 .",
    "we omit this case to maintain clarity .",
    "suppose we have the user - defined functions @xmath39 in the case of @xmath1 , the definition holds when there is a different @xmath1 for each attribute @xmath40 .",
    "however , we only write the case when all the @xmath1 are the same for clarity .",
    "we require that @xmath41 and @xmath29 obey the equations @xmath42 we also have a technical consistency requirement that @xmath29 produce tables of the same schema ( attribute names ) @xmath43 .",
    "given the above , we define the lara operators as @xmath44 [ ] } { } b : a , c , b \\to z : 0^z \\otimes 0^{z } \\\\ ( & a { \\join[\\otimes ] [ ] } { } b)(a , c , b ) : = [ z\\colon \\pi_z a(a , c ) \\otimes \\pi_z b(c , b ) ] \\\\ & a { \\union[\\oplus ] [ ] } { } b : c \\to x , z , y :   0^x , 0^z , 0^y \\\\ ( & a { \\union[\\oplus ] [ ] } { } b)(c ) : =       \\big[x\\colon \\bigoplus_a \\pi_x a(a , c ) , \\\\   & \\qquad    z\\colon \\bigoplus_a \\pi_z a(a , c ) \\oplus \\bigoplus_b \\pi_z b(c , y),\\ ,      y\\colon \\bigoplus_b \\pi_x b(c , y ) \\big ] \\\\ & \\operatorname{ext}_f a : a , c , k ' \\to v ' : 0 ' \\\\ ( & \\operatorname{ext}_f a)(a , c , k ' ) : = f(a , c , a(a , c))(k')\\end{aligned}\\ ] ] the `` big @xmath45 '' denotes summation over all values of key attribute @xmath46 ; the sum is always finite since we sum over associative tables which have finite support . the ext definition can be seen as un - currying the function given by @xmath47 .",
    "we use one shorthand notation .",
    "when taking a union with an empty table ( one with no support ) , as in the previous agg syntax , we adopt a unary version of union written @xmath48[x ] } a$ ] , where @xmath32 are the key attributes of the empty table .",
    "we also use @xmath49 for cases of @xmath50 that add no new keys .",
    "[ [ lifted - properties ] ] lifted properties + + + + + + + + + + + + + + + + +    some properties from the user - defined functions @xmath1 and @xmath2 automatically apply to union and join . if @xmath1 or @xmath2 are associative , commutative , or idempotent , then so are @xmath48[]}$ ] or @xmath51[]}$ ] respectfully .",
    "these follow directly from the definitions .",
    "[ [ distributive - laws ] ] distributive laws + + + + + + + + + + + + + + + + +    first we examine the conditions for distributing join over union .",
    "if @xmath2 distributes over @xmath1 such that @xmath52 , then the same law applies to distribute @xmath51[]}$ ] over @xmath48[]}$ ] such that @xmath53[]}\\ , ( b { \\union[\\oplus][]}c ) = ( a { \\join[\\otimes][]}b ) { \\union[\\oplus][]}\\ , ( a { \\join[\\otimes][]}c)$ ] under two conditions : @xmath17 and @xmath22 must have no keys in common not also present in @xmath54 , and @xmath17 and @xmath54 must have no keys in common not also present in @xmath22 . put more simply , the distributive law requires @xmath55 , where @xmath56 is symmetric difference .",
    "next we examine how to push union through join .",
    "the following result follows from the generalized distributive law @xcite . assuming that @xmath2 distributes over @xmath1 , @xmath57[]}\\ , & b )",
    "{ \\union[\\oplus][]}c = \\\\ & ( ( { \\union[\\oplus][k_b \\cup k_c ] } a ) { \\join[\\otimes][]}\\ , ( { \\union[\\oplus][k_a \\cup k_c ] } b ) ) { \\union[\\oplus][]}\\ , ( { \\union[\\oplus][k_a \\cup k_b ] } c)\\end{aligned}\\ ] ]    we refer the reader to a previous technical report for proof of the above two laws @xcite .    [ [ matrix - equations ] ] matrix equations + + + + + + + + + + + + + + + +    to illustrate the ability to reason about non - trivial equivalences in lara , consider the rotation invariance of matrix multiplication inside a matrix trace : @xmath58 .",
    "the trace of a matrix , @xmath59 , sums its diagonal entries .",
    "we prove the equation s lara analogue on tables @xmath60 to reduce notation , we use subscript @xmath61 in place of @xmath62 .",
    "@xmath63[]}\\operatorname{ext}_{i = l } ( a_{ij}b_{jk}c_{kl } ) & \\text{$\\operatorname{tr}$ defn.}\\\\ = \\ ; & { \\union[+][]}\\operatorname{ext}_{i = l } { \\union[+][i , l ] } ( { \\union[+][i , k]}\\ , ( a_{ij } { \\join[\\otimes][]}b_{jk } ) { \\join[\\otimes][]}c_{kl } )       & \\text{$abc$ defn . } \\\\",
    "= \\ ; & { \\union[+][]}{\\union[+][i , l ] } \\operatorname{ext}_{i = l } ( { \\union[+][i , k]}\\ , ( a_{ij } { \\join[\\otimes][]}b_{jk } ) { \\join[\\otimes][]}c_{kl } )       & \\text{push $ \\operatorname{ext}$ into $ \\union$ } \\\\ = \\ ; & { \\union[+][]}\\operatorname{ext}_{i = l } ( { \\union[+][i , k]}\\ , ( a_{ij } { \\join[\\otimes][]}b_{jk } ) { \\join[\\otimes][]}c_{kl } )       & \\text{combine $ \\union$ } \\\\ = \\ ; & { \\union[+][]}\\ , ( { \\union[+][i , k]}\\ , ( a_{ij } { \\join[\\otimes][]}b_{jk } ) { \\join[\\otimes][]}c_{ki } )       & \\text{apply ext } \\\\ = \\ ; & { \\union[+][]}{\\union[+][i , k]}\\ , ( a_{ij } { \\join[\\otimes][]}b_{jk } { \\join[\\otimes][]}c_{ki } )       & \\text{distr .",
    "$ \\join\\,$ into $ \\union$ } \\\\",
    "= \\ ; & { \\union[+][]}\\ , ( a_{ij } { \\join[\\otimes][]}b_{jk } { \\join[\\otimes][]}c_{ki } )       & \\text{combine $ \\union$ } \\\\",
    "= \\ ; & { \\union[+][]}\\ , ( b_{jk } { \\join[\\otimes][]}c_{ki } { \\join[\\otimes][]}a_{ij } )       & \\text{commute $ { \\join[\\otimes][]}$ } \\\\",
    "= \\ ; & \\dots \\textit { // reversing the above steps } \\\\",
    "= \\ ; & \\operatorname{tr}(bca ) \\end{aligned}\\ ] ]    due to space considerations , we do not include additional proofs of this form , but we have also sketched proofs of all of the simple rules considered in the context of systemml @xcite , including @xmath64 , @xmath65 , @xmath66 , and others .",
    "figure [ tlaralara ] summarizes how each ra and la operator can be written as a lara expression .",
    "first we examine ra in figure [ tlaralara ] .",
    "selection ( @xmath67 ) by a predicate @xmath68 is a map that sends tuples failing @xmath68 to the default value .",
    "projecting away value attributes ( @xmath69 ) is also a map ; projecting away keys is treated as an aggregation .",
    "aggregation ( @xmath70 ) and relational union ( @xmath71 ) are both instances of lara union .",
    "relational natural join ( @xmath72 ) and cartesian product ( @xmath73 ) are lara joins after ensuring that the join attributes are keys that match in name .",
    "general @xmath74-joins can be modeled via @xmath75 .",
    "second we examine la in figure [ tlaralara ] .",
    "we chose representative operations for la based on the emerging graphblas standard @xcite .",
    "matrix multiply ( @xmath76 ) is a lara join and union , after ensuring that the correct dimension of the two matrices match in name .",
    "element - wise multiply ( @xmath2 ) and addition ( @xmath1 ) are a join and union .",
    "matrix reduction is a union .",
    "matrix sub - referencing by sets of indices ( @xmath77 ) is a join of @xmath17 with each set @xmath78 and @xmath79 , treating the sets as indicator vectors with value 1 for each present position and default 0 otherwise .",
    "function application ( @xmath80 ) is a map .",
    "transpose ( @xmath81 ) is a rename .",
    "the matrix sub - reference translation highlights an interesting property : joining a matrix to a vector @xmath53[]}v$ ] _ expands _ @xmath8 to the shape of @xmath17 and multiplies them together .",
    "dually , the union @xmath82[]}v$ ] _ reduces _ @xmath17 to the shape of @xmath8 and sums them together . in la one must manually adjust shapes before these operations .",
    "lara adjusts them automatically .",
    "ra and la have more advanced operators we do not cover here , including outer join , difference , division , pivot , masks , and convolution .",
    "these too are expressible in lara @xcite .",
    "in this section we extend lara to a physical algebra atop an abstraction of _ partitioned sorted maps_. these maps are a model for many implementations , including matrix systems ( e.g. , those that support csr , csc , and dcsc @xcite storage ) , relational systems ( e.g. , row and column stores ) , and nosql systems ( e.g. , bigtable - style @xcite key - value databases ) .",
    "we call the new physical algebra plara",
    ". we derive it from lara in three steps .",
    "first , we augment the associative table by imposing an order on their key attributes called an _ access path_. second , we extend the three lara operators with semantics for associative tables with access paths .",
    "third , we add the operators load and * sort*. after defining plara , we show how plara admits a number of ra and la - style optimization opportunities in the context of the sensor example , and we describe an implementation of plara on the accumulo database .",
    "a _ sorted associative table _ is an associative table with an order imposed on its key attributes .",
    "we refer to the ordering as an _",
    "access path_. for example , the access path of table @xmath17 in figure [ fpa ] is @xmath83 $ ] . its type is written @xmath84 \\to v : \\bot$ ] .",
    "we model the map s backing store as a row - wise layout of @xmath17 s tuples sorted by access path and partitioned into segments that can be independently processed or stored .",
    "`` split points '' that delineate partitions are chosen by the implementation , usually in as equal sizes as possible to avoid skew .",
    "the above scheme performs horizontal partitioning .",
    "vertical partitioning can be achieved by separate associative tables ; storing @xmath85 value attributes separately is equivalent to manipulating @xmath86 , where each @xmath87 is a one - attribute table .",
    "more sophisticated 2-d and higher schemes could be designed but fall outside this paper s scope .",
    "[ [ sort - on - write ] ] sort - on - write + + + + + + + + + + + + +    writing out an associative table according to an access path sorts and partitions its data as a side effect .",
    "this mechanism is natural for many database implementations , where inserts automatically sort and partition on a clustered index ( sql ) or by keys ( nosql ) .",
    "a chief goal of an optimizer is to minimize the number of sort / write operations .",
    "we typeset    * sort * @xmath17 to @xmath88 $ ]    in bold to highlight the performance impact of re - sorting .    1",
    "@l@ll@     + @xmath17 = load ` s1 ' & @xmath89 $ ] & ( e ) encode numeric attributes in packed byte form + @xmath902 @xmath91 = map @xmath17 by [ @xmath8 : if(@xmath92 ) @xmath8 else @xmath3 ] & @xmath89 $ ] & ( f ) push filter into load ` s1 ' from 460 to 860 + @xmath903 @xmath94 = ext @xmath91 by [ @xmath95 : bin(@xmath96 : @xmath97 : @xmath98 & @xmath99 $ ] & + @xmath100 @xmath101 = * sort * @xmath94 to @xmath102 $ ] & @xmath102 $ ] & + @xmath103 @xmath104 = mergeagg @xmath101 on @xmath105 by@xmath106 [ @xmath8 : + , @xmath107 : + ] & @xmath108 $ ] & + @xmath109 @xmath110 = map @xmath104 by [ @xmath8 : @xmath111 & @xmath108 $ ] & + @xmath112 @xmath113 _ // repeat above for second sensor _ & @xmath108 $ ] + @xmath114 @xmath11 = mergejoin @xmath115 by [ @xmath8 : @xmath116 & @xmath108 $ ] & ( p ) propagate @xmath20 s partition splits throughout + @xmath117 @xmath118 = map @xmath11 by [ @xmath8 : @xmath98 & @xmath108 $ ] + @xmath119 @xmath120 = mergeagg @xmath118 on @xmath95 by [ @xmath8 : any ] & @xmath121 $ ] + @xmath122 @xmath123 = agg @xmath120 by [ @xmath8 : + ] & @xmath124 $ ] & ( c ) store scalar @xmath123 at client instead of a table + @xmath125 @xmath126 = * sort * @xmath11 to @xmath127 $ ] & @xmath127 $ ] & ( z ) if @xmath128 relaxed to _ sparse _ matrix interpretation , + @xmath129 @xmath130 = map @xmath126 by [ @xmath8 : @xmath97 : @xmath98 & @xmath127 $ ] & identify @xmath3 with @xmath131 , discarding 0-valued entries + @xmath132 @xmath133 = mergeagg @xmath130 on @xmath7 by [ @xmath8 : + , @xmath107 : + ] & @xmath134 $ ] & in @xmath130 and all following tables + @xmath135 @xmath136 = map @xmath133 by [ @xmath8 : @xmath137 ) ] & @xmath134 $ ] & ( d ) defer @xmath138 to future scans on @xmath126 , + @xmath139 store @xmath136 & @xmath134 $ ] & eliminating write - out of @xmath136 + @xmath140 @xmath141 = mergejoin @xmath142 by [ @xmath8 : @xmath116 & @xmath127 $ ] & ( r ) reuse @xmath126 data source ( common sub - expression ) + @xmath143 @xmath144 = * sort * @xmath141 to @xmath108 $ ] & @xmath108 $ ] & ( @xmath145 has a similar sub - expression below ) + @xmath146 @xmath147 = rename @xmath144 from @xmath7 to @xmath148 & @xmath149 $ ] & ( s ) @xmath0 is symmetric ; only compute upper triangle + @xmath150 @xmath145 = mergejoin @xmath151 by [ @xmath8 : @xmath73 ] & @xmath153 $ ] & via map filter @xmath154 + @xmath155 @xmath156 = * sort * @xmath145 to @xmath157 $ ] & @xmath157 $ ] & ( a ) push sum of partial products into @xmath156 compaction and + @xmath158 @xmath159 = mergeagg @xmath156 on @xmath160 by [ @xmath8 : + ] & @xmath161 $ ] & flush ; assume no repeated writes due to server failure + @xmath162 @xmath54 = mergejoin @xmath163 by [ @xmath8 : @xmath164 & @xmath161 $ ] & ( d ) defer @xmath165 to future scans on @xmath156 , + @xmath166 store @xmath54 & @xmath161 $ ] & eliminating final pass    [ [ sorted - join - union - ext ] ] sorted join , union , ext + + + + + + + + + + + + + + + + + + + + + + +    we assume a single primitive for reading data at the physical level : an efficient _ range scan _ over the keys of a partitioned sorted map .",
    "range scans are often implemented as _ range iterators _ that execute user - defined code , including filters , transforms , and aggregations , on streams of data .",
    "in previous work we have shown how to re - purpose range iterators , normally designed for single - table parallel scans , to multi - table computation @xcite .",
    "this approach enables us to implement the lara operators inside range iterators .",
    "join and union take the form of _ merge - scans _ : range scans on one table that themselves scan matching entries from another .",
    "processing tables in this way is efficient when both tables are sorted on the attributes to be merged ; if not , one must re - sort the input tables prior to the merge - scan .",
    "specifically we implement join , union , and agg as    mergejoin @xmath20 by @xmath26 + mergeunion @xmath20 by @xmath21 + mergeagg @xmath17 on @xmath88 $ ] by @xmath21    ext @xmath17 by @xmath29 maintains the same syntax in plara .",
    "the access path of each operation is as follows .",
    "assume @xmath17 has access path @xmath167 $ ] , and @xmath22 has access path @xmath168 $ ] .",
    "mergejoin has access path @xmath169 $ ] .",
    "mergeunion has access path @xmath134 $ ] .",
    "mergeagg has access path @xmath88 $ ] .",
    "if @xmath29 produces tables sorted on @xmath170 $ ] , then ext @xmath17 by @xmath29 has access path @xmath171 $ ] .",
    "the behavior of the these operators is as follows .",
    "mergejoin @xmath20 takes the cartesian product of tuples that match on their common keys . for each match",
    ", it streams through @xmath22 s matching tuples while holding @xmath17 s matching tuples in memory , and it applies an @xmath2 function to each pair .",
    "mergeunion aggregates tuples by an @xmath1 for each common key .",
    "the execution of mergeunion depends on the properties of the @xmath1 function . at a minimum",
    ", @xmath1 must have an identity 0 matching the default values of its input tables , or else correctness is not guaranteed .",
    "a basic execution strategy folds @xmath1 across matching tuples in order on a single partition .",
    "if @xmath1 is associative , then @xmath1 may run across multiple partitions in parallel , computing local sums before combining them into a global sum during the next * sort * ( see optimization ( a ) in the next section ) .",
    "if @xmath1 is idempotent , then @xmath1 can run more than once on the same tuples , which is helpful for guaranteeing correctness when recovering from server failure .",
    "if @xmath1 is commutative , then @xmath1 can run out of order .",
    "ext @xmath17 by @xmath29 applies @xmath29 to each tuple , producing a nested table for each tuple which is immediately flattened .",
    "map is similar to ext , but never needs to flatten .",
    "load initiates a range scan on an existing table , possibly restricted to a sub - range .",
    "its access path is given by a database catalog .",
    "we also include a store operator , implemented as a * sort * that does not change the access path .    [ [ physical - plan - for - sensor - example ] ] physical plan for sensor example + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    figure [ fp ] presents a line - by - line translation of the lara logical plan from figure [ fl ] into a plara physical plan . the bulk of the translation is tracking the access path induced by each lara operator and inserting a * sort * where access path requirements are unmet .",
    "this occurs for table @xmath104 , which aggregates on @xmath95 and @xmath7 but follows an ext with access path @xmath99 $ ] ; table @xmath133 , which aggregates on @xmath7 but stems from @xmath11 with access path @xmath108 $ ] ; table @xmath145 , which joins on @xmath95 but stems from @xmath141 with access path @xmath127 $ ] ; and table @xmath159 , which aggregates on @xmath7 and @xmath148 but stems from @xmath145 with path @xmath153 $ ] .",
    "+    figure [ fpopt ] illustrates a few optimization rules on the plara algebra ; figure [ fp ] pinpoints where these and other rules apply to our running sensor example .",
    "we evaluate the impact of these optimizations in section [ sexpexpr ] .",
    "some of the most important optimizations act on * sort * operations . rule ( a ) pushes aggregations that run after a * sort * into the * sort * itself . we call the fused operation * sortagg*. speedup from fusing aggregation into sorting can be dramatic , since the implementation can compute partial sums which reduces the burden of sorting and storage .    rule ( m ) eliminates * sort*s after an ext when they are unnecessary .",
    "normally additional key attributes produced by an ext append to the end of its input s access path . moving new attributes up in the access path past existing key attributes requires re - sorting .",
    "if @xmath29 is _ monotone _ with respect to existing key attributes , however , the new key attributes may be promoted past those existing ones without sorting . here , monotone means that @xmath172 , using @xmath173 to refer to the keys of the tables produced by @xmath29 .",
    "rule ( f ) pushes filter operations into the load statements that start a range scan .",
    "these filter operations restrict data to a range of keys on a prefix of the loaded table s access path .",
    "the result restricts scanned data to the desired range , as opposed to naively scanning all data and post - filtering .",
    "rules ( z - sort ) , ( z - map ) , ( z - agg ) , and ( z - join ) push `` discarding zeros '' through a lara expression .",
    "these rules generalize to ext and union ; in fact , they apply at the logical lara level , but we list them here since they are usually associated with physical storage . the null - to - zero function ",
    "ntz(@xmath8 ) = if ( @xmath174 ) 0 else @xmath8changes @xmath8 s default value from @xmath3 to 0 .",
    "the change allows implementations to discard zeros without fear of incorrectness .    in order to apply the ( z )",
    "rewrites , the inference rules check that the function to push ntz through treats @xmath3 and 0 `` the same '' . for example , read the first as `` if we ext @xmath17 by @xmath29 and afterward discard zeros , and it holds that @xmath175 and @xmath176 , then we can safely discard zeros before the ext '' .",
    "we now discuss a few rules not listed in figure [ fpopt ] .",
    "rule ( s ) leverages symmetry of the inner product computed in lines 1517 : @xmath177 .",
    "lara expresses this identity as a rename .",
    "if @xmath54 is relaxed to restrict its output to its upper triangle , then ( s ) could push the filter `` map @xmath54 by @xmath154 '' up through the plan to the point immediately after line 16 by means of rules in the same style as the ( z ) rules .",
    "rule ( d ) defers the last pass before a store to scans on the last materialized table , i.e. , the last * sort * result .",
    "this rule partitions the plan into parts computed `` eagerly '' and parts computed `` lazily '' .",
    "the performance impact to future scans of deferring the last computation is usually minimal , since * sort*s are never deferred and so the deferred operations can be streamed . in figure [ fp ] ,",
    "lines 1113 defer to scans on @xmath126 , and lines 1718 defer to scans on @xmath156 .",
    "rule ( e ) encodes numbers in a packed byte format . like ( z ) and ( s ) , ( e ) involves a change in the output that , if allowed , can be pushed through the computation , in this case all the way to the original data sources @xmath17 and @xmath22 .    rule ( r )",
    "is a form of common sub - expression elimination . in the accumulo implementation ,",
    "( r ) entails re - using a single range iterator to serve two separate data streams .",
    "rule ( p ) acts on the table splits that partition data .",
    "it pre - splits new tables using the splits of existing tables .",
    "pre - splitting tables improves insert performance by increasing parallelism before the implementation splits data on its own .",
    "additional optimizations are possible . for instance",
    ", we might forgo sorting in favor of hash - shuffling when correct to do so , just as tenzing employed @xcite .",
    "we implemented plara on the architecture of google s bigtable @xcite , a design that closely resembles plara s sorted partitioned map abstraction .",
    "operations in the bigtable architecture consist of inserts and range scans . during scans",
    ", the user can execute arbitrary code in the form of _ iterators _ that run server - side as data streams from each partition in parallel .",
    "iterator code can even initiate scans on or write entries to additional tables , a fact we previously exploited in the graphulo matrix math library @xcite .",
    "bigtable s range iterators suffice to implement lara .",
    "in particular , we implemented plara on apache accumulo , an open - source adaptation of bigtable s design .",
    "however , we emphasize that our implementation applies just as much to other bigtable systems , including hypertable and apache hbase , and that we see no fundamental barriers to implementing lara atop other systems with some concept of key and value , including relational and matrix systems .",
    "even nested relational systems for json - like data fit into lara , either by flattening or new indexing techniques @xcite .    for this prototype implementation , we chose a simple model that stores the first key , subsequent keys , and values in the accumulo row , column qualifier , and value , respectively .",
    "keys are stored ( and sorted ) according to the table s access path .",
    "we coded ext , mergejoin , and mergeunion as iterator fragments linked by the graphulo library .",
    "in this section we conduct an experiment with two goals : to assess lara s ability to express a complex computation with elements of both ra and la , and to measure the impact of optimizations that lara affords on this computation .",
    "we implemented each optimization manually ; building an optimizer that applies them automatically is future work .",
    "the experimental task is the sensor quality control plan detailed in figure [ fp ] .",
    "we obtained 1.5 months data from two `` array of things '' sensors @xcite managed by argonne national laboratory . the raw data amounts to 1.2 gb ; however , this reduces to about 60 mb after parsing , projecting , and storing the data in accumulo s default compressed format .",
    "we partitioned each sensor s data into 3 day segments . the plan s filter step restricts analysis to a 30 day period .",
    "we experimented on an amazon ec2 ` m3.large ` cluster of 4 workers , 3 coordinators , and 1 monitor machine .",
    "each has 7.5 gb memory , 2 vcpus , and a 30 gb ssd drive .",
    "figure [ fpopt ] plots sensor task runtime with different optimizations enabled . at the left we plot the baseline , no optimizations , at 1230 seconds .",
    "we then plot each optimization individually as well as the combined effect of all optimizations .",
    "most of the runtime is spent calculating the covariance @xmath54 .",
    "this matches our expectations because computing the inner product @xmath0 generates a large number of partial products .",
    "for this reason , optimization ( a ) yields the greatest performance increase , since it drastically increases the efficiency of summing partial products . without ( a ) , all partial products must be materialized before they can be summed .",
    "optimizations ( d ) and ( s ) both affect the @xmath54 calculation and deliver the next best performance improvement .",
    "( s ) eliminates half the computation to compute @xmath54 , and ( d ) defers finishing the summation to future scans",
    ".    other optimizations proved effective but had less impact since they applied less to the covariance bottleneck .",
    "the impact of ( z ) depends on the number of zero - valued entries materialized during the @xmath141 and @xmath54 computation .",
    "( p ) increased parallelism in each step , somewhat reducing worker skew .",
    "( f ) sped up the first phase by 4x , decreasing its runtime from 87 to 22 seconds .",
    "( e ) and ( m ) had smaller effects .",
    "we conclude that lara and plara are sufficient to express the sensor quality control computation , as well as several optimizations useful for impacting performance .",
    "in this section we conduct an experiment to test whether laradb competes in performance with the analytics engine natively integrated with accumulo : mapreduce .",
    "the task we run is matrix multiplication ( mxm ) . in terms of ra , mxm consists of a join followed by an aggregation . in terms of la , many other la kernels can be simulated by mxm .",
    "for example , matrix reduction can be realized as multiplication by a vector of 1s , and matrix subset can be realized as multiplication on the left by a diagonal matrix that selects rows and on the right by a diagonal matrix that selects columns .",
    "composition of these kernels lead to more complex graph algorithms such as triangle enumeration @xcite , vertex similarity , k - truss , and matrix factorization @xcite .",
    "because our goal is to compare the performance of the laradb and the mapreduce execution engines , rather than the difference between two mxm algorithms , we wrote the laradb and mapreduce code implementing mxm as similarly as possible .",
    "both read inputs from and write outputs to accumulo tables .",
    "both implement the the mxm @xmath178 outer product algorithm @xcite on pre - indexed data with @xmath17 sorted column - major and @xmath22 sorted row - major .",
    "both have optimizations ( a ) and ( d ) from section [ spopt ] enabled .    the main operational difference between the laradb and mapreduce execution is that laradb executes inside accumulo s range scan iterators while mapreduce executes as external processes managed by the yarn scheduler . specifically , mapreduce performs a reduce - side join @xcite .",
    "we generated test data via the graph500 unpermuted power law graph generator @xcite .",
    "we chose the generator because power law distributions well model properties of real world data such as skew @xcite .",
    "generated matrices range from @xmath179 rows ( scale 10 ) to @xmath180 rows ( scale 19 ) , each with roughly 16 nonzero entries per row . multiplying",
    "the largest matrices formed close to @xmath181 ( @xmath182 ) partial products .",
    "we used the same amazon ec2 experiment environment as section [ sexpexpr ] , except with 8 workers instead of 4 .",
    "each worker allocated 3 gb of memory to yarn and 3 gb to accumulo .",
    "the 8-worker environment is well - suited to gauging inter - node parallelism ; intra - node parallelism , however , was limited by the small number of vcpus ( 2 ) per machine .",
    "figure  [ ftime1 ] plots mxm runtime as problem size increases .",
    "graphulo dominates mapreduce at smaller problem sizes .",
    "this is due to the large startup cost that mapreduce programs are infamous for ; the yarn scheduler takes roughly 30s to start any task as a result of job submission , container allocation , jar copying , and other cold start overheads .",
    "laradb , on the other hand , has a warm start since it runs inside the already - running accumulo tablet servers .",
    "these tablet servers have a standing thread pool ready to service scan requests as soon as they receive a remote procedure call .",
    "we conclude that laradb is much better suited to interactive and small - scale computation , such as analytics on a subset of data extracted from an accumulo table .    at larger problem sizes , laradb and mapreduce",
    "converge in performance .",
    "the convergence meets our expectations because the two libraries run similar code in a similar pattern of parallelism over the same data partitioning .",
    "their execution environment , jvms over hadoop , is also similar given sufficient time to amortize yarn s startup cost .",
    "we conclude that our laradb implementation is competitive with at least one major ra / la system at scale .",
    "we take this as initial evidence that systems built atop the lara algebra can and do have strong performance .",
    "linear algebra ( la ) and relational algebra ( ra ) are , in a sense , two sides of the same coin .",
    "we offer lara as that coin , expressive enough to subsume la and ra yet with more structure than mapreduce that in turn affords greater reasoning . lowering lara to a physical algebra",
    "brings this reasoning to the domain of partitioned sorted maps , a broad abstraction that encompasses la , ra , and key - value systems including the laradb implementation on accumulo .",
    "our experiments demonstrate that ( 1 ) lara expresses high and low - level optimizations that make a difference in the execution of real - world tasks , and ( 2 ) that the laradb implementation outperforms an existing data processing system vastly at small scale and competitively at large scale .    in the future",
    ", we aim to use lara as a conduit for studying and computationally exploiting the relationship between la and ra .",
    "a database optimizer is an ideal place to realize the benefits of this study for joint linear - relational analytics .",
    "this material is partially supported by nsf graduate research fellowship dge-1256082 .",
    "thanks to david maier , jeremy kepner , and tim mattson for their enthusiasm and comments ."
  ],
  "abstract_text": [
    "<S> analytics tasks manipulate structured data with variants of relational algebra ( ra ) and quantitative data with variants of linear algebra ( la ) . </S>",
    "<S> the two computational models have overlapping expressiveness , motivating a common programming model that affords unified reasoning and algorithm design . at the logical level we propose lara , a lean algebra of three operators , that expresses ra and la as well as relevant optimization rules . </S>",
    "<S> we show a series of proofs that position lara at just the right level of expressiveness for a middleware algebra : more explicit than mapreduce but more general than ra or la . at the physical level </S>",
    "<S> we find that the lara operators afford efficient implementations using a single primitive that is available in a variety of backend engines : range scans over partitioned sorted maps .    </S>",
    "<S> to evaluate these ideas , we implemented the lara operators as range iterators in apache accumulo , a popular implementation of google s bigtable . </S>",
    "<S> first we show how lara expresses a sensor quality control task , and we measure the performance impact of optimizations lara admits on this task . </S>",
    "<S> second we show that the laradb implementation outperforms accumulo s native mapreduce integration on a core task involving join and aggregation in the form of matrix multiply , especially at smaller scales that are typically a poor fit for scale - out approaches . </S>",
    "<S> we find that laradb offers a conceptually lean framework for optimizing mixed - abstraction analytics tasks , without giving up fast record - level updates and scans .    </S>",
    "<S> < ccs2012 > < concept > < concept_id>10002951.10002952.10003190.10003192.10003398</concept_id > </S>",
    "<S> < concept_desc > information systems  query operators</concept_desc > < concept_significance>500</concept_significance > </S>",
    "<S> < /concept > < concept > < concept_id>10002951.10002952.10003197</concept_id > </S>",
    "<S> < concept_desc > information systems  query languages</concept_desc > < concept_significance>300</concept_significance > </S>",
    "<S> < /concept > < concept > < concept_id>10002951.10002952.10003190.10003192.10003210</concept_id > </S>",
    "<S> < concept_desc > information systems  query optimization</concept_desc > < concept_significance>100</concept_significance > < </S>",
    "<S> /concept > < concept > </S>",
    "<S> < concept_id>10002951.10002952.10002953.10010819</concept_id > < concept_desc > information systems  physical data models</concept_desc > < concept_significance>100</concept_significance > < /concept > </S>",
    "<S> < /ccs2012 > </S>"
  ]
}