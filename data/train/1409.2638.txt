{
  "article_text": [
    "` big data ' often refers to a large collection of observations and the associated computational issues in processing the data .",
    "some of the new challenges from a statistical perspective include :    1 .",
    "the analysis has to be computationally efficient while retaining statistical efficiency ( * ? ? ?",
    "2 .   the data are ` dirty ' : they contain outliers , shifting distributions , unbalanced designs , to mention a few .",
    "there is also often the problem of dealing with data in real - time , which we add to the ( broadly interpreted ) first challenge of computational efficiency ( * ? ? ?",
    "* cf . ) .",
    "we believe that many large - scale data are inherently inhomogeneous : that is , they are neither i.i.d . nor stationary observations from a distribution .",
    "standard statistical models ( e.g. linear or generalized linear models for regression or classification , gaussian graphical models ) fail to capture the inhomogeneity structure in the data . by ignoring it , prediction performance can become very poor and interpretation of model parameters might be completely wrong .",
    "statistical approaches for dealing with inhomogeneous data include mixed effect models @xcite , mixture models @xcite and clusterwise regression models @xcite : while they are certainly valuable in their own right , they are typically computationally very cumbersome for large - scale data .",
    "we present here a framework and methodology which addresses the issue of inhomogeneous data while still being vastly more efficient to compute than fitting much more complicated models such as the ones mentioned above .    [",
    "[ subsampling - and - aggregation . ] ] subsampling and aggregation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    if we ignore the inhomogeneous part of the data for a moment , a simple approach to address the computational burden with large - scale data is based on ( random ) subsampling : construct groups @xmath0 with @xmath1 , where @xmath2 denotes the sample size and @xmath3 is the index set for the samples .",
    "the groups might be overlapping ( i.e. , @xmath4 for @xmath5 ) and do not necessarily cover the index space of samples @xmath3 . for every group @xmath6 , we compute an estimator ( the output of an algorithm ) @xmath7 and these estimates are then aggregated to a single `` overall '' estimate @xmath8 , which can be achieved in different ways .",
    "if we divide the data into @xmath9 groups of approximately equal size and the computational complexity of the estimator scales for @xmath2 samples like @xmath10 for some @xmath11 , then the subsampling - based approach above will typically yield a computational complexity which is a factor @xmath12 faster than computing the estimator on all data , while often just incurring an insubstantial increase in statistical error .",
    "in addition , and importantly , effective parallel distributed computing is very easy to do and such subsampling - based algorithms are well - suited for computation with large - scale data .",
    "subsampling and aggregation can thus partially address the first challenge about feasible computation but fails for the second challenge about proper estimation and inference in presence of inhomogeneous data .",
    "we will show that a tweak to the aggregation step , which we call `` maximin aggregation '' , can often deal also with the second challenge by focusing on effects that are common to all data ( and not just mere outliers or time - varying effects ) .",
    "[ [ bagging - aggregation - by - averaging . ] ] bagging : aggregation by averaging . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in the context of homogeneous data , @xcite showed good prediction performance in connection with mean or majority voting aggregation and tree algorithms for regression or classification , respectively .",
    "bagging simply averages the individual estimators or predictions .",
    "[ [ stacking - and - convex - aggregation . ] ] stacking and convex aggregation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    again in the context of homogeneous data , the following approaches have been advocated . instead of assigning a uniform weight to each individual estimator as in bagging , @xcite and @xcite proposed to learn the optimal weights by optimizing on a new set of data .",
    "convex aggregation for regression has been studied in @xcite and has been proved to lead to to approximately equally good performance as the best member of the initial ensemble of estimators .",
    "but in fact , in practice , bagging and stacking can exceed the best single estimator in the ensemble if the data are homogeneous .",
    "[ [ magging - convex - maximin - aggregation . ] ] magging : convex maximin aggregation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    with inhomogeneous data , and in contrast to data being i.i.d .  or stationary realizations from a distribution ,",
    "the above schemes can be misleading as they give all data - points equal weight and can easily be misled by strong effects which are present in only small parts of the data and absent for all other data .",
    "we show that a different type of aggregation can still lead to consistent estimation of the effects which are common in all heterogeneous data , the so - called maximin effects @xcite . the maximin aggregation , which we call magging , is very simple and general and",
    "can easily be implemented for large - scale data .",
    "we now give some more details for the various aggregation schemes in the context of linear regression models with an @xmath13 predictor ( design ) matrix @xmath14 , whose rows correspond to @xmath2 samples of the @xmath15-dimensional predictor variable , and with the @xmath2-dimensional response vector @xmath16 ; at this point , we do not assume a true p - dimensional regression parameter , see also the model in ( [ mod1 ] ) .",
    "suppose we have an ensemble of regression coefficient estimates @xmath17 , where each estimate has been obtained from the data in group @xmath6 , possibly in a computationally distributed fashion .",
    "the goal is to aggregate these estimators into a single estimator @xmath8 .      bagging @xcite simply averages the ensemble members with equal weight to get the aggregated estimator @xmath18 one could equally average the predictions @xmath19 to obtain the predictions @xmath20 .",
    "the advantage of bagging is the simplicity of the procedure , its variance reduction property @xcite , and the fact that it is not making use of the data , which allows simple evaluation of its performance .",
    "the term `` bagging '' stands for * * b**ootstrap * * agg**regat*ing * ( mean aggregation ) where the ensemble members @xmath7 are fitted on bootstrap samples of the data , that is , the groups @xmath6 are sampled with replacement from the whole data .",
    "@xcite and @xcite propose the idea of `` stacking '' estimators .",
    "the general idea is in our context as follows .",
    "let @xmath21 be the prediction of the @xmath22-th member in the ensemble .",
    "then the stacked estimator is found as    @xmath23    where the space of possible weight vectors is typically of one of the following forms : @xmath24 if the ensemble of initial estimators @xmath25 is derived from an independent dataset , the framework of stacked regression has also been analyzed in @xcite .",
    "typically , though , the groups on which the ensemble members are derived use the same underlying dataset as the aggregation .",
    "then , the predictions @xmath26 are for each sample point @xmath27 defined as being generated with @xmath28 , which is the same estimator as @xmath7 with observation @xmath29 left out of group @xmath6 ( and consequently @xmath30 if @xmath31 ) . instead of a leave - one - out procedure",
    ", one could also use other leave - out schemes , such as e.g. the out - of - bag method @xcite . to this end",
    ", we just average for a given sample over all estimators that did not use this sample point in their construction , effectively setting @xmath32 if @xmath33 .",
    "the idea of `` stacking '' is thus to find the optimal linear or convex combination of all ensemble members .",
    "the optimization is @xmath9-dimensional and is a quadratic programming problem with linear inequality constraints , which can be solved efficiently with a general - purpose quadratic programming solver .",
    "note that only the inner products @xmath34 and @xmath35 for @xmath36 are necessary for the optimization .",
    "whether stacking or simple mean averaging as in bagging provides superior performance depends on a range of factors .",
    "mean averaging , as in bagging , certainly has an advantage in terms of simplicity .",
    "both schemes are , however , questionable when the data are inhomogeneous .",
    "it is then not evident why the estimators should carry equal aggregation weight ( as in bagging ) or why the fit should be assessed by weighing each observation identically in the squared error loss sense ( as in stacked aggregation ) .",
    "we propose here * * m**aximin * * agg**regat*ing * , called magging , for heterogeneous data : the concept of maximin estimation has been proposed by @xcite , and we present a connection in section [ sec.maximin ] .",
    "the differences and similarities to mean and stacked aggregation are :    1 .",
    "the aggregation is a weighted average of the ensemble members ( as in both stacked aggregation and bagging ) .",
    "2 .   the weights are non - uniform in general ( as in stacked aggregation ) .",
    "3 .   the weights do not depend on the response @xmath37 ( as in bagging ) .",
    "the last property makes the scheme almost as simple as mean aggregation as we do not have to develop elaborate leave - out schemes for estimation ( as in e.g. stacked regression ) .",
    "magging is choosing the weights as a convex combination to minimize the @xmath38-norm of the fitted values : @xmath39 if the solution is not unique , we take the solution with lowest @xmath38-norm of the weight vector among all solutions .",
    "the optimization and computation can be implemented in a very efficient way .",
    "the estimators @xmath7 are computed in each group of data @xmath6 separately , and this task can be easily performed in parallel . in the end , the estimators only need to be combined by calculating optimal convex weights in @xmath9-dimensional space ( where typically @xmath40 and @xmath41 ) with quadratic programming ; some pseudocode in ` r ` @xcite for these convex weights is presented in the appendix .",
    "computation of magging is thus computationally often massively faster and simpler than a related direct estimation estimation scheme proposed in @xcite .",
    "furthermore , magging is very generic ( e.g. one can choose its own favored regression estimator @xmath7 for the @xmath22-th group ) and also straightforward to use in more general settings beyond linear models .",
    "the magging scheme will be motivated in the following section [ sec.maximin ] with a model for inhomogeneous data and it will be shown that it corresponds to maximizing the minimally `` explained variance '' among all data groups .",
    "the main idea is that if an effect is common across all groups @xmath42 , then we can not `` average it away '' by searching for a specific convex combination of the weights .",
    "the common effects will be present in all groups and will thus be retained even after the minimization of the aggregation scheme .",
    "the construction of the groups @xmath42 for magging in presence of inhomogeneous data is rather specific and described in section [ subsec.genunknown ] for various scenarios .",
    "there , examples 1 and 2 represent the setting where the data within each group is ( approximately ) homogeneous , whereas example 3 is a case with randomly subsampled groups , despite the fact of inhomogeneity in the data .",
    "we motivate in the following why magging ( maximin aggregation ) can be useful for inhomogeneous data when the interest is on effects that are present in all groups of data .    in the linear model setting , we consider the framework of a mixture model @xmath43 where @xmath44 is a univariate response variable , @xmath45 is a @xmath15-dimensional covariable , @xmath46 is a @xmath15-dimensional regression parameter , and @xmath47 is a stochastic noise term with mean zero and which is independent of the ( fixed or random ) covariable .",
    "every sample point @xmath29 is allowed to have its own and different regression parameter : hence , the inhomogeneity occurs because of changing parameter vectors , and we have a mixture model where , in principle , every sample arises from a different mixture component .",
    "the model in ( [ mod1 ] ) is often too general : we make the assumption that the regression parameters @xmath48 are realizations from a distribution @xmath49 : @xmath50 where the @xmath46 s do not need to be independent of each other .",
    "however , we assume that the @xmath46 s are independent from the @xmath45 s and @xmath47 s .",
    "_ example 1 : known groups .",
    "_ consider the case where there are known groups @xmath6 with @xmath51 for all @xmath52 .",
    "thus , this is a clusterwise regression problem ( with _ known _ clusters ) where every group @xmath6 has the same ( unknown ) regression parameter vector @xmath53 .",
    "we note that the groups @xmath54 are the ones for constructing the magging estimator described in the previous section .",
    "_ example 2 : smoothness structure .",
    "_ consider the situation where there is a smoothly changing behavior of the @xmath46 s with respect to the sample indices @xmath29 : this can be achieved by positive correlation among the @xmath46 s . in practice",
    ", the sample index often corresponds to time .",
    "there are no true ( unknown ) groups in this setting .    _ example 3 : unknown groups .",
    "_ this is the same setting as in example 1 but the groups @xmath6 are unknown . from an estimation point of view , there is a substantial difference to example  1 @xcite .      in model ( [ mod1 ] ) and in the examples",
    "13 mentioned above , we have a `` multitude '' of regression parameters .",
    "we aim for a single @xmath15-dimensional parameter , which contains the common components among all @xmath46 s ( and essentially sets the non - common components to the value zero ) .",
    "this can be done by the idea of so - called maximin effects which we explain next .    consider a linear model with the fixed @xmath15-dimensional regression parameter @xmath55 which can take values in the support of @xmath49 from ( [ mod2 ] ) : @xmath56 where @xmath45 and @xmath47 are as in ( [ mod1 ] ) and assumed to be i.i.d .",
    "we will connect the random variables @xmath46 in ( [ mod1 ] ) to the values @xmath55 via a worst - case analysis as described below : for that purpose , the parameter @xmath55 is assumed to not depend on the sample index @xmath29 .",
    "the variance which is explained by choosing a parameter vector @xmath57 in the linear model ( [ mod.b ] ) is @xmath58 where @xmath59 denotes the covariance matrix of @xmath14 .",
    "we aim for maximizing the explained variance in the worst ( most adversarial ) scenario : this is the definition of the maximin effects .",
    "_ definition @xcite . _",
    "the maximin effects parameter is @xmath60 and note that the definition uses the negative explained variance @xmath61 .",
    "the maximin effects can be interpreted as an aggregation among the support points of @xmath49 to a single parameter vector , i.e. , among all the @xmath46 s ( e.g. in example  2 ) or among all the clustered values @xmath53 ( e.g. in examples 1 and 3 ) , see also fact [ fact1 ] below .",
    "the maximin effects parameter is different from the pooled effects @xmath62 $ ] and a bit surprisingly , also rather different from the prediction analogue @xmath63 . \\end{aligned}\\ ] ] in particular , the value zero has a special status for the maximin effects parameter @xmath64 , unlike for @xmath65 or @xmath66 , see @xcite .",
    "the following is an important `` geometric '' characterization which indicates the special status of the value zero , see also figure [ fig.charact ] .",
    "[ fact1]@xcite let @xmath67 be the convex hull of the support of @xmath49",
    ". then @xmath68 that is , the maximin effects parameter @xmath64 is the point in the convex hull @xmath67 which is closest to zero with respect to the distance @xmath69 : in particular , if the value zero is in @xmath67 , the maximin effects parameter equals @xmath70 .     in dimension @xmath71 .",
    "]    the characterization in fact [ fact1 ] leads to an interesting robustness issue which we will discuss below in section [ subsec.robustness ] .",
    "the connection to magging ( maximin aggregation ) can be made most easily for the setting of example 1 with known groups and constant regression parameter @xmath53 within each group @xmath6 .",
    "we can rewrite , using fact [ fact1 ] : @xmath72 where @xmath73 is as in ( [ eq : maximin ] ) .",
    "the magging estimator is then using the plug - in principle with estimates @xmath7 for @xmath53 and @xmath74 for @xmath75 .      .",
    "left panel : the values @xmath76 are possible realizations of @xmath46 , and @xmath64 is the closest point to zero in the convex hull of @xmath77 ( in black ) .",
    "when adding a new additional realization @xmath78 , the convex hull becomes larger ( in dashed blue ) .",
    "as long as the new support point is in the blue shaded half - space , the maximin effects parameter @xmath64 remains the same _ regardless _ of how far away the new support point is added .",
    "right panel : a new additional realization @xmath78 arises which does not lie in the blue shaded half - space , the convex hull becomes larger ( in dashed blue ) and the new maximin effects parameter becomes @xmath79 . since the new convex hull ( in dashed blue ) gets enlarged by a new realized value @xmath78 , the corresponding new maximin effects parameter @xmath79 must be closer to the origin than the original parameter @xmath64 .",
    "thus , it is impossible to shift @xmath64 away from zero by placing new realizations at arbitrary positions.,title=\"fig : \" ] .",
    "left panel : the values @xmath76 are possible realizations of @xmath46 , and @xmath64 is the closest point to zero in the convex hull of @xmath77 ( in black ) .",
    "when adding a new additional realization @xmath78 , the convex hull becomes larger ( in dashed blue ) .",
    "as long as the new support point is in the blue shaded half - space , the maximin effects parameter @xmath64 remains the same _ regardless _ of how far away the new support point is added .",
    "right panel : a new additional realization @xmath78 arises which does not lie in the blue shaded half - space , the convex hull becomes larger ( in dashed blue ) and the new maximin effects parameter becomes @xmath79 .",
    "since the new convex hull ( in dashed blue ) gets enlarged by a new realized value @xmath78 , the corresponding new maximin effects parameter @xmath79 must be closer to the origin than the original parameter @xmath64 .",
    "thus , it is impossible to shift @xmath64 away from zero by placing new realizations at arbitrary positions.,title=\"fig : \" ]    it is instructive to see how the maximin effects parameter is changing if the support of @xmath49 is extended , possibly rendering the support non - finite .",
    "there are two possibilities , illustrated by figure  [ fig.outlier1 ] . in the first case , illustrated in the left panel of figure  [ fig.outlier1 ]",
    ", the new parameter vector @xmath78 is not changing the point in the convex hull of the support of @xmath49 that is closest to the origin .",
    "the maximin effects parameter is then unchanged .",
    "the second situation is illustrated in the right panel of figure  [ fig.outlier1 ] .",
    "the addition of a new support point here does change the convex hull of the support such that there is now a point in the support closer to the origin . consequently",
    ", the maximin effects parameter will shift to this new value .",
    "the maximin effects parameter thus is either unchanged or is moving closer to the origin .",
    "therefore , maximin effects parameters and their estimation exhibit an excellent robustness feature with respect to breakdown properties",
    ".      we will derive now some statistical properties of magging , the maximin aggregation scheme , proposed in ( [ eq : maximin ] ) .",
    "they depend also on the setting - specific construction of the groups @xmath80 which is described in section [ subsec.genunknown ] .",
    "[ [ assumptions . ] ] assumptions .",
    "+ + + + + + + + + + + +    consider the model ( [ mod1 ] ) and that there are @xmath9 groups @xmath81 of data samples .",
    "denote by @xmath82 and @xmath83 the data values corresponding to group @xmath6 .",
    "( a1 ) : :    let @xmath84 be the optimal regression vector in each    group , that is    @xmath85 $ ] .",
    "assume that @xmath64 is in the    convex hull of @xmath86 .",
    "( a2 ) : :    we assume random design with a mean - zero random predictor variable    @xmath14 with covariance matrix @xmath59 and let    @xmath87 be the    empirical gram matrices .",
    "let    @xmath25 be the estimates in each    group .",
    "assume that there exists some @xmath88    such that @xmath89    where @xmath90 is the minimal sample size    across all groups .",
    "( a3 ) : :    the optimal and estimated vectors are sparse in the sense that there    exists some @xmath91 such that    @xmath92    assumption ( a1 ) is fulfilled for known groups , where the convex hull of @xmath86 is equal to the convex hull of the support of @xmath49 and the maximin - vector @xmath64 is hence contained in the former .",
    "example 1 is fulfilling the requirement , and we will discuss generalizations to the settings in examples 2 and 3 below in section [ subsec.genunknown ] .",
    "assumptions ( a2 ) and ( a3 ) are relatively mild : the first part of ( a3 ) is an assumption that the underlying model is sufficiently sparse .",
    "if we consider standard lasso estimation with sparse optimal coefficient vectors and assuming bounded predictor variables , then ( a2 ) is fulfilled with high probability for @xmath93 of the order @xmath94 ( faster rates are possible under a compatibility assumption ) and @xmath95 of order @xmath96 , where @xmath97 denotes the minimal sample size across all groups ; see for see for example  @xcite .",
    "define for @xmath98 , the norm @xmath99 and let @xmath100 be the magging estimator ( [ eq : maximin ] ) .",
    "[ theo : main ] assume ( a1)-(a3 ) . then @xmath101    a proof is given in the appendix .",
    "the result implies that the maximin effects parameter can be estimated with good accuracy by magging ( maximin aggregation ) if the individual effects in each group can be estimated accurately with standard methodology ( e.g. penalized regression methods ) .",
    "theorem [ theo : main ] hinges mainly on assumption ( a1 ) .",
    "we discuss the validity of the assumption for the three discussed settings under appropriate ( and setting - specific ) sampling of the data - groups .",
    "_ example 1 : known groups ( continued ) .",
    "_ obviously , the groups @xmath102 are chosen to be the true known groups .",
    "assumption ( a1 ) is then trivially fulfilled with known groups and constant regression parameter within groups ( clusterwise regression ) .",
    "_ example 2 : smoothness structure ( continued ) .",
    "_ we construct @xmath9 groups of non - overlapping consecutive observations . for simplicity",
    ", we would typically use equal group size @xmath103 so that @xmath104 .    when taking sufficiently many groups and for a certain model of smoothness structure , condition  ( a1 ) will be fulfilled with high probability @xcite : it is shown there that it is rather likely to get some groups of consecutive observations where the optimal vector is approximately constant and the convex hull of these `` pure '' groups will be equal to the convex hull of the support of @xmath49 .",
    "_ example 3 : unknown groups ( continued ) .",
    "_ we construct @xmath9 groups of equal size @xmath105 by random subsampling : sample without replacement within a group and with replacement between groups .",
    "this random subsampling strategy can be shown to fulfill condition  ( a1 ) when assuming an additional so - called pareto condition @xcite . as an example ,",
    "a model with a fraction of outliers fulfills ( a1 ) and one obtains an important robustness property of magging which is closely connected to section [ subsec.robustness ] .      .",
    "the second column shows the realizations of @xmath82 for the first groups @xmath106 , while the third shows the least - squares estimates of the signal when projecting onto the space of periodic signals in a certain frequency - range .",
    "the last column shows from top to bottom : ( a ) the pooled estimate one obtains when adding all groups into one large dataset and estimating the signal on all data simultaneously ( the estimate does not match closely the common effects shown in red ) ; ( b ) the mean aggregated data obtained by averaging the individual estimates ( here identical to pooled estimation ) ; ( c ) the ( less generic ) maximin effects estimator from @xcite , and ( d ) magging : maximin aggregated estimators  ( [ eq : maximin ] ) , both of which match the common effects quite closely . , scaledwidth=99.0% ]",
    "we illustrate the difference between mean aggregation and maximin aggregation ( magging ) with a simple example .",
    "we are recording , several times , data in a time - domain .",
    "each recording ( or group of observations ) contains a common signal , a combination of two frequency components , shown in the top left of figure  [ fig.example ] .",
    "on top of the common signal , seven out of a total of 100 possible frequencies ( bottom left in figure  [ fig.example ] ) add to the recording in each group with a random phase .",
    "the 100 possible frequencies are the first frequencies @xmath107 , @xmath108 for periodic signal with periodicity @xmath109 defined by the length of the recordings .",
    "they form the dictionary used for estimation of the signal . in total @xmath110 recordings",
    "are made , of which the first 11 are shown in the second column of figure  [ fig.example ] .",
    "the estimated signals are shown in the third column , removing most of the noise but leaving the random contribution from the non - common signal in place .",
    "averaging over all estimates in the mean sense yields little resemblance with the common effects .",
    "the same holds true if we estimate the coefficients by pooling all data into a single group ( first two panels in the rightmost column of figure  [ fig.example ] ) .",
    "magging ( maximin aggregation ) and the closely related but less generic maximin estimation @xcite , on the other hand , approximate the common signal in all groups quite well ( bottom two panels in the rightmost column of figure  [ fig.example ] ) .",
    "@xcite provide other real data results where maximin effects estimation leads to better out - of - sample predictions in two financial applications .",
    "large - scale and ` big ' data poses many challenges from a statistical perspective .",
    "one of them is to develop algorithms and methods that retain optimal or reasonably good statistical properties while being computationally cheap to compute .",
    "another is to deal with inhomogeneous data which might contain outliers , shifts in distributions and other effects that do not fall into the classical framework of identically distributed or stationary observations .",
    "here we have shown how magging ( `` maximin aggregation '' ) can be a useful approach addressing both of the two challenges .",
    "the whole task is split into several smaller datasets ( groups ) , which can be processed trivially in parallel .",
    "the standard solution is then to average the results from all tasks , which we call `` mean aggregation '' here .",
    "in contrast , we show that finding a certain convex combination , we can detect the signals which are common in all subgroups of the data . while `` mean aggregation '' is easily confused by signals that shift over time or which are not present in all groups , magging ( `` maximin aggregation '' ) eliminates as much as possible these inhomogeneous effects and just retains the common signals which is an interesting feature in its own right and often improves out - of - sample prediction performance .",
    "breiman , l. ( 1996a ) . bagging predictors . , 24:123140 .",
    "breiman , l. ( 1996b ) . .",
    ", 24:4964 .",
    "breiman , l. ( 2001 ) . .",
    ", 45:532 .",
    "bhlmann , p. and yu , b. ( 2002 ) . analyzing bagging . ,",
    "30:927961 .",
    "bunea , b. , tsybakov , a. , and wegkamp , m. ( 2007 ) .",
    "aggregation for gaussian regression .",
    ", 35:16741697 .",
    "chandrasekaran , v. and jordan , m.  i. ( 2013 ) .",
    "computational and statistical tradeoffs via convex relaxation . , 110:e1181e1190 .",
    "desarbo , w. and cron , w. ( 1988 ) . a maximum likelihood methodology for clusterwise linear regression .",
    ", 5:249282 .",
    "mahoney , m.  w. ( 2011 ) .",
    "randomized algorithms for matrices and data .",
    ", 3:123224 .",
    "mclachlan , g. and peel , d. ( 2004 ) . .",
    "john wiley & sons .",
    "meinshausen , n. and bhlmann , p. ( 2014 ) .",
    "maximin effects in inhomogeneous large - scale data .",
    "preprint arxiv:1406.0596 .",
    "pinheiro , j. and bates , d. ( 2000 ) . .",
    "springer .",
    "( 2014 ) . .",
    "r foundation for statistical computing , vienna , austria .",
    "wolpert , d. ( 1992 ) . . ,",
    "_ proof of theorem  [ theo : main ] : _ define for @xmath111 ( where @xmath112 is as defined in ( [ eq : maximin ] ) the set of positive vectors that sum to one ) , @xmath113 and let for @xmath114 , @xmath115 then @xmath116 and @xmath117 and @xmath118 and @xmath119 .",
    "now , using ( a3 ) @xmath120 hence , as @xmath121 and @xmath118 , @xmath122 for @xmath123 , @xmath124 where @xmath125 follows by the definition of the maximin vector @xmath126 . combining the last inequality with ( [ eq:1 ] ) , @xmath127 furthermore , by ( a3 ) , @xmath128 using the equality for @xmath129 , @xmath130 combining ( [ eq:2 ] ) and ( [ eq:3 ] ) , @xmath131 which completes the proof.@xmath132        hats",
    "< - t(x ) % * % x /",
    "n                 # empirical covariance matrix of x h < - t(theta ) % * % hats % * % theta     # assume that it is positive definite                                      # ( use h + xi * i , xi > 0 small , otherwise ) a < - rbind(rep(1,g),diag(1,g ) )       # constraints b < - c(1,rep(0,g ) )                d < - rep(0,g )                        # linear term is zero w < - solve.qp(h,d,t(a),b , meq = 1 )   # quadratic programming solution to                                       # argmin(x^t h x ) such that ax > = b and                                       # first inequality is an equality    ...."
  ],
  "abstract_text": [
    "<S> large - scale data analysis poses both statistical and computational problems which need to be addressed simultaneously . </S>",
    "<S> a solution is often straightforward if the data are homogeneous : one can use classical ideas of subsampling and mean aggregation to get a computationally efficient solution with acceptable statistical accuracy , where the aggregation step simply averages the results obtained on distinct subsets of the data . however , </S>",
    "<S> if the data exhibit inhomogeneities ( and typically they do ) , the same approach will be inadequate , as it will be unduly influenced by effects that are not persistent across all the data due to , for example , outliers or time - varying effects . </S>",
    "<S> we show that a tweak to the aggregation step can produce an estimator of effects which are common to all data , and hence interesting for interpretation and often leading to better prediction than pooled effects . </S>"
  ]
}