{
  "article_text": [
    "cloud computing used to be tech leaders dream : a paradigm where computing becomes a service .",
    "it allows companies and users to use as much compute power  or storage  as needed , paying only for what is used . in this way , using one hundred computers during one hour costs the same as using one single computer for one hundred hours @xcite .",
    "+ users of cloud computing have many different needs .",
    "some use a website , for example a social network , some perform mathematical calculations and some use a virtual machine to deploy a production web application , among many other uses .",
    "they all require different levels of abstractions .",
    "a user of a webmail client does not need to know what operating system the server is using while someone who wants to create a website might not know how to manage a webserver .",
    "+ the most common paradigms these days are saas , software as a service ; paas , platform as a service ; and iaas , infrastructure as a service @xcite .",
    "if we look at them from the point of view of security , in the first two cases , the user has limited capabilities to conduct fraudulent activities due to the lack of power he has .",
    "there are always security risks  no one can deny this  , but the paradigm that is most useful for fraudulent activity is the last one : iaas , which normally offers complete administrator permissions to the user making it essential to have an intrusion detection system in place , to avoid other users being affected by the behavior of the fraudulent user @xcite .",
    "+ usually , users in a cloud are completely isolated , meaning that it is not common for one user to get access to data owned by someone else or directly interfere with other workloads . however , most clouds are configured to be flexible , allowing a user to use more than the assigned resources at some points , to account for pikes in use , or just because they are not being used .",
    "in addition , ethernet links are shared amongst all the workloads , which make them an easy target .",
    "the more bandwidth someone uses , the slower the other workloads will run .",
    "+ in addition , there are activities , which might appear as non - fraudulent to the user but are undesired for cloud providers .",
    "i am talking about bitcoin mining @xcite , or litecoin mining @xcite , which is becoming increasingly popular and harder to do .",
    "mining cryptocurrencies uses much higher power consumption and , furthermore , it reduces the lifespan of our software @xcite .",
    "in addition , there is the legal aspect of creating digital money using someone else s infrastructure . as a consequence , cryptocurrency mining is something most cloud providers would like to avoid .",
    "+ most intrusion detection systems ( ids ) nowadays require network packet inspections @xcite , invading a user s privacy . in the age of internet surveillance , it is understandable that users do not feel comfortable having all their data inspected by a third party application . is there a way to detect fraudulent activity while respecting a user s privacy ?",
    "a way of doing this would be to use data aggregates , which do not give a lot of detail , such as cpu usage or the number of outgoing packets in a closed interval , to perform a first classification .",
    "data aggregates as a privacy - friendly method to collect data has previously been used by many others such as @xcite . in case",
    "a fraudulent activity is suspected , then a more in - depth method can be used .",
    "this way allows users who run regular workloads to keep their privacy while detecting suspicious activity .",
    "this article details a method to collect samples of data from an openstack cluster @xcite , featuring regular workloads and fraudulent ones . with this data we are to try different classification algorithms such as svm or random forests in order to find the one offering the best results .",
    "+ we will try to classify 5 types of jobs : a regular workload , hadoop , which can use resources in many different ways ; a very cpu - intensive job , simulating mathematical calculations ; an internal ddos attack , from one virtual machine to another inside our cloud ; a physical network failure , to see if i can distinguish it from a regular job ; and cryptocurrency mining , i ll get data of both bitcoin and litecoin mining . + once initial results are shown and an algorithm chosen , we will proceed to discuss ways to improve the results by making use of a meta - classifier , creating a proof - of - concept that , using the existing data , simulates how it would work in a real cloud , in real time .",
    "+ in conclusion , we define a method to detect fraudulent or suspicious activity in a cloud environment using metrics , which are normally collected for billing purposes , which do not offer too much insight on the user s activities but it can detect an activity as suspicious and trigger an alarm for further investigation .",
    "openstack , born as a joint project of nasa and rackspace in 2010 , is an open - source cloud computing platform for public and private clouds .",
    "its main goal is to enable companies of any size to have their own private cloud at a low cost , feature rich and with a vibrant community developing it all over the world @xcite . before the release of open source cloud operating systems",
    ", companies had two options : they either bought clusters or rely on external companies who provided a cloud , such as amazon .",
    "with openstack , or other alternatives , such as cloudstack @xcite , companies can use their existing infrastructure for an in - premise , private cloud , which grants them a much higher security .",
    "+ openstack offers a set of separate components , which make it very easy to scale , offering different services such as compute , bock or object storage , network , databases or orchestration as a service ( look at the figure below ) .",
    "the following sections take a look to the most important of these components .",
    "nova is the subproject that takes care of providing compute power as a service .",
    "it can be plugged to one or several hypervisors such as qemu @xcite or kvm @xcite .",
    "it consists of a controller node , which uses an api to take requests , and several compute nodes , which are the ones where the virtual machines will reside and whose resources they will use @xcite . + compute nodes can be added at any point very easily , as well as removed , making nova highly available , with a single failure point , which is the controller , and very easy to scale .",
    "the controller node is the only one who has knowledge of all the computes and , based on the resources they have , their availability and another increasing number of constraints , it decides where to deploy each new virtual machine .",
    "glance provides the user with a restful api to store , discover and serve virtual machine images .",
    "it can also store volumes , which can be used as a template for new ones @xcite .",
    "+ these images can be stored in a wide variety of file systems , either in block storage or object storage file systems such as the one managed openstack swift .",
    "cinder takes care of providing block storage as a service . in reality , it started as a fork of nova , which was modified to manage virtual volumes instead of virtual machines .",
    "these volumes can , again , feature any file system suitable to the user , such as ceph @xcite .",
    "the keystone subproject manages authentication , tokens and policies for the entire openstack ecosystem by providing a restful api used by projects to control access permissions as well as to discover and provide access to other services in openstack , which are registered in keystone @xcite .",
    "neutron , previously known as quantum , provides network as a service to any resource created and managed by other components .",
    "for example , it can create different vlans to isolate several virtual machines from each other , as well as creating virtual network components such as routers .",
    "furthermore , with recent updates , it also provides firewall as a service and load balancer as a service @xcite .",
    "swift is the project in charge of highly available , distributed object storage .",
    "recently , a lot of companies are using swift together with ceph , an open source distributed file system by inktank , in order to provide higher availability @xcite .",
    "the most relevant component for this article is ceilometer , now known as openstack telemetry .",
    "it was first meant to be a metering component for openstack . however , many other applications have been suggested , some of them are very interesting .",
    "the reason why so many different applications have arisen is because ceilometer s job is to report usage metrics , data from each resource in openstack at regular intervals @xcite .",
    "the following section will describe ceilometer with some more detail .",
    "the openstack cluster used to collect the data for this article was a set of 3 intel nucs d54250wykh1 @xcite provided by cisco systems .",
    "the nucs have 2 network cards each , an ethernet one , connecting them through a switch , and a wireless one , giving them internet access . in terms of storage",
    ", they have 2 solid state drives each , a 250 gb one for the operating system and openstack installation and a 1 tb one to be used by cinder for object storage .",
    "in addition , each has 16 gb of ram .",
    "+ the operating system used is ubuntu 14.04 amd 64 + openstack icehouse .",
    "the ethernet nic has been used as the flat interface and the wireless one as the public interface .",
    "neutron , the networking component of openstack has not been used in favor of nova s legacy networking , which was simpler to install and , because of the types of data collected , did not alter the result .",
    "+ one of the nucs was used as both a controller and a compute node , and the other two acted as compute nodes .",
    "as previously mentioned , we plan to use data aggregates of several common metrics , instead of performing packet inspections or looking for flow patterns like many intrusion detection systems in the market currently do , such as snort @xcite .",
    "+ the reason for this is to offer a higher privacy to the user . in a society where concern for privacy is increasing exponentially , we need more solutions that offer security without sacrificing a user s privacy . a way to respect it is to make the data anonymous by hiding detailed patterns .",
    "we collect slices of usage , which show us rates of change of different metrics over the last few seconds .",
    "similar rates of change probably mean similar activity .",
    "+ in our case we use five seconds aggregates of 3 of the most common metrics available in all systems : cpu , disk and network .",
    "one of the initial goals was to use a small number of metrics . before using each metric",
    "i have transformed it to show the rate of change of the metric during the current interval @xcite .",
    "the metrics are :    * cpu_util : in tan percent . *",
    "disk.read.requests.rate : in requests / second .",
    "* disk.read.bytes.rate : in bytes / second .",
    "* disk.write.requests.rate : in requests / second * disk.write.bytes.rate : in bytes / second .",
    "* network.incoming.bytes.rate : in bytes / second . *",
    "network.incoming.packets.rate : in packets / second .",
    "* network.outgoing.bytes.rate : in bytes / second . * network.outgoing.packets.rate : in packets / second    all this data is collected per each virtual resource in our cluster  virtual machines , volumes , virtual routers , etc .  , every 5 seconds . in this way , we can easily remove data from resources we do not need such as other users virtual machines .",
    "it is worth pointing out the reason why both network incoming / outgoing bytes and packets are collected .",
    "the reason for this is to enable the classifier to distinguish between applications that send few , big packets , instead of many , small ones , allowing it to create a ratio with the number of packets and size . +",
    "this pipeline is configured in the ceilometer pipeline.yml file located under /etc / ceilometer in each compute node .",
    "once it has been modified , you need to restart the ceilometer - agent - compute process in all the servers .",
    "the file should instantly appear in the specified path and data samples will be appended to it every 5 seconds .",
    "+ the next step is to run an experiment , such as the ones described below , to generate data .      in order to proof that this method can not only detect fraudulent activities but",
    "also classify them , to a certain extent , i decided to run several experiments including several types of fraudulent activities , infrastructure failures and regular workloads .",
    "most the common workloads that run in our clouds have very different fingerprints @xcite",
    ". some will require little activity , while some others will use a lot of resources .",
    "+ in order to run experiments that represent as many common workloads as possible , i decided to use hadoop and run a benchmark suite by intel , called hibench @xcite , which runs several hadoop jobs in a sequence , containing benchmarks stressing certain resources , such as cpu or disk , but also regular algorithms such as pagerank .",
    "+ in addition , in order to make sure i can distinguish between cryptocurrencies and very cpu - intensive workloads , i also ran extra experiments using the linux stress tool @xcite .",
    "the first fraudulent activity i created data of is an internal ddos attack . by internal , i mean that it originates from virtual machines within our cloud and it is aimed to another virtual machine within our cloud .",
    "openstack offers resource over allocation by default , when possible , so a hacker could perform such an attack in order to attempt to slow down resources belonging to other users , impacting in their business operations and resulting in a potential decrease in income .",
    "+ the attack was a ping flood attack @xcite .",
    "i set it up by creating a custom ubuntu image that , upon boot , would monitor a remote file  in a different cluster  waiting for a victim s ip .",
    "once an ip was set in this remote file , it would create one hundred threads that would ping the victim , with the maximum allowed icmp packet size , 65,535 bytes .",
    "+ the experiment consisted of 50 virtual machines that attacked another machine in the same cluster using its ip .",
    "while running it , i could clearly notice a decreased overall speed in the cluster and , in some cases , i received timeout errors when trying to access virtual webservers hosted in one of the nodes affected by the attack .",
    "cryptocurrencies have become increasingly popular in recent years , starting with the release of bitcoins reaching a price of $ 1242 per coin in november 29th 2013 ( see figure below ) . with this growth",
    "comes a growth in the difficulty of mining these currencies @xcite .",
    "it is not unexpected , as a consequence , that users try to use cloud environments to mine them , using a lot of resources , and decreasing the life of the hardware in the data center .",
    "i decided to run experiments both mining bitcoins and litecoins .",
    "bitcoins use the sha-256 algorithm , while litecoins use the scrypt algorithm .",
    "initially , i tried to distinguish between them , as separate classes .",
    "unfortunately , no good results where achieved , due to almost identical fingerprints , so for the purpose of this article , both bitcoins and litecoins have been treated as a single class .",
    "+ for the experiments i used minerd , a cpu miner , needed because of the lack of gpu most servers have .",
    "mining using the cpu is usually slower , but if several big virtual machines perform it , acceptable speeds could be reached .      to simulate hardware failure",
    "i decided to disconnect the network of one of the physical nodes while an instance of hibench was running in a virtual machine .",
    "several nodes , a master and one or more slaves , which perform the task while the master coordinates them , form a hadoop cluster .",
    "the network failure caused the virtual hadoop node to become isolated from the hadoop cluster , not being able to contact the master , thus terminating the current job as a failure .",
    "+ while running this experiment almost all activity stopped in the physical cluster , including in its virtual machines .",
    "it makes sense for this to happen because openstack relies on network connectivity to synchronize and manage its services as well as to provide internet to the virtual resources .",
    "each one of the experiments should have different characteristics , which make it classifiable . before trying different classifiers",
    ", this section discusses what might make each case unique and if it would be possible to classify them manually , showing the data obtained after the experiments .",
    "as previously discussed , two types of general workloads have been run : a hadoop benchmark and a highly cpu - intensive job .",
    "it seems a fair assumption that the hadoop experiment will have values scattered along the entire spectrum , in all the metrics .",
    "as i said , hibench is a collection of jobs that benchmark different aspects of a hadoop cluster , so we would expect , for example , both low and high network usage , as well as cpu and disk .",
    "if we look at a scatter plot representing how network and cpu relate to each other as shown in the figure below , we can see that there is no clear pattern .",
    "on the other hand , we have the cpu - intensive job .",
    "this job simulates mathematical calculation , which could be taking place in the cloud as means to improve performance and speed .",
    "this kind of job would probably have very low , if any , network traffic as well as a relatively low disk usage , in most cases .",
    "however , cpu should be peaking most of the time . looking at the scatter plot below we can see that the samples cluster on the highest point in cpu usage while staying very low in both network and disk .",
    "the ping flood attack works by sending a big number of icmp packets to a victim , using most of its resources , as a consequence .",
    "we would expect the data collected to have high values for network and some cpu load , while having relatively low values for disk . +",
    "as it can be seen in the figure below , a very high number of bytes are being sent while we have a low , but constant cpu usage .",
    "note that we lost some of the data , which appears as several dots in the lowest left part of the plot , possibly due to timeouts caused by the attack .",
    "the process of mining a digital currency usually consists of calculating hashes , what is called proof - of - work , using costly algorithms , which take time and cpu resources .",
    "this method is also used for password encryptions in order to render a brute force attack unfeasible . in this experiment",
    "i have data of both bitcoin , using the sha-256 algorithm , and litecoin , using the more common scrypt algorithm .",
    "+ in addition , in order to make the experiment more real , i joined mining pools , so i would also expect a noticeable amount of network traffic present during the mining operation , probably a regular amount of traffic .",
    "+ looking at the scatter plot below , containing mixed data of bitcoin and litecoin generation ; we can see that the expected results were correct .",
    "the vertical strips visible in the plot show us that there was a repetition of network traffic patterns , which would be correct for cryptocurrency mining .",
    "finally , for the network down experiment , we would expect to see minimal values of all the metrics , probably making it almost invisible in the plots where several experiments appear together .",
    "+ indeed , the following scatter plot shows that less than 3          having discussed the data we get from each experiment , we also need to discuss whether they are visually distinguishable when represented together",
    ". the easier they can be distinguished visually , the easier it will be to classify them .",
    "orange @xcite offers an option to evaluate different representations and return the plot showing us the most relevant information . after running this algorithm ,",
    "the best ranking representation is the one with cpu utilization and network outgoing packet rate , so let s look at a scatter plot containing all the data collected .",
    "as it can be seen in the plot above , the data seems to be fairly easy to classify visually .",
    "there clearly is some overlapping with the cryptocurrency and cpu - intensive samples and the network failure data does not appear to be visible , overlapping with the hadoop samples .",
    "as previously expected , the hadoop data is scattered over the mining and the cpu intensive data .",
    "however , we will see if this affects the results of the classification algorithms . finally , the ddos samples are completely separated from the rest , which should make it very easy to detect .",
    "there are many different classifiers , which are used for many different applications .",
    "it is common to try several of them to see which one offers a better result with the kind of data for each problem .",
    "+ in order to rapidly try different algorithms , i used orange to build a topology , which took the initial data and evaluated several learners , generating reports in order to choose the best one .",
    "the topology used was the following :        as it can be seen in the previous topology , the initial data is loaded and basic transformations are applied to tell orange which field is the class and which are features . next , all the classification algorithms are applied to the data . in order to validate the results ,",
    "cross - validation is performed .",
    "+ cross - validation is common technique in machine learning to ensure that the results are not coincidence and to ensure they can be replicated . in order to do this , the data we have is divided into folds , being 10 the most used number .",
    "once we have 10 equally sized folds , we use 9 to train the classification model , and the remaining one to evaluate its efficiency .",
    "once the process has finalized , we rotate the folds and repeat it again 10 times , so each fold is used once for evaluation and 9 times for training . as a consequence at the end of the cross - validation process",
    ", we can be sure that the results are not coincidence and that they are replicable using different data .",
    "+ once all the classifiers have finished , the results are sent to the learner tester , which creates statistics and comparisons on performance and accuracy for each classifier .",
    "these results can then be plotted or converted to a confusion matrix .",
    "+ the data used contains 36004 instances and , after evaluating it , we will see that the best algorithm was random forest .",
    "the following plot represents the average proportion of truths  the percentage of correct classifications  of the algorithms shown in the topology in the previous section .",
    "the best one is clearly the random forest classifier with a 97.5% of samples correctly classified . +",
    "a random forest is an ensemble of decision trees , which work together .",
    "the features of the input data are divided among different trees , depending on how relevant it is for the final choice .",
    "the accuracy of the classification can change depending on the number of trees of the forest .",
    "+ now that the best algorithm had been found , i created a python program that takes the same data as input and applies the random forest classifier from the scikit library .",
    "i wrote this , not only to confirm the results provided by orange but also , to be able to automate trials with more configurations .",
    "with it i was able to quickly get performance results for a varying number of trees and with different amounts of folds for cross - validation .",
    "these confirmed that the results were correct .",
    "+ next , i will discuss how the chosen tree looks like and what features seem to be more relevant for a correct classification .",
    "this will provide us with a unique insight on the patterns required for identifying these workloads as well as a good understanding of what is happening while a data sample is classified .",
    "+ the chosen tree for the classification is depicted at the figure at the end of the article .",
    "the image shows only the first four levels of the tree but it is already enough to see what the most relevant metric is in this case : network.incoming.bytes.rate , which is the root of the tree . according to the condition of whether this value is higher or lower than 50.800 bytes / second , we can already separate most of the samples representing a ddoss or a hadoop job , which represent a higher usage of the network , from the rest .",
    "+ if we look at the left branch , the tree then checks the cpu utilization . in our samples ,",
    "a hadoop workload shows a lower cpu than a ddos attack , being able to separate most cases of hadoop samples and more than half of the ddos ones .",
    "following again the left branch , if the cpu is higher than a threshold , then it might be a cryptocurrency mining or cpu intensive sample .",
    "on the other hand , if the cpu is lower than the threshold , we can almost be certain that we are classifying a ddos sample . + going one step back and looking at the right branch , the tree checks the network.incoming.packets.rate .",
    "a ddos attack will always have a high value for this meter , while a hadoop workload will have a lower one .",
    "this condition allows us to classify these two packets with very high accuracy .",
    "+ going back to the root node and looking at the right branch , we can see that the classification is not as straight forward , and 4 levels are not enough to see what is going on in the tree .",
    "+ these are some important sub - trees that help make decisions to separate classes :        here we can see some of the most interesting nodes , where cryptocurrency mining is separated from a cpu - intensive activity according to the cpu utilization .",
    "it is interesting because classifying between two very cpu intensive workloads using this condition will probably be the cause of some of the incorrect results .",
    "this condition is repeated in different places along the tree .",
    "in addition , if different workloads are added in the future , the borderline between cpu - intensive allowed activities and cryptocurrency mining will become more blurry in terms of cpu utilization .",
    "this justifies the use of a meta - classifier to perform decisions based on several of the last collected samples .",
    "the next example shows how we can distinguish the cpu - intensive workload from the cryptocurrency mining one by looking at the network usage .",
    "indeed , if we do mathematical calculations , the network usage will be much lower than bitcoin mining , where there is a constant network flow with the mining pool . in this case",
    ", if we have more than 70.000 outgoing bytes / second , it decides we are mining a cryptocurrency .",
    "otherwise , it decides we are performing an allowed cpu - intensive activity . unlike the previous example , this condition is not bound to introduce many errors in the classification , as both jobs have a distinctive network usage fingerprint .",
    "finally , i ll show an example were the tree classifies between a ddos and a hadoop job , represented in the image below , in orange and yellow .",
    "+ when this point in the tree is reached , it means we have very high network utilization and a relatively low cpu utilization . what the tree does in this case is to look at the disk data , which is the metric that makes a difference in this case . + as it can be seen ,",
    "if we have more than a certain number of disk write requests per second ( 4.333 ) , then we are trying to classify a hadoop sample . on the other hand ,",
    "if we have high network usage , relatively low cpu and a low number of disk write requests , we are most likely dealing with a ddos sample .",
    "we now have a classifier that works approximately 97.5% of the times , however , as the number of workloads increase , this number is bound to decrease . in real life , we can not expect to detect a bitcoin mining operation 5 seconds after it starts .",
    "furthermore , it makes sense that such an activity keeps running for a long time , so we want to be able to detect an ongoing fraudulent activity . if we assume that a fair detection interval is one hour , then we can use the individual classifications to perform an hourly statistic and , depending on the overall results , we can decide on a positive or a negative decision .",
    "+ the meta - classifier will look at the results obtained during the last hour .",
    "if the percentage of false positives is under a specified threshold , then a positive decision will be made .",
    "+ the previously mentioned python program contains a proof of concept of such a meta - classifier .",
    "given all the classification results calculated in the last hour , plus their correct class , it tries different thresholds to determine which is the minimum error threshold we need with the current model to achieve a 100% classification success .",
    "surprisingly , with such big classification accuracy , the threshold can be quite low , in fact , as low as 4% of error would ensure us 100% accuracy with the current data .",
    "+ in real environments this threshold could be higher or lower depending on how cautious we want to be .",
    "if we want to investigate all possible cases , we would set a high error threshold , to make sure that even if we have a 30% - 40% of positives in an hour , an alarm gets triggered .",
    "+ on the other hand , if we only want to be noticed when there is a case with a very high probability , we would set the threshold lower , to 5% - 10% , for example .",
    "+ the algorithm i used to test the results with different thresholds simulates an hour by taking only the samples that would have been generated in this period of time and assumes that there will be only one virtual machine for each case , to keep it simple .",
    "given this constraint and the correct classification values , it checks if the results from the random forest have an error lower than the threshold , which has been passed as a parameter .",
    "this method prints the cases that have not been properly classified or , in case all of them are successfully classified , it prints a success message at the end .",
    "this article has shown a method to detect fraudulent activities running in our openstack cluster by means of a random forest .",
    "the data used to detect such activities is formatted of a slice of the last 5 seconds of resource usage for each virtual resource ; let it be virtual machines or volumes , among others .",
    "as with any method , it offers certain advantages but it also has some shortcomings , which will be discussed in this section .      using data aggregates to classify the activity provides a layer of privacy other methods do nt . of course",
    ", detecting a user s activities is not privacy - friendly itself , but with this framework , you do not have detailed information about what they are doing , but are able to detect certain high level patterns , which identify suspicious activity .",
    "for example , if a company is using our cloud for confidential activities , we would only be able to know that they are running mathematical calculations , or that they are using distributed systems , which is done by many companies these days .",
    "we would not be able , however , to tell what they are calculating or what information is being sent among nodes in distributed systems .",
    "+ another advantage is the simplicity of collecting this kind of data .",
    "imagine we are a web hosting company and we run on top of openstack",
    ". then we will boot virtual machines to be used as web server by our customers .",
    "let s also assume that the cloud provider does not offer an intrusion detection system . using the method proposed here , even without being administrators of the cloud , not having direct access to the physical resources data",
    ", we can still obtain the required data from our virtual machines and send it to our own systems for offline , or even real time , analysis .      as with any method",
    ", there are shortcomings to using it and it has some limitations . + the most important shortcoming is the amount of lost information in the data aggregates . while it does not affect much with the tested activities , as the number of different classes to classify increases , so will the number of similar workloads , decreasing the effectiveness of the classification .",
    "this is why this method can be used of a first step of a fraudulent activity detection pipeline .",
    "it can be used to detect suspicious activity and then trigger an alarm that starts a more in - depth ids that will determine if it was truly a fraudulent workload or not . by doing this , we reduce the amount of data an ids has to process , while being able to reuse the data that is collected in order to bill the customer .",
    "thanks to the use of openstack , this article has been accepted as a conference talk in the next openstack summit , which takes places in paris , this november .",
    "the talk , which will be a more openstack - focused , is called `` using ceilometer data to detect fraudulent activity in our openstack cluster '' @xcite and several people have already emailed the author expressing their interest in watching it . + in terms of development , the next step is to create a real time pipeline that runs in a production ready openstack cluster , classifying the activity , using a meta - classifier , and sending hourly reports . in order to do this , an easy way would be to modify ceilometer so it performs the classification for each sample and saves the result in the database as a new metric . then , once an hour , the meta - classifier can retrieve the last samples and perform the final decision . + this method , modifying ceilometer , makes it easy to create a real time pipeline for openstack . however , this would not be compatible with other clouds , such as aws or microsoft azure",
    ". it would probably be better to create everything as a separate program that accepts data input via a restful api .",
    "this would allow the classification itself to take place anywhere , even in the cloud itself .",
    "in addition , it would allow cloud users , not administrators , to monitor their own resources , having complete control of this data .",
    "00    m.  armbrust , a.  fox , r.  griffith , a.  d. joseph , r.  katz , a.  konwinski , g.  lee , d.  patterson , a.  rabkin , i.  stoica , _ et  al .",
    "_ , `` a view of cloud computing , '' _ communications of the acm _ , vol .  53 , no .  4 , pp .  5058 , 2010 .",
    "j.  peng , x.  zhang , z.  lei , b.  zhang , w.  zhang , and q.  li , `` comparison of several cloud computing platforms , '' in _ information science and engineering ( isise ) , 2009 second international symposium on _ , pp .  2327 , ieee , 2009 .",
    "w.  he , x.  liu , h.  nguyen , k.  nahrstedt , and t.  abdelzaher , `` pda : privacy - preserving data aggregation in wireless sensor networks , '' in _",
    "infocom 2007 .",
    "26th ieee international conference on computer communications .",
    "ieee _ , pp .  20452053 , ieee , 2007 .",
    "m.  solanas , j.  hernandez - castro , and d.  dutta , `` using ceilometer data to detect fraudulent activity in our openstack cluster . ''",
    "available at https://www.openstack.org/summit/openstack-paris-summit-2014/session-videos/presentation/using-ceilometer-data-to-detect-fraudulent-activity-in-our-openstack-cluster"
  ],
  "abstract_text": [
    "<S> more users and companies make use of cloud services every day . they all expect a perfect performance and any issue to remain transparent to them . </S>",
    "<S> this last statement is very challenging to perform . </S>",
    "<S> a user s activities in our cloud can affect the overall performance of our servers , having an impact on other resources . </S>",
    "<S> + we can consider these kind of activities as fraudulent . </S>",
    "<S> they can be either illegal activities , such as launching a ddos attack or just activities which are undesired by the cloud provider , such as bitcoin mining , which uses substantial power , reduces the life of the hardware and can possibly slow down other user s activities . </S>",
    "<S> + this article discusses a method to detect such activities by using non - intrusive , privacy - friendly data : billing data . </S>",
    "<S> we use openstack as an example with data provided by telemetry , the component in charge of measuring resource usage for billing purposes . </S>",
    "<S> results will be shown proving the efficiency of this method and ways to improve it will be provided as well as its advantages and disadvantages .    </S>",
    "<S> openstack , ceilometer , cloud computing , machine learning , classification , security , networks , intrusion , random forest , bitcoin mining </S>"
  ]
}