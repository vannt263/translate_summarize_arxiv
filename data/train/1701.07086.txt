{
  "article_text": [
    "the minimum covariance determinant ( mcd ) method  @xcite is a highly robust estimator of multivariate location and scatter .",
    "given an @xmath0 data matrix @xmath1 with @xmath2 , its objective is to find @xmath3 observations whose sample covariance matrix has the lowest possible determinant . here",
    "@xmath4 is fixed .",
    "the mcd estimate of location is then the average of these @xmath3 points , whereas the scatter estimate is a multiple of their covariance matrix .",
    "consistency and asymptotic normality of the mcd estimator have been shown by  @xcite and  @xcite .",
    "the mcd has a bounded influence function  @xcite and has the highest possible breakdown value ( i.e.  @xmath5 ) when @xmath6  @xcite .",
    "the mcd approach has been applied to various fields such as chemistry , finance , image analysis , medicine , and quality control , see e.g. the review paper of @xcite .",
    "a major restriction of the mcd approach is that the dimension @xmath7 must satisfy @xmath8 for the covariance matrix of any @xmath3-subset to be non - singular .",
    "in fact , for accuracy of the estimator it is often recommended to take @xmath9 , e.g. in @xcite .",
    "this limitation creates a gap in the availability of high breakdown methods for so - called `` fat data '' , in which the number of rows ( observations ) is small compared to the number of columns ( variables ) . to fill this gap we propose a modification of the mcd to make it applicable to high dimensions .",
    "the basic idea is to replace the subset - based covariance by a regularized covariance estimate , defined as a weighted average of the sample covariance of the @xmath3-subset and a predetermined positive definite target matrix .",
    "the proposed minimum regularized covariance determinant ( mrcd ) estimator is then the regularized covariance based on the @xmath3-subset which makes the overall determinant the smallest .",
    "in addition to its availability for high dimensions , the main features of the mrcd estimator are that it preserves the good breakdown properties of the mcd estimator and is well - conditioned by construction . since the estimated covariance matrix is guaranteed to be invertible it is suitable for computing robust distances , and for linear discriminant analysis and graphical modeling @xcite .",
    "furthermore , we will generalize the c - step theorem of @xcite by showing that the objective function is reduced when concentrating the @xmath3-subset to the @xmath3 observations with the smallest robust distance computed from the regularized covariance .",
    "this c - step theorem forms the theoretical basis for the proposed fast mrcd estimation algorithm .",
    "the remainder of the paper is organized as follows . in section [ mcdtomrcd ]",
    "we introduce the mrcd covariance estimator and discuss its properties .",
    "section [ estimation ] proposes a practical and fast algorithm for the mrcd .",
    "the extensive simulation study in section [ simulation ] confirms the good properties of the method .",
    "section [ applications ] uses the mrcd estimator for outlier detection and regression analysis on real data sets from chemistry and criminology .",
    "the main findings and suggestions for further research are summarized in the conclusion .",
    "let @xmath10 be a dataset in which @xmath2 denotes the @xmath11-th observation ( @xmath12 ) .",
    "the observations are stored in the @xmath0 matrix @xmath1 .",
    "we assume that most of them come from an elliptical distribution with location @xmath13 and scatter matrix @xmath14 .",
    "the remaining observations can be arbitrary outliers , and we do not know beforehand which ones they are . the problem is to estimate @xmath13 and @xmath14 despite the outliers .",
    "the mcd approach searches for an @xmath3-subset of the data ( where @xmath15 ) whose sample covariance matrix has the lowest possible determinant .",
    "clearly , the subset size @xmath3 affects the efficiency of the estimator as well as its robustness to outliers . for robustness",
    ", @xmath16 should be at least the number of outliers .",
    "when many outliers could occur one may set @xmath17 .",
    "typically one sets @xmath18 to get a better efficiency . throughout the paper",
    ", @xmath19 denotes a set of @xmath3 indices reflecting the observations included in the subset , and @xmath20 is the collection of all such sets . for a given @xmath19 in @xmath20 we denote the corresponding @xmath21 submatrix of @xmath22 by @xmath23 . throughout the paper , we use the term @xmath3-subset to denote both @xmath19 and @xmath23 interchangeably .",
    "the mean and sample covariance matrix of @xmath23 are then @xmath24 the mcd approach then aims to minimize the determinant of @xmath25 among all @xmath26 : @xmath27 where we take the @xmath7-th root of the determinant for numerical reasons .",
    "note that the @xmath7-th root of the determinant of the covariance matrix is the geometric mean of its eigenvalues ; @xcite calls it the standardized generalized variance .",
    "the mcd can also be seen as a multivariate least trimmed squares estimator in which the trimmed observations have the largest mahalanobis distance with respect to the sample mean and covariance of the @xmath3-subset @xcite .",
    "the mcd estimate of location @xmath28 is defined as the average of the @xmath3-subset , whereas the mcd scatter estimate is given as a multiple of its sample covariance matrix : @xmath29 where @xmath30 is a consistency factor such as the one given by @xcite , and depends on the trimming percentage @xmath31 .",
    "@xcite and  @xcite prove consistency and asymptotic normality of the mcd estimator , and @xcite show that it has the highest possible breakdown value ( i.e. ,  @xmath5 ) when @xmath6 . accurately estimating a covariance matrix requires a sufficiently high number of observations",
    ". a rule of thumb is to require @xmath32 @xcite .",
    "when @xmath33 the mcd is ill - defined since all @xmath25 have zero determinant .",
    "we will generalize the mcd estimator to high dimensions . as is common in the literature",
    ", we first standardize the @xmath7 variables .",
    "for this we compute the median of each variable and stack them in a location vector @xmath34 .",
    "we also estimate the scale of each variable by the qn estimator of @xcite , and put these scales in a diagonal matrix @xmath35 .",
    "the standardized observations are then @xmath36 this disentangles the location - scale and correlation problems , as in @xcite .    in a second step ,",
    "we use a predetermined and well - conditioned symmetric and positive definite target matrix @xmath37 .",
    "following @xcite , we call such a matrix well - conditioned if the condition number ( i.e. , the ratio between the largest and smallest eigenvalues ) is at most 1000",
    ". we also use a scalar weight coefficient @xmath38 , henceforth called the regularization parameter .",
    "we then define the regularized covariance matrix of an @xmath3-subset @xmath19 of the standardized data @xmath39 as @xmath40 where @xmath41 is as defined in ( [ covarianceh ] ) but for @xmath39 , and @xmath30 is the same consistency factor as in ( [ eq : cfactormcd ] )",
    ".    it will be convenient to use the singular value decomposition @xmath42 where @xmath43 is the diagonal matrix holding the eigenvalues of @xmath37 and @xmath44 is the orthogonal matrix holding the corresponding eigenvectors .",
    "we can then rewrite the regularized covariance matrix @xmath45 as @xmath46      \\mathbf{\\lambda}^{1/2 } \\mathbf{q } '      \\label{subshrinkage}\\ ] ] where the @xmath47 matrix @xmath48 consists of the transformed standardized observations @xmath49 it follows that @xmath50 .",
    "the mrcd subset @xmath51 is defined by minimizing the determinant of the regularized covariance matrix @xmath45 in ( [ subshrinkage ] ) : @xmath52 since @xmath37 , @xmath44 and @xmath43 are fixed , @xmath51 can also be written as @xmath53 once @xmath51 is determined , the mrcd location and scatter estimates of the original data matrix @xmath22 are defined as @xmath54              \\mathbf{\\lambda}^{1/2 } \\mathbf{q } ' \\mathbf{d}_{{\\boldsymbol{x } } }    \\label{mrcdestimates}\\end{aligned}\\ ] ] in which @xmath55 and where @xmath56 in ( [ sstar ] ) is the diagonal matrix holding the square roots of the diagonal elements of @xmath57 .",
    "we use @xmath58 to rescale the diagonal elements of the final mrcd covariance matrix ( [ mrcdestimates ] ) .",
    "when the target matrix @xmath59 is the identity , these diagonal elements become those of the initial scale estimates in @xmath35 .",
    "also note that the consistency factor @xmath30 cancels in ( [ mrcdestimates ] ) .",
    "because of the initial standardization step , the mrcd scatter estimate is location invariant and scale equivariant .",
    "this means that for any diagonal @xmath60 matrix @xmath61 and any @xmath62 vector @xmath63 the mrcd scatter estimate @xmath64 equals @xmath65 .",
    "the precision matrix is the inverse of the scatter matrix , and is needed for the calculation of robust mrcd - based mahalanobis distances , for linear discriminant analysis , for graphical modeling @xcite , and for many other computations .",
    "the mrcd scatter matrix ( [ mrcdestimates ] ) is computationally convenient to invert by its construction , yielding the expression @xmath66^{-1 }      \\mathbf{q } \\mathbf{\\lambda}^{-1/2 } \\mathbf{d}_{{\\boldsymbol{x}}}^{-1}\\;\\;.    \\label{invmrcdestimates}\\end{aligned}\\ ] ] by the sherman - morrison - woodbury identity @xcite this can also be written as @xmath67^{-1 } \\mathbf{q }   \\mathbf{\\lambda}^{-1/2 } \\mathbf{d}_{{\\boldsymbol{x}}}^{-1 } \\label{invmrcdestimates2}\\end{aligned}\\ ] ] where @xmath68 is the @xmath47 matrix holding the standardized observations @xmath69 where @xmath70 is the indicator function , hence @xmath71 .",
    "note that the mrcd should not be confused with the regularized minimum covariance determinant ( rmcd ) estimator of @xcite .",
    "the latter assumes sparsity of the precision matrix , and maximizes the penalized log - likelihood function of each @xmath72subset by the glasso algorithm of @xcite .",
    "the repeated application of glasso is time - consuming .",
    "the mrcd estimate depends on two quantities : the target matrix @xmath37 and the regularization parameter @xmath38 . for the target matrix @xmath37 on @xmath39 we can take the identity matrix ; relative to the original data @xmath73 this is the diagonal matrix with the robustly estimated univariate scales on the diagonal . depending on the application , we can also take a non - diagonal target matrix @xmath37 .",
    "when this matrix is estimated in a first step , it should be robust to outliers in the data .",
    "a reasonable choice is to compute a rank correlation matrix of @xmath39 , which incorporates some of the relation between the variables .",
    "when we have reasons to suspect an equicorrelation structure , we can set @xmath37 equal to @xmath74 with @xmath75 the @xmath60 matrix of ones , @xmath76 the identity matrix , and @xmath77 to ensure positive definiteness .",
    "the parameter @xmath78 in the equicorrelation matrix ( [ equicor ] ) can be estimated by averaging robust correlation estimates over all pairs of variables , under the constraint that the determinant of @xmath79 is above a minimum threshold value .",
    "when the regularization parameter @xmath38 equals zero @xmath45 becomes the sample covariance @xmath80 , and when @xmath38 equals one @xmath45 becomes the target .",
    "we require @xmath81 to ensure that @xmath45 is positive definite ( as it is a convex combination of positive definite matrices ) , hence invertible .    in practice ,",
    "we recommend a data - driven approach which sets @xmath38 at the lowest nonnegative value for which @xmath82 is well - conditioned .",
    "this is easy to implement , as we only need to compute the eigenvalues @xmath83 of @xmath84 once , since the eigenvalues of @xmath82 equal @xmath85 note that by this heuristic we only use regularization when needed . indeed ,",
    "if @xmath86 is well - conditioned , the heuristic sets @xmath38 equal to zero . also note that the eigenvalues in ( [ eq : eigenvalues ] ) are at least @xmath38 , so the smallest eigenvalue of the mrcd scatter estimate is bounded away from zero .",
    "therefore the mrcd scatter estimator has a 100% implosion breakdown value , compared to the 50% implosion breakdown value of the mcd .",
    "[ estimation ]    a naive algorithm for the optimization problem ( [ subset2star ] ) would be to compute @xmath87 for every possible @xmath3-subset @xmath19 . however , for realistic sample sizes this type of brute force evaluation is infeasible .",
    "the original mcd estimator ( [ subset1star ] ) has the same issue .",
    "the current solution for the mcd consists of either selecting a large number of randomly chosen initial subsets @xcite or starting from a smaller number of deterministic subsets @xcite . in either case",
    "one iteratively applies so - called c - steps .",
    "the c - step of mcd improves an @xmath3-subset @xmath88 by computing its mean and covariance matrix , and then puts the @xmath3 observations with smallest mahalanobis distance in a new subset @xmath89 .",
    "the c - step theorem of @xcite proves that the covariance determinant of @xmath89 is lower than or equal to that of @xmath90 , so c - steps lower the mcd objective function .",
    "we will now generalize this theorem to regularized covariance matrices .",
    "[ cstep ] starting from a @xmath3-subset @xmath91 one can compute @xmath92 and @xmath93 .",
    "the matrix @xmath94 is positive definite hence invertible , so we can compute @xmath95 for @xmath12 .",
    "let @xmath89 be an @xmath3-subset for which @xmath96 and compute @xmath97 , @xmath98 and @xmath99 then @xmath100 with equality if and only if @xmath101 and @xmath102 .",
    "note that for @xmath103 our theorem [ cstep ] specializes to theorem 1 of @xcite , but with the weaker condition ( [ eq : concentration ] ) , thereby strengthening the result .",
    "the proof of theorem [ cstep ] is given in appendix a.    making use of the generalized c - step we can now construct the actual algorithm . in the pseudocode we see that step 3.1 determines the initial scatter estimates @xmath104 of @xmath48 in the same way as in the detmcd algorithm of @xcite . in case an initial @xmath104 is ( nearly ) singular , step 3.2 regularizes it with parameter @xmath105 ( experiments have indicated that in most cases the same results are obtained for @xmath106 and @xmath107 ) . based on these subsets we then set @xmath38 in a conservative way to ensure the mrcd covariance computed on each of them is well - conditioned . the value of @xmath38 for which the condition number of @xmath82 equals 1000 is obtained using line search and formula ( [ eq : eigenvalues ] ) .    for each initial subset",
    "we then apply c - steps until the subset no longer changes , which typically requires only a few steps .",
    "finally , out of the six resulting subsets we select the one with the lowest objective , and use it to compute our final location and scatter estimates according to ( [ mrcdestimates ] )",
    ".    0.2 in *  * + -0.4 in * mrcd algorithm * + -0.4 in *  * + -0.4 in    1 .",
    "compute the standardized observations @xmath108 as defined in ( [ uobs ] ) using the median and the qn estimator for univariate location and scale .",
    "2 .   perform the singular value decomposition of @xmath37 into @xmath109 where @xmath43 is the diagonal matrix holding the eigenvalues of @xmath37 and @xmath44 is the orthogonal matrix whose columns are the corresponding eigenvectors .",
    "compute @xmath110 .",
    "3 .   find the mrcd subset : 1 .",
    "follow subsection 3.2 in @xcite to obtain initial location estimates @xmath111and scatter estimates @xmath112 for @xmath113 .",
    "2 .   if @xmath112 is not invertible , replace @xmath112 by @xmath114 where @xmath115 .",
    "3 .   determine the subsets @xmath116 of @xmath48 containing the @xmath3 observations with lowest mahalanobis distance in terms of @xmath111 and @xmath112 .",
    "4 .   for each subset @xmath116 , determine the smallest value of @xmath117 for which @xmath118 is well - conditioned .",
    "denote this value as @xmath119 .",
    "5 .   set @xmath120 .",
    "6 .   for each initial subset @xmath116 ( @xmath121 ) ,",
    "repeat the generalized c - steps from theorem [ cstep ] until convergence .",
    "denote the resulting subsets as @xmath122 .",
    "let @xmath51 be the subset for which @xmath123 has the lowest determinant among the six candidates .",
    "4 .   from @xmath51",
    "compute the final mrcd location and scatter estimates as in ( [ mrcdestimates ] ) .",
    "-0.2 in *  *",
    "we now investigate the empirical performance of the mrcd . in this section and the next we will use an equicorrelated target matrix as in ( [ equicor ] ) .",
    "we estimate the equicorrelation parameter @xmath78 as the average of all pairwise correlation estimates obtained using kendall s tau , the robustness of which was studied in @xcite . to guarantee positive definiteness , we impose that @xmath78 is at least 0.1 above @xmath124 .",
    "we compare the mrcd estimator to the ogk estimator of @xcite , which can also robustly estimate location and scatter in high dimensions but by itself does not guarantee the scatter matrix is well - conditioned . the ogk estimator is described in appendix b.    [ [ data - generation - setup . ] ] data generation setup .",
    "+ + + + + + + + + + + + + + + + + + + + + +    in the simulation experiment we generated @xmath125 contaminated samples of size @xmath126 from a @xmath7-variate normal distribution , with @xmath47 taken as either @xmath127 , @xmath128 @xmath129 and @xmath130 . in the first",
    "setting we generated data with mean zero and all variances equal to one ( this is without loss of generality , given the equivariance properties of the estimators being considered ) .",
    "the correlation matrices were generated randomly following @xcite , henceforth alyz , to ensure that the performance is not tied to a particular choice of correlation matrix .",
    "the randomization was restricted to correlation matrices with condition number equal to 100 .",
    "our second setup is the three - factor model used by @xcite . under this model @xmath131",
    "is generated as @xmath132 where the 3-dimensional random vectors @xmath133 are normal random variables and the @xmath134 factor loadings @xmath63 are drawn from a multivariate normal distribution .",
    "the mean and covariance of each of the component distributions match those in ( * ? ? ?",
    "* table 1 ) .",
    "the @xmath7-dimensional independent error terms are also normal random variables , with zero means and standard deviations generated from a gamma distribution that is bounded from below by a threshold value .    to contaminate the data sets , we follow @xcite and randomly replace @xmath135 observations by outliers along the eigenvector direction of @xmath136 with smallest eigenvalue , since this is the direction where the contamination is hardest to detect .",
    "the distance between the outliers and the mean of the good data is denoted by @xmath137 , which is set to @xmath138 for medium - sized outlier contamination and to @xmath139 for far outliers .",
    "we let the fraction of contamination @xmath140 be either 0% ( clean data ) , 10% or 20% .",
    "[ [ evaluation - setup . ] ] evaluation setup .",
    "+ + + + + + + + + + + + + + + + +    on each generated data set we run the mrcd with different subset sizes @xmath3 , taken as 50% , 75% , 90% and 100% of the sample size @xmath126 , and compare the results obtained when using @xmath103 ( mcd estimator ) versus using the data - driven choice of @xmath38 with the condition number at most 1000 .",
    "we measure the inaccuracy of our scatter estimates @xmath141 compared to the true covariance @xmath142 by their mean squared error ( mse ) given by @xmath143 note that the true @xmath142 differs across values of @xmath144 when generating data according to alyz .",
    "[ table : alyz ]    [ table : factor ]    the results are reported in tables 1 and 2 , where the left panel shows the mse and the right panel lists the average value of the data - driven @xmath38 . the top panel shows the results in the absence of outlier contamination , i.e. @xmath145 . in the lower panels we see the effects of 10% and 20% contamination in the data , for different values of @xmath137 .",
    "[ [ discussion - of - results . ] ] discussion of results .",
    "+ + + + + + + + + + + + + + + + + + + + + +    let us first study the relationship between the mrcd and the mcd .",
    "the latter is found in the rows labeled @xmath103 , and only exists in the settings where @xmath146 .",
    "the mrcd instead uses the smallest value of @xmath147 for which the scatter matrix is well - conditioned , so when the mcd is well - conditioned the mrcd also obtains @xmath103 and thus coincides with the mcd in that case .",
    "we see this in the tables when @xmath0 equals @xmath148 and @xmath149 .",
    "but when @xmath140 is larger , the mcd part becomes singular and hence more weight @xmath38 is put on the target matrix , so the regularization makes the mrcd scatter matrix well - conditioned .",
    "interestingly , for @xmath150 of outliers the choice @xmath151 is too high , and in that case we see that the mrcd actually has a lower mse for @xmath152 than for @xmath153 .",
    "this is due to the fact that outliers further away induce higher @xmath38 values .",
    "the main alternative for the mrcd in high - dimensional scatter matrix estimation is the ogk method of @xcite .",
    "as can be seen in appendix b , the ogk estimator does not result from optimizing a explicit objective function like the m(r)cd approach .",
    "nevertheless it often works well in practice .",
    "the ogk estimator is listed in the bottom line of each panel in tables 1 and 2 .",
    "we see that the mse of the mrcd with @xmath154 is roughly similar to that of the ogk for alyz data , whereas for the factor data the mrcd estimator substantially outperforms the ogk .    in conclusion ,",
    "the simulation study confirms that the mrcd is a good method for estimating location and scatter in high dimensions .",
    "it only regularizes when needed . when @xmath3 is less than @xmath7 and the number of clean observations , the resulting @xmath38 is typically less than 0.05 , implying that the mrcd strikes a balance between being similar to the mcd for tall data and achieving a well - conditioned estimate in the case of fat data .",
    "we illustrate the mrcd on two datasets with low @xmath155 , so using the original mcd is not indicated .",
    "the octane data set described in @xcite consists of near - infrared absorbance spectra with @xmath156 wavelengths collected on @xmath157 gasoline samples .",
    "it is known that the samples 25 , 26 , 36 , 37 , 38 and 39 are outliers which contain added ethanol @xcite .",
    "of course , in most applications the number of outliers is not known in advance hence it is not obvious to set the subset size @xmath3 . the choice of @xmath3 matters because increasing @xmath3 improves the efficiency at uncontaminated data but hurts the robustness to outliers .",
    "our recommended default choice is @xmath154 , safeguarding the mrcd covariance estimate against up to @xmath158 of outliers .",
    "alternatively , one could employ a data - driven approach to select @xmath3 .",
    "it consists of computing the mrcd for a range of @xmath3 values , and looking for an important change in the objective function or the estimates at some value of @xmath3 .",
    "this is not too hard , since we only need to obtain the initial estimates @xmath104 once .",
    "figure [ fig : octaneh ] plots the mrcd objective function ( [ subset2star1 ] ) for each value of @xmath3 , while figure [ fig : octanedisth ] shows the frobenius distance between the values of the mrcd scatter estimates of the standardized data ( _ i.e. _ , @xmath159 , as defined in ( [ mrcdestimates ] ) ) obtained for @xmath160 and @xmath3 .",
    "both figures clearly indicate that there is an important change at @xmath161 , so we choose @xmath162 . the total computation time to produce these plots was only 12 seconds on an intel(r ) core(tm ) i7 - 5600u cpu with 2.60 ghz .",
    ".,scaledwidth=90.0% ]    .,scaledwidth=100.0% ]    .,scaledwidth=100.0% ]    we then calculate the mrcd estimator with @xmath163 , yielding @xmath164 .",
    "the condition number of the scatter matrix equals @xmath165 .",
    "figure [ octanedist ] shows the corresponding robust distances @xmath166 where @xmath167 and @xmath168 are the mrcd location and scatter estimates of ( [ mrcdestimates ] ) .",
    "the flagged outliers ( red triangles ) stand out , showing the mrcd has correctly identified the 6 samples with added ethanol .",
    "@xcite regress the murder rate per 100,000 residents in the @xmath169 states of the us in 1980 on 25 demographic predictors , and mention that graphical tools reveal one clear outlier .",
    "the data can be retrieved from _",
    "svaelst / software / rlars.html_@xmath170 .    for lower - dimensional data",
    ", @xcite applied the mcd estimator to the response(s ) and predictors together to robustly estimate a multivariate regression . here",
    "we investigate whether for high - dimensional data the same type of analysis can be carried out based on the mrcd . in the murder rate data",
    "this yields a total of 26 variables .    as for the octane data ,",
    "we compute the mrcd estimates for the candidate range of @xmath3 . in figure",
    "[ fig : regh ] we see a big jump in the objective function when going from @xmath171 to @xmath172 .",
    "but in the plot of the frobenius distance between successive mrcd scatter matrices ( figure [ fig : regdisth ] ) we see evidence of four outliers , which lead to a substantial change in the mrcd when included in the subset .     and @xmath3.,scaledwidth=100.0% ]     and @xmath3.,scaledwidth=100.0% ]    as a conservative choice we set @xmath173 , which allows for up to 5 outliers .",
    "we then partition the mrcd scatter matrix on all 26 variables as follows : @xmath174 where @xmath175 stands for the vector of predictors and @xmath176 is the response variable . the resulting estimate of the slope vector is then @xmath177        the resulting mrcd and ols slope coefficients are shown in figure [ fig : beta ] . for most variables the coefficients are similar , except for the variable `` ph '' corresponding to the number of telephones per 100 residents , for which the mrcd coefficient ( -1.55 ) is about three times the ols coefficient ( -0.48 ) .",
    "the telephone density might serve as a proxy for the technological level of the state .",
    "a negative coefficient then indicates that on average the more technologically advanced the state , the lower the murder rate , other things being equal ( which they rarely are ) .",
    "figure [ fig : datavars ] plots the murder rate against the phone density . in this scatter plot ,",
    "arkansas and nevada appear as outliers .",
    "arkansas is a bad leverage point : it has the lowest phone density ( in 1980 ) but an average murder rate .",
    "nevada is a vertical outlier , as it lies above the downward sloping regression line fitting the bulk of the data , meaning that its murder rate is higher than one would expect on the basis of the phone density alone .",
    "the red triangles are the observations not included in the mrcd subset .",
    "we see that mrcd regression on all predictors has effectively flagged arkansas and nevada . omitting them , as implicitly done in the mrcd regression ,",
    "has led to a more negative value of this slope .",
    ".,scaledwidth=85.0% ]    finally , we note that mrcd regression can be plugged into existing robust algorithms for variable selection , which avoids the limitation mentioned in @xcite that `` a robust fit of the _ full _ model may not be feasible due to the numerical complexity of robust estimation when [ the dimension ] @xmath178 is large ( e.g. , @xmath178 @xmath179 200 ) or simply because @xmath178 exceeds the number of cases , @xmath126 . ''",
    "the mrcd could be used in such situations because its computation remains feasible in higher dimensions .",
    "in this paper we generalize the minimum covariance determinant estimation approach of @xcite to higher dimensions , by regularizing the sample covariance matrices of subsets before minimizing their determinant . the resulting minimum regularized covariance determinant ( mrcd )",
    "estimator is well - conditioned by construction , even when @xmath180 , and preserves the good robustness of the mcd .",
    "we were able to construct a fast algorithm for the mrcd by generalizing the c - step used by the mcd , and proving that this generalized c - step is guaranteed to reduce the covariance determinant .",
    "we verified the performance of the mrcd estimator in an extensive simulation study including both clean and contaminated data .",
    "the simulation study also confirms that the mrcd can be interpreted as a generalization of the mcd , because when @xmath126 is sufficiently large compared to @xmath7 and the mcd is well - conditioned the regularization parameter in mrcd becomes zero so the mrcd estimate coincides with the mcd .",
    "finally , we illustrated the use of the mrcd for outlier detection and robust regression on two fat data applications from chemistry and criminology , for which @xmath181 .",
    "we believe that the mrcd is a valuable addition to the tool set for robust multivariate analysis , especially in high dimensions .",
    "we look forward to further research on its use in principal component analysis where the original mcd has proved useful  @xcite , and analogously in factor analysis  @xcite , classification  @xcite , clustering  @xcite , multivariate regression  @xcite , penalized maximum likelihood estimation @xcite and other multivariate techniques . a further research topic is to study the finite sample distribution of the robust distances computed from the mrcd .",
    "our experiments have shown that the usual chi - square and f - distribution results for the mcd distances @xcite are no longer good approximations when @xmath7 is large relatively to @xmath126 .",
    "a better approximation would be useful for improving the accuracy of the mrcd by reweighting .",
    "rousseeuw , p. ( 1985 ) .",
    "multivariate estimation with high breakdown point . in w.  grossmann , g.  pflug , i.  vincze , and w.  wertz ( eds . ) , _ mathematical statistics and applications , vol .",
    "b _ , pp .   283297 . reidel publishing company , dordrecht .                      0.4 in * appendix a : proof of theorem [ cstep ] * 0.1 in generate a @xmath7-variate sample @xmath182 with @xmath183 points for which @xmath184 is nonsingular and@xmath185 .",
    "then @xmath186 has mean zero and covariance matrix @xmath76 .",
    "now compute @xmath187 , hence @xmath188 has mean zero and covariance matrix @xmath37 .",
    "next , create the artificial dataset @xmath189 with @xmath190 points , where @xmath191 are the members of @xmath88 .",
    "the factors @xmath192 are given by @xmath193 the mean and covariance matrix of @xmath194 are then @xmath195 and @xmath196 the regularized covariance matrix @xmath197 is thus the actual covariance matrix of the combined data set @xmath198 .",
    "analogously we construct @xmath199 where @xmath200 are the members of @xmath201 .",
    "@xmath202 has zero mean and covariance matrix @xmath203 .      the first inequality ( [ eq : a2 ] )",
    "can be shown as follows .",
    "put @xmath206 and @xmath207 and note that @xmath208 is the average of the @xmath209",
    ". then ( [ eq : a2 ] ) becomes @xmath210 which follows from the fact that @xmath211 is the unique minimizer of the least squares objective @xmath212 , so ( [ eq : a2 ] ) becomes an equality if and only if @xmath213 which is equivalent to @xmath101 .",
    "+ it follows that @xmath214 now put @xmath215 if we now compute distances relative to @xmath216 , we find @xmath217 from the theorem in @xcite , it follows that @xmath218 is the unique minimizer of @xmath219 among all @xmath220 for which @xmath221 ( note that the mean of @xmath222 is zero ) .",
    "therefore @xmath223 we can only have @xmath224 if both of these inequalities are equalities .",
    "for the first , by uniqueness we can only have equality if @xmath225 .",
    "for the second inequality , equality holds if and only if @xmath226 . combining both yields @xmath102 .",
    "moreover , @xmath226 implies that ( [ eq : a2 ] ) becomes an equality , hence @xmath101 .",
    "this concludes the proof of theorem [ cstep ] .",
    "+ 0.2 in * appendix b : the ogk estimator * + @xcite presented a general method to obtain positive definite and approximately affine equivariant robust scatter matrices starting from a robust bivariate scatter measure .",
    "this method was applied to the bivariate covariance estimate of  @xcite . the resulting multivariate location and scatter estimates are called orthogonalized gnanadesikan - kettenring ( ogk ) estimates and are calculated as follows :    1 .",
    "let @xmath227 and @xmath228 be robust univariate estimators of location and scale .",
    "2 .   construct @xmath229 for @xmath12 with @xmath230 .",
    "3 .   compute the ` pairwise correlation matrix ' @xmath39 of the variables of @xmath231 , given by @xmath232 .",
    "this @xmath39 is symmetric but not necessarily positive definite .",
    "4 .   compute the matrix @xmath233 of eigenvectors of @xmath39 and 1 .",
    "project the data on these eigenvectors , i.e. @xmath234 ; 2 .",
    "compute ` robust variances ' of @xmath235 , i.e. @xmath236 ; 3 .",
    "set the @xmath237 vector @xmath238 where @xmath239 , and compute the positive definite matrix @xmath240 .",
    "5 .   transform back to @xmath73 , i.e. @xmath241 and @xmath242 .    in the ogk algorithm",
    "@xmath227 is a weighted mean and @xmath228 is a 1-step m - estimator of scale .",
    "step 2 makes the estimate location invariant and scale equivariant , whereas the next steps replace the eigenvalues of @xmath39 ( some of which may be negative ) by positive numbers .",
    "( should some of the @xmath243 be zero , we could replace them by small positive numbers . )"
  ],
  "abstract_text": [
    "<S> the minimum covariance determinant ( mcd ) approach estimates the location and scatter matrix using the subset of given size with lowest sample covariance determinant . </S>",
    "<S> its main drawback is that it can not be applied when the dimension exceeds the subset size . </S>",
    "<S> we propose the minimum regularized covariance determinant ( mrcd ) approach , which differs from the mcd in that the subset - based covariance matrix is a convex combination of a target matrix and the sample covariance matrix . </S>",
    "<S> a data - driven procedure sets the weight of the target matrix , so that the regularization is only used when needed . </S>",
    "<S> the mrcd estimator is defined in any dimension , is well - conditioned by construction and preserves the good robustness properties of the mcd . </S>",
    "<S> we prove that so - called concentration steps can be performed to reduce the mrcd objective function , and we exploit this fact to construct a fast algorithm . </S>",
    "<S> we verify the accuracy and robustness of the mrcd estimator in a simulation study and illustrate its practical use for outlier detection and regression analysis on real - life high - dimensional data sets in chemistry and criminology .    </S>",
    "<S> # 1    0    0    1    0    * the minimum regularized covariance determinant estimator *    _ keywords : _ breakdown point ; high - dimensional data ; regularization ; robust covariance estimation . </S>"
  ]
}