{
  "article_text": [
    "in a semiparametric model that is parameterized by two types of parameters  a finite - dimensional euclidean parameter and an infinite - dimensional parameter  oftentimes the infinite - dimensional parameter is considered as a nuisance parameter , and the two parameters are _ separated_. in many interesting statistical models , however , the parameter of interest and the nuisance parameter are _ bundled together _ , a terminology used by huang and wellner @xcite when they reviewed the linear models under interval censoring , which means that the infinite - dimensional parameter is an unknown function of the parameter of interest .",
    "for example , in a linear regression model for censored survival data , the unspecified error distribution function , often treated as a nuisance parameter , is a function of the regression coefficients .",
    "other examples include the single index model and the cox regression model with an unspecified link function .",
    "there is a rich literature of asymptotic distributional theories for m - estimation in a variety of semiparametric models with well - separated parameters ; see , for example , @xcite , among many others .",
    "though many methodologies of m - estimation for bundled parameters have been proposed in the literature , general asymptotic distributional theories for such problems are still lacking .",
    "the only estimation theories for bundled parameters we are aware of are the sieve generalized method of moment of @xcite and the estimating equation approach of @xcite .    in this article",
    ", we consider an extension of existing asymptotic distributional theories to accommodate situations where the estimation criteria are parameterized with bundled parameters .",
    "the proposed theory has similar flavor of theorem 2 in  @xcite , but they are different because the latter requires an existing uniform consistent estimator of the infinite - dimensional nuisance parameter with a convergence rate faster than @xmath0 , which is then treated as a fixed function of the parameter of interest in their estimating procedure , while we need to simultaneously estimate both parameters through a sieve parameter space ; furthermore , their existing nuisance parameter estimator needs to satisfy their condition ( 2.6 ) , which is usually hard to verify when its convergence rate is slower than @xmath1 .",
    "our proposed theory is general enough to cover a wide range of problems for bundled parameters including the aforementioned single index model , the cox model with unknown link function and a linear model under different censoring mechanisms .",
    "rigorous proofs for each of the models , however , will take lengthy derivations .",
    "we only use the efficient estimation in the semiparametric linear regression model with right censored data as an illustrative example that motivates such a theoretical development and will present results for other models elsewhere .",
    "note that the considered example can not be directly put into the framework of restricted moments due to right censoring , thus can not be handled by the method of @xcite .",
    "suppose that the failure time transformed by a known monotone transformation is linearly related to a set of covariates , where the failure time is subject to right censoring .",
    "let @xmath2 denote the transformed failure time and  @xmath3 denote the transformed censoring time by the same transformation for subject @xmath4 , @xmath5 .",
    "let @xmath6 and @xmath7 .",
    "then the semiparametric linear model we consider here can be written as @xmath8 where the errors @xmath9 are independent and identically distributed ( i.i.d . ) with an unspecified distribution .",
    "when the failure time is log - transformed , this model corresponds to the well - known accelerated failure time model @xcite .",
    "here we assume that @xmath10 , @xmath5 , are i.i.d . and independent of @xmath9 .",
    "this is a common assumption for linear models with censored survival data , which is particularly needed in @xcite to derive the efficient score function for  @xmath11 .",
    "such an assumption , however , is stronger than necessary in the usual linear regression without censoring , for which the error is only required to be uncorrelated with covariates ; see , for example , @xcite . we also avoid trivial transformations such as @xmath12 so that we always have @xmath13 s bounded from below .    the semiparametric linear regression model relates the failure time to the covariates directly .",
    "it provides a straightforward interpretation of the data and serves as an attractive alternative to the cox model @xcite in many applications .",
    "several estimators of the regression parameters have been proposed in the literature since late 1970s , including the rank - based estimators ( see , e.g. , @xcite ) and the buckley ",
    "james estimator ( see , e.g. , @xcite ) .",
    "there are two major challenges in the estimation for such a  linear model : ( 1 ) the estimating functions in the aforementioned methods are discrete , leading to potential multiple solutions as well as numerical difficulties ; ( 2 ) none of the aforementioned methods is efficient .",
    "recently , zeng and lin @xcite developed a kernel - smoothed profile likelihood estimating procedure for the accelerated failure time model . in this article , we consider a  sieve maximum likelihood approach for model ( [ eqmodel1 ] ) for censored data .",
    "the proposed approach is much intuitive , easy to implement numerically and asymptotically efficient .",
    "it is easy to see that @xmath14 and @xmath15 are independent conditional on @xmath16 under the assumption @xmath17 .",
    "hence the joint density function of @xmath18 can be written as @xmath19 where @xmath20 is the true cumulative hazard function for the error term @xmath21 and  @xmath22 is its derivative .",
    "@xmath23 only depends on the conditional distribution of @xmath15 given @xmath16 and the marginal distribution of @xmath16 , and is free of @xmath11 and  @xmath24 . to simplify the notation",
    ", we will ignore the factor @xmath25 from the likelihood function . then for i.i.d .",
    "observations @xmath26 , @xmath5 , from  ( [ eqjdf ] ) we obtain the log likelihood function for @xmath27 and @xmath28 as @xmath29 the log likelihood given in ( [ eqllk1 ] ) apparently is a semiparametric model , where the argument of the nuisance parameter @xmath28 involves @xmath27 ; thus @xmath27 and @xmath28 are bundled parameters .",
    "to keep the positivity of @xmath28 , let @xmath30 .",
    "then the log likelihood function for @xmath27 and @xmath31 , using the counting process notation , can be written as @xmath32 where @xmath33 is the counting process for subject @xmath4 .",
    "we propose a new approach by directly maximizing the log likelihood function in a sieve space in which function @xmath34 is approximated by b - splines .",
    "numerically , the estimator can be easily obtained by the newton  raphson algorithm or any gradient - based search algorithms .",
    "we show that the proposed estimator is consistent and asymptotically normal , and the limiting covariance matrix reaches the semiparametric efficiency bound , which can be estimated either by inverting the information matrix based on the efficient score function of the regression parameters derived by @xcite , or by inverting the observed information matrix of all parameters , taking into account that we are also estimating the nuisance parameters in the sieve space for the log hazard function .",
    "in this section , we extend the general theorem introduced by @xcite , which deals with the asymptotic normality of semiparametric m - estimators of regression parameters when the convergence rate of the estimator for nuisance parameters can be slower than @xmath1 . in their theorem , the parameters of interest and the nuisance parameters are assumed to be separated .",
    "we consider a more general setting where the nuisance parameter can be a function of the parameters of interest .",
    "the theorem is crucial in the proof of asymptotic normality given in theorem  [ thmnormality ] for our proposed estimators .",
    "some empirical process notation will be used from now on .",
    "we denote @xmath35 and @xmath36 , where @xmath37 is a probability measure , and @xmath38 is an empirical probability measure , and denote @xmath39 .",
    "given i.i.d .",
    "observations @xmath40 , we estimate the unknown parameters @xmath41 by maximizing an objective function for @xmath41 , @xmath42 , where @xmath27 is the parameter of interest , and @xmath43 is the nuisance parameter that can be a function of @xmath27 . here",
    "`` @xmath44 '' denotes the other arguments of @xmath45 besides @xmath27 , which can be some components of @xmath46 .",
    "if the objective function @xmath47 is the log - likelihood function of a single observation , then the estimator becomes the semiparametric maximum likelihood estimator .",
    "here we adopt similar notation in  @xcite .",
    "let @xmath48 , @xmath49 and @xmath50 , where @xmath51 is the parameter space of  @xmath27 , and @xmath52 is a class of functions mapping from @xmath53 to @xmath54 .",
    "let @xmath55 be the parameter space of @xmath56 .",
    "define a distance between @xmath57 by @xmath58 where is the euclidean distance , and @xmath59 is some norm .",
    "let @xmath60 be the sieve parameter space , a sequence of increasing subsets of the parameter space @xmath61 growing dense in @xmath61 as @xmath62 .",
    "we aim to find @xmath63 such that @xmath64 and @xmath65 is asymptotically normal .    for any fixed @xmath66 , let @xmath67 be a  smooth curve in @xmath52 running through @xmath68 at @xmath69 , that is , @xmath70 .",
    "assume all @xmath71 are at least twice - differentiable with respect to @xmath27 , and denote @xmath72 assume the objective function @xmath47 is twice frechet differentiable .",
    "since for a small  @xmath73 , we have @xmath74 , here @xmath75 ; then by the definition of functional derivatives it follows that @xmath76 \\\\ & & \\qquad\\quad { } + \\dot{m}_2(\\beta,\\zeta(\\cdot,\\beta);z)[\\dot{\\zeta } _ { \\beta}(\\cdot,\\beta ) ] \\\\ & & \\qquad = \\dot{m}_2(\\beta,\\zeta(\\cdot,\\beta);z)[\\dot{\\zeta}_{\\beta } ( \\cdot,\\beta)],\\end{aligned}\\ ] ] where the subscript 2 indicates that the derivatives are taken with respect to the second argument of the function .",
    "the last equality holds because @xmath77=0.\\ ] ] similarly we have @xmath78 -\\dot{m}_2(\\beta,\\zeta ( \\cdot,\\beta);z)[h(\\cdot,\\beta ) ] \\bigr\\ } \\\\ & & \\qquad = \\ddot{m}_{22}(\\beta,\\zeta(\\cdot,\\beta);z)[h(\\cdot,\\beta ) , \\dot{\\zeta}_{\\beta}(\\cdot,\\beta)]\\end{aligned}\\ ] ] and @xmath79 - \\dot{m}_2(\\beta,\\zeta(\\cdot,\\beta);z)[h(\\cdot,\\beta ) ] \\ } \\\\ & & \\qquad = \\dot{m}_2(\\beta,\\zeta(\\cdot,\\beta);z)[\\dot{h}_{\\beta}(\\cdot , \\beta)].\\end{aligned}\\ ] ] thus according to the chain rule of the functional derivatives , we have @xmath80,\\hspace*{-22pt}\\\\ \\dot{m}_{\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[h ] & = & \\frac{\\partial m(\\beta,(\\zeta+\\eta h)(\\cdot,\\beta);z)}{\\partial\\eta}\\bigg|_{\\eta=0 } \\\\ & = & \\dot{m}_2(\\beta,\\zeta(\\cdot,\\beta);z)[h(\\cdot,\\beta)],\\end{aligned}\\ ] ] @xmath81\\\\ & & { } + \\ddot{m}_{21}(\\beta,\\zeta(\\cdot,\\beta);z)[\\dot{\\zeta}_{\\beta } ( \\cdot,\\beta ) ] \\\\ & & { } + \\ddot{m}_{22}(\\beta,\\zeta(\\cdot,\\beta);z)[\\dot{\\zeta}_{\\beta } ( \\cdot,\\beta),\\dot{\\zeta}_{\\beta}(\\cdot,\\beta ) ] \\\\ & & { } + \\dot{m}_{2}(\\beta,\\zeta(\\cdot,\\beta);z)[\\ddot{\\zeta}_{\\beta \\beta}(\\cdot,\\beta ) ] , \\\\ \\ddot{m}_{\\beta\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[h ] & = & \\frac { \\partial\\dot{m}_{\\beta}(\\beta,(\\zeta+\\eta h)(\\cdot,\\beta ) ; z)}{\\partial\\eta } \\bigg\\vert_{\\eta=0 } \\\\ & = & \\ddot{m}_{12}(\\beta,\\zeta(\\cdot,\\beta);z)[h(\\cdot,\\beta ) ] \\\\ & & { } + \\ddot{m}_{22}(\\beta,\\zeta(\\cdot,\\beta);z)[\\dot{\\zeta}_{\\beta } ( \\cdot,\\beta),h(\\cdot,\\beta ) ] \\\\ & & { } + \\dot{m}_{2}(\\beta,\\zeta(\\cdot,\\beta);z)[\\dot{h}_\\beta(\\cdot , \\beta ) ] , \\\\ \\ddot{m}_{\\zeta\\beta}(\\beta,\\zeta(\\cdot,\\beta);z)[h ] & = & \\frac { \\partial\\dot{m}_2(\\beta,\\zeta(\\cdot,\\beta);z)[h(\\cdot,\\beta ) ] } { \\partial\\beta } \\\\ & = & \\ddot{m}_{21}(\\beta,\\zeta(\\cdot,\\beta);z)[h(\\cdot,\\beta ) ] \\\\ & & { } + \\ddot{m}_{22}(\\beta,\\zeta(\\cdot,\\beta);z)[h(\\cdot,\\beta ) , \\dot{\\zeta}_{\\beta}(\\cdot,\\beta ) ] \\\\ & & { } + \\dot{m}_{2}(\\beta,\\zeta(\\cdot,\\beta);z)[\\dot{h}_\\beta(\\cdot , \\beta ) ] , \\\\ \\ddot{m}_{\\zeta\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[h_1,h_2 ] & = & \\ddot{m}_{22}(\\beta,\\zeta(\\cdot,\\beta);z)[h_1(\\cdot,\\beta),h_2(\\cdot , \\beta)].\\end{aligned}\\ ] ]    as noted before , the subscript 1 or 2 in the derivatives indicates that the derivatives are taken with respect to the first or the second argument of the function , and @xmath82 inside the square brackets is a function denoting the direction of the functional derivative with respect to @xmath45 .",
    "note that for the second derivatives @xmath83 and",
    "@xmath84 , we implicitly require the direction @xmath82 to be a differentiable function with respect to @xmath85 .",
    "it is easily seen that when @xmath45 is free of @xmath27 , all the above derivatives reduce to that in  @xcite .",
    "following @xcite , we also define @xmath86 & = & p\\dot{m}_{\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[h ] , \\\\",
    "\\dot{s}_{\\beta , n}(\\beta,\\zeta(\\cdot,\\beta ) ) & = & \\mathbb{p}_n \\dot { m}_{\\beta}(\\beta,\\zeta(\\cdot,\\beta);z ) , \\\\",
    "\\dot{s}_{\\zeta , n}(\\beta,\\zeta(\\cdot,\\beta))[h ] & = & \\mathbb{p}_n \\dot { m}_{\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[h ] ,",
    "\\\\ \\ddot{s}_{\\beta\\beta}(\\beta,\\zeta(\\cdot,\\beta ) ) & = & p\\ddot{m}_{\\beta \\beta}(\\beta,\\zeta(\\cdot,\\beta);z ) , \\\\ \\ddot{s}_{\\zeta\\zeta}(\\beta,\\zeta(\\cdot,\\beta))[h , h ] & = & p\\ddot { m}_{\\zeta\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[h , h]\\end{aligned}\\ ] ] and",
    "@xmath87 = \\ddot{s}_{\\zeta \\beta}'(\\beta,\\zeta(\\cdot,\\beta))[h ] = p \\ddot{m}_{\\beta\\zeta}(\\beta , \\zeta(\\cdot,\\beta);z)[h].\\ ] ] furthermore , for @xmath88 , we denote @xmath89 & = & ( \\dot{m}_{\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[h_1],\\ldots , \\dot { m}_{\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[h_d ] ) ' , \\\\",
    "\\ddot{m}_{\\beta\\zeta}(\\beta,\\zeta(\\cdot,\\beta ) ; z)[\\mathbf{h } ] & = & ( \\ddot{m}_{\\beta\\zeta}(\\beta,\\zeta(\\cdot,\\beta ) ; z)[h_1],\\ldots , \\ddot{m}_{\\beta\\zeta}(\\beta,\\zeta(\\cdot,\\beta ) ; z)[h_d ] ) , \\\\",
    "\\ddot{m}_{\\zeta\\beta}(\\beta,\\zeta(\\cdot,\\beta ) ; z)[\\mathbf{h } ] & = & ( \\ddot{m}_{\\zeta\\beta}(\\beta,\\zeta(\\cdot,\\beta ) ; z)[h_1],\\ldots , \\ddot{m}_{\\zeta\\beta}(\\beta,\\zeta(\\cdot,\\beta ) ; z)[h_d ] ) ' , \\\\",
    "\\ddot{m}_{\\zeta\\zeta}(\\beta,\\zeta(\\cdot,\\beta ) ; z)[\\mathbf{h},h ] & = & ( \\ddot{m}_{\\zeta\\zeta}(\\beta,\\zeta(\\cdot,\\beta ) ; z)[h_1,h],\\ldots,\\\\ & & \\hspace*{26.2pt}\\ddot{m}_{\\zeta\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[h_d , h])'\\end{aligned}\\ ] ] and define correspondingly @xmath90 & = & p \\dot { m}_{\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[\\mathbf{h } ] , \\\\",
    "\\dot{s}_{\\zeta , n}(\\beta,\\zeta(\\cdot,\\beta))[\\mathbf{h } ] & = & \\mathbb { p}_n \\dot{m}_{\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[\\mathbf{h } ] , \\\\ \\ddot{s}_{\\beta\\zeta}(\\beta,\\zeta(\\cdot,\\beta))[\\mathbf{h } ] & = & p \\ddot { m}_{\\beta\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[\\mathbf{h } ] , \\\\ \\ddot{s}_{\\zeta\\beta } ( \\beta,\\zeta(\\cdot,\\beta))[\\mathbf{h } ] & = & p \\ddot{m}_{\\zeta\\beta}(\\beta,\\zeta(\\cdot,\\beta);z)[\\mathbf{h } ] , \\\\ \\ddot{s}_{\\zeta\\zeta}(\\beta,\\zeta(\\cdot,\\beta))[\\mathbf{h},h ] & = & p \\ddot{m}_{\\zeta\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[\\mathbf{h},h].\\end{aligned}\\ ] ]    to obtain the asymptotic normality result for the sieve m - estimator @xmath91 , the assumptions we will make in the following look similar to those in @xcite , but all the derivatives with respect to @xmath27 involve the chain rule and hence are more complicated , which is the key difference to @xcite . additionally , we focus on sieve estimators in the sieve parameter space . we list the following assumptions :    ( rate of convergence ) for an estimator @xmath92 and the true parameter @xmath93 , @xmath94 for some .",
    "@xmath95 and @xmath96=0 $ ] for all @xmath97 .",
    "( positive information ) there exists an @xmath98 , where for @xmath99 , such that @xmath100-\\ddot { s}_{\\zeta\\zeta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0))[\\mathbf{h}^*,h ] = 0\\ ] ] for all @xmath97 .",
    "furthermore , the matrix @xmath101 \\\\ & = & - p\\{\\ddot{m}_{\\beta\\beta}(\\beta_0,\\zeta_0(\\cdot,\\beta _ 0);z)-\\ddot{m}_{\\zeta\\beta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z)[\\mathbf { h}^*]\\}\\end{aligned}\\ ] ] is nonsingular .",
    "the estimator @xmath102 satisfies @xmath103 = o_p(n^{-1/2}).\\end{aligned}\\ ] ]    ( stochastic equicontinuity ) for some @xmath104 , @xmath105 and @xmath106 \\\\ & & \\hspace*{54pt}\\qquad { } - \\sqrt{n}(\\dot { s}_{\\zeta , n}-\\dot{s}_\\zeta)(\\beta_0,\\zeta_0(\\cdot,\\beta_0))[\\mathbf { h}^*(\\cdot,\\beta_0)]\\bigr\\vert = o_p(1).\\end{aligned}\\ ] ]    ( smoothness of the model ) for some @xmath107 satisfying @xmath108 , and for @xmath56 in a neighborhood of @xmath109 , @xmath110\\vert\\\\ & & \\qquad= o(d^\\alpha ( \\theta,\\theta_0))\\end{aligned}\\ ] ] and @xmath111 -\\dot{s}_\\zeta(\\beta_0,\\zeta_0(\\cdot,\\beta _ 0))[\\mathbf{h}^*(\\cdot,\\beta_0 ) ] \\\\ & & \\quad { } - \\ddot{s}_{\\zeta\\beta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0))[\\mathbf { h}^*(\\cdot,\\beta_0)](\\beta-\\beta_0 ) \\\\ & & \\hspace*{5pt}\\quad { } - \\ddot{s}_{\\zeta\\zeta } ( \\beta_0,\\zeta_0(\\cdot,\\beta_0))[\\mathbf{h}^*(\\cdot,\\beta_0),\\zeta ( \\cdot,\\beta)-\\zeta_0(\\cdot,\\beta_0 ) ] \\vert\\\\ & & \\hspace*{5pt}\\qquad= o(d^\\alpha(\\theta , \\theta_0)).\\end{aligned}\\ ] ]    note that @xmath112 in ( a1 ) depends on the entropy of the sieve parameter space for  @xmath45 and can not be arbitrarily small ; it is controlled by the smoothness of the model in ( a6 ) . the convergence rate in ( a1 )",
    "needs to be achieved prior to obtaining asymptotic normality .",
    "assumption ( a2 ) is a common assumption for the maximum likelihood estimation and usually holds .",
    "the direction @xmath113 in ( a3 ) may be found through the equation in ( a3 ) .",
    "it is the least favorable direction when @xmath47 is the likelihood function .",
    "assumptions ( a4 ) and ( a5 ) are usually verified either by the donsker property or the maximal inequality of  @xcite .",
    "assumption ( a6 ) can be obtained by a taylor expansion .",
    "the following theorem is an extension to theorem 6.1 in @xcite when the infinite - dimensional parameter @xmath45 is a function of the finite - dimensional parameter  @xmath27 .",
    "[ thmgenthm ] suppose that assumptions hold .",
    "then @xmath114 where @xmath115 , \\\\ b & = & p \\ { m^*(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z)^{\\otimes2}\\},\\end{aligned}\\ ] ] and",
    "@xmath116 is given in assumption . here",
    "@xmath117 .",
    "the proof follows similarly along the proof of theorem 6.1 in  @xcite .",
    "assumptions ( a1 ) and ( a5 ) yield @xmath118 since @xmath119 by ( a4 ) and @xmath120 by  ( a2 ) , we have @xmath121 similarly , @xmath122 + \\sqrt{n}\\dot{s}_{\\zeta , n}(\\beta_0,\\zeta_0(\\cdot,\\beta_0))[\\mathbf{h}^*(\\cdot,\\beta_0 ) ] = o_p(1).\\ ] ] combining these equalities and assumption ( a6 ) yields @xmath123 \\nonumber\\\\ & & \\quad { } + \\dot{s}_{\\beta , n}(\\beta_0,\\zeta_0(\\cdot,\\beta_0 ) ) + o(d^\\alpha ( \\hat\\theta_n , \\theta_0 ) ) \\\\ & & \\qquad = o_p(n^{-1/2})\\nonumber\\end{aligned}\\ ] ] and @xmath124(\\hat{\\beta}_n-\\beta_0)\\nonumber\\\\ & & \\quad{}+\\ddot { s}_{\\zeta\\zeta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0))[\\mathbf{h}^*(\\cdot , \\beta_0 ) ,   \\hat{\\zeta}_n(\\cdot,\\hat{\\beta}_n ) - \\zeta_0(\\cdot,\\beta_0)]\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\quad { } + \\dot { s}_{\\zeta , n}(\\beta_0,\\zeta_0(\\cdot,\\beta_0))[\\mathbf{h}^*(\\cdot,\\beta _ 0 ) ] + o(d^\\alpha(\\hat\\theta_n , \\theta_0 ) ) \\nonumber\\\\ & & \\qquad = o_p(n^{-1/2}).\\nonumber\\end{aligned}\\ ] ] since @xmath107 with @xmath108 , the rate of convergence assumption ( a1 ) implies @xmath125 , then ( [ eqgenthm1 ] ) and ( [ eqgenthm2 ] ) together with ( a3 ) yields @xmath126\\bigr)(\\hat{\\beta}_n-\\beta_0 ) \\\\ & & \\qquad = - \\bigl(\\dot{s}_{\\beta , n}(\\beta_0,\\zeta_0(\\cdot,\\beta_0))-\\dot { s}_{\\zeta , n}(\\beta_0,\\zeta_0(\\cdot,\\beta_0))[\\mathbf{h}^*(\\cdot,\\beta _ 0)]\\bigr)+o_p(n^{-1/2}),\\end{aligned}\\ ] ] that is , @xmath127 this yields @xmath128",
    "by taking logarithm to the positive function @xmath129 in ( [ eqllk1 ] ) , the function @xmath34 in ( [ eqllk2 ] ) is no longer restricted to be positive , which eases the estimation .",
    "we now describe the spline - based sieve maximum likelihood estimation for model ( [ eqmodel1 ] ) . under the regularity conditions ( c.1)(c.3 )",
    "stated in section  [ secasymp ] , we know that the observed residual times @xmath130 are confined in some finite interval .",
    "let @xmath131 $ ] be an interval of interest , where @xmath132 .",
    "let @xmath133 be a set of partition points of @xmath131 $ ] with @xmath134 and @xmath135 for some constant @xmath136 .",
    "let @xmath137 be the space of polynomial splines of order @xmath138 defined in @xcite , definition 4.1 .",
    "according to schumaker ( @xcite , corollary  4.10 ) , there exists a set of b - spline basis functions @xmath139 with @xmath140 such that for any @xmath141 , we can write @xmath142 where we follow @xcite by requiring @xmath143 that is allowed to grow with @xmath144 slowly enough . let @xmath145 .",
    "under suitable smoothness assumptions , @xmath146 can be well approximated by some function in @xmath137 .",
    "therefore , we seek a member of @xmath137 together with a value of @xmath147 that maximizes the log likelihood function .",
    "specifically , let @xmath148 be the value that maximizes @xmath149\\\\[-7pt ] & & \\hspace*{36pt } { } - \\int i(y_i\\geq t ) \\exp\\biggl\\{\\sum_{j=1}^{q_n } \\gamma_j b_j(t - x_i'\\beta)\\biggr\\ } \\,dt\\biggr].\\nonumber\\end{aligned}\\ ] ] taking the first order derivatives of @xmath150 with respect to @xmath27 and @xmath151 and setting them to zero , we can obtain the score equations . since the integrals here are univariate integrals , their numerical implementation can be easily done by the one - dimensional gaussian - quadrature method .",
    "a newton  raphson algorithm or any other gradient - based search algorithm can be applied to solve the score equations for all parameters @xmath152 , for example , @xmath153 where @xmath154 is the parameter estimate from the @xmath47th iteration , and @xmath155 are the score function and hessian matrix of parameter @xmath56 . for any fixed @xmath27 and @xmath144 ,",
    "it is clearly seen that @xmath156 in ( [ eqllk3 ] ) is concave with respect to @xmath151 and goes to @xmath157 if any @xmath158 approaches either @xmath159 or @xmath157 ; hence @xmath160 must be bounded which yields an estimator of @xmath161 in @xmath137 .",
    "as stated in the next section , the distribution of @xmath91 can be approximated by a normal distribution .",
    "one way to estimate the variance matrix of @xmath91 is to approximate the ( inverse of the ) information matrix based on the efficient score function for @xmath162 by plugging in the estimated parameters  @xmath163 .",
    "the consistency of such a variance estimator is given in theorem  [ thmvariance ] .",
    "another way is to invert the observed information matrix from the last newton ",
    "raphson iteration , taking into account that we are also estimating the nuisance parameter @xmath151 .",
    "the consistency of the latter approach may be proved in a similar way as example 4 in @xcite or via theorem  2.2 in @xcite , and we leave detailed derivation to interested readers .",
    "simulations indicate that both estimators work reasonably well .",
    "denote @xmath164 and @xmath165 .",
    "we assume the following regularity conditions :    the true parameter @xmath11 belongs to the interior of a compact set @xmath166 .",
    "\\(a ) the covariate @xmath16 takes values in a bounded subset @xmath167 ; + ( b ) @xmath168 is nonsingular .",
    "there is a truncation time @xmath169 such that , for some constant @xmath73 , @xmath170 almost surely with respect to the probability measure of @xmath16 .",
    "this implies that @xmath171 .",
    "the error @xmath21 s density @xmath172 and its derivative @xmath173 are bounded and @xmath174    the conditional density of @xmath15 given @xmath16 and its derivative @xmath175 are uniformly bounded for all possible values of @xmath16 , that is , @xmath176 for all @xmath177 with some constants @xmath178 , where @xmath179 is the truncation time defined in condition ( c.3 ) .",
    "let @xmath180 denote the collection of bounded functions @xmath31 on @xmath131 $ ] with bounded derivatives @xmath181 , @xmath182 , and the @xmath183th derivative @xmath184 satisfies the following lipschitz continuity condition : @xmath185,\\ ] ] where @xmath183 is a positive integer and @xmath186 $ ] such that @xmath187 , and @xmath188 is an unknown constant .",
    "the true log hazard function @xmath146 belongs to  @xmath180 , where @xmath131 $ ] is a bounded interval .    for some @xmath189 , @xmath190 almost surely for all @xmath191",
    ".    condition ( c.1 ) is a common regularity assumption that has been imposed in the literature ; see , for example , @xcite . conditions ( c.2)(a ) , ( c.3 ) and ( c.4 ) were also assumed in @xcite .",
    "condition ( c.5 ) implies condition @xmath192 in @xcite . in condition  ( c.6 )",
    ", we require @xmath193 to provide desirable controls of the spline approximation error rates of the first and second derivatives of @xmath194 ( see corollary 6.21 of @xcite ) , which are needed in verifying assumptions .",
    "condition ( c.7 ) was also proposed for the panel count data model in  @xcite . as noted in their remark 3.4 ,",
    "this condition  ( c.7 ) can be justified in many applications when condition ( c.2)(b ) is satisfied . the bounded interval @xmath131 $ ] in ( c.6 )",
    "may be chosen as @xmath195 and @xmath196 under  , which is what we use in the following .",
    "now define the collection of functions @xmath197 as follows : @xmath198 , x \\in\\mathcal{x } , \\beta\\in\\mathcal{b } \\},\\ ] ] where @xmath199 and @xmath180 is defined in ( c.6 ) . here",
    "@xmath45 is a composite function of @xmath31 composed with  @xmath200 .",
    "note that @xmath201 .",
    "then for @xmath202 we define the following norm : @xmath203 we also have the following collection of scores : @xmath204 in which @xmath205 .    for any @xmath206 and @xmath207 in the space of @xmath208 , define the following distance : @xmath209 let @xmath210 .",
    "denote @xmath211 , x \\in\\mathcal{x } , \\beta\\in\\mathcal{b } \\}\\ ] ] and @xmath212 .",
    "clearly @xmath213 for all @xmath214 .",
    "the sieve estimator @xmath215 , where @xmath216 , is the maximizer of the empirical log - likelihood @xmath217 over the sieve space  @xmath218 .",
    "the following theorem gives the convergence rate of the proposed estimator  @xmath219 to the true parameter @xmath220 .",
    "[ thmconvrate ] let @xmath221 , where @xmath222 satisfies the restriction @xmath223 with @xmath224 being the smoothness parameter defined in condition .",
    "suppose conditions hold , and the failure time @xmath14 follows model  ( [ eqmodel1 ] ) .",
    "then @xmath225 where @xmath226 is defined in ( [ eqdistance ] ) .",
    "it is worth pointing out that the sieve space @xmath227 does not have to be restricted to the b - spline space ; it can be any sieve space as long as the estimator @xmath228 satisfies the conditions of theorem 1 in @xcite .",
    "we refer to @xcite for a comprehensive discussion of the sieve estimation for semiparametric models in general sieve spaces .",
    "our choice of the b - spline space is primarily motivated by its simplicity of numerical implementation , which is a tremendous advantage of the proposed approach over exiting numerical methods for the accelerated failure time models , in particular , the linear programming approach .",
    "we provide a proof of theorem [ thmconvrate ] in the supplementary material @xcite by checking the conditions of theorem 1 in @xcite .",
    "theorem [ thmconvrate ] implies that if @xmath229 , @xmath230 which is the optimal convergence rate in the nonparametric regression setting .",
    "although the overall convergence rate is slower than @xmath1 , the next theorem states that the proposed estimator of the regression parameter is still asymptotically normal and semiparametrically efficient .",
    "[ thmnormality ] given the following efficient score function for the censored linear model derived by @xcite : @xmath231 where @xmath232 is the failure counting process martingale , and @xmath233 was shown by @xcite .",
    "suppose that the conditions in theorem [ thmconvrate ] hold , and @xmath234 is nonsingular , then @xmath235 in distribution .",
    "the proof of theorem [ thmnormality ] is where we need to apply our general sieve m - theorem proposed in section [ secgenthm ] .",
    "we prove by checking assumptions .",
    "details are provided in section [ secproofs ] .",
    "the following theorem gives consistency of the variance estimator based on the above efficient score .",
    "[ thmvariance ] suppose the conditions in theorem [ thmnormality ] hold .",
    "denote @xmath236 where @xmath237 and @xmath238 then @xmath239 in probability .",
    "it is clearly seen that @xmath240 in theorem [ thmvariance ] estimates in theorem [ thmnormality ] .",
    "the proof of theorem [ thmvariance ] is provided in the supplementary material  @xcite .",
    "extensive simulations are carried out to evaluate the finite sample performance of the proposed method . in the simulation studies ,",
    "failure times are generated from the model @xmath241 where @xmath242 is bernoulli with success probability 0.5 , @xmath243 is independent normal with mean 0 and standard deviation 0.5 truncated at @xmath244 .",
    "this is the same model used by @xcite and @xcite .",
    "we consider six error distributions : standard normal ; standard extreme - value ; mixtures of @xmath245 and @xmath246 with mixing probabilities @xmath247 and @xmath248 , denoted by @xmath249 and @xmath250 , respectively ; gumbel@xmath251 with @xmath252 being the euler constant and @xmath253 .",
    "the first four distributions were also considered by @xcite .",
    "similar to @xcite , the censoring times are generated from uniform @xmath254 $ ] distribution , where @xmath255 is chosen to produce a 25% censoring rate .",
    "we set the sample size @xmath144 to 200 , 400 and 600 .",
    "we choose cubic b - splines with one interior knot for @xmath256 and @xmath257 , and two interior knots for @xmath258 .",
    "we perform the sieve maximum likelihood analysis and obtain the estimates of the slope parameters using the newton ",
    "raphson algorithm that updates @xmath259 iteratively .",
    "we stop iteration when the change of parameter estimates or the gradient value is less than a pre - specified tolerance value that is set to be @xmath260 in our simulations .",
    "log - rank and gehan - weighted estimators are included for efficiency comparisons .",
    "we calculate the theoretical semiparametric efficiency bound @xmath261 , and scale it by the sample size , that is , @xmath262 , which serves as the reference standard error under the fully efficient situation .",
    "table [ tabletab2 ] summarizes the results of these studies based on 1,000 simulated datasets .",
    "the bias of the proposed estimators of @xmath263 and @xmath264 are negligible .",
    "both variance estimation procedures , denoted as @xmath265see ( the standard error estimates by inverting the information matrix based on the efficient score function ) and @xmath266see ( the standard error estimates by inverting the observed information matrix of all parameters including nuisance parameters ) , yield nice standard error estimates for the parameter estimators comparing to the empirical standard error se , and the @xmath267 confidence intervals have proper coverage probabilities , especially when the sample size is large .",
    "for the @xmath245 error and the two mixtures of normal errors that are also considered in @xcite , the proposed estimators are more efficient than the log - rank estimators and have similar variances to the gehan - weighted estimators . for the standard extreme - value error ,",
    "the proposed estimators are more efficient than the gehan - weighted estimator and similar to the log - rank estimator that is known to be the most efficient estimator under this particular error distribution . for the gumbel@xmath251 and @xmath253 errors ,",
    "the proposed estimators are more efficient than the other two estimators . under all six error distributions ,",
    "the standard errors of the proposed estimators are close to the efficient theoretical standard errors .",
    "the sample averages of the estimates for @xmath268 under different simulation settings are reasonably close to corresponding true curves ( results not shown here ; see @xcite for details ) .",
    "we use the stanford heart transplant data @xcite as an illustrative example .",
    "this dataset was also analyzed by @xcite using their proposed least squares estimators .",
    "following their analysis , we consider the same two models : the first one regresses the base-10 logarithm of the survival time on age at transplant and t5 mismatch score for the 157 patients with complete records on t5 measure , and the second one regresses the base-10 logarithm of the survival time on age and age@xmath266 .",
    "there were 55 censored patients .",
    "we fit these two models using the proposed method with five cubic b - spline basis functions .",
    "@lcd2.4cd2.4cd2.4d1.4@ & & & & + & & & & + & & & & & & & + m.1 & age & -0.0237 & 0.0068 & -0.0211 & 0.0106 & -0.015 & 0.008 + & t5 & -0.2118 & 0.1271 & -0.0265 & 0.1507 & -0.003 & 0.134 + m.2 & age & 0.1022 & 0.0245 & 0.1046 & 0.0474 & 0.107 & 0.037 + & age@xmath266 & -0.0016 & 0.0004 & -0.0017 & 0.0006 & -0.0017 & 0.0005 +    we report the parameter estimates and the standard error estimates in table  [ tabletab4 ] and compare them with the gehan - weighted estimators reported by @xcite and the buckley ",
    "james estimators reported by @xcite . for the first model , the parameter estimates for the age effect are fairly similar among all estimators , and the standard error estimate from the proposed method tends to be smaller , while the parameter estimates for the t5 mismatch score vary across different estimators with none of them being significant at the 0.05 level .",
    "the disparity of the t5 effect may be due to what was pointed out by @xcite : the accelerated failure time model with age and t5 as covariates does not fit the data ideally . for the second model with age and age@xmath266 being the covariates , the point estimates are very similar across all methods and the standard error estimates from the proposed method are the smallest .",
    "by applying the proposed general sieve m - estimation theory for semiparametric models with bundled parameters , we are able to derive the asymptotic distribution for the sieve maximum likelihood estimator in a linear regression model where the response variable is subject to right censoring . by providing a both statistically and computationally efficient estimating procedure",
    ", this work makes the linear model a more viable alternative to the cox proportional hazards model . comparing to the existing methods for estimating @xmath27 in a linear model",
    ", the proposed method has three advantages .",
    "first , the estimating functions are smooth functions in contrast to the discrete estimating functions in the existing estimation methods ; thus the root search is easier and can be done quickly by conventional iterative methods such as the newton  raphson algorithm .",
    "second , the standard error estimates are obtained directly by inverting either the efficient information matrix for the regression parameters or the observed information matrix of all parameters ; either method is more computationally tractable compared to the re - sampling techniques .",
    "third , the proposed estimator achieves the semiparametric efficiency bound .",
    "the proposed general sieve m - estimation theory can also be applied to other statistical models , for example , the single index model , the cox model with an unknown link function and the linear model under different censoring mechanisms . such research is undergoing and will be presented elsewhere .",
    "empirical process theory developed in @xcite will be heavily involved in the proof .",
    "we use the symbol @xmath269 to denote that the left - hand side is bounded above by a constant times the right - hand side and  @xmath270 to denote that the left - hand side is bounded below by a constant times the right - hand side . for notational simplicity",
    ", we drop the superscript @xmath271 in the outer probability measure @xmath272 whenever an outer probability applies .",
    "we first introduce several lemmas that will be used for the proofs of theorems [ thmconvrate ] , [ thmnormality ] and [ thmvariance ] .",
    "proofs of these lemmas are provided in the supplementary material @xcite .",
    "[ lemlem1 ] under conditions and , the log - likelihood @xmath273 where @xmath274 , has bounded and continuous first and second derivatives with respect to @xmath147 and @xmath275 .",
    "[ lemlem2 ] for @xmath276 , there exists a function @xmath277 such that @xmath278    [ lemlem3 ] let @xmath279 with @xmath280 defined in lemma [ lemlem2 ] .",
    "denote @xmath281 .",
    "assume that conditions and hold , then the @xmath282-bracketing number associated with norm for @xmath283 is bounded by @xmath284 , that is , @xmath285}(\\varepsilon,\\mathcal{f}_n,\\|\\cdot\\|_{\\infty } ) \\lesssim(1/\\varepsilon)^{cq_n+d}$ ] for some constant @xmath286 .",
    "[ lemlem4 ] let @xmath287 , where @xmath288 .",
    "assume conditions hold , then there exists @xmath289 such that @xmath290 , or equivalently , @xmath291 where @xmath292 .",
    "[ lemlem5 ] for @xmath293 defined in lemma [ lemlem4 ] , denote the class of functions",
    "@xmath294\\dvtx\\theta \\in \\theta^p_n , h_j\\in\\mathcal{h}^2_n , d(\\theta,\\theta_0 ) \\leq\\eta , \\| h_j - h_j^*\\|_{\\infty } \\leq\\eta\\}.\\ ] ] assume conditions hold , then @xmath285}(\\varepsilon,\\mathcal{f}_n^j(\\eta),\\|\\cdot\\|_{\\infty } ) \\lesssim(\\eta/\\varepsilon)^{cq_n+d}$ ] for some constant @xmath286 .",
    "[ lemlem6 ] for @xmath99 , define the following two classes of functions : @xmath295 and @xmath296-\\dot{l}_{\\zeta}(\\theta_0;z)[h_j^*(\\cdot , \\beta _ 0)]\\dvtx\\theta\\in\\theta^p_n , d(\\theta,\\theta_0 ) \\leq\\eta\\},\\ ] ] where @xmath297 is the @xmath298th element of @xmath299 , @xmath300 denotes the derivative of @xmath34 and @xmath293 is defined in lemma [ lemlem5 ] .",
    "assume conditions hold , then @xmath285}(\\varepsilon,\\mathcal { f}_{n , j}^{\\beta}(\\eta ) , \\|\\cdot\\|_{\\infty } ) \\lesssim(\\eta/\\varepsilon ) ^{c_1q_n+d}$ ] and @xmath285}(\\varepsilon,\\mathcal{f}_{n , j}^{\\zeta}(\\eta ) , \\| \\cdot\\|_{\\infty } ) \\lesssim(\\eta/\\varepsilon)^{c_2q_n+d}$ ] for some constants @xmath301 .",
    "we prove the theorem by checking assumptions ( a1)(a6 ) in section [ secgenthm ] . here",
    "the criterion function of a single observation is the log - likelihood function @xmath302 .",
    "so instead of @xmath47 , we use @xmath303 to denote the criterion function . by theorem [ thmconvrate ]",
    "we know that assumption ( a1 ) holds with @xmath304 and the norm @xmath305 defined in ( [ eqnorm ] ) .",
    "assumption ( a2 ) automatically holds for the scores . for ( a3 )",
    ", we need to find an @xmath306 with @xmath307 such that @xmath308-\\ddot { s}_{\\zeta\\zeta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0))[\\mathbf{h}^*,h]\\\\ & & \\qquad = p\\{\\ddot{l}_{\\beta\\zeta } ( \\beta_0,\\zeta_0(\\cdot,\\beta _ 0);z)[h]-\\ddot{l}_{\\zeta\\zeta}(\\beta_0,\\zeta_0(\\cdot,\\beta _ 0);z)[\\mathbf{h}^*,h]\\}=0\\end{aligned}\\ ] ] for all @xmath97 with @xmath309 .",
    "note that @xmath310-\\ddot { l}_{\\zeta\\zeta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z)[\\mathbf{h}^*,h]\\ } \\\\ & & \\qquad = p\\biggl\\{-x \\biggl[\\delta\\dot{w}(\\epsilon_0)-\\int _ { a}^{b}1(\\epsilon_0 \\geq t ) \\exp\\{g_0(t)\\}\\dot{w}(t ) \\,dt \\biggr ] \\\\ & & \\qquad\\quad\\hspace*{14pt } { } + \\int_{a}^{b}1(\\epsilon_0\\geq t ) \\exp\\{g_0(t)\\ } w(t)[x\\dot{g}_0(t)+\\mathbf{w}^*(t ) ] \\,dt \\biggr\\}.\\end{aligned}\\ ] ] since @xmath311\\vert x\\ } = 0 $ ] for all @xmath97 , replacing @xmath312 by @xmath313 we have @xmath314\\biggr\\ } \\\\ & & \\qquad = p\\biggl\\{-x \\cdot p\\biggl[\\delta\\dot{w}(\\epsilon_0)-\\int _ { a}^{b}1(\\epsilon_0 \\geq t ) \\exp\\{g_0(t)\\}\\dot{w}(t ) \\,dt \\big\\vert x \\biggr ] \\biggr\\}\\\\ & & \\qquad = p\\{-x \\cdot0\\ } = 0.\\end{aligned}\\ ] ] hence we only need to find a @xmath315 such that @xmath316 \\,dt \\biggr\\ } \\\\ & & \\qquad = \\int_{a}^{b } \\exp\\{g_0(t)\\}w(t ) \\{\\dot { g}_0(t)p[1(\\epsilon_0 \\geq t)x]+\\mathbf{w}^*(t)p[1(\\epsilon_0 \\geq t)]\\ } \\,dt = 0.\\end{aligned}\\ ] ] one obvious choice for @xmath315 ( or @xmath113 ) is @xmath317}{p[1(\\epsilon_0 \\geq t ) ] } = -\\dot { g}_0(t)p(x|\\epsilon_0 \\geq t).\\ ] ] then it follows @xmath318 \\\\ & & \\qquad = \\delta\\{-\\dot{g}_0(y - x'\\beta_0)\\ } \\{x - p(x|\\epsilon_0 \\geq y - x'\\beta_0)\\ } \\\\ & & \\qquad\\quad { } - \\int1(y - x'\\beta_0 \\geq t ) \\{x - p(x|\\epsilon_0 \\geq t)\\}\\{-\\dot{g}_0(t)\\}\\exp\\{g_0(t)\\ } \\,dt \\\\ & & \\qquad = \\int\\{x - p(x|\\epsilon_0 \\geq t)\\}\\{-\\dot{g}_0(t)\\}\\ , d m(t)\\\\ & & \\qquad = l_{\\beta_0}^*(y,\\delta , x),\\end{aligned}\\ ] ] which is the efficient score function for @xmath11 originally derived by @xcite , where @xmath319 by the fact of zero - mean for a score function , it is straightforward to verify the following equalities : @xmath320 & = & - p\\ { \\dot{l}_\\beta(\\beta,\\zeta(\\cdot,\\beta);z)\\dot{l}_\\zeta'(\\beta,\\zeta ( \\cdot,\\beta);z)[h]\\ } , \\\\",
    "p\\ddot{l}_{\\zeta\\beta}(\\beta,\\zeta(\\cdot,\\beta);z)[h ] & = & - p\\{\\dot { l}_\\zeta(\\beta,\\zeta(\\cdot,\\beta);z)[h]\\dot{l}_{\\beta}'(\\beta,\\zeta ( \\cdot,\\beta);z)\\ } , \\\\",
    "p\\ddot{l}_{\\beta\\beta}(\\beta,\\zeta(\\cdot,\\beta);z)&=&-p\\{\\dot { l}_\\beta(\\beta,\\zeta(\\cdot,\\beta);z)\\dot{l}_\\beta'(\\beta,\\zeta(\\cdot , \\beta);z)\\ } , \\\\",
    "p\\ddot{l}_{\\zeta\\zeta}(\\beta,\\zeta(\\cdot,\\beta);z)[h_1,h_2]&=&-p\\ { \\dot{l}_\\zeta(\\beta,\\zeta(\\cdot,\\beta);z)[h_1 ] \\dot{l}_\\zeta'(\\beta , \\zeta(\\cdot,\\beta);z)[h_2]\\}.\\end{aligned}\\ ] ] then together with the fact that @xmath321-\\ddot{l}_{\\zeta\\zeta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z)[\\mathbf { h}^*,\\mathbf{h}^*]\\}=0,\\ ] ] the matrix @xmath116 in assumption ( a3 ) of theorem [ thmgenthm ] is given by @xmath322 \\\\ & & \\hspace*{11.4pt } { } + \\ddot{l}_{\\beta\\zeta}(\\beta_0,\\zeta_0(\\cdot,\\beta _ 0);z)[\\mathbf{h}^*]-\\ddot{l}_{\\zeta\\zeta}(\\beta_0,\\zeta_0(\\cdot,\\beta _ 0);z)[\\mathbf{h}^*,\\mathbf{h}^*]\\}\\\\ & = & p\\{\\dot{l}_{\\beta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z)\\dot{l}_{\\beta } ' ( \\beta_0,\\zeta_0(\\cdot,\\beta_0);z)\\\\ & & \\hspace*{11.4pt } { } - \\dot{l}_{\\zeta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z)[\\mathbf { h}^*]\\dot{l}_{\\beta}'(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z ) \\\\ & & \\hspace*{11.4pt } { } - \\dot{l}_{\\beta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z)\\dot { l}_{\\zeta}'(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z)[\\mathbf{h}^ * ] \\\\ & & \\hspace*{11.4pt } { } + \\dot{l}_{\\zeta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z)[\\mathbf { h}^*]\\dot{l}_{\\zeta}'(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z)[\\mathbf{h}^*]\\ } \\\\ & = & p\\{\\dot{l}_{\\beta}(\\beta_0,\\zeta_0(\\cdot,\\beta_0);z)-\\dot{l}_{\\zeta } ( \\beta_0,\\zeta_0(\\cdot,\\beta_0);z)[\\mathbf{h}^*]\\}^{\\otimes2}\\\\ & = & pl_{\\beta_0}^*(y,\\delta , x)^{\\otimes2},\\end{aligned}\\ ] ] which is the information matrix for @xmath11 .    to verify ( a4 ) , we note that the first part automatically holds since @xmath91 satisfies the score equation @xmath323 . next we shall show that @xmath324 \\\\ & & \\qquad = \\mathbb{p}_n \\biggl\\{\\delta w_j^*(y - x'\\hat{\\beta } _ n)-\\int1(y\\geq t ) \\exp\\{\\hat{\\zeta}_n(t , x,\\hat{\\beta}_n)\\ } w_j^*(t - x'\\hat{\\beta}_n ) \\,dt \\biggr\\ } \\\\ & & \\qquad = o_p(n^{-1/2}),\\end{aligned}\\ ] ] where @xmath325 , @xmath99 , is the @xmath298th component of  @xmath326 given in ( [ eqhstar ] ) . according to lemma [ lemlem4 ]",
    ", there exists @xmath327 such that @xmath328 .",
    "then by the score equation for @xmath329 and the fact that @xmath330 can be written as @xmath331 for some coefficients @xmath332 and the basis func - tions  @xmath333 of the spline space , it follows that @xmath334 so it suffices to show that for each @xmath335 , @xmath336 = o_p(n^{-1/2}).\\ ] ] since @xmath337 \\}\\,{=}\\,0 $ ] , we decompose @xmath338 into , where @xmath339\\ ] ] and @xmath340 -\\dot{l}_\\zeta(\\beta_0,\\zeta_0(\\cdot , \\beta_0);z)[h_j^*-h_{j , n}^ * ] \\}.\\ ] ] we will show that @xmath341 and @xmath342 are both @xmath343 .    first consider @xmath341 . according to lemma [ lemlem5 ] , the @xmath282-bracketing number associated with @xmath344 norm for the class @xmath345 defined in lemma [ lemlem5 ]",
    "is bounded by @xmath346 .",
    "this implies that @xmath347}(\\varepsilon,\\mathcal{f}_n^j(\\eta),l_2(p ) ) \\leq\\log n_{[{\\,}]}(\\varepsilon,\\mathcal{f}_n^j(\\eta),\\| \\cdot\\|_{\\infty } ) \\lesssim q_n \\log(\\eta/\\varepsilon),\\ ] ] which leads to the bracketing integral @xmath348}(\\eta,\\mathcal{f}_n^j(\\eta),l_2(p ) ) & = & \\int_0^{\\eta } \\sqrt { 1+\\log n_{[{\\,}]}(\\varepsilon,\\mathcal{f}_n^j(\\eta),l_2(p))}\\ , d\\varepsilon\\\\ & \\lesssim & q_n^{1/2}\\eta.\\end{aligned}\\ ] ] now we pick @xmath349 to be @xmath350 , then @xmath351 and since @xmath352 , @xmath353 therefore , @xmath354 \\in\\mathcal{f}_n^j(\\eta_n)$ ] .",
    "denote @xmath355 for notational simplicity , for any @xmath356 \\in\\mathcal{f}_n^j(\\eta_n)$ ] , it follows that @xmath357\\}^2 \\\\ & & \\qquad = p\\biggl\\{\\delta(w_j^*-w)(\\epsilon_\\beta ) + \\int_a^b 1(\\epsilon_0 \\geq t)\\exp\\{g(t_{\\beta})\\}(w_j^*-w)(t_{\\beta } ) \\,dt \\biggr\\}^2 \\\\ & & \\qquad \\lesssim\\| w_j^*-w \\|_{\\infty}^2 + p\\biggl\\{\\int_a^b \\exp\\ { 2g(t_{\\beta})\\}(w_j^*-w)^2(t_{\\beta } ) \\,dt\\biggr\\ } \\\\ & & \\qquad \\lesssim\\| w_j^*-w \\|_{\\infty}^2 + \\| w_j^*-w \\|_{\\infty}^2 \\int_a^b p[\\exp\\{2g(t_{\\beta})\\ } ] \\,dt , \\end{aligned}\\ ] ] where the first inequality holds because of the cauchy ",
    "schwarz inequality . since @xmath358 , by the same argument as ( @xcite , page 591 ) , for slowly growing @xmath359 ( their @xmath360 ) , for example , @xmath361 , we know that @xmath362 \\|_{\\infty}$ ] is bounded by some constant @xmath363 and @xmath364\\}^2 \\lesssim\\eta_n$ ] for a slightly enlarged @xmath365 obtained by a fine adjustment of @xmath222 . then by the maximal inequality in lemma 3.4.2 of @xcite , it follows that @xmath366}(\\eta_n,\\mathcal{f}_n^j(\\eta_n),l_2(p ) ) \\biggl(1+\\frac{j_{[{\\,}]}(\\eta _",
    "n,\\mathcal{f}_n^j(\\eta_n),l_2(p))}{\\eta_n^2\\sqrt{n}}m \\biggr ) \\\\[-2pt ] & \\lesssim & q_n^{1/2}\\eta_n + q_nn^{-1/2 } \\\\[-2pt ] & = & o\\bigl\\{n^{\\nu/2-\\min(2\\nu,(1-\\nu)/2)}\\bigr\\ } + o(n^{\\nu-1/2 } ) \\\\[-2pt ] & = & o\\bigl\\{n^{-\\min(3\\nu/2,1/2-\\nu)}\\bigr\\}+ o(n^{\\nu-1/2 } ) = o(1),\\end{aligned}\\ ] ] where the last equality holds because @xmath367 .",
    "thus by markov s inequality , @xmath368 = o_p(n^{-1/2})$ ] .",
    "next for @xmath342 , the taylor expansion for @xmath369 $ ] at @xmath370 yields @xmath371 - \\dot{l}_\\zeta(\\beta_0,\\zeta_0(\\cdot , \\beta_0);z)[h_j^*-h_{j , n}^ * ] \\\\[-2pt ] & & \\qquad = ( \\hat{\\beta}_n-\\beta_0)'\\ddot{l}_{\\beta\\zeta}(\\tilde{\\beta } _",
    "n,\\tilde{\\zeta}_n(\\cdot,\\tilde{\\beta}_n);z)[h_j^*-h_{j , n}^ * ] \\\\[-2pt ] & & \\qquad\\quad { } + \\ddot{l}_{\\zeta\\zeta}(\\tilde{\\beta}_n,\\tilde{\\zeta } _ n(\\cdot,\\tilde{\\beta}_n);z)[h_j^*-h_{j , n}^*,\\hat{\\zeta}_n-\\zeta_0],\\end{aligned}\\ ] ] where @xmath372 is between @xmath373 and @xmath374 .",
    "then it follows that @xmath375| \\\\[-2pt ] & & \\qquad = \\biggl| x\\biggl\\{\\delta(\\dot{w}_j^*-\\dot{w}_{j , n}^*)(\\epsilon _ { \\tilde{\\beta}_n})\\\\[-2pt ] & & \\qquad\\quad\\hspace*{16pt } { } -\\int_a^b 1(\\epsilon_0 \\geq t ) \\exp\\{\\tilde { g}_n(t_{\\tilde{\\beta}_n})\\}[(\\dot{w}_j^*-\\dot { w}_{j , n}^*)(t_{\\tilde{\\beta}_n } ) \\\\[-2pt ] & & \\hspace*{144pt}\\qquad\\quad { } + \\dot{\\tilde{g}}_n(t_{\\tilde{\\beta}_n } ) ( w_j^*-w_{j , n}^*)(t_{\\tilde{\\beta}_n } ) ] \\,dt \\biggr\\ } \\biggr| \\\\[-2pt ] & & \\qquad \\lesssim\\|\\dot{w}_j^*-\\dot{w}_{j , n}^ * \\|_{\\infty } + \\|\\dot { w}_j^*-\\dot{w}_{j , n}^ * \\|_{\\infty } \\biggl\\{\\int_a^b \\exp\\{\\tilde { g}_n(t_{\\tilde{\\beta}_n})\\ } \\,dt \\biggr\\ } \\\\[-2pt ] & & \\qquad\\quad { } + \\| w_j^*-w_{j , n}^ * \\|_{\\infty } \\biggl\\ { \\int_a^b \\exp\\ { \\tilde{g}_n(t_{\\tilde{\\beta}_n})\\}\\dot{\\tilde{g}}_n(t_{\\tilde{\\beta } _ n } ) \\,dt \\biggr\\ } \\\\[-2pt ] & & \\qquad \\lesssim\\|\\dot{w}_j^*-\\dot{w}_{j , n}^ * \\|_{\\infty } + \\| w_j^*-w_{j , n}^ * \\|_{\\infty } \\\\[-2pt ] & & \\qquad = o(n^{-\\nu})+o(n^{-2\\nu})\\\\[-2pt ] & & \\qquad = o(n^{-\\nu}),\\end{aligned}\\ ] ] where the second inequality holds because @xmath376 and its first derivative @xmath377 are bounded ( or growing with @xmath144 slowly enough so it can be effectively treated as bounded based on the same argument of @xcite on page 591 ) , and the last equality holds due to the corollary 6.21 of @xcite that @xmath378 .",
    "thus , @xmath379|\\\\ & & \\qquad= |\\hat{\\beta}_n-\\beta_0| \\cdot o(n^{-\\nu } ) \\\\ & & \\qquad= o_p\\bigl\\{n^{-\\min(p\\nu,(1-\\nu)/2)}\\bigr\\}\\cdot o(n^{-\\nu } ) \\\\ & & \\qquad= o_p\\bigl\\ { n^{-\\min((p+1)\\nu,(1 + 3\\nu)/2)}\\bigr\\}.\\end{aligned}\\ ] ] also , @xmath380| \\\\ & & \\qquad = \\biggl| \\int_a^b 1(\\epsilon_0 \\geq t)\\exp\\{\\tilde { g}_n(t_{\\tilde{\\beta}_n})\\ } ( w_j^*-w_{j , n}^*)(t_{\\tilde{\\beta}_n})(\\hat { g}_n - g_0)(t_{\\tilde{\\beta}_n } ) \\,dt \\biggr| \\\\ & & \\qquad \\leq\\| w_j^*-w_{j , n}^ * \\|_{\\infty } \\cdot\\biggl\\ { \\int_a^b \\exp\\{\\tilde{g}_n(t_{\\tilde{\\beta}_n})\\}(\\hat{g}_n - g_0)(t_{\\tilde{\\beta } _ n } ) \\,dt\\biggr\\ } \\\\ & & \\qquad = \\| w_j^*-w_{j , n}^ * \\|_{\\infty}\\cdot i_{3n}.\\ ] ] by the cauchy ",
    "schwarz inequality and the boundedness of @xmath376 , we have @xmath381 hence @xmath382 and @xmath383|\\\\ & & \\qquad \\lesssim\\| w_j^*-w_{j , n}^*\\|_{\\infty } \\cdot d(\\hat{\\theta } _",
    "n,\\theta_0 ) = o(n^{-2\\nu})\\cdot o_p\\bigl\\{n^{-\\min(p\\nu,(1-\\nu)/2)}\\bigr\\}\\\\ & & \\qquad = o_p\\bigl\\ { n^{-\\min((p+2)\\nu,(1 + 3\\nu)/2)}\\bigr\\}.\\end{aligned}\\ ] ] since @xmath384 , it follows that @xmath385 .",
    "thus @xmath386 , and condition ( a4 ) holds .",
    "now we verify assumption ( a5 ) .",
    "first by lemma [ lemlem6 ] , the @xmath282-bracketing numbers for the classes of functions @xmath387 and @xmath388 are both bounded by @xmath346 , which implies that the corresponding @xmath282-bracketing integrals are both bounded by @xmath389 , that is , @xmath390}(\\eta,\\mathcal{f}_{n , j}^{\\beta}(\\eta),l_2(p ) ) \\lesssim q_n^{1/2}\\eta\\quad\\mbox{and}\\quad j_{[{\\,}]}(\\eta,\\mathcal{f}_{n , j}^{\\zeta } ( \\eta),l_2(p ) ) \\lesssim q_n^{1/2}\\eta.\\ ] ] then for @xmath391 , by applying the cauchy ",
    "schwarz inequality , together with subtracting and adding the terms @xmath392 , @xmath393 , @xmath394 and @xmath395 , we have @xmath396\\\\ & & \\qquad\\quad\\hspace*{4pt}{}+x_j \\int_a^b 1(\\epsilon_0 \\geq t)\\bigl[e^{g(t_\\beta)}\\dot{g}(t_\\beta ) -e^{g_0(t)}\\dot{g}_0(t)\\bigr ] \\,dt\\biggr\\}^2 \\\\ & & \\qquad \\lesssim\\{\\delta[\\dot{g}(\\epsilon_\\beta)-\\dot{g}_0(\\epsilon_0)]^2\\ } + \\biggl\\ { \\int_a^b \\bigl[e^{g(t_\\beta)}\\dot{g}(t_\\beta)-e^{g_0(t)}\\dot{g}_0(t)\\bigr]^2 \\,dt\\biggr\\ } \\\\ & & \\qquad \\lesssim\\{\\delta[\\dot{g}(\\epsilon_\\beta)-\\dot{g}(\\epsilon_0)]^2\\ } + \\{\\delta[\\dot{g}(\\epsilon_0)-\\dot{g}_0(\\epsilon_0)]^2\\ } \\\\ & & \\qquad\\quad { } + \\int_a^b \\bigl\\ { \\bigl[e^{g(t_\\beta)}-e^{g_0(t_\\beta)}\\bigr]^2 + \\bigl[e^{g_0(t_\\beta)}-e^{g_0(t)}\\bigr]^2 \\bigr\\ } \\dot{g}^2(t_\\beta ) \\,dt \\\\ & & \\qquad\\quad { } + \\int_a^b e^{2g_0(t)}\\bigl\\{[\\dot{g}(t_\\beta)-\\dot { g}_0(t_\\beta)]^2 + e^{2g_0(t)}[\\dot{g}_0(t_\\beta)-\\dot{g}_0(t)]^2 \\bigr\\ } \\,dt \\\\ & & \\qquad = b_1+b_2+b_3+b_4.\\end{aligned}\\ ] ]    for @xmath397 , since @xmath398 is bounded and the largest eigenvalue of @xmath399 satisfies @xmath400 by condition ( c.2)(b ) , it follows that @xmath401 ^ 2 \\lesssim p[x'(\\beta-\\beta_0)]^2 \\\\ & \\leq&\\lambda_d |\\beta-\\beta_0|^2 \\lesssim|\\beta-\\beta_0|^2 \\leq\\eta^2.\\end{aligned}\\ ] ] for @xmath402 , we have @xmath403 for @xmath404 , by using the mean value theorem , it follows that @xmath405 ^ 2 + \\bigl[e^{g_0(t_{\\tilde{\\beta}})}x'(\\beta-\\beta_0)\\bigr]^2 \\bigr\\ } \\dot { g}^2(t_\\beta ) \\,dt\\biggr\\ } \\\\[-2pt ] & \\lesssim & \\int_{\\mathcal{x}}\\int_a^b ( g - g_0)^2(t_\\beta ) \\,d \\lambda _ 0(t ) \\,d f_x(x ) + p[x'(\\beta-\\beta_0)]^2 \\\\[-2pt ] & \\lesssim & \\| \\zeta(\\cdot,\\beta)-\\zeta_0(\\cdot,\\beta_0 ) \\|_2 ^ 2 + |\\beta -\\beta_0|^2 \\leq\\eta^2,\\end{aligned}\\ ] ] where @xmath406 for some @xmath407 and thus is bounded . finally for  @xmath408 , by the mean value theorem , it follows that @xmath409 ^ 2 + e^{2g_0(t)}[\\dot{g}_0(t_\\beta)-\\dot{g}_0(t)]^2 \\bigr\\ } \\,dt \\biggr\\ } \\\\[-2pt ] & \\lesssim & \\int_{\\mathcal{x}}\\int_a^b ( \\dot{g}-\\dot{g}_0)^2(t_\\beta ) \\,d \\lambda_0(t ) \\,d f_x(x ) + p \\int_a^b [ \\ddot{g}_0(t_{\\tilde{\\beta } } ) x'(\\beta-\\beta_0)]^2 \\,dt \\\\[-2pt ] & \\lesssim & \\| \\dot{g}(\\psi(\\cdot,\\beta))-\\dot{g}_0(\\psi(\\cdot,\\beta ) ) \\| _ 2 ^ 2 + p[x'(\\beta-\\beta_0)]^2 \\\\[-2pt ] & \\lesssim & \\| \\dot{g}(\\psi(\\cdot,\\beta))-\\dot{g}_0(\\psi(\\cdot,\\beta_0 ) ) \\|_2 ^ 2 + |\\beta-\\beta_0|^2 \\lesssim\\eta^2.\\end{aligned}\\ ] ] therefore we have @xmath410 .",
    "using the similar argument , we can show that @xmath411-\\dot{l}_\\zeta(\\theta_0;z)[h_j^*]\\}^2 \\lesssim \\eta^2 $ ] . by lemma [ lemlem1 ] , we also have @xmath412 and @xmath413-\\dot{l}_{\\zeta}(\\theta_0;z)[h_j^ * ] \\| _ { \\infty}$ ] are both bounded .",
    "now we pick @xmath349 as @xmath414 , then by the maximal inequality in lemma 3.4.2 of @xcite , it follows that @xmath415 & = & o\\bigl\\{n^{\\max(({3}/{2}-p)\\nu,\\nu-{1}/{2})}\\bigr\\}+ o(n^{\\nu-{1}/{2 } } ) = o(1),\\end{aligned}\\ ] ] where the last equality holds since @xmath193 and @xmath416 .",
    "similarly , we have @xmath417 .",
    "thus for @xmath418 and @xmath419 by markov s inequality , @xmath420 \\sup_{d(\\theta,\\theta_0 ) \\leq cn^{-\\xi}}\\mathbb{g}_n\\{\\dot{l}_{\\zeta } ( \\beta,\\zeta(\\cdot,\\beta);z)[h_j^*]-\\dot{l}_{\\zeta}(\\beta_0,\\zeta _ 0(\\cdot,\\beta_0);z)[h_j^*]\\ } & = & o_p(1).\\end{aligned}\\ ] ] this completes the verification of assumption ( a5 ) .    finally , assumption ( a6 ) can be verified by using the taylor expansion . since the proofs for the two equations in ( a6 ) are essentially identical , we just prove the first equation .",
    "in a neighborhood of @xmath421 with @xmath422 , the taylor expansion for @xmath423 yields @xmath424 \\\\[-2pt ] & = & \\dot{l}_{\\beta}(\\theta_0;z ) + \\ddot{l}_{\\beta\\beta}(\\theta _ 0;z)(\\beta-\\beta_0 ) + \\ddot{l}_{\\beta\\zeta}(\\theta_0;z)[\\zeta(\\cdot , \\beta)-\\zeta_0(\\cdot,\\beta_0 ) ] \\\\[-2pt ] & & { } + \\{\\ddot{l}_{\\beta\\beta}(\\tilde{\\theta};z)(\\beta-\\beta _ 0)-\\ddot{l}_{\\beta\\beta}(\\theta_0;z)(\\beta-\\beta_0)\\ } \\\\[-2pt ] & & { } + \\{\\ddot{l}_{\\beta\\zeta}(\\tilde{\\theta};z)[\\zeta(\\cdot , \\beta)-\\zeta_0(\\cdot,\\beta_0)]-\\ddot{l}_{\\beta\\zeta}(\\theta_0;z)[\\zeta ( \\cdot,\\beta)-\\zeta_0(\\cdot,\\beta_0)]\\},\\end{aligned}\\ ] ] where @xmath425 is a midpoint between @xmath370 and @xmath56 .",
    "so @xmath426\\ } \\\\ & & \\qquad = p\\{\\ddot{l}_{\\beta\\beta}(\\tilde{\\theta};z)-\\ddot{l}_{\\beta\\beta } ( \\theta_0;z)\\}(\\beta-\\beta_0 ) \\\\ & & \\qquad\\quad { } + p\\{\\ddot{l}_{\\beta\\zeta}(\\tilde{\\theta};z)[\\zeta(\\cdot , \\beta)-\\zeta_0(\\cdot,\\beta_0)]-\\ddot{l}_{\\beta\\zeta}(\\theta_0;z)[\\zeta ( \\cdot,\\beta)-\\zeta_0(\\cdot,\\beta_0)]\\}.\\end{aligned}\\ ] ] then by direct calculation we have @xmath427 by applying a similar argument that we used before for verifying ( a5 ) and condition ( c.6 ) , we can show @xmath428 similarly , we can show @xmath429 and @xmath430 where @xmath431 .",
    "therefore , @xmath432 and thus @xmath433 where the last equality holds since @xmath352 , so @xmath434 , @xmath435 and @xmath436 .",
    "similarly we can show @xmath437-\\ddot{l}_{\\beta\\zeta}(\\theta_0;z)[\\zeta(\\cdot,\\beta ) -\\zeta_0(\\cdot,\\beta_0)]| \\\\ & & \\qquad = o\\bigl\\{n^{-\\min(2(p-1)\\nu,{1}/{2}+(p-{5}/{2})\\nu,1-\\nu ) } \\bigr\\ } \\\\ & & \\qquad= o(n^{-1/2}).\\end{aligned}\\ ] ] therefore , we have @xmath438\\}| \\\\ & & \\qquad = o\\bigl\\{n^{-\\min(2(p-1)\\nu,{1}/{2}+(p-{5}/{2})\\nu,1-\\nu)}\\bigr\\ } = o(n^{-\\alpha\\xi}),\\end{aligned}\\ ] ] where @xmath439 and @xmath440 .    therefore , we have verified all six assumptions , and thus we have @xmath441 where @xmath442 $ ] is the efficient score function for  @xmath11 and @xmath443 , which is shown when verifying ( a3 ) .",
    "hence @xmath444 and @xmath445 , and @xmath446 thus we complete the proof of theorem [ thmnormality ] .",
    "the authors would like to thank two referees and an associate editor for their very helpful comments ."
  ],
  "abstract_text": [
    "<S> in many semiparametric models that are parameterized by two types of parameters  a euclidean parameter of interest and an infinite - dimensional nuisance parameter  the two parameters are bundled together , that is , the nuisance parameter is an unknown function that contains the parameter of interest as part of its argument . </S>",
    "<S> for example , in a linear regression model for censored survival data , the unspecified error distribution function involves the regression coefficients . </S>",
    "<S> motivated by developing an efficient estimating method for the regression parameters , we propose a general sieve m - theorem for bundled parameters and apply the theorem to deriving the asymptotic theory for the sieve maximum likelihood estimation in the linear regression model for censored survival data . the numerical implementation of the proposed estimating method can be achieved through the conventional gradient - based search algorithms such as the newton  raphson algorithm . </S>",
    "<S> we show that the proposed estimator is consistent and asymptotically normal and achieves the semiparametric efficiency bound . </S>",
    "<S> simulation studies demonstrate that the proposed method performs well in practical settings and yields more efficient estimates than existing estimating equation based methods . </S>",
    "<S> illustration with a real data example is also provided .    .    . </S>"
  ]
}