{
  "article_text": [
    "astronomy needs well calibrated data to make precise measurements , but also wants to make use of large data sources that are poorly calibrated .",
    "unreliable data sets such as historical archives , amateurs collections , and engineering data contain important information , especially in the time domain .",
    "astronomy needs methods by which data of unknown provenance , quality , and calibration can be vetted , calibrated , and made reliably useable by the community .",
    "there is an enormous amount of information about proper motions , stellar and agn variability , transients , and solar system bodies in historical plate archives and the collections of good amateur astronomers .",
    "the harvard college observatory astronomical plate stacks alone contain enough photographic exposures to cover the entire sky 500 times over , and span many decades with good coverage and imaging depth .",
    "however , in many cases , it is challenging to use those images quantitatively .",
    "often the details of observing date , telescope pointing , bandpass , and exposure time are lost because the logs have been lost , because the information was written incorrectly or illegibly , or because it is difficult or expensive to associate each image with the appropriate record in the log .",
    "the astronomical world is moving towards the development of a virtual observatory , in which a heterogeneous set of data providers communicate with researchers and the public through open data - sharing protocols .",
    "these protocols can be easily spoofed  intentionally and unintentionally  and permit the dissemination of badly calibrated , erroneous , or untrustworthy data .",
    "indeed , the lack of a `` trust '' model may compromise the vo s goals of making it easy for astronomers to discover and use wide varieties of input data ; without trust , all the time that saved in searching and using has to be spent in verification and tracking of provenance .",
    "amateur astronomers and educators at college / planetarium observatories are , in many cases , well equipped and can take science - quality observations , especially for time - domain science .",
    "in addition , most of these astronomers are interested in contributing to research astronomy .",
    "however , it is challenging at present for these potential data providers to provide their data to the community in a form that is trivially useable by the research community .",
    "these observers need automated systems to calibrate their data and hardware and to package the data and meta - data in standards - compliant forms .",
    "even professional observatories often produce data with incorrect or standards - violating meta - data because of telescope faults , software bugs , or the youth of most standards and conventions for digital data formats .",
    "we have begun a large project ( _ astrometry.net _ ) to vet , restore , determine , and package in standards - compliant form the calibration meta - data for astronomical images for which such information is lost , damaged , or unreliable @xcite .",
    "our system can astrometrically calibrate an image ( that is , determine the world coordinate system ) using the information in the image pixels alone .",
    "it starts by identifying asterisms that determine astrometric calibration .",
    "once the astrometry is correct , the sources in the image can be identified with catalogs and other calibration meta - data can be inferred through quantitative image analysis . in the process of testing and running this calibration system ,",
    "we have indeed confirmed that many historical and amateur images  and even some scientific images from modern professional facilities  have missing or incorrect astrometric meta - data ; automated calibration , as a vetting step at the very least , is essential for all data sources .",
    "for the same reason that the time domain is interesting , it can also be used to calibrate the _ date _ at which an image was taken .",
    "stars are moving and varying , so the particular configuration and relative brightnesses of the stars in an image provide , in principle , a measure of the time at which the image was taken . here",
    "we show that stellar motions catalogued at the present day can be used to age - date plates from a plate archive to within a few years , even for very old plates .",
    "this new capability takes the _ astrometry.net _",
    "project a small step closer to being a comprehensive image meta - data vetting and automated calibration system .",
    "the larger goal of our project is to add calibration meta - data to data of unknown provenance . for this reason ,",
    "the system begins truly `` blind '' in the sense that we ignore all meta - data associated with each input image , and start with only the image pixels themselves .",
    "we calibrate the images using the _ astrometry.net_astrometric system ; this calibration provides output that is taken as input data for the _",
    "blind  date _  analysis .    for each automatically detected star @xmath1 in the image",
    ", there is a centroid @xmath2 in the input image measured in pixels . for each source , this centroid is the location of the maximum of a second - order polynomial ( generalized parabolic ) surface fit to the area immediately surrounding the center of the image star .",
    "the fit also provides uncertainties @xmath3 in these centroids . for unsaturated stars these",
    "are taken ( arbitrarily ) to be one pixel , and for saturated stars ( which are common in the digitized plate data ) these are taken to be one - third of the radius of the saturated region .",
    "these are over - estimates , since the stars are detected at good signal - to - noise , but this is conservative ; furthermore , in the case of saturated stars , it is possible for the saturated `` disk '' to be non - concentric with the true centroid .",
    "the system also provides a rough world coordinate system ( wcs ) for the image ; that is , first guesses at two functions : @xmath4 , which transforms points from the image plane in pixels into celestial coordinates in angular units , and @xmath5 , the inverse .",
    "a derived quantity from these functions is the pixel scale @xmath6 , measured in angle per pixel , which we will use below .",
    "strictly @xmath6 is a function of position in the image , but in typical science images it does not vary substantially .",
    "the wcs effectively identifies the sources from the usno - b catalog  @xcite that are in or likely to be in the image . for each catalog",
    "`` star '' @xmath7 ( the usno - b catalog  contains both stars and compact galaxies ) inside the image , the catalog contains a j2000 celestial position @xmath8 on the celestial sphere ( extrapolated to epoch @xmath9 ) measured in angular units , an uncertainty @xmath10 in that position , a proper motion @xmath11 measured in angle per time , and an uncertainty @xmath12 in that proper motion .",
    "the usno - b catalog  uncertainties required some processing and adjustment . for the sake of clarity and simplicity",
    ", we make as few assumptions as possible in transforming the uncertainties .",
    "our approach here is not intended to be definitive .",
    "many of these entries in the catalog have values of zero for the uncertainty of position or proper motion . in the case of zero - valued uncertainty in position",
    ", we assume that the uncertainty is below the precision of @xmath13 at which the catalog was reported .",
    "we therefore set all zero - valued position uncertainties to one - half of the precision ( @xmath14 ) . for entries with zero - valued uncertainty in proper motion ,",
    "there is more we need to consider ; a nonzero - valued proper motion paired with a zero - valued uncertainty indicates that the uncertainty is below the precision of the catalog , and therefore should be set to half of the precision .",
    "a zero - valued proper motion paired with a zero - valued uncertainty indicates that the proper motion of the entry could not be measured accurately ( dave monet , private communication ) .",
    "we therefore set the uncertainty in the proper motion for such entries to three times the median value of nonzero proper motion uncertainties .",
    "this captures the idea that , generally speaking , we are significantly more uncertain about the proper motion of such entries than we are most other entries . a more principled approach could certainly be attempted but we found that this worked well enough for our purposes .",
    "much work has already been done by the _",
    "astrometry.net _",
    "team to identify spurious sources in the usno - b catalog  that appear to have been created by diffraction spikes and reflection halos @xcite . for the purposes of this project",
    ", we ignore the entries in the catalog which have been flagged as spurious .",
    "testing has shown that ignoring these sources generally improves the accuracy of our results .    using a technique that will be described in a future paper from the _ astrometry.net _",
    "team , we are able to estimate the bandpass of each image being processed , in that we determine which bandpass of the usno - b catalog  most closely predicts the brightness ordering of the stars in the image . though this technique is still in an experimental stage , its results are not very controversial ; all of harvard s images of m44 appear to best match the blue bands ( @xmath15 and @xmath16 emulsions ) of the usno - b catalog .",
    "this finding is reinforced through experimentation with manually setting each image s band : on average , _ blind  date _  performs better on these images using the @xmath16 emulsion of the catalog than on any other band .    assuming that we have estimated the bandpass correctly , the @xmath17 stars in the image should correspond  roughly  to the @xmath17 brightest catalog stars that lie within the area of the image .",
    "for that reason , we use only the @xmath17 brightest catalog stars in what follows .",
    "we use the usno - b catalog  positions and proper motions to `` wind '' the @xmath17 catalog stars backwards and forward through time along the celestial sphere .",
    "once the catalog has been adjusted , we can use the image wcs to project the catalog entries onto the image plane , making a synthetic catalog for that image at that time , in image coordinates .",
    "we then attempt to fit the image stars to that synthetic image of the moved catalog stars .",
    "we choose a freedom with which the image star positions are allowed to warp to fit the catalog star positions , and a scalar objective function that is minimized when the positions are `` best '' warped .",
    "we warp the input image to the catalog `` wound '' to different times , and use the best - fit values of the objective function to determine the year at which the image was taken .",
    "we estimate the celestial coordinates of catalog star @xmath7 at the arbitrary date @xmath18 , and then project them onto the image plane @xmath19      ,      { { \\mathrm{dec}}_{j } } - { \\mu_{{\\mathrm{dec}}{j } } } [ t-({2000.0}\\,{{\\mathrm{yr } } } ) ]      \\right )      \\quad , \\end{aligned}\\ ] ] where we have adjusted the @xmath20 proper motion vector components and their associated uncertainties into coordinate derivatives @xmath21 by @xmath22 we estimate the uncertainty of the location of catalog star @xmath7 at year @xmath18 on the image plane with @xmath23 where @xmath24 and @xmath25 are the dates at which the usno - b catalog  source imagery were taken , which in this patch of the sky are 1955.0  and 1990.0 , respectively .",
    "all positions in the catalog , however , were extrapolated to the year @xmath9 ( epoch and equinox ) .",
    "this means that though we are given each catalog star s location at the year @xmath9 , we know that each star s measured location is , in general , more accurate between the two epochs in which the images were taken , and less accurate at years further from that range .",
    "this means that when `` winding '' locations through time , we look at the difference between @xmath18 and the year @xmath9 ; when `` winding '' uncertainties through time , we look at the maximum distance between @xmath18 and both @xmath24 and @xmath25 .",
    "this is equivalent to using @xmath26 as our reference year , and assuming a non - zero uncertainty on the measurement of each star s proper motion at that reference year .",
    "note that in our notation for @xmath27 and @xmath28 , we do not reference @xmath18 .",
    "this is because once the catalog has been wound through time and projected onto the image plane , we consider time to be fixed . note that in sections [ sec : objectivesection ] and [ sec : fittingsection ] , time will remain fixed , and therefore @xmath18 is not mentioned in any of the notation except for in @xmath29 .",
    "determination of the image coordinate system and date involves finding parameters  astrometric parameters and the date  that optimize an objective function .",
    "the choice of this objective is therefore the fundamental scientific choice in the project .",
    "we seek an objective function that has the following properties , listed in rough order of priority : the function must decrease as image - coordinate distances between catalog and image stars decrease .",
    "the function must be insensitive to anomalous outliers , and more sensitive to well - measured stars than to poorly - measured stars .",
    "the function must be some approximation to a likelihood or have some equivalent justification so that changes in the function with respect to parameters can be interpreted in terms of uncertainties in those parameters .",
    "the function ought to be differentiable and second - differentiable with respect to all fit parameters ( in particular time and astrometric calibration ) .",
    "the function should be easily optimized .",
    "we have identified an objective function that has all of these properties ; it is so similar to the least - square function that we call it a `` modified chi - squared '' and denote it `` @xmath30 '' .    for all @xmath31 and @xmath32",
    "we compute @xmath33 , the euclidian distance between image star @xmath1 and catalog stars @xmath7 in the image plane .",
    "we also estimate the uncertainty @xmath35 of each pair s distance measurement , using the previously defined values for @xmath3 and @xmath28 .",
    "we calculate the combined uncertainty for the pair in @xmath36 and @xmath37 at time @xmath18 , and then simply take the mean as an estimation of the uncertainty of that pair :    @xmath38    we define a weighting function for each pair that returns a value between 0 and 1 based on the ratio of the distance of a pair to the uncertainty of that pair : @xmath39 ( see also figure  [ fig : weightcurve ] ) .",
    "this function has the property that outliers are down - weighed in a smooth manner ; it causes the influence of a image  catalog pair to smoothly drop to zero at large displacement .",
    "this permits us to avoid the discontinuous optimization problem of sigma clipping , which is the standard `` robust estimation '' technique in common use in astronomy applications .",
    "the weighting function has a number of properties which make it well - suited to our purpose : @xmath40    we use this weighting function to assign a weight @xmath41 to all pairs as follows : @xmath42 our final objective function is : @xmath43 where the sum is over all possible image  catalog pairs .    for `` good '' ( small - separation ) image  catalog pairs ,",
    "the weight function is near unity ( equation [ eqn : wnearzero ] ) and has near - zero derivatives ( equation [ eqn : dwnearzero ] ) , so small changes in separation do not enter strongly into derivatives of the objective function . for `` bad '' ( large - separation ) image  catalog pairs , the pair s contribution to @xmath29 is nearly constant ( equation [ eqn : wbig ] ) .",
    "this makes optimization and interpretation of our objective function very like optimization and interpretation of a chi - squared fitting system .",
    "this weighted chi - squared objective function could also be interpreted as a geman - mclure error function , in this framework , optimizing the objective function is equivalent to robust m - estimation @xcite .",
    "in constructing the modified chi - squared , we use  in principle  all image  catalog pairs , irrespective of their separation in image coordinates .",
    "the fact that the contribution of a pair to the objective function quickly converges to @xmath44 as the pair becomes highly separated allows us  in practice  to ignore all highly separated pairs .",
    "we therefore choose to approximate the contributions of all pairs where @xmath45 as @xmath46 , which is @xmath47 .",
    "this dramatically speeds up our computation .      at this point",
    ", we have our image stars on the image plane , our catalog stars ( wound to the time of interest ) projected onto the image plane , and an objective function that we wish to minimize .",
    "we need to find the transformation that we can apply to the image that minimizes the objective function , which we take to be the transformation that best `` fits '' the image to the catalog .",
    "testing has suggested that the initial location of the image returned by the _ astrometry.net _",
    "solver is close enough to the optimal location that locally minimizing the objective function is sufficient for finding the global minimum , and that we generally do not run the risk of falling into a false local minimum .",
    "therefore , we only present our method for locally minimizing the optimal function through iteratively reweighted least - squares ( irls ) .",
    "we experimented with techniques such as ransac to fit images to the catalog in the face of extreme noise , but no technique was more effective and robust than our irls method .",
    "let us first construct a solution to a simplified version of this problem , in which we know which correspondences are true : we assume that image point @xmath1 corresponds to catalog point @xmath1 for all @xmath48 . assuming that we are interested in solving for an affine transformation",
    "( first - order linear transformation plus shift ) , this means that we need to find the transformation matrix that best satisfies the following equations : @xmath49      \\left[\\begin{array}{c }      { x_{i } } \\\\      { y_{i } } \\\\      1      \\end{array } \\right ]      =      \\left[\\begin{array}{c }      { u_{i } } \\\\      { v_{i } }      \\end{array } \\right]\\ ] ]    this can be generalized straightforwardly for higher order transformations .",
    "the transformation that best satisfies these equations can be found using a standard least - squares solver .",
    "we can then use this transformation to warp all of the image points onto the catalog points ( and vice - versa ) , thus solving our simplified problem .",
    "of course , since we do not know which image stars correspond to which catalog stars , we must include equations for all image  catalog pairs .",
    "we are not interested in the solution to this problem , as it would describe a transformation from _ every _ image star to _ every _ catalog star . to specify a transformation that satisfies _ likely _ image  catalog correspondences",
    ", we must use our weighting function to make soft assignments regarding correspondences .",
    "we therefore use the following equations : @xmath50      \\left[\\begin{array}{ccc }      m_x & m_y & t_x \\\\",
    "n_x & n_y & t_y      \\end{array } \\right ]      \\left[\\begin{array}{c }      { x_{i } } \\\\",
    "{ y_{i } } \\\\      1      \\end{array } \\right ]      =      \\left[\\begin{array}{cc }      \\frac{\\sqrt{{w_{ij}}}}{{{\\bar{\\sigma}}_{ij } } } & 0 \\\\      0 & \\frac{\\sqrt{{w_{ij}}}}{{{\\bar{\\sigma}}_{ij } } }      \\end{array } \\right ]      \\left[\\begin{array}{c }      { u_{j } } \\\\      { v_{j } }      \\end{array } \\right]\\ ] ]    we begin our solution by constructing a linear system which contains all of the previously described ( unweighted ) equations .",
    "@xmath51           \\left[\\begin{array}{c }      m_x \\\\      m_y \\\\      t_x",
    "\\\\      n_x \\\\      n_y \\\\",
    "t_y       \\end{array } \\right ]      =      \\left[\\begin{array}{c }      { u_{1 } } \\\\      { v_{1 } } \\\\      { u_{2 } } \\\\      { v_{2 } } \\\\      \\ldots \\\\      { u_{n-1 } } \\\\      { v_{n-1 } } \\\\      { u_{n } } \\\\      { v_{n } }      \\end{array } \\right]\\ ] ]    we can write this matrix equation as : @xmath52    to introduce the weight values described in the equations , we construct our weight matrix @xmath53 , as follows : @xmath54    our final matrix equation can then be written as : @xmath55 we then find @xmath56 such that the squared residuals , @xmath57 , are minimized . by construction",
    ", @xmath56 describes the transformation that best satisfies all of the equations we previously constructed , and can be found using a standard least - squares solver .    using the transformation defined by @xmath56 we can calculate the coordinates of our warped image points .",
    "@xmath58    unlike the simplified version of this problem , the warp found after one iteration is not our final solution . as we warp the image , the distances and weights between the image and catalog points change , and",
    "so our objective function is no longer minimized .",
    "the solution is to repeatedly recalculate our @xmath33 and @xmath41 values and our weighted least squares transformation .",
    "we perform this iteratively reweighted least - squares operation until the resulting transformations stop changing , which by construction is also when our objective function stops changing .",
    "the solution which we converge upon is returned as the best fit of the image onto the catalog .",
    "the irls method minimizes the objective function , as it was constructed such that the objective function is equal to the sum of the squares of the weighted residuals of the matrix equation .",
    "we say that the residuals only _ approximate _ the optimal function because we use the weights of the previous transformed image ( @xmath41 ) and the distances of the next warped transformed image ( @xmath60 ) .",
    "though this may seem strange , once the irls has converged on a solution and the optimal function has stopped changing , the distances in one iteration are effectively equal to the distances in the following iteration .",
    "for numerical stability we always use the initial coordinates when constructing our transformation , and only return the final warped image coordinates .",
    "the intermediate warped image coordinates are only used for calculating distances and weights .",
    "this means that our final image coordinates have only been subjected to one transformation , as opposed to many small transformations .",
    "see figure  [ fig : fitexamples ] for some examples of fit and unfit images against correct and incorrect catalog dates .",
    "now that we have defined a method for fitting an image to the catalog , and a metric by which we can assess the degree to which an image can be fit to the catalog , we can use a number of techniques to estimate the year in which the image can best be fit to the catalog .",
    "we take that year to be the year in which the image was created .",
    "this problem can be phrased as such : an image has some unknown chi - squared curve @xmath29 , of which we wish to find the year @xmath61 , the theoretical optimal year such that : @xmath62    in the algorithm we will describe , finding @xmath61 is not computationally feasible , so we must settle on finding a reasonable approximation of the minimum @xmath63 , such that given an accuracy threshold @xmath64 we are confident that : @xmath65    once we have found @xmath63 and therefore @xmath66 , we also wish to find the extents of our uncertainty region , @xmath67 , such that : @xmath68}{\\forall } { \\ , \\chi^2(t ) \\leq \\chi^2(t^*)+1 }      \\end{array}\\ ] ]    ideally , we want to find all of these values as accurately as possible while sampling the @xmath30 curve as few times as possible .",
    "there are a number of methods by which we can accomplish this task , each with different tradeoffs concerning efficiency and assumptions about the shape of the curve .    the simplest method for estimating the origin date is through brute force .",
    "we sample our @xmath30 curve at regular intervals , and take the year in which our objective function scored the lowest as @xmath63 .",
    "we then linearly interpolate along the curve to find the extents of the uncertainty region .",
    "this method is terribly inefficient and assumes nothing about the shape of the curve , so we only use it as an approximate ground truth to which we will compare our final algorithm .",
    "our search algorithm begins with first sampling our @xmath30 curve at a very broad , regular interval .",
    "though only one initial sample is required , for the figures shown in this paper we sample the curve at @xmath69 , @xmath70 , and @xmath71 .",
    "we take the sampled year with the lowest @xmath30 score to be @xmath72 , and we then iteratively refine @xmath72 into @xmath73 until we believe that we have found @xmath63 .",
    "our objective function was constructed such that we could efficiently calculate @xmath74 and @xmath75 .",
    "the equations for these analytical derivatives are elaborate , so we do not present them here .",
    "since @xmath66 is a minimum in the @xmath30 curve , we can assume that @xmath76 .",
    "we can therefore use newton s method to find @xmath73 : @xmath77    we iteratively evaluate @xmath73 until @xmath78 holds for two consecutive iterations , at which point we take @xmath73 as @xmath63 . since newton s method generally converges quadratically , this is an extremely fast process .",
    "is not globally quadratic , newton s method is not guaranteed to converge from any starting point . to improve numerical conditioning and ensure that the search remains well - behaved in the face of somewhat irregular @xmath30 curves , we require that @xmath79 , where @xmath80 is the smallest representable number @xmath81 on our machine and we require that @xmath82 years . additionally , if newton s method appears to be diverging or failing to converge , we find @xmath73 using a binary - search approach in which we sample the midpoint of the area in which the minimum appears to lie . this collection of restrictions effectively amounts intelligent gradient descent , which we switch to when newton s method can not be performed",
    ". this fallback system is rarely required , but does sometimes prevent oscillation and search failure . ]",
    "once we have found @xmath63 , we can locate @xmath83 and @xmath84 .",
    "we use our modified newton s method with these new goals : @xmath85 this uncertainty region would be the true one - sigma uncertainty region in the limit that the modified chi - squared were the standard linear - fitting chi - squared . because of the weighting function , in practice this criterion over - estimates the one - sigma uncertainty .",
    "we can perform root - finding on these equations using the following formula for iteration : @xmath86    we begin our two new searches with a sensible initial estimate of the bounds of the uncertainty region , based on our present knowledge of the @xmath30 curve .",
    "these searches operate under all of the constraints under which the previously detailed search operated , and also converges when @xmath87 holds for two consecutive iterations .",
    "additionally , we can speed up the total search process by using the transformed image points from the previous iteration to find the new transformation for the next iteration .",
    "this heuristic means that as the search converges on a final result , the amount of time required to query new years is dramatically reduced . also , it becomes easier to visualize the search algorithm as a single bidirectional fitting process , in which we repeatedly fit the image to the catalog and the catalog to the image until both fittings have converged . just as in the previous section ,",
    "all transformations are constructed using the initial positions of the points , so our final transformation after searching the @xmath30 curve is still very numerically stable .",
    "figure  [ fig : searchmethodcompare ] shows a comparison of this search algorithm against a brute - force `` ground truth '' .",
    "figure  [ fig : chisqresults ] shows the modified @xmath30 curves for each image as they were estimated by this search algorithm .    the output of this process is a polynomial description of the image astrometric wcs , a best - fit year value @xmath63 , and an uncertainty region around that value .      _",
    "blind  date _",
    "s two primary performance bottlenecks are calculating the distances between image and catalog points and solving the weighted least - squares problems . in both cases , we are able to use the properties of our weighting function to construct approximate solutions that very effectively approximate the true solution .",
    "our algorithm theoretically requires us to repeatedly calculate the distances of all image  catalog pairs .",
    "however , due to the properties of the weighting function as described in section [ sec : objectivesection ] , we do not need to know the distances of significantly separated pairs . because the transformation applied at each iteration tends to be very small",
    ", we can generally assume that significantly separated pairs in one iteration will also be significantly separated in the next iteration .",
    "this allow us to do one initial calculation of all image  catalog distances , but in later iterations only calculate the distances of image  catalog pairs that will likely cause a change in the value of the objective function .",
    "this is a rough heuristic , so we safeguard ourselves by manually recalculating all image  catalog distances every 10 iterations , as well as whenever the irls begins to converge .",
    "this dramatically speeds up our algorithm , while producing nearly identical results to the nave brute - force approach .",
    "we use the aforementioned properties of our weight function to determine if an image  catalog pair should be considered in the weighted least - squares calculation .",
    "a highly separated pair always contributes a nearly - constant value to the residuals , and therefore can be safely ignored .",
    "this gives us a slight performance boost .",
    "we require an additional threshold for the difference between the optimal function from one iteration to the next , which determines when our irls operation has converged .",
    "we use the very conservative value of @xmath88 as the maximum amount that the @xmath30 score of the final irls iteration is allowed to change from those of the previous two irls iterations .",
    "harvard s interface for accessing its scanned plates of m44 made it difficult to obtain more than @xmath89 by @xmath89 pixel subsets of the images , though the entire plates are significantly larger .",
    "the interface for downloading the images did not provide an obvious mechanism for selecting the same @xmath89 by @xmath89 pixel subsets of each image , which means that such selection was done by hand , and is therefore not very accurate .",
    "many of the plates suffer from the many sources of noise typical of historical imagery : some are multiple - exposures , badly out of focus , badly saturated , or cracked , and some contain handwritten labels , digital scanning artifacts , and bad trailing . for the sake of fairly assessing _ blind  date _",
    "s performance , we split the images into two sets ; 27  `` science - quality '' images and 20``low - quality '' ( see figure  [ fig : imageexamples ] for examples ) . for our convenience , we used the jpeg versions of the images , which probably introduces some minor noise in the form of compression artifacts .",
    "harvard graciously provided ground - truth dates for each image , which presumably were taken from logs or from writing on the plates .",
    "we take these dates to be true .",
    "though the ground - truth dates range from 1910 to 1975 , the dates are not uniformly distributed .",
    "see the distribution of images along the y - axis of figure  [ fig : performance ] for a demonstration of this clustering .",
    "the tests were performed using the modified newton s method , with affine distortions and an accuracy threshold @xmath64 of @xmath88 year .",
    "tests were done on a 2007 macbook with a 2ghz intel core 2 duo processor , and 2 gb of ram .",
    "median runtime for estimating each image s date was @xmath90 seconds per image , after source extraction and _",
    "astrometry.net_ s initial calibration .",
    "results are shown in table  [ table : performance ] .",
    ".accuracy of estimated dates for the two subsets of data .",
    "see figures [ fig : performance ] and [ fig : errordist ] for a more detailed visualization of the results . [",
    "cols=\"<,^,^,^\",options=\"header \" , ]     though the results shown were generated by fitting an affine ( linear ) transformation in image coordinates , we experimented with increasing the order of the polynomial warp being fitted .",
    "results were very similar to those obtained using affine transformations , although the median absolute error for science - quality images dropped to @xmath91 years for second - order transformation , and to @xmath92 years for third - order transformations .",
    "median absolute error for low - quality images also decreased slightly as the order was increased , as did the bias for science - quality images .",
    "additionally , we tested _",
    "blind  date _  on the five usno - b catalog  source images of m44 that we were able to retrieve from the us naval observatory precision measuring machine data archive , and on the sloan digital sky survey @xcite image of m44 .",
    "the dates of the usno - b catalog  source imagery were estimated accurately ( all within six years of the true dates , and all within the uncertainties ) , which is as good as we would expect performance to be given the relatively low resolution ( @xmath93 ) of the source imagery that we were able to obtain . the sdss image was estimated to have been taken in late 2004 , and was actually taken in 2006 .",
    "we assembled an alternate test - bed of ten amateur images of m44 that we were able to find online .",
    "the websites on which we found most of our imagery did not explicitly note the date at which the image was taken , so we were forced to use the `` date '' tag in each image s exif meta - data as the ground truth .",
    "for many of the images , _ blind  date _  provided accurate dates and uncertainties that are consistent with our previous findings : all estimated dates lay within the our uncertainty bounds , accuracy generally depended on the resolution and quality of each image , and most estimated dates ( for all sufficiently high - resolution images ) were within a few years from the true dates .",
    "our ground - truth dates are , unfortunately , very unreliable , as the exif data may simply reflect the date at which an image was digitized or modified , rather than the date at which it was imaged . though this means that we are not able to truly vet _",
    "blind  date _",
    "s performance for these amateur images , this issue also highlights the utility of this system : the dates of origin of these images are effectively lost , but can be re - estimated .",
    "we have shown that our _ blind  date _  system can successfully attach time meta - data to historical imaging data .",
    "the system runs in seconds on standard inexpensive consumer computer equipment ; it does not require large investments of time or money to vet or create time meta - data for large collections of astronomical imaging .    the performance of _ blind  date _  will depend on the properties of the input image , and on the properties of the catalog information known about the region of the sky that is being imaged .",
    "we can phrase this as two questions : `` what is the information content in an image ? '' , and `` what is the information content in a catalog star ? ''    in an attempt to empirically assess the information content of an image , we ran a simple experiment in which we varied the resolution of an input images and the number of stars contained in an input image ( by downsampling and cropping the image , respectively ) .",
    "the results are shown in figure  [ fig : performancevs ] , where we see that performance depends heavily on an image containing a large number of stars imaged at high resolution .",
    "we also explored the effects of different kinds of imaging defects .",
    "our objective function is designed to be robust to false sources , and as such , _ blind  date",
    "_  performs very well on images with multiple exposures .",
    "our experiments suggests that saturation , large psf due to poor focus or trailing , and short exposure time ( low sensitivity ) most negatively effect _ blind  date _",
    "s performance .",
    "see figure  [ fig : imageexamples ] for examples of our accuracy in the face of different kinds of imaging defects . trailing and saturation",
    "can effectively be thought of as decreasing the resolution of our input image ( by decreasing our ability to accurately centroid stars ) , and shallow imaging is effectively equivalent to dropping dim stars out of the image ; these are the two trends demonstrated in figure  [ fig : performancevs ] . once again , _ blind  date _",
    "s accuracy appears to depend on an image containing many well - imaged ( high resolution ) stars .",
    "the information in a single catalog star ( provided that it has been detected in the input image , in the limit that our procedure is equivalent to least - square fitting ) is proportional to that star s contribution to the second derivative of @xmath30 with respect to date .",
    "this contribution is approximately the square of the magnitude of the star s proper motion , divided by the square of the uncertainty , that is , the square of the signal - to - noise at which the proper motion is detected ( where the `` noise '' in this case is the combined uncertainty from the catalog and the image as in equation  [ eq : uncertainty ] ) .",
    "we expect _ blind  date _ s performance on an image to scale roughly with the sum of the squares of the detected catalog stars proper motion signal - to - noise ratios .",
    "imagery unlike the imagery analyzed here ought to obtain date calibration with uncertainty that goes down as the sum of the detected catalog stars proper motion signal - to - noise ratios goes up .",
    "increasing the polynomial order of the transformation on the image plane produces slightly more accurate date estimates , presumably because the input images do have distortions that are represented reasonably by these functions .",
    "we are reluctant to advocate unnecessarily large polynomial orders , as  theoretically  the more freedom we give the transformation model , the more irregular our resulting @xmath30 curves may become .",
    "that being said , we have not seen any evidence that reinforces such a concern . in principle",
    ", even more accurate results could be obtained without increasing the number of degrees of freedom in the fit by employing a physical camera model that represents known distortions in the particular camera used to take the data .    for each image in our dataset",
    ", we performed an experiment to determine the range of initial dates for which our search algorithm is robust .",
    "we discovered each image had a range of at least @xmath94 years ( and on average , @xmath95 years ) roughly centered around the true year from which the search could be initialized without the final result being affected .",
    "if we ignore our precaution of using gradient descent when the second derivative of the @xmath30 curve is non - positive , this range is significantly smaller ( on average , about @xmath96 years ) .",
    "this finding highlights the importance of the modifications we make to newton s method in constructing our search algorithm , and also suggests that the coarse grid of queries with which we initialize our search is unnecessary  a single initial query in the correct century would have been more than sufficient .    _",
    "blind  date _  largely ignores one very important source of information , namely the brightnesses of the image and catalog stars .",
    "this data is used in our band - pass estimation step , but is then largely ignored . in future versions of",
    "_ astrometry.net _",
    "we plan to utilize this data in a number of ways .",
    "we eventually hope to simultaneously estimate all parameters of a given image , including the image s wcs , its date of origin , and its band - pass .",
    "estimating all of these simultaneously should means that brightness information is implicitly used in our estimation of the location and date , and should improve our results accordingly .",
    "this would also solve our current conundrum regarding this process , which is that image  catalog correspondences are required for band - pass estimation , while band - pass estimation is required for finding image  catalog correspondences .",
    "analysis of the brightness of image stars may be able to play a profound role in date estimation if we consider the subset with periodic variability .",
    "given an image containing @xmath97 stars with different periods , and given sufficient information concerning the periodic variations in their brightnesses , we should be able to constrain the date of origin of the image to one of a set of time intervals in which those @xmath97 stars are at whatever particular point in their periods ( to within photometric precision ) .",
    "given the set of intervals constrained by the periodic variations , we can use the range of dates determined by _",
    "blind  date _",
    "( that is , from the proper motions of the stars ) to select a potentially very narrow time interval in which the image must have originated . in principle",
    ", it may even be possible to determine the date of origin of an image solely though periodic brightness , though that would require very good measurements of the periods of the catalog stars and of the brightnesses of the image stars .    _ blind  date _ s value , on the most superficial level , is clear : this system could be used to recover lost meta - data ( at low precision ) for historical and amateur data that have been archived poorly or not at all .",
    "now that large scanning projects are underway at photographic archives and the web is providing new opportunities for file sharing among amateurs and professionals , we need systems that automatically vet and provide meta - data for data of unknown provenance .",
    "regardless of whether or not imagery already contains reliable date meta - data , the techniques described in _ blind  date _  may have deep - seated implications for the calibration of all imagery not taken at the year @xmath9 .",
    "the fact that the date can be well estimated from input images demonstrates both that the images contain important information about stellar motions , _ and _ that astrometric calibration is hampered when calibration is performed with a catalog projected to an epoch far from the date of the image . a system that is time sensitive , such as _ blind  date _",
    ", will plausibly provide the best astrometric calibration possible for arbitrary imaging .    what may be _ blind  date _",
    "s most important consequence is an inversion of the system , in which we attempt to use imagery to re - estimate the proper motions of catalog stars .",
    "the most straightforward approach to this would be to `` cheat '' and use the ground - truth dates of all input imagery .",
    "one could repeatedly : calibrate each image using the catalog wound to that image s date - of - origin , re - estimate the proper motions of the catalog stars , and re - wind the catalog using those new proper motions .",
    "this could be thought of as performing expectation - maximization on the proper motions of the catalog .",
    "of course , an ideal system would be robust to some ( or all ) input imagery not having ground - truth ages .",
    "we could estimate the date - of - origin of all unlabeled imagery , and use these estimates ( and their uncertainties ) in our expectation - maximization .",
    "labeled and unlabeled data could be treated equivalently , except that labeled data would have much less uncertainty associated with it .",
    "this system would then become a two - way street , in which we do not just reposition images relative to the sky , but also reposition the sky relative to the images , and dynamically develop a consensus between the two .",
    "the future _ astrometry.net _  `` catalog '' would not have to be a static entity , but would instead be a consensus of all available imagery  using the usno - b catalog  as a static `` prior . '' _ blind  date",
    "_  takes us one step closer to this grand long - term hope for _",
    "_  becoming an always - changing database of everything we know about the sky , by allowing _",
    "time _ to become one more dimension of our data .",
    "we would like to acknowledge generous assistance from mike blanton , rob fergus , yann lecun , brett mensh , keir mierle , and dave monet .",
    "we thank the usno - b and dasch teams for providing the data used for this study .",
    "this project made use of the nasa astrophysics data system , the us naval observatory precision measuring machine data archive , and data and code from the _ astrometry.net _  project ."
  ],
  "abstract_text": [
    "<S> astrometric calibration is based on patterns of cataloged stars and therefore effectively assumes a particular epoch , which can be substantially incorrect for historical images . with the known proper motions of stars we can `` run back the clock '' to an approximation of the night sky in any given year , and in principle </S>",
    "<S> the year that best fits stellar patterns in any given image is an estimate of the year in which that image was taken . in this paper </S>",
    "<S> we use 47 scanned photographic images of m44 spanning years 19101975 to demonstrate this technique . </S>",
    "<S> we use only the pixel information in the images ; we use no prior information or meta - data about image pointing , scale , orientation , or date . _ </S>",
    "<S> blind  date _  returns date meta - data for the input images . </S>",
    "<S> it also improves the astrometric calibration of the image because the final astrometric calibration is performed at the appropriate epoch . the accuracy and reliability of _ blind  date _  </S>",
    "<S> are functions of image size , pointing , angular resolution , and depth ; performance is related to the sum of proper - motion signal - to - noise ratios for catalog stars measured in the input image . </S>",
    "<S> all of the science - quality images and @xmath0  percent of the low - quality images in our sample of photographic plate images of m44 have their dates reliably determined to within a decade , many to within months . </S>"
  ]
}