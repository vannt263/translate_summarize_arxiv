{
  "article_text": [
    "with the rapid growth in the availability and size of digital health data and wearable sensors , along with the rise of newer machine learning methods , health care analytics has become a hot area of research today .",
    "the main bottlenecks for solving a healthcare data analytics problem are : a ) effort required to build good models in terms of time , money and expertise b ) interpreting model features so that a healthcare expert can do a causality analysis and take preventable measures or derive meaningful insights backed by domain knowledge .",
    "a typical analytics solution requires a ) pre - processing b ) feature extraction c ) feature selection d ) modeling such as classification or regression . among these steps ,",
    "feature extraction and feature selection together form feature engineering ( fe ) and is the most time consuming and human expertise demanding among the rest .",
    "feature engineering can be broadly carried out in four ways : ( a ) manually selecting features guided by domain knowledge ( b ) recommending features by automated analysis - proposed method ( c ) feature transforms like principal component analysis ( pca ) ( d ) representation learning using deep architectures such as deep multi - layered perceptron ( mlp ) and convolutional neural network ( cnn ) . through experiments on 3 different types of healthcare datasets including a recent challenge dataset and comparison of the approaches ,",
    "the utility of our proposed method ( b ) has been shown .",
    "interpretation of features is not supported by deep learning and feature transform methods .",
    "but , manual feature engineering and our proposed method yield interpretable features which is very helpful in prognostic domains like healthcare .",
    "while in deep architectures , the different activation functions can be hierarchically stacked to form new structures , in our approach , this does not hold true .",
    "for example , wavelet transforms applied on fourier transforms does not make sense .",
    "hence , here the emphasis is on creating a wide architecture with meaningful hierarchies so that lowest layer contains basic feature extraction techniques , and as we move up we keep adding more meaningful layers on top of what was extracted .",
    "this helps in deriving physical interpretation of features ( from bottom to top ) .",
    "the dataset is partioned into p - folds of training , evaluation and testing sets ( range of p is 5 to 10 ) .",
    "the performance is reported on the hidden testing set .",
    "the proposed method consists of 3 steps :    _ 1 .",
    "feature listing _ : we have organized commonly reported features ( in the literature of sensor data analytics ) in a hierarchical manner as shown in figure 1 .",
    "the basic features ( level 0 ) can be mainly categorized as : ( i ) time domain features ( td ) ( ii ) fourier transformation based features ( fd ) like short - time fourier transform ( stft ) ( iii ) discrete wavelet transformation based features ( dwt ) .",
    "one major challenge of using dwt features is the selection of suitable mother wavelet , as more than 100 different types of mother wavelets were reported in different papers .",
    "the automated mother wavelet selection is done by measuring energy to entropy ratio [ 1 ] . in level 1 , spectral ,",
    "statistical and peak - trough features are extracted .",
    "level 2 includes different ratios and derivatives of the level 1 features .",
    "the system has capability of easy plugging of new feature extraction algorithms that will lead to a collaborative ecosystem .",
    "hence , it is possible to get huge number ( say , @xmath0 ) of features ( including the transform domain coefficients ) from the sensor signals .",
    "this results in @xmath1 possible combinations of features , whose exploration is practically infeasible , thereby demanding usage of feature selection .",
    "_ 2 . feature selection _ : in our method , we followed an iterative feature selection where @xmath2-features are selected ( k@xmath3n ) at each iteration and system performance ( e.g. classification accuracy ) is checked for this feature set . if the selected feature set results in _ expected _ performance , we return the feature set as the recommended one .",
    "otherwise , another @xmath2-features are chosen in the next iteration and the same steps are repeated . for checking the classification accuracy ,",
    "we choose svm ( support vector machine ) based classification with different kernels .",
    "svm was selected as a classifier as it generalizes well and converges fast .",
    "several values of @xmath2 are tried to choose an optimal value . for a given value of @xmath2",
    ", features are selected using two techniques namely , mrmr [ 2 ] and mrms [ 3 ] , described below : _ minimum redundancy and maximum relevance_(mrmr ) : in order to select effective features , mrmr optimizes an objective function , either mutual information difference ( mid ) or mutual information quotient ( miq ) , by minimizing the redundancy and maximizing the relevance of the features .",
    "mid ( additive ) and miq ( multiplicative ) are defined as follows .",
    "+    @xmath4     + where @xmath5 minimizes redundancy by computing f - statistics and @xmath6 maximizes relevance by computing correlation between a pair of features . + _ maximal relevance maximum significance_@xmath7(mrms ) : this technique uses fuzzy - rough set selection criteria to select relevant and non - redundant ( significant ) features . the objective function is : +    @xmath8     + where @xmath9 computes relevance of a recommended feature with respect to a class label and @xmath10 computes the significance of a pair of recommended features by computing their correlation , and @xmath11 is the weight parameter .",
    "let , @xmath12 and @xmath13 be the sets of features recommended by mrmr and mrms , respectively .",
    "then the recommended set of features r is @xmath14 , where @xmath15 , where @xmath16 . note that mrmr and mrms cover different aspects of feature selection .",
    "for instance , mrmr is classifier independent where as mrms is effective to reduce real valued noisy features which are likely to occur in large feature sets .",
    "_ 3 . feature recommendation _ : the system finds 2 feature sets for a particular performance metric ( such as accuracy , sensitivity , specificity , precision , f - score ) : a ) fe1 - that produces the highest metric in any fold of cross - validation",
    "b ) fe2 - that is most consistent and performs well across all folds .",
    "the above step of feature selection is done hierarchically - if layer 0 does not produce expected results set by pre - set threshold @xmath17 or maximum possible value of a selected metric , then layer 1 is invoked . similarly if layer 1 does not yield expected results , layer 2 is invoked .",
    "this follows the principle that if simple features can do the task , there is no need for complex features .",
    "` c ' is a regularizer for ` k ' and is dependent on the hardware capabilities of the system .",
    "the intuition is that on a high - end machine ( having higher valued ` c ' ) , feature combinations ( @xmath18 ) can be carried in acceptable time . using the recommended feature sets ,",
    "any classifier like svm or random forest can be trained to see the results obtained . also by looking up the recommend features from the feature listing database , interpretation of the features",
    "can be easily obtained by a domain expert .",
    "experiments were carried on 3 datasets : d1 , d2 , d3 in order to provide a comparison among the feature engineering ways ( proposed method , manual , dimension reduction and deep learning ) .",
    "_ d1 : _ the physionet 2016 challenge dataset [ 4 ] consists of 3153 heart sounds , including 2488 normal and 665 abnormal recordings .",
    "the ground truth label ( normal or abnormal heart sound ) of each record is manually annotated by expert doctors .",
    "raw pcg ( phonocardiogram ) is further down sampled to 1 khz from 2 khz , in order to segregate four cardiac states ( s1 , systole , s2 and diastole ) using the logistic regression based hsmm approach [ 5 ] .",
    "the winner [ 6 ] of the challenge used 124 features and used deep learning for classification .",
    "the challenge used their own modified metric for ranking participants , however for consistency of results across datasets , we have used accuracy score as the performance metric .",
    "we participated in the challenge using manual features and got only 1% increase in performance compared to the proposed automated method .",
    "_ d2 : _ the second dataset is derived from mimic - ii patients dataset [ 7 ] .",
    "a subset of the dataset containing ppg ( photoplethysmogram ) data was created after noise cleaning and the ground truth blood pressure ( bp ) was obtained from the simultaneously recorded arterial bp waveform , resulting in equally balanced 36 high ( > 140 mmhg reading ) and 36 low bp patient waveform data instances .",
    "_ d3 : _ the third dataset ( used to classify the emotion into happy and sad ) records the fingertip pulse oximeter ppg data of 33 healthy subjects ( female : 13 and male : 20 ) with average age 27 years .",
    "we used standard video stimuli as ground - truth and time synchronization errors were minimized .",
    "table 1 lists the obtained result for a dataset along with the corresponding configuration and effort for each of the feature engineering approaches .",
    "experiments has been carried out using theano based multi - layer perceptron with _ dropout _ and varying number of layers to see if features can be automatically learned on the datasets under experimentation .",
    "different epochs ( 5 to 15 ) has been tried to see how the learning rate affects performance .",
    "different activation functions like rectified linear unit ( relu ) , tanh , softmax , sigmoid , etc . has been tried out at different layer level to get an ideal architecture for classification task for the given problems .",
    "table 1 shows that mlp based techniques fail when compared to the state of the art and the proposed method .",
    "the problem with mlp and newer deep learning techniques like cnn is that they need a lot of data to train and there is no way to interpret the features .",
    "principal component analysis ( pca ) is a statistical procedure that uses an orthogonal transformation to derive principal components representative of the features under consideration .",
    "experiments have been carried out with aforementioned datasets and gaussian kernel is used for svm based classification .",
    "the different dimension reduction techniques used are singular value decomposition ( svd ) , eigen value decomposition ( eig ) and alternating least squares ( als ) .",
    "a varying number of principal components ( like 5 , 10 , 15 ) are also tried out .",
    "table 1 shows that pca based methods are outperformed by our proposed method .",
    "another drawback of pca and similar feature reduction techniques is that the derived features are not interpretable .",
    "it is seen that for 2nd and 3rd dataset , the proposed approach outperforms the state of the art ( soa ) methods , and for the 1st dataset , 94.38% of the accuracy level of the winner was reached by this method . in terms of effort taken to build the solution , the proposed method clearly beats others .",
    "[ cols=\"<,^,^,^,^,^\",options=\"header \" , ]     interpretable feature engineering has been found to be the most demanding task among all the subtasks of health data analytics .",
    "hence , a system was built to automate this part of the process .",
    "the system has been tested on three healthcare datasets and was found to give good results when compared to state of the art . apart from manual feature engineering , comparison has been made with mlp and pca which are feature engineering approaches of different directions .",
    "interpretation of features is one of the strong points of the proposed method .",
    "another strong point of the proposed method is huge reduction in effort to develop a typical analytics solution .",
    "integration of knowledge bases for ease of interpreting features and automated causality analysis is also planned . the work will be exteneded to other domains such as machine prognostics .",
    "[ 1 ] ngui , w. k. et al . ( 2013 ) wavelet analysis : mother wavelet selection methods , _ applied mechanics and materials _ ; vol .",
    "953 - 958 .",
    "[ 2 ] peng , h et al .",
    "( 2005 ) feature selection based on mutual information criteria of max - dependency , max - relevance , and min - redundancy . _ ieee transactions on pattern analysis and machine intelligence _ , 27.8 : 1226 - 1238 .    [ 3 ] maji , p et al ( 2012 ) fuzzy - rough mrms method for relevant and significant attribute selection , _ advances on computational intelligence : 14th international conference on information processing and management of uncertainty in knowledge - based systems ."
  ],
  "abstract_text": [
    "<S> in this paper , a wide learning architecture is proposed that attempts to automate the feature engineering portion of the machine learning ( ml ) pipeline . </S>",
    "<S> feature engineering is widely considered as the most time consuming and expert knowledge demanding portion of any ml task . </S>",
    "<S> the proposed feature recommendation approach is tested on 3 healthcare datasets : a ) physionet challenge 2016 dataset of phonocardiogram ( pcg ) signals , b ) mimic ii blood pressure classification dataset of photoplethysmogram ( ppg ) signals and c ) an emotion classification dataset of ppg signals . while the proposed method beats the state of the art techniques for 2nd and 3rd dataset , it reaches 94.38% of the accuracy level of the winner of physionet challenge 2016 . in all cases , </S>",
    "<S> the effort to reach a satisfactory performance was drastically less ( a few days ) than manual feature engineering . </S>"
  ]
}