{
  "article_text": [
    "recently neural networks ( nn ) have achieved state - of - the - art performance in various applications ranging from computer vision @xcite to natural language processing @xcite . however , nn trained by stochastic gradient descent ( sgd ) or its variants is known to suffer from overfitting especially when training data is insufficient .",
    "besides overfitting , another problem of nn comes from the underestimated uncertainty , which could lead to poor performance in applications like active learning .",
    "bayesian neural networks ( bnn ) offer the promise of tackling these problems in a principled way .",
    "early bnn works include methods based on laplace approximation @xcite , variational inference ( vi ) @xcite , and monte carlo sampling @xcite , but they have not been widely adopted due to their lack of scalability .",
    "some recent advances in this direction seem to shed light on the practical adoption of bnn .",
    "@xcite proposed a method based on vi in which a monte carlo estimate of a lower bound on the marginal likelihood is used to infer the weights .",
    "recently , @xcite used an online version of expectation propagation ( ep ) , called ` probabilistic back propagation ' ( pbp ) , for the bayesian learning of nn , and @xcite proposed ` bayes by backprop ' ( bbb ) , which can be viewed as an extension of @xcite based on the ` reparameterization trick ' @xcite .",
    "more recently , an interesting bayesian treatment called ` bayesian dark knowledge ' ( bdk ) was designed to approximate a teacher network with a simpler student network based on stochastic gradient langevin dynamics ( sgld ) @xcite .",
    "although these recent methods are more practical than earlier ones , several outstanding problems remain to be addressed : ( 1 ) most of these methods require sampling either at training time @xcite or at test time @xcite , incurring much higher cost than a ` vanilla ' nn ; ( 2 ) as mentioned in @xcite , methods based on online ep or vi do not involve sampling , but they need to compute the predictive density by integrating out the parameters , which is computationally inefficient ; ( 3 ) these methods assume gaussian distributions for the weights and neurons , allowing no flexibility to customize different distributions according to the data as is done in probabilistic graphical models ( pgm ) . to address the problems , we propose _ natural - parameter networks _",
    "( npn ) as a class of probabilistic neural networks where the input , target output , weights , and neurons can all be modeled by arbitrary exponential - family distributions ( e.g. , poisson distributions for word counts ) instead of being limited to gaussian distributions .",
    "input distributions go through layers of linear and nonlinear transformation deterministically before producing distributions to match the target output distributions ( previous work @xcite shows that providing distributions as input by corrupting the data with noise plays the role of regularization ) . as byproducts , output distributions of intermediate layers may be used as second - order representations for the associated tasks .",
    "thanks to the properties of the exponential family @xcite , distributions in npn are defined by the corresponding natural parameters which can be learned efficiently by backpropagation . unlike @xcite , npn explicitly propagates the estimates of uncertainty back and forth in deep networks .",
    "this way the uncertainty estimates for each layer of neurons are readily available for the associated tasks .",
    "our experiments show that such information is helpful when neurons of intermediate layers are used as representations like in autoencoders ( ae ) . in summary ,",
    "our main contributions are :    we propose npn as a class of probabilistic neural networks .",
    "our model combines the merits of nn and pgm in terms of computational efficiency and flexibility to customize the types of distributions for different types of data .    leveraging the properties of the exponential family ,",
    "some sampling - free backpropagation - compatible algorithms are designed to efficiently learn the distributions over weights by learning the natural parameters .",
    "unlike most probabilistic nn models , npn obtains the uncertainty of intermediate - layer neurons as byproducts , which provide valuable information to the learned representations . experiments on real - world datasets show that npn can achieve state - of - the - art performance on classification , regression , and unsupervised representation learning tasks .",
    "the exponential family refers to an important class of distributions with useful algebraic properties .",
    "distributions in the exponential family have the form @xmath0 where @xmath1 is the random variable , @xmath2 denotes the natural parameters , @xmath3 is a vector of sufficient statistics , and @xmath4 is the normalizer . for a given type of distributions , different choices of @xmath2 lead to different shapes .",
    "for example , a univariate gaussian distribution with @xmath5 corresponds to @xmath6 .    motivated by this observation , in npn , only the natural parameters need to be learned to model the distributions over the weights and neurons .",
    "consider an npn which takes a vector random distribution ( e.g. , a multivariate gaussian distribution ) as input , multiplies it by a matrix random distribution , goes through nonlinear transformation , and outputs another distribution .",
    "since all three distributions in the process can be specified by their natural parameters ( given the types of distributions ) , learning and prediction of the network can actually operate in the space of natural parameters .",
    "for example , if we use element - wise ( factorized ) gamma distributions for both the weights and neurons , the npn counterpart of a vanilla network only needs twice the number of free parameters ( weights ) and neurons since there are two natural parameters for each univariate gamma distribution .",
    "we use boldface uppercase letters like @xmath7 to denote matrices and boldface lowercase letters like @xmath8 for vectors .",
    "similarly , a boldface number ( e.g. , @xmath9 or @xmath10 ) represents a row vector or a matrix with identical entries . in npn , @xmath11 is used to denote the values of neurons in layer @xmath12 before nonlinear transformation and @xmath13 is for the values after nonlinear transformation . as mentioned above",
    ", npn tries to learn _ distributions _ over variables rather than _",
    "variables _ themselves .",
    "hence we use letters _ without _",
    "subscripts @xmath14 , @xmath15 , @xmath16 , and @xmath17 ( e.g. , @xmath11 and @xmath13 ) to denote ` random variables ' with corresponding distributions .",
    "subscripts @xmath14 and @xmath15 are used to denote natural parameter pairs , such as @xmath18 and @xmath19 .",
    "similarly , subscripts @xmath16 and @xmath17 are for mean - variance pairs .",
    "note that for clarity , many operations used below are implicitly element - wise , for example , the square @xmath20 , division @xmath21 , partial derivative @xmath22 , the gamma function @xmath23 , logarithm @xmath24 , factorial @xmath25 , @xmath26 , and @xmath27 .",
    "for the data @xmath28 , we set @xmath29 ( input distributions with @xmath30 resemble ae s denoising effect . ) as input of the network and @xmath31 denotes the output targets ( e.g. , labels and word counts ) . in the following text we drop the subscript @xmath32 ( and sometimes the superscript @xmath33 ) for clarity .",
    "the bracket @xmath34 denotes concatenation or pairs of vectors .",
    "here we first introduce the linear form of a general npn . for simplicity , we assume distributions with two natural parameters ( e.g. , gamma distributions , beta distributions , and gaussian distributions ) , @xmath35 , in this section . specifically , we have factorized distributions on the weight matrices , @xmath36 , where the pair @xmath37 is the corresponding natural parameters . for @xmath38 , @xmath11 , and @xmath13 we assume similar factorized distributions .    in a traditional nn , the linear transformation follows @xmath39 where @xmath40 is the output from the previous layer . in nn @xmath40 , @xmath41 , and",
    "@xmath38 are deterministic variables while in npn they are exponential - family distributions , meaning that the result @xmath11 is also a distribution . for convenience of subsequent computation",
    "it is desirable to approximate @xmath11 using another exponential - family distribution .",
    "we can do this by matching the mean and variance . specifically , after computing @xmath42 and @xmath43 , we can get @xmath44 and @xmath45 through the mean @xmath46 and variance @xmath47 of @xmath11 as follows : @xmath48 where @xmath49 denotes the element - wise product and the bijective function @xmath50 maps the natural parameters of a distribution into its mean and variance ( e.g. , @xmath51 in gamma distributions )",
    ". similarly we use @xmath52 to denote the inverse transformation .",
    "@xmath53 , @xmath54 , @xmath55 , and @xmath56 are the mean and variance of @xmath41 and @xmath38 obtained from the natural parameters .",
    "the computed @xmath46 and @xmath47 can then be used to recover @xmath44 and @xmath45 , which will subsequently facilitate the feedforward computation of the nonlinear transformation described in section [ sec : nonlinear ] .",
    "after we obtain the linearly transformed distribution over @xmath11 defined by natural parameters @xmath44 and @xmath45 , an element - wise nonlinear transformation @xmath57 ( with a well defined inverse function @xmath58 ) will be imposed .",
    "the resulting activation distribution is @xmath59 , where @xmath60 is the factorized distribution over @xmath11 defined by @xmath61 . though @xmath62 may not be an exponential - family distribution , we can approximate it with one , @xmath63 , by matching the first two moments .",
    "once the mean @xmath64 and variance @xmath65 of @xmath62 are obtained , we can compute corresponding natural parameters with @xmath52 ( approximation accuracy is sufficient according to preliminary experiments ) . the feedforward computation is : @xmath66 here the key computational challenge is computing the integrals in equation ( [ eq : int ] ) .",
    "closed - form solutions are needed for their efficient computation .",
    "if @xmath67 is a gaussian distribution , closed - form solutions exist for common activation functions like @xmath68 and @xmath69 ( details are in section [ sec : gaussian ] ) .",
    "unfortunately this is not the case for other distributions . leveraging the convenient form of the exponential family",
    ", we find that it is possible to design activation functions so that the integrals for non - gaussian distributions can also be expressed in closed form .    [",
    "th : moment ] assume an exponential - family distribution @xmath70 , where the vector @xmath71 ( m is the number of natural parameters ) .",
    "if activation function @xmath72 is used , the first two moments of @xmath73 , @xmath74 and @xmath75 , can be expressed in closed form . here",
    "@xmath76 ( different @xmath77 corresponds to a different set of activation functions ) and @xmath78 , @xmath79 , and @xmath80 are constants .",
    "we first let @xmath81 , @xmath82 , and @xmath83 .",
    "the first moment of @xmath73 is @xmath84 similarly the second moment can be computed as @xmath85 .",
    "a more detailed proof is provided in the supplementary material . with theorem [ th : moment ] , what remains is to find the constants that make @xmath73 strictly increasing and bounded ( table [ table : activation ] shows some exponential - family distributions and their possible activation functions ) .",
    "for example in equation ( [ eq : int ] ) , if @xmath86 , @xmath87 for the gamma distribution .",
    "-0.1 cm    0.1 cm    .activation functions for exponential - family distributions [ cols=\"<,<,<,<\",options=\"header \" , ]     -0.4 cm",
    "in this section we provide details on the hyperparameters and preprocessing of the experiments as mentioned in the paper .      for the toy 1d regression task",
    ", we use networks with one hidden layer containing @xmath88 neurons and relu activation , as in @xcite . for the gaussian npn",
    ", we use the kl divergence loss and isotropic gaussian priors with precision @xmath89 for the weights ( and biases ) .",
    "the same priors are used in other experiments .      for preprocessing , following @xcite ,",
    "pixel values are normalized to the range @xmath90 $ ] . for the npn variants",
    ", we use these hyperparameters : minibatch size @xmath91 , number of epochs @xmath92 ( the same as bdk ) . for the learning rate",
    ", adadelta is used . note that since npn is dropout - compatible , we can use dropout ( with nearly no additional cost ) for effective regularization .",
    "the training and testing of dropout npn are similar to those of the vanilla dropout nn .      for all models ,",
    "we preprocess the bow vectors by normalizing them into the range @xmath90 $ ] .",
    "although theoretically poisson npn does not need any preprocessing since poisson distributions naturally model word counts , in practice , we find normalizing the bow vectors will increase both stability during training and the predictive performance . for simplicity , in the poisson npn , @xmath78 is set to @xmath93 and @xmath94 ( these two hyperparameters can be tuned to further improve performance ) . for the gaussian npn , sigmoid activation is used .",
    "the other hyperparameters of npn are the same as in the mnist experiments .",
    "* input : * the data @xmath95 , number of iterations @xmath96 , learning rate @xmath97 , number of layers @xmath98 .",
    "compute ( @xmath99 from @xmath100 .",
    "@xmath101 . compute @xmath102 from @xmath103 .",
    "compute the error @xmath104 .",
    "compute @xmath105 , @xmath106 , @xmath107 , and @xmath108 .",
    "compute @xmath109 , @xmath110 , @xmath111 , and @xmath112 .",
    "update @xmath113 , @xmath114 , @xmath115 , and @xmath116 in all layers .",
    "in gamma npn , parameters @xmath113 , @xmath114 , @xmath115 , and @xmath116 can be learned following algorithm [ alg : npn ] .",
    "specifically , during the feedforward phase , we will compute the error @xmath104 given the input @xmath117 ( @xmath118 ) and the parameters ( @xmath113 , @xmath114 , @xmath115 , and @xmath116 ) . with the mean @xmath119 and variance @xmath120 from the previous layer ,",
    "@xmath46 and @xmath47 can be computed according to equations in section 2.2 of the paper , where @xmath121 after that we can get the proxy natural parameters using @xmath122 .    with the proxy natural parameters for the gamma distributions over @xmath11 , the mean",
    "@xmath123 and variance @xmath124 for the nonlinearly transformed distribution over @xmath13 would be obtained . as mentioned before ,",
    "using traditional activation functions like tanh @xmath125 and relu @xmath126 could not give us closed - form solutions for the integrals .",
    "following theorem [ th : moment ] , closed - form solutions are possible with @xmath127 ( @xmath128 and @xmath129 ) where @xmath78 and @xmath80 are constants .",
    "this function has a similar shape with the positive half of @xmath125 with @xmath78 as the saturation point and @xmath80 controlling the slope .    with the computation procedure for the feedforward phase ,",
    "the gradients of @xmath104 with respect to parameters @xmath113 , @xmath114 , @xmath115 , and @xmath116 can be derived and used for backpropagation .",
    "note that to ensure positive entries in the parameters we can use the function @xmath130 or @xmath131 . for example , we can let @xmath132 and treat @xmath133 as parameters to learn instead of @xmath115 .",
    "we can add the kl divergence between the learned distribution and the prior distribution on weights to the objective function to regularize gamma npn .",
    "if we use an isotropic gaussian prior @xmath134 for each entry of the weights , we can compute the kl divergence for each entry ( between @xmath135 and @xmath134 ) as :    @xmath136 where @xmath137 is the digamma function .      for details on the bayesian nonlinear transformation",
    ", please refer to section [ sec : gaussian ] above . for the kl divergence between the learned distribution and the prior distribution on weights , we can compute it as : @xmath138 as we can see , the term @xmath139 will help to prevent the learned variance @xmath15 from collapsing to @xmath140 ( in practice we can use @xmath141 , where @xmath142 and @xmath143 are hyperparameters , to approximate this term for better numerical stability ) and the term @xmath144 is equivalent to l2 regularization .",
    "similar to bdk , we can use a mixture of gaussians as the prior distribution .",
    "the poisson distribution , as another member of the exponential family , is often used to model counts ( e.g. , number of events happened or number of words in a document ) .",
    "different from the previous distributions , it has support over nonnegative integers .",
    "the poisson distribution takes the form @xmath145 with one single natural parameter @xmath146 ( we use @xmath14 as the proxy natural parameter ) .",
    "it is this single natural parameter that makes the learning of a poisson npn trickier . for text modeling , assuming poisson distributions for neurons is natural because they can model the counts of words and topics ( even super topics ) in documents . here",
    "we assume a factorized poisson distribution @xmath147 and do the same for @xmath13 . to ensure having positive natural parameters we use gamma distributions for the weights .",
    "interestingly , this design of poisson npn can be seen as a neural analogue of some poisson factor analysis models @xcite .",
    "following algorithm [ alg : npn ] , we need to compute @xmath104 during the feedforward phase given the input @xmath117 ( @xmath118 ) and the parameters ( @xmath113 , @xmath114 , @xmath115 , and @xmath116 ) , the first step being to compute the mean @xmath46 and variance @xmath47 . since",
    "gamma distributions are assumed for the weights , we can compute the mean and variance of the weights as follows : @xmath148 having computed the mean @xmath46 and variance @xmath47 of the linearly transformed distribution over @xmath11 , we map them back to the proxy natural parameters @xmath44",
    ". unfortunately the mean and variance of a poisson are the same , which is obviously not the case for @xmath46 and @xmath47 . hence we propose to find @xmath44 by minimizing the kl divergence of the factorized poisson distribution @xmath149 and the gaussian distribution @xmath150 , resulting in the mapping ( see section [ sec : poisson_mapping ] for proofs and justifications ) : @xmath151 after finding @xmath44 , the next step in algorithm [ alg : npn ] is to get the mean @xmath123 and variance @xmath124 of the nonlinearly transformed distribution .",
    "as is the case for gamma npn , traditional activation functions will not give us closed - form solutions .",
    "fortunately , the activation @xmath127 also works for poisson npn .",
    "specifically , @xmath152 where the superscript @xmath33 is dropped .",
    "similarly , we have @xmath153 full derivation is provided in section [ sec : poisson_activation ] .",
    "once we go through @xmath98 layers to get the proxy natural parameters @xmath154 for the distribution over @xmath155 , the error @xmath104 can be computed as the negative log - likelihood . assuming that the target output @xmath156 has nonnegative integers as entries , @xmath157 for @xmath156 with real - valued entries , the l2 loss could be used as the error @xmath104 .",
    "note that if we use the normalized bow as the target output , the same error @xmath104 can be used as the gaussian npn .",
    "besides this loss term , we can add the kl divergence term in equation ( [ eq : kl_gamma ] ) to regularize poisson npn .    during backpropagation ,",
    "the gradients are computed to update the parameters @xmath113 , @xmath114 , @xmath115 , and @xmath116 .",
    "interestingly , since @xmath44 is guaranteed to be nonnegative , the model still works even if we directly use @xmath53 and @xmath54 as parameters , though the resulting models are not exactly the same . in the experiments , we use this poisson npn for a bayesian autoencoder and feed the extracted second - order representations into a bayesian lr algorithm for link prediction .",
    "in this section we list the gradients used in backpropagation to update the npn parameters .                in the following",
    "we assume the sigmoid activation function and use cross - entropy loss .",
    "other activation functions and loss could be derived similarly . for the equations below , @xmath167 , @xmath168 , @xmath169 , and @xmath170 .          in the following",
    "we assume the activation function @xmath179 and use poisson regression loss @xmath180 ( the target output @xmath156 is a vector with nonnegative integer entries ) .",
    "gamma distributions are used on weights ."
  ],
  "abstract_text": [
    "<S> neural networks ( nn ) have achieved state - of - the - art performance in various applications . unfortunately in applications where training data is insufficient , they are often prone to overfitting . </S>",
    "<S> one effective way to alleviate this problem is to exploit the bayesian approach by using bayesian neural networks ( bnn ) . </S>",
    "<S> another shortcoming of nn is the lack of flexibility to customize different distributions for the weights and neurons according to the data , as is often done in probabilistic graphical models . to address these problems , </S>",
    "<S> we propose a class of probabilistic neural networks , dubbed natural - parameter networks ( npn ) , as a novel and lightweight bayesian treatment of nn . </S>",
    "<S> npn allows the usage of arbitrary exponential - family distributions to model the weights and neurons . </S>",
    "<S> different from traditional nn and bnn , npn takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions . as a bayesian treatment , efficient backpropagation ( bp ) </S>",
    "<S> is performed to learn the natural parameters for the distributions over both the weights and neurons . </S>",
    "<S> the output distributions of each layer , as byproducts , may be used as second - order representations for the associated tasks such as link prediction . </S>",
    "<S> experiments on real - world datasets show that npn can achieve state - of - the - art performance .    </S>",
    "<S> 0 = 2 =3 </S>"
  ]
}