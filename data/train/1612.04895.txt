{
  "article_text": [
    "to apply our machine learning approach , we represent the input @xmath11 and output @xmath12 as vectors .",
    "( henceforth vectors are denoted in boldface and matrices with a tilde overbar .",
    "energy , time and temperature units are chosen such that @xmath25 . )",
    "we represent @xmath11 as @xmath26 , a vector whose components are the values of @xmath7 at the points @xmath27 with @xmath28 ( many - body theory shows that @xmath29 . in typical many - body calculations a large number of @xmath9 points can be computed ; here we use @xmath30 .    following [ ] we approximate @xmath22 as a finite sum of conformal coefficients @xmath31 where @xmath32 with @xmath33 , @xmath34 and @xmath35 a natural energy unit .",
    "this representation is much more compact than a discretization across the region of support of @xmath22 , and it is less susceptible to violations of the non - negativity constraint than the obvious alternative representation in terms of a finite number of orthogonal functions .",
    "the output is a vector @xmath36 of length @xmath37 which we choose by increasing @xmath37 until the results cease to change ; we find @xmath38 to be sufficient in practice .",
    "machine learning approaches require databases of spectral function  green function pairs .",
    "we construct a learning database of @xmath39 examples for training the basic regression , a tuning set of @xmath40 examples for determining hyperparameters , and a test set of @xmath40 examples for assessing the predictive accuracy . to generate the databases we create spectral functions using simulated data ( sums of gaussians with",
    "randomly chosen centers and widths , see supporting information for details ) and compute the corresponding green s functions and conformal coefficients @xmath41 as well as other information including moments and electron densities .",
    "we restrict the @xmath12 that are included to be physically meaningful .",
    "we require that @xmath42 and further constrain the centers and widths such that all @xmath12 have a very small value for @xmath43 larger than a cutoff frequency @xmath44 .",
    "we allow high peaks only at small frequency ( the physical expectation is that at higher frequencies many decay channels are possible , implying only relatively broad structures in @xmath12 at higher frequencies ) , and we also impose a smoothness criterion at low frequency that prevents too large a spike in @xmath22 at any one point .",
    "these constraints on @xmath22 are an important regularization .",
    "we use a machine learning approach to perform an unconstrained regression to construct from the learning database a function which provides a predicted output @xmath45 corresponding to an input @xmath46 .",
    "many formulations of regression have been developed and the particular choice of method is not essential .",
    "we use kernel ridge regression ( krr ) @xcite ; this is a flexible nonparametric regression method which combines the parameter shrinkage properties of ridge regression with kernels to allow for nonlinear relationships . by solving a large number of inverse problems at once",
    ", it effectively shares data across problems to reduce relationship uncertainty .",
    "the key ingredients in our krr are a metric that specifies the distance between two input functions and a kernel that depends on this distance and controls the size of the region of influence of each example in the database .",
    "we use a simple gaussian kernel specified by a single hyperparameter @xmath47 , @xmath48 where @xmath49 is the squared euclidean distance for continuous functions , @xmath50 the @xmath51 conformal coefficient ( @xmath52 ) of the predicted output @xmath53 for a new input example @xmath26 is then determined by using the input @xmath54 to compute @xmath55 .",
    "@xmath56 is the kernel distance between the input @xmath46 and the @xmath57 entry in the learning set .",
    "the @xmath56 are assembled into a vector @xmath58 of size @xmath59 .",
    "the transpose of this vector is then right - multiplied by an @xmath60 vector @xmath61 to obtain the predicted component @xmath62 : @xmath63 the unconstrained prediction for the spectral function @xmath64 is obtained by inserting the @xmath65 into eq .  .",
    "the @xmath61 is found by minimizing the regularized squared error loss @xmath66 here @xmath67 is obtained by using @xmath68 and @xmath69 of the learning set in eq .  .",
    "the minimization can be carried out analytically with the result @xmath70 where @xmath71 is a @xmath72 matrix with entries @xmath73 and @xmath74 is the @xmath75 identity matrix . because we choose the kernel @xmath76 and hyperparameters @xmath47 and @xmath77 to be independent of @xmath78 ,",
    "all of the minimizations may be done in parallel .    in many physical cases ,",
    "the input data are noisy , for example when @xmath11 are obtained from quantum monte carlo calculations .",
    "it is thus important to understand how the predictive accuracy of the model deteriorates as noise is added to @xmath11 . while approaches have been proposed for treating inputs with noise ( see for example [ ] ) ,",
    "the numerical burden added is substantial .",
    "we instead choose the following approach .",
    "we train the krr model parameters using the learning data set without any noise added to @xmath26 , but we tune the krr hyperparameters @xmath79 using the tuning set with a noisy version of @xmath26 as the input for the krr model .",
    "we create the noisy input by adding to each @xmath26 in the tuning set a vector of independent mean - zero gaussian noise with standard deviation @xmath80 .",
    "we consider three levels for @xmath80 , from a fairly large @xmath81 to a fairly small @xmath82 .",
    "for each of these noise levels , we select those values for @xmath79 that minimize the maximum absolute error ( mae , see the supporting information for details ) in the first 10 conformal coefficents @xmath41 when predicting for the tuning set . fig .",
    "[ fig : cv ] shows the results .",
    "the optimal hyperparameters @xmath83 are @xmath84 for @xmath85 , @xmath86 for @xmath87 and @xmath88 for @xmath89 .",
    "while the optimal values of @xmath47 do not vary much with input noise , @xmath77 changes by orders of magnitude as the noise is varied .",
    "we also note that the best hyperparameter values will change with database size , composition etc . however , with everything else kept unchanged , the best hyperparameter values are stable with respect to changes in the temperature @xmath8 at which the @xmath11 were constructed .     and",
    "@xmath90 , as a function of the kernel width @xmath47 and regularization parameter @xmath77 . ]      a spectral function obtained from the unconstrained regression eq .   may violate the constraints of the problem .",
    "we therefore correct the unconstrained prediction using a second projection stage that finds the final estimate @xmath91 that fulfills the constraints while deviating least from the initial , unconstrained estimate @xmath92 : @xmath93 eq .",
    "( [ normmin ] ) is a well - posed convex optimization problem under linear equality and inequality constraints and is easily and efficiently solved ( we use a general purpose interior - point quadratic solver ) .",
    "the optimization is formulated directly in terms of the physical spectral function @xmath12 because some constraints ( for example , non - negativity ) are not easy to express in terms of expansion coefficients such as the conformal amplitudes @xmath41 .    in our application",
    ", we will include the following constraints :    * @xmath22 integrates to one : @xmath94 * @xmath22 is positive everywhere : @xmath95 * the particle density @xmath96 for @xmath22 is known with high precision from @xmath26 . *",
    "the first and the second non - central moment of @xmath22 , @xmath97 and @xmath98 , are also known with high precision from @xmath26 .",
    "the norm @xmath99 defines a distance between two functions .",
    "it is introduced because the uncertainty in the prediction @xmath92 is likely not constant across all @xmath15 .",
    "furthermore , prediction errors at one location @xmath100 may tend to be correlated with prediction errors at a different location @xmath101 .",
    "this suggests that we should require the norm @xmath102 to penalize deviations from @xmath92 more at locations @xmath100 where the predictions are more certain , and that we should take into account the correlation between prediction errors .",
    "the squared mahalanobis distance , @xmath103 which weights differences by an appropriate covariance matrix @xmath104 allows us to encode this behavior .",
    "we choose the covariance matrix @xmath105 in a data - driven way by defining the residual at frequency @xmath15 of a member @xmath106 of the learning set as @xmath107 where @xmath108 is the true @xmath22 and @xmath109 is the unconstrained prediction . the average residual at this frequency is @xmath110 .",
    "now , for each pair of frequencies ( @xmath111 , we define the respective element in the empirical residual covariance matrix as @xmath112 the inverse of the empirical covariance matrix is typically poorly conditioned because the covariance matrix is constructed from a number of pieces of information of the order of the square of the number of frequency points @xmath113 , which will typically be greater than the total number of members of the training set . in these circumstances a principal components estimator based on the eigenvalue decomposition @xmath114 is appropriate ( see , for example , [ ] ) .",
    "we then retain only the @xmath115 principal components with the highest eigenvalues , approximating  @xmath105 as @xmath116 the tuning parameter @xmath115 is chosen to minimize the error between predictions @xmath91 and the true densities of states @xmath117 on the tuning set and @xmath118 typically suffices in practice .",
    "the estimate @xmath119 is well - conditioned and the inverse , whether taken directly or through an equivalent cholesky decomposition without direct inversion , has to be performed only once .",
    "prediction errors occur because finite database size causes imperfect interpolation , and because noise in an input @xmath26 will propagate even with perfect prediction .",
    "quantifying prediction uncertainty is an important part of any solution .",
    "we present a two step approach .",
    "first we define an interpolation measure that is used to determine if an input @xmath26 is similar enough to examples in the database so that prediction is appropriate . for inputs @xmath26 that pass the interpolation test , we then formulate a quantile regression approach that provides pointwise uncertainty bounds around the final predicted @xmath91 .",
    "[ [ an - interpolation - measure ] ] an interpolation measure : + + + + + + + + + + + + + + + + + + + + + + + + +    to determine whether the input @xmath26 is sufficiently similar to examples in the database we analyze the initial unconstrained krr prediction .",
    "an input which is too far away from database samples for the regression to be useful is certainly not appropriate for further analysis . for this purpose ,",
    "we consider the uncertainty in the predicted conformal coefficients @xmath120 , which is straightforward because krr is a linear model .    mathematically speaking , we condition the output ( the unconstrained vector of conformal coefficients @xmath120 ) on the examples in the learning set and the input vector . for a given input @xmath54",
    "we obtain the vector @xmath121 as defined in eq .  . a standard analysis ( see , for example , chap",
    ". 2 of [ ] ) then says that a measure of confidence in the prediction for our unconstrained vector of conformal coefficients @xmath120 is @xmath122 this interpolation measure is independent of @xmath78 because we use the same krr hyperparameters across all conformal coefficients .",
    "a value close to one reflects that the prediction is for an input that is close to many examples in the database , which suggests high prediction accuracy .",
    "decreases from one mean increased prediction error .",
    "we empirically determine a cutoff for deviations from unity below which the prediction quality is typically too low , so that the prediction can not be trusted .",
    "the interpolation metric evaluated for the members of the test set , as well as a measure of the total prediction error on @xmath123 , are shown in fig .",
    "[ fig : mrevspostvar ] .",
    "we find that in this application the interpolation measure must be within 0.1% of  1 .",
    "( red diamond ) and [ fig : a_indmaxent500_490ml_std1en03_predint ] ( red square ) .",
    "we see that a good prediction can only be expected if the interpolation measure is very close to one . ]",
    "[ [ obtaining - pointwise - prediction - bounds ] ] obtaining pointwise prediction bounds : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    if the interpolation metric defined in eq .",
    "is sufficiently close to one , we proceed to obtain bounds on the prediction error at a given frequency  @xmath15 .",
    "we treat the residual @xmath124 as a random variable with distribution @xmath125 and study the quantiles of @xmath126 .",
    "the quantile @xmath127 of @xmath128 is the value for which the probability that @xmath129 is @xmath130 .",
    "we can therefore write @xmath131 we see that , for @xmath132 , @xmath133\\ ] ] is a symmetric , pointwise @xmath134 prediction interval for the true , unknown @xmath135 . we will focus on pointwise , symmetric 90% prediction intervals , which means that @xmath136 in eq .  .",
    "the quantile functions @xmath127 and @xmath137 are obtained by applying a quantile regression method  @xcite to a database of @xmath138 pairs .",
    "various quantile regression methods have been discussed ; here we adopt the kernel quantile regression method  @xcite ( kqr ) , a highly flexible quantile regression model that is very similar to krr .",
    "( note that we use kqr to bound @xmath139 , whereas we use krr to predict @xmath22 . ) separately for each frequency @xmath15 , we model the quantile using kqr as @xmath140 and estimate @xmath141 , separately for each @xmath15 , by minimizing the regularized quantile loss @xmath142 where @xmath143 this minimization needs to be carried out numerically .",
    "we use the same distance function , kernel and hyperparameters for kqr as we did for krr and we reserve more specific training for future work .    because our approach is based on residuals only",
    ", it is agnostic to the underlying model that specifies the relationship between @xmath11 and @xmath12 and can therefore be used with any model that is capable of predicting @xmath12 , including maxent .",
    "one standard error bounds .",
    "the error level obtained using acwpr for zero noise is shown as a solid black line.,title=\"fig : \" ]   one standard error bounds .",
    "the error level obtained using acwpr for zero noise is shown as a solid black line.,title=\"fig : \" ]",
    "we now assess the performance of acwpr and compare it to a highly optimized publicly available implementation of maxent , omegamaxent @xcite , using a test data set of 500 observations randomly chosen from the full test set . to simulate the effect of noisy inputs",
    ", we perform this comparison at three different levels of delta - correlated noise added to each @xmath7 .",
    "we employ two error metrics : the kullback ",
    "leibler  ( kl ) loss @xmath144 which penalizes relative deviations , and the mean absolute error , @xmath145 which assesses average deviations between the true function .",
    "[ fig : avgkl ] shows that according to the kl metric ( which maxent is designed to optimize ) , acwpr performs minimally worse than maxent at the smallest noise levels on average , but its performance is much better for large noise levels . according to the mae metric , acwpr is clearly superior for all input noise levels . also shown on these figures",
    "is the average prediction error computed for noiseless data using acwpr .",
    "we see that the error at the smallest noise level is comparable to the intrinsic prediction error of noiseless data . comparing acwpr error and maxent error case by case ( see supplemental information ) acwpr performs better in @xmath146 of the cases using the kl error metric and @xmath147/@xmath148/@xmath149 of the cases using the mae error metric at noise levels @xmath150 .",
    "we now have a closer look at two examples at large input noise .",
    "we begin with a spectral function characterized by a sharp peak at zero energy and broader structures at high frequency that happens to be the one example with the smallest mae for maxent . fig .",
    "[ fig : mrevspostvar ] shows that the interpolation metric is very close to one so the prediction can be trusted .",
    "we show the true spectral function , the acwpr prediction with uncertainty bounds and the maxent prediction in fig .",
    "[ fig : a ] . for this case , both acwpr and maxent give good predictions .",
    "line ) and maxent prediction ( blue @xmath151 line ) spectral function .",
    "pointwise 5%95% prediction interval for acwpr in gray . ]    fig .",
    "[ fig : a_indmaxent500_490ml_std1en03_predint ] shows a more challenging case : a spectral function with a gap ( region where @xmath152 ) . once again , fig .",
    "[ fig : mrevspostvar ] shows that the prediction can be trusted .",
    "acwpr performs well , unlike maxent , whose prediction is significantly broadened into the gap .",
    "line ) and maxent prediction ( blue @xmath151 line ) .",
    "pointwise 5%95% prediction interval for acwpr in gray . ]",
    "the inversion of fredholm integrals of the first kind is an example of an ill - conditioned problem .",
    "any useful method of solution must provide a regularization@xcite .",
    "direct construction of the inverse operator is challenging and typically not robust to noise in the input data @xcite . in this paper",
    "we propose to replace inversion of an operator by regression from a library of known solutions .",
    "constraints are straightforwardly included by projecting the results of an unconstrained regression onto the subspace of functions satisfying the constraints .",
    "estimates of prediction uncertainty are readily computed from the distribution of the residuals in out - of - sample prediction .",
    "the approach is advantageous for ill - conditioned problems because it provides the needed regularization in three ways : via choice of entries in the database of solved problems , via representation of the kernel regression in terms of a restricted set of basis functions , and via the tuning parameters used in the kernel .",
    "because the entries in the database of solved problems can be freely chosen , a physical motivated regularization may be employed .",
    "we have applied the approach to the problem of determining a spectral function ( density of states of allowed excitations ; mathematically , the value of a branch - cut discontinuity of an otherwise analytic function ) from measurements of a correlation function .",
    "this problem is of considerable importance in the context of condensed matter physics and materials science , and is also representative of a broad class of important problems .",
    "although no particular effort has been invested in optimization , the method was found to compare very favorably to a state of the art and highly optimized maximum entropy implementation .",
    "we showed that our approach provides robust reconstruction of spectral functions even in the presence of noisy data and that it captures gap edge behavior well .",
    "the approach can be applied in many other similar contexts ; the main requirement is that the forward computation is cheap and stable .",
    "l .- f.a . and a.j.m .",
    "were supported by the office of science of the u.s .",
    "department of energy under subcontract no .",
    "r.n . and l.a.h .",
    "also received support from columbia university ids - roads project , ur009033 - 05 .",
    "l .- f.a . thanks dominic bergeron for support with his software omegamaxent and z.  he for insightful discussions .    c. l. epstein and j. schottland , siam review * 50 * , 504 ( 2008 ) .",
    "a. georges , g. kotliar , w. krauth , and m.j .",
    "rozenberg , rev .",
    "phys . * 68 * , 13 ( 1996 ) .",
    "t. maier , m. jarrell , t. pruschke , and m.h .",
    "hettler , rev .",
    "phys . * 77 * , 1027 ( 2005 ) .",
    "g. kotliar , s.y .",
    "savrasov , k. haule , v.s .",
    "oudovenko , o. parcollet , and c.a .",
    "marianetti , rev .",
    "phys . * 78 * , 865 ( 2006 ) .",
    "e. gull , a.j .",
    "millis , a.i .",
    "lichtenstein , a.n .",
    "rubtsov , m. troyer , and p. werner , rev .",
    "mod . phys . * 83 * , 349 ( 2011 ) .",
    "silver , d.s .",
    "sivia , and j.e .",
    "gubernatis , phys .",
    "b * 41 * , 2380 ( 1990 ) .",
    "w. von der linden , appl .",
    "phys . a : mater .",
    "sci . process .",
    "* 60 * , 155 ( 1995 ) .",
    "a.w sandwick , phys . rev .",
    "b * 57 * , 10287 ( 1998 ) .",
    "beach , arxiv : cond - mat/0403055 , unpublished ( 2004 ) .    m. jarrell and j.e .",
    "gubernatis , phys . rep . * 269 * , 133 ( 1996 ) .",
    "a. yevick , m. hannel , and d. grier , opt .",
    "express * 22 * , 26884 ( 2014 ) .",
    "l. waller and l. tian , nature * 523 * , 416 ( 2015 ) .",
    "a.a abrikosov , i.e. dzyaloshinski and l.p .",
    "gorkov lp , methods of quantum field theory in statistical physics , ( dover , 1975 ) .",
    "g. baym and d.n .",
    "mermin , j. math .",
    "phys . * 2 * , 232 ( 1961 ) .",
    "krivenko , a.n .",
    "rubtsov , arxiv : cond - mat/0612233 , unpublished ( 2006 ) .",
    "b. schlkopf b and a.j .",
    "smola , _ learning with kernels _ ( mit press , 2002 ) .",
    "j. quionero - candela and s.t .",
    "roweis , semi - described and semi - supervised learning with gaussian processes . nips ( 2003 ) .",
    "a. girard a and r. murray - smith , _ switching and learning in feedback systems _ * 3355 * lncs , 158 ( springer , 2005 ) .",
    "a. mchutchon and c.e .",
    "rasmussen , gaussian process training with input noise .",
    "nips * 24 * , 1341 ( 2011 ) .",
    "m. pourahmadi , _ high - dimensional covariance estimation _",
    "( wiley series in probability and statistics , 2013 ) .",
    "j. bai and s. shi , ann .",
    "finan . , 199 ( 2011 ) .",
    "c.k.i williams and c.e .",
    "rasmussen , _ gaussian processes for machine learning _ , ( mit press , 2006 ) .",
    "r. koenker , _ quantile regression _",
    "( cambridge university press , 2005 ) .",
    "i. takeuchi , q.v .",
    "sears and a.j .",
    "smola , j. mach .",
    "* 7 * ( 2006 ) .",
    "d. bergeron and a .-",
    "s. tremblay , phys .",
    "e * 94 * , 023303 ( 2016 ) .",
    "a. dienstfrey and l. greengard , inverse problems * 17 * , 1307 ( 2001 ) .",
    "* supporting information *    * arsenault et al . *",
    "we generate a database of 25,000 @xmath153 pairs by first generating @xmath22 and then computing from it @xmath11 .",
    "each @xmath12 in the database is represented as a sum of @xmath139 gaussians with center @xmath154 , width @xmath155 and weight @xmath156 chosen randomly subject to constraints described below : @xmath157 we use physical insight to restrict the @xmath12 in the database .",
    "we imposed that @xmath22 should have narrow peaks only in a region centered around @xmath158 .",
    "for this purpose , we split the energy window @xmath159 into three parts using splits at @xmath160 and @xmath161 .",
    "we chose different values of @xmath139 in eq .   in the range @xmath162 and generated centers @xmath154 for gaussians where only up to three are allowed to be in the center region .",
    "the widths were chosen such that narrow peaks are only allowed in or close to the center region .",
    "we then verify that , with the generated @xmath154 , @xmath155 and @xmath156 , the resulting @xmath12 has small enough a value at @xmath163 to be essentially zero chosen here to be @xmath164 .",
    "we also imposed a constraint that a peak in the center region can not be extremely high relative to the rest of the density of states .",
    "we impose it by considering the smoothness criterion @xmath165 as well as a not too large a ratio between the maximum value of @xmath12 in the center region and the maximum value of @xmath12 in the other two regions chosen here to be respectively @xmath166 and 10.9 .",
    "examples of @xmath12 randomly chosen from the database are shown in fig .",
    "[ fig : aexamples ] .",
    "we then compute @xmath11 from @xmath12 at temperature @xmath167 .",
    "the very weak temperature dependence of the hyperparameters means that other temperatures can be considered without difficulty .     in the database , title=\"fig : \" ]   in the database , title=\"fig : \" ]",
    "we determine the optimal values for @xmath79 for krr as those that minimize mae between the true and predicted first ten @xmath41 ( both real and imaginary parts ) out of sample . for this purpose",
    ", we train krr on the ( noiseless ) learning set and predict on the ( noisy ) tuning set .",
    "the mae then is @xmath168\\big ) .",
    "\\notag\\end{aligned}\\ ] ] we separately tune krr for three levels of noise , from fairly large @xmath81 to fairly small @xmath82 .",
    "to use the projection method , the covariance matrix estimator must first be trained , which means choosing the value of @xmath115 in eq .  ( 14 ) .",
    "to do so , we use the learning set and the tuning set .",
    "we only use a noise level of @xmath81 for validation . using the krr model",
    ", we make a krr prediction for the ten thousand @xmath12 of the learning set with noise added to the inputs .",
    "we then build the empirical covariance matrix using eq .",
    "( 13 ) . for different values of @xmath115",
    ", we first construct a covariance matrix from eq .",
    "( 14 ) and then use the tuning set , again with noise added to the inputs @xmath11 , to predict the projected @xmath169 of eq .  ( 10 ) for the five thousand members of tuning set .",
    "we then compute a mean absolute error for these five thousand predictions to obtain the error as a function of @xmath115 as @xmath170.\\ ] ] we find that values of @xmath115 in the range of 16 to 30 offer similar error values and we chose to use @xmath171 , no matter the noise level .",
    "[ fig : n03 ] compares , at three different noise levels , the error of acwpr to the error of the maxent approach according to both the kullback  leibler distance ( kl ) and the mean absolute error ( mae ) .",
    "we see that acwpr performs as well , or better , than maxent at all noise levels ."
  ],
  "abstract_text": [
    "<S> we present a machine learning approach to the inversion of fredholm integrals of the first kind . </S>",
    "<S> the approach provides a natural regularization in cases where the inverse of the fredholm kernel is ill - conditioned . </S>",
    "<S> it also provides an efficient and stable treatment of constraints . </S>",
    "<S> the key observation is that the stability of the forward problem permits the construction of a large database of outputs for physically meaningful inputs . </S>",
    "<S> we apply machine learning to this database to generate a regression function of controlled complexity , which returns approximate solutions for previously unseen inputs ; the approximate solutions are then projected onto the subspace of functions satisfying relevant constraints . </S>",
    "<S> we also derive and present uncertainty estimates . </S>",
    "<S> we illustrate the approach by applying it to the analytical continuation problem of quantum many - body physics , which involves reconstructing the frequency dependence of physical excitation spectra from data obtained at specific points in the complex frequency plane . under standard </S>",
    "<S> error metrics the method performs as well or better than the maximum entropy method for low input noise and is substantially more robust to increased input noise . </S>",
    "<S> we expect the methodology to be similarly effective for any problem involving a formally ill - conditioned inversion , provided that the forward problem can be efficiently solved .    </S>",
    "<S> this article is addressed to two different readerships . for physical scientists </S>",
    "<S> , we aim to demonstrate , via the solution of a specific and important example , the power of machine learning and statistical regression methods to provide effective solutions to an important class of inverse problems . for statisticians and data scientists we hope to provide an introduction to a class of problems where regression approaches may be applicable .    </S>",
    "<S> the class of problems of interest is described by an equation of the form @xmath0 where the kernel @xmath1 is known , @xmath2 typically denotes integration , and we are interested in determining @xmath3 from measurements of @xmath4 . </S>",
    "<S> this type of problem occurs frequently in the physical sciences . </S>",
    "<S> one example is the reconstruction of a potential from measurements of its effects on incident waves or more generally inversion of a laplace transform @xcite . </S>",
    "<S> another example , considered in detail in this paper , is the reconstruction of the values of branch cut discontinuities of a function of a complex variable from data obtained at a sequence of points in the complex plane . </S>",
    "<S> this case is important in quantum many - body physics , because powerful methods exist @xcite for computing functions along imaginary times or frequencies , while the quantity of direct physical relevance is the spectral function , which describes physical transitions between states and which is related to the magnitude of a branch cut discontinuity across the real frequency axis . </S>",
    "<S> inverting the relation to obtain @xmath3 given @xmath4 is thus important in theory  experiment comparison .    </S>",
    "<S> the forward computation of @xmath4 given @xmath3 typically can be efficiently and stably performed , but reconstructing @xmath3 may be difficult because the operators @xmath1 typically encountered have many very small eigenvalues , so that the formal inversion @xmath5 is ill - conditioned : small errors in @xmath4 or in carrying out the @xmath2 operation can lead to very large errors in  @xmath3 . </S>",
    "<S> an approximate solution of eq .   must _ regularize _ the problem , which means in some way excluding the eigenfunctions corresponding to the very small eigenvalues of @xmath1 , so that the solution is robust against small errors in  @xmath4 . additionally , in many physically relevant cases @xmath3 is constrained , for example to be non - negative and to have specified low order moments ; it is desirable that approximate solutions of eq .  </S>",
    "<S> ( [ inverse ] ) respect such constraints . </S>",
    "<S> finally , an approximate solution for  @xmath3 should be accompanied by uncertainty estimates .    in the quantum many - body and materials science communities the standard approach to eq .   </S>",
    "<S> is the maximum entropy ( maxent ) method  @xcite , which approximates @xmath3 as the minimum of an objective function consisting of the sum of the @xmath6 difference between the predicted and actual data for @xmath4 and an entropy contribution involving a default model ( which incorporates constraints ) and a temperature - like parameter that provides a relative weighting of the @xmath6 and entropy contributions ( a variant , not widely employed due to its computational expense , uses the maxent objective function as the energy in a monte carlo process @xcite ) . </S>",
    "<S> experience indicates that certain features can be reasonably well predicted by maxent , but other aspects are the subject of considerable and not easily quantifiable uncertainty @xcite . the temperature - like parameter </S>",
    "<S> is typically chosen phenomenologically . </S>",
    "<S> as normally implemented the method does not provide uncertainty estimates ( but see [ ] ) .    </S>",
    "<S> here , we propose a fundamentally different approach based on modern statistical tools ( such methods have recently been applied successfully to solving inverse problems in other areas of the physical sciences , see [ ] ) . </S>",
    "<S> our key idea is to treat the solution of eq .   as a regression problem which is solved by applying statistical machine learning methods to a large database of input  output pairs . </S>",
    "<S> the stability of the forward problem means that the needed database of input  output pairs is easily generated , and the formulation as a complexity - constrained regression problem provides a regularization that conditions the formally ill - conditioned inversion . </S>",
    "<S> solutions that respect needed constraints are found by projecting approximated solutions from the unconstrained regression onto the space of functions with the desired properties . </S>",
    "<S> an interpolation test is formulated that determines whether a given input @xmath4 is sufficiently close to elements of the database that a solution can be trusted . for inputs that pass the interpolation test </S>",
    "<S> , we use a quantile regression method to provide pointwise uncertainty estimates .    we demonstrate the power of the approach by applying it to the quantum many - body analytical continuation problem ; we call this analytical continuation with projected regression ( acwpr ) </S>",
    "<S> . we focus on the case where @xmath4 is the electron green s function , @xmath7 , a correlation function related to the propagation of an electron in a material at thermal equilibrium at a temperature @xmath8 . </S>",
    "<S> the @xmath7 may be expressed as a function of an imaginary time variable @xmath9 defined on the interval @xmath10 . </S>",
    "<S> powerful numerical methods exist for calculating @xmath11 .    in this context , @xmath3 is the spectral function @xmath12 , a function of a real frequency which has the physical meaning of the density of states for adding or removing an electron at energy @xmath13 and can be measured in photoemission experiments . </S>",
    "<S> fundamental results of quantum field theory @xcite imply that @xmath14 for all @xmath15 , that @xmath16 , and that all higher moments @xmath17 ( @xmath18 ) are finite . </S>",
    "<S> the integral @xmath19 gives the particle density @xmath20 , where @xmath21 is the fermi  </S>",
    "<S> dirac distribution .    </S>",
    "<S> the relationship between @xmath7 and @xmath22 is @xcite @xmath23 and the problem of inverting eq .   to infer the quantity of direct physical relevance , @xmath12 , is referred to in the physics literature as the analytical continuation problem . </S>",
    "<S> the inversion is of the form of eq .   with @xmath24 . </S>",
    "<S> the inversion is ill - conditioned , and the complications deriving from the many small eigenvalues of @xmath1 are exacerbated by the fact that the input data typically come from quantum monte carlo calculations , which are subject to statistical errors and provide values only at a finite number @xmath9 values . for these reasons , </S>",
    "<S> the analytical continuation problem has been a continuing challenge in quantum many - body physics . </S>"
  ]
}