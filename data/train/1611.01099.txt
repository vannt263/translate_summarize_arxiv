{
  "article_text": [
    "we are interested in answering two very basic questions about continuous - time , discrete - symbol stochastic processes :    * what are their minimal maximally predictive models  their ? * what are information - theoretic characterizations of their randomness , predictability , and complexity ?    for shorthand , we refer to the former as _ causal architecture _ and the latter as _ informational architecture_. minimal maximally predictive models of discrete - time , discrete - state , discrete - output processes are relatively well understood ; e.g. , see refs .",
    "some progress has been made on understanding minimal maximally predictive models of discrete - time , continuous - output processes ; e.g. , see refs . @xcite .",
    "relatively less is understood about minimal maximally predictive models of continuous - time , discrete - output processes , beyond those with exponentially decaying state - dwell times @xcite .",
    "the following is a first attempt at a remedy that complements the spectral methods developed in ref .",
    "@xcite , as we address the less tractable case of uncountably infinite causal states .",
    "we start by analyzing continuous - time renewal processes , as addressing the challenges there carries over to other continuous - time processes .",
    "( elsewhere , we outline the wide interest and applicability of renewal processes in physics and the quantitative sciences generally @xcite . )",
    "the difficulties are both technical and conceptual .",
    "first , the causal states are now continuous or hybrid discrete - continuous random variables , unless the renewal process is poisson .",
    "second , transitions between causal states are now described by partial differential equations . finally , and perhaps most challenging , most informational architecture quantities must be redefined . with these challenges",
    "addressed , we turn our attention to a very general class of continuous - time , discrete - alphabet processes  stateful renewal processes generated by unifilar hidden semi - markov models . we identify their  and find new expressions for entropy rate and other informational architecture quantities , extending results in ref .",
    "@xcite .",
    "our main thesis is rather simple : minimal maximally predictive models of continuous - time , discrete - symbol processes require a wholly new  calculus . to develop it , sec .",
    "[ sec : background ] describes the required new notation and definitions that enable extending the  framework which is otherwise well understood for discrete - time processes @xcite .",
    "sections  [ sec : causalstates]-[sec : infoarch ] determine the causal and informational architecture of continuous - time renewal processes .",
    "section  [ sec : uhsmms ] characterizes the  and calculates the entropy rate and excess entropy of unifilar hidden semi - markov models .",
    "we conclude by describing potential applications to bayesian  inference algorithms using new enumerations of  topologies and to information measure estimation using the formulae of sec .",
    "[ sec : uhsmms ] .",
    "a continuous - time , discrete - symbol time series @xmath0 is described by a list of symbols @xmath1 in a finite alphabet @xmath2 and dwell times @xmath3 for those symbols . in this representation",
    ", we demand that @xmath4 to enforce a unique presentation of the time series .",
    "sections  [ sec : causalstates]-[sec : infoarch ] focus on point processes for which @xmath5 . and so , in this case , we label the time series only with dwell times : @xmath6 .",
    "we view the time series @xmath7 as a realization of random variables @xmath8 .",
    "when the observed time series is strictly stationary and the process ergodic , in principle , we can calculate the probability distribution @xmath9 from a single realization @xmath7 .    demarcating the present splits @xmath10 into two parts : the time @xmath11 since first emitting the previous symbol and the time @xmath12 to next symbol .",
    "thus , we define @xmath13 as the _ past _ and @xmath14 as the _ future_. ( to reduce notation , we drop the @xmath15 indices . ) the _ present _ @xmath16 itself extends over an infinitesimally small length of time .",
    "continuous - time renewal processes have a relatively simple generative model . _ interevent intervals _ @xmath17 are drawn from a probability density function @xmath18 . the _ survival function _",
    "@xmath19 is the probability that an interevent interval is greater than or equal to @xmath20 and , in a nod to neuroscience , we define the _ mean firing rate _ @xmath21 as : @xmath22 the minimal generative model for a continuous - time renewal process is therefore a single causal - state machine with a continuous - value observable @xmath23 ; as shown in fig .",
    "[ fig : genmodel ] .     of periods of silence ( corresponding to output symbol @xmath24 )",
    "are drawn independently , identically distributed ( iid ) from probability density @xmath18 . ]",
    "a process _ forward - time causal states _ are defined , as usual , by the _",
    "predictive _ equivalence relation @xcite , written here for the case of point processes : @xmath25 it is straightforward to write the predictive equivalence relation for continuous - time , discrete - alphabet point processes using the notation .",
    "this partitions the set of allowed pasts .",
    "each equivalence class of pasts is a forward - time causal state @xmath26 , in which @xmath27 is the function that maps a past to its causal state .",
    "the set of forward - time causal states @xmath28 inherits a probability distribution @xmath29 from the probability distribution over pasts @xmath30 .",
    "_ forward - time prescient statistics _ are any refinement of the forward - time causal - state partition . by construction , they are a sufficient statistic for prediction , but not necessarily _ minimal _ sufficient statistics @xcite",
    ".    _ reverse - time causal states _ are essentially forward - time causal states of the time - reversed process . in short , reverse - time causal states @xmath31 are the classes defined by the retrodictive equivalence relation , written here for the case of point processes : @xmath32 it is , again , straightforward to write the predictive equivalence relation for continuous - time , discrete - alphabet point processes using the notation given above . and , similarly , reverse - time causal states @xmath33 inherit a probability measure @xmath34 from the probability distribution @xmath35 over futures .",
    "reverse - time prescient statistics are any refinement of the reverse - time causal - state partition .",
    "they are sufficient statistics for retrodiction , but not necessarily minimal .",
    "the main import of these definitions derives from the _ causal shielding _ relations : @xmath36 the consequence of these is illustrated in fig .",
    "[ fig : intuition2 ] .",
    "that is , arbitrary functions of the past and future do not shield the two aggregate past and future random variables from one another .",
    "so , these causal shielding relations are special to prescient statistics , causal states , and their defining functions @xmath27 and @xmath37 .",
    "forward and reverse - time generative models do not , in general , have state spaces that satisfy eqs .",
    "( [ eq : cs1 ] ) and ( [ eq : cs2 ] ) .",
    "( left oval , red ) , the future @xmath38 ( right oval , green ) , the forward - time causal states @xmath39 ( left circle , purple ) , and the reverse - time causal states @xmath40 ( right circle , blue ) .",
    "@xcite . ) the forward - time and reverse - time statistical complexities are the entropies of @xmath39 and @xmath40 , i.e. , the memories required to losslessly predict or retrodict , respectively .",
    "the excess entropy @xmath41 $ ] is a measure of process predictability ( central pointed ellipse , dark blue ) and theorem @xmath42 of ref .",
    "@xcite shows that @xmath43 $ ] by applying the causal shielding relations in eqs .",
    "( [ eq : cs1 ] ) and ( [ eq : cs2 ] ) . ]",
    "the _ forward - time _ is that with state space @xmath39 and transition dynamic between forward - time causal states .",
    "the _ reverse - time _ is that with state space @xmath40 and transition dynamic between reverse - time causal states .",
    "defining these transition dynamics for continuous - time processes requires a surprising amount of care , as discussed in secs .",
    "[ sec : causalstates]-[sec : infoarch ] .",
    "we are broadly interested in information - theoretic characterizations of a process predictability , compressibility , and randomness .",
    "a list of current quantities of interest , though by no means exhaustive , is given in figs .",
    "[ fig : intuition2 ] and [ fig : intuition3 ] .",
    "curiously , many lose meaning when naively applied to continuous - time processes ; e.g. , see refs . @xcite .",
    "this section , as a necessity , will redefine many of these in relatively simple , but new ways to avoid trivial divergences and zeros .",
    "the _ forward - time statistical complexity _",
    "$ ] is the cost of coding the forward - time causal states and the _ reverse - time statistical complexity _",
    "$ ] is the cost of coding reverse - time causal states .",
    "when @xmath39 or @xmath40 are mixed or continuous random variables , one employs differential entropies for @xmath46 $ ] .",
    "the result , though , is that the statistical complexities are potentially negative or infinite or both ( * ? ? ?",
    "8.3 ) , perhaps undesirable characteristics for a definition of process complexity .",
    "this definition , however , allows for consistency with complexity definitions for discretized continuous - time processes .",
    "@xcite for possible alternatives for @xmath46 $ ] .    together , a process _ causal irreversibility",
    "_ @xcite is defined as the difference between the forward and reverse - time statistical complexities : @xmath47 if the forward- and reverse - time process  are isomorphic  i.e . , if the process is temporally reversible  then @xmath48 .",
    "renewal processes are temporally symmetric : @xmath48 @xcite . as such",
    ", we will refer to forward - time causal states and the forward - time  as simply causal states or the , with the understanding that reverse - time causal states and reverse - time  will take the exact same form with slight labeling differences .",
    "we start by describing prescient statistics for continuous - time processes .",
    "the lemma which does this exactly parallels that of lemma @xmath42 of ref .",
    "the only difference is that the prescient statistic is the _ time _ since last event , rather than the number of @xmath24s ( count ) since last event .",
    "the time @xmath11 since last event is a prescient statistic of renewal processes .",
    "[ lem : ctrp_prescient ]    from bayes rule : @xmath49 interevent intervals @xmath17 are independent of one another , so @xmath50 .",
    "the random variables @xmath11 and @xmath12 are functions of @xmath10 and the location of the present . both @xmath11 and @xmath12 are independent of other interevent intervals .",
    "and so , @xmath51 . this implies : @xmath52 the predictive equivalence relation groups two pasts @xmath53 and @xmath54 together when @xmath55 .",
    "we see that @xmath56 is a sufficient condition for this from eq .",
    "( [ eq : lemproof ] ) . the lemma follows .",
    "some renewal processes are quite predictable , while others are purely random .",
    "a poisson process is the latter : interevent intervals are drawn independently from an exponential distribution and so knowing the time since last event provides no predictive benefit",
    ". a fractal renewal process can be the former . there",
    ", the interevent interval is so structured that the resultant process can have power - law correlations @xcite .",
    "then , knowing the time since last event can provide quite a bit of predictive power @xcite .",
    "intermediate between these two extremes is a broad class of renewal processes whose interevent intervals are structured up to a point and then fall off exponentially only after some time @xmath57 .",
    "these intermediate cases can be classified as either of the following types of renewal process , in analogy with ref .",
    "@xcite s classification .",
    "note that an eventually @xmath58-poisson process , but not an eventually poisson process , will generally have a discontinuous @xmath18 .",
    "an _ eventually poisson process _ has : @xmath59 for some @xmath60 and @xmath61 almost everywhere .",
    "we associate the eventually poisson process with the minimal such @xmath62 .",
    "[ def : ep ]    an _ eventually @xmath58-poisson process _ with @xmath63 has an interevent interval distribution satisfying : @xmath64 for the smallest possible @xmath57 for which @xmath65 exists .",
    "[ def : edp ]    a familiar example of an eventually poisson process is found in the spike trains generated by poisson neurons with refractory periods @xcite .",
    "there , the neuron is effectively prevented from firing two spikes within a time @xmath62 of each other  the period during with its ion channels re - energize the membrane voltage to their nonequilibrium steady state .",
    "after that , the time to next spike is drawn from an exponential distribution . to exactly predict the spike train s future",
    ", we must know the time since last spike , as long as it is less than @xmath62 .",
    "we gain a great deal of predictive power from that piece of information .",
    "however , we do not care much about the time since last spike exactly if it is greater than @xmath62 , since at that point the neuron acts as a memoryless poisson neuron .",
    "these intuitions are captured by the following classification theorem .",
    "a renewal process has three different types of causal state :    1 .   when the renewal process is not eventually @xmath58-poisson , the causal states are the time since last event ; 2 .   when the renewal process is eventually poisson , the causal states are the time since last event up until time @xmath57 ; or 3",
    "when the renewal process is eventually @xmath58-poisson , the causal states are the time since last event up until time @xmath57 and are the times since @xmath57 mod @xmath58 thereafter .",
    "[ the : renewcausalstates ]    lemma [ lem : ctrp_prescient ] implies that two pasts are causally equivalent if they have the same time since last event , if @xmath66 . from lemma [ lem : ctrp_prescient ]",
    "s proof , we further see that two times since last event are causally equivalent when @xmath67 . in terms of @xmath18",
    ", we find that : @xmath68 using manipulations very similar to those in the proof of thm .",
    "@xmath42 of ref .",
    "so , to find causal states , we look for @xmath69 such that : @xmath70 for all @xmath71 .    to unravel the consequences of this",
    ", we suppose that @xmath72 without loss of generality .",
    "define @xmath73 and @xmath74 , for convenience .",
    "the predictive equivalence relation can then be rewritten as : @xmath75 for any @xmath71 , where @xmath76 .",
    "iterating this relationship , we find that : @xmath77 this immediately implies the theorem s first case . if a renewal process is _ not _",
    "eventually @xmath58-poisson , then @xmath78 for all @xmath71 implies @xmath66 , so that the prescient statistics of lemma [ lem : ctrp_prescient ] are also minimal .    to understand the theorem s last two cases , we consider more carefully the set of all pairs @xmath79 for which @xmath80 for all @xmath71 holds .",
    "define the set : @xmath81 and define the parameters @xmath57 and @xmath65 by : @xmath82 and : @xmath83 note that @xmath57 and @xmath65 defined in this way are unique and exist , as we assumed that @xmath84 is nonempty . when @xmath85 , then the process is eventually @xmath58-poisson .",
    "if @xmath86 , then the process must be an eventually poisson process with parameter @xmath57 . to see this , we return to the equation : @xmath87 and rearrange terms to find : @xmath88 as @xmath89 , we can take the limit that @xmath90 and we find that : @xmath91 the righthand side is a parameter independent of @xmath92 .",
    "so , this is a standard ordinary differential equation for @xmath18 .",
    "it is solved by @xmath93 for @xmath94 .",
    "theorem  [ the : renewcausalstates ] implies that there is a qualitative change in @xmath39 depending on whether or not the renewal process is poisson , eventually poisson , eventually @xmath58-poisson , or not eventually poisson . in the first case , @xmath39 is a discrete random variable ; in the second case , @xmath39 is a mixed discrete - continuous random variable ; and in the third and fourth cases , @xmath39 is a continuous random variable .",
    "identifying causal states in continuous - time follows an almost entirely similar path to that used for discrete - time renewal processes in ref .",
    "the seemingly slight differences between the causal states of eventually poisson , eventually @xmath58-poisson , and not eventually @xmath58-poisson renewal processes , however , have surprisingly important consequences for continuous - time .    as described by thm .",
    "[ the : renewcausalstates ] , there are often an uncountable infinity of continuous - time causal states . as one might anticipate from refs .",
    "@xcite , however , there is an ordering to this infinity of causal states that makes calculations tractable .",
    "there is one major difference between discrete - time  and continuous - time : transition dynamics often amount to specifying the evolution of a probability density function over causal - state space .    as such",
    ", a continuous - time  constitutes an unusual presentation of a hidden markov model : they appear as a system of conveyor belts or , under special conditions , like conveyor belts with a trash bin or a second mini - conveyor belt . beyond the picaresque metaphor , in fact they operate like conveyor belts in that they transport the time since the last event , resetting it here and there in a stateful way .",
    "unsurprisingly , the exception to this general rule is given by the poisson process itself .",
    "the  of a poisson process is exactly the minimal generative model shown in fig .",
    "[ fig : genmodel ] . at each iteration",
    ", an interevent interval is drawn from a probability density function @xmath95 , with @xmath96 . knowing the time since last event does not aid in predicting the time to next event , above and beyond knowing @xmath97 . and",
    "so , the poisson  has only a single state .    in the general",
    "setting , though , the  dynamic describes the evolution of the probability density function over its causal states .",
    "how to represent this ?",
    "we might search for labeled transition operators @xmath98 such that @xmath99 , giving partial differential equations that govern the labeled - transition dynamics .",
    ", tracking the time since last event and depicted as the semi - infinite horizontal line , are isomorphic with the positive real line .",
    "if no event is seen , probability flows towards increasing time since last event , as described in eq .",
    "( [ eq : mathcalo0 ] ) .",
    "otherwise , arrows denote allowed transitions back to the reset state or `` @xmath24 node '' ( solid black circle at left ) , denoting that an event occurred . ]",
    "the  of a renewal process that is not eventually poisson takes the state - transition form shown in fig .",
    "[ fig : nedp ] .",
    "let @xmath100 be the probability density function over the causal states @xmath101 at time @xmath20 .",
    "our approach to deriving labeled transition dynamics parallels well - known approaches to determining fokker - planck equations using a kramers - moyal expansion @xcite . here",
    ", this means that any probability at causal state @xmath101 at time @xmath102 could only have come from causal state @xmath103 at time @xmath20 , if @xmath104 .",
    "this implies : @xmath105 however , @xmath106 is simply the probability that the interevent interval is greater than @xmath101 , given that the interevent interval is at least @xmath107 , or : @xmath108 together , eqs .",
    "( [ eq : probflow1 ] ) and ( [ eq : probflow2 ] ) imply that : @xmath109 from this , we obtain : @xmath110 hence , the labeled transition operator @xmath111 given no event takes the form : @xmath112 the probability density function @xmath100 changes discontinuously after an event occurs , though .",
    "all probability mass shifts from @xmath113 resetting back to @xmath114 : @xmath115 in other words , an event `` collapses the wavefunction '' .    the stationary distribution @xmath116 over causal states is given by setting @xmath117 to @xmath24 and solving .",
    "( at the risk of notational confusion , we adopt the convention that @xmath116 denotes the stationary distribution and that @xmath100 does not . ) straightforward algebra shows that : @xmath118    from this , the continuous - time statistical complexity directly follows : @xmath119 this was the nondivergent component of the infinitesimal time - discretized renewal process statistical complexity found in ref .",
    "@xcite .",
    "are isomorphic with the real line only to @xmath120 $ ] , as they again denote time since last event .",
    "a leaky absorbing node at @xmath57 ( solid white circle at right ) corresponds to any time since last event after @xmath57 .",
    "if no event is seen , probability flows towards increasing time since last event or the leaky absorbing node , as described in eqs .",
    "( [ eq : mathcalo0 ] ) and ( [ eq : mathcalo0b ] ) . when an event occurs the process transitions ( curved arrows ) back to the reset state",
    " node @xmath24 ( solid black circle at left ) . ]      as thm .  [ the : renewcausalstates ]",
    "anticipates , there is a qualitatively different topology to the  of an eventually poisson renewal process , largely due to the continuous - time causal states being mixed discrete - continuous random variables . for @xmath121 ,",
    "there is `` wave '' propagation completely analogous to that described in eq .",
    "( [ eq : mathcalo0 ] ) of sec .",
    "[ sec : nedp ] .",
    "however , there is a new kind of continuous - time causal state at @xmath122 , which does not have a one - to - one correspondence to the dwell time .",
    "instead , it denotes that the dwell time is _ at least _ some value ; viz . , @xmath57 .",
    "new notation follows accordingly : @xmath100 , defined for @xmath123 , denotes a probability density function for @xmath121 and @xmath124 denotes the probability of existing in causal state @xmath125 .",
    "normalization , then , requires that : @xmath126    the transition dynamics for @xmath124 are obtained similarly to that for @xmath100 , in that we consider all ways in which probability flows to @xmath127 in a short time window @xmath128 .",
    "probability can flow from any causal state with @xmath129 or from @xmath122 itself .",
    "that is , if no event is observed , we have : @xmath130 the term @xmath131 corresponds to probability flow from @xmath122 and the integrand corresponds to probability influx from states @xmath132 with @xmath133 .",
    "assuming differentiability of @xmath124 with respect to @xmath20 , we find that : @xmath134 where @xmath135 is shorthand for @xmath136 .",
    "this implies that the labeled transition operator @xmath111 takes a piecewise form which acts as in eq .",
    "( [ eq : mathcalo0 ] ) for @xmath121 and as in eq .",
    "( [ eq : mathcalo0b ] ) for @xmath122 . as earlier , observing an event causes the `` wavefunction collapse '' to a delta distribution at @xmath114 .",
    "the causal - state stationary distribution is determined again by setting @xmath137 and @xmath138 to @xmath24 .",
    "equivalently , one can use the prescription suggested by thm .  [ the : renewcausalstates ] to calculate @xmath139 via integration of the stationary distribution over the prescient machine given in sec .",
    "[ sec : nedp ] : @xmath140 if we recall that @xmath141 , we find that : @xmath142 the process continuous - time statistical complexity  precisely , entropy of this mixed random variable  is given by : @xmath143 this is the sum of the nondivergent @xmath144 component and the rate of divergence of @xmath144 of the infinitesimal time - discretized renewal process @xcite .    -poisson renewal process : graphical elements as in the previous figure .",
    "the circular causal - state space at @xmath57 ( circle on right ) has total duration @xmath65 , corresponding to any time since last event after @xmath57 mod @xmath65 . if no event is seen , probability flows as indicated around the circle , as described in eq .",
    "( [ eq : mathcalo0 ] ) . ]      probability wave propagation equations , like those in eq .",
    "( [ eq : mathcalo0 ] ) , hold for @xmath121 and for @xmath145 . at @xmath122 , if no event is observed , probability flows in from both @xmath146 and from @xmath147 , giving rise to the equation : @xmath148 unfortunately , there is a discontinuous jump in @xmath100 at @xmath122 coming from @xmath147 and @xmath149 . and",
    "so , we can not taylor expand either @xmath150 or @xmath151 about @xmath152 .",
    "again , we can use the prescription suggested by thm .",
    "[ the : renewcausalstates ] to calculate the probability density function over these causal states and , from that , calculate the continuous - time statistical complexity .",
    "below @xmath121 , the probability density function over causal states is exactly that described in sec .",
    "[ sec : nedp ] : @xmath153 . for @xmath154 ,",
    "the probability density function becomes : @xmath155 recalling def .",
    "[ def : edp ] , we see that @xmath156 and so find that for @xmath157 : @xmath158 altogether , this gives the statistical complexity : @xmath159    [ cols= \" < , < \" , ]",
    "we define continuous - time information anatomy @xcite quantities as _ rates_. as mentioned earlier , the present extends over an infinitesimal time . to define information anatomy rates , we let @xmath160 be the symbols observed over an arbitrarily small length of time @xmath161 , starting at the present @xmath162",
    ". it could be that @xmath160 encompasses some portion of @xmath163 ; the notation leaves this ambiguous .",
    "the entropy rate is now : @xmath164}{d\\delta }    ~. \\label{eq : hmupercs}\\end{aligned}\\ ] ] this is equivalent to the more typical random - variable `` block '' definition of entropy rate @xcite : @xmath165 / \\delta$ ] .",
    "similarly , we define the _ single - measurement entropy rate _ as : @xmath166}{d\\delta }    ~ , \\label{eq : h0percs}\\end{aligned}\\ ] ] the _ bound information rate _ as : @xmath167}{d\\delta }    ~ , \\label{eq : bmupercs}\\end{aligned}\\ ] ] the _ ephemeral information rate _ as : @xmath168}{d\\delta }    ~,\\end{aligned}\\ ] ] and the _ co - information rate _ as : @xmath169}{d\\delta }    ~.\\end{aligned}\\ ] ] in direct analogy to discrete - time process information anatomy , we have the relationships : @xmath170 so , the entropy rate @xmath171 , the instantaneous rate of information creation , again decomposes into a component @xmath172 that represents active information storage and a component @xmath173 that represents `` wasted '' information .    ):",
    "information diagram for the past @xmath174 , infinitesimal present @xmath160 , and future @xmath175 .",
    "the measurement entropy rate @xmath176 is the rate of change of the single - measurement entropy @xmath177 $ ] at @xmath178 .",
    "the ephemeral information rate @xmath179 $ ] is the rate of change of useless information generation at @xmath178 .",
    "the bound information rate @xmath180 $ ] is the rate of change of active information storage . and , the co - information rate @xmath181 $ ] is the rate of change of shared information between past , present , and future .",
    "these definitions closely parallel those in ref . @xcite . ]    prescient states ( not necessarily _ minimal _ ) are adequate for deriving all information measures aside from @xmath182 . as such",
    ", we focus on the transition dynamics of noneventually @xmath58-poisson  and , implicitly , their bidirectional machines .",
    "to find the joint probability density function of the time to next event @xmath183 and time since last event @xmath184 , we note that @xmath185 is an interevent interval ; hence : @xmath186 the normalization factor of this distribution is : @xmath187 so , the joint probability distribution is : @xmath188 equivalently , we could have calculated the conditional probability density function of time - to - next - event given that it has been at least @xmath184 since the last event .",
    "this , by similar arguments , is @xmath189 .",
    "this would have given the same expression for @xmath190 .    to find the excess entropy ,",
    "we merely need calculate @xcite : @xmath191 \\\\    & = \\int_0^{\\infty } \\int_0^{\\infty } \\mu\\phi ( { { \\sigma } } ^+ , { { \\sigma } } ^- ) \\log \\frac{\\mu \\phi ( { { \\sigma } } ^+ , { { \\sigma } } ^-)}{\\phi ( { { \\sigma } } ^+)\\phi ( { { \\sigma } } ^- ) } d { { \\sigma } } ^+ d { { \\sigma } } ^-     ~.\\end{aligned}\\ ] ] algebra not shown here gives : @xmath192 unsurprisingly @xcite , this agrees with the formula given in ref .",
    "@xcite , which was derived by considering the limit of infinitesimal time discretization .",
    "now , we turn to the more technically challenging task of calculating differential information anatomy rates .",
    "suppose that @xmath193 is a random variable for paths of length @xmath161 .",
    "each path is uniquely specified by a list of times of events .",
    "let @xmath194 be a random variable defined by : @xmath195 we first illustrate how to find @xmath176 , since the same technique allows calculating @xmath171 .",
    "we can rewrite the path entropy as : @xmath196 = { \\operatorname{h}}[x_{\\delta } ] + { \\operatorname{h}}[\\gamma_{\\delta}|x_{\\delta } ]    ~.\\end{aligned}\\ ] ] for renewal processes , when @xmath21 can be defined , we see that : @xmath197 straightforward algebra shows that : @xmath198 & = \\mu\\delta - \\mu\\delta\\log ( \\mu\\delta ) + o(\\delta^2\\log\\delta )    ~.\\end{aligned}\\ ] ] we would like to find a similar asymptotic expansion for @xmath199 $ ] , which can be rewritten as : @xmath200    & = { \\pr}(x_{\\delta}=0 ) { \\operatorname{h}}[\\gamma_{\\delta}|x_{\\delta}=0 ] \\nonumber \\\\    & \\qquad + { \\pr}(x_{\\delta}=1 ) { \\operatorname{h}}[\\gamma_{\\delta}|x_{\\delta}=1 ] \\nonumber \\\\    & \\qquad + { \\pr}(x_{\\delta}=2 ) { \\operatorname{h}}[\\gamma_{\\delta}|x_{\\delta}=2 ]    ~.\\end{aligned}\\ ] ] first , we notice that @xmath193 is deterministic given that @xmath201the path of all silence .",
    "so , @xmath202 = 0 $ ] .",
    "second , we can similarly ignore the term @xmath203 $ ] since @xmath204 is @xmath205 and , we claim , @xmath206 $ ] is @xmath207 : by standard maximum entropy arguments , @xmath206 $ ] is at most @xmath208 , and by noting that trajectories with only one event are a strict subset of trajectories with more than one event but with multiple events arbitrarily close to one another , @xmath206\\geq { \\operatorname{h}}[\\gamma_{\\delta}|x_{\\delta}=1]$ ] which , by arguments below , is @xmath207 .",
    "thus , the term @xmath209 $ ] is @xmath210 at most . finally , to calculate @xmath211 $ ]",
    ", we note that when @xmath212 , paths can be uniquely specified by an event time , whose probability is @xmath213 .",
    "a taylor expansion about @xmath214 shows that @xmath215 for some @xmath216 in which @xmath217 for all @xmath218 .",
    "so , overall , we find that : @xmath219 where @xmath220 for any @xmath193 with at least one event in the path .",
    "the largest corrections to @xmath221 come from ignoring the paths with two or more events , rather than from approximating all paths with only one event as equally likely . in sum",
    ", we see that : @xmath200    & = \\mu\\delta\\log\\delta + o(\\delta^2\\log\\delta )    ~.\\end{aligned}\\ ] ] together , these manipulations give : @xmath196    & = \\mu\\delta - \\mu\\delta\\log\\mu + o(\\delta^2\\log\\delta )    ~.\\end{aligned}\\ ] ] this then implies : @xmath222}{d\\delta } \\\\    & = \\mu-\\mu\\log\\mu    ~.\\end{aligned}\\ ] ] a similar series of arguments helps to calculate @xmath223 defined in eq .",
    "( [ eq : hmupercs ] ) , where now , @xmath21 is replaced by @xmath224 : @xmath225 which gives : @xmath226 algebra ( namely , integration by parts ) not shown here yields the expression : @xmath227 as expected , this is the nondivergent component of the expression given in eq .",
    "( @xmath228 ) of ref .",
    "@xcite for the @xmath161-entropy rate of renewal processes . and",
    ", it agrees with expressions derived in alternative ways @xcite .",
    "we need slightly different techniques to calculate @xmath172 , as we no longer need to decompose a path entropy . from eq .",
    "( [ eq : bmupercs ] ) , we have : @xmath229}{d\\delta }    ~.\\end{aligned}\\ ] ] let s develop a short - time @xmath161 asymptotic expansion for @xmath230 .",
    "first , we notice that @xmath231 , so that : @xmath232 we already can identify : @xmath233 to understand @xmath234 , we expand : @xmath235 recall that @xmath236 is @xmath205 , that : @xmath237 and that : @xmath238 then , straightforward algebra not shown gives : @xmath239 this can be used to derive : @xmath240 in nats .",
    "when @xmath241 , for instance , @xmath242 for all @xmath184 , confirming in a much more complicated calculation that poisson processes really are memoryless .",
    "this allows us to calculate the total @xmath172 as : @xmath243 in nats . and , from this , we find @xmath173 using : @xmath244 continuing",
    ", we calculate @xmath245 from : @xmath246 and , we calculate @xmath247 via : @xmath248 all these quantities are gathered in table [ tab:1 ] , which gives them in bits rather than nats .",
    "the  of discrete - time , discrete - symbol processes are well understood and , as we now appreciate from secs .  [",
    "sec : causalstates]-[sec : infoarch ] , the predictive equivalence relation defining them readily applies to continuous - time renewal processes .",
    "this gives the latter s analogous maximally predictive models : continuous or hybrid discrete - continuous , when minimal .",
    "here , we introduce a new class of process generators that are unifilar versions of ref .",
    "@xcite s hidden semi - markov models , but whose dwell time distributions can take any form . ( note that general semi - markov models are a strict subset . ) roughly speaking , they are stateful renewal processes , but this needs to be clarified .",
    "many of their calculations reduce to those in secs .  [",
    "sec : causalstates]-[sec : infoarch ] .",
    "when appropriate , we skip these steps .",
    "we start by introducing the minimal generative models in fig .",
    "[ fig : uhsmm ] .",
    "let @xmath249 be the set of states in this generative model . each state",
    "@xmath250 emits a symbol @xmath251 and a dwell time @xmath252 for that symbol , and , based on the state @xmath253 and emitted symbol @xmath1 , transitions to a new state @xmath254 .",
    "we assume that the underlying generative model is _ unifilar _ : that the new state @xmath253 is uniquely specified by the prior state @xmath253 and emitted symbol @xmath1 .",
    "we introduce a perhaps unfamiliar restriction on the labeled transition matrices @xmath255 .",
    "define @xmath256 and @xmath257 .",
    "then , we focus only on generative models for which @xmath258 .",
    "this simply ensures that there is no uncertainty in when one dwell time finishes and another begins .",
    "for example , consider the generator in fig .",
    "[ fig : uhsmm](bottom ) : if states @xmath259 and @xmath260 were both to emit a @xmath24 in succession , it would be impossible to tease apart when the process switched from state @xmath259 to state @xmath260 .",
    "the restriction introduces no loss of generality for our purposes .    ,",
    "@xmath261 , @xmath262 , and @xmath263 are isomorphic to @xmath264 . during an event interval ,",
    "symbol @xmath265 is emitted .",
    "a transition occurs to a new event when the state dwell time is exhausted at @xmath266 , which is distributed according to @xmath267 .",
    "( bottom ) generative model with three hidden states ( @xmath268 , @xmath259 , and @xmath260 ) emits symbols @xmath269 for dwell times @xmath270 drawn from probability density functions @xmath271 , @xmath272,@xmath273 , and @xmath274 , respectively .",
    "( transition labels as in fig .",
    "[ fig : genmodel ] . ) ]    a prescient model of this combined process is shown in fig .",
    "[ fig : uhsmm](top ) .",
    "each state @xmath250 comes equipped with one or more renewal process - like tails ( semi - infinite spaces that act as continuous counters ) that generically take the form of fig .",
    "[ fig : nedp ] . the leakiness of these ( dissipative ) counters is given by @xmath275 , the probability density function from which the dwell time is drawn .",
    "this new form of state - transition diagram depicts the  of these hidden semi - markov processes .",
    "moreover , if one or more of the dwell - time distributions gives an eventually poisson or an eventually-@xmath58 poisson structure , the presentation in fig .",
    "[ fig : uhsmm](top ) is a prescient machine , but not the .",
    "more generally , any such unifilar minimal generative model has a prescient machine with a `` node '' for each underlying hidden state @xmath253 and as many counters as needed  one for every almost - everywhere unique @xmath276 .",
    "each counter leaks probability to the next underlying hidden state @xmath254 , which is completely determined by @xmath253 and @xmath1 .",
    "the presentation in fig .",
    "[ fig : uhsmm](top ) is a prescient machine for the process generated by the unifilar hidden semi - markov model of fig .",
    "[ fig : uhsmm](bottom ) .",
    "[ the:2 ]    to show that this is a prescient machine , we need to show that the present model state consisting of hidden state @xmath253 , current emitted symbol @xmath277 , and dwell time @xmath270 is uniquely specified by the observed past almost surely . the observed symbol @xmath1 is given by the current symbol in the observed past .",
    "the restriction on successive emitted symbols ( that @xmath278 ) implies that the observed dwell time @xmath270 is exactly the observed length of @xmath1 .",
    "finally , the underlying hidden state @xmath253 is determined uniquely by a _ function _ of the past almost surely , in which all dwell - time information is removed , by assumption : the restriction mentioned earlier implies there is no uncertainty in when one dwell time finishes and another begins . and , the unifilarity of the dynamic on hidden states @xmath253 implies that the sequence of symbols in the observed past are sufficient to specify the hidden state @xmath253 almost surely .",
    "hence , @xmath253 is determined uniquely from the observed past almost surely .",
    "the theorem follows .    theorem  [ the:2 ] can be straightforwardly generalized to specify conditions under which the presentation is an , a minimal prescient machine , by incorporating the conditions of thm .",
    "[ the : renewcausalstates ] .",
    "the stationary distribution for @xmath279 directly follows the treatment for the continuous - time renewal processes in sec .",
    "[ sec : nedp ] , and so : @xmath280 where @xmath281 .",
    "then we note that : @xmath282 and so : @xmath283 to find @xmath284 , we again calculate the probability mass dumped at @xmath285 in terms of @xmath284 : @xmath286 after a straightforward substitution of eq .",
    "( [ eq : foo ] ) and noting that @xmath287 , we find : @xmath288 so : @xmath289 let @xmath290 be the stationary distribution for the underlying discrete - state : @xmath291 where the eigenvector is normalized such that the sum of its entries is @xmath42 .",
    "then : @xmath292 or , rewriting and normalizing , we have : @xmath293 altogether , we find that the steady - state distribution is given by : @xmath294 using the formulae for entropies of mixed random variables @xcite , we find a statistical complexity of : @xmath295 \\\\         & = \\left\\langle { \\operatorname{h}}[\\rho(\\tau|g , { { x } } ) ] \\right\\rangle_{p(g , { { x } } ) }         + { \\operatorname{h}}[p(g , { { x } } ) ] \\\\         & = \\left\\langle \\int_0^{\\infty } \\mu_{g , { { x } } } \\phi_{g , { { x } } } ( \\tau )         \\log \\frac{1}{\\mu_{g , { { x } } } \\phi_{g , { { x } } } ( \\tau ) } d\\tau         \\right\\rangle_{p(g , { { x } } ) } \\\\         & \\qquad + { \\operatorname{h}}\\left [ \\frac{\\pi(g )         t_g^ { ( { { x } } ) } /\\mu_{g , { { x } } } } { \\sum_{g ' , { { x } } ' } \\pi(g ' )         t_{g'}^ { ( { { x } } ' ) } /\\mu_{g ' , { { x } } ' } } \\right ]         ~.\\end{aligned}\\ ] ] note that @xmath296 $ ] is the statistical complexity of the underlying discrete - time  and that @xmath297 $ ] is the statistical complexity of a noneventually @xmath58-poisson renewal process with interevent distribution @xmath298 , averaged over @xmath253 and @xmath1 .",
    "hence , the statistical complexity of these unifilar hidden semi - markov processes differs from the statistical complexity of its `` components '' by : @xmath299    - { \\operatorname{h}}[\\pi(g ) ]    ~.\\end{aligned}\\ ] ] whether this difference is positive or negative depends on both matrices @xmath300 . in general",
    ", we expect the difference to be positive .    since there are multiple observed symbols @xmath301 generated by these machines , @xmath176 ( and so @xmath247 ) and @xmath172 as defined in sec .",
    "[ sec : infoarch ] diverge .",
    "however , the entropy rate @xmath171 and excess entropy @xmath302 as defined in sec .",
    "[ sec : infoarch ] do not diverge for processes generated by this restricted class of unifilar hidden semi - markov models . from the steady - state distribution given in eq .",
    "( [ eq : steadystate ] ) and from the entropy rate expressions in eqs .",
    "( [ eq : hmust])-([eq : hmufinal ] ) of sec .",
    "[ sec : infoarch ] , we immediately have the entropy rate for these unifilar hidden semi - markov models : @xmath303}{d\\delta } \\nonumber \\\\    & = \\sum_{g , { { x } } } \\rho(g , { { x } } )    \\left(-\\mu_{g , { { x } } } \\int_0^{\\infty } \\phi_{g , { { x } } } ( \\tau)\\log \\phi_{g , { { x } } } ( \\tau ) d\\tau\\right )    \\nonumber \\\\    & = -\\frac{\\sum_{g , { { x } } } \\pi(g ) t_g^ { ( { { x } } ) } \\int_0^{\\infty } \\phi_{g , { { x } } } ( \\tau)\\log \\phi_{g , { { x } } } ( \\tau ) d\\tau }    { \\sum_{g ' , { { x } } ' } \\pi(g ' ) t_{g'}^ { ( { { x } } ' ) } /\\mu_{g ' , { { x } } ' } }    ~. \\label{eq : hmu_hsmm}\\end{aligned}\\ ] ] to ground intuition , recall that each state in the underlying  for semi - markov processes corresponds to a unique observation symbol .",
    "hence , setting @xmath304 to @xmath305 and noting that each @xmath253 is uniquely associated to some @xmath1 in eq .",
    "( [ eq : hmu_hsmm ] ) recovers the results of ref .",
    "@xcite for the entropy rate of semi - markov processes , though the notation differs somewhat . ) to a semi - markov process in reverse - time so that the underlying model is unifilar rather than co - unifilar ; but entropy rate is invariant to time reversal @xcite . ]    the process excess entropy @xmath43 $ ] can be calculated if we can find the joint probability distribution @xmath306 of forward- and reverse - time causal states .",
    "to this end , we add an additional restriction on the generative model : we focus only on generative models for which @xmath307 . with this restriction on labeled transition matrices ,",
    "the time - reversed  of the process has the same form as the  of the forward - time process , but with a different @xmath249 .",
    "the latter is related to the forward - time @xmath249 via manipulations described in ref . @xcite . as such",
    ", we can write down @xmath308 : @xmath309 where we obtain @xmath310 from standard methods @xcite applied to ( only ) the dynamic on @xmath249 . note that @xmath311 reduces to @xmath312 as @xmath254 and @xmath313 uniquely",
    "specify the distribution from which @xmath314 is drawn and since @xmath315 .",
    "we leave the the final steps to @xmath302 as an exercise .",
    "though the definition of continuous - time causal states parallels that for discrete - time causal states , continuous - time  and information measures are markedly different from their discrete - time counterparts .",
    "similar technical difficulties arise more generally when describing minimal maximally predictive models of other continuous - time , discrete - symbol processes that are not the continuous - time markov processes analyzed in ref .",
    "the resulting  do not appear like conventional hmms  recall figs .",
    "[ fig : nedp]-[fig : edp ] and , especially , fig .",
    "[ fig : uhsmm](top)and most of the information measures  excepting the excess entropy  are reinterpreted as differential information rates",
    ".    moreover , the  continuous - time machinery gave us a new way to calculate these information measures .",
    "traditionally , expressions for such information measures come from calculating the time - normalized path entropy of arbitrarily long trajectories ; e.g. , as in ref .",
    "instead , we calculated the path entropy of arbitrarily short trajectories , conditioned on the past .",
    "this allowed us to extend the results of ref .",
    "@xcite for the entropy rate of continuous - time discrete - output processes to a previously untouched class of processes  unifilar hidden semi - markov processes .",
    "there are two immediate practical benefits to an in - depth look at the  of continuous - time hidden semi - markov processes .",
    "first , statistical model selection when searching through unifilar hidden markov models is significantly easier than when searching through nonunifilar hidden markov models @xcite , and these benefits should carry over to the case of continuous - time .",
    "second , the formulae in table [ tab:1 ] and those in sec .",
    "[ sec : uhsmms ] provide new approaches to binless plug - in information measure estimation ; e.g. , following ref .",
    "@xcite .",
    "the machinery required to use continuous - time  is significantly different than that accompanying the study of discrete - time .",
    "our results here pave the way toward understanding the difficulties that lie ahead when studying the structure and information in continuous - time processes .",
    "the authors thank the santa fe institute for its hospitality during visits .",
    "jpc is an sfi external faculty member .",
    "this material is based upon work supported by , or in part by , the u. s. army research laboratory and the u. s. army research office under contract number w911nf-13 - 1 - 0390 .",
    "sm was funded by a national science foundation graduate student research fellowship , a u.c .",
    "berkeley chancellor s fellowship , and the mit physics of living systems fellowship .",
    "v.  girardin . on the different extensions of the ergodic theorem of information theory . in r.",
    "baeza - yates , j.  glaz , h.  gzyl , j.  husler , and j.  l. palacios , editors , _ recent advances in applied probability theory _ , pages 163179 .",
    "springer us , 2005 .",
    "actually , we would apply eq .",
    "( [ eq : hmu_hsmm ] ) to a semi - markov process in reverse - time so that the underlying model is unifilar rather than co - unifilar ; but entropy rate is invariant to time reversal @xcite ."
  ],
  "abstract_text": [
    "<S> we introduce the minimal maximally predictive models ( ) of processes generated by certain hidden semi - markov models . </S>",
    "<S> their causal states are either hybrid discrete - continuous or continuous random variables and causal - state transitions are described by partial differential equations . </S>",
    "<S> closed - form expressions are given for statistical complexities , excess entropies , and differential information anatomy rates . </S>",
    "<S> we present a complete analysis of the  of continuous - time renewal processes and , then , extend this to processes generated by unifilar hidden semi - markov models and semi - markov models . </S>",
    "<S> our information - theoretic analysis leads to new expressions for the entropy rate and the rates of related information measures for these very general continuous - time process classes . </S>"
  ]
}