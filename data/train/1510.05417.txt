{
  "article_text": [
    "the analysis of ordinal categorical data  @xcite is required in various areas including finance , econometrics , and bioinformatics .",
    "for instance , credit ratings of financial instruments are typical ordinal categorical data , and thus , many previous studies have analyzed such data by means of ordinal classification models ( see , e.g. , @xcite ) .",
    "a sequential logit model  @xcite , also known as the continuation ratio model  @xcite , is a commonly used ordinal classification model .",
    "it predicts an ordinal class label for each sample by successively applying separate logistic regression models .",
    "one can find various applications of sequential logit models : kahn and morimune  @xcite used this model to explain the duration of unemployment of workers ; weiler  @xcite investigated the choice behavior of potential attendees in higher education institutions ; fu and wilmot  @xcite estimated dynamic travel demand caused by hurricane evacuation .    in order to enhance the reliability of these data analyses , it is critical to carefully choose a set of relevant features for model construction .",
    "such a feature subset selection problem is of essential importance in statistics , data mining , artificial intelligence , and machine learning ( see , e.g. , @xcite ) .",
    "the mixed integer optimization ( mio ) approach to feature subset selection has recently received a lot of attention as a result of algorithmic advances and hardware improvements ( see , e.g. , @xcite ) .",
    "in contrast to heuristic algorithms , e.g. , stepwise regression  @xcite , @xmath0-penalized regression  @xcite , and metaheuristic strategies  @xcite , the mio approach has the potential of providing an optimality guarantee for the selected set of features under a given goodness - of - fit measure .",
    "tanaka and nakagawa  @xcite recently devised an mio formulation for feature subset selection in a sequential logit model .",
    "it is hard to exactly solve the feature subset selection problem for a sequential logit model , because its objective contains a nonlinear function called the logistic loss function . to resolve this issue , they employed a quadratic approximation of the logistic loss function .",
    "the resultant mixed integer quadratic optimization ( miqo ) problem can be solved with standard mathematical optimization software ; however , there is a significant gap between the logistic loss function and its quadratic approximation . as a result",
    ", the miqo formulation may fail to find a good - quality solution to the original feature subset selection problem .",
    "the purpose of this paper is to give a novel mio formulation for feature subset selection in a sequential logit model .",
    "sato et al .",
    "@xcite used a piecewise - linear approximation in the feature subset selection problem for binary classification . in line with sato et al .",
    "@xcite , we shall apply a piecewise - linear approximation to the sequential logit model for ordinal multi - class classification .",
    "consequently , the problem is posed as a mixed integer linear optimization ( milo ) problem .",
    "this approach is capable of approximating the logistic loss function more accurately than the quadratic approximation does .",
    "moreover , our milo formulation has the advantage of selecting a set of features with an optimality guarantee on the basis of an information criterion , such as the akaike information criterion ( aic , @xcite ) or bayesian information criterion ( bic , @xcite ) .",
    "the effectiveness of our milo formulation is assessed through computational experiments on several datasets from the uci machine learning repository  @xcite .",
    "the computational results demonstrate that our piecewise - linear approximation approach found a better subset of features than the quadratic approximation approach did .",
    "let us suppose that we are given @xmath1 samples of pairs , @xmath2 for @xmath3 . here , @xmath4 is a @xmath5-dimensional feature vector , and @xmath6 is a ordinal class label to be predicted for each sample @xmath3 . in the sequential logit model for ordinal classification , we sequentially apply the following logistic regression models in order to predict a class label of each sample ( see , e.g. , @xcite ) , @xmath7 where the intercept , @xmath8 , and the @xmath5-dimensional coefficient vector , @xmath9 , are parameters to be estimated .    as shown in figure  [ fig : seqlog ] , a feature vector @xmath10 is moved into class 1 with a probability @xmath11 . in the next step",
    ", it falls into class 2 with a probability @xmath12 . in the similar manner",
    ", it reaches class @xmath13 with a probability @xmath14 .",
    "( 400,160)(0,0 ) ( 0,0)(1,0)400 ( 0,160)(1,0)400    ( 10,120)(1,0)60 ( 80,120)(1,0)60 ( 150,120)(1,0)60 ( 250,120)(1,0)60 ( 320,120)(1,0)60    ( 10,120)(1,-1)63 ( 80,120)(1,-1)63 ( 150,120)(1,-1)63 ( 250,120)(1,-1)63 ( 320,120)(1,-1)63    ( 20,130)@xmath15 ( 90,130)@xmath16 ( 160,130)@xmath17 ( 260,130)@xmath18 ( 340,130)@xmath19    ( 25,70)@xmath11 ( 95,70)@xmath20 ( 165,70)@xmath21 ( 260,70)@xmath22 ( 340,70)@xmath23    ( 10,120 ) ( 80,120 ) ( 150,120 ) ( 250,120 ) ( 320,120 ) ( 390,120 )    ( 80,50 ) ( 150,50 ) ( 320,50 ) ( 390,50 )    ( 10,120 ) ( 80,120 ) ( 150,120 ) ( 320,120 ) ( 390,120 )    ( 80,50 ) ( 150,50 ) ( 320,50 ) ( 390,50 )    ( 70,30)@xmath24 ( 140,30)@xmath25 ( 300,30)@xmath26 ( 375,30)@xmath27 ( 370,100)@xmath28    ( 230,80)@xmath29    here , we define @xmath30 it then follows that @xmath31 therefore , the occurrence probability of a sample @xmath2 is modeled as follows : @xmath32    we refer to model   as a forward sequential logit model because binary classification models   are applied in order from @xmath33 to @xmath34 .",
    "we can also consider the backward model that makes binary classification in the reverse order from @xmath34 to @xmath33 .",
    "it is known that these two models do not produce the same results  ( see @xcite ) .",
    "the maximum likelihood estimation method estimates the parameters , @xmath35 and @xmath36 , so that the log likelihood function , @xmath37 , is maximized : @xmath38 where @xmath39 the function @xmath40 is called the logistic loss function .",
    "this function is convex because its second derivative always has a positive value .",
    "hence , maximizing the log likelihood function   is a convex optimization problem .    from , and",
    ", we obtain a compact formulation of the log likelihood function , @xmath41 where @xmath42",
    "this section presents mixed integer optimization ( mio ) formulations for feature subset selection in the sequential logit model .",
    "similarly to the previous research  @xcite , we shall employ information criteria , e.g. , the akaike information criterion ( aic ,  @xcite ) and bayesian information criterion ( bic ,  @xcite ) , as a goodness - of - fit measure for the sequential logit model .    let @xmath43 be a set of selected features . accordingly , by setting the coefficients of other candidate features to zero , most information criteria can be expressed as follows : @xmath44 where @xmath45 is a penalty for the number of selected features .",
    "for instance , @xmath46 and @xmath47 correspond to the aic and bic , respectively .",
    "let @xmath48 be a vector of 0 - 1 decision variables ; @xmath49 if @xmath50 ; @xmath51 , otherwise .",
    "the feature subset selection problem for minimizing the information criterion   of the sequential logit model can be formulated as a mixed integer nonlinear optimization ( minlo ) problem , @xmath52    the logical implications   can be represented by a special ordered set type one ( sos1 ) constraint  @xcite .",
    "this constraint implies that not more than one element in the set can have a nonzero value , and it is supported by standard mio software .",
    "therefore , to incorporate the logical implications  , it is only necessary to impose the sos1 constraint on @xmath53 . indeed",
    ", if @xmath51 , then @xmath54 has a nonzero value , and @xmath55 must be zero from the sos1 constraints .",
    "the objective function   to be minimized is a convex but nonlinear function , which may cause numerical instabilities in the computation .",
    "moreover , most mio software can not handle such a nonlinear objective function . in view of these facts ,",
    "tanaka and nakagawa  @xcite used a quadratic approximation of the logistic loss function .",
    "the second - order maclaurin series of the logistic loss function   is written as follows : @xmath56 this quadratic approximation of the logistic loss function reduces the minlo problem   to a mixed integer quadratic optimization ( miqo ) problem , @xmath57    figure  [ fig : qa ] shows the graphs of the logistic loss function   ( dashed curve ) and its quadratic approximation   ( solid curve ) .",
    "we can see that the approximation error sharply increases with distance from @xmath58 .",
    "more importantly , the quadratic approximation function increases on the right side , while the logistic loss function monotonically decreases so that it reduces penalties on correctly classified samples .",
    "this means that the quadratic approximation imposes large penalties on correctly classified samples .",
    "consequently , the miqo problem   may fail to find a good subset of features .",
    "in order to approximate the logistic loss function more accurately , we propose the use of a piecewise - linear approximation instead of a quadratic approximation .    by following sato et al .",
    "@xcite , we make a piecewise - linear approximation of the logistic loss function .",
    "let @xmath59 be a set of @xmath60 discrete points .",
    "since the graph of a convex function lies above its tangent lines , the logistic loss function   can be approximated by the pointwise maximum of a family of tangent lines , that is , @xmath61        figure  [ fig : pla ] shows the graph of the logistic loss function   ( dashed curve ) together with the tangent lines ( solid lines ) at @xmath62 . also note that @xmath63 as shown in figure  [ fig : pla ] , the pointwise maximum of the four tangent lines creates a piecewise - linear underestimator of the logistic loss function",
    "it is clear that this approach approximates the logistic loss function more accurately than the quadratic approximation approach does .    by utilizing a piecewise - linear approximation of the logistic loss function",
    ", the feature subset selection problem for the sequential logit model can be posed as a mixed integer linear optimization ( milo ) problem , @xmath64 where @xmath65 is a decision variable for calculating the value of piecewise - linear approximation function .",
    "this milo problem approaches the original minlo problem   by increasing the number of tangent lines at appropriate points .",
    "moreover , this milo problem , as well as the miqo problem   , can be solved with standard mathematical optimization software .",
    "this section compares the effectiveness of our piecewise - linear approximation approach with that of the quadratic approximation approach employed by tanaka and nakagawa  @xcite .",
    "we downloaded eight datasets for ordinal classification from the uci machine learning repository  @xcite .",
    "table  [ tab : ins ] lists these instances , where @xmath1 and @xmath5 are the number of samples and number of candidate features , respectively ; and # class is the number of ordinal class labels , i.e. , @xmath66 .",
    ".list of instances [ cols=\"<,>,>,>,<\",options=\"header \" , ]     [ tab : bic_b ]    tables  [ tab : aic_f ]  and  [ tab : aic_b ] show the results of aic minimization in the forward and backward sequential logit models .",
    "these tables reveal that aic values of pwl were smaller than those of quad for all the instances .",
    "the computation time of pwl was longer than that of quad because the problem size of pwl is dependent on the number of samples ( see the constraints  ) .",
    "nevertheless , we should notice that pwl provided better - quality solutions than quad did in spite of the time limit of 10000 seconds .",
    "in addition , the number of features selected by pwl sometimes differed greatly from that selected by quad ; for instance , quad and pwl respectively selected 4 and 10 features for ` wine - r ` in table  [ tab : aic_f ] .",
    "we can see from tables  [ tab : aic_f ]  and  [ tab : aic_b ] that pwl approximated the logistic loss function very accurately , whereas quad caused a major gap between aic and objval .",
    "additionally , since the objective function of pwl is an underestimator relative to aic , its optimal value serves as a lower bound of the smallest aic . in the case of ` wine - r ` in table  [ tab : aic_f ] , aic and objval of quad were 3057.5 and 4204.6 , whereas those of pwl were 3028.4 and 3013.2 .",
    "this also implies that pwl found a subset of features such that the associated aic value was 3028.4 , and it collaterally guaranteed that the smallest aic value was greater than 3013.2 .",
    "this optimality guarantee is the most notable characteristic of our piecewise - linear approximation approach , and it can not be shared by the quadratic approximation approach .",
    "tables  [ tab : bic_f ]  and  [ tab : bic_b ] show the results of bic minimization in the forward and backward sequential logit models .",
    "we should recall that the penalty , @xmath45 , for the number of selected features in bic ( i.e. , @xmath47 ) is larger than that in aic ( i.e. , @xmath46 ) .",
    "hence , bic minimization selected a small number of features , and accordingly , quad and pwl often yielded the same subset of features for each instance in tables  [ tab : bic_f ]  and  [ tab : bic_b ] .",
    "meanwhile , when these two methods provided different subsets of features , pwl always found the better one .",
    "this paper dealt with the feature subset selection problem for a sequential logit model .",
    "we formulated it as a mixed integer linear optimization ( milo ) problem by applying a piecewise - linear approximation to the logistic loss functions .",
    "the computational results confirmed that our formulation has a clear advantage over the mixed integer quadratic optimization ( miqo ) formulation proposed in the previous study  @xcite .",
    "in contrast to the miqo formulation , the approximation accuracy of the logistic loss function can be controlled by the number of tangent lines in our milo formulation .",
    "furthermore , after the milo problem is solved , it provides an optimality guarantee of the selected features on the basis of information criteria . to the best of our knowledge",
    ", this paper is the first to compute a subset of features with an optimality guarantee for a sequential logit model .",
    "a future direction of study will be to extend our piecewise - linear approximation approach to other logit models .",
    "however , this will be a difficult task because it is imperative to approximate a multivariate objective function .",
    "another direction of future research is to analyze actual data by means of our feature subset selection method .",
    "for instance , sato et al .",
    "@xcite investigated consumers store choice behavior by applying feature subset selection based on mixed integer optimization . since",
    "proper feature subset selection is essential for data analysis , our approach has a clear advantage over heuristic algorithms .",
    "this work was partially supported by grants - in - aid for scientific research by the ministry of education , culture , sports , science and technology of japan .",
    "e. m. l. beale : two transportation problems . in g. kreweras and g. morlat ( eds . ) : _ proceedings of the third international conference on operational research _ ( dunod , paris and english universities press , london , 1963 ) , 780788",
    ".    e. m. l. beale and j. a. tomlin : special facilities in a general mathematical programming system for non - convex problems using ordered sets of variables .",
    "in j. lawrence ( ed . ) : _ proceedings of the fifth international conference on operational research _ ( tavistock publications , london , 1960 ) , 447454",
    ".                      h. t. kiiveri : a general approach to simultaneous model fitting and variable elimination in response models for biological data with many more variables than observations . _",
    "bmc bioinformatics , _ * 9 * ( 2008 ) , 195 .                    w. p. poon , m. firth , and h. g. fung : a multivariate analysis of the determinants of moody s bank financial strength ratings .",
    "_ journal of international financial markets , institutions and money , _ * 9 * ( 1999 ) , 267283 .    t. sato , y. takano , r. miyashiro , and a. yoshise : feature subset selection for logistic regression via mixed integer optimization .",
    "department of policy and planning sciences , discussion paper series , no .",
    "university of tsukuba ( 2015 ) .        k. tanaka and h. nakagawa : a method of corporate credit rating classification based on support vector machine and its validation in comparison of sequantial logit model .",
    "_ transactions of the operations research of japan , _ * 57 * ( 2014 ) , 92111 ."
  ],
  "abstract_text": [
    "<S> this paper concerns a method of selecting a subset of features for a sequential logit model . </S>",
    "<S> tanaka and nakagawa  ( 2014 ) proposed a mixed integer quadratic optimization formulation for solving the problem based on a quadratic approximation of the logistic loss function . </S>",
    "<S> however , since there is a significant gap between the logistic loss function and its quadratic approximation , their formulation may fail to find a good subset of features . to overcome this drawback </S>",
    "<S> , we apply a piecewise - linear approximation to the logistic loss function . </S>",
    "<S> accordingly , we frame the feature subset selection problem of minimizing an information criterion as a mixed integer linear optimization problem . </S>",
    "<S> the computational results demonstrate that our piecewise - linear approximation approach found a better subset of features than the quadratic approximation approach . </S>"
  ]
}