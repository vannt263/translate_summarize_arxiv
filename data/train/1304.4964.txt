{
  "article_text": [
    "multilinear models have proved useful in analyzing data in a variety of fields .",
    "we focus on data that derives from a poisson process , such as the number of packets sent from one ip address to another on a specific port  @xcite , the number of papers published by an author at a given conference  @xcite , or the count of emails between users in a given time period  @xcite .",
    "data in these applications is nonnegative and often quite sparse , i.e. , most tensor elements have a count of zero .",
    "the tensor factorization model corresponding to such sparse count data is computed from a nonlinear optimization problem that minimizes the kullback - leibler ( k - l ) divergence function and contains nonnegativity constraints on all variables .    in this paper",
    "we show how to make second - order optimization methods suitable for poisson - based tensor models of large sparse count data .",
    "multiplicative update is one of the most widely implemented methods for this model , but it suffers from slow convergence and inaccuracy in discovering the underlying sparsity . in large sparse tensors , the application of nonlinear optimization techniques requires consideration of sparsity and problem structure to get better performance .",
    "we show that , by exploiting the partial separability of the subproblems , we can successfully apply second - order methods .",
    "we develop algorithms that scale to large sparse tensor applications and are quick in identifying sparsity in the factors of the model .",
    "there is a need for second - order methods because computing factor matrices to high accuracy , as measured by satisfaction of the first - order kkt conditions , is effective in revealing sparsity .",
    "by contrast , multiplicative update methods can make elements small but are slow to reach the variable bound at zero , forcing the user to guess when `` small '' means zero .",
    "we demonstrate that guessing a threshold is inherently difficult , making the high accuracy obtained with second - order methods desirable .",
    "we start from a standard gauss - seidel alternating block framework and show that each block subproblem is further separable into a set of independent functions , each of which depends on only a subset of variables .",
    "we optimize each subset of variables independently , an obvious idea which has nevertheless not previously appeared in the setting of sparse tensors .",
    "we call this a _ row subproblem formulation _ because the subset of variables corresponds to one row of a factor matrix .",
    "each row subproblem amounts to minimizing a strictly convex function with nonnegativity constraints , which we solve using two - metric gradient projection techniques and exact or approximate second - order information .",
    "the importance of the row subproblem formulation is demonstrated in section [ subsec : exp - subprob ] , where we show that applying a second - order method directly to the block subproblem is highly inefficient .",
    "we provide evidence that a more effective way to apply second - order methods is through the use of the row subproblem formulation .",
    "our contributions in this paper are as follows :    1 .   a new formulation for nonnegative tensor factorization based on the kullback - leibler divergence objective that allows for the effective use of second - order optimization methods .",
    "the optimization problem is separated into row subproblems containing @xmath0 variables , where @xmath0 is the number of factors in the model .",
    "the formulation makes row subproblems independent , suggesting a parallel method , although we do not explore parallelism in this paper .",
    "two matlab algorithms for computing factorizations of sparse nonnegative tensors : one using second derivatives and the other using limited - memory quasi - newton approximations .",
    "the algorithms are made robust with an armijo line search , damping modifications when the hessian is ill conditioned , and projections to the bound of zero based on two - metric gradient projection ideas in @xcite .",
    "the two algorithms have different computational costs : the second derivative method is preferred when @xmath0 is small , and the quasi - newton when @xmath0 is large .",
    "test results that compare the performance of our two new algorithms with the best available multiplicative update method and a related quasi - newton algorithm that does not formulate using row subproblems .",
    "the most significant test results are reported in this paper ; detailed results of all experiments are available in the supplementary material ( appendix b ) .",
    "test results showing the ability of our methods to quickly and accurately determine which elements of the factorization model are zero without using problem - specific thresholds .",
    "the paper is outlined as follows : the remainder of section 1 surveys related work and provides a review of basic tensor properties .",
    "section 2 formalizes the poisson nonnegative tensor factorization optimization problem , shows how the gauss - seidel alternating block framework can be applied , and converts the block subproblem into independent row subproblems .",
    "section 3 outlines two algorithms for solving the row subproblem , one based on the damped hessian ( pdn - r for projected damped newton ) , and one based on a limited - memory approximation ( pqn - r for projected quasi - newton ) .",
    "section 4 details numerical results on synthetic and real data sets and quantifies the accuracy of finding a truly sparse factorization .",
    "additional test results are available in the supplementary material .",
    "section 5 contains a summary of the paper and concluding remarks .      in this paper",
    ", we specifically consider nonnegative tensor factorization ( ntf ) in the case of the canonical polyadic ( also known as candecomp / parafac ) tensor decomposition .",
    "our focus is on the k - l divergence objective function , but we also mention related work for the least squares ( ls ) case . additionally , we consider related work for nonnegative matrix factorization ( nmf ) for both k - l and ls .",
    "note that there is much more work in the ls case , but the k - l objective function is different enough that it deserves its own attention .",
    "we do not discuss other decompositions such as tucker .",
    "nmf in the ls case was first proposed by paatero and tapper @xcite and also studied by bro  @xcite .",
    "lee and seung later consider the problem for both ls and k - l formulations and introduce multiplicative updates based on the convex subproblems @xcite .",
    "their work is extended to tensors by welling and weber  @xcite .",
    "many other works have been published on the ls versions of nmf @xcite and ntf @xcite .",
    "lee and seung s multiplicative update method @xcite is the basis for most ntf algorithms that minimize the k - l divergence function .",
    "chi and kolda provide an improved multiplicative update scheme for k - l that addresses performance and convergence issues as elements approach zero @xcite ; we compare to their method in section  [ sec : exp ] . by interpreting the k - l divergence as an alternative csiszar - tusnady procedure , zafeiriou and petrou",
    "@xcite provide a probabilistic interpretation of ntf along with a new multiplicative update scheme .",
    "the multiplicative update is equivalent to a scaled steepest - descent step @xcite , so it is a first - order optimization method . since our method uses second - order information , it allows for convergence to higher accuracy and a better determination of sparsity in the factorization .",
    "second - order information has been used before in connection with the k - l objective .",
    "zdunek and cichocki  @xcite propose a hybrid method for blind source separation applications via nmf that uses a damped hessian method similar to ours .",
    "they recognize that the hessian of the k - l objective has a block diagonal structure but do not reformulate the optimization problem further as we do .",
    "consequently , their hessian matrix is large , and they switch to the ls objective function for the larger mode of the matrix because their newton method can not scale up . mixing objective functions in this manner is undesirable because it combines two different underlying models . as a point of comparison",
    ", a problem in  @xcite of size @xmath1 is considered too large for their newton method , but our algorithms can factor a data set of this size with @xmath2 components to high accuracy in less than ten minutes ( see the supplementary material ) . the hessian - based method in  @xcite has most of the advanced optimization features that we use ( though details differ ) , including an armijo line search , active set identification , and an adjustable hessian damping factor .",
    "we also note that zheng and zhang  @xcite compute a damped hessian search direction and find an iterate with a backtracking line search , though this work is for the ls objective in nmf .",
    "recently , hsiel and dhillon @xcite reported algorithms for nmf with both ls and k - l objectives .",
    "their method updates one variable at a time , solving a nonlinear scalar function using newton s method with a constant step size .",
    "they achieve good performance for the ls objective by taking the variables in a particular order based on gradient information ; however , for the more complex k - l objective , they must cycle through all the variables one by one .",
    "our algorithms solve convex row subproblems with @xmath0 variables using second - order information ; solving these subproblems one variable at a time by coordinate descent will likely have a much slower rate of convergence  @xcite .",
    "a row subproblem reformulation similar to ours is noted in earlier papers exploring the ls objective , but it never led to hessian - based methods that exploit sparsity as ours do .",
    "gonzales and zhang use the reformulation with a multiplicative update method for nmf  @xcite but do not generalize to tensors or the k - l objective .",
    "phan et al .",
    "@xcite note the reformulation is suitable for parallelizing a hessian - based method for ntf using ls .",
    "kim and park use the reformulation for ntf with ls  @xcite , deriving small bound - constrained ls subproblems .",
    "their method solves the ls subproblems by exact matrix factorization , without exploiting sparsity , and features a block principal pivoting method for choosing the active set .",
    "other works solve the ls objective by taking advantage of row - by - row or column - by - column subproblem decomposition @xcite .",
    "our algorithms are similar in spirit to the work of kim , sra and dhillon  @xcite , which applies a projected quasi - newton algorithm ( called pqn in this paper ) to solving nmf with a k - l objective . like pqn",
    ", our algorithms identify active variables , compute a newton - like direction in the space of free variables , and find a new iterate using a projected backtracking line search .",
    "we differ from pqn in reformulating the subproblem and in computing a damped newton direction ; both improvements make a huge difference in performance for large - scale tensor problems .",
    "we compare to pqn in section  [ sec : exp ] .",
    "all - at - once optimization methods , including hessian - based algorithms , have been applied to ntf with the ls objective function .",
    "as an example , paatero replaces the nonnegativity constraints with a barrier function  @xcite to yield an unconstrained optimization problem , and phan , tichavsky and cichocki  @xcite apply a fast damped gauss - newton algorithm for minimizing a similar penalized objective .",
    "we are not aware of any work on all - at - once methods for the k - l objective in ntf .",
    "finally , we note that all methods , including ours , find only a locally optimal solution to the ntf problem .",
    "finding the global solution is generally much harder ; for instance , vavasis  @xcite proves it is np - hard for an nmf model that fits the data exactly .      for a thorough introduction to tensors ,",
    "see  @xcite and references therein ; we only review concepts that are necessary for understanding this paper .",
    "a tensor is a multidimensional array .",
    "an @xmath3-way tensor @xmath4 has size @xmath5 .",
    "to differentiate between tensors , matrices , vectors , and scalars , we use the following notational convention : @xmath4 is a tensor ( bold , capitalized , calligraphic ) , @xmath6 is a matrix ( bold , capitalized ) , @xmath7 is a vector ( bold , lowercase ) , and @xmath8 is a scalar ( lowercase ) . additionally , given a matrix @xmath6",
    ", @xmath9 denotes its @xmath10th column and @xmath11 denotes its @xmath12th row .",
    "just as a matrix can be decomposed into a sum of outer products between two vectors , an @xmath3-way tensor can be decomposed into a sum of outer products between @xmath3 vectors .",
    "each of these outer products ( called components ) yields an @xmath3-way tensor of rank one .",
    "the cp ( candecomp / parafac ) decomposition  @xcite represents a tensor as a sum of rank - one tensors ( see figure  [ fig : cp ] ) : @xmath13 where @xmath14 is a vector and each @xmath15 is an @xmath16 _ factor matrix _ containing the @xmath0 vectors contributed to the outer products by mode @xmath17 , i.e. , @xmath18 .\\ ] ] equality holds in  ( [ kruskal ] ) when @xmath0 equals the rank of @xmath4 , but often a tensor is approximated by a smaller number of terms .",
    "we let @xmath19 denote the multi - index @xmath20 of an element @xmath21 of @xmath4 .",
    "components . ]",
    "we use of the idea of matricization , or unfolding a tensor into a matrix .",
    "specifically , unfolding along mode @xmath17 yields a matrix of size @xmath22 , where @xmath23 we use the notation @xmath24 to represent a tensor @xmath4 that has been unfolded so that its @xmath17th mode forms the rows of the matrix , and @xmath25 for its @xmath26 element . if a tensor @xmath4 is written in kruskal form ( [ kruskal ] ) , then the mode-@xmath17 matricization is given by @xmath27 where @xmath28 and @xmath29 denotes the khatri - rao product  @xcite .",
    "tensor results are generally easier to interpret when the factors ( [ factors ] ) are sparse .",
    "moreover , many sparse count applications can reasonably expect sparsity in the factors .",
    "for example , the 3-way data considered in  @xcite counts publications by authors at various conferences over a ten year period . the tensor representation has a sparsity of 0.14% ( only 0.14% of the data elements are nonzero ) , and the factors computed by our algorithm with @xmath30 ( see section  [ subsec : exp - fullprob ] ) have sparsity 9.3% , 2.7% , and 77.5% over the three modes .",
    "one meaning of sparsity in the factors is to say that a typical outer product term connects about 9% of the authors with 3% of the conferences in 8 of the 10 years .",
    "linking particular authors and conferences is an important outcome of the tensor analysis , requiring clear distinction between zero and nonzero elements in the factors .",
    "in this section we state the optimization problem , examine its structure , and show how to separate it into simpler subproblems .",
    "we seek a tensor model in cp form to approximate data @xmath4 : @xmath31 the value of @xmath0 is chosen empirically , and the scaling vector @xmath14 and factor matrices @xmath15 are the model parameters that we compute .    in @xcite , it is shown that a k - l divergence objective function results when data elements follow poisson distributions with multilinear parameters .",
    "the best - fitting tensor model under this assumption satisfies : @xmath32 where @xmath21 denotes element @xmath33 of tensor @xmath34 and @xmath35 denotes element @xmath33 of the model @xmath36 .",
    "the model may have terms where @xmath37 and @xmath38 ; for this case we define @xmath39 .",
    "note that for matrix factorization , ( [ fullprob ] ) reduces to the k - l divergence used by lee and seung @xcite .",
    "the constraint that normalizes the column sum of the factor matrices serves to remove an inherent scaling ambiguity in the cp factor model .    as in @xcite",
    ", we unfold @xmath4 and @xmath40 into their @xmath17th matricized mode , and use ( [ matricization ] ) to express the objective as @xmath41                     { { \\bm{\\mathbf{\\makelowercase{e } } } } } \\ ; , \\ ] ] where @xmath42 is a vector of all ones , the operator @xmath43 denotes elementwise multiplication , @xmath44 is taken elementwise , @xmath45 note that by expanding the khatri - rao products in  ( [ pimatrix ] ) and remembering that column vectors @xmath46 are normalized , each row of @xmath47 conveniently sums to one .",
    "this is a consequence of using the @xmath48 norm in ( [ fullprob ] ) .",
    "the above representation of the objective motivates the use of an alternating block optimization method where only one factor matrix is optimized at a time . holding the other factor matrices fixed ,",
    "the optimization problem for @xmath15 and @xmath49 is @xmath50 { { \\bm{\\mathbf{\\makelowercase{e } } } } }   \\\\      \\text{s.t . }",
    "\\quad        & { { \\bm{\\mathbf{\\makeuppercase{\\lambda } } } } } \\geq 0 , \\ ; { { { \\bm{\\mathbf{\\makeuppercase{a}}}}}^{(n ) } } \\geq 0 , \\ ; { { \\bm{\\mathbf{\\makelowercase{e}}}}}^t { { { \\bm{\\mathbf{\\makeuppercase{a}}}}}^{(n ) } } = { { \\bm{\\mathbf{\\makelowercase{1 } } } } } .",
    "\\end{split }    \\label{modenprobnonconvex}\\ ] ]    problem ( [ modenprobnonconvex ] ) is not convex .",
    "however , ignoring the equality constraint and letting @xmath51 , we have @xmath52                       { { \\bm{\\mathbf{\\makelowercase{e } } } } }   \\label{modenprob}\\ ] ] which is convex with respect to @xmath53 .",
    "the two formulations are equivalent in that a kkt point of ( [ modenprob ] ) can be used to find a kkt point of ( [ modenprobnonconvex ] ) .",
    "chi and kolda show in @xcite that ( [ modenprob ] ) is _ strictly _ convex given certain assumptions on the sparsity pattern of @xmath24 .",
    "we pause to think about ( [ modenprob ] ) when the tensor is two - way . in this case , we solve for two factor matrices by alternating over two block subproblems ; for instance , with @xmath54 the subproblem  ( [ modenprob ] ) finds @xmath55 with @xmath56 . for an @xmath3-way problem ,",
    "the only change to ( [ modenprob ] ) is @xmath57 , which grows in size exponentially with each additional factor matrix . to efficiently solve the subproblems ( [ modenprob ] ) for large sparse tensors we can exploit sparsity to reduce the computational costs . as discussed in the next section , columns of @xmath57 need to be computed only when the corresponding column in the unfolded tensor @xmath24 has a nonzero element .",
    "our row subproblem formulation carries this idea further because each row subproblem generally uses only a fraction of the nonzero elements in @xmath24 .    at this point",
    "we define algorithm  [ alg : outline ] , a gauss - seidel alternating block method .",
    "the algorithm iterates over each mode of the tensor , solving the convex optimization block subproblem .",
    "steps  [ algoutline : rescale1 ] and [ algoutline : rescale2 ] rescale the factor matrix columns , redistributing the weight into @xmath14 . for the moment , we leave the subproblem solution method in step  [ algoutline : subproblem ] unspecified . a proof that algorithm  [ alg : outline ] convergences to a local minimum of ( [ fullprob ] )",
    "is given in @xcite .",
    "given data tensor @xmath4 of size @xmath58 , and the number of components @xmath0 + return a model @xmath59 $ ]    initialize @xmath60 for @xmath61 [ algoutline : for ] let @xmath62 [ algoutline : formpi ] use algorithm  [ alg : subproblem - sparse ] to compute @xmath63 that minimizes @xmath64 s.t .",
    "@xmath65 [ algoutline : subproblem ] @xmath66 [ algoutline : rescale1 ] @xmath67 where @xmath68 [ algoutline : rescale2 ] [ algoutline : endfor ] all mode subproblems have converged    this outline of algorithm  [ alg : outline ] corresponds exactly with the method proposed in @xcite ; where we differ is in how to solve subproblem  ( [ modenprob ] ) in step  [ algoutline : subproblem ] .",
    "note also that this algorithm is the same as for the least squares objective ( references were given in section  [ subsec - relatedwork ] ) ; there the subproblem in step  [ algoutline : subproblem ] is replaced by a linear least squares subproblem .",
    "we now proceed to describe our method for solving  ( [ modenprob ] ) .",
    "we examine the objective function @xmath64 in ( [ modenprob ] ) and show that it can be reformulated into independent functions .",
    "as mentioned in the previous section , rows of @xmath47 sum to one if the columns of factor matrices are nonnegative and sum to one .",
    "when @xmath47 is formed at step  [ algoutline : formpi ] of algorithm  [ alg : outline ] , the factor matrices satisfy these conditions by virtue of steps  [ algoutline : rescale1 ] and [ algoutline : rescale2 ] ; hence , the first term of @xmath64 is @xmath69    the second term of @xmath64 is a sum of elements from the @xmath70 matrix @xmath71 . recall that the operations in this expression are elementwise , so the scalar matrix element @xmath26 of the term can be written as @xmath72 adding all the elements and combining with the first term gives @xmath73 where @xmath74 and @xmath75 are the @xmath12th row vectors of their corresponding matrices , and @xmath76 problem  ( [ modenprob ] ) can now be rewritten as @xmath77 this is a completely separable set of @xmath78 _ row subproblems _ , each one a convex nonlinear optimization problem containing @xmath0 variables .",
    "the relatively small number of variables makes second - order optimization methods tractable , and that is the direction we pursue in this paper .",
    "algorithm  [ alg : subproblem - sparse ] describes how the reformulation fits into algorithm  [ alg : outline ] .",
    "given @xmath24 of size @xmath70 , and @xmath57 of size @xmath79 + return a solution @xmath63 consisting of row vectors @xmath80    select row @xmath75 of @xmath24 generate one column of @xmath57 for each nonzero in @xmath75 [ algsubproblem : sparsepi ] use algorithm  [ alg : rowsubprobpdn ] or [ alg : rowsubprobpqn ] to compute @xmath81 that solves @xmath82 [ algsubproblem : rowsubproblem ]    the independence of row subproblems is crucial for handling large tensors . for example , if a three - way tensor of size @xmath83 is factored into @xmath84 components , then @xmath57 is a @xmath85 matrix .",
    "however , elements of @xmath57 appear in the optimization objective only where the matricized tensor @xmath24 has nonzero elements , so in a sparse tensor many columns of @xmath57 can be ignored ; this point was first published in @xcite .",
    "algorithm  [ alg : subproblem - sparse ] exploits this fact in step  [ algsubproblem : sparsepi ] .",
    "algorithm  [ alg : subproblem - sparse ] also points the way to a parallel implementation of the cp tensor factorization .",
    "we note , as did @xcite , that each row subproblem can be run in parallel and storage costs are determined by the sparsity of the data . in a distributed computing architecture",
    ", an algorithm could identify the nonzero elements of each row subproblem at the beginning of execution and collect only the data needed to form appropriate columns of @xmath57 at a given processing element .",
    "we do not implement a parallel version of the algorithm in this paper .",
    "in this section we show how to solve the row subproblem  ( [ rsp - ptf ] ) using second - order information .",
    "we describe two algorithms , one applying second derivatives in the form of a damped hessian matrix , and the other using a quasi - newton approximation of the hessian . both algorithms use projection , but the details differ .",
    "each row subproblem consists of minimizing a strictly convex function of @xmath0 variables with nonnegativity constraints .",
    "one of the most effective methods for solving bound - constrained problems is second - order gradient projection ; see  @xcite .",
    "we employ a form of two - metric gradient projection from bertsekas  @xcite .",
    "each variable is marked in one of three states based on its gradient and current location : fixed at its bound of zero , allowed to move in the direction of steepest - descent , or free to move along a newton or quasi - newton search direction .",
    "details are in section  [ subsec : subprobactiveset ] .",
    "an alternative to bertsekas is to use methods that employ gradient projection searches to determine the active variables ( those set to zero ) .",
    "examples include the generalized cauchy point  @xcite and gradient projection along the steepest - descent direction with a line search .",
    "we experimented with using the generalized cauchy point to determine the active variables , but preliminary results indicated that this approach sets too many variables to be at their bound , leading to more iterations and poor overall performance .",
    "gradient projection steps with a line search calls for an extra function evaluation , which is computationally expensive . given a more efficient method for evaluating the function ,",
    "this might be a better approach since , under mild conditions , gradient projection methods find the active set in a finite number of iterations  @xcite .    for notational convenience ,",
    "in this section we use @xmath86 for the column vector representation of row vector @xmath74 ; that is , @xmath87 .",
    "iterations are denoted with superscript @xmath88 , and @xmath89 represents the derivative with respect to the @xmath90th variable .",
    "let @xmath91 $ ] be the projection operator that restricts each element of vector @xmath92 to be nonnegative .",
    "we make use of the first and second derivatives of @xmath93 , given by @xmath94      at each iteration @xmath88 we must choose a set of variables to update such that progress is made in decreasing the objective .",
    "bertsekas demonstrated in  @xcite that iterative updates of the form @xmath95\\ ] ] are not guaranteed to decrease the objective function unless @xmath96 is a positive diagonal matrix .",
    "instead , it is necessary to predict the variables that have the potential to make progress in decreasing the objective and then update just those variables using a positive definite matrix .",
    "we present the two - metric technique of bertsekas as it is executed in our algorithm , which differs superficially from the presentation in  @xcite .",
    "a variable s potential effect on the objective is determined by how close it is to zero and by its direction of steepest - descent .",
    "if a variable is close to zero and its steepest - descent direction points towards the negative orthant , then the next update will likely project the variable to zero and its small displacement will have little effect on the objective . a closeness threshold @xmath97 is computed from a user - defined parameter @xmath98 as @xmath99 \\right\\|_2 .",
    "\\label{activesetepsilon}\\ ] ] we then define index sets @xmath100 where superscript @xmath101 denotes the set complement .",
    "variables in the set @xmath102 are fixed at zero , variables in @xmath103 move in the direction of the negative gradient , and variables in @xmath104 are free to move according to second - order information .",
    "note that if @xmath105 then @xmath106 , @xmath103 is empty , and the method reduces to defining an active set of variables by instantaneous line search  @xcite .      the damped newton direction is taken with respect to only the variables in the set @xmath104 from ( [ activesetdef ] ) .",
    "let @xmath107_{\\mathcal{f } } , \\    { { \\bm{\\mathbf{\\makeuppercase{h}}}}}^k_f = [ \\nabla^2 f_{\\rm row}({{\\bm{\\mathbf{\\makelowercase{b}}}}}^k)]_{\\mathcal{f } } , \\    { { \\bm{\\mathbf{\\makelowercase{b}}}}}^k_f = [ { { \\bm{\\mathbf{\\makelowercase{b}}}}}^k]_{\\mathcal{f } } , \\ ] ] where @xmath108_{\\mathcal{f}}$ ] chooses the elements of vector @xmath92 corresponding to variables in the set @xmath104 .",
    "since the row subproblems are strictly convex , the full hessian and @xmath109 are positive definite .",
    "the damped hessian has its roots in trust region methods . at every iteration we form a quadratic approximation @xmath110 of the objective plus a quadratic penalty .",
    "the penalty serves to ensure that the next iterate does not move too far away from the current iterate , which is important when the hessian is ill conditioned .",
    "the quadratic model plus penalty expanded about @xmath111 for variables @xmath112 is @xmath113 the unique minimum of @xmath114 is @xmath115 where @xmath116 is known as the damped hessian . adding a multiple of the identity to @xmath109 increases each of its eigenvalues by @xmath117 , which has the effect of diminishing the length of @xmath118 , similar to the action of a trust region .",
    "the step @xmath118 is computed using a cholesky factorization of the damped hessian , and the full space search direction @xmath119 is then given by @xmath120 where index sets @xmath102 , @xmath103 , and @xmath104 are defined in ( [ activesetdef ] ) , and @xmath121 is an @xmath122 matrix that elongates the vector @xmath123 to length @xmath0 .",
    "specifically , @xmath124 if @xmath12 is the row subproblem variable corresponding to the @xmath10th variable in @xmath104 , and zero otherwise .",
    "the damping parameter @xmath117 is adjusted by a levenberg - marquardt strategy  @xcite .",
    "first define the ratio of actual reduction over predicted reduction , @xmath125 where @xmath114 is defined by ( [ quadmodel ] ) . note the numerator of ( [ rho ] ) calculates @xmath93 using all variables , while the denominator calculates @xmath114 using only the variable in @xmath104 .",
    "the damping parameter is updated by the following rule @xmath126 since @xmath123 is the minimum of ( [ quadmodel ] ) , the denominator of ( [ rho ] ) is always negative .",
    "if the search direction @xmath127 increases the objective function , then the numerator of ( [ rho ] ) will be positive ; hence @xmath128 and the damping parameter will be increased for the next iteration . on the other hand , if the search direction @xmath127 decreases the objective function , then the numerator will be negative ; hence @xmath129 and the relative sizes of the actual reduction and predicted reduction will determine how the damping parameter is adjusted .",
    "after computing the search direction @xmath127 , we ensure the next iterate decreases the objective by using a projected backtracking line search that satisfies the armijo condition  @xcite .",
    "given scalars @xmath130 and @xmath131 , we find the smallest nonnegative integer @xmath132 that satisfies the inequality @xmath133 ) - f_{\\rm row}({{\\bm{\\mathbf{\\makelowercase{b}}}}}^k )    \\leq    \\sigma ( p_{+}[{{\\bm{\\mathbf{\\makelowercase{b}}}}}^k + \\beta^t { { \\bm{\\mathbf{\\makelowercase{d}}}}}^k]-{{\\bm{\\mathbf{\\makelowercase{b}}}}}^k)^t           \\nabla f_{\\rm row}({{\\bm{\\mathbf{\\makelowercase{b}}}}}^k ) .",
    "\\label{armijo}\\ ] ] we set @xmath134 and the next iterate is given by @xmath135 .\\ ] ]      as an alternative to the damped hessian step , we adapt the projected quasi - newton step from  @xcite .",
    "their work employs a limited - memory bfgs ( l - bfgs ) approximation  @xcite in a framework suitable for any convex , bound - constrained problem .",
    "l - bfgs estimates hessian properties based on the most recent @xmath136 update pairs @xmath137 , @xmath138 $ ] , where @xmath139 l - bfgs uses a two - loop recursion through the stored pairs to efficiently compute a vector @xmath140 , where @xmath141 approximates the inverse of the hessian @xmath142^{-1}$ ] using the pairs @xmath137 .",
    "storage is set to @xmath143 pairs in all experiments .",
    "on the first iterate when @xmath144 , we use a multiple of the identity matrix so that @xmath145 is in the direction of the gradient .",
    "l - bfgs updates require the quantity @xmath146 to be positive .",
    "we check this condition and skip the update pair if it is violated .",
    "this can happen if all row variables are at their bound of zero , or from numerical roundoff at a point near a minimizer .",
    "see  ( * ? ? ?",
    "* chapter 7 ) for further detail .    the projected quasi - newton search direction @xmath127 , analogous to ( [ finaldirection ] ) , is @xmath147 where @xmath104 and @xmath102 are determined from ( [ activesetdef ] ) , and @xmath121 is the elongation matrix defined in ( [ finaldirection ] ) .    the step @xmath148 is computed from an l - bfgs approximation over all variables in the row subproblem ; in contrast , the step @xmath123 computed from the damped hessian in section  [ subsec : dampednewton ] is derived from the second derivatives of only the free variables in @xmath104 .",
    "we could build an l - bfgs model over just the free variables as is done in  @xcite , but the computational cost is higher .",
    "our l - bfgs step is therefore influenced by second - order information from variables not in @xmath104 .",
    "this information is irrelevant to the step , but we find that algorithm performance is still good .",
    "we now express the influence in terms of the reduced hessian and inverse of the reduced hessian .",
    "let @xmath149 and @xmath150 denote the true hessian and inverse hessian matrices over all variables in a row subproblem .",
    "suppose the variables in @xmath104 are the first @xmath151 variables , and the remaining variables are in @xmath152",
    ". then we can write @xmath149 in block form as @xmath153 , \\ ] ] with @xmath154 , @xmath155 , and @xmath156 .",
    "the damped hessian search direction in  ( [ finaldirection ] ) is computed from the inverse of the reduced hessian ; that is , @xmath157 .",
    "let @xmath158 and @xmath159 denote the l - bfgs approximation to the true and inverse hessian . to obtain the step @xmath148 we use the inverse approximation @xmath159 , then extract just the free variables for use in ( [ lbfgs_final ] ) ; hence , we compute the search direction using the approximation @xmath160 .",
    "assuming the schur complement exists , this matrix is @xmath161 comparing with the true reduced hessian , we see the extra term @xmath162 , a matrix of rank @xmath163 .",
    "this is the influence in the l - bfgs approximation of variables not in @xmath104 ; we are effectively using the l - bfgs approximation of the reduced inverse hessian to compute the step .",
    "note that a small value of the tuning parameter @xmath164 in  ( [ activesetepsilon ] ) can help reduce the size of @xmath163 , lessening the influence .",
    "since the row subproblems are convex , any point satisfying the first - order kkt conditions is the optimal solution .",
    "specifically , @xmath165 is a kkt point of ( [ rsp - ptf ] ) if it satisfies @xmath166 where @xmath167 is the vector of dual variables associated with the nonnegativity constraints .",
    "knowing the algorithm keeps all iterates @xmath111 nonnegative , we can express the kkt condition for component @xmath90 as @xmath168 a suitable stopping criterion is to approximately satisfy the kkt conditions to a tolerance @xmath169 . we achieve this by requiring that all row subproblems satisfy @xmath170    the full algorithm solves to an overall tolerance @xmath171 when the @xmath172 of every row subproblem satisfies ( [ kktviolcond ] ) . this condition is enforced for all the row subproblems ( step  [ algsubproblem : rowsubproblem ] of algorithm  [ alg : subproblem - sparse ] ) generated from all the tensor modes ( step  [ algoutline : subproblem ] of algorithm  [ alg : outline ] ) .",
    "note that enforcement requires examination of @xmath172 for all row subproblems whenever the solution of any subproblem mode is updated , because the solution modifies the @xmath57 matrices of other modes .      having described the ingredients , we pull everything together into complete algorithms for solving the row subproblem in step  [ algsubproblem : rowsubproblem ] of algorithm  [ alg : subproblem - sparse ] .",
    "we present two methods in algorithm  [ alg : rowsubprobpdn ] and algorithm  [ alg : rowsubprobpqn ] : pdn - r uses a damped hessian matrix , and pqn - r uses a quasi - newton hessian approximation ( the ` -r ' designates a row subproblem formulation ) . both algorithms employ a two - metric projection framework for handling bound constraints and a line search satisfying the armijo condition .    given data @xmath173 and @xmath174 , constants @xmath175 , @xmath176 , @xmath177 , @xmath178 , stop tolerance @xmath171 , and initial values @xmath179 + return a solution @xmath165 to step  [ algsubproblem : rowsubproblem ] of algorithm  [ alg : subproblem - sparse ]    compute the gradient , @xmath180 , using @xmath173 and @xmath174 in ( [ eq : grad_frow ] )",
    "compute the first - order kkt violation @xmath181 @xmath182 find the indices of free variables from ( [ activesetdef ] ) with @xmath183 in ( [ activesetepsilon ] ) [ algpdn : activeset ] calculate the hessian for free variables @xmath184_{\\mathcal{f}}\\ ] ] compute the damped newton direction @xmath185 [ algpdn : searchdir ] construct search direction @xmath127 over all variables using @xmath186 and @xmath187 in ( [ finaldirection ] ) perform the projected line search ( [ armijo ] ) using @xmath176 and @xmath177 to find step length @xmath188 [ algpdn : linesearch ] update the current iterate @xmath189\\ ] ] update the damping parameter @xmath190 according to ( [ rho])-([updatedamping ] ) @xmath182    given data @xmath173 and @xmath174 , constants @xmath175 , @xmath176 , @xmath177 , @xmath178 , stop tolerance @xmath171 , and initial values @xmath179 + return a solution @xmath165 to step  [ algsubproblem : rowsubproblem ] of algorithm  [ alg : subproblem - sparse ]    compute the gradient , @xmath180 , using @xmath173 and @xmath174 in ( [ eq : grad_frow ] )    compute the first - order kkt violation @xmath181 @xmath182 find the indices of free variables from ( [ activesetdef ] ) with @xmath191 in ( [ activesetepsilon ] ) [ algpqn : activeset ] construct search direction @xmath127 using @xmath187 in ( [ lbfgs_final ] ) perform the projected line search ( [ armijo ] ) using @xmath176 and @xmath177 to find step length @xmath188 [ algpqn : linesearch ] update the current iterate @xmath189\\ ] ] update the l - bfgs approximation with @xmath192 and @xmath193 [ algpqn : lbfgs ] @xmath182    as mentioned , pqn - r is related to  @xcite . specifically , we note    1 .",
    "the free variables chosen in step  [ algpqn : activeset ] of pqn - r are found with @xmath191 in ( [ activesetepsilon ] ) , while  @xcite effectively uses @xmath105 for an instantaneous line search . as noted in section",
    "[ subsec : subprobactiveset ] , convergence is guaranteed when @xmath98 .",
    "step  [ algpdn : activeset ] of pdn - r uses the default value @xmath183 because we find it generally leads to faster convergence .",
    "2 .   the line search in step  [ algpqn : linesearch ] of pqn - r and step  [ algpdn : linesearch ] of pdn - r satisfies the armijo condition .",
    "this differs from  @xcite , which used @xmath194 on the right - hand side of ( [ armijo ] ) .",
    "we use ( [ armijo ] ) because it correctly measures predicted progress .",
    "in particular , it is easier to satisfy when @xmath195 is large and many variables hit their bound for small @xmath196 .",
    "3 .   updates to the l - bfgs approximation in step  [ algpqn : lbfgs ] of pqn - r are unchanged from  @xcite .",
    "information is included from all row subproblem variables , whether active or free .",
    "we express the computational cost of pdn - r and pqn - r in terms of the cost per iteration of algorithm  [ alg : outline ] ; that is , the cost of executing steps  [ algoutline : for ] through [ algoutline : endfor ] .",
    "the matrix @xmath47 is formed for the row subproblems of every mode , with the cost for each mode proportional to the number of nonzeros in the data tensor , nnz@xmath197 .",
    "this should dominate the cost of reweighting factor matrices in steps  [ algoutline : rescale1 ] and [ algoutline : rescale2 ] .",
    "the @xmath17th mode solves @xmath78 convex row subproblems , each with @xmath0 unknowns , using algorithm  [ alg : rowsubprobpdn ] ( pdn - r ) or algorithm  [ alg : rowsubprobpqn ] ( pqn - r ) .",
    "row subproblems execute over at most @xmath178 inner iterations . near a local minimum we expect pdn - r to take fewer inner iterations than pqn - r because the damped newton method converges asymptotically at a quadratic rate , while l - bfgs convergence is at best r - linear .",
    "however , the cost estimate will assume the worst case of @xmath178 iterations for all row subproblems .",
    "the dominant cost of algorithm  [ alg : rowsubprobpdn ] is solution of the damped newton direction in step  [ algpdn : searchdir ] , which costs @xmath198 operations to solve the @xmath199 dense linear system .",
    "hence , the cost per iteration of pdn - r is @xmath200 the dominant costs of algorithm  [ alg : rowsubprobpqn ] are computation of the search direction and updating the l - bfgs matrix , both @xmath201 operations .",
    "hence , the cost per iteration of pqn - r is @xmath202",
    "this section characterizes the performance of our algorithms , comparing them with multiplicative update @xcite and second - order methods that do not use the row subproblem formulation .",
    "all algorithms fit in the alternating block framework of algorithm  [ alg : outline ] , differing in how they solve  ( [ modenprob ] ) in step  [ algoutline : subproblem ] .",
    "our two algorithms are the projected damped hessian method ( pdn - r ) and the projected quasi - newton method ( pqn - r ) , from algorithms  [ alg : rowsubprobpdn ] and  [ alg : rowsubprobpqn ] , respectively . recall that ` -r ' means the row subproblem formulation is applied . in this paper",
    "we do not tune the algorithms to each test case , but instead chose a single set of parameter values : @xmath203 , @xmath204 , and @xmath205 . the bound constraint threshold in pdn - r from  ( [ activesetepsilon ] )",
    "was set to @xmath183 for pdn - r and @xmath191 for pqn - r , values that are observed to give best algorithm performance .",
    "the l - bfgs approximations in pqn - r stored the @xmath143 most recent update pairs ( [ curvpairs ] ) .",
    "the multiplicative update ( mu ) algorithm that we compare with is that of chi and kolda @xcite , available as function cp_apr in the matlab tensor toolbox @xcite .",
    "it builds on tensor generalizations of the lee and seung method , specifically treating _ inadmissible zeros _ ( their term for factor elements that are active but close to zero ) to improve the convergence rate .",
    "algorithm mu can be tuned by selecting the number of inner iterations for approximately solving the subproblem at step  [ algoutline : subproblem ] of algorithm  [ alg : outline ] .",
    "we found that ten inner iterations worked well in all experiments .",
    "we also compare to a projected quasi - newton ( pqn ) algorithm adopted from kim et al .",
    "pqn is similar to pqn - r but solves  ( [ modenprob ] ) without reformulating the block subproblem into row subproblems .",
    "pqn identifies the active set using @xmath105 in ( [ activesetepsilon ] ) and maintains a limited - memory bfgs approximation of the hessian .",
    "however , pqn uses one l - bfgs matrix for the entire subproblem , storing the three most recent update pairs .",
    "we used matlab code from the authors of @xcite , embedding it in the alternating framework of algorithm  [ alg : outline ] , with the modifications described in section  [ subsec : pqnmods ] .    additionally , we compare pdn - r to a projected damped hessian ( pdn ) method that uses one matrix for the block subproblem instead of a matrix for every row subproblem .",
    "pdn exploits the block diagonal nature of the hessian to construct a search direction for the same computational cost as pdn - r ; i.e. , one search direction of pdn takes the same effort as computing one search direction for all row subproblems in pdn - r .",
    "similar remarks apply to computation of the objective function for the subproblem ( [ modenprob ] )",
    ". however , pdn applies a single damping parameter @xmath117 to the block subproblem hessian and updates all variables in the block subproblem from a single line search along the search direction .    all algorithms were coded in matlab using the sparse tensor objects of the tensor toolbox @xcite .",
    "all experiments were performed on a linux workstation with 12 gb memory .",
    "data sets were large enough to be demanding but small enough to fit in machine memory ; hence , performance results are not biased by disk access issues .      1 .",
    "the row subproblem formulation is better suited to second - order methods than the block subproblem formulation because it controls the number of iterations for each row subproblem independently , and because its convergence is more robust .",
    "pdn - r and pqn - r are faster than the other algorithms in terms of in reducing the @xmath172 , especially when solving to high accuracy .",
    "this holds for any number of components .",
    "pqn - r becomes faster than pdn - r as the number of components increases .",
    "3 .   pdn - r and pqn - r reach good solutions with high sparsity more quickly than the other algorithms , a desirable feature when the factor matrices are expected to be sparse .    in section  [ subsec : exp - subprob ]",
    "we report only performance in solving a single block subproblem  ( [ modenprob ] ) since the time is representative of the total time it will take to solve the full tensor factorization problem  ( [ fullprob ] ) . in section  [ subsec : exp - fullprob ] we report results from solving the full problem within the alternating block framework ( algorithm 1 ) .",
    "we begin by examining algorithm performance on the convex subproblem  ( [ modenprob ] ) of the alternating block framework . here",
    "we look at a single representative subproblem . our goal is to characterize the relative behavior of algorithms on the representative block subproblem .",
    "appendix  [ app - generate ] describes our method for generating synthetic test problems with reasonable sparsity .",
    "we investigate a three - way tensor of size @xmath206 , generating @xmath207 data samples .",
    "the number of components , @xmath0 , is varied over the set @xmath208 . for each value of @xmath0 , the procedure generates a sparse multilinear model @xmath209 and data tensor @xmath4 .",
    "table  [ tbl : subproblem - sizes ] lists the number of nonzero elements found in the data tensor @xmath4 that results from @xmath210 data samples , averaged over ten random seeds .",
    "the number of nonzeros , a key determiner of algorithm cost in equations  ( [ cost - pdnr ] ) and ( [ cost - pqnr ] ) , is approximately the same for all values of @xmath0 ."
  ],
  "abstract_text": [
    "<S> tensor factorizations with nonnegative constraints have found application in analyzing data from cyber traffic , social networks , and other areas . </S>",
    "<S> we consider application data best described as being generated by a poisson process ( e.g. , count data ) , which leads to sparse tensors that can be modeled by sparse factor matrices . in this paper </S>",
    "<S> we investigate efficient techniques for computing an appropriate canonical polyadic tensor factorization based on the kullback - leibler divergence function . </S>",
    "<S> we propose novel subproblem solvers within the standard alternating block variable approach . </S>",
    "<S> our new methods exploit structure and reformulate the optimization problem as small independent subproblems . </S>",
    "<S> we employ bound - constrained newton and quasi - newton methods . </S>",
    "<S> we compare our algorithms against other codes , demonstrating superior speed for high accuracy results and the ability to quickly find sparse solutions . </S>"
  ]
}