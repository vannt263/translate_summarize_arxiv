{
  "article_text": [
    "the term `` semi - supervised learning '' was coined in the machine learning literature to describe a situation in which some of the data is labeled while the rest of the data is unlabeled @xcite .",
    "such situations occur when the label variable is difficult to observe and may require a complicated or expensive procedure .",
    "a typical example is web document classification , where the classification is done by a human agent while there are many more unlabeled on - line documents .",
    "specifically , a sample of @xmath6 observations from the joint distribution of @xmath7 is given , where @xmath0 is a one - dimensional _ label _ variable and @xmath1 is a p - dimensional vector of _ covariates _ or _",
    "predictors_. also , an additional sample of size @xmath8 is observed where only @xmath1 is given .",
    "the purpose is to study procedures that make use of the additional unlabeled data to better capture the shape of the underlying joint distribution of the labeled data .",
    "a large body of literature focuses on the case that @xmath0 takes a small number of values and the problem reduces to a classification task ; see e.g. , @xcite and references therein .",
    "@xcite divide the different methods into two approaches : distributional and margin - based .",
    "the distributional approach relies on an assumption relating the conditional expectation @xmath2 $ ] to the marginal distribution of @xmath1 and the margin - based approach uses the extra information on @xmath9 for estimating the bayes decision boundary ; see @xcite for a bayesian perspective within this basic approach .",
    "other works consider continuous @xmath0 s and use the unlabeled data to learn the structure of the @xmath1 s in order to better estimate a non - parametric regression @xcite .",
    "these works could be very helpful in situations where non - parametric regression is useful and unlabeled data are available .",
    "here we follow a third methodology .",
    "this methodology makes use of possible unmodeled non - linearities in @xmath10 and adapts information from the unlabeled data to partially adjust for these . in a recent manuscript ,",
    "@xcite follow the same methodological plan but the technical details within their approach are then quite different from ours .",
    "their estimates offer some advantages and some disadvantages in relation to ours , which we intend to explore in a future study .",
    "we aim at estimating the vector @xmath11 composed of the parameters of the best linear predictor of @xmath0 , but we do not necessarily assume that @xmath2 $ ] is exactly linear in @xmath1 .",
    "the estimates of @xmath11 can then be used to provide predictions",
    ". the standard approach in the statistical literature may be best summarized by the following quote from @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the related problem of missing values in the outcome @xmath0 was prominent in the early history of missing - data methods , but is less interesting in the following sense : if the @xmath9 s are complete and the missing values of @xmath0 are missing at random , then the incomplete cases contribute no information to the regression of @xmath0 on @xmath12 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    but see ( * ? ? ?",
    "* chapter 7 ) for a different view more closely in tune with our development .",
    "@xcite show that @xmath11 does not depend on the distribution of @xmath1 if and only if @xmath2 $ ] is linear in @xmath1 . when the conditional expectation is linear , there is typically at most a limited amount of additional information in the unlabeled data .",
    "@xcite shows that even in this case there may be some useful information , but it will not provide an asymptotic advantage in the manner we suggest in the current treatment when the conditional expectation is not linear .",
    "the assumption that @xmath2 $ ] is linear is unrealistic in many situations , and we show that in the absence of such an assumption the unlabeled sample can be used to provide useful information for the estimation of @xmath11 .",
    "we consider two scenarios . in the first one the distribution of @xmath1",
    "is known exactly .",
    "this is equivalent to having infinitely many unlabeled observations ( @xmath13 ) . in the second ,",
    "more frequently encountered , scenario @xmath14 but it is still significant in the sense that @xmath15 is bounded away from 0 .",
    "we call these two scenarios total and partial information , respectively . in both situations",
    "we provide an asymptotically better estimate of @xmath11 than the standard least squares estimator , and hence also a better linear predictor .",
    "our new procedure is closely related to the semi - supervised estimations of means , which is the topic of a recent manuscript @xcite .    to understand our basic methodology ,",
    "consider the simple linear regression model , @xmath16 where @xmath17 , @xmath18 are least squares coefficients and @xmath19 is a remainder term ; these will be defined in more detail shortly .",
    "when @xmath20 and @xmath21 are known , we construct an estimator of @xmath18 that asymptotically dominates the least squares estimator .",
    "this is done by replacing model with a different model where @xmath18 is the intercept and also the expectation of the newly defined label variable . for this latter model ,",
    "@xcite show that the intercept estimator dominates the simple empirical estimate in a semi - supervised setting .",
    "we use their ideas to show that the intercept estimator in this new model is better than the least squares estimator from the original model as explained below .    to present the intercept model and the methodology , begin by stating the general form of the best linear approximation .",
    "the approximation is used in two different models , and therefore we introduce now the notation @xmath22 and @xmath23 instead of @xmath1 and @xmath0 .",
    "suppose that @xmath24 and @xmath25 are random variables with joint distribution @xmath26 and finite second moments .",
    "the best linear predictor is @xmath27 notice that this is a population version of the least squares where @xmath28 minimizes the @xmath29 distance in the population from @xmath23 to any linear function of @xmath22 .",
    "it follows that @xmath30 where the remainder term @xmath31 is orthogonal to @xmath22 , i.e. , @xmath32 .",
    "given a sample of @xmath6 observations from @xmath26 , the standard least squares estimate , @xmath33 , satisfies asymptotically @xcite @xmath34 unlike the standard fixed @xmath1 assumption , the asymptotic variance has a `` sandwich '' form , @xmath35 forming the `` bread '' and @xmath36 the `` meat '' . see @xcite for further discussion of this form of the sandwich .",
    "we now return to model and assume that @xmath20 and @xmath21 are known .",
    "we can therefore assume w.l.o.g that @xmath9 is standardized ( i.e. , @xmath37 and @xmath38 ) .",
    "in this case we can write model with @xmath39 , @xmath40 and @xmath41 ; here we consider model with @xmath42 , @xmath43 , @xmath44 and the remainder term is @xmath45 .",
    "the standard lse is a consistent estimate for @xmath18 and satisfies , according to , @xmath46    our aim it to better estimate @xmath40 . to this end , we multiply by @xmath9 setting @xmath47 to be the labeled variable @xmath23 .",
    "furthermore , we also center the varibale @xmath22 setting @xmath48 to be the intercept . specifically , @xmath49 now we consider model with @xmath50 , @xmath43 , @xmath44 , @xmath51 and @xmath52 ; here @xmath53 are @xmath54 defined by . in setting @xmath40 to be the intercept coefficient we used that @xmath55 , i.e. , that we know the first and second moment of @xmath9",
    ". we define our new estimator , @xmath56 ( ti for total information ) , to be the intercept estimator of .",
    "this estimate has a simple closed formula , which is given below in .",
    "the sandwich theorem implies that @xmath57    since @xmath58 is a linear function of @xmath59 , then it is orthogonal to the remainder term of , which is @xmath60 , ( i.e. , @xmath61 ) and therefore , @xmath62 this implies that the asymptotic variance of @xmath56 is smaller than that of @xmath63 with equality iff @xmath64 .",
    "the latter occurs iff @xmath65 and @xmath66 or equivalently that @xmath19 is uncorrelated with both @xmath67 and @xmath68 .",
    "this occurs when the non - linear part of @xmath69 $ ] ( if exists ) is uncorrelated with @xmath67 and @xmath68 . in this case ,",
    "models and are essentially the same model and nothing is gained in the new methodology . on the other hand , when @xmath19 is correlated with either @xmath67 or @xmath68 , then models and are different and @xmath56 has smaller asymptotic variance than @xmath63 .",
    "we further show that a similar decomposition to holds for the partial information case and we generalize the results to the @xmath70-dimensional case .",
    "the rest of the paper is organized as follows .",
    "section [ sec : prel ] provides the basic setting the and loss functions that we use .",
    "the main results are given in section [ sec : main ] .",
    "section [ sec : sim ] describes an extensive simulation study and section [ sec : est_se ] discusses estimation of the asymptotic covariance matrix of the estimates .",
    "an implementation of the new methodology to infer homeless population in los angeles is discussed in section [ sec : inferring ] .",
    "section [ sec : dis ] concludes with final remarks .",
    "the proofs are given in section [ sec : proofs ] .",
    "consider a sample of @xmath6 i.i.d observations @xmath71 from a joint distribution @xmath26 , where @xmath72 , @xmath73 , and an additional set of @xmath8 independent observations @xmath74 from the marginal distribution of @xmath1 .",
    "we use super - index to denote the number of the observation , and sub - index to denote coordinates of @xmath1 .",
    "the notation @xmath75 without super - index denotes a random vector whose distribution is @xmath26 .",
    "we write @xmath76 to be a vector @xmath1 with an additional constant 1 to accommodate an intercept term .",
    "assume that the second moments of @xmath26 exists and that the matrix @xmath77 is invertible .",
    "then , we can define @xmath78    in the presence of non - linearity , @xmath11 is still a meaningful parameter that describes the overall association between @xmath0 and @xmath1 @xcite .",
    "we have in mind two related purposes .",
    "the first purpose is just to better estimate the parameters of interest , while the second purpose pertains to prediction .",
    "the latter is formalized now .",
    "suppose that an independent observation @xmath79 is given .",
    "the optimal linear predictor is @xmath80 .",
    "we consider the excess loss of an estimator @xmath81 @xmath82 we have that ( see lemma [ lem1 ] ) @xmath83 ^ 2 +   e\\left\\ { ( { \\boldsymbol \\beta}- \\tilde{\\boldsymbol \\beta})^t { \\bf m } ( { \\boldsymbol \\beta}- \\tilde{\\boldsymbol \\beta } ) \\right \\}= e \\tilde{l}(\\tilde{\\alpha},\\tilde{\\boldsymbol \\beta}),\\ ] ] where @xmath84 is the expression inside the expectation and @xmath85 is the covariance matrix of @xmath1 .",
    "we further define the asymptotic risk as @xmath86 the loss is of order @xmath87 and therefore we consider expectation of @xmath88 .",
    "this is truncated by an arbitrarily large number @xmath89 , since when the loss is large , it makes sense not to penalize any further .",
    "also , the truncation helps to avoid issues of uniform integrability .",
    "this is done for example in @xcite , chapter 5 .",
    "the following proposition provides a simple expression for the asymptotic risk .",
    "[ prop1 ] let @xmath81 satisfies @xmath90 and assume that @xmath91 satisfies @xmath92 then , @xmath93 ^ 2 + { \\rm trace } ( { \\bf m } { \\boldsymbol \\sigma}),\\ ] ] where @xmath85 is the covariance matrix of @xmath1 .",
    "the first term in does not depend on the distribution of @xmath91 .",
    "hence , proposition [ prop1 ] shows that the excess risk is minimized when @xmath94 is small .",
    "thus , we aim at estimators @xmath91 with asymptotic distribution @xmath95 such that @xmath96 is `` smaller '' than the covariance matrix of lse , in the sense that the difference is positive semidefinite .",
    "such an estimator asymptotically better estimates @xmath11 and also has smaller asymptotic excess risk .",
    "for two estimates @xmath97 and @xmath98 , proposition [ prop1 ] implies that @xmath99 and therefore the difference of the prediction errors is @xmath100 it follows that the difference of the errors of the prediction of the mean is also @xmath101    notice that we estimate the intercept , as in , using @xmath102 rather than the possibly known expectations @xmath103 .",
    "this is in accordance with the findings of @xcite .",
    "in this section we provide the main theoretical results of the paper . before we deal with the p - dimensional case",
    ", we first introduce the results for one - dimensional @xmath1 .",
    "the reason is twofold : first , the presentation in the one - dimensional case is simpler and captures the main ideas , and second , our results for the p - dimensional @xmath1 are obtained by reducing the regression problem to p problems , each of which is closely related to the one dimensional @xmath1 regression .      in this section",
    "we study the one dimensional case .",
    "summary of the notation used here is presented in table [ tab : not1 ] .",
    "when @xmath105 has finite second moments we can write @xmath106 where @xmath107 , @xmath108 and @xmath109 .",
    "equation is the best linear approximation in the population in the sense that @xmath110 minimizes @xmath111 over all @xmath112 and @xmath19 is orthogonal to @xmath113 , i.e. , @xmath114 .",
    "the regular lse is @xmath115 this is a consistent and asymptotically unbiased and asymptotically normal estimator for @xmath18 .",
    "see @xcite for contemporary discussion .",
    "[ ht ! ]",
    ".summary of the notation used in section [ sec : one_dim ] . [ cols=\"^,^,^\",options=\"header \" , ]     [ tab : dataex ]",
    "in this work we showed that @xmath11 can be better estimated when additional information on the predictors @xmath1 is available .",
    "the key idea is to replace the regression model @xmath116 with @xmath70 regressions of the form @xmath117 for @xmath118 , where @xmath119 is defined in .",
    "regression can be used only when the first and second moments of @xmath1 are known .",
    "we showed that the intercept estimator of is a better estimate of @xmath120 , in terms of smaller asymptotic variance , than the standrd lse of .",
    "furthermore , we showed that even if the second moments of @xmath1 are not known exactly then improvement can be made over the standard lse provided that the unlabeled sample is non - negligible with respect to the labeled sample , i.e. , @xmath15 is bounded away from 0 .",
    "here we considered the classical framework where @xmath70 is fixed and @xmath121 .",
    "@xcite study the semi - supervised mean estimation problem where @xmath122 both go to infinity but @xmath70 grows relatively slowly , i.e. , @xmath123 .",
    "the findings of @xcite indicate that our results can be extended to the latter case .",
    "however , our method obviously requires that @xmath124 and for high - dimensional regression models where @xmath125 , a different approach is needed .",
    "high - dimensional missspecified regression models are the topic of a recent manuscript by @xcite .    in this work",
    "we did not study variable selection and considered the covariates as given .",
    "however , improved asymptotic performance may be achieved by including functions of the existing covariates as additional new covariates .",
    "section 3 of @xcite discusses how this can be done for the problem of semi - supervised estimation of a mean .",
    "on the other hand , when the labeled sample , @xmath6 , is small , then it may be better not to include all covariates .",
    "we hope to study the problem of variable selection in this context in a future research .",
    "a possible extension of these results is to generalized linear models , where the label variable ( @xmath0 ) takes a small number of values and the regression model reduces to a classification problem .",
    "our hope is that our method can be extended to these cases and improvement can be made over naive classifiers that consider only the labeled data .    in summary , in this work we considered the framework where the best linear predictor is of interest even if @xmath126 is not linear .",
    "under this framework , additional information on the distribution of @xmath1 is useful to construct better estimates than the standard estimates .",
    "we believe that this framework is of practical importance and also can lead to interesting statistical issues , which are yet to be studied .",
    "[ lem1 ] we have that @xmath127 ^ 2 +   e\\left\\ { ( { \\boldsymbol \\beta}- \\tilde{\\boldsymbol \\beta})^t { \\bf m } ( { \\boldsymbol \\beta}- \\tilde{\\boldsymbol \\beta } ) \\right \\}.\\ ] ]    * proof . * + first , @xmath128 where the last equality holds true since @xmath129 is orthogonal to @xmath130 .",
    "we can further write @xmath131 ^ 2\\\\ = e\\left\\ { ( \\alpha -\\tilde{\\alpha } ) +   ( { \\boldsymbol \\beta}- \\tilde{\\boldsymbol \\beta})^t e({\\bf x } ) \\right\\}^2 + e\\ { ( { \\boldsymbol \\beta}- \\tilde{\\boldsymbol \\beta})^t { \\bf m } ( { \\boldsymbol \\beta}- \\tilde{\\boldsymbol \\beta } )   \\},\\end{gathered}\\ ] ] for the first term in notice that @xmath132 and @xmath133 ; therefore , @xmath134",
    "^ 2,\\ ] ] and is established .",
    "* proof of proposition [ prop1 ] . *",
    "+ since for any numbers @xmath135 we have that @xmath136 and @xmath137 , then @xmath138 ^ 2,b\\right ) \\\\+   \\lim_{b \\to \\infty } \\lim_{n \\to \\infty }   e   \\min\\left (   n\\left\\ { ( { \\boldsymbol \\beta}- \\tilde{\\boldsymbol \\beta})^t { \\bf m } ( { \\boldsymbol \\beta}- \\tilde{\\boldsymbol \\beta } ) \\right \\},b\\right).\\end{gathered}\\ ] ] we start with the first summand in . since @xmath139 is asymptotically normal then @xmath140 =   \\sum_{j=1}^p ( { \\beta}_j- \\tilde{\\beta}_j)\\{\\bar { x}_j - e(x_j)\\}= o_p(1/\\sqrt{n}).\\ ] ] therefore , by the continuous mapping theorem and since the random variables are bounded , then for any @xmath89 , @xmath141 ^ 2,b\\right ) \\\\ = \\lim_{n \\to \\infty }   e \\min\\left ( n\\big [   \\bar{y } - \\sum_{j=1}^p { \\beta}_j\\ { { \\bar x}_j - e(x_j)\\ } -e(y)\\big]^2,b\\right)\\\\ = e \\min\\left ( z_1 ^ 2,b\\right),\\end{gathered}\\ ] ] where @xmath142 is normal with mean zero and variance @xmath143 ^ 2 $ ] .    for the second summand in , the asymptotic normality @xmath144 implies that for any @xmath89 , @xmath145 where @xmath146 is a normal vector with mean zero and variance matrix @xmath147 . taking limits as @xmath148 in and completes the proof of the proposition since @xmath149 .",
    "* proof of theorem [ thm : p_dim ] . *",
    "+ we have that @xcite @xmath150 .",
    "furthermore , @xmath151 since @xmath152 the results now follows from the multivariate clt and slutsky s theorem .",
    "+ for each @xmath153 , @xmath154 again , the multivariate clt and slutsky s theorem imply the result .    to prove the furthermore part , notice that for each @xmath153",
    ", @xmath155 is a linear function of @xmath156 and hence is orthogonal to @xmath157 , i.e. , @xmath158 for all @xmath153 .",
    "therefore , @xmath159 we show that @xmath160 hence , @xmath161 . since @xmath162 is orthogonal to @xmath163 , then @xmath164    we now prove by a somewhat lengthy calculation .",
    "we have that @xmath165 therefore , and yield @xmath166 we have that @xmath167 and hence , @xmath168 we now consider the summands in .",
    "we start with the first summand @xmath169 since @xmath170 then , @xmath171 and therefore , @xmath172                buja , a. , berk , r. , brown , l. , george , e. , pitkin , e. , traskin , m. , zhao , l. , zhang , k. ( 2016 ) .",
    "models as approximations : how random predictors and model violations invalidate classical inference in regression .",
    "_ submitted _ ; arxiv:1404.1578 ."
  ],
  "abstract_text": [
    "<S> we study a regression problem where for some part of the data we observe both the _ label _ variable ( @xmath0 ) and the _ predictors _ ( @xmath1 ) , while for other part of the data only the predictors are given . </S>",
    "<S> such a problem arises , for example , when observations of the label variable are costly and may require a skilled human agent . </S>",
    "<S> if the conditional expectation @xmath2 $ ] is exactly linear in @xmath1 then typically the additional observations of the @xmath1 s do not contain useful information , but otherwise the unlabeled data can be informative . in this case , </S>",
    "<S> our aim is at constructing the best linear predictor . </S>",
    "<S> we suggest improved alternative estimates to the naive standard procedures that depend only on the labeled data . </S>",
    "<S> our estimation method can be easily implemented and has simply described asymptotic properties . </S>",
    "<S> the new estimates asymptotically dominate the usual standard procedures under certain non - linearity condition of @xmath2 $ ] ; otherwise , they are asymptotically equivalent . </S>",
    "<S> the performance of the new estimator for small sample size is investigated in an extensive simulation study . </S>",
    "<S> a real data example of inferring homeless population is used to illustrate the new methodology .    </S>",
    "<S> technion  israel institute of technology@xmath3 , wharton  university of pennsylvania@xmath4 and stanford university@xmath5 </S>"
  ]
}