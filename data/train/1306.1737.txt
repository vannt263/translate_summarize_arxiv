{
  "article_text": [
    "pair interactions , simd , gpu , molecular dynamics , verlet list",
    "in most particle simulations , more than half of the computational time is spent in calculating pair interactions with limited spatial range .",
    "when long - range interactions are present , such as electrostatics , the long - range part is usually calculated on a mesh .",
    "certain types of analysis , such as determining particle pair correlation functions , also involve evaluating pair interactions with limited range .",
    "many codes that compute these kind of interactions employ cpu algorithms consisting of a simple double loop to iterate through a list of particle pairs .",
    "this nave approach has a quadratic computational complexity which makes it prohibitively expensive already for moderate numbers of particles .",
    "however , by exploiting the limited interaction range imposed by the typically spherical cut - off , the computational cost can be reduced to linear .",
    "this is achieved by reducing the number of neighboring particles that need to be considered .",
    "to do so the verlet list @xcite and the linked cell @xcite algorithms as well as the combination of the two are widely used . in particular , in molecular dynamics ( md ) simulation codes @xcite these algorithms are most commonly employed .",
    "although these algorithms suffer from limitations on modern simd architectures @xcite , there have been only a few attempts to overcome them , most of them specific to gpus @xcite without achieving generality .    before the advent of cpu simd units , the performance of the simple double loop over the neighbor list was quite good as the compiler can usually unroll the inner loop . because the speed of the main memory has not kept up with the processor speed ,",
    "caching became more important . in calculating pair interactions",
    "this means that the location of particles in memory should correlate with their spatial location to increase cache hits .",
    "several publications have dealt with this issue @xcite . however , as the width of the simd units increases , reordering or shuffling the input and output data for convenient access in the simd units becomes a severe bottleneck . when calculating pair interactions between all particle pairs in the system , a perfectly linear memory access pattern can be used that avoids shuffling . however ,",
    "when a cut - off is used , a significant part of the particle neighbor list will    not be ordered sequentially .",
    "the relative cost of shuffling depends on the cost of calculating a single pair interaction and on the simd width . in molecular dynamics simulations particles",
    "usually interact via a lennard - jones ( lj ) and a coulomb potential .",
    "when the popular particle - mesh ewald ( pme ) electrostatics method @xcite is used , a complementary error function must be calculated .",
    "pennycook et al .",
    "@xcite provide a detailed analysis of the shuffling ( also called gather - scatter ) and their impact on performance with only lj interactions considered . in their work , with 8-way simd reordering instructions represent a third of the total , with 16-way simd the ratio is more than a half . in practice , the performance is affected even more . since shuffling introduces more data dependencies between instructions , reducing the instructions available for scheduling will result in low instructions per cycle ( ipc ) .",
    "we will show that even when calculating lj and pme interactions , the shuffling ends up taking more than half of the time with 4-way simd .",
    "on gpus , shuffling data is typically not required as the execution model allows hardware threads to access data from different memory locations . however , loading particle data requires scattered memory access which will waste gpu memory bandwidth as well as cycles ( due to instruction replay ) and will render a standard implementation memory bound . moreover",
    ", the throughput - oriented gpu architecture requires high level of parallelism and is sensitive to memory access patterns . in order to target gpus",
    ", some codes combine the traditional algorithms with data regularization techniques @xcite , but such approaches can still lead to inefficient execution . recasting the algorithms to a more regular data access has been shown to result in higher ipc on gpus , but not without additional trade - offs @xcite .",
    "although on cpus the relative memory bandwidth is higher , the data dependencies can still cause bottlenecks in simd - optimized algorithms .",
    "the main issues faced when considering data parallelization in traditional particle - pair based neighbor - lists schemes are the irregular sizes and non - contiguous nature of the neighbor lists of each particle .",
    "we propose to address both of these issues by considering pair - interactions between clusters of particles of fixed size , similar to the work of friedrichs et al .",
    "however , important distinguishing features of our algorithm are high parallel work - efficiency and the inherent flexibility which enables tuning for the simd width and other specifics of the hardware . by changing the size of the clusters",
    ", our algorithm can be adapted to simd units of different widths .",
    "adjusting the cluster size also allows tuning the number of operations `` in flight '' as well as the ratio of arithmetic to memory operations .",
    "this flexibility , together with the high ratio of arithmetic to load / store operations , ensures that the algorithm can reach high performance on current , as well as future cpu and gpu hardware .",
    "it is also well suited to more exotic hardware such as fpgas , but as the implementation is still ongoing , result will be reported in the future . in case of cpus , the additional major advantage is that , by matching the cluster size to the simd width , no shuffle operations are required at all .",
    "this not only improves performance by at least a factor of 2 , but also makes the code much easier to write and read .",
    "there is a price to pay for the improvements as the cluster pairs will contain particle pairs in addition to the ones in the original interaction sphere .",
    "this results in extra interactions calculated between particles otherwise not within range , which we know will evaluate to zero . as we will show later ,",
    "although this does lead to reduction in algorithmic work - efficiency , the performance gain still outweighs the extra cost .",
    "we would like to note that the algorithm operates on the lowest level of the interaction calculation and any optimization available in the literature can be applied . for md ,",
    "we use it together with a verlet buffer .",
    "furthermore , all parallelization strategies developed for traditional algorithms can be used with little or no modification .",
    "we have designed and implemented non - bonded pair interaction kernels for x86 sse2 , sse4.1 , avx and avx+fma ( amd bulldozer ) simd architectures , as well as nvidia gpus .",
    "the kernels utilize lj interactions and monopole - monopole electrostatic interactions of general form .",
    "we implemented analytical electrostatics kernels for reaction - field ( rf ) and pme , as well as tabulated electrostatic potentials .",
    "we plan to support sphero - symmetric potential of arbitrary shape through tabulated interactions .",
    "while the required additional table lookups per pair will lower the efficiency of the kernels on current cpus , on gpus and with avx2 ( which will support table lookups ) performance should be good .",
    "the algorithms described here have been implemented in the gromacs molecular simulation package @xcite and are available in the official version 4.6 release , combined with hybrid mpi+openmp parallelization .",
    "the source code can be obtained under the lgplv2 license from http://www.gromacs.org .",
    "note that the cpu kernels in gromacs 4.6 have an additional optimization , not discussed in this paper , for systems where less than half of the particles have lj interactions . for water this",
    "improves kernel performance by up to 10% .",
    "we are looking for an algorithm that can execute single instructions on multiple data ( simd ) , while not being limited by loading and storing data from and to ( cache-)memory .",
    "the standard implementation of the verlet - list algorithm loads a particle and calculates pair interactions by looping over its neighbors . thus a single pair interaction is calculated for each particle load and store .",
    "the relatively cheap interactions in md simulations render this algorithm effectively memory bound .",
    "to remedy this , our algorithm loads a cluster of @xmath0 particles and calculate @xmath0 interactions for each neighbor loaded .",
    "this increases the data reuse by a factor of @xmath0 .",
    "the loop over neighboring particles is replaced by a loop over clusters consisting of @xmath1 particles .",
    "the values of @xmath0 and @xmath1 will be tuned for the simd hardware .",
    "the standard implementation of the verlet - list algorithm can be seen as a special case of this cluster algorithm where @xmath0=1 and @xmath1=1 .      in general ,",
    "the easiest way to achieve simd parallelization is to let the compiler vectorize loops , possibly with the help of the programmer aided by feedback from the compiler . at a first glance",
    "this might seem to be a good strategy since a particle usually has hundreds of neighbors which leads to long vectorizable loops .",
    "for efficient loading , the order of particles in memory needs to be strongly correlated with spatial ordering to increase cache hits .",
    "ideally , sequential particles would be loaded in groups of size equal to the simd width , but this not compatible with a spherical interaction volume . even when particles can be loaded in groups , vectorizing the inner - loop",
    "will only give a small speed - up on wider simd units , as memory operations and data shuffling can take more time than the actual calculation .",
    "for lj only with fixed parameters on avx 8-way simd , memory and shuffling operations account for 32 of the 70 operations @xcite ; with parameter loading , the ratio increases beyond 50% . when calculating all interactions of neighbors with one particle , we need to load 3 coordinate components , 3 parameters , as well as load and store 3 force components for each neighbor . in theory , on current cpus this should not lead to a memory - bound algorithm , but in practice performance will be far from peak due to limitations on the instruction scheduling .",
    "the coordinates are loaded per particle as triplets of @xmath2 , @xmath3 , @xmath4 requiring data - shuffling .",
    "the wider the cpu simd unit is , the more data shuffling is required and the longer the dependency chain gets between loading data , computation and storing forces .",
    "hence , for efficient simd calculations it is very advantageous to use packed sequences of coordinates , e.g @xmath5 , @xmath6 and @xmath7 with 4-way simd .",
    "on gpus , such packing is not needed as vector types are supported , but a much higher arithmetic to memory operation ratio is required to achieve peak performance . constructing the neighbor list , also called pair list , is a similar operation , but with less arithmetic , which makes it even more memory intensive .",
    "although the pair list is usually not reconstructed every step , it involves looping over more pairs than the non - bonded kernel processes , so this can become a limiting factor . the only way to hide the latency of memory operations is to perform more calculation per load / store operation . at first sight",
    "this might seem impossible , but this can actually be achieved with a simple scheme .",
    "the basic idea behind our work is to spatially cluster particles in groups of fixed size and use such a cluster as the computational unit of our algorithm .",
    "these groups can then be mapped directly to the simd hardware units , which have a fixed width .",
    "given a 4-way simd unit , we can spatially cluster particles in groups of 4 .",
    "we can load a cluster of 4 , so called , @xmath9-particles in simd registers and then loop over the neighboring clusters of 4 , so called , @xmath10-particles ( see fig .",
    "[ simd ] ) . with this @xmath8 = 4@xmath114 setup ,",
    "we compute 16 pair interactions while only performing memory load and store operations for 4 @xmath10-particles . after having looped over all neighboring @xmath10-clusters of an @xmath9-cluster , usually a few hundred , we also have to do memory operations for the @xmath9-particles , but the cost of this is negligible . in this example",
    "the memory bandwidth is reduced by a factor of 4 , but more importantly , as we always access particles in cluster of size 4 , we can organize all data packed in groups of 4 . this eliminates the need for data shuffling which is the main performance bottleneck of the standard way of calculating non - bonded interactions on simd units .",
    "this is the simplest version of the algorithm .",
    "the same 4@xmath114 clusters can also be processed on 8-way simd hardware .",
    "then two i - clusters are loaded in one simd register and each j - cluster is duplicated in one simd register .",
    "this setup halves the number of arithmetic operations and adds a few shuffle operations . in cuda ,",
    "memory access is more flexible .",
    "hardware threads on nvidia gpus are organized in `` warps '' . on current gpus ,",
    "each warp consists of 32 threads which execute the same instruction every cycle .",
    "this results in a simd - like execution model called single instruction multiple threads ( simt ) . unlike the simd model which requires explicit programming for the simd width",
    ", the simt architecture allows thread - level parallel programming , and the warp - based lockstep execution model needs to be considered only for performance .",
    "this enables more flexible memory operations ( different addresses in different threads ) and divergence among threads in a warp .",
    "the simt model allows spreading out all particle pairs in an 8@xmath114 cluster pair over the 32 threads of a warp , thus processing one particle pair on each thread .",
    "we illustrate this in fig .",
    "[ simd ] , for the sake of example using 16-way simt .    1 and the 4@xmath114 setups with 4-way simd and 16-way simt .",
    "all numbers are particle indices , each black dot represents an interaction calculation and the arrows indicate the computational flow .",
    "the simd registers for @xmath9- and @xmath10-particles are shown in green and blue , respectively .",
    "the 4@xmath114 setup calculates 4 times as many interactions per particle load / store and requires fewer memory operations ( shown in red ) . unlike the 1@xmath111 setup , the 4@xmath114 setup does not require data shuffling in registers .",
    "[ simd],width=302 ]    we will now describe in detail how the algorithm works , starting with building the cluster pair list .      the algorithmic unit of particle data representation is a cluster rather than a single particle . beside this",
    "minor , but important difference , the overall algorithm closely follows the standard verlet or neighbor list setup .",
    "hence , in the following , unless explicitly stated , pair list will refer to a list of cluster pairs .",
    "note that the cluster pair list this work uses as data representation does not define a strict particle - particle in - range relationship because , as we will show later , the list by design includes particles not in - range .",
    "moreover , the presented algorithms use newton s third law to calculate pair interactions , hence the pair list contains each pair only once , not twice .",
    "since for each particle there is no explicit list of all particles in its neighborhood , we prefer the term `` pair list '' to the term `` neighbor list '' .",
    "we construct a pair list using a verlet buffer ( also called `` skin '' ) which is essentially an extension of the cut - off distance to account for particle movement allowing the list to be retained for a number of steps @xcite .",
    "the exact number depends on the relative cost of the list construction and the dependence of the buffer size on the lifetime of the list .",
    "pair interactions are then determined for the fixed list of particle pairs defined by pairs of clusters . in the most general case ,",
    "we need to generate a pair list of clusters of size @xmath0 particles versus clusters of size @xmath1 . in the simplest setup",
    ", the simd width will be equal to @xmath1 , but a width of @xmath12 , where @xmath0 is divisible by @xmath13 , will also work . on a gpu",
    "the best performance will be achieved when matching @xmath8 to the width of the simt execution model , i.e. 32 for cuda .",
    "first we need to group the particles into clusters of fixed size . to minimize the number of additional particle pairs in the pair list , the clusters need to be as compact as possible .",
    "a simple and efficient way of generating compact , fixed - size clusters is spatial gridding in two dimensions and spatial binning in the third dimension , see fig .",
    "[ clustering ] .",
    "first we construct a rectangular grid along x and y with a grid spacing of @xmath14 , where @xmath15 is the particle density .",
    "then we sort the particles into columns of this grid . for each column",
    "we sort the particles on z - coordinate and as a result we get the spatial clusters as consecutive groups of @xmath0 or @xmath1 particles . because the number of particles in a column is typically not a multiple of @xmath0 , we add dummy particles to the last cluster when needed .",
    "the fraction of dummy particles is @xmath16 ; with 10000 particles and clusters of size 8 this gives 4% dummy particles in the cpu algorithm .",
    "for the gpu we use a hierarchical cluster setup . as we can store 8 i - clusters in shared memory , we group 2@xmath112@xmath112=8 clusters of size 8 together .",
    "this reduces the number of dummy particles to 1% with 10000 particles .",
    "all these operations can be done efficiently in linear time .",
    "the next step is calculating bounding boxes for each cluster , this can be done using simd instructions , as the number of particles in a cluster is constant . in the case of @xmath17 ,",
    "adjacent pairs of bounding boxes are combined to generate clusters of double the number of particles .",
    "a pair list can then be constructed by checking distances between the bounding boxes .",
    "this is very efficient , as it requires one bounding box - pair distance check for @xmath18 particle pairs .",
    "however , this results in more cluster or particle pairs than strictly necessary , as bounding boxes might be within range while none of the particle pairs falls within range . to avoid this overhead we prune pairs of clusters at distances close to the cut - off using a particle - pair distance criterion . for the gpu implementation",
    ", the pair list construction is performed on the cpu , but the pruning is done on the gpu where this can be done more efficiently .",
    "periodic boundary conditions can be implemented in a simple and efficient fashion by moving the @xmath9-clusters by the required periodic image shifts and storing these shifts in the cluster pair list for use during the pair interaction calculation .",
    "-cluster list in green for the red @xmath9-cluster .",
    "[ clustering],width=226 ]    .... / * bb = cluster bounding box * / for each i - cluster ci    determine grid range in x within rlist of bb[ci ]    for each grid cell gx in range      determine grid range in y within rlist of bb[ci ]      for each grid cell gy in range        determine j - clusters at gx , gy within rlist of bb[ci ]        for each j - cluster cj in range          if ( bbdistance(ci , cj ) < rlist )            if ( bbdistance(ci , cj ) < rbb or                atomdistance(ci , cj ) < rlist )              put cj in cjlist[ci ]    set forcefield exclusion masks in cjlist[ci ] ....    at this point we would like to note that the cluster pair list , being simply a verlet list of particle clusters , can be seen as a generalized version of the classical neighbor list .",
    "consequently , the neighbor list corresponds to the special case of cluster size @xmath19 and in the following we will refer to it as the 1@xmath111 scheme .",
    "the cluster - based pair list contains inherently more particle pairs than the ones within the cut - off radius , the number of pairs for different @xmath8 is shown on fig .",
    "[ zeros ] .",
    "the fraction of extra interactions increases rapidly with the cluster size and decreases rapidly with the cut - off radius .",
    "however , the increase in the efficiency of the presented algorithm should outweigh the cost of calculating these extra interactions .",
    "as can be seen in fig .",
    "[ zeros ] , when it comes to the number of extra pairs , lists with @xmath17 are less favorable than @xmath20 which results in clusters with close to cubic shape .",
    "this shape minimizes the number intersecting cut - off spheres , resulting in a more compact list .",
    "pair lists normalized by the average number of pairs in a sphere of radius @xmath21 , as a function of the pair list radius @xmath21 , for a 3-site water model , number density @xmath15=100 nm@xmath22 .",
    "[ zeros],width=283 ]      having constructed the pair list , for each cluster we now have the lists of all clusters in range .",
    "we still need to take care of particle pairs that need to be excluded .",
    "there are three types of exclusions .",
    "two of those occur within cluster self - pairs . here",
    "particle pairs occur twice , whereas we should only calculate them once and there are self interactions .",
    "we want to calculate each pair interaction only once and skip the self interactions .",
    "these two types of exclusions are handled in the pair interaction kernel .",
    "additionally , there can be exclusions defined by the force field .",
    "normal lj and electrostatic interactions should not be calculated for such excluded pairs of particles , whereas the rf or pme correction should still be applied . to treat these exclusions in the non - bonded kernels , we encode them in a bitmask stored per cluster pair in the pair list .",
    "this compact representation saves memory and also allows for easy and fast decoding of exclusions using bitwise operations .",
    "on cpus we sort the pair list according to the presence of exclusions so we only need to mask exclusions when really needed .",
    "this improves performance by 15% .",
    "the total computational cost of the pair list construction is proportional to the number of particles . to understand how the total cost scales with different parameters , it is worth looking into the details of the different tasks involved .",
    "computational cost in terms of cycles as a function of pairs per particle is shown in fig .",
    "[ cycles_npair ] . for the implementation of the sorting ,",
    "after the gridding , we assume that the particle distribution is homogeneous on longer length scales , but other suitable sorting techniques can be applied in the inhomogeneous case .",
    "then a simple pigeonhole sorting is used , which scales linearly and provides good performance .",
    "the cost of the pair search is not proportional to the number of pairs , as only the boundary of the interaction sphere needs to be determined .",
    "this cost is high when the number of pairs is small compared to @xmath0 and @xmath1 and it is proportional to the radius squared for large radius .",
    "when the radius is large , the cost of search decreases proportionally with @xmath8 .",
    "this makes the search far more efficient than a particle - pair based search .",
    "another implication of the much lower number of ( now cluster- ) pairs , is that advanced search algorithms , such as interaction sorting @xcite , will not help and a simple search algorithm performs well . finally , the cost of interaction calculation is proportional to the total number of pair interactions calculated .",
    "but the overhead of pairs beyond the cut - off distance decreases with increasing number of pairs .",
    "the cost of the search and lj+pme force calculation on cpus are similar , as can been seen in fig .",
    "[ cycles_npair ] ; rf kernels are about twice as fast , which makes the search relatively more expensive .",
    "the optimal balance between search cost and extra cost due to the verlet buffer is usually achieved with a pair list update interval of 10 and 20 when only using a cpu . on the gpu ,",
    "the interaction throughput is much higher which makes the cpu search relatively more expensive . as mentioned , before , we do most of the cluster - pair pruning on the gpu , which reduces the cpu search cost significantly , as can been seen in fig .",
    "[ cycles_npair ] . depending on the speed of the cpu versus the gpu , and especially the number of cores in each ,",
    "the optimal pair list update interval is between 10 and 50 .",
    "furthermore , as the search algorithm maps well to gpus @xcite , we plan to port it in the near future .",
    ", for 8@xmath118 the search cost is also shown for checking bounding box distances only .",
    "the pair count is within the spherical volume , not including the extra pairs due to the irregular cluster - pair volume .",
    "note that the wiggles on the curves for searching are caused by jumps in the number of grid cells fitting in a cut - off sphere .",
    "all timings were done on an intel sandy bridge cpu with single precision 256-bit avx kernels using a single thread .",
    "[ cycles_npair],width=283 ]    when the pair list needs to be updated for every interaction calculation , the particle - pair distance based pruning should be skipped and replaced by a conditional in the interaction kernels . with very cheap interactions , such as for a pair correlation function calculation ,",
    "no conditional should be used at all .",
    "as in molecular simulations usually more than half of the computational time is spent in the calculating non - bonded pair interactions , it is well worth carefully optimizing these kernels .",
    "we now have a list of cluster pairs of @xmath0 versus @xmath1 particles . as can be seen in fig .",
    "[ code_kernel ] , writing a simd kernel for this setup is rather straightforward .",
    "however , achieving optimal performance is not trivial .",
    "it is often very hard to judge how close the kernel performance is to the maximum achievable performance , as it depends both on hardware characteristics , mainly the type of simd unit and the performance of the cache system and load / store units , as well as on software characteristics , mainly the compiler(s ) used .",
    ".... for each ci cluster    load m coords+params for ci      for each cj cluster      load n coords+params for cj      / * these loops are unrolled using simd * /      for j=0 to m        for i=0 to n          calculate interaction ci*m+i with cj*n+j      store n cj - forces    store m ci - forces ....    for cpus we chose to write the kernels in c with extensive use of sse and avx simd - intrinsics as the current gnu and intel compilers do a good job at optimizing such code and typically achieve better performance across multiple architectures than equivalent hand - written assembly .",
    "for gpus we chose to concentrate on the nvidia cuda programming model as the available development tools are more mature and provide higher performance than that of the alternatives .",
    "there are two main factors that affect kernel performance and require special attention .",
    "one is the choice of @xmath0 and @xmath1 , the other is the treatment of the exclusion and cut - off checks . for the latter the options are using conditionals , which should be avoided on cpus , or masking interactions using bitmasks .",
    "masking is usually more efficient than conditionals .",
    "on cpus simd bitwise and operations are used for masking , whereas on gpus we simply multiply by 0 or 1 and use a conditional for the cut - off check . using a conditional can reduce the number of instructions issued when all pairs stored in a simd register are beyond the cut - off distance . on the cpu this should only be used when all @xmath8 pairs are beyond the cut - off , as otherwise the force reduction cost increases .",
    "this only improves the performance when an overly long pair list buffer is used , so we only use a conditional for the 1@xmath111 kernels where it helps in most cases . with the latest cpu compilers ,",
    "not much code optimization is required , as long as the fastest possible intrinsic is used for the respective instruction set , e.g. sse2 , sse4.1 or avx . in cuda optimization is less straightforward ; as the architecture is changing rapidly , compilers and drivers are less mature .",
    "additionally , gpus are massively parallel processors with more simple cores than cpus , which puts more burden on the programmer and compiler to pick the right optimization which might not even carry across hardware generations .",
    "the main goal is to keep the computational units as busy as possible by avoiding stalls due to dependencies on memory operations or instruction latencies .",
    "the ratio of compute to memory operations scales with @xmath0 , as for each loaded @xmath10-particle , @xmath0 interactions with @xmath0 @xmath9-particles are calculated in a single inner - loop iteration . on the cpu it turns out that the best performance is achieved for @xmath0=4 . while using 2 @xmath9-particles",
    "is also possible , with 4 there seem to be enough arithmetic instructions fed to the scheduler to hide most memory operations .",
    "using @xmath23 will lead to a marginally higher ipc and flop rate , at the expense of calculating many more zeros .",
    "the choice of @xmath1 depends on the the simd width . here , for cpus",
    ", we only consider value of @xmath1 equal to the full or half simd width . fitting two @xmath10-clusters in a simd - width",
    "will simply halve the number of arithmetic operations and add a few shuffle operations .",
    "the instruction count is largely independent of @xmath1 .",
    "the only exception is the table lookup for tabulated electrostatics , the number of which scales with both @xmath0 and @xmath1 .",
    "the precision , either single or double , also does nt affect the instruction count , except for the inverse square root operation , which needs an extra newton - raphson iteration in double precision .",
    "an issue specific to cpu simd kernels is that lj pair parameter look - up is costly , as one simd load operation is required for each particle pair . with geometric or lorentz - berthelot combination rules only two loads are required per cluster pair , latency of which can be hidden with computation .",
    "the cuda kernels turn out to be instruction latency limited , not memory limited , although this requires some tricks and a tight packing of the pair list in memory .",
    "the pseudocode of the kernel is shown in fig .",
    "[ code_kernel_gpu ] .",
    "the inner loop calculates interactions between clusters of @xmath0=8 and @xmath1=4 , this way 32 threads of a warp calculate a pair interaction of an entire cluster pair simultaneously .",
    "we chose @xmath1 smaller than @xmath0 such that we have more computation per memory operation .",
    "we group cluster pairs two by two in the pair list , hence the pair search can be done on clusters of 8 particles and the computation on two 8@xmath114 clusters independently on two warps . additionally , we store clusters in range for 8 @xmath9-clusters in a single pair list with this further improving the data reuse .",
    "a @xmath10-cluster interacts with half of these 8 @xmath9-clusters on average , which additionally reduces the memory pressure by a factor of 4 . as the pseudocode on fig .",
    "[ code_kernel_gpu ] shows , non - interacting @xmath10-clusters are skipped based on an bitmask - encoded cluster interaction mask , which only causes a minor overhead .",
    "this hierarchical grouping requires minor modifications in the pair - search code , only storing the packed exclusions masks becomes more complex . with this setup we can load the coordinates , atom types , and charges for 64 @xmath9-particles in registers on the gpu and thereby maximize the number of calculations per @xmath10-particle load .",
    "two warps in a cuda thread block operate on a group of 8 @xmath9-clusters and their @xmath10-cluster neighbors . as the two warps by definition access different @xmath10-particles",
    ", they can run independently and no synchronization is required during computing . on the fermi architecture partial forces are accumulated in registers and reduced in shared memory . in contrast , the kepler architecture provides a special `` warp - shuffle '' operation which can be used for efficient synchronization - free warp - level reduction .",
    "after a lot of testing and optimization , the cuda kernels turned out to be compact and more readable than the cpu simd kernels .",
    "more code is required for managing gpu device initialization , kernel launches and transfers between cpu and gpu .    ....",
    "/ * each of the mxn i - j pairs is assigned to a thread .",
    "the sci i - supercluster consists of 8 ci clusters . * / sci = thread block index for each ci in sci load i - atom data into shared mem .    / * loop over all cj in range of any ci in sci * / for each cj cluster    load j - i cluster interaction and exclusion mask / * per warp * /    if cj not masked / * non - interacting cj - sci * /      load j - atom data        / * loop over the 8 i - clusters * /      for each ci cluster in sci        if cj not masked / * non - interacting cj - ci * /          load i atom data from shared mem .",
    "r2 = sqrt(|xj - xi| )          extract excl_bit exclusion / interaction bit for j - i pair          if ( ( r2 < rc_squared ) * excl_bit )            calculate i - j coulomb and lj forces            accumulate i- and j - forces in registers          reduce j - forces    reduce i - forces ....      calculating energies is only required infrequently in molecular dynamics , therefore we will concentrate on force - only kernels .",
    "the lennard - jones force is : @xmath24 where @xmath25 is 0 for excluded particle pairs and 1 otherwise .",
    "the lj coefficients @xmath26 and @xmath27 can be different for each atom pair @xmath9-@xmath10 . in practice",
    "there is a limited number of atom types and often combination rules are used to obtain the parameters between two atom types . with",
    "x86 simd instructions loading arbitrary pair parameters can be costly due to the many load and shuffle operations required .",
    "using combination rules , either geometric or lorentz - berthelot , is more efficient .",
    "the electrostatic interaction form we consider is : @xmath28 where @xmath29 is the long - range correction force . for reaction - field",
    "electrostatic we have @xmath30 with @xmath31 a constant .",
    "this can be evaluated efficiently analytically .",
    "for pme we have @xmath32 , with @xmath33 a constant .",
    "evaluation of the pme correction force is more costly .",
    "but as it is bounded and very smooth , linear table interpolation can reach full single precision with a limited table size . as a second option we consider an analytical approximation using a quotient of two polynomials .",
    "this requires 24 multiplications and additions and one division to reach full single precision .",
    "fused multiply - add ( fma ) instructions , currently available on gpus and the amd bulldozer microarchitecture , can speed up this polynomial evaluation significantly .",
    "achieving good performance of load and store intensive kernels requires detailed understanding of many low - level software optimization aspects : simd instruction set , throughput and latency of instructions on different processor microarchitectures , cache behavior , as well as experience with compiler - related performance issues .",
    "unfortunately , it takes a lot of time and effort to reach optimal performance .",
    "fortunately , this effort is required infrequently and our results can be used by anyone , as our compute - kernels are released as part of an open source project , freely available for anyone to use .      to compare the different variants of the algorithm",
    ", we focus on the intel sandy bridge cpu architecture .",
    "the reason for this is that at the time of writing this architecture supports avx , the newest and widest simd instruction set on the x86 platform , and it also provides 256-bit operations .",
    "this allows a direct comparison between the 4@xmath114 and 4@xmath118 setup , as well as between 128- and 256-bit avx . for comparison with other architectures",
    ", we also show results on the amd bulldozer architecture using 128-bit avx and fma instructions as well as nvidia fermi ( gf100 ) and kepler2 ( gk110 ) @xcite gpus . as all cpu architectures in focus support",
    "the avx instruction set , form here on 128- or 256-bit simd will refer to avx instructions of the respective type .",
    "we report all performance data in cycles which depends only on the microarchitecture , but not the exact cpu or gpu model .",
    "all cpu kernels were compiled with the gnu c compiler version 4.7.1 with ` -o3 ` as the only optimization .",
    "other optimization - related compiler options did not improve the non - bonded kernel performance .",
    "both intel c compiler versions 12.1.3 and 13.1.1 produced slightly slower code even with cpu architecture - specific optimizations enabled .",
    "the gnu c compiler has greatly improved with recent versions , the difference between version 4.5.3 and 4.7.1 on sandy bridge with analytical ewald kernels is 22% while with recent intel compilers even slight regressions have been observed .",
    "the cuda gpu kernel were compiled with the cuda compiler version 5.0.7 with ` -use_fast_math ` as well as the architecture - specific optimization options ` -arch = sm_20 ` and ` -arch = sm_30 ` for fermi and kepler2 gpus , respectively .    on the cpu we store the properties of the @xmath0 i - particles in simd registers and loop over the list of clusters of @xmath1 j - particles .",
    "the pair interactions for the @xmath0 different i - particles are not interdependent , except that we want to load and store the j - particle properties only once .",
    "there are several choices to be made when transforming this algorithm into actual code . for instruction",
    "( re-)scheduling it is advantageous to write out the @xmath0 operations for the i - particles , so it is clear to the compiler that it can reorder them . for most kernels @xmath1 matches the simd width , but for the 256-bit flavor",
    "we also consider @xmath1=4 , which is half the simd width . on new architectures with wider simd units , such as intel mic with 16-way simd in single precision , having @xmath1 smaller than the simd width is even more important .",
    "performance of the most important flavors of the fully optimized kernel versions is reported in table [ kernperf_rf ] and table [ kernperf_ewald ] for rf and ewald , respectively .",
    "the metrics shown in table [ kernperf_rf ] and table [ kernperf_ewald ] represent the peak performance of the respective kernels .",
    "the performance of cpu kernels is constant in the regime of 100 - 100000 particles .",
    "in contrast , gpus are massively parallel multi - processors which require a high level of data - parallelism and hence many particle - pairs to reach peak performance .",
    "the cuda kernels are within 5% of the peak performance from around 20000 particles ; the scaling depends both on generation of architecture and number of multiprocessors .",
    "we present four different performance metrics .",
    "the the number of pairs calculated per 1000 compute cycles ( pairs / cycle ) is the only relevant measure for the raw performance of the algorithm .",
    "the instructions per cycle ( ipc ) provides an estimate of the hardware utilization .",
    "the last two are the number of floating point operations per pair ( flops / pair ) and per cycle ( flops / cycle ) , where we try to minimize the former and maximize the latter . as a reference we show performance for 1@xmath111 kernels which fill the simd unit by unrolling the inner loop over @xmath10 .",
    "these kernels do not use lj combination rules , as parameters need to be looked up either way , which saves two floating point operations per pair .",
    "this standard way of employing simd results in low performance and low flop rates ( the theoretical peak rate for intel sandy bridge is 8 for 128-bit and 16 for 256-bit instructions , respectively ) .",
    "the high measured ipc indicates that the instructions are scheduled very efficiently .",
    "however , a large part of the instructions load , store and shuffle data , rather than doing computation .",
    "the 256-bit rf kernel is only 13% faster than the 128-bit variant while it has similar ipc .",
    "as both kernels execute the same arithmetic instructions , the observed rather small performance increase is explained by the overhead of shuffle and data load operations .",
    "we aim to address these bottlenecks with the proposed algorithms by reducing the need for shuffles and loads . in comparison to the work of pennycook @xcite , here ,",
    "the effect is much more pronounced as we need to load two lj parameters and a charge per @xmath10 particle , while they only implement an lj potential with fixed particle type . the large drop in performance",
    "when using a single thread shows that the 1@xmath111 kernels are mainly limited by instruction scheduling and hyperthreading ( ht ) improves performance by offering the possibility of scheduling instructions from both threads running on the same physical core .",
    "in double precision with 128-bit avx we can use 2-way simd and we can compare the performance for small @xmath0 and @xmath1 .",
    "the 4@xmath112 rf kernel is 26% faster than the 2@xmath112 kernel , which outweighs the negative impact of zero interactions in most cases .",
    "additionally , the pair search for 2@xmath112 takes significant time .",
    "this shows that @xmath34 is not a viable option and we therefore only consider @xmath35 or larger . in the 256-bit kernels we can use the 4@xmath114 scheme which gives 50% higher performance than 4@xmath112 and even more on a single thread .",
    "we continue with the single precision kernels for different functional forms . with 256-bit avx the @xmath8 rf kernels have a 3.3 times higher pair rate than 1@xmath111 , for ewald this factor is 2.2 .",
    "this shows that our approach works .",
    "256-bit is 25% to 65% faster than 128-bit depending on the interaction type and the of use ht .",
    "the performance of the analytical ewald kernels is similar to that of the tabulated version with ht , even though the flop rate is very different . without ht",
    "the tabulated kernels get significantly slower because of the latencies involved in reading table entries .",
    "the amd bulldozer , in contrast with the simultaneous multi - threading intel ht implements , uses a cluster multi - threading architecture with much of the functional units , including simd units , shared between a pair of cores organized in a so called module .",
    "therefore , we compare performance of a hyperthreaded core on intel with a module on amd , both of which support two threads . even though bulldozer has double the theoretical throughput of 4-way simd instructions and fma gives another doubling of the theoretical flop rate , the performance is only marginally higher than 128-bit simd on sandy bridge .",
    "moreover , sandy bridge using 256-bit simd provides a 20% higher pair rate .",
    "the cuda gpu kernels provide significantly higher performance when comparing one streaming multiprocessor with one cpu core .",
    "the analytical and tabulated ewald kernels have similar performance , the former being slightly faster on the kepler architecture even though this kernel executes about 10% more instructions .",
    "this is explained by the fact that the additional instructions are mainly fma - s and intrinsics which allow higher instruction level parallelism , higher ipc and better absolute performance than the texture - based table loads .",
    "the analytical pme kernels achieve about half of the real - world peak flop rate , which is mainly because they do nt contain enough fma instructions .",
    "also , the presence of conditionals for checking the interaction of each of the 8 i - clusters in a super - cluster deteriorates the performance by 15% .",
    "ccccccccc precision & simd width & @xmath18 & pairs / kcycle & 1 thread & ipc & flops / pair & flops / cycle + single & 4 & 1@xmath111 & 67 & @xmath3623% & 2.32 & 38 & 2.6 + single & 8 & 1@xmath111 & 76 & @xmath3624% & 2.16 & 38 & 2.9 +   + single & 4 & 4@xmath114 & 175 & @xmath3619% & 2.36 & 40 & 7.0 + single & 8 & 4@xmath114 & 223 & @xmath3627% & 1.96 & 40 & 8.9 + single & 8 & 4@xmath118 & 248 & @xmath362% & 1.68 & 40 & 9.9 +   + double & 4 & 2@xmath112 & 52 & @xmath3630% & 1.74 & 45 & 2.3 + double & 4 & 4@xmath112 & 66 & @xmath3625% & 2.16 & 45 & 3.0 + double & 8 & 4@xmath114 & 98 & @xmath3610% & 1.58 & 45 & 4.4 +    cccccccccc pu & simd width & ewald & @xmath18 & pairs / kcycle & 1 thread & ipc & flops / pair & flops / cycle + sb & 4 & ana . &",
    "1@xmath111 & 51 & @xmath3631% & 2.20 & 66 & 3.4 + sb & 8 & ana . & 1@xmath111 & 63 & @xmath3616% & 1.98 & 66 & 4.2 +   + sb & 4 & tab . & 4@xmath114 & 111 & @xmath3615% & 2.42 & 43 & 4.8 + sb & 8 & tab . &",
    "4@xmath114 & 147 & @xmath3626% & 2.26 & 43 & 6.3 + sb & 8 & tab . & 4@xmath118 & 134 & @xmath366% & 1.88 & 43 & 5.8 + sb & 4 & ana . & 4@xmath114 & 110 & @xmath3614% & 2.40 & 68 & 7.5 + sb & 8 & ana . &",
    "4@xmath114 & 139 & @xmath3611% & 1.76 & 68 & 9.5 + sb & 8 & ana . & 4@xmath118 & 137 & + 1% & 1.52 & 68 & 9.3 +   + bd & 4 & ana . &",
    "4@xmath114 & 114 & @xmath3623% & 2.16 & 68 & 7.8 +   + fermi & 32 & tab . &",
    "8@xmath114 & 549 & & 1.66 & 41 & 24 + kepler2 & 32 & tab . &",
    "8@xmath114 & 1130 & & 3.2 & 41 & 46 + kepler2 & 32 & ana . & 8@xmath114 & 1151 & & 3.7 & 69 & 85 +",
    "the number of pair interactions calculated in a cycle reflects how a non - bonded algorithm performs on a certain hardware .",
    "however , this measure is not the best indicator of the effective performance , since both the buffer region and the cluster - pair scheme add interactions beyond the cut - off , which by definition evaluate to zero . to get a useful pair interaction rate which reflects the absolute performance , only the non - zero interactions should be considered . as shown on fig .",
    "[ zeros ] , at a commonly used cut - off distance of 1 nm the 4@xmath114 setup adds 86% additional pair interactions .",
    "however , as shown later , with a more than doubled pair interaction evaluation rate , the 4@xmath114 still proves to be faster than 1@xmath111 .",
    "moreover , in certain cases we can actually make use of the extra pairs that the cluster scheme @xmath37 kernels add . in molecular dynamics",
    "the standard procedure is to ensure that no interacting pair of particles is ever missed .",
    "this is usually done by generating a pair list with a so - called verlet buffer @xcite which allows particles to move over a small distance without invalidating the pair list . at any step ,",
    "if any of the particles has moved by more than half the buffer length , the pair list is regenerated @xcite .",
    "this condition is sufficient , but not necessary .",
    "the pair list needs to be invalidated only when the distance between a pair of particles decreases by the buffer length , which will happen far less frequently .",
    "additionally , this commonly used setup is inconvenient for parallel simulations . in practice",
    ", we can often tolerate small imperfections in the pair list . in a constant temperature ensemble",
    "perfect energy conservation is not a requirement , as a thermostat will remove excess heat .",
    "moreover , the amount of energy drift that can be tolerated is very problem dependent .",
    "as there are multiple factors affecting the energy conservation in a simulation , we can allow the non - bonded interactions to cause a drift of similar magnitude like all other factors . if the buffer is too small , some particle pairs which are not in the pair list can move within the cut - off .",
    "we can determine an upper bound to the drift caused by such events in a constant temperature ensemble , this is derived in the appendix .",
    "the upper bound can be used to set the buffer size for simulations . with pme ,",
    "the pair potential at the cut - off is very small , hence the effect of missing pairs will also be very small . to quantify this effect ,",
    "we show the drift as a function of the verlet buffer size for spc / e water @xcite with a pair list lifetime of 18 fs , see fig .",
    "[ drift ] .",
    "this is a representative system as hydrogens in water are the fastest moving particles in nearly all atomistic simulations . with single precision floating point",
    "coordinates , the settle @xcite and shake @xcite constraint algorithms cause an energy drift of -0.01 and 0.1 @xmath38/ns per atom , respectively .",
    "the 4@xmath114 setup shows a drift of similar magnitude even without any additional buffering .",
    "thus , in practice , no explicit buffer is required in single precision .",
    "one thing to note is that at longer buffer length only repulsive hydrogen pairs contribute to the drift . at zero length ,",
    "attractive oxygen - hydrogen pairs also contribute which leads to a cancellation of errors .     at a cut - off distance of 0.9 nm .",
    "the settle algorithm causes negative drift in single precision for large buffers .",
    "[ drift],width=283 ]    the effective performance is given by the number of interactions within the cut - off radius that can be calculated per cycle . to compare the traditional and cluster schemes we show the performance of 1@xmath111 , and 4@xmath114 256-bit avx kernels , as well as 8@xmath114 cuda gpu kernels with both rf and ewald electrostatics in table [ effperf ] .",
    "there is one factor that complicates the comparison .",
    "the ratio of the cost of the search and the force calculation affects the optimal list update frequency , which in turn affects the required buffer size . in our implementation ,",
    "the pair list construction for the 1@xmath111 setup takes four times longer than calculating the interactions once , where for a 4@xmath114 setup both take about equal time .",
    "we think there is some room for speed - up in our 1@xmath111 search implementation , which has not been fully optimized .",
    "if we assume we can get it twice as fast , the optimal list update frequency is somewhere between 10 and 15 steps .",
    "the optimal update frequency for 4@xmath114 and 8@xmath114 is around 10 steps . for the following comparison we will use the same update frequency of 10 steps for all setups to simplify the comparison .",
    "the effective speed - up of the force calculation of the 4@xmath114 over the 1@xmath111 scheme on cpus is a factor of 1.8 and 1.4 for rf and ewald electrostatics , respectively .",
    "this speedup is mainly due to higher achieved pair rate , but the smaller buffer also contributes . assuming the 1@xmath111 search cost can be brought down to twice the force calculation cost , the total performance improvement including the search cost is a factor of 2.0 and 1.5 for rf and ewald electrostatics , respectively .",
    "the 8@xmath114 scheme results in a lower algorithmic work - efficiency due to the increase in the ratio of zero interactions calculated .",
    "note that these results are for a cut - off of 1 nm or 210 non - zero pairs per particle . with increasing cut - off radius , the efficiency increases and the performance improvement approaches a factor of 3 .",
    "as we run the pair search on the , slower , cpu , a longer list update interval often provides better total performance .",
    "the gpu kernels use a conditional for skipping pairs beyond the cut - off , unlike the cpu @xmath8 kernels , which use masking .",
    "therefore the pair - rate increases with buffer size . but calculating more pair distances always decreases the effective performance .",
    "still , the cluster algorithm demonstrates the potential of streaming architectures with an effective performance of a factor of 5 and 7 higher than the 4@xmath114 cpu rf and ewald kernels , respectively .    [ cols=\"^,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]",
    "for calculating non - bonded interactions in molecular simulations , the standard particle - based pair interaction algorithms commonly simd - parallelized by loop unrolling have reached their limits .",
    "kernels based on these approaches are often limited by the high memory to arithmetic operation ratio , the number of data shuffle operations required , and restrictions in instruction scheduling which reduces the potential for memory latency hiding .",
    "the pair list construction is affected by the same issues .",
    "we have presented a simple and flexible approach to overcome these problems .",
    "a scheme using cluster pairs of @xmath0 versus @xmath1 atoms leads to kernels that efficiently utilize current cpu and gpu simd units .",
    "the memory pressure is reduced by a factor @xmath0 on cpus .",
    "we found that @xmath0=4 usually provides the best performance . on gpus",
    "we use @xmath0=8 and the memory pressure is reduced by another factor of 4 by loading and operating on up to 8 @xmath9-clusters at once .",
    "the algorithm reorganizes the data representation at the lowest , particle level .",
    "therefore , any method in the literature that applies to particles , can be applied to the clusters in our method .",
    "an example used here is the verlet buffer .",
    "while the widely used linked cell list for reducing the search space can be applied , this does not offer any advantage as the locality of the clusters is already available through the grid used to generate the clusters .",
    "the performance advantage of our method over traditional algorithms depends on the computational cost of the interactions , the number of particle pairs within the cut - off and the simd width . while in many cases our cluster - based algorithm significantly outperforms the particle - based algorithms , in some cases it can be less advantageous . for cheap interactions the reduction of shuffling and memory operations will favor the cluster setup , whereas for expensive interactions the extra zero interactions can outweigh the gains .    for typical atomistic molecular simulations our method performs very well and is a factor 1.5 to 3 faster on 8-wide simd than traditional methods .",
    "on intel sandy bridge cpus as well as cuda gpus the flop rate is above 60% of the peak .",
    "most importantly ,",
    "our scheme inherently maps well to future cpu and gpu architectures as well as existing ones not discussed here .",
    "as the number of floating point operation per load / store operation can be tuned , a reduction of the arithmetic cycles per kernel , e.g. by introduction of fma instructions , will result in higher performance . additionally , wider simd units , for example 16-way simd in intel xeon phi , can be used efficiently with a limited amount of effort .",
    "this work was supported by the european research council ( grants nr . 258980 and nr . 209825 ) , the swedish e - science research center and the scalalife eu fp7 project .",
    "the authors thank erik lindahl for providing the analytical approximation of the ewald correction force and for his advice on x86 simd optimization , nvidia for advice on cuda optimization and mark abraham for thoroughly reviewing the code and this manuscript .",
    "for a canonical ensemble , an upper bound on the average energy drift due to the finite verlet buffer size can be derived .",
    "this depends on the atomic displacements and the shape of the potential at the cut - off .",
    "the displacement - distribution along one dimension for a freely moving particle with mass @xmath39 over time @xmath40 at temperature @xmath41 is gaussian with zero mean and variance @xmath42 .",
    "the variance of the distance between two non - interacting particles is @xmath43 . in practice , particles interact with each other over time @xmath40 .",
    "these interactions make the displacement distribution narrower , since any interaction will hinder free motion of particles .",
    "ignoring the effect of interactions on the displacements thus provides an upper bound .",
    "we calculate interactions with a non - bonded interaction cut - off distance of @xmath44 and a pair list cut - off of @xmath45 , where @xmath46 is the verlet buffer size .",
    "we can then write the average energy drift over time @xmath40 for pair interactions between a particle of type 1 surrounded by particles of type 2 with number density @xmath47 , when the inter - particle distance changes from @xmath48 to @xmath49 , as :    @xmath50 g\\!\\left(\\frac{r_t - r_0}{\\sigma}\\right ) d r_0 \\ , d r_t\\\\ & \\approx & 4 \\pi ( r_\\ell+\\sigma)^2 \\rho_2 \\int_{-\\infty}^{r_c } \\int_{r_\\ell}^\\infty \\big [ v'(r_c ) ( r_t - r_c ) + \\\\ & & \\phantom{4 \\pi ( r_\\ell+\\sigma)^2 \\rho_2 \\int_{-\\infty}^{r_c } \\int_{r_\\ell}^\\infty \\big [ } v''(r_c)\\frac{1}{2}(r_t - r_c)^2 \\big ] g\\!\\left(\\frac{r_t - r_0}{\\sigma}\\right ) d r_0 \\ , d r_t\\\\ & = & 4 \\pi ( r_\\ell+\\sigma)^2 \\rho_2 \\bigg\\ { \\frac{1}{2}v'(r_c)\\left[r_b \\sigma g\\!\\left(\\frac{r_b}{\\sigma}\\right ) - ( r_b^2+\\sigma^2)e\\!\\left(\\frac{r_b}{\\sigma}\\right ) \\right ] + \\\\ & & \\phantom{4 \\pi ( r_\\ell+\\sigma)^2 \\rho_2 \\bigg\\ { } \\frac{1}{6}v''(r_c)\\left [ \\sigma(r_b^2+\\sigma^2)g\\!\\left(\\frac{r_b}{\\sigma}\\right ) - r_b(r_b^2 + 3\\sigma^2 ) e\\!\\left(\\frac{r_b}{\\sigma}\\right ) \\right ] \\bigg\\}.\\end{aligned}\\ ] ]    here , @xmath51 is a gaussian distribution with zero mean , unit variance , and @xmath52 .",
    "we always want to achieve small energy drift , so @xmath53 will be small compared to both @xmath44 and @xmath54 .",
    "thus , the approximations in the above equations are good since the gaussian distribution decays rapidly . to calculate the total energy drift",
    ", the drift needs to be averaged over all particle pairs and weighted with the particle count .",
    "24 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , ( ) .",
    ", , ( ) . , ( ) . , , , , , , , , , , ( ) . , , , ( ) . , , , , ( ) . , , , , ( ) . , , in : . , , , , in : . , , , , , ( ) . , , , , , , , , , ( ) . , , , , ( ) . , , , ( ) . , , , , , , ( ) . , , , , , , ( ) .",
    ", , , , , , , , , , , , ( ) . , ( ) .",
    ", , , ( ) . , , ( ) .",
    ", , , ( ) . , , , , , , , , ( ) ."
  ],
  "abstract_text": [
    "<S> calculating interactions or correlations between pairs of particles is typically the most time - consuming task in particle simulation or correlation analysis . </S>",
    "<S> straightforward implementations using a double loop over particle pairs have traditionally worked well , especially since compilers usually do a good job of unrolling the inner loop . in order to reach high performance on modern cpu and accelerator architectures , single - instruction multiple - data ( simd ) parallelization </S>",
    "<S> has become essential . </S>",
    "<S> avoiding memory bottlenecks is also increasingly important and requires reducing the ratio of memory to arithmetic operations . </S>",
    "<S> moreover , when pairs only interact within a certain cut - off distance , good simd utilization can only be achieved by reordering input and output data , which quickly becomes a limiting factor . here </S>",
    "<S> we present an algorithm for simd parallelization based on grouping a fixed number of particles , e.g. 2 , 4 , or 8 , into spatial clusters . calculating all interactions between particles in a pair of such clusters </S>",
    "<S> improves data reuse compared to the traditional scheme and results in a more efficient simd parallelization . </S>",
    "<S> adjusting the cluster size allows the algorithm to map to simd units of various widths . </S>",
    "<S> this flexibility not only enables fast and efficient implementation on current cpus and accelerator architectures like gpus or intel mic , but it also makes the algorithm future - proof . </S>",
    "<S> we present the algorithm with an application to molecular dynamics simulations , where we can also make use of the effective buffering the method introduces .    </S>",
    "<S> * notice : * this is the author s version of a work that was accepted for publication in computer physics communications . </S>",
    "<S> changes resulting from the publishing process , such as peer review , editing , corrections , structural formatting , and other quality control mechanisms may not be reflected in this document . </S>",
    "<S> changes may have been made to this work since it was submitted for publication . </S>"
  ]
}