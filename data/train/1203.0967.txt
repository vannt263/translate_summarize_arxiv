{
  "article_text": [
    "principal components analysis ( pca ) is a widely used technique in reducing dimensionality of multivariate data . a traditional setting where pca is applicable",
    "involves repeated observations from a multivariate normal distribution .",
    "two key theoretical questions are : _",
    "i ) what is the relation between the sample eigenvectors and the population ones ? and ii )",
    "how well can population eigenvectors be estimated under various sparsity assumptions ?",
    "_ when the dimension @xmath9 of the observations is fixed and the sample size @xmath10 increases to infinity , the asymptotic properties of the sample eigenvalues and eigenvectors are well - known @xcite .",
    "most of this asymptotic analysis is based on the fact that the sample covariance approximates well the population covariance when the sample size is large .",
    "however , it is increasingly common to encounter statistical problems where the dimensionality of the observations is of the same order of magnitude as ( or even bigger than ) the sample size . in such cases , the sample covariance matrix , in general , is not a reliable estimate of the population covariance matrix .    to overcome this curse of dimensionality",
    ", several works studied the estimation of the population covariance matrix , under various models of sparsity .",
    "these include the development of banding and thresholding schemes @xcite , and analysis of their rate of convergence in the spectral norm .",
    "more recent works , such as @xcite and @xcite established the minimax rate of convergence under the matrix @xmath11 norm and the spectral norm , and its dependence on the assumed sparsity level .",
    "in contrast to these works , that studied estimation of the population covariance matrix , in this paper we consider a related but different problem , namely , the estimation of its leading eigenvectors .",
    "the interest in comparing these two problems is partially due to the fact that , when the population covariance is a low rank perturbation of the identity , which is a primary focus of this paper , sparsity of the eigenvectors corresponding to the non - unit eigenvalues implies sparsity of the whole covariance .",
    "note that consistency of an estimator of the whole covariance matrix also implies convergence of its leading eigenvalues to their population counterparts .",
    "if the gaps between the neighboring distinct eigenvalues remain bounded away from zero , it also implies convergence of the corresponding eigen - subspaces @xcite .",
    "moreover , for population eigenvalues with multiplicity one and gaps with neighboring eigenvalues bounded away from zero , the upper bounds for the whole covariance estimation under the spectral norm , derived in @xcite and @xcite , also yield an upper bound on the rate of convergence of the corresponding eigenvectors under the @xmath0 loss .",
    "these works , however , did not study the following fundamental problem , considered in this paper : _ how well can the leading eigenvectors be estimated , namely , what are the minimax rates for eigenvector estimation ? _",
    "we formulate this eigenvector estimation problem under the well - studied `` spiked population model '' which assumes that    * the eigenvalues of the population covariance matrix @xmath12 are @xmath13 for some @xmath14 , where @xmath15 and @xmath16 .",
    "this is a standard model in several scientific fields , including for example array signal processing ( e.g. see @xcite ) where the observations are modeled as the sum of an @xmath17-dimensional random signal and an independent , isotropic noise .",
    "it also arises as a latent variable model for multivariate data , for example in factor analysis @xcite .",
    "the assumption that the leading @xmath17 eigenvalues are distinct is made to simplify the analysis , as it ensures that the corresponding eigenvectors are identifiable up to a sign change .",
    "the assumption that all remaining eigenvalues are equal is not crucial as our analysis can be generalized to the case when these are only bounded by @xmath18 .",
    "asymptotic properties of the eigenvalues and eigenvectors of the sample covariance matrix under this model , in the setting when @xmath19 as @xmath20 , have been studied by @xcite , @xcite , @xcite and @xcite , among others .",
    "a conclusion of these studies is that when @xmath21 , the eigenvectors of standard pca are inconsistent estimators of the population eigenvectors .    in analogy to the sparse covariance estimation setting , several works considered various models of sparsity for the leading eigenvectors and developed improved sparse estimators . for example @xcite and @xcite , among others , imposed @xmath11-type sparsity constraints directly on the eigenvector estimates and proposed optimization procedures for obtaining them .",
    "@xcite suggested a regularized low rank approach to sparse pca .",
    "the consistency of the resulting leading eigenvectors was recently proven in @xcite , using a formulation of sparsity in which the sample size @xmath10 is fixed while @xmath22 .",
    "@xcite suggested a semi - definite programming ( sdp ) problem as a relaxation to the @xmath23-penalty for sparse @xmath12 . assuming a single spike",
    ", @xcite studied the asymptotic properties of the leading eigenvector of the covariance estimator obtained by @xcite , in the joint limit as both sample size and dimension tend to infinity .",
    "specifically , @xcite considered a leading eigenvector with exactly @xmath24 nonzero entries all of the form @xmath25 . for this hardest subproblem in the @xmath26-sparse @xmath23-ball , @xcite first derived information theoretic lower bounds , and then , under the assumption that the sdp problem has a rank one solution , proved that it attains the optimal rate of convergence .    in this paper , in contrast , following @xcite we study the estimation of the leading eigenvectors of @xmath12 assuming that these are approximately sparse , with a bounded @xmath27 norm . under this model",
    ", @xcite developed an estimation procedure based on coordinate selection by thresholding the diagonal of the sample covariance matrix , followed by the spectral decomposition of the submatrix corresponding to the selected coordinates .",
    "@xcite further proved consistency of this estimator assuming dimension grows at most polynomially with sample size , but did not study its convergence rate .",
    "since this estimation procedure is considerably simpler to implement and computationally much faster than the @xmath11 penalization procedures cited above , it is of interest to understand its theoretical properties .",
    "more recently , @xcite developed a related scheme named itspca ( iterative thresholding sparse pca ) which is based on repeated application of filtering , thresholding and orthogonalization steps that result in sparse estimators of the subspaces spanned by the leading eigenvectors .",
    "he also proved consistency and derived rates of convergence of the proposed estimator under appropriate loss functions and sparsity assumptions .    in this paper , which is partly based on the ph.d .",
    "thesis @xcite and @xcite , we study the estimation of the leading eigenvectors of @xmath12 within the framework of @xcite , but with an arbitrary number of spikes ( i.e. , @xmath14 ) whose corresponding eigenvectors all belong to appropriate @xmath27 spaces .",
    "our analysis thus extends the setting studied in @xcite and complements the work of @xcite that considered the @xmath23-sparsity setting . for simplicity",
    ", we assume gaussian observations in our analysis .",
    "however , up to multiplicative constants , the bounds on the minimax rate reported in this paper continue to hold under a relaxed assumption of sub - gaussian tail behavior for the probability distributions of the random variables .",
    "the main contributions of this paper are as follows .",
    "first , we establish lower bounds on the rate of convergence of the minimax risk for any eigenvector estimator under the @xmath0 loss .",
    "this analysis points to three different regimes of sparsity , which we denote as _ dense , sparse , and ultra - sparse _ , each having a different rate of convergence .",
    "we show that in the `` dense '' setting ( as defined in section [ sec : lower_bound ] ) , the standard pca estimator attains the optimal rate of convergence , whereas in sparse settings it is not even consistent .",
    "next , we show that while the diagonal thresholding scheme of @xcite is consistent under these sparsity assumptions , in general , it is not rate optimal .",
    "this motivates us to propose a new method ( augmented sparse pca , or aspca ) for estimating the eigenvectors that is based on a two - stage coordinate selection scheme , and is a refinement of the thresholding scheme of @xcite .",
    "while beyond the scope of this paper , it is possible to show that in the ultra - sparse setting , both our aspca procedure , as well as the method of @xcite achieve the lower bound on the minimax risk obtained by us , and are thus rate - optimal procedures .",
    "there is an intermediate region where a gap exists between the current lower bound and the upper bound on the risk .",
    "it is an open question whether the lower bound can be improved in this scenario , or a better estimator can be derived .",
    "table [ t : comparison ] provides a comparison of the lower bounds and rates of convergence of various estimators .",
    "the theoretical results also show that under comparable scenarios , the optimal rate of convergence for eigenvector estimation , @xmath28 ( under squared - error loss ) is faster than the optimal rate for covariance estimation , @xmath29 ( under squared operator norm loss ) , as obtained by @xcite and @xcite . finally , we emphasize that to obtain good finite - sample performance for both our two - stage scheme , as well as for other thresholding methods , the exact thresholds need to be carefully tuned . this issue and",
    "the detailed theoretical analysis of the aspca estimator is beyond the scope of this paper , and will be presented in a future publication .",
    "after this paper was completed , we learned of @xcite , which cites @xcite and contains results overlapping with some of the work of @xcite and this paper .",
    "the rest of the paper is organized as follows . in section [ sec : model ] , we describe the model for the eigenvectors and analyze the risk of the standard pca estimator . in section [ sec : lower_bound ] , we present the lower bounds on the minimax risk of any eigenvector estimator .",
    "in section [ sec : diagonal_thresh ] , we derive a lower bound on the risk of the diagonal thresholding estimator proposed by @xcite . in section [ sec : two_stage ] , we propose a new estimator named aspca ( augmented sparse pca ) that is a refinement of the diagonal thresholding estimator . in section [ sec : summary ] , we discuss the question of attainment of the risk bounds .",
    "proofs of the results are given in section [ sec : proofs ] in the appendix .",
    ".comparison of lower bounds on eigenvector estimation and worst case rates of various procedures . [ cols=\"<,^,^,^\",options=\"header \" , ]     [ t : comparison ]",
    "first we introduce certain notations . throughout , @xmath30 denotes the unit sphere in @xmath31 centered at the origin , @xmath32 denotes the largest integer less than or equal to @xmath33 .",
    "let @xmath34 be a triangular array , where for each @xmath10 , the @xmath35 random vectors @xmath36 are independent and identically distributed on a common probability space .",
    "throughout we assume that @xmath37 s are i.i.d .",
    "as @xmath38 , where the population matrix @xmath12 is a finite rank perturbation of ( a multiple of ) the identity .",
    "in other words , @xmath39 where @xmath40 , and the vectors @xmath41 are orthonormal , which implies ( * ) .",
    "@xmath42 is the eigenvector of @xmath12 corresponding to the @xmath43-th largest eigenvalue , namely , @xmath44 .",
    "the term `` finite rank '' means that @xmath17 remains fixed even as @xmath45 .",
    "the asymptotic setting involves letting both @xmath10 and @xmath9 grow to infinity simultaneously .",
    "for simplicity , we assume that the @xmath46 s are fixed while the parameter space for the @xmath42 s varies with @xmath9 .    the observations can be described in terms of the model @xmath47 here , for each @xmath10 , @xmath48 , @xmath49 are i.i.d .",
    "since the eigenvectors of @xmath12 are invariant to a scale change in the original observations , it is assumed that @xmath51 .",
    "hence , @xmath52 in the asymptotic results should be replaced by @xmath53 when ( [ eq : sigma_basic ] ) holds with an arbitrary @xmath54 .",
    "since the main focus of this paper is estimation of eigenvectors , without loss of generality we consider the uncentered sample covariance matrix @xmath55 , where @xmath56 $ ] .",
    "the following condition , termed _ basic assumption _ , will be used throughout the asymptotic analysis , and will be referred to as * ba*.    * ( [ eq : basic ] ) holds with @xmath51 ; @xmath57 as @xmath45 ; @xmath58 are fixed ( do not vary with @xmath9 ) , where @xmath17 is unknown but fixed .",
    "given data @xmath59 , the goal is to estimate @xmath17 and the eigenvectors @xmath60 . for simplicity , to derive the lower bounds , we first assume that @xmath17 is known . in section [ subsec : aspca - m_hat ]",
    "we derive an estimator of @xmath17 , which can be shown to be consistent under the assumed sparsity conditions . to assess the performance of any estimator , a minimax risk analysis approach is proposed .",
    "the first task is to specify a loss function @xmath61 between the estimated and true eigenvector .",
    "since the model is invariant to sign changes of each @xmath42 , we consider the following loss function , also invariant to sign changes .",
    "@xmath62 where @xmath63 and @xmath64 are @xmath35 vectors with unit @xmath0 norm .",
    "an estimator @xmath65 is called consistent with respect to @xmath66 , if @xmath67 in probability as @xmath45 .",
    "we first consider the asymptotic risk of the leading eigenvectors of the sample covariance matrix ( henceforth referred to as the standard pca estimators ) when the ratio @xmath68 is small .",
    "specifically , it is assumed that @xmath69 as @xmath45 .    for future use , we define @xmath70 and @xmath71 in @xcite ( theorem 1 ) it was shown that under a single spike model , as @xmath72 , the standard pca estimator of the leading eigenvector is consistent .",
    "the following result , proven in the appendix , is a refinement of that , as it also provides the leading error term .",
    "[ thm : opca_risk_bound ] let @xmath73 be the eigenvector corresponding to the @xmath43-th largest eigenvalue of @xmath74 .",
    "assume that * ba * holds and @xmath75 such that @xmath69 , and moreover , @xmath76 .",
    "then , for each @xmath77 , @xmath78(1+o(1)).\\ ] ]    observe that theorem [ thm : opca_risk_bound ] does not assume any special structure ( e.g. , sparsity ) for the eigenvectors .",
    "the first term on the rhs of ( [ eq : opca_risk_bound ] ) is a nonparametric component which arises from the interaction of the noise terms with the different coordinates , while the second term is a parametric component which results from the interaction with the remaining @xmath79 eigenvectors corresponding to different eigenvalues .",
    "the second term shows that the closer the successive eigenvalues are , the larger is the estimation error .",
    "the upshot of ( [ eq : opca_risk_bound ] ) is that standard pca provides a consistent estimator of the leading eigenvectors of the population covariance matrix when the dimension - to - sample - size ratio ( @xmath68 ) is asymptotically negligible .      as shown by various authors @xcite , when @xmath80 $ ] , standard pca provides inconsistent estimators for the population eigenvectors .",
    "in this subsection we consider the following model for approximate sparsity of the eigenvectors . for each @xmath77 , we assume that @xmath42 belongs to an @xmath27 ball with radius @xmath81 , for some @xmath82 .",
    "specifically , we assume that @xmath83 , where @xmath84 note that our condition of sparsity is slightly different from that of @xcite .",
    "note that since @xmath85 , for @xmath86 to be nonempty , one needs @xmath87 .",
    "further , if @xmath88 , then the space @xmath86 is all of @xmath30 because in this case , the least sparse vector @xmath89 is in the parameter space .    the parameter space for @xmath90 $ ] is denoted by @xmath91 where @xmath86 is defined through ( [ eq : theta_q_c ] ) , and @xmath92 for all @xmath77 .",
    "[ rem : covariance_sparsity ] while our focus is on eigenvector sparsity , condition ( [ eq : theta_q_m ] ) also implies sparsity of the covariance matrix itself . in particular , for @xmath93 , a spiked covariance matrix satisfying ( [ eq : theta_q_m ] ) also belongs to the class of sparse covariance matrices analyzed by @xcite , @xcite and @xcite .",
    "indeed , @xcite obtained the minimax rate of convergence for covariance matrix estimators under the spectral norm when the rows of the population matrix satisfy a weak-@xmath27 constraint .",
    "however , as we will show below , the minimax rate for estimation of the leading eigenvectors is faster than that for covariance estimation .",
    "we now derive lower bounds on the minimax risk of estimating @xmath42 under the loss function ( [ eq : loss_fn ] ) . to aid in describing and interpreting the lower bounds , we define the following two auxiliary parameters .",
    "the first is an _ effective noise level per coordinate _ @xmath94 and the second is an _ effective dimension _",
    "@xmath95 where @xmath96 , @xmath97 and @xmath98 and @xmath99 .",
    "the phrase _ effective noise level per coordinate _ is motivated by the risk bound in theorem [ thm : opca_risk_bound ] , since dividing both sides of ( [ eq : opca_risk_bound ] ) by @xmath9 , the expected `` per coordinate '' risk ( or variance ) of the pca estimator is asymptotically @xmath100 .",
    "next , following @xcite , let us provide a different interpretation of @xmath101 .",
    "consider a sparse @xmath42 and an oracle that , regardless of the observed data , selects a set @xmath102 of all coordinates of @xmath42 that are larger than @xmath103 in absolute value , and then performs pca on the sample covariance restricted to these coordinates .",
    "since @xmath104 , the maximal squared - bias is @xmath105 which follows by the correspondence @xmath106 , and the convexity of the function @xmath107 . on the other hand , by theorem [ thm : opca_risk_bound ] ,",
    "the maximal variance term of this oracle estimator is of the order @xmath108 where @xmath109 is the maximal number of coordinates of @xmath42 exceeding @xmath103 .",
    "again , @xmath110 implies that @xmath111 .",
    "thus , to balance the bias and variance terms , we need @xmath112 .",
    "this heuristic analysis shows that @xmath101 can be viewed as an _ oracle threshold _ for the coordinate selection scheme , i.e. , the best possible estimator of @xmath42 based on individual coordinate selection can expect to recover only those coordinates that are above the threshold @xmath101 .    to understand why @xmath113 is an _ effective dimension _ , consider the least sparse vector @xmath114 .",
    "this vector should have as many nonzero coordinates of equal size as possible .",
    "if @xmath115 then the vector with coordinates @xmath116 does the job . otherwise , we set the first coordinate of the vector to be @xmath117 for some @xmath118 and choose all the nonzero coordinates to be of magnitude @xmath101 .",
    "clearly , we must have @xmath119 , where @xmath120 is the maximal number of nonzero coordinates , while the @xmath27 constraint implies that @xmath121 .",
    "the last inequality shows that the maximal @xmath122 is just a constant multiple of @xmath113 .",
    "this construction also constitutes the key idea in the proof of theorems [ th : three - way - lower ] and [ th : sparse - lower ] . finally , we set @xmath123 where the origin of @xmath124 will be explained in the proof .",
    "we may think of @xmath131 as the effective dimension of the least favorable configuration . in",
    "the _ sparse _ setting , @xmath132^{q/2 } < c_1 n$ ] ( i.e. , @xmath133 for some @xmath134 ) , and the lower bound is of the order @xmath135^{1 - q/2 } }              \\asymp \\frac{c_\\nu^q}{n^{1-q/2}}~.\\ ] ] on the other hand , in the _ dense _ setting , @xmath136 .",
    "if @xmath137 for some @xmath138 , then @xmath139 , and so any estimator of the eigenvector @xmath42 is inconsistent . if @xmath69 then the lower bound is @xmath140 eq .",
    "( [ eq : dense_rate ] ) and theorem [ thm : opca_risk_bound ] imply that in the dense setting with @xmath141 , the standard pca estimator @xmath73 attains the optimal rate of convergence .",
    "a sharper lower bound is possible in what we call an _ ultra - sparse _ setting which happens if @xmath142 for some @xmath143 . in this case",
    "the dimension @xmath9 is much larger than the quantity @xmath144 measuring the effective dimension . hence , we define a modified effective noise level per - coordinate @xmath145 and a modified effective dimension @xmath146    [ th : sparse - lower ] assume that * ba * holds , @xmath125 , and @xmath147 such that @xmath148 for some @xmath143",
    ". then , assuming that @xmath149 for @xmath10 sufficiently large , the minimax bound holds with @xmath150}\\ ] ]    note that in the ultra - sparse setting @xmath129 is larger by a factor of @xmath151 compared to the sparse setting , eq .",
    "( [ eq : sparse_rate ] ) .",
    "in this section , we analyze the convergence rate of the spca scheme ( henceforth referred to as the diagonal thresholding or d.t .",
    "scheme ) proposed by @xcite . in this section and in section",
    "[ sec : two_stage ] , we assume for simplicity that @xmath152 . let the sample variance of the @xmath26-th coordinate ( i.e. , the @xmath26-th diagonal entry of @xmath74 ) be denoted by @xmath153 . then the d.t . scheme consists of the following steps .    1 .",
    "define @xmath154 to be the set of indices @xmath155 such that @xmath156 for some threshold @xmath157 .",
    "2 .   let @xmath158 be the submatrix of @xmath74 corresponding to the coordinates @xmath159 .",
    "perform an eigen - analysis of @xmath158 .",
    "denote the eigenvectors by @xmath160 .",
    "3 .   for @xmath77 ,",
    "estimate @xmath42 by the @xmath161 vector @xmath162 , obtained from @xmath163 by augmenting zeros to all the coordinates in @xmath164 .",
    "assuming that @xmath114 , @xcite showed that the d.t .",
    "scheme with a threshold of the form @xmath165 for some @xmath166 leads to a consistent estimator of @xmath42 .",
    "the risk of this estimator , however , was not analyzed in @xcite . as we prove below , the risk of the d.t .",
    "estimator is not rate optimal .",
    "this can be anticipated from the lower bound on the minimax risk ( theorems [ th : three - way - lower ] and [ th : sparse - lower ] ) which indicate that to attain the optimal risk , a coordinate selection scheme must select all coordinates of @xmath42 of size at least @xmath167 . with a threshold of the form @xmath168 above , however , only coordinates of size @xmath169 are selected .",
    "as shown in the following theorem , even for the case of a single signal ( @xmath170 ) this leads to a much larger lower bound .",
    "[ thm : diagonal_thresholding_risk ] suppose that * ba * holds with @xmath171 .",
    "let @xmath172 , @xmath125 , and @xmath173 be such that @xmath174",
    ". then the diagonal thresholding estimator @xmath175 proposed by @xcite satisfies , for any @xmath82 , @xmath176 for a constant @xmath177 , where @xmath178 .    comparing ( [ eq : dt_risk ] ) with the lower bound ( [ eq : sparse_rate ] ) , shows the large gap between the two rates , @xmath179 vs. @xmath180 .",
    "the reason for this difference is that the d.t .",
    "scheme uses only the diagonal of the sample covariance matrix @xmath181 , ignoring the information in its off - diagonal entries . in the next section",
    "we propose a refinement of the d.t .",
    "scheme , denoted aspca , that constructs an improved eigenvector estimate using all entries of @xmath181 .",
    "as discussed above , the dt scheme can reliably detect only those eigenvector coordinates @xmath182 , whereas to reach the lower bound one needs to detect those coordinates of size @xmath183 .    to motivate an improved coordinate selection scheme , consider a partition of the @xmath9 coordinates into two sets @xmath184 and @xmath185 , where the former contains all those @xmath26 such that @xmath186 is `` large '' ( selected by the d.t .",
    "scheme ) , and the latter contains the remaining smaller coordinates .",
    "partition the matrix @xmath12 as @xmath187 observe that , @xmath188 .",
    "let @xmath189 be a `` preliminary '' estimator of @xmath190 such that @xmath191 for some @xmath192 ( e.g. , @xmath189 could be the d.t .",
    "estimator ) .",
    "then we have the relationship , @xmath193 for some @xmath194 bounded below by @xmath195 , say .",
    "thus , one possible strategy is to additionally select all those coordinates of @xmath196 that are larger ( in absolute value ) than some constant multiple of @xmath197 . in practice",
    "we do not know @xmath198 or @xmath199 but we can use @xmath200 as a surrogate for the former and the largest eigenvalue of @xmath201 to obtain an estimate for the latter . a technical challenge is to show , that with probability tending to 1 , such a scheme indeed recovers all coordinates @xmath26 with @xmath202 , while discarding all coordinates @xmath26 with @xmath203 for some constants @xmath204 .",
    "figure 1 provides a pictorial description of the d.t . and aspca coordinate coordinate selection schemes .",
    "[ fig : thresholding_schemes ]     while the thresholds for the aspca scheme is @xmath205 .",
    "the schemes select the coordinates above the upper limits ( indicated by the multiplier @xmath206 ) and discard the coordinates below the lower limits ( indicated by multiplier @xmath207 ) with high probability . here ,",
    "@xmath208 are generic constants.,width=480,height=384 ]      based on the ideas described above , we now present the aspca algorithm .",
    "it first makes two stages of coordinate selection , whereas the final stage consists of an eigen - analysis of the submatrix of @xmath74 corresponding to the selected coordinates .",
    "the algorithm is described below .    for any @xmath166 define @xmath209 let @xmath210 for @xmath211 and @xmath212 be constants to be specified later .",
    "*   * let @xmath213 where @xmath214 .",
    "* denote the eigenvalues and eigenvectors of @xmath158 by @xmath215 and @xmath216 respectively , where @xmath217 , * estimate @xmath17 by @xmath218 defined in section [ subsec : aspca - m_hat ] .",
    "*   * let @xmath219 $ ] and @xmath220 * let @xmath221 for some @xmath222 .",
    "define @xmath223 . *",
    "* for @xmath224 , denote by @xmath225 the @xmath43-th eigenvector of @xmath226 , augmented with zeros in the coordinates @xmath227 .",
    "[ rem : gamma_choice ] the aspca scheme is specified up to the choice of parameters @xmath228 and @xmath229 , that determine its rate of convergence . it can be shown that choosing @xmath230 , @xmath231 for some @xmath232 , and @xmath233 given by @xmath234 with @xmath235 results in an asymptotically optimal rate .",
    "again , we note that for finite @xmath9 , @xmath10 , the actual performance in terms of the risk of the resulting eigenvector estimate may have a strong dependence on the threshold . in practice ,",
    "a delicate choice of thresholds can be highly beneficial .",
    "this issue , as well as the analysis of the risk of the aspca estimator , are beyond the scope of this paper and will be studied in a separate publication .",
    "estimation of the dimension of the signal subspace is a classical problem .",
    "if the signal eigenvalues are strong enough ( i.e. , @xmath236 for all @xmath77 , for some @xmath237 independent of @xmath238 ) , then nonparametric methods that do not assume eigenvector sparsity can asymptotically estimate the correct @xmath17 ( see , e.g. @xcite ) .",
    "when the eigenvectors are sparse , we can detect much weaker signals , as we describe below .",
    "we estimate @xmath17 by thresholding the eigenvalues of the submatrix @xmath239 where @xmath240 for some @xmath241 .",
    "let @xmath242 and @xmath243 be the nonzero eigenvalues of @xmath239 .",
    "let @xmath244 be a user - defined threshold .",
    "then , define @xmath218 by @xmath245 it can be shown that under appropriate sparsity conditions , with a suitable choice of threshold @xmath246 , @xmath218 is a consistent estimator of @xmath17 .",
    "in this paper we derived lower bounds on eigenvector estimates under three different sparsity regimes , denoted dense , sparse , and ultra - sparse . in the _ dense _ setting , theorems [ thm : opca_risk_bound ] and [ th : three - way - lower ] show that when @xmath69 , the standard pca estimator attains the optimal rate of convergence . in the _ ultra - sparse _ setting , theorem 3.1 of @xcite",
    "shows that the maximal risk of the itspca estimator proposed by him attains the same asymptotic rate as the corresponding lower bound of theorem [ th : sparse - lower ] .",
    "this implies that in the ultra - sparse setting , the lower bound on the minimax rate is indeed sharp . in a separate paper",
    ", we prove that in the ultra - sparse regime , the aspca algorithm also attains the minimax rate .",
    "finally , our analysis leaves some open questions in the intermediate sparse regime . according to theorem [ th : three - way - lower ] , the lower bound in this regime is smaller by a factor of @xmath151 , as compared to the ultra - sparse setting .",
    "therefore , whether there exists an estimator ( and in particular , one with low complexity ) , that attains the current lower bound , or whether this lower bound can be improved is an open question for future research .",
    "to prove theorem [ thm : opca_risk_bound ] , on the risk of the pca  estimator , we use the following lemmas .      in our analysis , we shall need a probabilistic bound for deviations of @xmath247 .",
    "this is given in the following lemma , proven in section [ sec : auxiliary_results ] .",
    "[ lemma : eigen_deviation_bound ] let @xmath248 where @xmath249 .",
    "let @xmath250 be an @xmath251 matrix with i.i.d .",
    ". then for any @xmath252 , there exists @xmath253 such that for all @xmath254 , @xmath255      the following lemma is due to @xcite .",
    "[ lemma : chi_square_large_dev ] let @xmath256 denote a chi - square random variable with @xmath10 degrees of freedom .",
    "then , @xmath257    the following lemma is from @xcite .",
    "[ lemma : quad_form_large_dev ] let @xmath258 be two sequences of mutually independent , i.i.d",
    ". @xmath50 random variables . then for large @xmath10 and any @xmath259 s.t .",
    "@xmath260 , @xmath261      the following lemma from @xcite is convenient for risk analysis of estimators of eigenvectors .",
    "several variants of this lemma appear in the literature , most based on the approach of @xcite .",
    "[ lemma : evec_perturb_bound ] let @xmath184 and @xmath185 be two symmetric @xmath262 matrices .",
    "let the eigenvalues of matrix @xmath184 be denoted by @xmath263 .",
    "set @xmath264 and @xmath265 .",
    "for any @xmath266 , if @xmath267 is a unique eigenvalue of @xmath184 , i.e. , if @xmath268 , then denoting by @xmath269 the eigenvector associated with the @xmath270-th eigenvalue , @xmath271 where @xmath272 and @xmath273 denotes the projection matrix onto the eigenspace @xmath274 corresponding to eigenvalue @xmath275 ( possibly multi - dimensional ) . define @xmath276 and @xmath277 as @xmath278 \\label{eq : eigen_delta_r } \\\\",
    "\\overline{\\delta}_r & = & \\frac{\\parallel b\\parallel } { \\min_{1\\leq j \\neq r \\leq m } |\\lambda_j(a ) - \\lambda_r(a)|}~.\\end{aligned}\\ ] ] then , the residual term @xmath279 can be bounded by @xmath280\\right\\}\\end{aligned}\\ ] ] where the second bound holds only if @xmath281 .",
    "[ rem : perturbation_bound ] we can simplify the bound on the perturbation in ( [ eq : eigenvec_error ] ) to show that if @xmath282 , then @xmath283 where we can take @xmath284 . to see this , note that @xmath285 and that @xmath286^{-1}$ ] , so that , @xmath287 now , defining @xmath288 and @xmath289 , we have @xmath290 , and the bound ( [ eq : eigenvec_error ] ) may be expressed as @xmath291 for @xmath292 , the function @xmath293 .",
    "further , if @xmath294 , then @xmath295 and so we conclude that @xmath296    for notational simplicity , throughout this subsection , we write @xmath297 to mean @xmath73 .",
    "recall that the loss function @xmath298 . invoking lemma [ lemma : evec_perturb_bound ] with @xmath299 and @xmath300",
    "we get @xmath301 where @xmath302 where @xmath303 .",
    "note that @xmath304 and that @xmath305 .",
    "the key quantity in bounding the error term @xmath306 is @xmath307 indeed , from ( [ eq : eigenvec_error_leading ] ) , when @xmath308 , we have , for some constant @xmath172 , @xmath309 set @xmath310 .",
    "we will show that as @xmath20 , @xmath311 with probability approaching 1 and @xmath312 theorem [ thm : opca_risk_bound ] then follows from an ( exact , non - asymptotic ) evaluation @xmath313 we begin with the evaluation of ( [ eq : expected_h_s_theta_nu ] ) .",
    "first we derive a convenient representation of @xmath314 . in matrix form ,",
    "model ( [ eq : basic ] ) becomes @xmath315 for @xmath77 , define @xmath316 define @xmath317 then we have @xmath318 using ( [ eq : z_w_nu_def ] ) , @xmath319 using ( [ eq : h_nu_sigma ] ) , @xmath320 for @xmath321 , and we arrive at the desired representation @xmath322 by orthogonality ,",
    "@xmath323 now we compute the expectation .",
    "one verifies that @xmath324 independently of each other and of each @xmath325 , so that @xmath326 independently .",
    "hence , for @xmath321 , @xmath327 from ( [ eq : z_w_nu_def ] ) , @xmath328 \\\\ &",
    "= & \\tr(\\mathbf{z}\\mathbf{z}^t p_\\perp \\mathbf{z}\\mathbf{z}^t \\theta_\\mu\\theta_\\mu^t ) + \\lambda_\\nu \\tr(p_\\perp \\mathbf{z}\\mathbf{z}^t).\\end{aligned}\\ ] ] now , it can be easily verified that if @xmath329 @xmath330 , then for arbitrary symmetric @xmath331 matrices @xmath332 , @xmath333 , we have , @xmath334 + n^2 \\tr(qr).\\ ] ] taking @xmath335 and @xmath336 , by ( [ eq : trace_quadratic ] ) we have @xmath337 = n \\tr(p_\\perp ) + n \\lambda_\\nu \\tr(p_\\perp ) = n(n - m)(1+\\lambda_\\nu).\\ ] ] combining ( [ eq : expected_w_mu_w_nu ] ) with ( [ eq : expected_w_nu_p_z_quad ] ) in computing the expectation of ( [ eq : h_nu_s_theta_nu_norm_squared ] ) , we obtain the expression ( [ eq : expected_h_s_theta_nu ] ) for @xmath338 .",
    "we begin with the decomposition of the sample covariance matrix @xmath74 .",
    "introduce the abbreviation @xmath340 .",
    "then , @xmath341 and hence @xmath342 where @xmath343 denotes the kronecker symbol .",
    "let @xmath344 be the intersection of all the events ( for some constant @xmath252 ) : @xmath345 since @xmath346 independent of @xmath250 , we have @xmath347 independently of @xmath348 , and @xmath349 .",
    "moreover , @xmath350 hence , we use lemmas [ lemma : chi_square_large_dev ] and [ lemma : quad_form_large_dev ] to prove that @xmath351 define @xmath352 to be be the event that @xmath353 with @xmath354 as in lemma [ lemma : eigen_deviation_bound ] with @xmath355 so that @xmath356 .",
    "lemma [ lemma : eigen_deviation_bound ] also establishes that @xmath357 . using the notation @xmath358",
    ", we have , on @xmath359 , @xmath360 recalling that @xmath361 for @xmath77 , we have for large @xmath10 that @xmath362 where , say @xmath363 .",
    "observe that @xmath364 .",
    "now , substitute ( [ eq : s_sigma_diff_prob_bound ] ) to conclude that there are functions @xmath365 such that on @xmath366 , @xmath367 our assumptions imply that @xmath368 so that @xmath369 . to summarize ,",
    "choose @xmath370 , say , so that on @xmath371 , which has probability at least @xmath372 , we have @xmath373 .",
    "this completes the proof of ( [ eq : l_theta_nu_hat_bounds ] ) .",
    "theorem [ thm : opca_risk_bound ] now follows from noticing that @xmath374 and so @xmath375 \\leq 2 \\mathbb{p}((d_1 \\cap d_2)^c ) = o(n_n^{-2 } ) = o(\\mathbb{e}\\parallel h_\\nu \\mathbf{s}\\theta_\\nu \\parallel^2),\\ ] ] and an additional computation using ( [ eq : h_nu_s_theta_nu_norm_squared ] ) which shows that @xmath376 \\leq ( \\mathbb{e}[\\parallel h_\\nu \\mathbf{s } \\theta_\\nu \\parallel^4)^{1/2 } p(d_n^c ) = o(\\mathbb{e}[\\parallel h_\\nu \\mathbf{s } \\theta_\\nu \\parallel^2).\\ ] ]      in this subsection , we prove theorems [ th : three - way - lower ] and [ th : sparse - lower ] . the key idea in",
    "the proofs is to utilize the geometry of the parameter space in order to construct appropriate finite dimensional subproblems for which bounds are easier to obtain .",
    "we first give an overview of the general machinery used in the proof .      a key tool for deriving lower bounds on the minimax risk",
    "fano s lemma_. in this subsection , we use superscripts on vectors @xmath377 as indices , not exponents .",
    "first , we construct a large finite subset @xmath378 of @xmath379 , such that the following property holds , for a given @xmath380 .    * if @xmath381 , then @xmath382 , for some @xmath383 ( to be chosen ) .",
    "this property will be referred to as `` @xmath384-distinguishability in @xmath42 '' . given any estimator @xmath385 of @xmath386 , based on data @xmath387 ,",
    "define a new estimator @xmath388 , whose @xmath17 components are given by @xmath389 , where @xmath225 is the @xmath43-th column of @xmath385 .",
    "then , by chebyshev s inequality and the @xmath384-distinguishability in @xmath42 , it follows that @xmath390 the task is then to find an appropriate lower bound for the quantity on the right hand side of ( [ eq : risk_lb_m ] ) . for this",
    ", we use the following version of fano s lemma , due to @xcite , modifying a result of @xcite ( p. 1570 - 71 ) .    [",
    "lemma : fano_lb_large ] let @xmath391 be a family of probability distributions on a common measurable space , where @xmath392 is an arbitrary parameter set .",
    "let @xmath393 be the minimax risk over @xmath394 with the loss function @xmath395 ,    @xmath396 where @xmath397 denotes an arbitrary estimator of @xmath377 with values in @xmath392 .",
    "then for any finite subset @xmath378 of @xmath392 , with elements @xmath398 where @xmath399 , @xmath400 where @xmath401 , and @xmath332 is an arbitrary probability distribution , and @xmath402 is the kullback - leibler divergence of @xmath332 from @xmath403 .",
    "the following lemma , proven in section [ sec : auxiliary_results ] , gives the kullback - leibler discrepancy corresponding to two different values of the parameter .",
    "[ prop : multi_kl_div ] let @xmath404 $ ] , @xmath405 be two parameters ( i.e. , for each @xmath406 , @xmath407 s are orthonormal ) .",
    "let @xmath408 denote the matrix given by ( [ eq : sigma_basic ] ) with @xmath409 ( and @xmath51 ) .",
    "let @xmath410 denote the joint probability distribution of @xmath10 i.i.d .",
    "observations from @xmath411 .",
    "then the kullback - leibler discrepancy of @xmath412 with respect to @xmath413 is given by @xmath414 .",
    "next , we describe the construction of a large set of hypotheses @xmath378 , satisfying the @xmath384 distinguishability condition . our construction is based on the well studied sphere packing problem , namely how many unit vectors can be packed onto @xmath415 , with given minimal pairwise distance between any two vectors .",
    "here we follow the construction due to @xcite ( p. 77 ) .",
    "let @xmath122 be a large positive integer , and @xmath416 .",
    "define @xmath417 as the maximal set of points of the form @xmath418 in @xmath415 such that the following is true : @xmath419 for any @xmath420 , the maximal number of points lying on @xmath415 such that any two points are at distance at least 1 , is called the _ kissing number _ of an @xmath122-sphere .",
    "@xcite uses the construction described above to derive a lower bound on the _ kissing number _ , by showing that @xmath421 for @xmath122 large .",
    "next , for @xmath422 we use the sets @xmath417 to construct our hypothesis set @xmath423 of same size , @xmath424 . to this end , let @xmath425 denote the standard basis of @xmath31 .",
    "our initial set @xmath426 is composed of the first @xmath17 standard basis vectors , @xmath427 $ ] . then , for fixed @xmath43 , and values of @xmath428 yet to be determined , each of the other hypotheses @xmath429 has the same vectors as @xmath430 for @xmath431 .",
    "the difference is that the @xmath43-th vector is instead given by @xmath432 where @xmath433 , @xmath434 , is an enumeration of the elements of @xmath417 .",
    "thus @xmath435 perturbs @xmath436 in subsets of the fixed set of coordinates @xmath437 , according to the sphere packing construction for @xmath415 .",
    "the construction ensures that @xmath438 are orthonormal for each @xmath406 .",
    "furthermore , ( [ eq : multi_kl_div ] ) simplifies to @xmath439 finally , by construction , for any @xmath440 with @xmath441 @xmath442 in other words , the set @xmath378 is @xmath443-distinguishable in @xmath42 .",
    "consequently , combining ( [ eq : risk_lb_m ] ) and ( [ eq : kl_div ] ) , @xmath444,\\ ] ] with @xmath445      let @xmath122 be an integer yet to be specified and let @xmath118 .",
    "let @xmath417 be the sphere - packing set defined above , and let @xmath423 be the corresponding set of hypotheses , defined via ( [ eq : theta_nu_j_f ] ) .",
    "let @xmath124 , then we have @xmath446 , where @xmath447 as @xmath448 .",
    "inserting the following value for @xmath449 , @xmath450 into eq .",
    "( [ eq : a_r_f_0 ] ) gives that @xmath451 therefore , so long as @xmath452 , an absolute constant , we have @xmath453 .",
    "we need to ensure that @xmath454 . since exactly @xmath455 coordinates are non - zero out of @xmath456 , @xmath457 where @xmath458 . a sufficient condition for @xmath459",
    "is that @xmath460 substituting ( [ eq : rdef ] ) puts this into the form @xmath461^{q/2}.\\ ] ]    to simultaneously ensure that ( i ) @xmath462 , ( ii ) @xmath122 does not exceed the number of available co - ordinates , @xmath463 , and ( iii ) @xmath454 , we set @xmath464 where @xmath465 . recalling the notations ( [ eq : tau - nu2 ] ) , ( [ eq : m - nu ] ) and ( [ eq : nprime ] ) , this becomes ( without loss of generality assuming @xmath466 and @xmath113 to be integers ) @xmath467 and theorem [ th : three - way - lower ] follows .      the construction of the set of hypotheses in the proof of theorem [ th : three - way - lower ] considered a fixed set of potential non - zero coordinates , namely @xmath468 .",
    "however , in the _ ultra - sparse _ setting , when the effective dimension is significantly smaller than the nominal dimension @xmath9 , it is possible to construct a much larger collection of hypotheses by allowing the set of non - zero coordinates to span all remaining coordinates @xmath469 .    in the proof of theorem [ th : sparse - lower ]",
    "we shall use the following lemma , proven in section [ sec : auxiliary_results ] .",
    "call @xmath470 an _ @xmath471set _ if @xmath472 .",
    "[ lemma : counting ] let @xmath26 be fixed , and let @xmath473 be the maximal collection of @xmath471sets such that the intersection of any two members has cardinality at most @xmath474 . then",
    ", necessarily , @xmath475 let @xmath476 + 1 $ ] and @xmath477 $ ] with @xmath478 suppose that @xmath479 with @xmath480",
    ". then @xmath481 ( 1 + o(1)).\\ ] ] where @xmath482 is the shannon entropy function , @xmath483    let @xmath484 be an @xmath471set contained in @xmath485 , and construct a family @xmath486 by modifying to use the set @xmath484 rather than the fixed set @xmath437 as in theorem [ th : three - way - lower ] : @xmath487 we will choose @xmath122 below to ensure that @xmath488 . let @xmath489 be a collection of sets @xmath484 such that , for any two sets @xmath484 and @xmath490 in @xmath489 , the set @xmath491 has cardinality at most @xmath492 .",
    "this ensures that the sets @xmath493 are disjoint for @xmath494 , since each @xmath495 is nonzero in exactly @xmath496 coordinates .",
    "this construction also ensures that @xmath497 define @xmath498",
    ". then @xmath499 by lemma [ lemma : counting ] , there is a collection @xmath489 such that @xmath500 is at least @xmath501(1+o(1)))$ ] . since @xmath502",
    ", it follows from ( [ eq : big_f_0_lower ] ) that , @xmath503 since @xmath504 . proceeding as for theorem [ th : three - way - lower ] , we have latexmath:[$\\log    @xmath447 .",
    "let us set ( with @xmath122 still to be specified ) @xmath506    again , we need to ensure that @xmath488 , which as before is implied by . substituting ( [ eq : rdef_refined ] ) puts this into the form @xmath507 to simultaneously ensure that ( i ) @xmath462 ; ( ii ) @xmath122 does not exceed the number of available co - ordinates , @xmath463 ; and ( iii ) @xmath454 , we set @xmath508 as @xmath509 , we have that @xmath510 , and theorem [ th : sparse - lower ] follows .      to prove theorem [ thm : diagonal_thresholding_risk ] , assume w.l.g . that @xmath511 , and decompose the loss as @xmath512 where @xmath154 is the set of coordinates selected by the d.t .",
    "scheme and @xmath513 denotes the subvector of @xmath190 corresponding to this set . note that , in ( [ eq : loss_decomp ] ) , the first term on the right can be viewed as a bias term while the second term can be seen as a variance term .",
    "we choose a particular vector @xmath514 so that @xmath515 this , together with ( [ eq : loss_decomp ] ) , proves theorem [ thm : diagonal_thresholding_risk ] since the worst case risk is clearly at least as large as ( [ eq : diagonal_thresh_bias ] ) .",
    "accordingly , set @xmath516 , where @xmath178 .",
    "since @xmath517 , we have @xmath518 , and so for sufficiently large @xmath10 , we can take @xmath519 and define @xmath520 where @xmath521 . then by construction @xmath522 , since @xmath523 where the last inequality is due to @xmath82 and @xmath524 .    for notational convenience , let @xmath525 .",
    "recall that d.t .",
    "selects all coordinates @xmath26 for which @xmath526 .",
    "therefore , coordinate @xmath26 is _ not selected _ with probability @xmath527 where @xmath528 .",
    "notice that , for @xmath529 , @xmath530 , and @xmath531 for @xmath532 .",
    "hence , @xmath533 thus , to finish the proof of theorem [ thm : diagonal_thresholding_risk ] , it is enough to show that @xmath534 for some @xmath535 that converges to 0 as @xmath20 . rewrite ( [ eq : dt_selection_prob ] ) as @xmath536",
    ", it follows that @xmath537 so that @xmath538 as @xmath20 .",
    "this , together with ( [ eq : large_dev_chisq_2 ] ) , shows that @xmath539 where we can choose @xmath540 .",
    "[ lemma : extreme_singval_concen ] let @xmath541 be a @xmath542 matrix of i.i.d . @xmath50 entries with @xmath543 .",
    "let @xmath544 and @xmath545 denote the largest and the smallest singular value of @xmath541 , respectively . then , @xmath546    we apply lemma [ lemma : extreme_singval_concen ] separately for @xmath547 and for @xmath548 . observe first that , @xmath549 consider first @xmath547 and let @xmath550 denote the maximum and minimum singular values of @xmath551 .",
    "define @xmath552 for @xmath553 . then , since @xmath554 , and letting @xmath555 we have @xmath556 now , applying lemma [ lemma : extreme_singval_concen ] with @xmath557 and @xmath558 , we get @xmath559 we observe that @xmath560 now consider @xmath548 . noting that @xmath561 , we have @xmath562 this time , let @xmath563 and @xmath564 .",
    "we apply lemma [ lemma : extreme_singval_concen ] with @xmath565 , @xmath566 , so that @xmath567 and observe that @xmath568 thus from ( [ eq : delta_n_t ] ) and ( [ eq : delta_n_t ] ) , we have @xmath569 now choose @xmath570 so that tail probability is at most @xmath571 . the result is now proved , since if @xmath572 then @xmath573 .",
    "recall that , if distributions @xmath574 and @xmath575 have density functions @xmath576 and @xmath577 , respectively , such that the support of @xmath576 is contained in the support of @xmath577 , then the kullback - leibler discrepancy of @xmath575 with respect to @xmath574 , to be denoted by @xmath578 , is given by @xmath579 for @xmath10 i.i.d .",
    "observations @xmath580 , the kullback - leibler discrepancy is just @xmath10 times the kullback - leibler discrepancy for a single observation .",
    "therefore , without loss of generality we take @xmath581 . since",
    "@xmath582 the log - likelihood function for a single observation is given by @xmath583 from ( [ eq : multi_log_likelihood ] ) , we have @xmath584 \\nonumber\\\\ & = & \\frac{1}{2 }   \\sum_{\\nu=1}^m \\eta(\\lambda_\\nu ) [ \\langle \\theta_\\nu^{1 } , \\sigma_{(1 ) } \\theta_\\nu^{1}\\rangle - \\langle \\theta_\\nu^{2 } , \\sigma_{(1 ) } \\theta_\\nu^{2}\\rangle ] \\nonumber\\\\ & = & \\frac{1}{2 }   \\sum_{\\nu=1}^m \\eta(\\lambda_\\nu ) \\left[(\\parallel\\theta_\\nu^1 \\parallel^2 - \\parallel\\theta_\\nu^2 \\parallel^2 )   + \\sum_{\\mu=1}^m \\lambda_{\\mu } \\{(\\langle \\theta_{\\mu}^{1},\\theta_\\nu^{1}\\rangle)^2 - ( \\langle \\theta_{\\mu}^{1 } , \\theta_\\nu^{2}\\rangle)^2\\}\\right],\\end{aligned}\\ ] ] which equals the rhs of ( [ eq : multi_kl_div ] ) , since the columns of @xmath585 are orthonormal for each @xmath405 .",
    "let @xmath586 be the collection of all @xmath471sets of @xmath587 , clearly @xmath588 for any @xmath471set @xmath184 , let @xmath589 denote the collection of `` inadmissible '' @xmath471sets @xmath590 for which @xmath591 .",
    "clearly @xmath592 if @xmath473 is maximal , then @xmath593 , and so follows from the inequality @xmath594 and rearrangement of factorials .",
    "turning to the second part , we recall that stirling s formula shows that if @xmath26 and @xmath595 , @xmath596 where @xmath597 .",
    "the coefficient multiplying the exponent in @xmath598 is @xmath599 under our assumptions , and this yields .",
    "k.  r. davidson and s.  szarek . local operator theory , random matrices and banach spaces . in lindenstrauss",
    "j. johnson , w.  b. , editor , _ handbook on the geometry of banach spaces , v. 1 _ , pages 317366 .",
    "elsevier science , 2001 .",
    "d.  paul and i.  m. johnstone . augmented sparse principal component analysis for high dimensional data .",
    "( ` http://anson.ucdavis.edu/\\simdebashis/techrep/augented-spca.pdf ` ) .",
    "technical report , university of california , davis , 2007 ."
  ],
  "abstract_text": [
    "<S> we study the problem of estimating the leading eigenvectors of a high - dimensional population covariance matrix based on independent gaussian observations . we establish a lower bound on the minimax risk of estimators under the @xmath0 loss , in the joint limit as dimension and sample size increase to infinity , under various models of sparsity for the population eigenvectors . the lower bound on the risk points to the existence of different regimes of sparsity of the eigenvectors . </S>",
    "<S> we also propose a new method for estimating the eigenvectors by a two - stage coordinate selection scheme .    </S>",
    "<S> .1 in aharon birnbaum@xmath1 , iain m. johnstone@xmath2 , boaz nadler@xmath3 _ and _ debashis paul@xmath4    .1in__@xmath5 hebrew university of jerusalem ; @xmath6 stanford university ; @xmath7 weizmann institute of science ; </S>",
    "<S> @xmath8 university of california , davis _ _    .1in*keywords :* minimax risk , high - dimensional data , principal component analysis , sparsity , spiked covariance model </S>"
  ]
}