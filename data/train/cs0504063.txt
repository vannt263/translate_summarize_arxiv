{
  "article_text": [
    "the largest source of information today is the world wide web .",
    "the estimated number of documents nears 10 billion .",
    "similarly , the number of documents changing on a daily basis is also enormous .",
    "the ever - increasing growth of the web presents a considerable challenge in finding novel information on the web .",
    "in addition , properties of the web , like scale - free small world ( sfsw ) structure @xcite may create additional challenges .",
    "for example the direct consequence of the scale - free small world property is that there are numerous urls or sets of interlinked urls , which have a large number of incoming links .",
    "intelligent web crawlers can be easily trapped at the neighborhood of such junctions as it has been shown previously @xcite .",
    "we have developed a novel artificial life ( a - life ) method with intelligent individuals , crawlers , to detect new information on a news web site .",
    "we define a - life as a population of individuals having both static structural properties , and structural properties which may undergo continuous changes , i.e. , adaptation .",
    "our algorithms are based on methods developed for different areas of artificial intelligence , such as evolutionary computing , artificial neural networks and reinforcement learning .",
    "all efforts were made to keep the applied algorithms as simple as possible subject to the constraints of the internet search .",
    "evolutionary computing deals with properties that may be modified during the creation of new individuals , called multiplication. descendants may exhibit variations of population , and differ in performance from the others .",
    "individuals may also terminate .",
    "multiplication and selection is subject to the fitness of individuals , where fitness is typically defined by the modeler .",
    "for a recent review on evolutionary computing , see @xcite . for reviews on related evolutionary theories and the dynamics of self - modifying systems",
    "see @xcite and @xcite , respectively .",
    "similar concepts have been studied in other evolutionary systems where organisms compete for space and resources and cooperate through direct interaction ( see , e.g. , @xcite and references therein . )",
    "selection , however , is a very slow process and individual adaptation may be necessary in environments subject to quick changes . the typical form of adaptive learning is the connectionist architecture , such as artificial neural networks .",
    "multilayer perceptrons ( mlps ) , which are universal function approximators have been used widely in diverse applications .",
    "evolutionary selection of adapting mlps has been in the focus of extensive research @xcite .    in a typical reinforcement learning ( rl )",
    "problem the learning process @xcite is motivated by the expected value of long - term cumulated profit .",
    "a well - known example of reinforcement learning is the td - gammon program of tesauro @xcite .",
    "the author applied mlp function approximators for value estimation .",
    "reinforcement learning has also been used in concurrent multi - robot learning , where robots had to learn to forage together via direct interaction @xcite .",
    "evolutionary learning has been used within the framework of reinforcement learning to improve decision making , i.e. , the state - action mapping called policy @xcite .    in this paper",
    "we present a selection based algorithm and compare it to the well - known reinforcement learning algorithm in terms of their efficiency and behavior . in our problem ,",
    "fitness is not determined by us , but fitness is implicit .",
    "fitness is jointly determined by the ever changing external world and by the competing individuals together .",
    "selection and multiplication of individuals are based on their fitness value .",
    "communication and competition among our crawlers are indirect .",
    "only the first submitter of a document may receive positive reinforcement .",
    "our work is different from other studies using combinations of genetic , evolutionary , function approximation , and reinforcement learning algorithms , in that i ) it does not require explicit fitness function , ii ) we do not have control over the environment , iii ) collaborating individuals use value estimation under ` evolutionary pressure ' , and iv ) individuals work without direct interaction with each other .",
    "we performed realistic simulations based on data collected during an 18 days long crawl on the web .",
    "we have found that our selection based weblog update algorithm performs better in scale - free small world environment than the rl algorithm , eventhough the reinforcement learning algorithm has been shown to be efficient in finding relevant information @xcite .",
    "we explain our results based on the different behaviors of the algorithms .",
    "that is , the weblog update algorithm finds the good relevant document sources and remains at these regions until better places are found by chance .",
    "individuals using this selection algorithm are able to quickly collect the new relevant documents from the already known places because they monitor these places continuously .",
    "the reinforcement learning algorithm explores new territories for relevant documents and if it finds a good place then it collects the existing relevant documents from there .",
    "the continuous exploration of rl causes that it finds relevant documents slower than the weblog update algorithm .",
    "also , crawlers using weblog update algorithm submit more different documents than crawlers using the rl algorithm .",
    "therefore there are more relevant new information among documents submitted by former than latter crawlers .",
    "the paper is organized as follows . in section [",
    "s : related ] we review recent works in the field of web crawling .",
    "then we describe our algorithms and the forager architecture in section [ s : architecture ] .",
    "after that in section [ s : experiments ] we present our experiment on the web and the conducted simulations with the results . in section [ s : discussion ]",
    "we discuss our results on the found different behaviors of the selection and reinforcement learning algorithms .",
    "section [ s : conclusion ] concludes our paper .",
    "our work concerns a realistic web environment and search algorithms over this environment . we compare selective / evolutionary and reinforcement learning methods .",
    "it seems to us that such studies should be conducted in ever changing , buzzling , wabbling environments , which justifies our choice of the environment .",
    "we shall review several of the known search tools including those @xcite that our work is based upon .",
    "readers familiar with search tools utilized on the web may wish to skip this section .",
    "there are three main problems that have been studied in the context of crawlers .",
    "rungsawang et al . @xcite and references therein and menczer @xcite studied the topic specific crawlers .",
    "risvik et al . @xcite and references therein address research issues related to the exponential growth of the web .",
    "cho and gracia - molina @xcite , menczer @xcite and edwards et .",
    "al @xcite and references therein studies the problem of different refresh rates of urls ( possibly as high as hourly or as low as yearly ) .",
    "rungsawang and angkawattanawit @xcite provide an introduction to and a broad overview of topic specific crawlers ( see citations in the paper ) .",
    "they propose to learn starting urls , topic keywords and url ordering through consecutive crawling attempts .",
    "they show that the learning of starting urls and the use of consecutive crawling attempts can increase the efficiency of the crawlers .",
    "the used heuristic is similar to the weblog algorithm @xcite , which also finds good starting urls and periodically restarts the crawling from the newly learned ones .",
    "the main limitation of this work is that it is incapable of addressing the freshness ( i.e. , modification ) of already visited web pages .",
    "menczer @xcite describes some disadvantages of current web search engines on the dynamic web , e.g. , the low ratio of fresh or relevant documents .",
    "he proposes to complement the search engines with intelligent crawlers , or web mining agents to overcome those disadvantages .",
    "search engines take static snapshots of the web with relatively large time intervals between two snapshots .",
    "intelligent web mining agents are different : they can find online the required recent information and may evolve intelligent behavior by exploiting the web linkage and textual information .",
    "he introduces the infospider architecture that uses genetic algorithm and reinforcement learning , also describes the myspider implementation of it .",
    "menczer discusses the difficulties of evaluating online query driven crawler agents .",
    "the main problem is that the whole set of relevant documents for any given query are unknown , only a subset of the relevant documents may be known . to solve this problem he introduces two new metrics that estimate the real recall and precision based on an available subset of the relevant documents . with these metrics search engine and online crawler performances",
    "can be compared .",
    "starting the myspider agent from the 100 top pages of altavista the agent s precision is better than altavista s precision even during the first few steps of the agent .",
    "the fact that the myspider agent finds relevant pages in the first few steps may make it deployable on users computers .",
    "some problems may arise from this kind of agent usage .",
    "first of all there are security issues , like which files or information sources are allowed to read and write for the agent .",
    "the run time of the agents should be controlled carefully because there can be many users ( google answered more than 100 million searches per day in january - february 2001 ) using these agents , thus creating huge traffic overhead on the internet .",
    "our weblog algorithm uses local selection for finding good starting urls for searches , thus not depending on any search engines .",
    "dependence on a search engine can be a suffer limitation of most existing search agents , like myspiders .",
    "note however , that it is an easy matter to combine the present algorithm with urls offered by search engines .",
    "also our algorithm should not run on individual users s computers .",
    "rather it should run for different topics near to the source of the documents in the given topic ",
    "e.g. , may run at the actual site where relevant information is stored .",
    "risvik and michelsen @xcite mention that because of the exponential growth of the web there is an ever increasing need for more intelligent , ( topic-)specific algorithms for crawling , like focused crawling and document classification . with these algorithms crawlers and search engines",
    "can operate more efficiently in a topically limited document space .",
    "the authors also state that in such vertical regions the dynamics of the web pages is more homogenous .",
    "they overview different dimensions of web dynamics and show the arising problems in a search engine model .",
    "they show that the problem of rapid growth of web and frequent document updates creates new challenges for developing more and more efficient web search engines .",
    "the authors define a reference search engine model having three main components : ( 1 ) crawler , ( 2 ) indexer , ( 3 ) searcher .",
    "the main part of the paper focuses on the problems that crawlers need to overcome on the dynamic web . as a possible solution the authors propose a heterogenous crawling architecture .",
    "they also present an extensible indexer and searcher architecture .",
    "the crawling architecture has a central distributor that knows which crawler has to crawl which part of the web .",
    "special crawlers with low storage and high processing capacity are dedicated to web regions where content changes rapidly ( like news sites ) .",
    "these crawlers maintain up - to - date information on these rapidly changing web pages .",
    "the main limitation of their crawling architecture is that they must divide the web to be crawled into distinct portions manually before the crawling starts .",
    "a weblog like distributed algorithm  as suggested here  my be used in that architecture to overcome this limitation .",
    "cho and garcia - molina @xcite define mathematically the freshness and age of documents of search engines .",
    "they propose the poisson process as a model for page refreshment .",
    "the authors also propose various refresh policies and study their effectiveness both theoretically and on real data .",
    "they present the optimal refresh policies for their freshness and age metrics under the poisson page refresh model .",
    "the authors show that these policies are superior to others on real data , too .",
    "they collected about 720000 documents from 270 sites . although they show that in their database more than 20 percent of the documents are changed each day , they disclosed these documents from their studies .",
    "their crawler visited the documents once each day for 5 months , thus can not measure the exact change rate of those documents . while in our work we definitely concentrate on these frequently changing documents .",
    "the proposed refresh policies require good estimation of the refresh rate for each document .",
    "the estimation influences the revisit frequency while the revisit frequency influences the estimation .",
    "our algorithm does not need explicit frequency estimations .",
    "the more valuable urls ( e.g. , more frequently changing ) will be visited more often and if a crawler does not find valuable information around an url being in it s weblog then that url finally will fall out from the weblog of the crawler .",
    "however frequency estimations and refresh policies can be easily integrated into the weblog algorithm selecting the starting url from the weblog according to the refresh policy and weighting each url in the weblog according to their change frequency estimations .",
    "menczer @xcite also introduces a recency metric which is 1 if all of the documents are recent ( i.e. , not changed after the last download ) and goes to 0 as downloaded documents are getting more and more obsolete . trivially immediately after a few minutes run of an online crawler the value of this metric will be 1 , while the value for the search engine will be lower .",
    "edwards et al .",
    "@xcite present a mathematical crawler model in which the number of obsolete pages can be minimized with a nonlinear equation system .",
    "they solved the nonlinear equations with different parameter settings on realistic model data .",
    "their model uses different buckets for documents having different change rates therefore does not need any theoretical model about the change rate of pages .",
    "the main limitations of this work are the following :    * by solving the nonlinear equations the content of web pages can not be taken into consideration .",
    "the model can not be extended easily to ( topic-)specific crawlers , which would be highly advantageous on the exponentially growing web @xcite , @xcite , @xcite . * the rapidly changing documents ( like on news sites ) are not considered to be in any bucket , therefore increasingly important parts of the web are disclosed from the searches .",
    "however the main conclusion of the paper is that there may exist some efficient strategy for incremental crawlers for reducing the number of obsolete pages without the need for any theoretical model about the change rate of pages .",
    "there are two different kinds of agents : the foragers and the reinforcing agent ( ra ) .",
    "the fleet of foragers crawl the web and send the urls of the selected documents to the reinforcing agent .",
    "the ra determines which forager should work for the ra and how long a forager should work .",
    "the ra sends reinforcements to the foragers based on the received urls .",
    "we employ a fleet of foragers to study the competition among individual foragers .",
    "the fleet of foragers allows to distribute the load of the searching task among different computers .",
    "a forager has simple , limited capabilities , like limited number of starting urls and a simple , content based url ordering .",
    "the foragers compete with each other for finding the most relevant documents . in this way",
    "they efficiently and quickly collect new relevant documents without direct interaction .    at first",
    "the basic algorithms are presented .",
    "after that the reinforcing agent and the foragers are detailed .",
    "a forager periodically restarts from a url randomly selected from the list of starting urls .",
    "the sequence of visited urls between two restarts forms a path .",
    "the starting url list is formed from the @xmath0 first urls of the weblog . in the weblog",
    "there are @xmath1 urls with their associated weblog values in descending order .",
    "the weblog value of a url estimates the expected sum of rewards during a path after visiting that url .",
    "the weblog update algorithm modifies the weblog before a new path is started ( algorithm [ t : weblog_pseudo ] ) .",
    "the weblog value of a url already in the weblog is modified toward the sum of rewards in the remaining part of the path after that url .",
    "a new url has the value of actual sum of rewards in the remaining part of the path .",
    "if a url has a high weblog value it means that around that url there are many relevant documents .",
    "therefore it may worth it to start a search from that url .    ' '' ''    ' '' ''    [ t : weblog_pseudo]*weblog update*. @xmath2 was set to 0.3    ' '' ''    xxx = xx = xx = xx = xx = xx = xx = xx = xx = ` input ` + @xmath3 the steps of the given path + @xmath4 the sum of rewards for each step in the given path + ` output ` + starting url list + ` method ` + @xmath5 cumulated sum of @xmath6 in reverse order + @xmath7 not having value in @xmath8 + @xmath9 having value in @xmath8 + ` for each ` @xmath10 + @xmath11 + ` endfor ` + ` for each ` @xmath12 + @xmath13 + @xmath14 + ` endfor ` + @xmath15 descending order of values in @xmath8 + @xmath15 truncate @xmath8 after the @xmath16 + element + starting url list @xmath17 first @xmath18 elements of @xmath8    ' '' ''    ' '' ''    without the weblog algorithm the weblog and thus the starting url list remains the same throughout the searches .",
    "the weblog algorithm is a very simple version of evolutionary algorithms . here",
    ", evolution may occur at two different levels : the list of urls of the forager is evolving by the reordering of the weblog . also , a forager may multiply , and its weblog , or part of it may spread through inheritance . this way , the weblog algorithm incorporates most basic features of evolutionary algorithms .",
    "this simple form shall be satisfactory to demonstrate our statements .",
    "a forager can modify its url ordering based on the received reinforcements of the sent urls .",
    "the ( immediate ) profit is the difference of received rewards and penalties at any given step .",
    "immediate profit is a myopic characterization of a step to a url .",
    "foragers have an adaptive continuous value estimator and follow the _ policy _ that maximizes the expected long term cumulated profit ( ltp ) instead of the immediate profit .",
    "such estimators can be easily realized in neural systems @xcite .",
    "policy and profit estimation are interlinked concepts : profit estimation determines the policy , whereas policy influences choices and , in turn , the expected ltp .",
    "( for a review , see @xcite . ) here , choices are based on the greedy ltp policy : the forager visits the url , which belongs to the _ frontier _ ( the list of linked but not yet visited urls , see later ) and has the highest estimated ltp .    in the particular simulation",
    "each forager has a @xmath19 dimensional probabilistic term - frequency inverse document - frequency ( prtfidf ) text classifier @xcite , generated on a previously downloaded portion of the geocities database .",
    "fifty clusters were created by boley s clustering algorithm @xcite from the downloaded documents .",
    "the prtfidf classifiers were trained on these clusters plus an additional one , the @xmath20 , representing general texts from the internet .",
    "the prtfidf outputs were non - linearly mapped to the interval [ -1,+1 ] by a hyperbolic - tangent function . the classifier was applied to reduce the texts to a small dimensional representation .",
    "the output vector of the classifier for the page of url @xmath21 is @xmath22 .",
    "( the @xmath20 output was dismissed . )",
    "this output vector is stored for each url ( algorithm [ t : pageinfo_urlordering_pseudo ] ) .    ' '' ''    ' '' ''    [ t : pageinfo_urlordering_pseudo]*page information storage *    ' '' ''    xxx = xx = xx = xx = xx = xx = xx = xx = xx = ` input ` + @xmath23 urls of pages to be stored + ` output ` + @xmath24 the classifier output vectors for pages of @xmath25 + ` method ` + ` for each ` @xmath26 + @xmath27 text of page of @xmath28 + @xmath29 classifier output vector for @xmath30 + ` endfor `    ' '' ''    ' '' ''    a linear function approximator is used for ltp estimation .",
    "it encompasses @xmath31 parameters , the _ weight vector _ @xmath32 .",
    "the ltp of document of url @xmath21 is estimated as the scalar product of @xmath33 and @xmath34 : @xmath35 . during url ordering the url with highest ltp estimation",
    "is selected .",
    "the url ordering algorithm is shown in algorithm [ t : urlordering_pseudo ] .    ' '' ''    ' '' ''    [ t : urlordering_pseudo]*url ordering *    ' '' ''    xxx = xx = xx = xx = xx = xx = xx = xx = xx = ` input ` + @xmath36 the set of available urls + @xmath24 the stored vector representation of the urls + ` output ` + @xmath37 url with maximum ltp value + ` method ` + ` for each ` @xmath38 + @xmath39 + ` endfor ` + @xmath37 url with maximal ltp @xmath40    ' '' ''    ' '' ''    the weight vector of each forager is tuned by temporal difference learning @xcite .",
    "let us denote the current url by @xmath41 , the next url to be visited by @xmath42 , the output of the classifier for @xmath43 by @xmath44 and the estimated ltp of a url @xmath43 by @xmath45 .",
    "assume that leaving @xmath41 to @xmath42 the immediate profit is @xmath46 .",
    "our estimation is perfect if @xmath47 .",
    "future profits are typically discounted in such estimations as @xmath48 , where @xmath49 .",
    "the error of value estimation is    @xmath50    we used throughout the simulations @xmath51 . for each step @xmath52 the weights of the value function were tuned to decrease the error of value estimation based on the received immediate profit @xmath46 .",
    "the @xmath53 estimation error was used to correct the parameters .",
    "the @xmath54 component of the weight vector , @xmath55 , was corrected by    @xmath56    with @xmath57 and @xmath58 .",
    "these modified weights in a stationary environment would improve value estimation ( see , e.g , @xcite and references therein ) .",
    "the url ordering update is given in algorithm [ t : urlordering_update_pseudo ] .    ' '' ''    ' '' ''    [ t : urlordering_update_pseudo]*url ordering update *    ' '' ''    xxx = xx = xx = xx = xx = xx = xx = xx = xx = ` input ` + @xmath59 the step for which the reinforcement is received + @xmath60 the previous step before @xmath42 + @xmath61 reinforcement for visiting @xmath42 + ` output ` + @xmath62 the updated weight vector + ` method ` + @xmath63 + @xmath64    ' '' ''    ' '' ''    without the update algorithm the weight vector remains the same throughout the search .",
    "a document or page is possibly relevant for a forager if it is not older than 24 hours and the forager has not marked it previously .",
    "algorithm [ t : relevant_pseudo ] shows the procedure of selecting such documents .",
    "the selected documents are sent to the ra for further evaluation .    ' '' ''    ' '' ''    [ t : relevant_pseudo]*document relevancy at a forager *    ' '' ''    xxx = xx = xx = xx = xx = xx = xx = xx = xx = ` input ` + @xmath65 the pages to be examined + ` output ` + @xmath66 the selected pages + ` method ` + @xmath67 previously selected relevant pages + @xmath66 all pages from @xmath68 which are + not older than 24 hours and + not contained in @xmath69 + @xmath67 add @xmath70 to @xmath69    ' '' ''    ' '' ''      during multiplication the weblog is randomly divided into two equal sized parts ( one for the original and one for the new forager ) .",
    "the parameters of the url ordering algorithm ( the weight vector of the value estimation ) are either copied or new random parameters are generated . if the forager has a url ordering update algorithm then the parameters are copied .",
    "if the forager does not have any url ordering update algorithm then new random parameters are generated , as shown in algorithm [ t : multiplication_pseudo ] .    ' '' ''    ' '' ''    [ t : multiplication_pseudo]*multiplication *    ' '' ''",
    "xxx = xx = xx = xx = xx = xx = xx = xx = xx = ` input ` + @xmath8 + weight vector of url ordering + ` output ` + @xmath71 + @xmath72 + ` method ` + @xmath73 randomly selected + urls and values from @xmath8 + @xmath15 delete @xmath71 from @xmath8 + ` if ` forager has url ordering update algorithm + @xmath74 copy the weight vector of url ordering + ` else ` + @xmath74 generate a new random weight vector + ` endif `    ' '' ''    ' '' ''      a reinforcing agent controls the `` life '' of foragers .",
    "it can start , stop , multiply or delete foragers .",
    "ra receives the urls of documents selected by the foragers , and responds with reinforcements for the received urls .",
    "the response is @xmath75 ( a.u . ) for a relevant document and @xmath76 ( a.u . ) for a not relevant document .",
    "a document is relevant if it is not yet seen by the reinforcing agent and it is not older than 24 hours .",
    "the reinforcing agent maintains the score of each forager working for it .",
    "initially each forager has @xmath77 score .",
    "when a forager sends a url to the ra , the forager s score is decreased by @xmath78 .",
    "after each relevant page sent by the forager , the forager s score is increased by @xmath79 ( algorithm [ t : manageurl_pseudo ] ) .    ' '' ''    ' '' ''    [ t : manageurl_pseudo]*manage received url *    ' '' ''    xxx = xx = xx = xx = xx = xx = xx = xxxxx = ` input ` + @xmath80 received url from forager + ` output ` + reinforcement to forager + updated forager score + ` method ` + @xmath81 relevant pages seen by the ra + @xmath27 get page of @xmath28 + decrease forager s score with @xmath82 + ` if ` @xmath83 or page date is older than 24 hours + send @xmath84 to forager + ` else ` + @xmath81 add @xmath30 to @xmath85 + send @xmath86 to forager + increase forager s score with @xmath87 + ` endif `    ' '' ''    ' '' ''    when the forager s score reaches @xmath88 and the number of foragers is smaller than @xmath89 then the forager is multiplied .",
    "that is a new forager is created with the same algorithms as the original one has , but with slightly different parameters .",
    "when the forager s score goes below @xmath90 and the number of foragers is larger than @xmath91 then the forager is deleted ( algorithm [ t : manageforager_pseudo ] ) .",
    "note that a forager can be multiplied or deleted immediately after it has been stopped by the ra and before the next forager is activated .    ' '' ''    ' '' ''    [ t : manageforager_pseudo ] * : manage forager *    ' '' ''    xxx = xx = xx = xx = xx = xx = xx = xxxxx = ` input ` + @xmath92 the forager to be multiplied or deleted + ` output ` + possibly modified list of foragers + ` method ` + ` if ` ( @xmath93 s score @xmath94 @xmath95 ` and ` + number of foragers @xmath96 @xmath97 ) + @xmath98 call @xmath93 s + * multiplication , alg .",
    "[ t : multiplication_pseudo ] * + @xmath93 may modify it s own weblog + @xmath99 create a new forager with the received + @xmath8 and @xmath100 + set the two foragers score to @xmath101 + ` else if ` ( @xmath93 s score @xmath102 @xmath103 ` and ` + number of foragers @xmath104 @xmath105 ) + delete @xmath93 + ` endif `    ' '' ''    ' '' ''    foragers on the same computer are working in time slices one after each other .",
    "each forager works for some amount of time determined by the ra .",
    "then the ra stops that forager and starts the next one selected by the ra .",
    "the pseudo - code of the reinforcing agent is given in algorithm [ t : reinforcing_pseudo ] .    ' '' ''    ' '' ''    [ t : reinforcing_pseudo ] * : reinforcing agent *    ' '' ''    xxx = xx = xx = xx = xx = xx = xx = xxxxx = ` input ` + seed urls + ` output ` + @xmath81 found relevant documents + ` method ` + @xmath81 empty set /*set of all observed relevant pages + initialize @xmath105 foragers with the seed urls + set one of them to be the next + ` repeat ` + start next forager + receive possibly relevant url + call * manage received url , alg .",
    "[ t : manageurl_pseudo ] * with url + stop forager if its time period is over + call * manage forager , alg . [ t : manageforager_pseudo ] * with this forager + choose next forager + ` until ` time is over    ' '' ''    ' '' ''      a forager is initialized with parameters defining the url ordering , and either with a weblog or with a seed of urls ( algorithm [ t : initforager_pseudo ] ) . after its initialization a forager crawls in search paths , that is after a given number of steps the search restarts and the steps between two restarts form a path . during each path",
    "the forager takes @xmath106 number of steps , i.e. , selects the next url to be visited with a url ordering algorithm . at the beginning of a path a url",
    "is selected randomly from the starting url list .",
    "this list is formed from the 10 first urls of the weblog .",
    "the weblog contains the possibly good starting urls with their associated weblog values in descending order .",
    "the weblog algorithm modifies the weblog and so thus the starting url list before a new path is started .",
    "when a forager is restarted by the ra , after the ra has stopped it , the forager continues from the internal state in which it was stopped .",
    "the pseudo code of step selection is given in algorithm [ t : stepselection_pseudo ] .    ' '' ''    ' '' ''    [ t : initforager_pseudo]*initialization of the forager *    ' '' ''    xxx = xx = xx = xx = xx = xx = xx = xx = xx = ` input ` + weblog or seed urls + url ordering parameters + ` output ` + initialized forager + ` method ` + set path step number to @xmath107 /*start new path + set the weblog + either with the input weblog + or put the seed urls into the weblog with 0 weblog value + set the url ordering parameters in url ordering algorithm    ' '' ''    ' '' ''    ' '' ''    ' '' ''    [ t : stepselection_pseudo]*url selection *    ' '' ''    xxx = xx = xx = xx = xx = xx = xx = xx = xx = ` input ` + @xmath36 set of urls available in this step + @xmath108 set of visited urls in this path + ` output ` + @xmath109 selected url to be visited next + ` method ` + ` if ` path step number @xmath110 + @xmath109 selected url by * url ordering , alg .",
    "[ t : urlordering_pseudo ] * + increase path step number + ` else ` + call the * weblog update , alg .",
    "[ t : weblog_pseudo ] * to update the weblog + @xmath109 select a random url from the starting url list + set path step number to 1 + @xmath36 empty set + @xmath108 empty set + ` endif `    ' '' ''    ' '' ''    the url ordering algorithm selects a url to be the next step from the frontier url set .",
    "the selected url is removed from the frontier and added to the visited url set to avoid loops .",
    "after downloading the pages , only those urls ( linked from the visited url ) are added to the frontier which are not in the visited set .    in each step",
    "the forager downloads the page of the selected url and all of the pages linked from the page of selected url .",
    "it sends the urls of the possibly relevant pages to the reinforcing agent .",
    "the forager receives reinforcements on any previously sent but not yet reinforced urls and calls the url ordering update algorithm with the received reinforcements .",
    "the pseudo code of a forager is shown in algorithm [ t : forager_pseudo ] .    ' '' ''    ' '' ''    [ t : forager_pseudo]*forager *    ' '' ''    xxx = xx = xx = xx = xx = xx = xx = xx = xx = ` input ` + @xmath36 set of urls available in the next step + @xmath108 set of visited urls in the current path + ` output ` + sent documents to the ra + modified @xmath111 and @xmath112 + modified @xmath8 and url ordering weight vector + ` method ` + ` repeat ` + @xmath109 call * url selection , alg .",
    "[ t : stepselection_pseudo ] * + @xmath36 remove @xmath113 from @xmath111 + @xmath108 add @xmath113 to @xmath112 + @xmath27 download the page of @xmath113 + @xmath114 links of @xmath30 + @xmath115 @xmath116 which are not @xmath112 + @xmath36 add @xmath117 to @xmath111 + download pages of @xmath116 + call * page information storage , alg . [ t : pageinfo_urlordering_pseudo ] * with @xmath117 + @xmath66 call * document relevancy , alg .",
    "[ t : relevant_pseudo ] * for + all pages + send @xmath70 to reinforcing agent + receive reinforcements for sent but not yet reinforced pages + call * url ordering update , alg .",
    "[ t : urlordering_update_pseudo ] * with + the received reinforcements + ` until ` time is over    ' '' ''    ' '' ''",
    "we conducted an 18 day long experiment on the web to gather realistic data .",
    "we used the gathered data in simulations to compare the weblog update ( section [ sss : weblog ] ) and reinforcement learning algorithms ( section [ sss : rl ] ) . in web experiment",
    "we used a fleet of foragers using combination of reinforcement learning and weblog update algorithms to eliminate any biases on the gathered data .",
    "first we describe the experiment on the web then the simulations .",
    "we analyze our results at the end of this section .",
    "we ran the experiment on the web on a single personal computer with celeron 1000 mhz processor and 512 mb ram .",
    "we implemented the forager architecture ( described in section [ s : architecture ] ) in java programming language .    in this experiment a fixed number of foragers were competing with each other to collect news at the cnn web site .",
    "the foragers were running in equal time intervals in a predefined order .",
    "each forager had a 3 minute time interval and after that interval the forager was allowed to finish the step started before the end of the time interval .",
    "we deployed 8 foragers using the weblog update and the reinforcement learning based url ordering update algorithms ( 8 wlrl foragers ) .",
    "we also deployed 8 other foragers using the weblog update algorithm but without reinforcement learning ( 8 wl foragers ) .",
    "the predefined order of foragers was the following : 8 wlrl foragers were followed by the 8 wl foragers .",
    "we investigated the link structure of the gathered web pages . as it is shown in fig .",
    "[ f : sf ] the links have a power - law distribution ( @xmath118 ) with @xmath119 for outgoing links and @xmath120 for incoming links .",
    "that is the link structure has the scale - free property .",
    "the clustering coefficient @xcite of the link structure is 0.02 and the diameter of the graph is 7.2893 .",
    "we applied two different random permutations to the origin and to the endpoint of the links , keeping the edge distribution unchanged but randomly rewiring the links .",
    "the new graph has 0.003 clustering coefficient and 8.2163 diameter .",
    "that is the clustering coefficient is smaller than the original value by an order of magnitude , but the diameter is almost the same .",
    "therefore we can conclude that the links of gathered pages form small world structure .    ) . vertical axis : relative frequency of number of edges at different urls ( @xmath121 ) .",
    "dots and dark line correspond to outgoing links , crosses and gray line correspond to incoming links .",
    ", width=240 ]    the data storage for simulation is a centralized component .",
    "the pages are stored with 2 indices ( and time stamps ) .",
    "one index is the url index , the other is the page index .",
    "multiple pages can have the same url index if they were downloaded from the same url .",
    "the page index uniquely identifies a page content and the url from where the page was download . at each page download of any foragers we stored the followings ( with a time stamp containing the time of page download ) :",
    "1 .   if the page is relevant according to the ra then store `` relevant '' 2 .",
    "if the page is from a new url then store the new url with a new url index and the page s state vector with a new page index 3 .   if the content of the page is changed since the last download then store the page s state vector with a new page index but keep the url index 4 .   in both previous cases store the links of the page as links to page indices of the linked pages 1 .",
    "if a linked page is from a new url then store the new url with a new url index and the linked page s state vector with a new page index 2 .",
    "if the content of the linked page is changed since the last check then store the page s state vector with a new page index but same url index      for the simulations we implemented the forager architecture in matlab .",
    "the foragers were simulated as if they were running on one computer as described in the previous section .      during simulations we used the web pages that we gathered previously to generate a realistic environment",
    "( note that the links of pages point to local pages ( not to pages on the web ) since a link was stored as a link to a local page index ) :    * simulated documents had the same state vector representation for url ordering as the real pages had * simulated relevant documents were the same as the relevant documents on the web * pages and links appeared at the same ( relative ) time when they were found in the web experiment - using the new url indices and their time stamps * pages and links are refreshed or changed at the same relative time as the changes were detected in the web experiment  using the new page indices for existing url indices and their time stamps * simulated time of a page download was the average download time of a real page during the web experiment .",
    "we conducted simulations with two different kinds of foragers .",
    "the first case is when foragers used only the weblog update algorithm without url ordering update ( wl foragers ) .",
    "the second case is when foragers used only the reinforcement learning based url ordering update algorithm without the weblog update algorithm ( rl foragers ) .",
    "each wl forager had a different weight vector for url value estimation  during multiplication the new forager got a new random weight vector .",
    "rl foragers had the same weblog with the first 10 urls of the gathered pages  that is the starting url of the web experiment and the first 9 visited urls during that experiment . in both cases initially there were 2 foragers and they were allowed to multiply until reaching the population of 16 foragers .",
    "the simulation for each type of foragers were repeated 3 times with different initial weight vectors for each forager .",
    "the variance of the results show that there is only a small difference between simulations using the same kind of foragers , even if the foragers were started with different random weight vectors in each simulation .",
    "table [ t : params ] shows the investigated parameters during simulations .    [ cols= \" < ,",
    "< \" , ]     from table [ t : data ] we can conclude the followings :    * rl and wl foragers have similar download efficiency , i.e. , the efficiencies from the point of view of the news site are about the same .",
    "* wl foragers have higher sent efficiencies than rl foragers , i.e. , the efficiency from the point of view of the ra is higher .",
    "this shows that wl foragers divide the search area better among each other than rl foragers .",
    "sent efficiency would be 1 if none of two foragers have sent the same document to the ra .",
    "* rl foragers have higher relative found url value than wl foragers .",
    "rl foragers explore more than wl foragers and rl found more urls than wl foragers did per downloaded page .",
    "* wl foragers find faster the new relevant documents in the already found clusters .",
    "that is freshness is higher and age is lower than in the case of rl foragers .",
    "[ f : efficiency ] shows other aspects of the different behaviors of rl and wl foragers .",
    "download efficiency of rl foragers has more , higher , and sharper peaks than the download efficiency of wl foragers has .",
    "that is wl foragers are more balanced in finding new relevant documents than rl foragers .",
    "the reason is that while the wl foragers remain in the found good clusters , the rl foragers continuously explore the new promising territories .",
    "the sharp peaks in the efficiency show that rl foragers _ find and recognize _ new good territories and then _ quickly collect _ the current relevant documents from there .",
    "the foragers can recognize these places by receiving more rewards from the ra if they send urls from these places .",
    "the predefined order did not influence the working of foragers during the web experiment . from fig .",
    "[ f : efficiency ] it can be seen that foragers during the 3 independent experiments did not have very different efficiencies . on fig .",
    "[ f : freshness ] we show that the foragers in each run had a very similar behavior in terms of age and freshness , that is the values remains close to each other throughout the experiments .",
    "also the results for individual runs were close to the average values in table [ t : data ] ( see the standard deviations ) . in each individual run",
    "the foragers were started with different weight vectors , but they reached similar efficiencies and behavior .",
    "this means that the initial conditions of the foragers did not influence the later behavior of them during the simulations .",
    "furthermore foragers could not change their environment drastically ( in terms of the found relevant documents ) during a single 3 minute run time because of the short run time intervals and the fast change of environment  large number of new pages and often updated pages in the new site . during",
    "the web experiment foragers were running in 8 wlrl , 8 wl , 8 wlrl , 8 wl ,  temporal order . because of the fact that initial conditions does not influence the long term performance of foragers and the fact that the foragers can not change their environment fully we can start to examine them after the first run of wlrl foragers .",
    "then we got the other extreme order of foragers , that is the 8 wl , 8 wlrl , 8 wl , 8 wlrl ,  temporal ordering . for the overall efficiency and behavior of foragers it did not really matter",
    "if wlrl or wl foragers run first and one could use mixed order in which after a wlrl forager a wl forager runs and after a wl forager a wlrl forager comes . however , for higher bandwidths and for faster computers , random ordering may be needed for such comparisons .",
    "our first conjecture is that selection is efficient on scale - free small world structures .",
    "lrincz and kkai @xcite and rennie et al .",
    "@xcite showed that rl is efficient in the task of finding relevant information on the web .",
    "here we have shown experimentally that the weblog update algorithm , selection among starting urls , is at least as efficient as the rl algorithm .",
    "the weblog update algorithm finds as many relevant documents as rl does if they download the same amount of pages .",
    "wl foragers in their fleet select more different urls to send to the ra than rl foragers do in their fleet , therefore there are more relevant documents among those selected by wl foragers then among those selected by rl foragers . also the freshness and age of found relevant documents are better for wl foragers than for rl foragers .    for the weblog update algorithm ,",
    "the selection among starting urls has no fine tuning mechanism . throughout its life",
    "a forager searches for the same kind of documents  goes into the same ` direction ' in the state space of document states  determined by its fixed weight vector .",
    "the only adaptation allowed for a wl forager is to select starting urls from the already seen urls .",
    "the wl forager can not modify its ( ` directional ' ) preferences according goes newly found relevant document supply , where relevant documents are abundant .",
    "but a wl forager finds good relevant document sources in its own direction and forces its search to stay at those places . by chance the forager can find better sources in its own direction if the search path from a starting url is long enough . on fig .",
    "[ f : efficiency ] it is shown that the download efficiency of the foragers does not decrease with the multiplication of the foragers .",
    "therefore the new foragers must found new and good relevant document sources quickly after their appearances .",
    "the reinforcement learning based url ordering update algorithm is capable to fine tune the search of a forager by adapting the forager s weight vector .",
    "this feature has been shown to be crucial to adapt crawling in novel environments @xcite .",
    "an rl forager goes into the direction ( in the state space of document states ) where the estimated long term cumulated profit is the highest .",
    "because the local environment of the foragers may changes rapidly during crawling , it seems desirable that foragers can quickly adapt to the found new relevant documents .",
    "relevant documents may appear lonely , not creating a good relevant document source , or do not appear at the right url by a mistake .",
    "this noise of the web can derail the rl foragers from good regions .",
    "the forager may `` turn '' into less valuable directions , because of the fast adaptation capabilities of rl foragers .",
    "our second conjecture is that selection fits sfsw better than rl .",
    "we have shown in our experiments that selection and rl have different behaviors .",
    "selection selects good information sources , which are worth to revisit , and stays at those sources as long as better sources are not found by chance .",
    "rl explores new territories , and adapts to those .",
    "this adaptation can be a disadvantage when compared with the more rigid selection algorithm , which sticks to good places until ` provably ' better places are discovered .",
    "therefore wl foragers , which can not be derailed and stay in their found ` niches ' can find new relevant documents faster in such already known terrains than rl foragers can .",
    "that is , freshness is higher and age is lower for relevant documents found by wl foragers than for relevant documents found by rl foragers .",
    "also , by finding good sources and staying there , wl foragers divide the search task better than rl foragers do , this is the reason for the higher sent efficiency of wl foragers than of rl foragers .",
    "we have rewired the network as it was described in section [ ss : real ] . this way a scale - free ( sf ) but not so small world was created . intriguingly , in this sf structure , rl foragers performed better than wl ones .",
    "clearly , further work is needed to compare the behavior of the selective and the reinforcement learning algorithms in other then sfsw environments .",
    "such findings should be of relevance in the deployment of machine learning methods in different problem domains .    from the practical point of view",
    ", we note that it is an easy matter to combine the present algorithm with urls offered by search engines .",
    "also , the values reported by the crawlers about certain environments , e.g. , the environment of the url offered by search engines represent the neighborhood of that url and can serve adaptive filtering .",
    "this procedure is , indeed , promising to guide individual searches as it has been shown elsewhere @xcite .",
    "we presented and compared our selection algorithm to the well - known reinforcement learning algorithm .",
    "our comparison was based on finding new relevant documents on the web , that is in a dynamic scale - free small world environment .",
    "we have found that the weblog update selection algorithm performs better in this environment than the reinforcement learning algorithm , eventhough the reinforcement learning algorithm has been shown to be efficient in finding relevant information @xcite .",
    "we explain our results based on the different behaviors of the algorithms .",
    "that is the weblog update algorithm finds the good relevant document sources and remains at these regions until better places are found by chance .",
    "individuals using this selection algorithm are able to quickly collect the new relevant documents from the already known places because they monitor these places continuously . the reinforcement learning algorithm explores new territories for relevant documents and if it finds a good place then it collects the existing relevant documents from there .",
    "the continuous exploration and the fine tuning property of rl causes that rl finds relevant documents slower than the weblog update algorithm .    in our future work",
    "we will study the combination of the weblog update and the rl algorithms .",
    "this combination uses the wl foragers ability to stay at good regions with the rl foragers fine tuning capability . in this way",
    "foragers will be able to go to new sources with the rl algorithm and monitor the already found good regions with the weblog update algorithm .",
    "we will also study the foragers in a simulated environment which is not a small world .",
    "the clusters of small world environment makes it easier for wl foragers to stay at good regions .",
    "the small diameter due to the long distance links of small world environment makes it easier for rl foragers to explore different regions .",
    "this work will measure the extent at which the different foragers rely on the small world property of their environment .",
    "this material is based upon work supported by the european office of aerospace research and development , air force office of scientific research , air force research laboratory , under contract no .",
    "fa8655 - 03 - 1 - 3036 .",
    "this work is also supported by the national science foundation under grants no .",
    "int-0304904 and no .",
    "any opinions , findings and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect the views of the european office of aerospace research and development , air force office of scientific research , air force research laboratory .",
    "j.  edwards , k.  mccurley , and j.  tomlin , _ an adaptive model for optimizing performance of an incremental web crawler _ , proceedings of the tenth international conference on world wide web , 2001 , pp .  106113 .",
    "b.  gbor , zs .",
    "palotai , and a.  lrincz , _ value estimation based computer - assisted data mining for surfing the internet _ , int .",
    "joint conf . on neural networks ( piscataway , nj 08855 - 1331 ) , ieee operations center , 26 - 29 july , budapest , hungary 2004 , pp .",
    "1035 . , ieee catalog number : 04ch37541c , ijcnn2004 cd  rom conference proceedings .",
    "thorsten joachims , _ a probabilistic analysis of the rocchio algorithm with tfidf for text categorization _ , proceedings of icml-97 , 14th international conference on machine learning ( nashville , us ) ( douglas  h. fisher , ed . ) , morgan kaufmann publishers , san francisco , us , 1997 , pp .  143151 .",
    "k.  tuyls , d.  heytens , a.  nowe , and b.  manderick , _ extended replicator dynamics as a key to reinforcement learning in multi - agent systems _ , ecml 2003 , lnai 2837 ( n.  lavrac et  al . , ed . ) , springer - verlag , berlin , 2003 , pp ."
  ],
  "abstract_text": [
    "<S> in this paper we compare the performance characteristics of our selection based learning algorithm for web crawlers with the characteristics of the reinforcement learning algorithm . </S>",
    "<S> the task of the crawlers is to find new information on the web . </S>",
    "<S> the selection algorithm , called weblog update , modifies the starting url lists of our crawlers based on the found urls containing new information . </S>",
    "<S> the reinforcement learning algorithm modifies the url orderings of the crawlers based on the received reinforcements for submitted documents . </S>",
    "<S> we performed simulations based on data collected from the web . </S>",
    "<S> the collected portion of the web is typical and exhibits scale - free small world ( sfsw ) structure . </S>",
    "<S> we have found that on this sfsw , the weblog update algorithm performs better than the reinforcement learning algorithm . </S>",
    "<S> it finds the new information faster than the reinforcement learning algorithm and has better new information / all submitted documents ratio . </S>",
    "<S> we believe that the advantages of the selection algorithm over reinforcement learning algorithm is due to the small world property of the web . </S>"
  ]
}