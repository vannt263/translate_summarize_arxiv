{
  "article_text": [
    "contemporary scientific investigations frequently encounter a common issue of exploring the relationship between a response and a number of covariates . in machine learning research , the subject is typically addressed through learning a underling rule from the data that accurately predicates future values of the response .",
    "for instance , in banking industry , financial analysts are interested in building a system that helps to judge the risk of a loan request .",
    "such a system is often trained based on the risk assessments from previous loan applications together with the empirical experiences . an incoming loan request",
    "is then viewed as a new input , upon which the corresponding potential risk ( response ) is to be predicted . in such applications ,",
    "the predictive accuracy of a trained rule is of the key importance .    in the past decade",
    ", various strategies have been developed to improve the prediction ( generalization ) capability of a learning process , which include @xmath4 regularization as an well - known example @xcite .",
    "the @xmath0 regularization learning prevents over - fitting by shrinking the model coefficients and thereby attains a higher predictive value . to be specific ,",
    "suppose that the data @xmath6 for @xmath7 are collected independently and identically according to an unknown but definite distribution , where @xmath8 is a response of @xmath9th unit and @xmath10 is the corresponding @xmath11-dimensional covariates .",
    "let @xmath12 be a sample dependent space ( sdhs ) with @xmath13 and @xmath14 being a positive definite kernel function .",
    "the coefficient - based @xmath0 regularization strategy ( @xmath0 regularizer ) takes the form of @xmath15 where @xmath16 is a regularization parameter and @xmath17 @xmath18 is defined by @xmath19 with different choices of order @xmath1 , ( [ algorihtm ] ) leads to various specific forms of the @xmath20 regularizer .",
    "in particular , when @xmath21 , @xmath22 corresponds to the ridge regressor @xcite , which smoothly shrinks the coefficients toward zero .",
    "when @xmath23 , @xmath22 leads to the lasso @xcite , which set small coefficients exactly at zero and thereby also serves as a variable selection operator . when @xmath24 , @xmath22 coincides with the bridge estimator @xcite , which tends to produce highly sparse estimates through a non - continuous shrinkage .",
    "the varying forms and properties of @xmath22 make the choice of order @xmath1 crucial in applications . apparently , an optimal @xmath1 may depend on many factors such as the learning algorithms , the purposes of studies and so forth .",
    "these factors make a simple answer to this question infeasible in general . to facilitate the use of @xmath4-regularization , alteratively",
    ", we intend to seek for a modeling strategy where an elaborative selection on @xmath1 is avoidable . specifically , we attempt to reveal some insights for the role of @xmath1 in @xmath4-learning via answering the following question :    * problem 1 . *",
    "are there any kernels such that the generalization capability of ( [ algorihtm ] ) is independent of @xmath1 ?    in this paper , we provides a positive answer to problem 1 under the framework of statistical learning theory .",
    "specifically , we provide a featured class of positive definite kernels , under which the @xmath20 estimators for @xmath5 attain similar generalization error bounds .",
    "we then show that these estimated bounds are almost essential in the sense that up to a logarithmic factor the upper and lower bounds are asymptotically identical . in the proposed modeling context",
    ", the choice of @xmath1 does not have a strong impact in terms of the generalization capability . from this perspective",
    ", @xmath1 can be arbitrarily specified , or specified merely by other no generalization criteria like smoothness , computational complexity , sparsity , etc ..    the reminder of the paper is organized as follows . in section 2 ,",
    "we provide a literature review and explain our motivation of the research .",
    "in section 3 , we present some preliminaries including spherical harmonics , gegenbauer polynomials and so on . in section 4",
    ", we introduce a class of well - localized needlet type kernels of petrushev and xu @xcite and show some crucial properties of them which will play important roles in our analysis . in section 5 , we then study the generalization capabilities of @xmath0-regularizer associated with the constructed kernels for different @xmath1 . in section 6 , we provide the proof of the main results .",
    "we conclude the paper with some useful remarks in the last section .",
    "in practice , the choice of @xmath1 in ( [ algorihtm ] ) is critical , since it embodies certain potential attributions of the anticipated solutions such as sparsity , smoothness , computational complexity , memory requirement and generalization capability of course .",
    "the following simple simulation illustrates that different choice of @xmath1 can lead to different sparsity of the solutions .    the samples are identically and independently drawn according to the uniform distribution from the two dimensional sinc function pulsing a gaussian noise @xmath25 with @xmath26 .",
    "there are totally 256 training samples and 256 test samples . in fig . 1",
    ", we show that different choice of @xmath1 may deduce different sparsity of the estimator for the kernel @xmath27 .",
    "it can be found that @xmath0 @xmath28 regularizers can deduce sparse estimator , while it impossible for @xmath29 regularizer .",
    "learning schemes , width=302,height=226 ]    therefore , for a given learning task , how to choose @xmath1 is an important and crucial problem for @xmath0 regularization learning . in other words",
    ", which standards should be adopted to measure the quality of @xmath0 regularizers deserves study . as the most important standard of statistical learning theory , the generalization capability of @xmath0 regularization scheme ( [ algorihtm ] ) may depend on the choice of kernel , the size of samples @xmath30 , the regularization parameter @xmath31 , the behavior of priors , and , of course , the choice of @xmath1 . if we take the generalization capability of @xmath0 regularization learning as a function of @xmath1 , we then",
    "automatically wonder how this function behaves when @xmath1 changes for a fixed kernel .",
    "if the generalization capabilities depends heavily on @xmath1 , then it is natural to choose the @xmath1 such that the generalization capability of the corresponding @xmath0 regularizer is the smallest .",
    "if the generalization capabilities is independent of @xmath1 , then @xmath1 can be arbitrarily specified , or specified merely by other no generalization criteria like smoothness , computational complexity , sparsity .",
    "however , the relation between the generalization capability and @xmath1 depends heavily on the kernel selection . to show this",
    ", we compare the generalization capabilities of @xmath29 , @xmath2 , @xmath32 and @xmath33 regularization schemes for two kernels : @xmath34 and @xmath35 in the simulation .",
    "the one case shows that the generalization capabilities of @xmath0 regularization schemes may be independent of @xmath1 and the other case shows that the generalization capability of ( [ algorihtm ] ) depends heavily on @xmath1 . in the left of fig .",
    "2 , we report the relation between the test error and regularization parameter for the kernel @xmath34 . it is shown that when the regularization parameters are appropriately tuned , all of the aforementioned regularization schemes may possess the similar generalization capabilities . in the right of fig .",
    "2 , for the kernel @xmath35 , we see that the generalization capability of @xmath0 regularization depends heavily on the choice of @xmath1 .",
    "regularization schemes with different @xmath1 . ]     regularization schemes with different @xmath1 . ]    from these simulations , we see that finding kernels such that the generalization capability of ( [ algorihtm ] ) is independent of @xmath1 is of special importance in theoretical and practical applications . in particular , if such kernels exist , with such kernels , @xmath1 can be solely chosen on the basis of algorithmic and practical considerations for @xmath0 regularization . here",
    "we emphasize that all these conclusions can , of course only be made in the premise that the obtained generalization capabilities of all @xmath0 regularizers are ( almost ) optimal .",
    "there have been several papers that focus on the generalization capability analysis of the @xmath0 regularization scheme ( [ algorihtm ] ) .",
    "wu and zhou @xcite were the first , to the best of our knowledge , to show a mathematical foundation of learning algorithms in sdhs .",
    "they claimed that the data dependent nature of the algorithm leads to an extra error term called hypothesis error , which is essentially different form regularization schemes with sample independent hypothesis spaces ( sihss ) .",
    "based on this , the authors proposed a coefficient - based regularization strategy and conducted a theoretical analysis of the strategy by dividing the generalization error into approximation error , sample error and hypothesis error .",
    "following their work , xiao and zhou @xcite derived a learning rate of @xmath2 regularizer via bounding the regularization error , sample error and hypothesis error , respectively .",
    "their result was improved in @xcite by adopting a concentration inequality technique with @xmath29 empirical covering numbers to tackle the sample error . on the other hand , for @xmath0 @xmath36 regularizers , tong et al .",
    "@xcite deduced an upper bound for generalization error by using a different method to cope with the hypothesis error .",
    "later , the learning rate of @xcite was improved further in @xcite by giving a sharper estimation of the sample error .    in all those researches , some sharp restrictions on the probability distributions ( priors ) have been imposed , say , both spectrum assumption of the regression function and concentration property of the marginal distribution should be satisfied . noting this , for @xmath29 regularizer , sun and",
    "wu @xcite conducted a generalization capability analysis for @xmath29 regularizer by using the spectrum assumption to the regression function only .",
    "for @xmath2 regularizer , by using a sophisticated functional analysis method , zhang et al .",
    "@xcite and song et al .",
    "@xcite built the regularized least square algorithm on the reproducing kernel banach space ( rkbs ) , and they proved that the regularized least square algorithm in rkbs is equivalent to @xmath2 regularizer if the kernel satisfies some restricted conditions . following this method , song and",
    "zhang @xcite deduced a similar learning rate for the @xmath2 regularizer and eliminated the concentration property assumption on the marginal distribution .",
    "limiting @xmath1 within @xmath37 $ ] is certainly incomplete to judge whether the generalization capability of @xmath0 regularization depends on the choice of @xmath1 .",
    "moreover , in the context of learning theory , to intrinsically characterize the generalization capability of a learning strategy , the essential generalization bound @xcite rather than the upper bound is required , that is , we must deduce a lower and an upper bound simultaneously for the learning strategy and prove that the upper and lower bounds can be asymptotically identical .",
    "we notice , however , that most of the previously known estimations on generalization capability of learning schemes ( [ algorihtm ] ) are only concerned with the upper bound estimation .",
    "thus , their results can not serve the answer to problem 1 .",
    "different from the pervious work , the essential bound estimation of generalization error for @xmath0 regularization schemes ( [ algorihtm ] ) with @xmath38 will be presented in the present paper . as a consequence",
    ", we provide an affirmative answer to problem 1 .",
    "in this section , we introduce some preliminaries on spherical harmonics , gegenbauer polynomial and orthonormal basis construction . , which will be used in the construction of the positive definite needlet kernel .",
    "the gegenbauer polynomials are defined by the generating function @xcite @xmath39 where @xmath40 and @xmath41 .",
    "the coefficients @xmath42 are algebraic polynomials of degree @xmath43 which are called the gegenbauer polynomials associated with @xmath44 .",
    "it is known that the family of polynomials @xmath45 is a complete orthogonal system in the weighted space @xmath46 , @xmath47 $ ] , @xmath48 and there holds @xmath49 where @xmath50    define @xmath51 then it is easy to see that @xmath52 is a complete orthonormal system for the weighted @xmath53 space @xmath46 , where @xmath54 .",
    "let @xmath55 be the unit ball in @xmath56 , @xmath57 be the unit sphere in @xmath56 and @xmath58 be the set of algebraic polynomials of degree not larger than @xmath43 defined on @xmath55 .",
    "denote by @xmath59 the aero element of @xmath57 . then @xmath60 the following important properties of @xmath61",
    "are established in @xcite .",
    "[ lem11 ] let @xmath61 be defined as above .",
    "then for each @xmath62 we have @xmath63 @xmath64 @xmath65 and @xmath66    where @xmath67 , and @xmath68 .      for any integer @xmath69 ,",
    "the restriction to @xmath70 of a homogeneous harmonic polynomial with degree @xmath71 is called a spherical harmonic of degree @xmath71 .",
    "the class of all spherical harmonics with degree @xmath71 is denoted by @xmath72 , and the class of all spherical polynomials with total degrees @xmath73 is denoted by @xmath74 .",
    "it is obvious that @xmath75 .",
    "the dimension of @xmath72 is given by @xmath76 and that of @xmath74 is @xmath77 where @xmath78 denotes that there exist absolute constants @xmath79 and @xmath80 such that @xmath81",
    ".    the well known addition formula is given by ( see @xcite and @xcite ) @xmath82 where @xmath83 is arbitrary orthonormal basis of @xmath84 .",
    "for @xmath85 and @xmath86 , we say that a finite subset @xmath87 is an @xmath88-covering of @xmath89 if @xmath90 where @xmath91 denotes the cardinality of the set @xmath92 and @xmath93 denotes the spherical cap with the center @xmath94 and the angle @xmath95 .",
    "the following positive cubature formula can be found in @xcite .    [",
    "fixed cubature ] there exists a constant @xmath96 depending only on @xmath11 such that for any positive integer @xmath43 and any @xmath97-covering of @xmath89 satisfying @xmath98 .",
    "there exists a set of numbers @xmath99 such that @xmath100      define @xmath101 then it follows from @xcite ( or @xcite ) that @xmath102 consists an orthonormal basis for @xmath58 , where @xmath103 of course , @xmath104 is an orthonormal basis for @xmath105 .",
    "the following lemma [ reproducing kernel ] defines a reproducing kernel of @xmath106 , whose proof will be presented in appendix a.    [ reproducing kernel ] the space @xmath107 is a reproducing kernel hilbert space .",
    "the unique reproducing kernel of this space is @xmath108",
    "in this section , we construct a concrete positive definite needlet kernel @xcite and show its properties .",
    "a function @xmath109 is said to be admissible if @xmath110 @xmath111 , and @xmath109 satisfies the following condition @xcite : @xmath112,\\eta(t)=1\\ \\mbox{on}\\ [ 0,1],\\ \\mbox{and}\\ 0\\leq\\eta(t)\\leq 1\\ \\mbox{on}\\ [ 1,2].\\ ] ] such a function can be easily constructed out of an orthogonal wavelet mask @xcite .",
    "we define a kernel @xmath113 as the following @xmath114    as @xmath115 is admissible , the constructed kernel @xmath116 called the needlet kernel ( or localized polynomial kernel ) @xcite henceforth , is positive definite .",
    "we will show that so defined kernel function @xmath117 , deduces the @xmath0 regularization learning whose learning rate is independent of the choice of @xmath1 . to this end",
    ", we first show several useful properties of the needlet kernel .",
    "the following proposition [ prop1.0 ] which can be deduced directly from lemma [ reproducing kernel ] and the definition of @xmath115 reveals that @xmath118 possesses reproducing property for @xmath58 .",
    "[ prop1.0 ] let @xmath118 be defined as in ( [ best kernel ] ) . for arbitrary @xmath119",
    ", there holds @xmath120    since @xmath115 is an admissible function by definition , it follows that @xmath121 is an algebraic polynomial of degree not larger than @xmath122 for any fixed @xmath123 . at the first glance , as a polynomial kernel",
    ", it may have good frequency localization property while have bad space localization property .",
    "the following proposition [ prop2 ] , which can be found in ( * ? ? ?",
    "* theorem 4.2 ) , however , advocates that @xmath118 is actually a polynomial kernel possessing very good spacial localized properties .",
    "this makes it widely applicable in approximation theory and signal processing @xcite .",
    "[ prop2 ] let @xmath118 be defined as in ( [ best kernel ] ) . for arbitrary @xmath124",
    ", there exists a constant @xmath125 depending only on @xmath126 , @xmath11 and @xmath109 such that @xmath127    let @xmath128 be the best approximation error of @xmath58 .",
    "define @xmath129    it has been shown in ( * ? ? ? * remak 4.8 ) that the integral operator @xmath130 possesses the following compressive property :    [ prop3.0 ] if @xmath131 is defined as in ( [ best operator ] ) , then , for arbitrary @xmath132 , there exists a constant @xmath133 depending only on @xmath11 and @xmath134 such that @xmath135    by propositions [ prop1.0 ] , [ prop2 ] and [ prop3.0 ] , a standard method in approximation theory @xcite yields the following best approximation property of @xmath136 .",
    "[ prop4 ] let @xmath137 and @xmath138 be defined in ( [ best operator ] ) , then for arbitrary @xmath139 , there exists a constant @xmath133 depending only on @xmath11 and @xmath134 such that @xmath140",
    "in this section , we conduct a detailed generalization capability analysis of the @xmath0 regularization scheme ( [ algorihtm ] ) when the kernel function @xmath141 is specified as @xmath117 .",
    "our aim is to derive an almost essential learning rate of @xmath0 regularization strategy ( [ algorihtm ] ) .",
    "we first present a quick review of learning theory .",
    "then , we given the main result of this paper , where a @xmath1-independent learning rate of @xmath0 regularization schemes ( [ algorihtm ] ) is deduced .",
    "at last , we present some remarks on the main result .",
    "let @xmath142 be an input space and @xmath143 an output space .",
    "assume that there exists a unknown but definite relationship between @xmath144 and @xmath145 , which is modeled by a probability distribution @xmath146 on @xmath147 .",
    "it is assumed that @xmath146 admits the decomposition @xmath148 let @xmath149 be a set of finite random samples of size @xmath30 , @xmath150 , drawn identically , independently according to @xmath146 from @xmath151 .",
    "the set of examples @xmath152 is called a training set . without loss of generality",
    ", we assume that @xmath153 almost everywhere .",
    "the aim of learning is to learn from a training set a function @xmath154 such that @xmath155 is an effective estimate of @xmath156 when @xmath157 is given .",
    "one natural measurement of the error incurred by using @xmath158 of this purpose is the generalization error , @xmath159 which is minimized by the regression function @xcite defined by @xmath160 we do not know this ideal minimizer @xmath161 , since @xmath146 is unknown , but we have access to random examples from @xmath162 sampled according to @xmath146 .",
    "let @xmath163 be the hilbert space of @xmath164 square integrable functions on @xmath165 , with norm @xmath166 in the setting of @xmath167 , it is well known that , for every @xmath168 , there holds @xmath169 the goal of learning is then to construct a function @xmath170 that approximates @xmath161 , in the norm @xmath171 , using the finite sample @xmath152 .",
    "one of the main points of this paper is to formulate the learning problem in terms of probability estimates rather than expectation estimates . to this end , we present a formal way to measure the performance of learning schemes in probability .",
    "let @xmath172 and @xmath173 be the class of all borel measures @xmath146 on @xmath151 such that @xmath174 .",
    "for each @xmath175 , we enter into a competition over all estimators established in the hypothesis space @xmath176 , @xmath177 and we define the accuracy confidence function by @xcite @xmath178 furthermore , we define the accuracy confidence function for all possible estimators based on @xmath30 samples @xmath179 by @xmath180 from these definitions , it is obvious that @xmath181 for all @xmath176 .",
    "the sample dependent hypothesis space ( sdhs ) associated with @xmath113 is then defined by @xmath182 and the corresponding @xmath0 regularization scheme is defined by @xmath183 where @xmath184    the projection operator @xmath185 from the space of measurable functions @xmath186 to @xmath187 $ ] is defined by @xmath188 as @xmath189 $ ] by assumption , it is easy to check @xcite that @xmath190 also , for arbitrary @xmath191 , we denote @xmath192 .",
    "we also need to introduce the class of priors . for any @xmath193 , denote by @xmath194 or @xmath195 the fourier transformation of @xmath158 , @xmath196 where @xmath197 .",
    "the inverse fourier transformation will be denoted by @xmath198 . in the space @xmath105 , the derivative of @xmath158 with order @xmath199",
    "is defined as @xmath200 where @xmath201 . here , fourier transformation and derivatives are all taken sense in distribution .",
    "let @xmath95 be any positive number .",
    "we consider the sobolev class of functions @xmath202 it follows from the well known sobolev embedding theorem that @xmath203 provided @xmath204    now , we state the main result of this paper , whose proof will be given in the next section .",
    "[ thm1 ] let @xmath205 with @xmath206 , @xmath150 , @xmath175 be any numbers , and @xmath207 .",
    "if @xmath22 is defined as in ( [ algorihtm1 ] ) with @xmath208 and @xmath38 , then there exist positive constants @xmath209 @xmath210 depending only on @xmath211 , @xmath146 , @xmath1 and @xmath11 , @xmath212 and @xmath213 satisfying @xmath214 such that for any @xmath215 , @xmath216 and for any @xmath217 , @xmath218      we explain theorem [ thm1 ] below in more detail . at first , we explain why the accuracy function is used to characterize the generalization capability of the @xmath0 regularization schemes ( [ algorihtm1 ] ) . in applications , we are often faced with the following problem : there are @xmath30 data available , and we are asked to product an estimator with tolerance at most @xmath219 by using these @xmath30 data only .",
    "in such circumstance , we have to know the probability of success .",
    "it is obvious that such probability depends on @xmath30 and @xmath219 .",
    "for example , if @xmath30 is too small , we can not construct an estimator within small tolerance .",
    "this fact is quantitatively verified by theorem [ thm1 ] .",
    "more specifically , ( [ negative ] ) shows that if there are @xmath30 data available and @xmath205 with @xmath220 , then @xmath0 @xmath221 regularization scheme ( [ algorihtm1 ] ) is impossible to yield an estimator with tolerance error smaller than @xmath222 .",
    "this is not a negative result , since we can see in ( [ negative ] ) also that the main reason of impossibility is the lack of data rather than inappropriateness of the learning scheme ( [ algorihtm1 ] ) .",
    "more importantly , theorem [ thm1 ] reveals a quantitive relation between the probability of success and the tolerance error based on @xmath30 samples .",
    "it says in ( [ theorem 1 ] ) that if the tolerance error @xmath219 is relaxed to @xmath223 or larger , then the probability of success of @xmath0 regularization is at least @xmath224 .",
    "the first inequality ( lower bound ) of ( [ theorem 1 ] ) implies that such confidence can not be improved further .",
    "that is , we have presented an optimal confidence estimation for @xmath0 regularization scheme ( [ algorihtm1 ] ) with @xmath38 .",
    "thus , theorem [ thm1 ] basically concludes the following thing : if @xmath225 , then every estimator deduced from @xmath30 samples by @xmath0 regularization can not approximate the regression function with tolerance smaller than @xmath219 , while if @xmath226 , then the @xmath0 regularization schemes with any @xmath38 can definitely yield the estimators that approximate the regression function with tolerance @xmath219 .    the values @xmath227 and @xmath223 thus are critical for indicating the generalization error of a learning scheme .",
    "indeed , the upper bound of generalization error of a learning scheme depends heavily on @xmath223 , while the lower bound of generalization error is relative to @xmath227 .",
    "thus , in order to have a tight generalization error estimate of a learning scheme , we naturally wish to make the interval @xmath228 $ ] as short as possible .",
    "theorem [ thm1 ] shows that , for @xmath0 regularization scheme ( [ algorihtm1 ] ) , @xmath229 , and @xmath230 , which shows that the interval @xmath228 $ ] is almost the shortest one in the sense that up to a logarithmic factor , the upper bound and lower bound are asymptotical identical . noting that the learning rate established in theorem [ thm1 ] is independent of @xmath1",
    ", we thus can conclude that the generalization capability of @xmath0 regularization does not depend on the choice of @xmath1 .",
    "this gives an affirmative answer to problem 1 .",
    "the other advantage of using the accuracy confidence function to measure the generalization capability is that it allows to expose some phenomenon that can not be founded if the classical expectation standard is utilized .",
    "for example , theorem [ thm1 ] shows a sharp phase transition phenomenon of @xmath0 regularization learning , that is , the behavior of the accuracy confidence function changes dramatically within the critical interval @xmath228 $ ] .",
    "it drops from a constant @xmath231 to an exponentially small quantity .",
    "we might call @xmath228 $ ] the interval of phase transition for a corresponding learning scheme . to make this more intuitive ,",
    "let us conduct a simulation on the phase transition of the confidence function below . without loss of generality , we implemented the @xmath29 regularization strategy ( [ algorihtm1 ] ) associated with the kernel ( [ best kernel ] ) for @xmath232 and @xmath233 to yield the estimator .",
    "the regularization parameter @xmath31 was chosen as @xmath234 .",
    "the training samples were drawn independently and identically according to the uniform distribution from the well known @xmath235 function , that is @xmath236 .",
    "the number of the training samples @xmath30 was chosen from @xmath237 to @xmath238 and the tolerance @xmath219 was chosen from @xmath239 to @xmath237 with step - length @xmath239 .",
    "then , there were totally 1000 test data @xmath240 drawn i. i. d according to the uniform distribution from @xmath241 .",
    "the test error was defined as @xmath242 we repeated 100 times simulations at each point , and labeled its value as @xmath237 if @xmath243 is smaller than the tolerance error and @xmath244 otherwise .",
    "simulation result is shown in fig.3 .",
    "we can see from fig.3 that in the upper right part , the colors of all points are red , which means that in those setting , the probability that @xmath243 is smaller than the tolerance is approximately @xmath244 .",
    "thus , if the number of samples is small , then @xmath29 regularization schemes can not provide an estimation with very small tolerance .",
    "in the lower left area , the colors of all points are blue , which means that the probability of @xmath243 smaller than the tolerance is approximately @xmath237 .",
    "between these two areas , there exists a band , that could be called the phase transition area , in which the colors of points vary from red to blue dramatically .",
    "it is seen that the length of phase transition interval monotonously decreases with @xmath30 .",
    "all these coincide with the theoretical assertions of theorem [ thm1 ] .",
    "regularization , width=415,height=302 ]    for comparison , we also present a generalization error bound result in terms of expectation error .",
    "corollary [ thm2 ] below can be directly deduced from theorem [ thm1 ] and ( * ? ? ?",
    "* chapter 3 ) , if we notice the identity : @xmath245    [ thm2 ] let @xmath205 with @xmath206 , @xmath246 , @xmath150 , and @xmath207 .",
    "if @xmath22 is defined as in ( [ algorihtm1 ] ) with @xmath247 and @xmath38 , then there exist constants @xmath248 and @xmath249 depending only on @xmath211 , @xmath11 , @xmath1 and @xmath146 such that @xmath250 where @xmath251 is the set of all possible estimators based on @xmath30 samples .    it is noted that the representation theorem in learning theory @xcite implies that the generalization capability of an optimal learning algorithm in sdhs is not worse than that of learning in rkhs with convex loss function .",
    "corollary [ thm2 ] then shows that if @xmath205 , then the generalization capability of an optimal learning scheme in sdhs associated with @xmath118 is not worse than that of any optimal learning algorithms in the corresponding rkhs .",
    "more specifically , ( [ generalization error ] ) shows that as far as the learning rate is concerned , all @xmath0 regularization schemes ( [ algorihtm1 ] ) for @xmath252 can realize the same almost optimal theoretical rate .",
    "that is to say , the choice of @xmath1 has no influence on the generalization capability of the learning schemes ( [ algorihtm1 ] ) .",
    "this also gives an affirmative answer to problem 1 in the sense of expectation .",
    "here , we emphasize that the independence of generalization of @xmath0 regularization on @xmath1 is based on the understanding of attaining the same almost optimal generalization error .",
    "thus , in application , @xmath1 can be arbitrarily specified , or specified merely by other no generalization criteria ( like complexity , sparsity , etc . ) .",
    "the methodology we adopted in the proof of theorem [ thm1 ] seems of novelty .",
    "traditionally , the generalization error of learning schemes in sdhs is divided into the approximation , hypothesis and sample errors ( three terms ) @xcite .",
    "all of the aforementioned results about coefficient regularization in sdhs falled into this style . according to @xcite",
    ", the hypothesis error has been regarded as the reflection of nature of data dependence of sdhs ( sample dependent hypothesis space ) , and an indispensable part attributed to an essential characteristic of learning algorithms in sdhs , compared with the learning in sihs ( sample independent hypothesis space ) . with the specific kernel function @xmath118",
    ", we will divide the generalization error of @xmath0 regularization in this paper into the approximation and sample errors ( two terms ) only .",
    "both of these two terms are dependent of the samples .",
    "the success in this paper then reveals that for at least some kernels , the hypothesis error is negligible , or can be avoided in estimation when @xmath0 regularization learning are analyzed in sdhs .",
    "we show that such new methodology can bring an important benefit of yielding an almost optimal generalization error bound for a large types of priors .",
    "such benefit may reasonably be expected to beyond the @xmath0 regularization .",
    "we sketch the methodology to be used as follows . due to",
    "the sample dependent property , any estimators constructed in sdhs may be a random approximant . to bound the approximation error",
    ", we first deduce a probabilistic cubature formula for algebraic polynomial .",
    "then we can discretize the near - best approximation operator @xmath130 based on the probabilistic cubature formula .",
    "thus , the well known jackson - type error estimate @xcite can be applied to derive the approximation error . to bound the sample error",
    ", we will use a different method from the tranditional approaches @xcite .",
    "since the constructed approximant in sdhs is a random approximant , the concentration inequality such as bernstein inequality @xcite can not be available . in our approach ,",
    "based on the prominent property of the constructed approximant , we will bound the sample error by using the concentration inequality established in @xcite twice . then the relation between the so - called pseudo - dimension and covering number @xcite yields the sample error estimate for @xmath0 regularization schemes ( [ algorihtm1 ] ) with arbitrary @xmath253 .",
    "hence , we divide the proof into four subsections . the first subsection is devoted to establish the probabilistic cubature formula .",
    "the second subsection is to construct the random approximant and study the approximation error .",
    "the third subsection is to deduce the sample error and the last subsectionis to derive the final learning rate .",
    "we present the details one by one below .      in this subsection , we establish a probabilistic cubature formula . at first , we need several lemmas . the weighted @xmath254 norm on the @xmath255-dimensional unit sphere @xmath256 is defined as follows .",
    "let @xmath257 and @xmath258 .",
    "define @xmath259    the following ( * ? ? ?",
    "* lemma 2.3 ) gives a weighted nikolskii inequality for spherical polynomial .",
    "[ weighted nikolskii ] let @xmath260 .",
    "then for any @xmath261 , @xmath262    where @xmath133 is a positive constant depending only on @xmath263 and @xmath1 .",
    "lemma [ relation ball sphere ] establishes a relation between cubature formula on the unit sphere and cubature formula on the unit ball , which can be found in ( * ? ? ?",
    "* theorem 4.2 ) .",
    "[ relation ball sphere ] if there is a cubature formula of degree @xmath43 on @xmath256 given by @xmath264 whose nodes are all located on @xmath256 , then there exists a cubature formula of degree @xmath43 on @xmath55 , that is , @xmath265 where @xmath266 are the first @xmath11 components of @xmath267 .",
    "the following lemma [ bernstein ] is known as the bernstein inequality for random variables , which can be found in @xcite .",
    "[ bernstein ] let @xmath94 be a random variable on a probability space @xmath151 with mean @xmath268 , variance @xmath269 .",
    "if @xmath270 for almost all @xmath271 . then , for all @xmath175 , @xmath272    we also need a lemma showing that if @xmath273 is a set of independent random variables drawn identically according to a distribution @xmath44 , then with high confidence the cubature formula holds .",
    "[ random cubature on sphere ] let @xmath274 , and @xmath275 . if @xmath276 are i.i.d .",
    "random variables drawn according to arbitrary distribution @xmath44 on @xmath256 , then there exits a set of real numbers @xmath277 such that @xmath278 holds with confidence at least @xmath279 subject to @xmath280    * proof .",
    "* for the sake of brevity , we write @xmath281 in the following . since the sampling set @xmath282 consists of a sequence of i.i.d .",
    "random variables on @xmath283 , the sampling points are a sequence of functions @xmath284 on some probability space @xmath285 . without loss of generality , we assume @xmath286 for arbitrary fixed @xmath134 . if we set @xmath287 , then we have @xmath288 where we have used the equality @xmath289 furthermore , @xmath290 it follows from lemma [ weighted nikolskii ] that @xmath291 hence @xmath292 on the other hand , we have @xmath293 then using lemma [ weighted nikolskii ] again , there holds @xmath294 thus it follows from lemma [ bernstein ] that with confidence at least @xmath295 there holds @xmath296 this means that if @xmath282 is a sequence of i.i.d .",
    "random variables , then the marcinkiewicz - zygmund inequality @xmath297 holds with probability at least @xmath298 then , almost same argument as that in ( * ? ? ?",
    "* theorem 4.1 ) or ( * ? ?",
    "* theorem 4.2 ) implies lemma [ random cubature on sphere ] .",
    "@xmath299    by virtue of the above lemmas , we can prove the following proposition [ probabilistic cubature ] .",
    "[ probabilistic cubature ] let @xmath275 and @xmath300 be a set of random variables independently and identically drawn according to arbitrary distribution @xmath44 .",
    "then there exits a set of real numbers @xmath277 and a constant @xmath133 depending only on @xmath11 such that the equality @xmath301 holds with confidence at least @xmath302 subject to @xmath303      to estimate the upper bound of @xmath304 we first introduce an error decomposition strategy .",
    "it follows from the definition of @xmath22 that , for arbitrary @xmath305 , @xmath306    since @xmath205 with @xmath307 , it follows from the sobolev embedding theorem that @xmath308 .",
    "thus , it can be deduced from proposition [ prop3.0 ] and proposition [ prop4 ] that there exists a @xmath309 such that @xmath310}(f_\\rho),\\ ] ] where @xmath311 $ ] denotes the largest integer not larger than @xmath312 and @xmath313 denotes the uniform norm on @xmath55 .",
    "the above inequalities together with the well known jackson inequality @xcite imply that there exists a @xmath309 such that for all @xmath205 with @xmath307 , there holds @xmath314    let @xmath315 , where @xmath316 is defined as in ( [ definitionc ] ) .",
    "define @xmath317 then we have @xmath318 where @xmath319 and @xmath320 is called the approximation error and sample error , respectively .",
    "[ approximation error ] let @xmath321 , @xmath206 and @xmath205 .",
    "then , with confidence at least @xmath322 there holds @xmath323 where @xmath133 and @xmath316 are constants depending only on @xmath11 and @xmath95 .    *",
    "proof . * from proposition [ prop1.0 ] , it is easy to deduce that @xmath324 thus , lemma [ probabilistic cubature ] with @xmath325 yields that with confidence at least @xmath322 there exists a set of real numbers @xmath277 satisfying @xmath326 for @xmath327 such that @xmath328 the above observation together with ( [ app1 ] ) implies that with confidence at least @xmath322 there exists a @xmath329 such that for arbitrary @xmath205 , there holds @xmath330 and @xmath331 where @xmath133 is a constant depending only on @xmath11 and @xmath211 . indeed , if @xmath332 , we have @xmath333 . without loss of generality , we assume @xmath334",
    ". then there holds @xmath335 if @xmath336 , it follows from the hlder inequality that @xmath337 thus , for all @xmath338 , there holds @xmath339 it thus follows from the definition of @xmath340 that the inequalities @xmath341 holds with confidence at least @xmath342 @xmath299      for further use , we also need introducing some quantities to measure the complexity of a space @xcite .",
    "let @xmath343 be a banach space and @xmath344 a compact set in @xmath343 .",
    "the quantity @xmath345 , where @xmath346 is the number of elements in least @xmath219-net of @xmath344 , is called @xmath219-entropy of @xmath344 in @xmath343 .",
    "the quantity @xmath346 is called the @xmath219-covering number of @xmath344 . for any @xmath347 , define @xmath348 if a vector @xmath349 belongs to @xmath350 , then we denote by @xmath351 the vector @xmath352 @xmath353,@xmath354 .",
    "the vc dimension of a set @xmath344 over @xmath55 , denoted as @xmath355 , is the maximal natural number @xmath30 such that there exists a collection @xmath356 in @xmath55 such that the cardinality of the sgn - vectors set @xmath357 equals to @xmath358 , that is , the set @xmath359 coincides with the set of all vertexes of unit cube in @xmath360 .",
    "the quantity @xmath361 is called pseudo - dimension of the set @xmath344 over @xmath55 , where @xmath362 runs all functions defined on @xmath55 and @xmath363 .",
    "mendelson and vershinin @xcite ( see also @xcite ) has established the following important relation between pseudo - dimension and @xmath219-entropy .",
    "[ relation covering number ] let @xmath364 be a class of functions which consists of all functions @xmath365 satisfying @xmath366 for all @xmath367 . then , @xmath368 where @xmath316 is an absolute positive constant .",
    "the following lemma [ covering number ] @xcite further shows that the pseudo - dimension of arbitrary @xmath30-dimensional vector space is @xmath30 .",
    "[ covering number ] let @xmath176 be an @xmath30-dimensional vector space of functions from @xmath55 into @xmath369",
    ". then @xmath370 .",
    "we also need to apply the following concentration inequality @xcite .",
    "[ concentration inequality ] let @xmath371 be a set of functions on @xmath151 such that , for some @xmath372 , @xmath373 almost everywhere and @xmath374 for each @xmath375 . then , for every @xmath175 , @xmath376    the following proposition [ sample error estimate ] give an upper bound of sample error .",
    "[ sample error estimate ] let @xmath321 , @xmath175 , and @xmath377 be defined as in ( [ algorihtm1 ] ) . then with confidence at least @xmath378 there holds @xmath379    * proof .",
    "* if we set @xmath380 and @xmath381 then @xmath382 both of which are random variables .",
    "hence , we can rewrite the sample error as @xmath383    define @xmath384 as @xmath385 , it follows from ( [ algorihtm1 ] ) that @xmath386 which implies @xmath387 .",
    "let @xmath388 then , for any fixed @xmath389 there exists @xmath390 such that @xmath391 .",
    "it is easy to deduce that @xmath392 @xmath393 and @xmath394.\\ ] ] since @xmath395 and @xmath396 almost everywhere , we find that @xmath397 of course , we have @xmath398 almost everywhere and @xmath399\\\\                 & \\leq &                  16m^2\\|\\pi_mf - f_\\rho\\|_\\rho^2=16m^2e(g),\\end{aligned}\\ ] ]    therefore , we can apply lemma [ concentration inequality ] to the set of functions @xmath400 with @xmath401 , yielding @xmath402 with confidence at least @xmath403 for every @xmath404 , we have @xmath405 thus , a @xmath406-covering of @xmath407 provides an @xmath219-covering of @xmath408 for any @xmath175 .",
    "this implies @xmath409 it is also needed to derive an upper bound estimation for @xmath410 . for @xmath327 , and @xmath411",
    ", it follows from proposition [ prop2 ] and the hlder inequality that @xmath412 for @xmath336 , and @xmath411 , using ( [ kernelbound ] ) again we can obtain @xmath413 consequently , for arbitrary @xmath411 and arbitrary @xmath252 , there holds @xmath414 noting that @xmath415 is a finite dimensional linear space with its dimension not larger than @xmath416 , it follows from lemma [ covering number ] and lemma [ relation covering number ] that @xmath417 accordingly , @xmath418 which together with ( [ tools1 ] ) further yields @xmath419 with confidence at least @xmath420    now , we turn to estimate @xmath421 . by definition of @xmath422 , we have @xmath423 .",
    "let @xmath424 then for any fixed @xmath425 there exists an @xmath426 such that @xmath427 .",
    "similarly , we have @xmath428 since @xmath395 , @xmath396 and @xmath429 almost everywhere , we get @xmath430 almost everywhere . furthermore , @xmath431    then we apply lemma [ concentration inequality ] again to the set of functions @xmath371 with @xmath432 and obtain @xmath433 with confidence at least @xmath434 for every @xmath435 , we have @xmath436 thus , for any @xmath175 , a @xmath437-covering of @xmath438 provides an @xmath219-covering of @xmath371 .",
    "this means @xmath439 by definition of @xmath440 , we then deduce from ( * ? ?",
    "* theorem 5.3 ) that @xmath441 hence , @xmath442 which together with ( [ tools2 ] ) yields @xmath443 with confidence at least @xmath444 this finishes the proof of proposition [ sample error estimate ] .",
    "@xmath299      now we are in a position to deduce the final learning rate of @xmath0 regularization schemes ( [ algorihtm1 ] ) .",
    "firstly , it follows from propositions [ approximation error ] and [ sample error estimate ] that @xmath445 holds with confidence at least @xmath446 then , by setting @xmath447 , @xmath448 $ ] and @xmath208 , it follows from @xmath206 that @xmath449 that is , for @xmath450 @xmath451 holds with confidence at least @xmath452 .    the lower bound can be more easily deduced .",
    "actually , it follows from ( * ? ? ?",
    "* equation ( 3.27 ) ) ( see also @xcite ) that for any estimator @xmath453 , there holds @xmath454 where @xmath455 and @xmath456 for some universal constant @xmath316 . with this , the proof of theorem [ thm1 ] is completed .",
    "in studies and applications , regularization is a fundamental skill to improve on performance of a learning machine .",
    "the @xmath0 regularization schemes ( [ algorihtm ] ) with @xmath38 are well known to be central in use . in this paper",
    ", we have studied the dependency problem of the generalization capability of @xmath0 regularization with the choice of @xmath1 . through formulating a new methodology of estimation of generalization error",
    ", we have shown that there is at least a positive definite kernel , say , @xmath457 , such that associated with such a kernel , the learning rate of the @xmath0 regularization schemes is independent of the choice of @xmath1 .",
    "( to be more precise , we verified that with the kernel @xmath118 , all @xmath0 regularization schemes ( [ algorihtm ] ) can attain the same almost optimal learning rate in the following sense : up to a logarithmic factor , the upper and lower bounds of generalization error of the @xmath0 regularization schemes are asymptotically identical ) .",
    "this implies that for some kernels , the generalization capability of @xmath0 regularization may not depend on @xmath1 .",
    "therefore , as far as the generalization capability is concerned , for those kernels , the choice of @xmath1 is not important , which then relaxes the model selection difficulty in applications .",
    "the problem is , however , far complicated .",
    "we have also illustrated in section 2 that there exists a kernel with which the generalization capability of @xmath0 regularization heavily depends on the choice of @xmath1 .",
    "thus , answering completely whether or not the choice of @xmath1 affects the generalization of @xmath0 regularization is by no means easy and completed .    though we have constructed a concrete kernel example , the localized polynomial kernel @xmath118 , with which implementing the @xmath0 regularization in sdhs can realize the almost optimal learning rate , and this is independence of the choice of @xmath1 , we have not provided a practically feasible algorithm to implement the learning with the almost optimal generalization capability .",
    "this is because the kernel @xmath118 we have constructed is not easily computed in practice , even though we can use the cubature formula ( lemma [ fixed cubature ] ) to discretize it .",
    "thus , seeking the kernels that possesses the similar property as that of @xmath118 and can be implemented easily deserve study .",
    "this is under our current investigation .",
    "to prove lemma [ reproducing kernel ] , we need the following aronszajn theorem ( see @xcite ) .    [ aronszajn ] let @xmath176 be a separable hilbert space of functions over @xmath165 with orthonormal basis @xmath458 .",
    "@xmath176 is a reproducing kernel hilbert space if and only if @xmath459 for all @xmath144 .",
    "the unique reproducing kernel @xmath141 is defined by @xmath460    * proof of lemma [ reproducing kernel ] . *",
    "since @xmath102 is an orthonormal basis for @xmath58 , for arbitrary @xmath461 , there exists a set of real numbers @xmath462 such that @xmath463 where the summation concerning the index @xmath464 is @xmath465 . on the other hand",
    ", it follows from ( [ basis ] ) that @xmath466 thus , the addition formula ( [ addition ] ) yields @xmath467 the above equality together with ( [ 3 ] ) and ( [ 4 ] ) implies @xmath468 therefore , there holds @xmath469 the above equality together with lemma [ aronszajn ] yields lemma [ reproducing kernel ] ."
  ],
  "abstract_text": [
    "<S> @xmath0-regularization has been demonstrated to be an attractive technique in machine learning and statistical modeling . </S>",
    "<S> it attempts to improve the generalization ( prediction ) capability of a machine ( model ) through appropriately shrinking its coefficients . </S>",
    "<S> the shape of a @xmath0 estimator differs in varying choices of the regularization order @xmath1 . </S>",
    "<S> in particular , @xmath2 leads to the lasso estimate , while @xmath3 corresponds to the smooth ridge regression . </S>",
    "<S> this makes the order @xmath1 a potential tuning parameter in applications . to facilitate the use of @xmath4-regularization </S>",
    "<S> , we intend to seek for a modeling strategy where an elaborative selection on @xmath1 is avoidable . in this spirit </S>",
    "<S> , we place our investigation within a general framework of @xmath4-regularized kernel learning under a sample dependent hypothesis space ( sdhs ) . for a designated class of kernel functions , </S>",
    "<S> we show that all @xmath4 estimators for @xmath5 attain similar generalization error bounds . </S>",
    "<S> these estimated bounds are almost optimal in the sense that up to a logarithmic factor , the upper and lower bounds are asymptotically identical . </S>",
    "<S> this finding tentatively reveals that , in some modeling contexts , the choice of @xmath1 might not have a strong impact in terms of the generalization capability . from this perspective </S>",
    "<S> , @xmath1 can be arbitrarily specified , or specified merely by other no generalization criteria like smoothness , computational complexity , sparsity , etc ..    * keywords : * learning theory , @xmath0 regularization learning , sample dependent hypothesis space , learning rate + * msc 2000 : * 68t05 , 62g07 .    \\1 . </S>",
    "<S> institute for information and system sciences , school of mathematics and statistics , xian jiaotong university xian 710049 , p r china    \\2 . </S>",
    "<S> the methodology center , the pennsylvania state university , department of statistics , 204 e. calder way , suite 400 , state college , pa 16801 , usa </S>"
  ]
}