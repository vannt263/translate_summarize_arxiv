{
  "article_text": [
    "usually , one solves @xmath0 , where @xmath1 is a noise vector , to full convergence .",
    "deforcrand pointed out@xcite that this is not necessary for gaussian noise .",
    "formally , @xmath2 introduce an auxilliary field , @xmath3 consider a change , @xmath4 where @xmath1 is a complex gaussian noise vector and @xmath5 is the residual vector in the solution for @xmath6 .",
    "one can show the change is accepted with probability @xmath7 where @xmath8    with the assumption that @xmath9 , @xmath10 , and @xmath5 are uncorrelated gaussian vectors of variance @xmath11 , @xmath11 and @xmath12 respectively ( @xmath11 is the dimensionality of @xmath13 ) , deforcrand shows that @xmath14 ( @xmath15 ) the computational overhead is simply one matrix - vector product plus several matrix dot products per acceptance check .",
    "this can save a factor of 2 to 3 in computer time .",
    "this is the general idea ; generalizations are also presented by deforcrand .",
    "gaussian noise is not optimal for signal extraction@xcite .",
    "therefore it is of interest to see if heatbath methods can be adapted to use a mixture of gaussian and z(n ) noise ( z(2 ) used here ) .    for this purpose ,",
    "we begin with the expression , @xmath16 where @xmath17 is a particular z(2 ) noise vector and @xmath18 is the number of z(2 ) noises in the vector space and @xmath19 . let @xmath20",
    ". one can do the integrals to get , @xmath21.\\end{aligned}\\ ] ] then with @xmath22 we see that the answer is just @xmath23 , but with a weighting over gaussian ( @xmath24 ) and z(2 ) ( @xmath25 ) noises .",
    "we introduce @xmath26 as before , @xmath27 treating @xmath17 as a dynamical variable , the change in the action is now , @xmath28.\\end{aligned}\\ ] ] this is again a heatbath , with @xmath29 where @xmath30 and @xmath31 .",
    "this is of the same form as above and has the same acceptance since @xmath32 and @xmath33 have variance @xmath11 .",
    "the rescaled @xmath34 is used to define the residual , @xmath35 , in the computer program .",
    "although the acceptance is the same , the number of iterations is greater for a given cutoff , @xmath35 , on the new residual vector since it is defined by dividing by @xmath36 .",
    "however , since the convergence on the residual is typically exponential , changes in @xmath37 are accomodated by a modest number of extra iterations .",
    "the fact that the acceptance is the same at the rescaled @xmath35 value we will see has the helpful consequence that one does not have to re - search for the optimum @xmath35 value at which to run , even though now we use a noise mixture .",
    "the above discussion can be generalized to justify more complicated exponential shifts @xmath38 where @xmath39 can depend on other noises , parameters , etc . , besides the specific one @xmath40 used above .",
    "i am using a solver which has enforced even / odd preconditioning in the wilson / dirac matrix @xmath41 .",
    "one can show that in this case one is not simulating @xmath42 directly as in the above discussion , but a different , shifted system and the residual vector is purely in one sector ( even / odd ) or the other .    figs .  1 and 2 show acceptance data on a @xmath43 lattice using the wilson / dirac matrix at @xmath44 .",
    "i am calculating the average acceptance on 100 noises at various cutoff values of @xmath35 and with @xmath45 and 1 .",
    "for this solver i get about a @xmath46 increase in the number of iterations for this ratio .",
    "( the model suggests about @xmath47 increase in the number of iterations for an @xmath48 mixture ( @xmath49 ) and about a @xmath50 increase for a @xmath51 mixture ( @xmath52 ) for the parameters in this simulation . )",
    "1 shows @xmath53 in the crucial window of @xmath35 of from @xmath54 through @xmath55 , where it varies between @xmath56 to @xmath57 for @xmath58 for a single gauge configuration .",
    "the acceptance is the same for both @xmath59 values within errors and agrees with eq.([acc ] ) .",
    "the reason this interval is crucial is illustrated in fig .",
    "we see that the number of mixed z(2)/gaussian heatbath iterations divided by the acceptance , @xmath60 , has a minimum at @xmath61 for @xmath58 . shown here",
    "also is the model value for this quantity given by @xmath62 where @xmath63 is assumed determined by @xmath64 .",
    "as noted above , i am using a solver which has enforced even / odd preconditioning in the wilson / dirac matrix @xmath41 .",
    "the upshot for this simulation is that @xmath11 in ( [ acc ] ) and ( [ num ] ) must include a factor of @xmath65 , @xmath66 .",
    "fig.3 shows the normed effective mixed z(2)/gaussian variance in the operator @xmath67 as a function of @xmath59 .",
    "it has a very shallow minimum at @xmath68 . by normed effective variance",
    "i mean the ratio @xmath69 , which takes into account that @xmath70 for @xmath71 .",
    "the model and data suggest that the minimum of this ratio is @xmath72 .",
    "the horizontal line gives the fully converged value of the variance ratio @xmath73 , which is @xmath74 .",
    "thus , one looses a factor of @xmath75 in the effective variance compared to the fully converged z(2 ) simulation . the gain in computer time",
    "is reduced from a factor of from 2 to 3 to a factor of from 1.33 to 2 .",
    "these numbers are apparently typical .",
    "other operators with larger converged @xmath73 ratios have sharper minima at smaller @xmath59 .",
    "ref.@xcite shows that heatbath methods can speed up simulations of many disconnected loop operators or by a factor of @xmath76 to 3 . however , gaussian noise is not optimal and so heatbath methods do not help for operators whose variance is diagonally dominate , such as wilson @xmath67 .",
    "it has been shown here that these methods can be generalized to a mixture of gaussian and z(n ) noise . with an mixture / iteration penalty factor of about @xmath77 ,",
    "the gaussian noise accelerates the z(n ) sector .    for diagonally dominate operators",
    ", there exists a z(n)/gaussian ratio that minimizes the variance .",
    "the noise can then be tailored to the operator ( designer noise \" ) .",
    "the optimum @xmath59 parameter can be numerically estimated from the parameters of the model and an independent measurement of the fully converged z(n)/gaussian variance ratio .",
    "it is not shown here , but the even / odd structure of the wilson / dirac matrix can be exploited to increase the computer time gain of diagonally dominate operators further by restricting the gaussian noise to only one sector or the other . this and other aspects of heatbath noise methods will be discussed in a future publication .",
    "this work was supported by nsf grant no .",
    "0070836 and the baylor university sabbatical program .",
    "the calculations were done at ncsa and utilized the sgi origin 2000 system at the university of illinois .",
    "the author thanks p.",
    "de forcrand for helpful comments ."
  ],
  "abstract_text": [
    "<S> in a recent paper , de forcrand has pointed out that matrix inversions using gaussian noise need not be iterated to full convergence , but instead may be solved approximately and treated as a heatbath . </S>",
    "<S> gaussian noise however is not optimal for minimizing variance . </S>",
    "<S> it shown here how his algorithm may be generalized to a mixture of gaussian and z(n ) noise , resulting in a smaller effective variance for some operators .    </S>"
  ]
}