{
  "article_text": [
    "intel s tera - scale computing vision ( _ a parallel path to the future _ ) is to aim for hundreds of cores on a chip . in their white paper",
    "@xcite intel puts `` programmability '' at the top of the list of challenges for the multi - core era , and considers the development of multi - core software to be amongst the greatest challenges for tera - scale computing .",
    "however , hill and marty @xcite propose that in exploiting multi - cores we should not just aim for _ speedup _ but also _ costup _ , i.e. an increase in performance that is greater than the increase in cost , be it measured in money or energy .    and that is our target , to maximise costup .",
    "we were presented with the following challenge .",
    "the second author had just completed an empirical study of exact algorithms for the maximum clique problem and a variety of algorithms had been implemented in java @xcite .",
    "being summer , a university environment and the students away , we had access to over 100 teaching machines that we could use but not modify ( limiting us to using ssh , nfs and java ) .",
    "we did nt have a shared memory system with hundreds of cores but we did have access to a hundred networked pcs , but only for four weeks .",
    "could we take one of our programs , make minimal changes to it , distribute it over the machines available and solve some big hard problems quickly ? to be more precise , starting mid - week ( wednesday 4th july )",
    "could we do this by friday ( the 6th ) , use the available resources over the weekend ( 7th and 8th ) , solve some big hard problems and analyse the results on monday ( the 9th ) ?",
    "the problem studied was the _",
    "maximum clique problem_.    a * simple undirected graph * g is a pair ( v(g),e(g ) ) where v(g ) is a set of vertices and e(g ) a set of edges .",
    "an edge @xmath0 is in e(g ) if and only if @xmath1 v(g ) and vertex @xmath2 is adjacent to vertex @xmath3 .",
    "a * clique * is a set of vertices c @xmath4 v(g ) such that every pair of vertices in c is adjacent in g.    clique is one of the six basic np - complete problems given in @xcite .",
    "it is posed as a decision problem [ gt19 ] : given a simple undirected graph g = ( v(g),e(g ) ) and a positive integer k @xmath5 @xmath6v(g)@xmath6 does g contain a clique of size k or more ?",
    "the optimization problems is then to find a _",
    "maximum clique _ , whose size is denoted @xmath7(g ) .",
    "a graph can be _ coloured _ by assigning colour values to vertices such that adjacent vertices take different colour values .",
    "the minimum number of different colours required is then the _ chromatic number _ of the graph @xmath8(g ) , and @xmath7(g ) @xmath9(g ) .",
    "therefore a colouring of the graph g can be used as an upper bound on @xmath7(g ) . finding the chromatic number is np - hard , but fast approximations exist such as @xcite .    in the next section",
    "we describe our starting point , the algorithm mc .",
    "we then show a simple modification to the algorithm that allows distribution at various levels of granularity .",
    "implementation details are then given followed by a report of our computational study .",
    "we then reflect , considering what we might have done differently given more time , and then conclude .",
    "we can address the optimization problem with an exact algorithm , such as a backtracking search @xcite .",
    "backtracking search incrementally constructs the set c ( initially empty ) by choosing a _ candidate vertex _ from the _ candidate set _ p ( initially all of the vertices in v(g ) ) and then adding it to c. having chosen a vertex the candidate set is then updated , removing vertices that can not participate in the evolving clique .",
    "if the candidate set is empty then c is maximal ( and if it is a maximum we save it ) and we then backtrack .",
    "otherwise p is not empty and we continue our search , selecting from p and adding to c.    there are other scenarios where we can cut off search , e.g. if what is in p is insufficient to unseat the champion ( the largest clique found so far ) then search can be abandoned .",
    "that is , an upper bound can be computed .",
    "graph colouring can be used to compute an upper bound during search , i.e. if the candidate set can be coloured with @xmath10 colours then it can contain a clique no larger than @xmath10 @xcite .",
    "there are also heuristics that can be used when selecting the candidate vertex , different styles of search , different algorithms to colour the graph and different orders in which to do this .",
    "for our study we use mc ( * m*aximum * c*lique ) , algorithm [ mc ] .",
    "mc is essentially algorithm mcsa1 in @xcite and corresponds to tomita s mcs @xcite with the colour repair step removed .",
    "mcsa1 is a state of the art algorithm and a close competitor to san segundo s bbmc @xcitepat / maxclique ] .",
    "the algorithm performs a binomial search ( see pages 6 and 7 of @xcite ) and uses a colour cutoff .",
    "vertices are selected from the candidate set p and added to the growing clique c. the graph induced by the vertices in p is coloured such that each vertex in p has an associated colour .",
    "vertices are then selected in non - increasing colour order ( largest colour first ) for inclusion in c. assume a vertex v is selected from p and has colour k. the graph induced by the vertices in p , including v , can be coloured with k colours and can therefore contain a clique no larger than k. consequently if the cardinality of c plus k is no larger than that of the largest clique found so far search can be abandoned .",
    "crucial to the success of the algorithm is the quality of the colouring of the candidate set and the time taken to perform that colouring .",
    "empirical evidence suggests that good performance can be had with any of the three colour orderings studied in @xcite .",
    "algorithms 1 and 2 are presented as procedures that deliver a result , possibly void ( line 12 ) and possibly a tuple ( line 22 ) .",
    "we assume that a * set * is an order preserving structure such that when an item @xmath3 is added to a * set * @xmath11 , i.e. @xmath12 , the last element in @xmath11 will be @xmath3 and when @xmath13 the elements in @xmath14 will occur in the same order as they appear in @xmath15 .",
    "mc , algorithm [ mc ] , takes as parameter a graph @xmath16 and has three global variables ( lines 3 to 5 ) : integer @xmath17 the number of vertices in @xmath16 , @xmath18 the set of vertices in the largest maximal clique found so far and integer @xmath19 the size of that clique .",
    "mc then calls expand ( line 8) to explore the backtrack tree to find a largest clique in @xmath16 .",
    "procedure @xmath20 is called ( in line 8) with three arguments : the candidate set @xmath21 ( line 6 ) and the growing clique @xmath22 ( line 7 , initially empty ) and the graph @xmath16 .",
    "initially the candidate set contains all the vertices in the graph , v(g ) , and is sorted in non - increasing degree order ( the call to @xmath23 in line 6 ) and this order is then used for colouring the graph induced by @xmath21 .",
    "procedure @xmath20 starts by colouring the graph induced by @xmath21 ( step 12 ) , delivering a pair @xmath24 where @xmath11 is a stack of vertices and colour is an array of colours ( integers )",
    ". if vertex @xmath3 is at the top of the stack then vertex @xmath3 has colour @xmath25 $ ] and all vertices in the stack have a colour less than or equal to @xmath25 $ ] .",
    "the procedure iterates over the stack ( while loop of line 13 ) , selecting and removing a vertex from the top of the stack ( line 14 )",
    ". if the colour of that vertex is too small then the graph induced by @xmath3 and the remaining vertices in the stack and the vertices in the growing clique @xmath22 will be insufficient to unseat the current champion and search can be terminated ( line 15 ) .",
    "otherwise the vertex @xmath3 is added to the clique ( line 16 ) and a new candidate set is produced @xmath26 ( line 17 ) where @xmath26 is the set of vertices in @xmath21 that are adjacent to the current vertex @xmath3 ( where @xmath27 delivers the _ neighbourhood _ of @xmath3 in @xmath16 ) , consequently each vertex in @xmath26 is adjacent to all vertices in @xmath22 . if the new candidate set is empty then @xmath22 is maximal and if it is larger than the largest clique found so far it is saved ( line 18 ) .",
    "but if @xmath26 is not empty @xmath22 is not maximal and @xmath22 can grow via the recursive call to @xmath20 ( line 19 ) .",
    "regardless , when all possibilities of expanding the current clique with the vertex @xmath3 have been considered that vertex can be removed from the current clique ( line 20 ) and from the candidate set ( line 21 ) .",
    "procedure @xmath28 , line 22 , corresponds to tomita s number - sort in @xcite .",
    "the vertices in @xmath21 are sequentially coloured ( assigned a colour number ) and sorted ( the vertices are delivered as a stack such that the vertices in the stack appear in descending colour order , largest colour at top of stack ) . in line 24",
    "an integer array of colours is created and in line 25 an array of sets is produced , such that the set @xmath29 $ ] contains non - adjacent vertices that have colour @xmath10 , i.e. @xmath29 $ ] is an _",
    "independent set_. the candidate set is iterated over in line 28 , and this will be in non - increasing degree order as a consequence of the initial sorting in line 6 .",
    "line 30 searches for a colour class for the current vertex @xmath3 : if any vertex in @xmath29 $ ] is adjacent to @xmath3 we then look in the next colour class ( increment @xmath10 ) ( lines 37 to 41 ) delivers @xmath30 if in the graph @xmath16 vertex @xmath3 is adjacent to a vertex in the set @xmath11 . ] . in line 31",
    "we have found a suitable @xmath29 $ ] ( possibly empty ) and we add @xmath3 to that @xmath31 , assign that colour to the vertex ( line 32 ) and take note of the number of colours used ( line 33 ) .",
    "once all vertices have been added to colour classes we sort the vertices using a pigeonhole sort ( lines 34 and 35 ) : for each colour class we push the vertices in that colour class onto the stack .",
    "the procedure then finishes by returning the colour - sorted vertices with their colours ( line 36 ) as a pair .",
    "@xmath32 @xmath33 @xmath34)~coloursort(\\textbf{set}~p,\\textbf{graph}~g)$ ] @xmath35",
    "there are a number of ways we might distribute mc across @xmath36 processors .",
    "we could split the problem into @xmath36 parts , run each part individually and merge the results , as a map - reduce style implementation .",
    "but we can not partition the problem into @xmath36 roughly - equally sized chunks .",
    "we could instead split the problem into @xmath17 parts , where @xmath17 is the number of vertices in the graph , and then use a worker pool model of execution .",
    "each job would expand a backtrack tree rooted on a node at level 1 where the current clique contained a single vertex .",
    "that is , we kick off @xmath17 jobs each with a different clique of size one . but given the uneven size of the search trees , @xmath17 is likely still too small to give well - balanced workloads . more generally , we could divide at level 2 potentially kicking off @xmath37 jobs where each process has an initial clique containing two adjacent vertices .",
    "more generally we might kick off @xmath38 jobs where each job expands @xmath39 backtrack trees rooted at specified nodes at depth @xmath10 .",
    "mcdist allows us to do that .    in algorithm [ mcdist ]",
    "mcdist ( line 1 ) takes the following arguments : the graph @xmath16 , a set of sets @xmath40 where each element of @xmath40 is of size @xmath41 , and integer @xmath42 the size of the largest clique reported by other processes .",
    "elements of the set @xmath40 describe the nodes in the backtrack tree to be expanded by this process .",
    "for example , if @xmath43 arity would equal 1 and a call to @xmath44 would explore the backtrack tree rooted on the clique @xmath45 . if @xmath46 a call to @xmath44 would be equivalent to the call @xmath47 . ] . if @xmath48 with @xmath49 and @xmath50 @xmath51 would start expanding search from all triangles ( level 3 ) looking for cliques of size greater than @xmath10 . and",
    "finally , we might divide the set of edges @xmath52 equally amongst the sets @xmath53 to @xmath54 and distribute calls to @xmath55 , for @xmath56 , across the available processors .",
    "each call to @xmath55 would occur within a separate job and the current best clique size @xmath42 would be used at dispatch time .    in algorithm [ mcdist ]",
    "we replace expand with its distributed counterpart distexpand .",
    "the essential difference between @xmath20 and @xmath57 is the call to @xmath58 in line 16 , i.e. the lines 16 to 20 in algorithm [ mc ] are now executed conditionally .",
    "procedure @xmath58 determines if the current clique can be considered for expansion .",
    "if the clique is below or above the critical size then expansion can proceed , otherwise search can proceed only if @xmath59 and @xmath22 corresponds to a specified node of interest , i.e. @xmath60 .    @xmath61 @xmath62 @xmath63    procedure @xmath57 is similar to the search state re - computation technique used by @xcite and it can be made more efficient .",
    "assume the argument @xmath41 equals @xmath64 .",
    "a call to @xmath28 is made on each call to @xmath57 as the clique @xmath65 is incrementally constructed ( repeatedly passing the test @xmath66 at line 25 ) although it might ultimately be rejected when @xmath67 ( failing the test @xmath60 at line 25 ) . in the worst case ,",
    "if @xmath40 was empty or contained a single tuple that did not correspond to any node in the backtrack tree , @xmath28 would be called @xmath68 times to no effect .",
    "this is the cost of making a simple modification to @xmath69 to give us @xmath51 .",
    "however in practice much of this cost is easily avoidable and in our studies this overhead has always been tolerable . a second improvement is to enhance @xmath58 such that rather than delivering true if @xmath70 we deliver true if @xmath66 and there exist a set @xmath71 such that @xmath72 .",
    "this will reduce redundant search .",
    "a further improvement is to remove elements of @xmath40 after they are expanded and put a test immediately after line 11 that makes a * return * if @xmath40 is empty .",
    "possibilities for implementation were constrained by the available resources . we intended to reuse an existing implementation of the algorithm , which was written in java .",
    "this was convenient , since the available machines all had a jvm installed . for communication",
    "we were limited to nfs ( with file - level locking only ) and ssh .",
    "the existing code was modified in line with the differences between mc and mcdist . rather than passing @xmath40 explicitly , for a graph with @xmath17",
    "vertices an integer @xmath73 between @xmath74 and @xmath75 was used to parameterise subproblems by splitting on the second level of the binomial search tree , as if @xmath41 was @xmath76 .",
    "a simple implementation is evident : we may number the top level nodes from @xmath77 down to @xmath74 , from right to left ( corresponding to the order in which they are popped from @xmath11 ) . then for a given @xmath73",
    ", every element of @xmath40 contains the element we numbered @xmath78 . on the second level of the tree",
    ", we again label elements of @xmath11 from right to left , from @xmath79 down to @xmath74 , and for a given @xmath73 , take those where the node s number divided by @xmath17 equals @xmath73 , modulo @xmath80 .    for each value of @xmath73 , a file named for that number was created in a `` pending '' directory .",
    "these files were split by the last two digits between 100 subdirectories , to reduce contention and to keep each directory sufficiently small to avoid nfs scalability problems ( initially no split was used , and lock contention prevented scalability beyond 10 machines ) .",
    "the restriction to @xmath75 jobs was necessary to avoid having too many files , but still have significantly more jobs than machines so that they can be distributed using a worker - pool model .",
    "in addition a global `` best so far '' file was used to hold the value @xmath42 for mcdist .",
    "this was set to contain @xmath74 initially .",
    "the worker programs were launched by ssh ( using public key / keychain logins to avoid having to repeatedly enter passwords ) , with one worker program per machine .",
    "each worker program reads in the graph , performs the initial colour ordering on the vertices , and then starts running subproblems .",
    "the worker randomly shuffles the 100 directories ( to reduce contention ) , and then for each directory in turn , while that directory is not empty , picks a job file from that directory and moves it to a `` running '' directory .",
    "the worker then reads in the `` best so far '' , runs the subproblem , saves the result to the job file and moves it to a `` results '' directory .",
    "the `` best so far '' is then updated , and another problem is attempted .",
    "note that the `` best so far '' is only read in before starting any individual subproblem .",
    "two sets of locking are required to avoid race conditions .",
    "firstly , a lock file is associated with each subproblem directory , to ensure that two machines can not both start running the same problem . here exclusive locking is used . secondly , the `` best so far '' file needs to be locked to avoid races when it is being updating , and to avoid the possibility of inconsistent reads .",
    "initially , a shared lock was used when reading in the value before launching a problem , and an exclusive lock was used when checking and updating the file afterwards .",
    "it was observed that this was a serious limiting factor on scalability ( on smaller problems , more than 50% of the total runtime was being wasted waiting for a lock ) .",
    "thus , a more sophisticated mechanism for updating the file was introduced .",
    "the value in the `` best so far '' file may only increase over time and its largest possible value is typically much smaller than the number of subproblems .",
    "this means most of the exclusive locks we were obtaining were in fact not being used to change the value .",
    "this suggests a better strategy : when a worker finishes a problem , it obtains a shared lock on the `` best so far '' file , and compares the existing value to the value it calculated",
    ". the lock is then released .",
    "if the newly calculated value is better than the existing value , then an exclusive lock is obtained ( avoiding the possibility of deadlock , since the shared lock is already released ) .",
    "the value is then re - compared ( in case it has been updated in between releasing the shared lock and obtaining the exclusive lock ) and written if necessary .",
    "we observed that in practice , this mechanism substantially reduced overhead .    after execution , obtaining the results is a simple matter of checking every file in the `` results '' directory . ( the `` best so far '' file only contains the size of the best clique , not its members . ) the existing implementation of the algorithm produced data on the number of nodes ( i.e. calls to @xmath57 ) and the time spent working as well as the clique found as part of its results .",
    "our study was performed over hard dimacs instances , instances from the bhoslib suite ( benchmarks with hidden optimum solutionskexu / benchmarks / graph - benchmarks.htm ] ) and erds - rnyi random graphs .",
    "these instances were selected because they took between minutes and weeks on a single machine and were hard enough to cover the start up costs of distribution .",
    "approximately 100 student lab pcs were used , running fedora 13 with an amd athlon 64 x2 5200 + , 4gbytes ram and access to an nfs server .",
    "machines were largely idle , but there was no guarantee regarding availability ( several machines were switched on and off or became unavailable whilst experiments were being run ) . in all cases",
    "a problem was split into @xmath81 jobs and these were then distributed across the machines , e.g. frb35 - 17 - 1 has 450 vertices , 3,600 jobs were produced and these were dispatched over 25 machines , then 50 machines and finally 100 machines .",
    "run time was measured in seconds and is the difference between the wall clock time at the start of the first job and the wall clock time at the end of the last job .",
    "clocks on the machines were loosely synchronised , sometimes differing by a couple of seconds .",
    "the single machine runtimes are for the sequential ( undistributed ) algorithm and exclude the read - in times for the problem instance as in @xcite , whereas the distributed results include program startup and read - in times .      table [ bigtable ] shows the run time in seconds to find and prove optimality using 25 , 50 and 100 machines compared to the undistributed time .",
    "we list the number of vertices in the graph ( @xmath17 ) , the size of the maximum clique ( @xmath7 ) and when more than one machine was used the speed up ( gain ) .",
    "the instances mann - a45 , brock400 and p - hat500 - 3 are from dimacs and frb * from bhoslib .",
    ".large hard instance : run time in seconds , using 1 to 100 machines .",
    "an entry of  corresponds to job that did not terminate after one week . [ cols=\"^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]      table [ randomtable ] gives results on 30 random graphs @xmath82 all with @xmath83 vertices .",
    "the first 10 graphs have edge probability of @xmath84 , @xmath85 for the next 10 graphs and @xmath86 for the last10 .",
    "the @xmath87 instances are easy on a single processor , taking less than a second .",
    "therefore the run time on many machines is largely overhead of dispatching @xmath88 jobs and collecting their results .",
    "this demonstrates that our approach is only applicable to large hard instances where that overhead can be amortised .",
    "the @xmath89 instances are relatively hard ( about 20 minutes on a single machine ) and the overhead pays off modestly with a speedup between 1.8 ( 1000 - 50 - 09 with 50 machines ) and 5.6 ( 1000 - 50 - 00 with 100 machines ) .",
    "why so modest ?",
    "analysing instance 1000 - 50 - 00 shows that none of the jobs took more than two seconds runtime yet incurred the same overhead as the easier @xmath87 instances .",
    "so again , the overhead remains significant for these instances .",
    "for @xmath86 only 50 machines were available to us , nevertheless we see speedups from a minimum of 19.8 ( 25 machines ) up to 42.3 ( 50 machines ) .      on several occasions during execution of some of the larger problems , a machine was either powered off or rebooted .",
    "here the nfs implementation was an advantage : the lost task could easily be identified and restarted .",
    "the `` results '' directory also provides an easy check that every task was in fact executed .",
    "the system is in no way tolerant of failures of the nfs server , but experience suggests that the nfs server is far more reliable than the lab machines .",
    "on reflection , there are many things we could do to improve performance if we were given more time .",
    "[ [ order - of - task - execution ] ] order of task execution : + + + + + + + + + + + + + + + + + + + + + + + +    we use a random order of task execution as a simple way of reducing contention .",
    "this is unlikely to be optimal , as the initial ordering of vertices has a large effect upon performance .",
    "this also significantly affects reproducibility of results .",
    "although reproducibility could be improved by using fixed per - machine orderings , a more sophisticated implementation that did not have to rely upon nfs would be able to dispatch jobs in the order in which they would be executed in the non - parallel version of the algorithm .",
    "[ [ slow - startup ] ] slow startup : + + + + + + + + + + + + +    with the current implementation it can take up to a minute for all 100 machines to start running .",
    "this is because ssh login attempts are deliberately rate limited .",
    "we could avoid this cost with a more sophisticated startup mechanism .",
    "[ [ re - reading - the - best - so - far ] ] re - reading the best so far : + + + + + + + + + + + + + + + + + + + + + + + + + + +    we only read and write the file containing the best clique found so far at the start and end of a task respectively .",
    "it would be better to do this more frequently .",
    "however , due to locking , this has considerable overhead on nfs .",
    "this also affects reproducibility of results : in some cases , a small delay before starting a job would lead to it having a better `` best so far '' value , which in turn would vastly reduce runtimes .",
    "however , some of the long tails in other problems would not be removed by this method . in some cases , a better initial `` best so far ''",
    "is of no help for the most time - consuming subproblems .",
    "[ [ finer - granularity - vs - work - stealing ] ] finer granularity vs work - stealing : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to reduce the long tail in cases where re - reading `` best so far '' does not help , we could split the second level fully into ( less than ) @xmath90 jobs , or split on the first three levels .",
    "this was not possible with the nfs server available to us , but would be an option for better implementations .",
    "an increase in splitting is still not enough to remove every long tail , however . in some cases there",
    "are a small number of areas deep down in the tree that contribute most to the runtime . dealing with these",
    "would need some kind of work stealing mechanism , which in turn requires much more sophisticated communication than was available .",
    "[ [ threading - the - workers ] ] threading the workers : + + + + + + + + + + + + + + + + + + + + + +    each of the lab machines we had available was dual core .",
    "memory limitations prevented us from running two instances of the worker program per machine ; threading the client ( and adding a second set of in - process locking , since file locks are per - program rather than per - thread ) could possibly give us the equivalent of doubling the number of machines , at the expense of a more complicated implementation .",
    "[ [ not - using - java ] ] not using java : + + + + + + + + + + + + + + +    java was used due to an existing implementation being available , and because of the ease of running java programs on non - identical systems . a reimplementation in a faster language and",
    "the use of bit encoded sets ( such as in @xcite ) would produce a substantial speed - up , at the cost of increased development time and complexity of implementation .",
    "so , how did we do ?",
    "did we get a _ costup _ ?",
    "did we get an increase in performance greater than the increase in cost ?",
    "we think so .",
    "just looking at the frb35 instances , each instance typically takes weeks to solve on a single machine .",
    "we solve frb35 - 17 - 5 in 11 hours , frb35 - 17 - 2 in under 7 hours ( and over 13 days on a single machine ) , frb35 - 17 - 1 in about 9 hours ( and more than a week on a single machine ) , frb35 - 17 - 4 in under 5 hours and frb35 - 17 - 3 in under 4 hours .",
    "we have spent less than a week to do this , less than a week to get more than a week s speedup .",
    "many of our difficulties were down to performance limitations with nfs , and these would be vastly reduced if we were using a shared memory multi - core or even just a numa multi - processor system .",
    "in other words , we believe that things are going to get better for this kind of costup in the future , not worse ."
  ],
  "abstract_text": [
    "<S> we take an existing implementation of an algorithm for the maximum clique problem and modify it so that we can distribute it over an ad - hoc cluster of machines . </S>",
    "<S> our goal was to achieve a significant speedup in performance with minimal development effort , i.e. a _ </S>",
    "<S> maximum costup_. we present a simple modification to a state - of - the - art exact algorithm for maximum clique that allows us to distribute it across many machines . </S>",
    "<S> an empirical study over large hard benchmarks shows that speedups of an order of magnitude are routine for 25 or more machines . </S>"
  ]
}