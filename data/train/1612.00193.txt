{
  "article_text": [
    "molecular dynamics ( md ) simulation has grown essential to the study of atomic and molecular physics in areas such as materials science  @xcite , biomedical engineering  @xcite and chemistry  @xcite .",
    "the validity of md depends crucially on the accuracy of the potential energy model , from which forces are derived . for numerical expedience ,",
    "most md simulations use simple classical potentials .",
    "these potentials are typically designed by fitting empirical parameters to a relatively limited quantity of experimental or theoretical data , and often fail to generalize to new situations  @xcite .    over the past decade",
    ", there has been increasing interest  @xcite in using machine learning methods to automatically generate md potentials based upon large quantities of _ ab initio _ calculation data .",
    "steady improvement in the accuracy , transferability , and computational efficiency of the machine learned models has been achieved .",
    "a crucial enabler , and ongoing challenge , is incorporating prior physical knowledge into the methodology .",
    "in particular , we know that md potentials should be invariant with respect to translations and rotations of atomic positions , and also with respect to permutations of identical atoms . designing machine learning methods that naturally encode these invariances can be a challenge .",
    "a common modeling approach is to decompose the energy of an atomic configuration as a sum of local contributions , and to represent each local atomic environment in terms of descriptors ( _ i.e. _ features ) that are inherently rotation and permutation invariant  @xcite .",
    "such descriptors may be employed in , e.g. , linear regression  @xcite or neural network models  @xcite .",
    "recently , sophisticated descriptors have been proposed based on an expansion of invariant polynomials  @xcite and , inspired by convolutional neural networks , on a cascade of multiscale wavelet transformations  @xcite .",
    "kernel learning is an alternative approach that can avoid direct use of descriptors  @xcite .",
    "instead , one specifies a kernel that measures similarity between two inputs and , ideally , directly satisfies known invariances .",
    "an advantage of kernel methods is that the accuracy can systematically improve by adding new configurations to the dataset .",
    "a disadvantage is that the computational cost to build a kernel regression model typically scales cubically with dataset size .",
    "kernel methods that model potential energies include gaussian approximation potentials ( gap )  @xcite , smooth overlap of atomic potentials ( soap )  @xcite , coulomb matrix methods  @xcite , and others  @xcite",
    ". however , there remain challenges .",
    "for example , to achieve rotational invariance in gap and soap , one requires a nontrivial integration over rotations . to achieve permutation invariance in coulomb methods ,",
    "one typically restricts attention to the matrix ordered eigenvalues , thus sacrificing some descriptive power and regularity .    here , we present the graph approximated energy ( grape ) framework .",
    "grape is based on two central ideas : we represent local atomic environments as weighted graphs , for which rotation invariance is inherent , and we compare local atomic environments by leveraging well known graph kernel methods invariant with respect to node permutation .",
    "there is much flexibility in implementing these ideas . in this paper , we define the weighted adjacency matrix elements ( _ i.e. _ the edge weights ) through an analogy with soap  @xcite , and we use generalized random walk graph kernels  @xcite to define similarity between graphs .",
    "graph kernels have commonly been employed to compare molecules , e.g. , for drug discovery applications  @xcite , but it appears that prior applications to energy regression are limited  @xcite . here , we demonstrate that grape is competitive with state of the art kernel methods for energy regression , as benchmarked on a standard dataset of organic molecules .",
    "the rest of the paper is organized as follows .",
    "section  [ sec : review ] reviews necessary background techniques : kernel ridge regression of energies , the soap kernel , and random walk graph kernels .",
    "we merge these tools to obtain our grape kernel , presented in sec .",
    "[ sec : grape ] .",
    "finally , we demonstrate the performance of grape on a standard molecular database in sec .",
    "[ sec : application ] and summarize our results in sec .",
    "[ sec : discussion ] .",
    "here we review the machine learning technique of kernel ridge regression and its application to modeling potential energy landscapes .",
    "we assume a dataset @xmath0 , where @xmath1 represents an atomic configuration and @xmath2 its corresponding energy .",
    "the goal is to build a statistical approximation @xmath3 that accurately estimates the energy of new configurations .",
    "a common approach begins with _ descriptors _ ( also called features or order parameters ) @xmath4 , designed to capture various aspects of a configuration @xmath5 .",
    "often @xmath6 describes the geometry of a local atomic environment .",
    "many descriptors suitable for modeling potential energy have been developed  @xcite .",
    "the descriptors can be used , for example , with simple linear regression  @xcite , @xmath7 or as inputs to a more complicated model such as a neural network  @xcite .",
    "kernel methods are an alternative machine learning approach , and avoid direct use of descriptors . instead , the starting point is a positive symmetric , semi - definite kernel @xmath8 , such that @xmath9 measures the similarity between configurations @xmath5 and @xmath10 .",
    "we use kernel ridge regression , for which the approximated energy reads @xmath11 where @xmath12 is the identity matrix , @xmath13 denotes a matrix with elements @xmath14 , and @xmath15 are the energies in the dataset .",
    "the parameter @xmath16 serves to regularize the model , and prevents overfitting by penalizing high frequencies  @xcite .",
    "equations   can be readily derived from the linear model  , using least squares error minimization with a ridge penalty ( see appendix  [ sec : kernelapprox ] for details ) .",
    "the kernel becomes @xmath17 , and thus satisfies the necessary criteria of symmetry and positive semi - definiteness .",
    "conversely , any function @xmath8 that is symmetric and positive semi - definite can be decomposed as above , where @xmath18 may be infinite  @xcite . thus , @xmath19 and @xmath8 are formally interchangeable .",
    "an advantage of specifying the kernel directly is that the corresponding , potentially infinite set of descriptors may remain fully implicit .    in the context of modeling potential energies and forces , physical locality is often a good approximation  @xcite .",
    "we assume that the energy of an atomic configuration @xmath20 with @xmath21 atoms can be decomposed as a sum of local contributions , @xmath22 where @xmath23 denotes a local view of environment @xmath20 centered on atom @xmath24 .",
    "the index @xmath24 iterates through all @xmath21 atoms of the atomic configuration @xmath20 , and we denote by @xmath25 the total number of local environments in the database .",
    "if long - range interactions exist , _ e.g. _ induced by electrostatics , they should be treated separately from  .",
    "the key idea is that statistical regression should begin with the local energy @xmath26 , which we formally decompose as in  .",
    "then , after a series of straightforward steps ( appendix  [ sec : local ] ) , we find  @xcite @xmath27 and the localized kernel regression finally reads for a configuration @xmath5 : @xmath28 where the entries of the matrix @xmath29 are @xmath30 ( _ i.e. _ the similarity between local environments @xmath24 and @xmath31 in configurations @xmath32 and @xmath33 ) , and @xmath34 is such that @xmath35 if the local environment @xmath24 belongs to configuration @xmath32 and @xmath36 otherwise .",
    "the important conclusion is that the kernel @xmath37 for _ local _ configurations confers a kernel @xmath8 for total configurations .",
    "this is a mathematical consequence of the energy decomposition , which encodes locality into our statistical model of global energies .",
    "note that the energies @xmath26 of the local atomic configurations @xmath38 need not be explicitly learned .",
    "the relatively small size of @xmath38 improves computational efficiency , and generally enhances transferability of the model .    to summarize , eqs .   and",
    "provide the recipe for kernel ridge regression of energy .",
    "the rest of the paper discusses approaches to designing the local kernel @xmath37 .",
    "here we review the powerful soap method  @xcite for regression of potential energy landscapes .",
    "the structure of the soap kernel will inspire our design of grape , which we present in sec .",
    "[ sec : grape ] .",
    "we begin by representing the local configuration @xmath38 with @xmath39 neighbors as a smooth atomic density centered on atom @xmath24 , @xmath40 here @xmath41 , with @xmath42 the position of the @xmath43 atom in @xmath38 and @xmath44 its atomic number with associated weight @xmath45 .",
    "there is flexibility is choosing the coefficient @xmath45 , provided that it is an injective function of the atomic number @xmath46 for atom @xmath32 .",
    "we also define a gaussian function to smooth the atomic densities , @xmath47 and a cut - off function , @xmath48 & \\quad r \\leq { r_{\\mathrm{cut}}},\\\\ 0 & \\quad r \\geq { r_{\\mathrm{cut}}}. \\end{array } \\right.\\ ] ] the parameter @xmath49 sets the smoothing length scale , and @xmath50 the cutoff distance .",
    "atom @xmath32 contributes to @xmath51 only if sufficiently close to atom @xmath24 , namely if @xmath52 .",
    "we define @xmath38 to include only those atoms in @xmath5 that satisfy this condition , yielding a substantial numerical speedup for large atomic configurations . in the following , for notational convenience , we suppress @xmath24 indices without loss of generality .",
    "namely , we consider two local atomic environments @xmath53 and @xmath54 centered on atomic positions @xmath55 and @xmath56 .    the next step in building the soap kernel is to define a scalar product between local atomic densities , @xmath57 the density fields @xmath58 and @xmath59 are naturally invariant to permutations of atomic indices , and @xmath60 inherits this symmetry .",
    "translation invariance is a consequence of comparing local environments .",
    "however , @xmath60 is not invariant with respect to rotations @xmath61 . to include this symmetry",
    ", one may integrate over rotations , @xmath62 where @xmath63 is the haar measure and @xmath64 generates the configuration with density @xmath65 .",
    "numerical evaluation of   is non - obvious , and the soap approach involves expanding @xmath66 in terms of spherical harmonics and applying orthogonality of the wigner matrices .",
    "@xcite in principle one could select integer @xmath67 as an arbitrary parameter , but in practice one typically fixes @xmath68 to simplify the expansion of spherical harmonics .    the final local soap kernel is defined by rescaling , @xmath69^{\\zeta},\\ ] ] for @xmath70 , which is possible because the unscaled kernel is strictly positive for nonempty environments .",
    "after this rescaling , we have @xmath71 and @xmath72 .",
    "one commonly selects @xmath73 to effectively amplify the kernel in regions where @xmath74 is largest .    by eq",
    ".  , the local kernel @xmath75 produces a total kernel @xmath8 that satisfies permutation , translation , and rotation invariance .",
    "the hyperparameters are @xmath76 , @xmath77 , @xmath78 , and @xmath79 .      here",
    "we present random walk graph kernels  @xcite .",
    "in particular , the exponential graph kernel reviewed below will be the basis of grape , our method for kernel regression of potential energies ( _ cf .",
    "[ sec : regression ] ) .",
    "our use of graphs to represent local atomic environments is inspired by applications in chemical informatics  @xcite .    a graph @xmath80 is defined as a set of vertices @xmath81 , and edges @xmath82 that connect pairs of vertices . an unweighted graph is equivalently represented by its adjacency matrix @xmath83 , where @xmath84 if vertices @xmath32 and @xmath85 are connected by an edge [ _ i.e. _ if @xmath86 and @xmath87 otherwise .",
    "we work with weighted graphs , such that each edge @xmath88 is assigned a nonzero weight @xmath89 .",
    "we represent such graphs via the weighted adjacency matrix , @xmath90 we consider undirected and unlabeled graphs , which means that @xmath91 and the nodes and edges have no additional structure .",
    "graph kernels measure similarity between two graphs .",
    "many graph kernels exist , including laplacian kernels  @xcite , shortest path kernels  @xcite , skew spectrum kernels  @xcite , graphlet kernels  @xcite , and functional graph kernels  @xcite .    here",
    "we focus on random walk graph kernels because they are simple , computationally efficient  @xcite , and suitable for learning potential energy landscapes , as will be made clear later .",
    "the motivating idea is to consider a random walk over graph vertices ( _ i.e. _ states ) . for this purpose",
    ", we assume for now that @xmath92 represents the transition matrix of a markov process , such that @xmath93 is the probability for a random walker to transitition from state @xmath85 to state @xmath32 , and the columns of @xmath92 conserve probability , @xmath94 . later we will relax these probabilistic constraints and allow arbitrary @xmath95 .",
    "we select the initial state according to a distribution @xmath96 , _ i.e. _ the walker starts at state @xmath32 with probability @xmath97 .",
    "then @xmath98 is the probability that the walker reaches state @xmath32 after @xmath99 steps in the markov chain . for a given stopping distribution @xmath100",
    ", @xmath101 represents the probability that a random walker stops after @xmath99 steps .",
    "our goal is to compare two graphs represented by adjacency matrices @xmath102 and @xmath103 .",
    "for this , we consider simultaneous random walks , one on each graph . the joint probability that the first walker transitions from @xmath104 and the second walker transitions from @xmath105 is the product of individual transition probabilities .",
    "these joint probabilities appear as the elements of @xmath106 , the direct ( kronecker ) product of adjacency matrices , defined by @xmath107 note that that @xmath108 can itself be interpreted as an adjacency matrix for the so - called direct product graph  @xcite , see fig .",
    "[ fig : productgraph ] .    given starting @xmath78 , @xmath109 and stopping @xmath110 , @xmath111 distributions , the probability that both walkers simultaneously stop after @xmath99 steps   is @xmath112 where @xmath113 , @xmath114 , and we have used the matrix product identity @xmath115 .",
    "two graphs can be compared by forming a weighted sum over paths of all lengths of the markov chain .",
    "the random walk graph kernel finally reads : @xmath116 the choice of @xmath117 is left to the user as long as the series converges , and in this case   is known to define a positive semi - definite kernel  @xcite .",
    "one must also select the starting and stopping distributions .",
    "choosing @xmath78 , @xmath109 , @xmath110 , @xmath111 to be uniform guarantees invariance with respect to permutation of vertices . to see this , consider permutation of the nodes , represented by permutation matrices @xmath118 and @xmath119 , which transform the adjacency matrices as @xmath120 and @xmath121 .",
    "the product matrix transforms as @xmath122 , where @xmath123 .",
    "consequently , the kernel   becomes @xmath124 , where @xmath125 and @xmath126 .",
    "if the starting and stopping distributions are uniform , we observe that @xmath127 and @xmath128 , and the kernel exhibits permutation invariance .",
    "note , however , that with uniform starting and stopping distributions , it is crucial that the columns of adjacency matrices @xmath92 and",
    "@xmath129 _ not _ be normalized .",
    "if they were , then we would have @xmath130 independently of @xmath92 , making it impossible to compare graphs . consequently , although we continue to use  , we abandon its probabilistic interpretation .    in our work",
    ", we select @xmath131 in   to get the exponential graph kernel  @xcite , @xmath132 where @xmath133 is a parameter controlling how fast the powers of @xmath108 go to zero .",
    "it can be shown to reweight the eigenvalues in the comparison process  @xcite .",
    "the direct product matrix @xmath108 contains @xmath134 matrix elements , a potentially large number , but the numerical cost of evaluating the kernel can be reduced by diagonalizing the factor matrices .",
    "indeed , given @xmath135 and @xmath136 , with @xmath137 and @xmath138 diagonal , we write @xmath139 so that   becomes @xmath140 because @xmath141 is itself a diagonal matrix , we can evaluate @xmath142 by applying the exponential to each of the @xmath143 diagonal elements .",
    "the computational cost to calculate @xmath144 is dominated by matrix diagonalization , which scales like @xmath145 assuming @xmath146 .",
    "if the eigen decompositions of @xmath92 and @xmath129 have been precomputed , the scaling reduces to @xmath147 .",
    "the main contribution of our work is grape , a random walk graph kernel tailored to local energy regression .",
    "since random walk graph kernels act on the direct product of adjacency matrices , our primary task is to select a suitable adjacency matrix associated with a local atomic environment @xmath148 . to motivate our choice of adjacency matrix , we first present a graphical interpretation of the soap kernel .",
    "the soap kernel is constructed from the inner product , eq .  , between local atomic densities  .",
    "combining these equations , we obtain a double sum over atoms from different local environments , @xmath149 where @xmath150 is again gaussian , @xmath151 here , for notational convenience , the positions of the atoms @xmath152 and @xmath153 are given relative to the centers @xmath154 and @xmath155 of the local environments .",
    "seeking a graph interpretation of  , we rewrite it in matrix form , @xmath156 the object @xmath157 is suggestive of an adjacency matrix .",
    "however , its indices @xmath32 and @xmath33 reference atoms from different environments @xmath148 and @xmath158 . indeed , the graphical interpretation of @xmath159 would be bipartite ( _ i.e. _ , containing only cross - links between the atoms of @xmath148 and @xmath158 ) as illustrated in fig .  [",
    "fig : crossgraph ] . note that the elements @xmath160 vary with the relative rotation between @xmath148 and @xmath158 , and indeed soap requires nontrivial integration   to achieve rotational invariance .    in grape",
    ", we achieve rotational invariance by representing local configurations as graphs .",
    "guided by the form of  , we introduce an analogous adjacency matrix @xmath161 that operates on a _ single _ local environment @xmath148 , @xmath162 for example , fig .",
    "[ fig : methanegraph ] illustrates the graph representation of a methane molecule .",
    "the elements @xmath163 of the adjacency matrix   link atoms @xmath32 and @xmath85 , both contained in @xmath148 . because these elements depend only on pairwise distances , they are manifestly rotation invariant .    to compare two local atomic environments , we apply the exponential graph kernel of sec .",
    "[ sec : graphtheory ] : @xmath164 figure  [ fig : productgraph ] illustrates the graph represented by the direct product matrix , @xmath165 , and should be contrasted with fig .  [",
    "fig : crossgraph ] .",
    "we again use uniform starting and stopping probability distributions @xmath78",
    ", @xmath109 , @xmath110 , @xmath111 to ensure invariance with respect to permutation of indices , while rotational invariance is naturally inherited from the adjacency matrices @xmath92 and @xmath129 .",
    "finally , we use   to rescale  , and   to build our model which is suitable for regression on total energies .",
    "the kernel is regular and its derivatives are calculable in closed form ( appendix  [ sec : forces ] ) , so atomic forces are also available from the regression model .",
    "grape shares most of its hyperparameters with soap , namely : @xmath76 , @xmath166 , and @xmath79 . however , the soap hyperparameter @xmath78 is replaced by grape s @xmath133 .",
    "we must also specify the regularization parameter @xmath167 in kernel regression  .",
    "the computational cost to directly evaluate the local grape kernel @xmath168 scales as @xmath169 ( see sec .",
    "[ sec : graphtheory ] ) , where @xmath170 is the typical number of atoms in local environments @xmath148 and @xmath158 .",
    "global energy regression   requires many local kernel evaluations , for both soap and grape methods .",
    "evaluating the total kernel   requires double summation over all @xmath146 atoms in total configurations @xmath5 and @xmath10 , and thus scales as @xmath171 .",
    "if there are @xmath172 atomic configurations @xmath173 in the dataset , then evaluating the full kernel matrix @xmath8 scales as @xmath174 .",
    "matrix inversion brings the total scaling to @xmath175 .",
    "an improvement is to precompute the diagonalization of every local adjacency matrix at cost @xmath176 .",
    "then subsequent local kernel evaluations scale like @xmath177 and the total cost of building the grape energy regression model becomes @xmath178 .",
    "once the model is built , computing the approximation energy   for a new configuration scales as @xmath179 .",
    "we demonstrate the competitiveness of grape by benchmarking it on a standard energy regression problem .",
    "we use the qm7 dataset of organic models used in ref .   and freely available at",
    "http://quantum - machine.org / datasets/. this database contains @xmath180 molecules randomly selected from the gdb-13 database with associated atomization energies , typically between @xmath181 and  @xmath182 kcal / mol , obtained from hybrid dft calculations  @xcite .",
    "gdb-13 contains approximately @xmath183 organic molecules up to @xmath184 atoms in size , and formed from elements h , c , n , o , and s. qm7 has been randomly partitioned into @xmath185 sequences , each containing 1433 molecules .",
    "we use the first @xmath172 molecules of partition 1 as training data ( with @xmath186 ) , and partitions 2 and 3 as validation data , _",
    "i.e. _ to select the hyperparameters necessary for the various regression methods ( discussed below ) . finally , after having locked the hyperparameters , we use partition 4 as our test data , from which we estimate the mean average error ( mae ) and root mean square error ( rmse ) of the energy regression models .",
    "we compare the performance of grape against that of the coulomb matrix method  @xcite ( appendix  [ sec : coulomb ] ) and soap  @xcite . after experimenting on the validation data , we selected the following hyperparameters for these methods .",
    "we set the gaussian width parameter in   to be @xmath187 for both soap and grape .",
    "this provides a small overlap of the gaussian densities of bonded atoms , which are typically separated by a few @xmath188 .",
    "we select the cutoff radius in   to be @xmath189 for both soap and grape , such that a typical local atomic neighborhood contains around 5 atoms .",
    "interestingly , we find that grape is especially sensitive to this hyperparameter , and that the energy regression error would nearly double if we instead selected @xmath190 for grape",
    ". we must also select the weight @xmath45 in   and   as a function of atomic number @xmath46 . again , after some experimentation , we select @xmath191 for soap and @xmath192 for grape .",
    "the latter implies that the grape hyperparameter @xmath133 appearing in   should scale like the square of the typical atomic number in the data ; we select @xmath193 .",
    "consistent with previous work  @xcite , we select @xmath68 for soap   and @xmath194 in   for both soap and grape .",
    "we select @xmath195 for the coulomb hyperparameter appearing in eq .  .",
    "the last hyperparameter @xmath167 , appearing in  , regularizes the kernel regression .",
    "we select @xmath196 for coulomb , @xmath197 for soap , and @xmath198 for grape . in selecting the above hyperparameters",
    ", we put approximately equal weight on the mae and rmse .",
    "we caution that all three models ( coulomb , soap , grape ) would likely benefit from a more exhaustive search over the hyperparameters .",
    "our results should thus be interpreted as a _ qualitative _ comparison of grape s performance relative to coulomb and soap .",
    "[ 1 ] > + m#1    c1.4cm|c1.2cm||c1.3cmc1.3cmc1.3cmc1.3 cm & @xmath172 & 100 & 300 & 500 & 1000 + & mae & 25.6 & 19.8 & 17.9 & 17.7 + & rmse & 50.8 & 33.5 & 27.1 & 28.5 + & mae & 15.6 & 11.3 & 10.4 & 9.7 + & rmse & 21.0 & 15.6 & 14.5 & 13.3 + & mae & 11.2 & 10.1 & 9.6 & 9.0 + & rmse & 14.9 & 13.9 & 13.3 & 12.7 +    table  [ tab : errors_regression ] displays the mae and rmse error estimates for all three methods and various sizes @xmath172 of the training dataset .",
    "figure  [ fig : energy ] shows regression energies for the test data with @xmath199 , and we observe that grape is competitive with soap ; given our limited tuning of the hyperparameters , we can not conclude that one method outperforms the other .",
    "however , both soap and grape significantly and consistently outperform the coulomb method .",
    "we note that our mae and rmse estimates for the coulomb method are consistent with those previously reported  @xcite , when holding @xmath172 fixed . as a final point of comparison , the authors of ref .   reported an mae of @xmath200 kcal / mol for @xmath199 using a neural network based on the random coulomb matrix representation .",
    "we introduced grape , a method for energy regression based on random walk graph kernels .",
    "the approach is invariant with respect to translations , rotations , and permutations of same - species atoms .",
    "moreover , it is flexible , straightforward to implement , regular with respect to atomic coordinates , and admits simple closed form derivatives .",
    "these properties make it a suitable candidate for fitting atomic forces . using a standard benchmark dataset of organic molecules",
    ", we demonstrated that grape is competitive with state of the art kernel methods .    like the coulomb method  @xcite ,",
    "grape essentially consists of comparing matrices .",
    "these matrices , built upon interatomic distances , are inherently invariant with respect to translation and rotation , while carrying complete information of the system .",
    "however , the matrix representation lacks permutation invariance . to restore permutation invariance ,",
    "the coulomb method restricts its attention to the list of ordered eigenvalues .",
    "unfortunately , due to this sorting , the associated kernel becomes non - differentiable , which can be a source of instability , as pointed out in ref .  .",
    "unlike coulomb , grape is completely regular .",
    "another advantage of grape is that its parameter @xmath133 enables effective reweighting of the eigenvalues , each of which corresponds to a different length scale  @xcite .",
    "an alternative machine learning approach is to begin by representing the atomic configuration as a density , _",
    "e.g. _ using gaussians peaked on each atomic position .",
    "this density field provides a natural way to compare two environments  @xcite , and this is the approach taken by the soap method  @xcite .",
    "a disadvantage of this approach is that the density is not inherently rotational invariant , and density - based kernels typically require an explicit integration over rotations .",
    "an achievement of the soap kernel is that its rotational integral , eq .",
    ", can be evaluated in closed form via a spherical harmonic expansion of the gaussian densities .",
    "grape obviates the need for such an integral in the first place",
    ".    an exciting aspect of the grape approach is its flexibility for future extensions .",
    "for example , one can use multiscale analysis techniques , such as the multiscale laplacian graph kernel  @xcite , or wavelet and scattering transforms over graphs  @xcite .",
    "another promising research direction is to encode more physical knowledge into the graph , _",
    "e.g. _ using labeled graphs  @xcite .",
    "the vertex labels could directly represent the atomic species , or could more subtly encode chemical information such as the number of electrons in the valence shell .",
    "the authors thank gbor csnyi , danny perez , arthur voter , sami siraj - dine and gabriel stoltz for fruitful discussions .",
    "work performed at lanl was supported by the laboratory directed research and development ( ldrd ) and the advanced simulation and computing ( asc ) programs .",
    "here we review the machine learning technique of kernel ridge regression .",
    "we assume a dataset of pairs @xmath0 . in our application ,",
    "each @xmath1 represents an atomic configuration and @xmath201 its corresponding energy .",
    "the goal is to build a model of energies @xmath202 for configurations @xmath5 not contained in the dataset . following ref .",
    "we begin with ordinary ridge regression .",
    "we model the energy as a linear combination of descriptors @xmath4 , @xmath203 where @xmath18 is possibly infinite .",
    "note that @xmath204 is linear in the regression coefficients @xmath205 but potentially very nonlinear in @xmath5 .",
    "we select regression coefficients @xmath206 that minimize a loss function @xmath207 the simplest error measure , @xmath208 , yields a linear system of equations to be solved for @xmath209 .",
    "the solution is the well known ridge regression model  @xcite : @xmath210 with matrix elements @xmath211 and vector @xmath212 .",
    "the regularization parameter @xmath213 effectively smooths the approximation @xmath3 by penalizing high frequencies , and consequently improves the conditioning of the linear inversion problem .",
    "it turns out that we can generalize this model by introducing the inner product kernel , @xmath214 which measures similarity between inputs @xmath5 and @xmath10 .",
    "note that @xmath215 where @xmath216 .",
    "the matrix identity @xmath217 allows us to rewrite  ( [ eq : rr_1_app ] ) and  ( [ eq : rr_2_app ] ) in the suggestive form : @xmath218 the key insight is that the descriptors @xmath219 no longer appear explicitly .",
    "one may select the kernel @xmath8 directly , and thus implicitly define the family @xmath19 .",
    "equations   and   together with a suitable kernel constitute the method of kernel ridge regression",
    ". the kernel must be symmetric and positive , _",
    "@xmath220 for all @xmath5 , @xmath10 , and @xmath221 for any non - empty collection of points and coefficients @xmath222 .",
    "by mercer s theorem  @xcite , these conditions are equivalent to the decomposition  .",
    "there are multiple ways to interpret eqs .   and  .",
    "in statistical learning theory , we may derive @xmath202 as the function in some space @xmath223 that best minimizes a least squared error measure @xmath224 subject to a regularization term @xmath225 .",
    "the kernel @xmath8 generates both @xmath223 ( the so - called reproducing kernel hilbert space ) and its norm @xmath226  @xcite .",
    "another interpretation is bayesian . in this case",
    ", one views the coefficients @xmath205 as independent random variables with gaussian prior probabilities ; then the kernel @xmath8 specifies the covariance of @xmath227 and @xmath228 , and @xmath202 becomes the posterior expectation for configuration @xmath5 .",
    "this approach is called gaussian process regression or kriging  @xcite .",
    "we note that eq .   [ but not ] is independent of the specific choice of error measure @xmath229 .",
    "for example , the method of support vector regression corresponds to the choice @xmath230 .",
    "we assume that the energy of an atomic configuration @xmath5 with @xmath231 atoms can be decomposed as a sum over local atomic environments @xmath38 , @xmath232    following eq .",
    "( [ eq : f_decomp_app ] ) , we model local energies @xmath233 as a linear combination of abstract descriptors @xmath234 , @xmath235 after inserting   into  , we observe that the model for total energy @xmath236 involves the same coefficients @xmath205 but new descriptors @xmath237    we assume a dataset @xmath0 containing configurations @xmath1 and total energies @xmath238 again minimizing the cost function of eq .",
    "( [ eq : ridge_cost ] ) , the kernel ridge regression model of eqs .",
    "( [ eq : krr_1_app ] ) and  ( [ eq : krr_2_app ] ) appears unchanged .",
    "however , the kernel between the full atomic configurations is now constrained .",
    "equations  ( [ eq : kernel_app ] ) and  ( [ eq : global_h ] ) together imply , @xmath239 where @xmath240    we conclude that the energy decomposition  ( [ eq : locality_app ] ) constrains the kernel @xmath8 to a sum over terms @xmath37 involving _ local _ environments . in `` kernelizing ''",
    "this model , we only require specification of the local kernel @xmath37 ; the descriptors @xmath241 are implicit and typically infinite in number .",
    "we review the coulomb matrix method of ref .  .",
    "here , we elide localized kernel   and consider only global atomic configurations @xmath5 as in ref .  .",
    "given a configuration with positions and atomic numbers @xmath242 , the coulomb matrix is @xmath243 although the matrix is invariant with respect to translations and rotations of the atomic positions , it is not invariant with respect to permutations of indices .",
    "the matrix eigenvalues , however , are permutation invariant .",
    "to compare configurations @xmath5 and @xmath10 ( of size @xmath231 and @xmath244 , respectively ) , the coulomb method compares the lists of eigenvalues ( @xmath245 and @xmath246 , respectively ) , sorted by decreasing magnitude and truncated at length @xmath247 .",
    "we define normalized eigenvalues , @xmath248 where @xmath249",
    ". then the coulomb kernel reads @xmath250,\\ ] ] with @xmath251 a hyperparameter .",
    "given a regression model for energy , it is often desirable to compute its gradient to obtain a regression model for forces .",
    "one motivation is to use machine learned forces within molecular dynamics simulations .",
    "another motivation is that typical datasets ( _ e.g. _ as generated by _ ab initio _ calculations ) often contain force information that can aid the energy regression  @xcite . in either case ,",
    "the key step is to calculate the derivative @xmath252 of the kernel @xmath9 with respect to some atomic position @xmath253 in configuration @xmath5 . here , we consider the exponential graph kernel   used in grape . as before , @xmath92 and @xmath129 denote the adjacency matrices of @xmath5 and @xmath10 respectively .",
    "assuming @xmath254 , the gradient of the unscaled kernel is , @xmath255    referring to eq .",
    "we see that the grape adjacency matrix element @xmath163 has a simple functional dependence on positions @xmath42 and @xmath256 ( and also an implicit dependence on the central atom position @xmath257 ) , and it is thus straightforward to express @xmath258 in closed form . to compute the normalized kernel  ,",
    "one also needs : @xmath259    56ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ] ",
    "+ 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty @noop * * ,   ( ) @noop _ _  ( , ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) link:\\doibase 10.1103/physrevb.56.8542 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevb.59.3393 [ * * ,   ( ) ] @noop * * ,   ( ) @noop * * ,   ( ) link:\\doibase 10.1103/physrevlett.59.2666 [ * * ,   ( ) ] @noop * * ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop ( ) @noop ( ) @noop ( ) @noop ( ) @noop _ _ ( ,  , ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) in  @noop _ _ ,  vol .",
    "( )  pp .   in @noop _",
    "( ,  ) pp .   @noop * * , ( ) @noop * * ,   ( ) in  @noop _ _  ( ,  ) pp .   in  @noop",
    "_ _  ( , )  pp .   @noop * * ,   ( ) @noop * * ,   ( ) in @noop _ _  ( ,  )  pp .   @noop * * ,   ( ) @noop * * ,   ( ) _ _ ,  @noop , ( ) @noop * * ,   ( ) in  @noop _ _  ( ,  )  pp .   in @noop",
    "_ _ ,  vol .",
    "( )  pp .   in  @noop _ _  ( ,  )  pp .   @noop ( ) @noop _ _ ( ,  ) link:\\doibase 10.1103/physrev.136.b864 [ * * , ( ) ] link:\\doibase 10.1103/physrev.140.a1133 [ * * , ( ) ] in  http://papers.nips.cc/paper/4830-learning-invariant-representations-of-molecules-for-atomization-energy-prediction.pdf[__ ] ,  ( ,  )  pp .   @noop ( ) @noop * * ,   ( ) in @noop _ _  ( )  pp .",
    "`` , ''  ( , ,  )  chap .  , pp .  ,",
    "ed . @noop * * ,   ( ) _ _ , @noop ph.d .",
    "thesis ,   ( ) @noop _ _  ( , ,  )"
  ],
  "abstract_text": [
    "<S> recent machine learning methods make it possible to model potential energy of atomic configurations with chemical - level accuracy ( as calculated from ab - initio calculations ) and at speeds suitable for molecular dynamics simulation . </S>",
    "<S> best performance is achieved when the known physical constraints are encoded in the machine learning models . </S>",
    "<S> for example , the atomic energy is invariant under global translations and rotations ; it is also invariant to permutations of same - species atoms . </S>",
    "<S> although simple to state , these symmetries are complicated to encode into machine learning algorithms . in this paper , we present a machine learning approach based on graph theory that naturally incorporates translation , rotation , and permutation symmetries . </S>",
    "<S> specifically , we use a _ random walk graph kernel _ to measure the similarity of two adjacency matrices , each of which represents a local atomic environment . </S>",
    "<S> we show on a standard benchmark that our graph approximated energy ( grape ) method is competitive with state of the art kernel methods . </S>",
    "<S> furthermore , the grape framework is flexible and admits many possible extensions . </S>"
  ]
}