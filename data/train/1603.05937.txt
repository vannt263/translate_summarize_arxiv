{
  "article_text": [
    "now that machines have taken over alpha ] mining , the number of available alphas is growing exponentially . on the flip side ,",
    "these  modern \" alphas are ever fainter and more ephemeral . to mitigate this effect , among other things ,",
    "one combines a large number of alphas and trades the so - combined  mega - alpha \" . and",
    "this is nontrivial .",
    "it is important to pick the alpha weights optimally , i.e. , to optimize the return , sharpe ratio and/or other performance characteristics of this alpha portfolio .",
    "the commonly used techniques in optimizing alphas are conceptually similar to the mean - variance portfolio optimization @xcite or sharpe ratio maximization @xcite for stock portfolios . however , there are some evident differences .",
    "the most prosaic difference is that the number of alphas can be huge , in hundreds of thousands , millions or even billions .",
    "the available history ( lookback ) , however , naturally is much shorter .",
    "this has implications for determining the alpha weights .",
    "let us look at vanilla sharpe ratio maximization of the alpha portfolio with weights @xmath7 , @xmath8 , where @xmath0 is the number of alphas .",
    "the optimal weights are given by @xmath9 where @xmath10 are the expected returns for our alphas , @xmath11 is the inverse of the alpha return covariance matrix @xmath12 , and @xmath13 is the normalization coefficient such that @xmath14 if we compute @xmath12 as a sample covariance matrix based on a time series of realized returns ( see ( [ sample.cov.mat ] ) ) , it is badly singular as the number of observations is much smaller than @xmath0 .",
    "this also happens in the case of stock portfolios . in that case one either builds a proprietary risk model to replace @xmath12 or opts for a commercially available ( multifactor ) risk model . in the case of alphas the latter option is simply not there .",
    "so , what is one to do ?",
    "we can try to build a risk model for alphas following a rich experience with risk models for stocks . in the case of stocks",
    "a more popular approach is to combine style risk factors ( i.e. , those based on measured or estimated properties of stocks , such as size , volatility , value , etc . ) and industry risk factors ( i.e. , those based on stocks membership in sectors , industries , sub - industries , etc .",
    ", depending on the nomenclature used by a particular industry classification employed ) .",
    "the number of style factors is limited , of order 10 for longer - horizon models , and about 4 for shorter - horizon models . in the case of stocks",
    ", at least for shorter - horizon models , it is the ubiquitous industry risk factors ( numbering in a few hundred for a typical liquid trading universe ) that add most value .",
    "however , there is no analog of the ( binary or quasi - binary ) industry classification for alphas . in practice , for many alphas it is not even known how they are constructed , only the ( historical and desired ) positions are known .",
    "even formulaic alphas @xcite are mostly so convoluted that realistically it is impossible to classify them in any meaningful way , at least not such that the number of the resulting ( binary or quasi - binary )  clusters \" would be numerous enough to compete with principal components ( see below ) . and there are only a few a priori relevant style factors for alphas @xcite to compete with the principal components .    just as in the case of stocks , we can resort to the principal components of the sample covariance ( or correlation ) matrix to build our multifactor risk model for alphas .",
    "this is where one of our key observations comes in . as we discuss in detail below , irrespective of how a factor model for @xmath12 is built , in the absence  which we assume based on our discussion above  of ( binary or quasi - binary )  clustering \" , when the number of alphas @xmath0 is large , the optimization ( [ weights ] ) invariably reduces to a ( weighted ) regression !",
    "this also holds for any reasonably realistic deformation ( e.g. , shrinkage @xcite ) of the sample covariance matrix for such deformations can be viewed as multifactor risk models .",
    "i.e. , there is no need to construct a full - blown risk model and compute the factor covariance matrix or even the specific ( idiosyncratic ) risk .",
    "so , as the simplest variant , we construct the factor loadings matrix from the first @xmath15 principal components of the sample covariance matrix corresponding to its positive eigenvalues and regress expected returns for alphas over this factor loadings matrix with the regression weights given by the inverse sample variances @xmath16 .",
    "however , it turns out that we do not even need to calculate the principal components thereby further reducing computational cost .",
    "here is a simple prescription for obtaining the weights @xmath7 .",
    "start with a time series @xmath17 of realized alpha returns ( @xmath18 labels the times @xmath19 ) .",
    "calculate the sample variances @xmath16 ( but not the sample correlations ) .",
    "this costs @xmath20 operations .",
    "normalize the returns via @xmath21 .",
    "demean @xmath22 both cross - sectionally and serially .",
    "let us call the so - demeaned returns @xmath23 .",
    "take only @xmath24 columns in @xmath23 ( e.g. , the first @xmath24 columns ) .",
    "take the expected returns @xmath25 for the alphas and normalize them via @xmath26 .",
    "regress @xmath27 over the @xmath28 matrix @xmath23 with unit weights and no intercept .",
    "this regression costs @xmath29 operations .",
    "take the residuals @xmath30 of this regression .",
    "then the optimal weights @xmath31 , where @xmath13 is fixed via ( [ norm ] ) .",
    "if the reader is only interested in the prescription and the source code , then the reader can go straight to appendix [ app.a ] , where we give r source code for this algorithm , is not written to be ",
    "fancy \" or optimized for speed or in any other way .",
    "its sole purpose is to illustrate the algorithms described in the main text in a simple - to - understand fashion .",
    "some legalese relating to this code is given in appendix [ app.c ] .",
    "] and skip the rest of the paper . however , if the reader would like to understand why this algorithm makes sense and also , among other things , how to potentially increase the number of risk factors from @xmath15 ( for a generous 1 year lookback @xmath32 ) to a few thousand , then the reader may wish to keep reading .    to summarize , calculating the weights @xmath7 does not require computing any principal components or inverting any large matrices , and it costs only @xmath29 operations , so it is linear in @xmath0 . furthermore ,",
    "the algorithm is not iterative , so there are no convergence issues to worry about .",
    "perhaps somewhat ironically , the simplicity of this algorithm is rooted in the very nature of this problem , that we have a small number of observations compared with the large ( in fact , huge ) number of alphas . as we discuss in more detail in the subsequent sections ,",
    "there is simply not much else one can do that would be out - of - sample stable with the exception of enlarging the number of risk factors provided there is additional information available to us .",
    "the remainder of this paper is organized as follows . in section [ sec.2 ]",
    "we set up our framework and notations . in section [ sec.3 ]",
    "we discuss why , absent  clustering \" , optimization using a factor model reduces to a regression in the large @xmath0 limit . in section [ sec.4 ]",
    "we discuss why this also applies to deformations of the sample covariance matrix as they too reduce to factor models .",
    "we also discuss why style factors add little value and how to enlarge the number of risk factors based on more detailed position data ( as opposed to the historical alpha return data ) . in section [ sec.5 ]",
    "we discuss a refinement whereby the analog of the  market \" mode for stocks is factored out to improve performance of the alpha portfolio .",
    "we also give the detailed algorithm for obtaining the optimal alpha weights and discuss the computational cost ( including why it is cheaper than doing principal components ) .",
    "we briefly conclude in section [ sec.6 ] .",
    "appendix [ app.a ] contains the source code for our algorithm .",
    "appendix [ app.c ] contains some legalese .",
    "parts of sections 3 and 4 are based on @xcite .",
    "so , we have @xmath0 time series of returns . a priori these returns can correspond to stocks or some other instruments , alphas , etc . here",
    "we will be general and refer to them simply as returns , albeit we will make some assumptions about these returns below .",
    "each time series contains @xmath33 observations corresponding to times @xmath19 , and we will denote our returns as @xmath17 , where @xmath8 and @xmath34 ( @xmath35 is the most recent observation ) .",
    "the sample covariance matrix ( scm ) is given by does not affect the weights @xmath7 in ( [ weights ] ) , so the difference between the unbiased estimate with @xmath15 in the denominator vs. the maximum likelihood estimate with @xmath33 in the denominator is immaterial for our purposes here .",
    "also , in most applications @xmath36 .",
    "] @xmath37 where @xmath38 are serially demeaned returns ; @xmath39 .",
    "we are interested in cases where @xmath40 , in fact , @xmath41 . when @xmath40 , @xmath12 is singular : we have @xmath42 , so only @xmath15 columns of the matrix @xmath43 are linearly independent .",
    "let us eliminate the last column : @xmath44 .",
    "then we can express @xmath12 via the first @xmath15 columns : @xmath45 here @xmath46 is a nonsingular @xmath47 matrix ( @xmath48 ) ; @xmath49 is a unit @xmath15-vector . note that @xmath50 is a 1-factor model ( see below ) .",
    "the challenge is to either deform @xmath12 such that it is nonsingular , or to replace it with a constructed nonsingular matrix @xmath51 such that it reasonably approximates @xmath12 in - sample and predicts it out - of - sample .",
    "let us first discuss the latter approach .",
    "a popular method  at least in the case of equities  for constructing a nonsingular replacement @xmath51 for @xmath12 is via a factor model : @xmath52 here : @xmath53 is the specific ( a.k.a .",
    "idiosyncratic ) risk for each return ; @xmath54 is an @xmath55 factor loadings matrix ; and @xmath56 is a @xmath57 factor covariance matrix ( fcm ) , @xmath58 . the number of factors @xmath59 to have fcm more stable than scm .    the nice thing about @xmath51 is that it is positive - definite ( and therefore invertible ) if fcm is positive - definite and all @xmath60 .",
    "for our purposes here it is convenient to rewrite @xmath51 via @xmath61 , where @xmath62 and @xmath63 . here",
    "( in matrix notations ) @xmath64 , and @xmath65 is the cholesky decomposition of @xmath66 , so @xmath67 .",
    "the weights ( [ weights ] ) ( with @xmath12 replaced by @xmath51 ) are given by @xmath68\\ ] ] where @xmath69 , and @xmath70 .",
    "the diagonal elements of this matrix are @xmath71 .",
    "it then follows that , if all @xmath72 , which we will argue to be the case momentarily , then @xmath73 and @xmath74 =   \\eta~z_i~\\varepsilon_i\\ ] ] where @xmath75 are the residuals of the cross - sectional weighted regression .",
    "] of @xmath25 over @xmath76 with the weights @xmath77 , or , equivalently , @xmath78 are the residuals of the unit - weighted regression of @xmath79 over @xmath80 .",
    "so , ( [ weights ] ) reduces to a regression .",
    "the question is why  or , more precisely , when  all @xmath81 .",
    "this is the case when : i ) @xmath0 is large , and ii ) there is no  clustering \" in the vectors @xmath80 .",
    "that is , we do not have vanishing or small values of @xmath82 for most values of the index @xmath83 with only a small subset thereof having @xmath84 . without",
    " clustering \" , to have @xmath85 , we would have to have @xmath86 , i.e. , @xmath87 and consequently @xmath51 would be almost diagonal .",
    "e.g. , consider a 1-factor model ( @xmath88 ) with uniform @xmath89 . in this model",
    "we have uniform pair - wise correlations @xmath90 . for these correlations not to be small",
    ", we must have @xmath91 .",
    "now , @xmath92 , where @xmath93 . for large @xmath0",
    "we have @xmath94 , @xmath95 , and in this case we have a weighted regression over the intercept .",
    "so , absent  clustering \" , when @xmath0 is large , the factor model is only useful to the extent of defining the regression weights @xmath96 via the specific risks @xmath53 .",
    "thus , fcm does not affect the regression residuals : they are invariant under linear transformations of @xmath80 , ( in matrix notations ) @xmath97 , where @xmath98 is a general nonsingular matrix .    what about  clustering \" ? for a large number of alphas trading largely overlapping universes ( e.g. , top 2,500 most liquid u.s .",
    "stocks ) there is no  clustering \" so long as they can not be classified in a binary fashion as in industry classifications for stocks , and such a classification of alphas usually is not possible .",
    "any risk factors then lack  clustering \" and are analogous to style factors or principal components .",
    "let us now discuss deforming  or regularizing  scm such that it is nonsingular .",
    "one method often used in this regard is the so - called shrinkage @xcite .",
    "it is often regarded as an  alternative \" to multifactor risk models . however , as was recently discussed in @xcite , shrunk scm is also a factor model .",
    "in fact , shrinkage is a special case of more general deformations , where instead of scm @xmath12 given by ( [ scm ] ) , one uses @xmath99 here the matrix @xmath100 is assumed to be positive - definite and ( relatively ) stable out - of - sample .",
    "we must have @xmath101 , so this imposes @xmath0 conditions on @xmath102 .",
    "a priori @xmath100 can be otherwise arbitrary .",
    "the matrix @xmath103 is some deformation of @xmath50 in ( [ scm ] ) . and",
    "(  shrinkage target \" ) @xmath104 , where the weight (  shrinkage constant \" ) @xmath105 , and @xmath51 ( usually chosen as a diagonal matrix or a @xmath106-factor model with low @xmath106 ) is such that @xmath107.[fn.shrunk ] ]    the issue with ( [ def ] ) is that : i ) in practice @xmath100 must have some relevance to the underlying returns whose covariance matrix we are attempting to model ; and ii ) a priori it is unclear what the deformed matrix @xmath103 should be .",
    "the available data is limited to the @xmath108 matrix @xmath43 , and the matrix @xmath50 , which is fixed . to go beyond this data",
    ", we invariably must introduce some additional input . as a 12th century georgian poet shota rustaveli put it , ",
    "what s in the jar is what will flow out .",
    "\" syllables per line with a caesura between the 8th and 9th syllables .",
    "how a human brain can come up with such perfection is mind - boggling , especially considering that this poem tells an extremely complex story complete with dialogs , aphorisms , etc . ]    however , not all is lost .",
    "the fact that we have large @xmath0 ( and no  clustering \" ) simplifies things . in practice",
    ", the matrix @xmath100 can not be arbitrary",
    ". it must be somehow  be it directly or indirectly  related to the returns whose covariance matrix we are after .",
    "the simplest choice is a diagonal matrix @xmath109 .",
    "more generally , we can take @xmath100 to be a @xmath106-factor model of the form ( [ fac.mod ] ) , @xmath110 , with a properly chosen diagonal @xmath111 .",
    "in fact , realistically , what else can @xmath100 be in practice ? if we knew how to write down a non - factor - model covariance matrix that approximates scm well and is out - of - sample stable , this paper would have been very different !",
    "so , assuming @xmath100 is a @xmath106-factor model ( with @xmath112 corresponding to a diagonal @xmath100 ) given by ( [ fac.mod ] ) , the deformed matrix @xmath113 is also a factor model .",
    "indeed , @xmath114 here : the index @xmath115 takes @xmath116 values ; @xmath117 ; @xmath118 , @xmath119 ; @xmath120 , @xmath58 ; @xmath121 , @xmath122 ; and @xmath123 .",
    "now we are in good shape . indeed , assuming large @xmath0 and no  clustering \" , we know that optimization using @xmath113 ( instead of @xmath12 ) in ( [ weights ] )  a factor model ",
    "reduces to a weighted regression of the expected returns over the @xmath124 factor loadings matrix @xmath125 , _ irrespective _ of fcm @xmath56 or the deformed matrix @xmath126 , and the latter we do not even have a ( constrained enough ) guiding principle for computing .",
    "all we need is to somehow compute the regression weights @xmath127 , i.e. , the specific risks .",
    "the specific risks follow from ( [ fac.mod.1 ] ) .",
    "however , to compute them , we do need to know @xmath56 and @xmath126 . indeed , recalling that @xmath101 ,",
    "we have so computed are positive and consistent with fcm .",
    "such algorithms and source code are given in @xcite ( see below ) . ]",
    "@xmath128 so , as far as the deformed matrix @xmath126 is concerned , a priori we have @xmath129 parameters to play with and not much guidance to play the game .",
    "in fact , there is no magic bullet here .",
    "simplicity is essentially the only beacon we can follow ...",
    "since at the end we have a weighted regression , which in itself does not require knowing @xmath56 or @xmath126 provided we know the weights , we can simply take @xmath130 .",
    "this might appear to contradict ( [ spec.risk ] ) , but it does not .",
    "this is because the residuals @xmath75 are invariant under the rescalings of the weights @xmath131 , where @xmath132 .",
    "so , setting @xmath130 is equivalent to setting @xmath133 , where @xmath134 , which simply puts @xmath0 conditions on @xmath135 ( from @xmath56 ) plus @xmath129 ( from @xmath126 ) a priori unknowns .",
    "this system may appear to be overconstrained for large enough @xmath0 , but there always exists a ",
    "solution \" : ) . ]",
    "we can simply take @xmath112 and @xmath136 .",
    "so , can we have weights other than the inverse sample variances ?",
    "a priori the answer is yes .",
    "here is a simple prescription . as above we can set @xmath136 , but take @xmath51 to be a nontrivial factor model ( @xmath137 ) .",
    "we must have @xmath138 .",
    "generally , @xmath139 need not equal rescaled @xmath140 .",
    "there is a notable exception : if we take @xmath51 to have a uniform correlation matrix .",
    "let the correlation be @xmath141 .",
    "then we have @xmath142 $ ] , where @xmath143 is the unit @xmath0-vector .",
    "in this case we have @xmath144 .",
    "so , in this 1-factor model the weights are the same as the inverse sample variances , albeit the regression is over @xmath33 columns",
    ". columns in @xmath43 , @xmath145 , plus a single column equal @xmath146 .",
    "usually , there is a high correlation between the latter and a linear combination of the former ( see below ) .",
    "in fact , we will argue below that the factor proportional to @xmath146 should be taken out altogether , i.e. , removed from the factor loadings matrix @xmath125 , irrespective of how the latter is constructed ( see section [ sec.5 ] ) . ]",
    "if we take a different factor model ( even a 1-factor model with nonuniform correlations ) , generally @xmath139 do not equal rescaled @xmath140 .",
    "so , what should / can the risk factor(s ) be ?",
    "since we are assuming no  clustering \" , i.e. , there is no binary classification we can construct for our returns , , where @xmath147 maps our @xmath0 returns to @xmath106  clusters \" .",
    "note , however , that the  weights \" @xmath148 ( not to be confused with the portfolio weights @xmath7 in ( [ weights ] ) ) can be arbitrary ( including negative ) and need not be proportional to the unit @xmath0-vector @xmath143 . ] a priori there are two evident choices for what the additional @xmath106 risk factors can be : i ) principal components and ii ) style factors ( analogous to those in equity risk models ) . below we will discuss a 3rd possibility .",
    "the idea is to take the first @xmath149 principal components of scm @xmath12 as @xmath54 .",
    "more precisely , there is another choice , to wit , to take @xmath150 , @xmath151 , where @xmath152 , @xmath153 , are the principal components of the sample correlation matrix @xmath154 .",
    "typically , the difference between the two choices is not make - it - or - break - it , with the latter preferred ( and usually producing better results ) as it factors out the ( skewed , quasi - log - normally distributed ) volatility @xmath146 and deals with the principal components of @xmath155 , whose off diagonal elements take values in the interval @xmath156 and have a tight distribution .",
    "so , we will adapt this approach here .",
    "as above , @xmath136 , we take @xmath157 , so our deformed scm , and @xmath158 . ] @xmath159 here : @xmath160 are the eigenvalues corresponding to the principal components @xmath161 ( @xmath162 , and @xmath163 for @xmath164 ) ; and @xmath165 ^ 2 $ ] . in terms of the regression ,",
    "this construction only affects the regression weights @xmath77 .",
    "indeed , the regression over the first @xmath15 principal components @xmath152 , @xmath166 , is the same as the regression over @xmath43 , @xmath167 , as these two matrices are related to each other via a linear transformation @xmath168 , where @xmath169 is a nonsingular @xmath47 matrix .",
    "as to @xmath139 , calculating it requires computing the first @xmath106 principal components.^2 $ ] ; the @xmath170 terms are weighted by smaller eigenvalues.[fn.xi ] ] for sufficiently low @xmath171 we can use the power iterations method @xcite ( see subsection 4.1 for the algorithm and appendix b for r source code in @xcite ) .",
    "operations , where the number of iterations @xmath172 .",
    "as @xmath106 increases , at some point @xmath173 and it makes more sense to use the next method . ]",
    "if @xmath174 , then we can use a no - iterations method ( see subsection 4.2 for the algorithm and appendix c for r source code in @xcite ) . operations . ]",
    "e.g. , we can calculate the regression weights by computing the first few principal components ( which are different from the inverse sample variances ) and then run a weighted regression of @xmath25 over @xmath43 .",
    "note , however , that for @xmath175 the 1st principal component typically has a large cross - sectional correlation with the intercept , i.e. , @xmath176 is close to 1 , so if we take @xmath88 , @xmath139 are close to rescaled @xmath140 .",
    "furthermore , higher principal component terms are subleading ( see footnote [ fn.xi ] ) . based on principal components are still close to rescaled @xmath140 . ]",
    "style factors are based on measured or estimated properties of our returns . even in the case of stocks ,",
    "their number is at most of order 10 . in the case of",
    "alphas the a priori possible style factors are logs of volatility , turnover , momentum and , possibly , capacity @xcite .",
    "we will discuss the first three below .",
    "there are two parts to the story here .",
    "first and foremost , if @xmath36 , then on general grounds it is clear that adding a few style factors to the regression can not make a big difference provided that we keep the regression weights fixed .",
    "second , the 3 style factors above turn out to be poor predictors for pair - wise correlations . for turnover",
    "this was argued based on empirical evidence in @xcite .",
    "a similar analysis for volatility yields the same conclusion , that log of volatility is not a good predictor for pair - wise correlations . , where @xmath177 is such that @xmath178 has zero mean .",
    "we define three symmetric tensor combinations @xmath179 , @xmath180 , and @xmath181 ( @xmath182 is the unit @xmath0-vector ) .",
    "we further define a composite index @xmath183 , which takes @xmath184 values , i.e. , we pull the off - diagonal lower - triangular elements of a general symmetric matrix @xmath185 into a vector @xmath186 .",
    "this way we can construct four @xmath187-vectors @xmath188 , @xmath189 , @xmath190 and @xmath191 . now we can run a linear regression of @xmath188 over @xmath189 , @xmath190 and @xmath191 .",
    "note that @xmath192 is simply the intercept ( the unit @xmath187-vector ) , so this is a regression of @xmath188 over @xmath190 and @xmath191 with the intercept .",
    "the results based on the same data as in @xcite are summarized in table [ table.corr.vol ] and figure 1 confirming our conclusion above.[fn.reg ] ] momentum is defined as an average realized return over some period of time .",
    "the expected return is also defined as an average realized return over some  possibly other ",
    "period of time . allocating capital into alphas inherently is a  momentum \" strategy : usually one does not bet against alphas that have performed well in the past . including momentum in the factor loadings matrix in the regression ( partly ) ",
    "kills \" alpha . and figure 2 summarize the regression results . ]",
    "one place where the style factors can make a difference is in computing the regression weights .",
    "i.e. , we do not include them in the regression , but take the weights to be @xmath77 , where the specific risks @xmath53 are for a factor model based on the style factors only .",
    "that is , we model the correlation matrix @xmath155 via a factor model based on all or some of the four factors , the intercept , log of volatility , log of turnover , and log of momentum .",
    "as mentioned above , the intercept by itself yields rescaled sample variances , but makes a difference in combination with other factors .",
    "we can always simply take the inverse sample variances as the regression weights and not bother with computing the specific risks based on the style factors . without a detailed analysis using real - life",
    "alphas it is unclear if style factor based regression weights add value .",
    "regardless , we need not include the style factors in the regression .      here",
    "we discuss the simplest  albeit neither the only nor necessarily the best ",
    "way to compute the specific risks .",
    "as above , instead of modeling scm @xmath12 via a factor model , it is convenient to model the sample correlation matrix @xmath155 .",
    "let the corresponding factor model be @xmath193 where @xmath194 , @xmath195 , and @xmath196 .",
    "first , without loss of generality we can assume that the columns of @xmath197 are linearly independent .",
    "second , we can assume that they form an orthonormal basis , i.e. , the matrix @xmath198 is the @xmath57 identity matrix : @xmath199 .",
    "indeed , we can always ensure orthonormality via the transformation ( in matrix notations ) @xmath200 , where @xmath201 is the cholesky decomposition of @xmath202 , so @xmath203 .",
    "furthermore , we have @xmath204 .    with orthonormal @xmath197 ,",
    "fcm @xmath56 is simply a projection of the sample correlation matrix onto the @xmath106-dimensional hyperplane defined by the columns of @xmath205 in the @xmath0-dimensional space : @xmath206 the specific risks then follow from ( [ fac.mod.2 ] ) : @xmath207 however , there is a caveat in this approach . for a generic matrix @xmath205",
    "there is no guarantee that the so - defined @xmath208 are positive , which they should be .",
    "this imposes nontrivial restrictions on @xmath205 . as an illustration ,",
    "let us discuss the @xmath88 case .",
    "for @xmath88 things simplify .",
    "let us denote the sole column of @xmath205 via @xmath209 .",
    "then @xmath210 and @xmath211 where @xmath212 , and @xmath213 is the largest eigenvalue of @xmath155 .",
    "so , a sufficient condition for having all @xmath214 is that all @xmath215 .",
    "we can replace this condition with a stronger one that avoids computing the largest eigenvalue : a sufficient condition is that all @xmath216 , where @xmath217 .",
    "note that @xmath218 , which corresponds to the intercept as the sole risk factor , satisfies this condition .",
    "violations of this condition usually occur for @xmath209 with a skewed distribution , e.g. , if @xmath219 ; however , for , e.g. , @xmath220 such violations are either absent altogether or rarer and can be dealt with by  reigning \" in the few violating elements .",
    "see the r function in appendix a of @xcite for such an algorithm , which is built for a general @xmath106-factor model ( not just @xmath88 ) .",
    "when we have multiple factors , however , things get trickier . even if @xmath221 in all @xmath106 1-factor models based on the individual columns of a multifactor @xmath205 , in the @xmath106-factor model we can still have some @xmath222 .",
    "see section 4 for the algorithm and appendix b for r source code in @xcite for circumventing this issue ( even for @xmath88 ) . however , let us note one issue associated with computing the specific risks even if they all come out to be positive .",
    "computing @xmath223 via ( [ xi.twiddle ] ) involves fcm @xmath56 , which in turn involves the sample correlation matrix @xmath155 via ( [ fcm ] ) . while @xmath56 is expected to be more out - of - stable than @xmath155",
    "as we have @xmath59 , using fcm in ( [ xi.twiddle ] ) still adds some noise to the regression weights .",
    "this is to be contrasted with using the inverse sample variances ( i.e. , @xmath224 ) , which are relatively stable out - of - sample , as the regression weights .",
    "this remark equally applies to all @xmath106 values .",
    "so , while we can try to play with the regression weights , it is not all that clear that this would make it or break it , especially that we are still limited to the @xmath15 risk factors , which are equivalent to the first @xmath15 principal components  albeit we never have to compute the principal components in the first instance .",
    "the question is , can we increase the number of columns in the factor loadings matrix in the regression as this presumably would cover more directions in the risk space and improve the out - of - sample performance .",
    "prosaically , the answer is that we need more information , i.e. , additional input , to achieve this . here",
    "is one approach @xcite .",
    "the idea here is that , assuming all alphas have essentially overlapping trading universes , we can treat exposure to each underlying tradable  for the sake of definiteness let us assume we are dealing with the u.s .",
    "equities as the underlying tradables  as a risk factor .",
    "this makes sense , but the question is what should the factor loadings matrix @xmath197 be ? in this case @xmath225 simply labels stocks in the trading universe , which is , say , top 2,500 most liquid tickers , so @xmath106 is large , much larger than the typical value of @xmath15 , which for a ( generous ) 1-year lookback is only about 250 .",
    "historical stock position data for each alpha must be available to us if we are to backtest them .",
    "let this position data be @xmath226 , which is the dollar holding of the alpha labeled by @xmath83 in the stock labeled by @xmath225 at time labeled by @xmath19 , normalized such that @xmath227 for each given pair @xmath228 .",
    "we can try to construct @xmath197 from @xmath226 by getting rid of the time series index @xmath229 .",
    "the most obvious choice @xmath230 does not work as the sign of @xmath226 flips over time frequently assuming alphas have reasonably short holding periods .",
    "we need an unsigned quantity to define @xmath197 .",
    "we can use @xmath231 this is simply average relative exposure of the @xmath83-th alpha to the stock labeled by @xmath225 .",
    "one potential  shortcoming \" of this definition is that , if the position bounds  call them @xmath232  are imposed at the level of individual alphas , for some , mostly less liquid , stocks @xmath233 could be saturating these bounds . on general risk management grounds",
    ", these bounds can very well be uniform across all alphas .",
    "e.g. , one may wish to cap the positions as the smaller of : i ) a small percentile of the total dollar investment  this is a diversification bound ; and ii ) a ( generally , different ) small percentile of addv ( average daily dollar volume )  this is a liquidity bound ( in case positions need to be liquidated ) .",
    "if the bounds are saturated most of the time , this can effectively reduce the number of independent risk factors , and the definition of @xmath197 may have to be modified for such stocks ( see @xcite ) . however , if the bounds are imposed at the level of the combined alpha , then this is a non - issue .    assuming @xmath234 , even with the larger number of risk factors ( [ modulus.1 ] ) , our optimization reduces to a weighted regression .",
    "we can simply choose the weights as the inverse sample variances .",
    "alternatively , we can attempt to compute specific risks . for this",
    "we need fcm @xmath56 . with appropriately normalized @xmath197 ,",
    "fcm @xmath56 is simply the covariance matrix for the stocks . in the zeroth approximation",
    "we can set it to @xmath235 , where @xmath236 are sample variances for stocks .",
    "alternatively , we can either use commercial risk models or construct them organically , as , e.g. , in @xcite and @xcite .",
    "also see section [ sec.6 ] hereof .",
    "so , to summarize , our optimization ( [ weights ] ) reduces to a regression of the normalized returns @xmath237 over the factor loadings matrix @xmath238 . for simplicity ,",
    "let us take @xmath239 .",
    "further , let us take @xmath54 to be the @xmath15 demeaned returns @xmath43 , @xmath119 , i.e. , we are not using any additional style or other risk factors",
    ". then the sample correlation matrix is given by ( we identify the index @xmath225 with the index @xmath229 ) @xmath240 it is evident that the columns of @xmath241 are nothing but some linear combinations of the first @xmath15 principal components @xmath152 , @xmath166 , of @xmath155 .",
    "for large @xmath0 the first principal component @xmath242 is close to the appropriately normalized unit @xmath0-vector : @xmath243 . recall from ( [ w.reg ] ) ( see the discussion right thereafter ) that the weights @xmath244 , where @xmath30 are the residuals of the regression of @xmath26 over @xmath241 with unit weights , or , equivalently , over the principal components @xmath152 .",
    "this implies that @xmath245 and , therefore , many weights @xmath7 are negative ( assuming the expected returns @xmath25 are all positive ) .",
    "i.e. , the vanilla optimization forces us to take bets against many positive expected returns because it hedges against all alphas simultaneously losing money .",
    "this is overkill and literally  kills \" alpha : if alphas are not highly correlated on average , most alphas losing money all at once is possible but highly unlikely .",
    "assuming tolerance for drawdowns , we can relax this hedge .      to further illustrate this point ,",
    "let us consider a simple example .",
    "let us hypothetically assume that the true correlation matrix has uniform off - diagonal elements : @xmath246 , where @xmath143 is the unit @xmath0-vector .",
    "then the weights are given by ( note that this is exact ) : @xmath247\\ ] ] where @xmath237 , and @xmath248 .",
    "so , if @xmath249 and @xmath250 , then the expression in the square brackets on the r.h.s . of ( [ w.1f ] ) is approximately equal cross - sectionally demeaned @xmath27 and many weights will be negative .",
    "is skewed and roughly log - normal , and so is that of @xmath25 , so the distribution of @xmath27 is not very skewed and is roughly normal ; see @xcite . ]",
    "how can we fix this ?      in the case of equity portfolios with a large number of tickers we also have the same behavior , that the first principal component of the sample correlation matrix is close to the rescaled intercept .",
    "this is known as the  market \" mode and describes the overall , in - sync movement of all stocks , i.e. , of the broad market as a whole .",
    "however , in the case of equity portfolios the issue of the  market \" mode is subdued .",
    "thus , for dollar - neutral portfolios roughly half of the weights are negative by the dollar - neutrality constraint . in long - only portfolios",
    "one typically does not optimize the raw expected returns directly but against a broad benchmark . in this",
    "regard , alpha portfolio optimization is analogous to long - only portfolio optimization for equities .",
    "so , here too we could optimize against a benchmark alpha portfolio ( as opposed to ( [ weights ] ) ) .",
    "e.g. , we can take @xmath251 which corresponds to taking the diagonal part of scm @xmath12 .",
    "there are other choices .    alternatively",
    ", we can simply remove the  overall \" mode ( i.e. , the analog of the  market \" mode for equity portfolios ) .",
    "one way is to take the @xmath15 principal components and simply remove the first principal component , i.e. , to run the regression over the factor loadings matrix @xmath252 , @xmath253 .",
    "however , this would require computing the principal components .",
    "there is a simpler way .",
    "we take @xmath254 in ( [ psi ] ) and demean its columns .",
    "let the resulting matrix be @xmath255 .",
    "only @xmath24 of its columns are linearly independent .",
    "so , we can regress over @xmath255 , @xmath256 .",
    "this removes the  overall \" mode and while some weights may still turn out to be negative , their number will be relatively limited ( not roughly half of the weights ) .",
    "the backtested sharpe ratio will go down , and the portfolio return will go up .      for clarity , let us put all the pieces together into a step - by - step summary :    @xmath257 1 ) start with a time series of alpha returns labels the alphas ; @xmath18 labels the times @xmath19 . ]",
    "@xmath17 , @xmath8 , @xmath18 .",
    "@xmath257 2 ) calculate the serially demeaned returns @xmath258 .",
    "@xmath257 3 ) calculate sample variances @xmath259 .",
    "@xmath257 4 ) calculate the normalized demeaned returns @xmath260 .",
    "@xmath257 5 ) keep only the first @xmath15 columns in @xmath23 : @xmath261 .",
    "@xmath257 6 ) cross - sectionally demean @xmath23 : @xmath262 .",
    "@xmath257 7 ) keep only the first @xmath24 columns in @xmath263 : @xmath256 .",
    "@xmath257 8) take the alpha expected returns @xmath25 and normalize them : @xmath26 .",
    "@xmath257 9 ) calculate the residuals @xmath30 of the unit - weighted regression of @xmath27 over @xmath263 .",
    "@xmath257 10 ) set the alpha portfolio weights to @xmath264 .",
    "@xmath257 11 ) set the normalization coefficient @xmath13 such that @xmath265 .",
    "source code in r for the above procedure is given in appendix [ app.a ] .",
    "if we wish to use the underlying tradables as the risk factors as in ( [ modulus.1 ] ) , then we simply replace @xmath263 in step 9 ) above by the corresponding factor loadings matrix ( see appendix [ app.a ] ) . ) or a union thereof with @xmath23 defined in step 5 ) above ( with any linearly dependent columns removed ) . ] also , in steps 4)-11 ) above instead of using sample variances @xmath266 , we can use specific variances @xmath139 computed based on principal components or style factors ( see above ) .",
    "it is evident than none of the steps above cost more than @xmath20 operations except perhaps for step 9 ) , the regression .",
    "the regression residuals are given by @xmath267 where @xmath268 .",
    "calculating this @xmath269 matrix costs @xmath29 operations . straightforwardly inverting it costs @xmath270 operations .",
    "has a factor form does not help as @xmath271 , in fact , in practice @xmath272 . ]",
    "the rest ( sums over @xmath273 , etc . )",
    "costs @xmath29 operations , and therefore so does the regression .",
    "the answer is yes . as we discussed above , the sample correlation matrix @xmath274 ( where @xmath46 ; @xmath275 is the unit @xmath15-vector ) , so the @xmath15 columns of @xmath23 are just linear combinations of the first @xmath15 principal components @xmath152 , @xmath166 , of @xmath155 .",
    "so , we could use the principal components instead of @xmath23 in the above steps and get the same result . ] for the weights @xmath7 .",
    "however , computing the first @xmath15 principal components costs additional @xmath29 operations .",
    "as we discussed above , in the absence of  clustering \" , when the number of alphas is large , optimization ( via maximizing the sharpe ratio ) reduces to a ( weighted ) regression irrespective of whether we start from a constructed factor model , or deform the sample covariance matrix .",
    "this is because such deformations themselves are nothing but factor models .",
    "we also argued that in most cases the factor loadings , over which the expected returns are regressed , are given by the ( properly demeaned , normalized and trimmed ) time series matrix of historical returns based on which the ( singular ) sample covariance matrix is computed .",
    "the regression weights , which can be recast as the normalizations of the expected returns , the factor loadings matrix and the alpha weights , can be taken as inverse sample variances or , alternatively , specific variances in some factor model .",
    "however , computation of these specific variances via ( [ xi.twiddle ] ) requires computing the factor covariance matrix via ( [ fcm ] ) ( thereby adding noise to the regression weights ) not needed to compute sample variances .",
    "there is a notable exception to this , to wit , if we use the underlying tradables ( stocks ) themselves as risk factors via ( [ modulus.1 ] ) . in this case",
    "the factor covariance matrix @xmath56 need not be computed via a sample covariance matrix of linear combinations of the alpha returns .",
    "instead , it can be taken to be the covariance matrix for stocks .",
    "the latter need not be computed as a sample covariance matrix of stock returns , which would be out - of - sample unstable or , even worse , singular .",
    "instead , we can use a constructed covariance matrix for stocks , e.g. , via a factor model @xcite . a priori we could use commercially available risk models ( albeit they are not necessarily expected to be out - of - sample stable ) , or build them organically via , e.g. , heterotic risk models @xcite or heterotic capm @xcite .",
    "in fact , in the zeroth approximation we can set @xmath276 , where @xmath277 are sample ( historical ) stock volatilities or implied volatilities from options .",
    "and once we nail down @xmath56 , we can compute the specific variances via ( [ xi.twiddle ] ) .",
    "however , in reality there is a caveat here .",
    "the caveat is that if we identify @xmath56 with the stock covariance matrix , then the factor loadings matrix is given by ( [ modulus.1 ] ) only up to an overall normalization constant which is a priori unknown .",
    "so we can treat it as a free parameter and consider a 1-parameter family of specific variances .",
    "the value of this parameter then can be fixed by optimizing for realized performance with the caveat that it need not be out - of - sample stable and may have to be recomputed frequently based on short lookbacks .",
    "this provides a well - defined prescription for computing specific variances .",
    "alternatively , we can use sample variances , which are relatively stable out - of - sample and simple to compute .",
    "yet another alternative is to use specific variances computed based on principal components ( see @xcite and above ) or style factors .",
    "the latter case generally requires using more sophisticated methods such as those discussed in @xcite .",
    "a nice thing about our optimization reducing to a regression , which is computationally cheap , is that it can be readily modified to incorporate bounds on the alpha weights @xmath7 . indeed , since @xmath7 are proportional to the regression residuals , we can simply use the bounded regression discussed in@xcite .",
    "similarly , we can incorporate trading costs via the method discussed in @xcite .",
    "finally , let us mention that if we use the algorithms of @xcite for building statistical risk models , which include fixing the number of risk factors ( i.e. , principal components ) @xmath106 together with the specific risks , we are going to end up with a regression over @xmath149 principal components with the regression weights equal the inverse specific variances . in the case of stocks",
    "this works better than regressing over @xmath15 principal components with the regression weights ad hoc set equal the inverse specific variances for a @xmath106-factor model with @xmath149 @xcite .",
    "same may or may not hold for , e.g. , @xmath278 real - life alphas .",
    "in this appendix we give the r source code for calculating the alpha weights based on a regression .",
    "the code below is essentially self - explanatory and straightforward as it simply follows the algorithm and formulas in subsection [ sub.reg.algo ] .",
    "it consists of a single function ; is an @xmath0-vector of expected returns we wish to optimize ; @xmath0 is the number of the underlying returns ( e.g. , alphas ) ; is an @xmath279 matrix of returns ; @xmath33 is the number of data points in the time series ( e.g. , days ) ; is an @xmath55 factor loadings matrix @xmath197 , @xmath151 , pre - computed , e.g. , via ( [ modulus.1 ] ) ; otherwise , if the default is used , the code computes the factor loadings matrix @xmath23 ( @xmath256 or @xmath145 depending on whether or  see below ) based on the time series via the algorithm of subsection [ sub.reg.algo ] ; is an @xmath0-vector of specific risks @xmath280 pre - computed , e.g. , via ( [ xi.twiddle ] ) or ( [ sp.risk ] ) ; otherwise , if the default is used , the code computes as the square root of the sample variances ; , if ( default ) , implies that the  overall \" mode is taken out ; otherwise , it is kept ( see subsection [ sub.reg.algo ] ) .",
    "the output is an @xmath0-vector @xmath7 of the optimized alpha weights normalized such that @xmath281 .",
    "the code can be easily modified , e.g. , to combine ( via ) the pre - computed factor - loadings matrix @xmath197 as in ( [ modulus.1 ] ) with the factor loadings matrix @xmath23 it already computes based on the time series .",
    "wherever the context so requires , the masculine gender includes the feminine and/or neuter , and the singular form includes the plural and _ vice versa_. the author of this paper (  author \" ) and his affiliates including without limitation quantigic@xmath6 solutions llc (  author s affiliates \" or  his affiliates \" ) make no implied or express warranties or any other representations whatsoever , including without limitation implied warranties of merchantability and fitness for a particular purpose , in connection with or with regard to the content of this paper including without limitation any code or algorithms contained herein (  content \" ) .",
    "the reader may use the content solely at his / her / its own risk and the reader shall have no claims whatsoever against the author or his affiliates and the author and his affiliates shall have no liability whatsoever to the reader or any third party whatsoever for any loss , expense , opportunity cost , damages or any other adverse effects whatsoever relating to or arising from the use of the content by the reader including without any limitation whatsoever : any direct , indirect , incidental , special , consequential or any other damages incurred by the reader , however caused and under any theory of liability ; any loss of profit ( whether incurred directly or indirectly ) , any loss of goodwill or reputation , any loss of data suffered , cost of procurement of substitute goods or services , or any other tangible or intangible loss ; any reliance placed by the reader on the completeness , accuracy or existence of the content or any other effect of using the content ; and any and all other adversities or negative effects the reader might encounter in using the content irrespective of whether the author or his affiliates is or are or should have been aware of such adversities or negative effects .    the r code included in appendix [ app.a ] hereof is part of the copyrighted r code of quantigic@xmath6 solutions llc and is provided herein with the express permission of quantigic@xmath6 solutions llc .",
    "the copyright owner retains all rights , title and interest in and to its copyrighted source code included in appendix [ app.a ] hereof and any and all copyrights therefor .",
    "bouchaud , j .-",
    "p . and potters , m.  financial applications of random matrix theory : a short review . \" in : akemann , g. , baik , j. and di francesco , p. ( eds . ) the oxford handbook of random matrix theory .",
    "oxford , united kingdom : oxford university press , 2011 .",
    ".summary for the cross - sectional regression of @xmath188 over @xmath190 and @xmath191 with the intercept , where @xmath190 and @xmath191 are based on log of volatility .",
    "see subsection [ sub.sub.style ] for details",
    ". also see figure 1 .",
    "[ cols= \" < , < , < , < , < \" , ]"
  ],
  "abstract_text": [
    "<S> we give an explicit algorithm and source code for computing optimal weights for combining a large number @xmath0 of alphas . </S>",
    "<S> this algorithm does not cost @xmath1 or even @xmath2 operations but is much cheaper , in fact , the number of required operations scales linearly with @xmath0 . </S>",
    "<S> we discuss how in the absence of binary or quasi - binary  clustering \" of alphas , which is not observed in practice , the optimization problem simplifies when @xmath0 is large . </S>",
    "<S> our algorithm does not require computing principal components or inverting large matrices , nor does it require iterations . </S>",
    "<S> the number of risk factors it employs , which typically is limited by the number of historical observations , can be sizably enlarged via using position data for the underlying tradables .    * how to combine a billion alphas *    zura kakushadze@xmath3@xmath4 solutions llc , and a full professor at free university of tbilisi . </S>",
    "<S> email : zura@quantigic.com ] and willie yu@xmath5    _ </S>",
    "<S> @xmath3 quantigic@xmath6 solutions llc _    _ 1127 high ridge road # 135 , stamford , ct 06905 solutions llc , the website or any of their other affiliates . ] _    </S>",
    "<S> _ @xmath4 free university of tbilisi , business school & school of physics _    </S>",
    "<S> _ 240 , david agmashenebeli alley , tbilisi , 0159 , georgia _    _ @xmath5 centre for computational biology , duke - nus medical school _    _ 8 college road , singapore 169857 _    ( february 27 , 2016 ) </S>"
  ]
}