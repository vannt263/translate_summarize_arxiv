{
  "article_text": [
    "mr imaging is one of the most valuable imaging methods in the clinic for the needs of diagnostic and therapeutic indications .",
    "however , the physical and physiological constraints basically limit the rate of mr acquisition .",
    "since the long scan time is one of the shortcomings of mr imaging , the efficient acceleration scheme for mr acquisition is important to reduce the acquisition time . accordingly , under - sampling of k - space is necessary , and many researchers have developed various reconstruction methods such as parallel imaging  @xcite and compressed sensing mri ( cs - mri )  @xcite that allow for accurate reconstruction from the insufficient k - space samples .",
    "for example , generalized autocalibrating partial parallel acquisition ( grappa)@xcite is a representative parallel mri ( pmri ) technique that interpolates the missing k - space data by exploiting the diversity of the coil sensitivity maps . on the other hand",
    ", cs - mri reconstructs a high - resolution image from randomly sub - sampled k - space data by utilizing the sparsity of the data in the transformed domain .",
    "cs algorithms are commonly formulated as penalized inverse problems that minimize the tradeoff between the data fidelity term in the k - space and the sparsity penalty in the transform domain .",
    "the state - of - the - art cs algorithm in this field is the annihilating filter - based low - rank hankel matrix approach ( aloha ) , in which the cs - mri and parallel mri can be unified as an interpolation problem in the weighted k - space domain using a low - rank structured matrix completion  @xcite .",
    "one of the limitations of these algorithms , however , is the computational complexity .",
    "although these cs algorithms can achieve state - of - the - art performance , computational complexity is usually high and leads to an increase in the time for image reconstruction .",
    "moreover , the incoherent scanning patterns required for the cs - mri are usually different from those of the standard acquisition , so additional pulse sequence programming is often required .",
    "recently , deep learning has proved to be an important framework for computer vision research @xcite , thanks to the massive datasets and the development of computing hardwares such as gpus . in particular ,",
    "convolution neural network ( cnn ) , which is a particular form of feedforward neural networks , use trainable filters ( or weights ) and convolution operations between the input and filters .",
    "cnn implicitly learns the filter coefficients to effectively extract local features from the training data .",
    "thus , cnn has a strong ability to capture the local features of the input images , resulting in a great success in classification problems  @xcite , as well as regression problems such as segmentation  @xcite , denoising  @xcite , super resolution  @xcite , etc .",
    "the success of deep learning has been recently investigated in statistical learning literatures @xcite , showing that the exponential expressivity or representation power has been attributed to its success @xcite . therefore , this paper aims to utilize the great ability of cnn to capture the feature of image structures for medical image reconstruction problems .    in x - ray",
    "computed tomography ( ct ) , there are some successes in the applications of deep learning in image reconstruction .",
    "kang et al .",
    "@xcite showed that the deep convolutional neural network ( cnn ) can efficiently remove the noises originated from the low - dose ct .",
    "they used the directional wavelets with deep cnn structure to reduce these low - dose artifacts . on the other hand",
    ", there were some studies in the sparse views ct to remove the globally distributed streaking artifacts originating from the limited number of projection views  @xcite .    in mri , the first try to apply the deep learning approach",
    "was carried out by wang et al  @xcite .",
    "they have trained the deep neural network from the downsampled reconstruction images to learn a fully sampled reconstruction .",
    "they then attempted to combine the deep learning outcome with cs - mri reconstruction methods in two ways .",
    "first , they used the image generated by the learned network to initialize the cs - mri and then reconstructed images .",
    "secondly , they used the output of the network as a reference image and used it as an additional regularization term in classical cs approaches .",
    "hammerinik et al .",
    "@xcite developed a deep network architecture as unfolded iterative steps of cs - mri .",
    "instead of using the classical regulizers such as @xmath4 and total varitaion , their networks learned a set of filters and corresponding penalty functions using a reaction diffusion model  @xcite .",
    "all the parameters of the network including the filters and the influence functions are trained from the set of training data .    in his seminal work on statistical learning theory @xcite",
    ", vapnik showed that the learning problem is a highly ill - posed inverse problem of the unknown _ probability distribution . _ inspired by this insight and the similarity with the compressed sensing problem , we are interested in adopting learning theory to expand the theory of compressed sensing to an inverse problem of the unknown _ distribution _ , rather than the inverse problem of a single realization .",
    "this view gives us much flexibility and clear directions in designing learning architecture for a given compressed sensing mr problem .",
    "specifically , to allow for an accurate distribution estimate , similar to the role of sparse transformation in cs theory , we must find a way to simplify the data distribution to meet the representation power of a given network . among various possible methods ,",
    "this paper examines various sub - sampling patterns , the associated aliasing artifact patterns , and the image against the artifact data manifold to find out what makes the data distribution simple .",
    "specifically , using a computational topology called persistent homology , we show that the aliasing artifact from uniform subsampled k - space data with a few low - frequency components has a simpler topological structure , so that learning these artifacts is easier than learning the original artifact - free images .",
    "this causes us to learn aliasing artifacts rather than the aliasing - free image from fully sampled data ( see fig .",
    "[ fig : concept ] ( b ) , ( c ) ) . once the aliasing artifacts are estimated , an aliasing - free image is then obtained by subtracting the estimated aliasing artifact as shown in fig .",
    "[ fig : concept ] ( a ) .",
    "another important contribution of this paper is that for the globally distributed aliasing artifacts , it is shown that a desired neural network should have a large receptive field to cover the entire artifacts . as a possible realization of the large receptive field ,",
    "the deconvolution network @xcite with contracting path , often referred to as a u - net structure  @xcite @xcite , is shown effective in estimating the aliasing artifacts . by combining the artifact learning scheme with the u - net structure , we show that the reconstruction performance can be significantly improved .",
    "furthermore , another very important advantage of the proposed method is that , once the network is trained , the network immediately produces accurate results while the existing cs algorithms require substantially higher time and computing costs .",
    "one might argue that the training time should be counted as the reconstruction time for a fair comparison .",
    "however , as long as the training can be carried out once with extensive datasets ( eg . on the manufacturer side ) , the reconstruction can be carried out immediately for each scanner .",
    "this is how current deep learning - based image or speech recognition systems were developed by google , baidu , etc . , and the success of the proposed method informs us that this type of approach might be possible in the mri problems .",
    "in a multi - channel cs - mri problem , the k - space measurement data is given by @xmath5 with @xmath6 where @xmath7 and @xmath8 denote the unknown image and the corresponding k - space measurements from the @xmath9-th coil , respectively ; @xmath10 is the number of coils ; @xmath11 is a subsampled fourier matrix .",
    "the minimum norm reconstruction from sub - sampled measurement can be obtained by @xmath12 where @xmath13 denoting the pseudo - inverse . for the cartesian trajectory , this can be easily obtained by taking the fast fourier transform ( fft ) after zero - padding k - space data . however , the main issue is that the minimum norm solution suffers from aliasing artifacts .    to address this , the popular approaches is using compressed sensing approach by imposing the sparsity in the transform domain .",
    "for example , @xmath4-spirit ( @xmath4- iterative self - consistent parallel imaging reconstruction ) @xcite utilizes the grappa type constraint as an additional constraint for a compressed sensing problem .",
    "a recent aloha algorithm fully extends the insight of grappa and converts cs - mri and pmri problem into a k - space interpolation problem using a low - rank interpolation using a structured matrix completion  @xcite .",
    "however , to solve the cs problem , computationally expensive iterative reconstruction methods must be used .",
    "thus , instead of solving the computationally expensive optimization problems , this paper is mainly interested in obtaining the original image @xmath14 from the aliased images using machine learning approaches .",
    "one possible approach is to learn the artifact free images from the aliased images ( fig .",
    "[ fig : concept ] ( b ) ) .",
    "specifically , suppose we are given a sequence of training data @xmath15 where @xmath16 where @xmath17 and @xmath18 denote the aliased image defined in and the artifact - free image in for the @xmath9-the data , respectively .",
    "then , a neural network @xmath19 is trained such that it minimizes the empirical risk : @xmath20    another approach is to learn the aliasing artifact as the difference between aliased mr image and artifact - free image ( fig .  [ fig : concept ] ( c ) ) .",
    "in particular , we define the artifact in the magnitude and the phase domains separately , since it is easier to learn the real valued data than the complex - valued ones .",
    "specifically , the magnitude and phase domain artifacts are defined as follows : @xmath21 where @xmath22 and @xmath23 represent the element - wise absolute and angle of a complex number .",
    "then , the training data is given by @xmath15 where @xmath24 for the @xmath9-the data .",
    "again , a neural network @xmath19 is trained such that it minimizes the empirical risk : @xmath20      however , the direct minimization of empirical risk , @xmath25 , is problematic due to the potential issue of the overfitting . in order to avoid overfitting",
    ", we must minimize the risk @xmath26 where @xmath27 $ ] denotes the expectation under the data distribution @xmath28 .",
    "however , the distribution @xmath28 is unknown , so we can not directly minimiize the risk ; instead , we are interested in bounding the risk with computable quantities .",
    "this is often called the _ generalization bound _ @xcite .",
    "specifically , the risk of a learning algorithm can be bounded in terms of complexity measures ( eg . vc dimension and shatter coefficient ) and the empirical risk  @xcite .",
    "the rademacher complexity  @xcite is one of the most modern notions to measure the complexity that is distribution - dependent and defined for any class of functions .",
    "specifically , with probability @xmath29 , @xmath30 where the empirical rademacher complexity @xmath31 is defined to be @xmath32,\\ ] ] where @xmath33 denotes a functional space , and @xmath34 are independent random variable uniformly chosen from @xmath35 .",
    "therefore , in order to reduce the risk , both the empirical risk ( i.e. data fidelity ) and the complexity terms in eq .   must be simultaneously minimized .    in a neural network",
    ", the value of the risk is determined by the representation power of the network @xcite , whereas the complexity term is determined by the structure of the network @xcite .",
    "in fact , the fundamental trade - off lies between the network complexity and empirical risk in the generalization bound .",
    "when a deep network becomes more complex , the representation power increases to reduce the empirical risk at the expense of the increased complexity penalty .",
    "so this gives us a further perspective in the design of a deep network .",
    "specifically , if there is a means to convert the learning problem more easily , we can use a simpler network with less complexity penalty that can also reduce the empirical risk so that the generalization bound can be reduced .",
    "in fact , it is shown that the complexity of a learning problem is determined by the complexity of the label data distribution .",
    "this issue is discussed in detail in the following section .      based on the above discussion of the fundamental trade - off , to measure the complexity of data distribution , which is a topological concept , we employ the recent computational topology tool called _ persistent homology _",
    "@xcite .    in persistent homology , the topology of a space",
    "is inferred by investigating the change of multidimensional holes observed in different scales .",
    "hole is an important topological characteristic which is invariant in the same topological class  @xcite . here",
    ", zero - dimensional hole is a connected components , one - dimensional hole is a cycle and two - dimensional hole is a void .",
    "for example , in fig .",
    "[ fig : topology ] ( a ) , we can figure out that the topology of two spaces are different because the doughnut - like space ( @xmath36 ) has a hole ( i.e. a cycle ) which the ball - like space ( @xmath37 ) does not have .    in practice , however , it is hard to infer the global topology of the continuous space directly with a discrete set of observed data ( point clouds ) . as we differ the scale of observation by changing the distance measure @xmath38 , the corresponding topology of the observed data space may vary ( fig.[fig : topology ] ( a ) ) .",
    "in persistent homology , instead of using a single fixed @xmath38 , we investigate the space with entire @xmath38 s and find holes which persist long over the evolutionary change .",
    "this process is called a _",
    "filtration_. a hole which persists over varying scales is considered to be an important feature while the one with short persistence is considered as a topological noise .",
    "for example , a cycle which is an important feature of @xmath36 space will persist long while the three connected components at @xmath39 will disappear as allowable distance increases to @xmath40 ( fig .",
    "[ fig : topology ] ( a ) ) . on the other hand ,",
    "a connected component of the ball emerges early ( @xmath41 ) and persists long till the end of filtration ( @xmath42 )  @xcite .    during this filtration process @xcite",
    ", the number of @xmath43-dimensional holes of a manifold called betti numbers ( @xmath44 ) are calculated .",
    "specifically , @xmath45 represents the number of connected components .",
    "as @xmath36 has more diverged and complicated topology than @xmath37 , its point cloud merges slowly , which are reflected as a slow decrease in betti numbers .",
    "this trend is illustrated using so - called _ barcodes _  @xcite .",
    "as shown in fig.[fig : topology ] ( b ) , red barcodes which represent connected components of point clouds from @xmath37 topology quickly merges to a single cluster while the black barcodes still remain to be separated .    in the experimental results section",
    ", we will confirm that the prediction from the topological analysis with betti numbers fully reflects reconstruction performance by neural networks .",
    "more specifically , we will compare the topological complexity of the original and aliasing artifact image spaces by the change in the betti numbers , as shown in fig .",
    "[ fig : topology ] ( c ) .",
    "the result clearly show that the manifold of the original images is topologically more complex than those of the artifact images with two different sub - sampling patterns ( unifacs and random ) . here",
    ", unifacs represents the uniform sampling with auto - calibration signal ( acs ) on low frequency region .",
    "the points in artifact image manifold merge much earlier to a single cluster , which informs that the underlying manifold has a simpler topology than the original image space .",
    "the complexity of the artifact space is also varied by sampling patterns , as shown in fig.[fig : topology ] ( c ) .",
    "this persistent homology results are shown in accordance with the reconstruction performance , as will be shown later",
    ".      there have been several studies that explain the benefit of depth in neural networks @xcite . in deep networks , the representation power grows exponentially with respect to the number of layers , whereas it grows at most polynomially in shallow ones  @xcite .",
    "specifically , telgarsky @xcite derived the bound of representation power , which can not be overcome by shallow networks . therefore , with the same number of resources , theoretical results supports a deep architecture being preferred to a shallow one , and it increases the performance of the network by reducing the empirical risk in eq ..    another important components of representation power is determined by the receptive field size .",
    "the receptive field size is a crucial issue in many visual tasks , as the output must respond to large enough areas in the image to capture information about large objects . in the following ,",
    "we describe our multi - scale artifact learning network with a large receptive field .    specifically , as shown in fig .",
    "[ fig : network ] , the proposed artifact learning network consists of convolution layer , batch normalization  @xcite , rectified linear unit ( relu ) @xcite , and contracting path connection with concatenation  @xcite .",
    "more specifically , each stage contains four sequential layers composed of convolution with @xmath46 kernels , batch normalization and relu layers .",
    "finally , the last stage has two sequential layers and the last layer only contains a convolution layer with @xmath47 kernel . in the first half of the network , each stage is followed by a max pooling layer , while an unpooling layer is used in the later half of the network .",
    "scale - by - scale contracting paths are used to concatenate the results from the front part of the network to the later part of network .",
    "the number of channels for each convolution layer is shown in fig . [",
    "fig : network ] . note that the number of channels after each pooling layers is doubled .",
    "the resulting multi - scale network has five scales of representation as shown in fig .",
    "[ fig : network ] .",
    "the input resolution is 256@xmath48256 , and the resolution of representation is halved for each change of the scale until it becomes 16@xmath4816 in scale 4 . then , with the aid of contracting path with concatenation ( the dotted line in fig .  [ fig : network ] )",
    ", each level of information is integrated serially from the low - resolution ( scale @xmath49 ) into high - resolution representation ( original resolution , scale 0 ) .",
    "the elements of the network are explained in more detail here . in the convolutional layer ,",
    "the weights of convolution play a key role to extract the features of the inputs .",
    "specifically , let @xmath50 denote the @xmath51-th layer input and @xmath52 , @xmath53 represent weights and bias of @xmath51-th convolution layer , respectively .",
    "then , the convolution layer operates as follows : @xmath54 where @xmath55 is 2d - discrete convolution operation and @xmath56 and @xmath53 have @xmath57 and @xmath58 dimension , respectively . here ,",
    "@xmath59 and @xmath60 are the number for input and output channels , respectively .",
    "after the convolution layer , we apply batch normalization ( bn )  @xcite and the rectified linear unit ( relu )  @xcite sequentially to construct a single block of operation , @xmath61 : @xmath62 where @xmath63 , and @xmath64 consists of convolution filter weights , @xmath65 , and scaling / shift parameters of batch normalization , @xmath66 . batch normalization has been widely used in cnn training by incorporating a normalization and scale - shift step before the input of non - linear layer .",
    "it stabilizes learning by normalizing the mini - batch of input to have proper mean and variance .",
    "it can significantly improve the performance by reducing the problem from poor initialization and helps gradient flow .",
    "relu is a necessary non - linear mapping which has an advantage for training the network by reducing the gradient vanishing problem@xcite .    the skipped connection or contracting path  @xcite is a tool to improve the performance of network by linking or bypassing the results of the previous layer to a downstream layer . as shown in fig .",
    "[ fig : network](a ) , the dotted line bypasses the result to a latter layer .",
    "there is a difference between skipped connection and contracting path with concatenation as follows : @xmath67 \\quad \\quad\\quad \\text{(contracting path with concatenation ) }   \\end{cases}\\end{aligned}\\ ] ] more specifically , in the contracting path @xmath68 , the input of @xmath51-th layer ( @xmath69 ) skips over @xmath49 layers and is concatenated to the output of @xmath70-th layer ( @xmath71 ) . on the other hand",
    ", @xmath69 is added to the output @xmath71 in the skipped connection  @xcite .",
    "the pooling layer reduces the spatial size of the representation to reduce the resolution in the network .",
    "it is common to use the max pooling layer , and we chose both pooling size and stride as @xmath72 .",
    "then , the @xmath73-th element of output from the pooling layer operates as follows : @xmath74 where @xmath75 are the row and column size of the input representation and @xmath76 is the patch extractor of @xmath73-th element . the patch extractor is defined as : @xmath77 after the pooling layer , the resolution of representation for both x - y dimension is halved . in fig .",
    "[ fig : network ] , we denote the change of resolution as a level of scale .",
    "the unpooling layer is a transpose operation of the pooling layer by upsampling the input with the rate of two . by utilizing the pooling layer",
    ", we can increase the receptive field of the proposed network more efficiently with the same number of convolution .",
    "this will be discussed in detail later .",
    "considering that the aliasing artifact has globally distributed pattern , the enlarged receptive field from the multi - scale artifact learning is more advantageous for removing the aliasing artifacts .",
    "we used brain mr image dataset consisting of total 81 axial brain images of nine subjects .",
    "the data were acquired in cartesian coordinate with a 3 t mr scanner with four rx coils ( siemens , verio ) .",
    "the following parameters were used for se and gre scans : tr 3000 - 4000ms , te 4 - 20ms , slice thickness 5 mm , 256@xmath48 256 acquisition matrix , four coils , fov 240@xmath48 240 , fa 90 degrees .",
    "the brain images in the data set have different scales of intensity and maximum values since they were acquired with various scan conditions ( gre / se , various te / tr , etc . ) .",
    "therefore , the data should be normalized for better performance before entering the network .",
    "we normalized the data individually to have the same maximum value of 256 .",
    "for single - channel experiments , we selected the first coil data from the four coil data . for parallel imaging experiments , we used all the coil images .",
    "we split the training and test data by randomly selecting 66 images for training and 15 images for testing .",
    "the original k - spaces were retrospectively down - sampled .",
    "there are several ways of k - space under - sampling to speed up the 2d mr acquisition .",
    "the irregular and regular samplings along the phase encoding direction are the examples .",
    "recall that our objective is to train the network to learn the aliasing artifacts .",
    "as will be shown later , in contrast to the cs - mri , it is easier to learn aliasing artifact from regular sampling patterns with a few low - frequency k - space data . here",
    ", additional low - frequency auto - calibration signal ( acs ) lines are necessary to compose the aliasing artifacts mainly from high frequency edge signals rather than low - frequency image repetitions .",
    "so we chose the regular sampling pattern with auto - calibration signal ( acs ) lines for down - sampling .",
    "in particular , this sampling pattern is a common sampling pattern for grappa .",
    "thus , it is not necessary to perform additional pulse sequence programming .",
    "specifically , the k - space data were retrospectively subsampled by a factor of four with 13 acs lines ( 5 percent of total phase encoding lines ) in the k - space center .    in order to make the network more robust",
    ", data augmentation is essential when only a few training data are available . in order to produce the augmented mr images ,",
    "the original full - sampled images are transformed by rotation , shearing and flipping .",
    "the transforms were performed on complex domain so that we can acquire the full and down- sampled k - space data of the augmented mr images . by applying the aforementioned transforms , 32 times more training samples were generated for data augmentation .",
    "we have trained the two artifact networks : one for the magnitude and the other for the phase . more specifically , by applying the inverse fourier transform , we first generated the aliased images as inputs to the network .",
    "then , for the artifact network to reconstruct magnitude images , the inputs of the network were the magnitude of distorted mr images and labels were the magnitude of the aliasing artifact - only images as shown in fig .",
    "[ fig : concept ] ( c ) .",
    "the artifact network for the phase reconstruction was similarly trained .",
    "the inputs and labels are the phases of the distorted images and the aliasing artifact - only images , respectively .",
    "both networks have the same structure .",
    "however , due to the property of phase image , there is an additional step for the phase reconstruction network . as shown in the phase image of fig .",
    "[ fig : recon_flow ] ( bottom row ) , the region within the brain has smooth structures and the values of pixels vary slowly from @xmath78 to @xmath79 .",
    "while the area outside of the brain has approximately zero in the magnitude images , they have large fluctuation in the phase images because these phases have random - like values of @xmath78 to @xmath79 .",
    "these random phases outside of the brain region make the network train more difficult .",
    "to improve the performance of the network for phase reconstruction , we used the phase masking to remove the effect of that random phases outside of the brain .",
    "specifically , we first trained the magnitude network to get the reconstructed magnitude images .",
    "then the phase masks were obtained from the reconstructed magnitude images using a simple thresholding . using the phase mask ,",
    "we can remove the effects of random - phases in the outside of the brain by zeroing out the outside of the roi in both the input and the artifact phase images .",
    "then , the phase network is trained by assigning the artifact phase data within phase mask as labels .",
    "after training the two networks , the reconstruction flow follows the same steps with the training process as in fig .",
    "[ fig : recon_flow ] .",
    "since the generation of phase masks should be prior to the phase reconstruction , the magnitude of the mr images is first reconstructed by the magnitude network ( fig .",
    "[ fig : recon_flow ] top row ) .",
    "then , the estimated aliasing artifact are subtracted from the distorted input images to generate the final reconstructed images . after the reconstruction of the magnitude images , the phase mask is generated by comparing the pixels of the reconstructed magnitude image to a threshold value .",
    "the outside of the aliased brain image is erased with the phase mask to remove the random phases .",
    "then , the artifact of phase image is reconstructed by the proposed phase network and we could complete the final phase reconstruction by subtracting the artifact from the input phase image ( fig .",
    "[ fig : recon_flow ] bottom row ) .",
    "the network was implemented with the matconvnet toolbox ( ver.20 )  @xcite in the matlab 2015a environment ( mathworks , natick ) .",
    "we used a gtx 1080 graphic processor and i7 - 4770 cpu ( 3.40ghz ) .",
    "the weights of the convolutional layers were initialized by gaussian random distribution with xavier method@xcite to obtain a correct scale .",
    "this has helped us avoid the signal exploiding or vanishing in the initial phase of learning .",
    "the stochastic gradient descent ( sgd ) method with the momentum was used to train the weights of the network and minimize the loss function .",
    "the learning rate was reduced logarithmically from @xmath80 to @xmath81 per epoch .",
    "the size of the mini - batch was set to three , which is the maximum number for the given hardware specification .",
    "it took about 13 hours for training the magnitude network and 9 hours for training the phase network .",
    "we took the square root of sum of squares ( ssos ) on output magnitude images for final reconstruction , and the ssos of the magnitude images of full k - spaces data were used as the ground - truth . for the phase images ,",
    "the phase images from full k - space data were similarly used as the ground - truth .",
    "the reconstruction performance was measured by the normalized mean square error ( nmse ) .      to verify the performance of the network , we used the aloha  @xcite reconstruction as the state - of - the - art cs algorithm for both single- and multi - channel reconstruction .",
    "we also compared the reconstruction results for multi - channel dataset with those of grappa  @xcite .",
    "we have also compared the performance of proposed network with different types of deep networks . specifically , we compared the three learning architectures : ( 1 ) image learning with a multi - scale network , ( 2 ) artifact learning with a single - scale network and ( 3 ) the proposed artifact learning with a multi - scale network ( fig .",
    "[ fig : network](a ) ) to confirm the importance of artifact learning in a multi - scale manner .",
    "the main difference in the network architecture between single and multi - scale is the use of pooling and unpooling . in the multi - scale network ,",
    "both x - y resolutions of input are halved and the number of channels is doubled after each pooling layer ( red arrow in fig .  [",
    "fig : network ] ( a ) ) . in the single - scale learning",
    ", there were no pooling and unpooling layer , so the same image resolution with the filter depth of 64 channels were used for all layers .",
    "the image learning with a multi - scale network could be implemented by simply changing the labels from the artifact images to the original images .",
    "the performance of the proposed network was first compared with that of aloha and grappa .",
    "the magnitude reconstruction results are displayed in fig .  [",
    "fig : res_mag ] .    in the single channel experiment ( fig .",
    "[ fig : res_mag](a ) ) , there was a significant amount of aliasing artifacts from the zero - filled reconstruction .",
    "the difference images show these aliasing artifacts more clearly .",
    "since the k - space is uniformly down - sampled , the coherent aliasing artifact appeared . however , due to the additional acs lines , the coherent aliasing artifact are mainly for the edge images .",
    "the result of grappa shows a small improvement , but the most important aliasing artifacts have remained because we have only used one coil image .",
    "furthermore , for compressed sensing algorithms , it was difficult to remove the aliasing artifacts clearly since compressed sensing is designed for incoherent sampling and artifacts .",
    "most of the existing cs algorithms failed , but only aloha was somewhat successful with some remaining aliasing artifact .",
    "the reconstruction image of aloha is better than the zero - filled image and the reconstruction image by grappa visually and quantitatively .",
    "however , the result was still blurry and the aliasing artifacts were remained in the reconstructed image .",
    "in contrast , the proposed artifact learning algorithm clearly showed accurate reconstruction by removing the coherent aliasing artifacts .",
    "as shown in the error image of the proposed method ( fig .",
    "[ fig : res_mag ] ) , the aliasing artifacts are effectively reduced and the nmse value is minimal compared to the reconstruction results of the above - mentioned methods .    for parallel imaging experiments with four channel data ( fig .",
    "[ fig : res_mag ] ( b ) ) , the zero - filled reconstruction images have severe aliasing artifacts and blurred details , but all multi - channel reconstruction showed improvements compared to the single channel experiments . the grappa reconstruction shows a better result compared to the single channel reconstruction , but still has many reconstruction errors .",
    "as shown in the error image , the reconstruction of grappa still shows remaining aliasings and the noise - like high frequency errors .",
    "aloha reconstruction was able to remove most of the aliasing artifacts , but the results were not perfect due to the coherent sampling .",
    "however , the proposed method provided a great reconstruction results as seen in the fig .",
    "[ fig : res_mag ] ( b ) .    the phase reconstruction results from @xmath484 acceleration",
    "are shown in fig .",
    "[ fig : res_ang ] .",
    "the phase images of the zero - filled reconstruction show strong aliasing artifacts on each coil image .",
    "the phase images are usually smooth and they could have discontinuity from @xmath78 to @xmath79 due to the phase wrapping . although grappa utilizes the multi - coil data , grappa shows poor reconstruction results .",
    "the aliasing artifacts are retained and the high frequency error has been enhanced in the vicinity of the discontinuous regions . and",
    "it results in large nmse values for each coil image .",
    "the phase reconstructions of aloha and the proposed method have little aliasing artifacts for all coil images . compared to aloha",
    ", the proposed method resulted in minimal errors for each coil image .",
    "even when there is a large signal jump due to the phase wrapping , the proposed network effectively removed the aliasing artifacts .    compared to grappa and aloha , which are the representitive algorithms in parallel imaging and cs , the proposed method shows better results for both magnitude and phase reconstruction .",
    "the reconstruction images using grappa have heavy aliasing artifacts and the enhancement of high frequency errors ( see fig .  [",
    "fig : res_mag ] and  [ fig : res_ang ] ) .",
    "this type of imperfect grappa reconstruction usually occurs when the grappa kernel is incorrectly estimated due to the insufficient number of coils and acs lines .",
    "although aloha was developed to reconsturct the k - space from irregular sampling pattern , aloha somehow reconstructs the images from uniform subsampled k - space data , but they still have strong aliasing artifacts .    fig .",
    "[ fig : res_net ] shows the reconstruction results of three different networks : ( 1 ) multi - scale image learning , ( 2 ) single - scale artifact learning and ( 3 ) the proposed multi - scale artifact learning .",
    "these three networks showed much improved reconstruction results compared to grappa and aloha in both single- and multi - channel data . in the single channel results , the proposed network showed a minimum nmse value .",
    "similarly , the proposed network showed a significant improvement in the multi - channel result ( fig .",
    "[ fig : res_net ] ( b ) ) .",
    "the multi - scale image learning removes the aliasing artifacts , but images are too blurry , causing the large errors on the reconstruction result .",
    "single - scale artifact learning showed much more improved result , which is better than that of the multi - scale image learning .",
    "however , the performance of the proposed artifact learning was the best .",
    "we also compared the networks for image learning and artifact learning with the convergence plots for the test dataset ( fig .",
    "[ fig : error_graph ] ) . in the magnitude reconstruction for single channel ( fig .",
    "[ fig : error_graph ] ( a ) ) , the two deep networks showed better performance than aloha ( yellow dashed ) and grappa ( black dashed ) . between the deep networks ,",
    "the nmse of the proposed network ( red ) converged with a minimal error compared to that of the image learning ( blue ) .",
    "we found similar results for the magnitude reconstruction of multi - channel data ( fig .",
    "[ fig : error_graph ] ( b ) ) . here , both deep networks showed better performance compared to the grappa and aloha .",
    "and the proposed muiti - scale artifact learning showed the best reconstruction performance . compared with the image learning ,",
    "the proposed artifact learning showed better results for both single and multi - channel reconstruction as shown in fig .",
    "[ fig : error_graph ] ( a ) and ( b ) .",
    "this strongly suggests that it is better for the network to learn the artifact pattern itself than to learn the original image . in fig .",
    "[ fig : error_graph2](a ) , we compared the two sampling patterns : uniform sampling with acs lines and gaussian random sampling .",
    "as shown in fig .",
    "[ fig : error_graph2 ] ( a ) , the artifact learning using random sampling has faster convergence and some improvement compared to the original image learning .",
    "however , the use of uniform sampling pattern with acs in artifact learning shows improvement compared to the others .    in fig .",
    "[ fig : error_graph2 ] ( c ) and ( d ) , the comparison of single - scale artifact learning and multi - scale artifact learning are also given .",
    "for both reconstructions of magnitude and phase images , the proposed multi - scale network shows much better reconstruction performance . in particular , for the phase reconstruction , the reconstruction performance of the multi - scale network is much more improved compared to the single - scale one .",
    "the reconstruction time of grappa was about 30 seconds for multi - channel data and about 5 seconds for single - channel data under the aforementioned hardware setting .",
    "the reconstruction time for aloha was about 10 min for four channel data and about 2 min for single channel data .",
    "the proposed network required less than 41 ms for a multi - channel image and about 30 ms for a single - channel image . in the case of phase reconstruction",
    ", it takes about 61 ms for each coil since it is necessary to run the amplitude network first to obtain the phase mask .",
    "since the reconstructions of the individual coil images can be computed in parallel , the total reconstruction time was about 61 ms .",
    "even when all coil images are serially reconstructed , the total time for the four phase images was less than 250 ms , which is much shorter than the reconstruction time of the existing cs reconstruction algorithms .",
    "to support our claim that the simpler data manifold is better for a deep learning , we analyzed the topology of the data manifold using persistent homology .",
    "a persistent homology analysis between the original image data and the aliasing artifact data was performed on the following datasets : ( 1 ) magnitude images of single channel data , ( 2 ) magnitude images of multi - channel data , and ( 3 ) phase images of single - channel data .    in fig .",
    "[ fig : error_graph ] ( b ) , the zero - dimensional barcodes of the artifact data ( red ) fell faster than the original image data ( blue ) .",
    "this means that the aliasing artifact has a simpler manifold .",
    "the simpler manifold resulted in a better performance as shown in the left graph of fig .",
    "[ fig : error_graph ] ( a ) .",
    "the errors ( red graph ) for artifact learning decreased significantly faster than the errors of the image learning ( blue graph ) . in the case of image learning ,",
    "many fluctuations were shown during the initial phase of learning , and it converged slowly .",
    "these large fluctuations became smaller in the final phase of the learning , but the resulting errors were still larger compared to the other convergence plots of artifact learnings .",
    "a similar correspondence between the convergence plot and the persistent homology analysis was found in the multi - channel data ( fig .",
    "[ fig : error_graph ] ( d ) ) .",
    "the zeroth barcode of multi - channel data indicates that the multi - channel image data is more complex than the artifact data .",
    "as predicted by the persistent homology analysis , artifact learning converged quickly to a smaller value than the image learning ( fig .",
    "[ fig : error_graph ] ( c ) ) .",
    "the results from the phase image data strongly support a persistent homology analysis as a suitable tool for deep network design .",
    "as can see in the barcode graph of fig .",
    "[ fig : error_graph ] ( f ) , the difference between the barcodes of the artifact and the image data is very small .",
    "this suggests that their topological complexity is similar .",
    "this prediction agreed with the convergence plot in fig .",
    "[ fig : error_graph ] ( e ) , which shows that their nmse convergence plots are similar to each other although there is a slight improvement in the artifact learning .      by using the same network structure , we compared the reconstruction performance between the use of unfiromly down - sampled data and gaussian random down - sampled data .",
    "previously , using a single coil data , we briefly showed that the artifact from the gaussian sample has a more complex topological diversity than that of the uniform downsampling with acs lines while it has a simpler topological manifold than that of the original images ( fig .",
    "[ fig : topology ] ( c ) ) . in fig .",
    "[ fig : error_graph2 ] ( b ) , we have also compared the complexity of the image and artifact manifolds for multi - channel data .",
    "the uniform sampling with acs lines again brought a simpler manifold .",
    "this is consistent with the reconstruction results showing that the artifact - learning with the uniformly down - sampled data with the acs lines has produced much better performance than other data sets , as shown in fig .",
    "[ fig : error_graph2 ] ( a ) .",
    "it is believed that the regular repetition of the artifact in the uniform down - sampled data can help train cnn more effectively .      to illustrate the importance of the large receptive field , we compared the multi - scale and the single - scale artifact learning . fig .",
    "[ fig : rfield ] compares the variation of depth - wise receptive field for a simplified form of the single - scale network ( a ) and the proposed multi - scale network ( b ) . both the single - scale and the multi - scale network consist of 18 layers of 3@xmath483 convolution filters .",
    "accordingly , the size of the final receptive field in single - scale network was 37@xmath4837 , while the receptive field of multi - scale network fully covered the 256@xmath48256 size of inputs .",
    "the receptive field of the single - scale network increases linearly through the series of convolution layers . on the other hand , by using the pooling layers , the receptive field in the multi - scale network increases exponentially . as a result",
    ", the receptive field of the proposed network with the same number of convolution layers completely covers the entire input as opposed to the single - scale network .",
    "specifically , each blue square of the input in fig .",
    "[ fig : rfield ] is used to reconstruct the yellow square of the output , showing that the multi - scale network could more effectively learn the globally distributed artifact patterns .",
    "this is confirmed in fig .",
    "[ fig : error_graph2](c ) and ( d ) , where multi - scale approaches gives the minimal errors .",
    "the advantage of multi - scale reconstruction was much clearer in phase reconstruction ( fig .",
    "[ fig : error_graph2 ] ( d ) ) .",
    "the single - scale learning of the phase showed a poor reconstruction result , while multi - scale learnings showed smaller error in the phase reconstruction .",
    "since the phase images are smooth and slowly changing , the bulk of the artifact comes from a globally distributed pattern instead of local one .",
    "this property of phase images is suitable for the application of the multi - scale network , which leads to a great performance improvement in the phase reconstruction ( fig .",
    "[ fig : error_graph2 ] ( d ) ) .",
    "this agrees with our earlier works on the x - ray ct application of deep learning @xcite .",
    "multi - scale network is more effective in eliminating globally distributed streaking artifacts from sparse projection views @xcite , while the single - scale network is better for removing locally distributed noise from low - dose ct @xcite .",
    "because the aliasing artifacts in compressed sensing mri is more globally distributed , the multi - scale network is more effective .",
    "this paper proposed a deep artifact learning network for the reconstruction of mr images from accelerated mr acquisition . based on the observation that the aliasing artifacts from uniformly subsampled",
    "k - space data with additional ac lines at low frequency have a simpler data manifold compared to the other sampling patterns and the artifact - free images , our network was designed to learn the artifact patterns instead of artifact - free images .",
    "our experimental results confirmed that the performance of deep network depends on the topological complexity of the label data manifold . in order to cope with the globally distributed artifact patterns , the proposed network also utilized the multi - scale network structure called u - net having a large receptive field .",
    "we have also confirmed that the multi - scale approach with u - net architecture exhibited a better reconstruction performance than the single - scale network . in particular ,",
    "the advantage of the multi - stage network was more pronounced in phase reconstruction , where most of the artifacts are global aliasing patterns rather than localized errors .",
    "although the training of the network takes a long time , the training could only be carried out once by the manufacturer , so in real application scenario , network runs very fast and the reconstruction can be performed quickly at each scanner .",
    "the very short reconstruction time was one of the great advantages compared to the cs - based iteration reconstruction methods .",
    "in summary , the proposed method operated on not only with multi - channel data but also with single - channel data .",
    "even with strong coherent aliasing artifacts , the proposed artifact learning network has successfully learned the aliasing artifacts while the existing parallel and cs reconstruction method have not been able to remove the aliasing artifacts .",
    "the significant advantages of both computational time and the reconstruction quality suggest that the proposed deep artifact learning is a promising research direction for accelerated mri with great potential impact .",
    "this study was supported by korea science and engineering foundation under grant nrf-2016r1a2b3008104 .    10    pruessmann  kp , weiger  m , scheidegger  mb , boesiger  p et  al . . magnetic resonance in medicine 1999 ; 42:952962 .",
    "griswold  ma , jakob  pm , heidemann  rm , nittka  m , jellus  v , wang  j , kiefer  b , haase  a. generalized autocalibrating partially parallel acquisitions ( grappa ) .",
    "magnetic resonance in medicine 2002 ; 47:12021210 .",
    "donoho  dl .",
    "compressed sensing .",
    "ieee transactions on information theory 2006 ; 52:12891306 .",
    "lustig  m , donoho  dl , santos  jm , pauly  jm . compressed sensing mri .",
    "ieee signal processing magazine 2008 ; 25:7282 .",
    "jin  kh , lee  d , ye  jc . a general framework for compressed sensing and parallel mri using annihilating filter based low - rank hankel matrix .",
    "ieee trans . on computational imaging 2016 ; 2:480495 .",
    "lee  d , jin  kh , kim  ey , park  sh , ye  jc . . magnetic resonance in medicine 2016 ; 76:1848?1868 .",
    "lee  j , jin  kh , ye  jc . .",
    "magnetic resonance in medicine 2016 ; 76:1775?1789 .",
    "ye  jc , kim  jm , jin  kh , lee  k. compressive sampling using annihilating filter - based low - rank interpolation .",
    "ieee transactions on information theory 2017 ; 63:777801 .    krizhevsky  a , sutskever  i , hinton  ge . imagenet classification with deep convolutional neural networks",
    ". advances in neural information processing systems , in : advances in neural information processing systems , 2012 . pp .",
    ".    ronneberger  o , fischer  p , brox  t. u - net : convolutional networks for biomedical image segmentation .",
    "international conference on medical image computing and computer - assisted intervention , in : international conference on medical image computing and computer - assisted intervention , 2015 .",
    "234241 .",
    "mao  xj , shen  c , yang  yb .",
    "image denoising using very deep fully convolutional encoder - decoder networks with symmetric skip connections .",
    "arxiv preprint 2016 ; .",
    "zhang  k , zuo  w , chen  y , meng  d , zhang  l. beyond a gaussian denoiser : residual learning of deep cnn for image denoising .",
    "arxiv preprint arxiv:1608.03981 2016 ; .",
    "dong  c , loy  cc , he  k , tang  x. learning a deep convolutional network for image super - resolution .",
    "european conference on computer vision , in : european conference on computer vision , 2014 .",
    "184199 .",
    "vapnik  vn , vapnik  v , `` statistical learning theory '' , vol .  1 .",
    "wiley new york , 1998 .",
    "telgarsky  m. benefits of depth in neural networks .",
    "arxiv preprint arxiv:1602.04485 2016 ; .",
    "bianchini  m , scarselli  f. on the complexity of neural network classifiers : a comparison between shallow and deep architectures .",
    "ieee trans . on neural networks and",
    "learning systems 2014 ; 25:15531565 .",
    "kang  e , min  j , ye  jc . a deep convolutional neural network using directional wavelets for low - dose x - ray ct reconstruction .",
    "arxiv preprint arxiv:1610.09736 2016 ; .",
    "jin  kh , mccann  mt , froustey  e , unser  m. deep convolutional neural network for inverse problems in imaging .",
    "arxiv preprint arxiv:1611.03679 2016 ; .",
    "han  y , yoo  j , ye  jc . deep residual learning for compressed sensing ct reconstruction via persistent homology analysis .",
    "arxiv preprint arxiv:1611.06391 2016 ; .",
    "wang  s , su  z , ying  l , peng  x , zhu  s , liang  f , feng  d , liang  d. accelerating magnetic resonance imaging via deep learning .",
    "2016 ieee 13th international symposium on biomedical imaging ( isbi ) , in : 2016 ieee 13th international symposium on biomedical imaging ( isbi ) , 2016 .",
    "514517 .",
    "hammernik  k , knoll  f , sodickson  d , pock  t. learning a variational model for compressed sensing mri reconstruction .",
    "proceedings of the international society of magnetic resonance in medicine ( ismrm ) , in : proceedings of the international society of magnetic resonance in medicine ( ismrm ) , 2016 .",
    "chen  y , yu  w , pock  t. on learning optimized reaction diffusion processes for effective image restoration .",
    "proceedings of the ieee conference on computer vision and pattern recognition , in : proceedings of the ieee conference on computer vision and pattern recognition , 2015 . pp .",
    "52615269 .",
    "chen  y , pock  t. trainable nonlinear reaction diffusion : a flexible framework for fast and effective image restoration .",
    "arxiv preprint arxiv:1508.02848 2015 ; .",
    "noh  h , hong  s , han  b. learning deconvolution network for semantic segmentation .",
    "proceedings of the ieee international conference on computer vision , in : proceedings of the ieee international conference on computer vision , 2015 . pp .",
    "15201528 .",
    "he  k , zhang  x , ren  s , sun  j. deep residual learning for image recognition .",
    "arxiv preprint arxiv:1512.03385 2015 ; .",
    "lustig  m , pauly  jm . .",
    "magnetic resonance in medicine 2010 ; 64:457471 .",
    "bartlett  pl , mendelson  s. rademacher and gaussian complexities : risk bounds and structural results . journal of machine learning research 2002 ; 3:463482 .",
    "edelsbrunner  h , harer  j. persistent homology - a survey .",
    "contemporary mathematics 2008 ; 453:257282 .",
    "ioffe  s , szegedy  c. batch normalization : accelerating deep network training by reducing internal covariate shift .",
    "arxiv preprint arxiv:1502.03167 2015 ; .",
    "glorot  x , bengio  y. understanding the difficulty of training deep feedforward neural networks .",
    "aistats , in : aistats , 2010 .",
    "249256 .    vedaldi  a , lenc  k. matconvnet : convolutional neural networks for matlab .",
    "proceedings of the 23rd acm international conference on multimedia , in : proceedings of the 23rd acm international conference on multimedia , 2015 . pp .",
    "4 with 5 percents of acs lines .",
    "the reconstruction results and its error images are shown in the first and second rows , respectively .",
    "the error is @xmath4810 amplified for better visualization and nmse values are displayed on the top of each reconstructed image .",
    ", width=604 ]     with 5 percent of the acs lines .",
    "the reconstruction results of grappa , aloha and proposed network are shown in the third , fourth and fifth column .",
    "the nmse values are displayed at the top of each image .",
    ", width=604 ]"
  ],
  "abstract_text": [
    "<S> * purpose : * compressed sensing mri ( cs - mri ) from single and parallel coils is one of the powerful ways to reduce the scan time of mr imaging with performance guarantee . </S>",
    "<S> however , the computational costs are usually expensive . </S>",
    "<S> this paper aims to propose a computationally fast and accurate deep learning algorithm for the reconstruction of mr images from highly down - sampled k - space data . </S>",
    "<S> + * theory : * based on the topological analysis , we show that the data manifold of the aliasing artifact is easier to learn from a uniform subsampling pattern with additional low - frequency k - space data . </S>",
    "<S> thus , we develop deep aliasing artifact learning networks for the magnitude and phase images to estimate and remove the aliasing artifacts from highly accelerated mr acquisition . </S>",
    "<S> + * methods : * the aliasing artifacts are directly estimated from the distorted magnitude and phase images reconstructed from subsampled k - space data so that we can get an aliasing - free images by subtracting the estimated aliasing artifact from corrupted inputs . moreover , to deal with the globally distributed aliasing artifact , we develop a multi - scale deep neural network with a large receptive field . </S>",
    "<S> + * results : * the experimental results confirm that the proposed deep artifact learning network effectively estimates and removes the aliasing artifacts . compared to existing cs methods from single and multi - coli data , the proposed network shows minimal errors by removing the coherent aliasing artifacts . </S>",
    "<S> furthermore , the computational time is by order of magnitude faster . </S>",
    "<S> + * conclusion : * as the proposed deep artifact learning network immediately generates accurate reconstruction , it has great potential for clinical applications . </S>",
    "<S> +    keywords : deep learning , artifact learning , convolutional neural network , compressed sensing , parallel imaging , topological data analysis , persistent homology    deep artifact learning for compressed sensing and parallel mri    dongwook lee@xmath0 , jaejun yoo@xmath1 and jong chul ye@xmath2 +    _ @xmath1dept . of bio and brain engineering + korea advanced institute of science & technology ( kaist ) + 373 - 1 guseong - dong yuseong - gu , daejon 305 - 701 , republic of korea + email : jong.ye@kaist.ac.kr + _     running head : deep aliasing artifact learning for cs and parallel mri + journal : magnetic resonance in medicine +    @xmath3 correspondence to : + jong chul ye , ph.d .   </S>",
    "<S> + professor + dept . of bio and brain engineering , kaist + 373 - 1 guseong - dong yuseong - gu , daejon 305 - 701 , korea + email : jong.ye@kaist.ac.kr + tel : 82 - 42 - 350 - 4320 + fax : 82 - 42 - 350 - 4310 +   +   + total word count : approximately 5000 words . </S>"
  ]
}