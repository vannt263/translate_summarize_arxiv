{
  "article_text": [
    "theories of planet - disk interactions have considerably evolved in the last two decades .",
    "a significant part of their progress is attributable to numerical experiments . in many of these ,",
    "the role played by the different parameters of the problem has been deciphered by systematic , computationally expensive explorations of the parameter space .",
    "these could be achieved in part thanks to the steadily increasing power of computational resources , and in part by the development of fast algorithms adapted to the specifics of gas motion in thin , nearly keplerian disks , known as orbital advection algorithms @xcite .",
    "although two - dimensional ( 2d ) calculations are still a valuable tool in tackling specific problems linked to planet - disk interactions , most of the latest results in this field have been obtained through three - dimensional calculations . in problems involving magnetic field tied to the matter",
    ", the prevalence of the magneto - rotational instability , which requires resolving the vertical dimension across the disk , implies that most studies of planet - disks interactions must be tackled through expensive , three dimensional calculations , with only a few notable exceptions .",
    "the development of the cuda language , aimed at tapping the huge computational resources of _ graphical processing units _ ( gpus ) for general purpose processing , commonly known as gpgpu computing , allows the development of astrophysical computational codes which can tackle expensive problems at relatively moderate cost .",
    "we can adopt as a rule of thumb that the same code runs between one and two orders of magnitude faster on a given high profile gpu than on a high profile cpu core of same generation .",
    "this ratio turns out to be of same order of magnitude as the vertical number of zones in many three - dimensional ( 3d ) studies .",
    "to put it simply , gpus put 3d calculations at the cost of 2d calculations on a cpu core .    with this in mind",
    ", we have developed a new code that solves the equations of hydrodynamics ( hd ) or magnetohydrodynamics ( mhd ) on a mesh ( either cartesian , cylindrical or spherical ) with special emphasis on describing protoplanetary disks and their interactions with forming planets .    while there is a plethora of astrophysical fluid dynamics codes",
    ", there are a lot fewer that can run on gpus .",
    "one of them is the public adaptive mesh refinement ( amr ) code enzo @xcite , which contains hard coded cuda versions of the ppm and mhd solvers . like enzo ,",
    "the gamer code @xcite is another amr code essentially oriented toward cosmological simulations , which contains a hydrodynamic solver coded in cuda .",
    "another one is the hydrodynamical code cholla @xcite , which is based on high order godunov s methods , and contains hard coded cuda kernels .",
    "another one is the penguin code @xcite , which is a lagrangian , dimensionally split , shock capturing hydrocode that uses the parabolic piecewise method ( ppm , * ? ? ?",
    "it is written in cuda and  c. as memory is a significant concern in cosmological simulations , and since the standard ram available on a cluster node is generally much larger than the ram on board of the gpus , enzo and gamer constantly transfer back and forth data between the cpu and gpu to benefit at the same time from the large memory of the host ( cpu ) and the large computational throughput of the gpu .",
    "since these transfers constitute a bottleneck for effective gpu performance , the gamer code uses sophisticated asynchronous data transfers in order to hide their latency .",
    "our needs are very different : in the domain of predilection of fargo3d , that of protoplanetary disks and their interactions with embedded planets , speed is generally much more a concern than memory .",
    "we therefore require that our whole simulation fits on the memory of the gpu(s ) , in order to avoid data transfer .",
    "all our routines , including those dedicated to boundary conditions , run on the gpu .",
    "consequently , we have striven to obtain the smallest possible memory footprint per cell .    when the hd or mhd algorithms are directly coded in cuda",
    ", a user who wishes to amend the core routines , or who wishes to incorporate new physics into them must be proficient in cuda .",
    "we have adopted a radically different approach : we have developed all our code for cpu , and enforced the use of strict syntax rules in all computationally expensive routines which expose their parallelism , so that they can be translated automatically during compilation to cuda code .",
    "this allows users who have no prior knowledge of gpu programming to easily modify the code for their own needs .",
    "this also made the development of the code much faster and reliable .",
    "the scope of this paper is a comprehensive description of the algorithms used in our code ; we do not present the syntax rules that we have developed to enable the automatic translation , which can be found in the online manual .",
    "we nonetheless describe the conceptual ideas of this process in section  [ sec : building - gpus ] .",
    "the code that we have developed is based on ideas similar to those used in the zeus code @xcite . in this code , the velocities are not centered on the cells but staggered on their faces .",
    "this allows us to calculate easily the fluxes of mass , momenta and specific energy on the cell edges , by the use of upwind methods .",
    "zeus - like codes have oftentimes been referred to as finite difference codes , as opposed to the so - called finite volume codes .",
    "these , like those based on godunov s method that have emerged in astrophysics in the past two decades , in which riemann s problem is used to evaluate fluxes at the faces between cells , automatically ensure the conservation of the physical quantities for which a flux can be evaluated .",
    "this could suggest that finite difference codes like the one we present in this work do not have such conservation properties .",
    "in fact , a significant fraction of the sub steps performed over a full update correspond to finite volume operations .",
    "in particular , we shall see that fargo3d conserves mass and momenta to machine accuracy .",
    "we summarize hereafter the salient points of our implementation choices :    1 .",
    "our method is dimensionally split , thereby requiring as few as possible temporary arrays .",
    "our code has probably the smallest possible memory imprint , which is well suited to gpus , which have moderate amounts of random access memory ( ram ) compared to cpus .",
    "2 .   staggered mesh codes , contrary to godunov s methods , do not have issues with steady flows with sources terms . this bears some importance in many situations in protoplanetary disks , for which the vertical and rotational equilibrium of the unperturbed disk , as well as that of the envelope that appears around embedded planets , must be accurately captured by the method .",
    "the cost of a full time step with a staggered mesh code is significantly smaller than that of a godunov s method .",
    "codes based on godunov s method use generally fluxes of total energy , thus enforcing the conservation of energy to machine accuracy . on the other hand , staggered mesh codes consider the fluxes of internal energy , and do not fulfill the conservation of energy to machine accuracy .",
    "although this at first sight may seem a disadvantage , it is not so : in protoplanetary disks , which usually have a large mach number , the kinetic energy is two to three orders of magnitude larger than the internal energy .",
    "any truncation error affecting the kinetic energy is forcibly transferred to the internal energy budget , which compounds the relative error .",
    "this problem is known as the high mach number problem @xcite .",
    "many planet - disk interaction problems require an accurate advection of entropy in the planet s coorbital region .",
    "it is therefore desirable to deal with the internal energy separately .",
    "inclusion of new processes generally requires significantly more work with godunov s method , such as rewriting or significantly editing the riemann solver , whereas it is easier to incorporate new or different physics in staggered mesh codes through the operator splitting technique .",
    "although we have striven to make the process of translation to gpus as transparent and automatic as possible , the use of simple routines ensures that our code can be straightforwardly adapted to the user s needs , without requiring proficiency in gpu programming .",
    "the predecessor of our new code , fargo , has been intensively used in the field of planet - disk interactions , and proven to perform well over the whole range of planetary masses , from deeply embedded , low mass objects , which require finely tuned rotational equilibrium and accurate entropy and vortensity advection , to giant planets , which clear a gap and excite strong shocks in their immediate vicinity .",
    "although godunov s codes are better at handling shocks than staggered mesh codes , the latter have been found to yield similar results to the former in problems involving gap clearing and shocks excitation @xcite .    as does its predecessor ,",
    "the fargo3d code includes orbital advection .",
    "initially , fargo ( _ fast advection in rotating gaseous objects _ ) was an orbital advection algorithm @xcite .",
    "this acronym was then used to name the code originally based on this algorithm and publicly released in 2005 .",
    "the orbital advection has been extended to mhd in the present implementation , using the upstream averaged electric field method of @xcite .",
    "one of the main novelties introduced in fargo3d is that it can be run effortlessly on gpu platforms with a high computational throughput .",
    "this , together with the fact that it features orbital advection in its hd and mhd versions , makes it a tool of choice to study protoplanetary disk dynamics and planet - disk interactions , which are extremely demanding in terms of computational power .",
    "this paper is organized as follows : in section  [ sec : overview ] we present the main characteristics of the code , list the equations that it solves and introduce our notation . in section  [",
    "sect : methods ] we present a flow chart of the code and describe step - by - step the numerical methods used to solve the governing equations . in section  [ sec : impl - cons ] we discuss some implementation details , in particular our choice to export computationally expensive routines to the gpu . in section  [ sec : tests ] we present a set of tests on different problems in hd and mhd . in section  [ sec : discussion ] , we provide some discussion on the impact of orbital advection on the properties of the code , both in hd and mhd , and we also present potential pitfalls of single precision calculations in the context of astrophysical disks . finally , we discuss ongoing and future developments in section  [ sec : perspectives ] . throughout the manuscript",
    "the reader will find many superscripts referring to end notes .",
    "these refer to the specifics of our implementation and should significantly speed up the learning process of the reader interested in modifying the code .",
    "fargo3d solves the equations of hd or mhd on an eulerian mesh , which can be either cartesian , cylindrical or spherical .",
    "as its name indicates , it is designed to solve three - dimensional problems , but it can also be used in lower dimension ( one or two - dimensions ) .",
    "the continuity equation is : @xmath0 where @xmath1 is the volumic density , and @xmath2 is the velocity of the fluid with respect to the mesh .",
    "the navier - stokes equation reads : @xmath3\\rho\\nonumber,\\end{aligned}\\ ] ] where @xmath4 is the pressure , @xmath5 the magnetic field , and @xmath6 is any external force ( e.g. gravity force ) , and where the second term of the right hand side is dealt with only if mhd is included . the last term of eq .",
    "accounts for a possible rotation of the mesh about the vertical axis , at a rate @xmath7 which can vary with time .",
    "the third term features the stress tensor @xmath8 , which has the following expression : @xmath9 , \\label{eq : stresstensor}\\ ] ] where @xmath10 is the kinematic viscosity and @xmath11 is the unit tensor of same rank as the tensor @xmath12 . for the energy equation",
    ", we use as discussed in section  [ sect : introduction ] a non - conservative form using the volumic internal energy @xmath13 : @xmath14 finally , when mhd is included , we solve the induction equation , which reads : @xmath15 where @xmath16 is the ohmic diffusivity . the eqs .   to",
    "are closed using an equation of state , giving a relation between pressure or the internal energy . in the public release ,",
    "two forms for the equation of state are provided : @xmath17 and @xmath18 where respectively @xmath19 is the isothermal sound speed and @xmath20 the ratio of specific heats at constant pressure and volume . the first form given by eq .",
    "is often called _ ( locally ) isothermal equation of state_. its field @xmath21 is constant in time and given by the initial conditions .",
    "when this equation is used , eq .",
    "is decoupled from the others and does not have to be solved .",
    "the second form , often called _ adiabatic _ or _ ideal equation of state _ , is the relation between the internal energy and pressure .",
    "this form is commonly used when eq .",
    "has to be solved .",
    "the three different directions are generically named @xmath22 , @xmath23 and @xmath24 in our implementation , even if the mesh geometry is non - cartesian . tab .  [ tab : corresp ] lists the correspondence between these names and the corresponding coordinate in the different geometries .    in cartesian",
    "coordinates one may use the shearing sheet formalism to describe a sheared flow in a rotating frame , subject to the standard expansion of the effective ( centrifugal plus central ) potential .",
    "this will trigger the use of a different momentum component along the @xmath22 direction ( see tab .",
    "[ tab : momenta ] ) and the inclusion of specific source terms ( see section  [ sec : source - step ] ) .",
    "the frame rotating rate is not allowed to vary in the shearing sheet case .",
    "[ cols=\"<,^,^,^\",options=\"header \" , ]     pablo bentez - llambay acknowledges financial support from conicet and the computational resources provided by iate and ccad ( universidad nacional de crdoba ) .",
    "f. masset acknowledges support from conacyt grant  178377 .",
    "+ we thank sbastien fromang for discussions and guidance during the development of the mhd solver , and for comments on an early version of this work .",
    "we thank gloria koenigsberger for a thorough reading of this manuscript , and constructive comments .",
    "we thank ulises amaya olvera , reyes garc a carren and jrme verleyen for their assistance in setting up the gpu cluster on which most of the calculations presented here have been run .",
    "during a full update on the mesh , one requires at several places the volume of a zone , the surface of all its faces , and the length of its edges . in order to save memory ( which is always a concern on gpus ) without sacrificing speed ,",
    "we define a number of one - dimensional arrays , and use products of these arrays to obtain the face surfaces or the zone volumes .",
    "our aim is that on relatively modest setups , these one - dimensional arrays may fit in the so - called _ constant memory _ , which is cached ( fast ) read - only memory on board the graphics card . in fargo3d",
    "the cell size is uniform in @xmath22 ( hence azimuth in cylindrical or spherical geometries ) .",
    "this is a limitation imposed by the ( nearly systematic ) use of orbital advection along this direction .",
    "we have the relationships : @xmath25 where the coefficients @xmath26 are defined in tab .  [",
    "tab : geom ] , and where @xmath27 is the surface of the lower face of cell @xmath28 perpendicular to direction @xmath29 ( the value of @xmath30 is unimportant for the reason given above , regardless of the geometry ) .      as we more often use the inverse of the volume of a zone than its mere value , and since divisions have a high computational cost , we prefer to have an expression that directly gives us the inverse of a cell volume .",
    "we use : @xmath31 where @xmath32 is the cell volume , and where the coefficients @xmath33 are defined in tab .  [",
    "tab : geom ] .",
    "we leave as an exercise to the reader to check that eqs .   and",
    "lead to the exact surfaces and ( inverse ) volumes in all cases .",
    "we provide hereafter the expression of its components in the three geometries ( see e.g. * ? ? ?",
    "* ) , the different terms being written exactly as implemented .",
    "cartesian coordinates : : :    @xmath34 cylindrical coordinates : : :    @xmath35\\\\\\nonumber        \\tau_{rr}&=&-\\rho\\nu\\left(2\\partial_rv_r-\\frac 23\\nabla\\cdot\\vec{v}\\right)\\\\\\nonumber        \\tau_{zz}&=&-\\rho\\nu\\left(2\\partial_zv_z-\\frac 23\\nabla\\cdot\\vec{v}\\right)\\\\\\nonumber        \\tau_{\\phi r}=\\tau_{r\\phi}&=&-\\rho\\nu\\left(\\partial_rv_\\phi-\\frac{v_\\phi}{r}+\\frac 1r\\partial_\\phi v_r\\right)\\\\\\nonumber        \\tau_{rz}=\\tau_{zr}&=&-\\rho\\nu\\left(\\partial_rv_z+\\partial_zv_r\\right)\\\\\\nonumber        \\tau_{\\phi z}=\\tau_{z\\phi}&=&-\\rho\\nu\\left(\\partial_zv_\\phi+\\frac 1r\\partial_\\phi v_z\\right ) .",
    "\\nonumber      \\end{aligned}\\ ] ] spherical coordinates : : :    @xmath36\\\\\\nonumber        \\tau_{\\theta\\theta}&=&-\\rho\\nu\\left[2\\left(\\frac 1r\\partial_\\theta v_\\theta+\\frac{v_r}{r}\\right)-\\frac 23\\nabla\\cdot\\vec{v}\\right]\\\\\\nonumber        \\tau_{\\phi r}=\\tau_{r\\phi}&=&-\\rho\\nu\\left(\\frac{1}{r\\sin\\theta}\\partial_\\phi v_r+\\partial_rv_\\phi-\\frac{v_\\phi}{r}\\right)\\\\\\nonumber        \\tau_{r\\theta}=\\tau_{\\theta r}&=&-\\rho\\nu\\left(\\partial_rv_\\theta-\\frac{v_\\theta}{r}+\\frac 1r\\partial_\\theta v_r\\right)\\\\\\nonumber        \\tau_{\\phi\\theta}=\\tau_{\\theta\\phi}&=&-\\rho\\nu\\left[\\frac{\\sin\\theta}{r}\\partial_\\theta\\left(\\frac{v_\\phi}{\\sin\\theta}\\right)+\\frac{1}{r\\sin\\theta}\\partial_\\phi v_\\theta\\right ] .",
    "\\end{aligned}\\ ] ]    the different components of the stress tensor are located at different positions .",
    "the diagonal terms are zone centered , whereas the cross terms are defined at the middle of the edges . for instance , inside a given cell , @xmath37 is defined at the middle of the edge along the @xmath38 dimension which has the lowest value for @xmath39 and @xmath40 . by circular permutation of the indices , the centering of other components ensues .",
    "the position of the different components is depicted in fig .",
    "[ fig : stress ] .",
    "the different components of the stress tensor , once calculated , are used to update directly the components of the velocity .",
    "this corresponds to cell  7d of fig .",
    "[ fig : flowchart ] .",
    "we list hereafter the partial derivative equations that correspond to this substep , in the three geometries .",
    "cartesian coordinates : : :    @xmath41 cylindrical coordinates : : :    @xmath42\\\\\\nonumber        \\partial_tv_r&=&-\\frac{1}{\\rho}\\left[\\frac 1r\\partial_r(r\\tau_{rr})+\\frac 1r\\partial_\\phi\\tau_{r\\phi}-\\frac{\\tau_{\\phi\\phi}}{r}+\\partial_z\\tau_{rz}\\right]\\\\\\nonumber        \\partial_tv_z&=&-\\frac{1}{\\rho}\\left[\\frac 1r\\partial_r(r\\tau_{rz})+\\frac 1r\\partial_\\phi\\tau_{\\phi z}+\\partial_z\\tau_{zz}\\right].\\\\\\nonumber      \\end{aligned}\\ ] ] spherical coordinates : : :    @xmath43\\\\\\nonumber    \\partial_tv_r&=&-\\frac{1}{\\rho}\\left[\\frac{1}{r^2}\\partial_r(r^2\\tau_{rr})+\\frac{1}{r\\sin\\theta}\\partial_\\theta(\\tau_{r\\theta}\\sin\\theta)\\right.\\\\\\nonumber    & & + \\left.\\frac{1}{r\\sin\\theta}\\partial_\\phi\\tau_{r\\phi}-\\frac{\\tau_{\\theta\\theta}+\\tau_{\\phi\\phi}}{r}\\right]\\\\\\nonumber        \\partial_tv_\\theta&=&-\\frac{1}{\\rho}\\left[\\frac{1}{r^3}\\partial_r(r^3\\tau_{r\\theta})+\\frac{1}{r\\sin\\theta}\\partial_\\theta(\\tau_{\\theta\\theta}\\sin\\theta)\\right.\\\\\\nonumber     & & + \\left.\\frac{1}{r\\sin\\theta}\\partial_\\phi\\tau_{\\theta\\phi}-\\frac{\\tau_{\\phi\\phi}\\cot\\theta}{r}\\right],\\\\\\nonumber       \\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> we present the fargo3d code , recently publicly released . </S>",
    "<S> it is a magnetohydrodynamics code developed with special emphasis on protoplanetary disks physics and planet - disk interactions , and parallelized with mpi . </S>",
    "<S> the hydrodynamics algorithms are based on finite difference upwind , dimensionally split methods . </S>",
    "<S> the magnetohydrodynamics algorithms consist of the constrained transport method to preserve the divergence - free property of the magnetic field to machine accuracy , coupled to a method of characteristics for the evaluation of electromotive forces and lorentz forces . </S>",
    "<S> orbital advection is implemented , and an n - body solver is included to simulate planets or stars interacting with the gas . </S>",
    "<S> we present our implementation in detail and present a number of widely known tests for comparison purposes . </S>",
    "<S> one strength of fargo3d is that it can run on both _ graphical processing units _ ( gpus ) or _ central processing unit _ ( cpus ) , achieving large speed up with respect to cpu cores . </S>",
    "<S> we describe our implementation choices , which allow a user with no prior knowledge of gpu programming to develop new routines for the cpu , and have them translated automatically for the gpu . </S>"
  ]
}