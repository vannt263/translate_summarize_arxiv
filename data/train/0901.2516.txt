{
  "article_text": [
    "quantum mechanics brings strange and wonderful features to the field of information theory .",
    "it introduces new information resources such as qubits with the power of superposition but also teasing restrictions such as the no - cloning theorem .",
    "we are interested in the possibility of the boosted transmission of classical information through a quantum channel with memory and no prior entanglement .",
    "great strides have been made in understanding the capacity of quantum channels .",
    "for example , the celebrated holevo - schumacher - westmoreland ( hsw ) theorem @xcite gives an expression for the classical capacity of a noisy memoryless quantum channel with product state inputs .",
    "the memoryless channel restriction has since been extended to , so called , _ forgetful _ memory channels @xcite .",
    "the inclusion of memory is the next step in the attempt of accurately modelling the complicated noise - correlated real world . now that these initial seeds of the theoretical framework are in place ,",
    "it is enlightening to use these tools , in specific cases , to analytically study the new effects that noise with memory has on the capacity .",
    "we construct a forgetful channel and incorporate memory effects by markov switching between two sub - channels . in order to investigate the classical product state capacity of this channel we must look at the entropy of the classical output .",
    "the output sequence of qubits and their associated errors are correlated . to manage this complicated conditional dependence , we use the hidden markov nature of the process to reformulate the problem using the algebraic measure construction @xcite .",
    "the algebraic measure approach allows us to derive an expression for the asymptotic entropy rate .",
    "we then explore the effects that our non - markovian memory has on the classical product state capacity .",
    "this paper is structured as follows . in section [ sec : classical_capacity ] , we take a closer look at the quantity we are investigating , namely the product state classical capacity . in section [ sec : depolarizing ] , we construct the forgetful channel with markovian noise correlations . in section [ sec : algebraic ] , algebraic measures are introduced , which are used in section [ sec : our_algebraic ] to reformulate the problem .",
    "finally , in section [ sec : results ] , we show how this allows us to easily calculate the capacity of the channel numerically .",
    "the information process we are studying is classical communication through a noisy quantum channel .",
    "the layout of this section largely follows that in @xcite .    with the classical information we want to send encoded using an input alphabet @xmath0",
    ", we choose for every element @xmath1 an encoding quantum state @xmath2 on a hilbert space @xmath3 .",
    "this input state is then transmitted using a quantum channel @xmath4 . for the channel to be a valid quantum channel it must be a completely positive trace preserving map . transmitting the element @xmath1 results in a quantum state @xmath5",
    "being received on the output side . on this side",
    ", the received quantum state is measured using a resolution of identity in @xmath6 .",
    "this resolution of identity is a set of positive operators @xmath7 on @xmath6 such that @xmath8 .",
    "the conditional probability of the receiver measuring @xmath9 , when the input @xmath10 was sent , is given by @xmath11 .",
    "if at the input side the element @xmath10 is sent with a probability @xmath12 , the amount of information that will be received is quantified by the classical shannon information , @xmath13    if the sender is allowed to use the channel @xmath14 times , the channel use can be described by the product channel @xmath15 on @xmath16 .",
    "the input alphabet is now @xmath17 and the probability distribution of a word @xmath18 being sent is again denoted by @xmath19 .",
    "the codeword corresponding to the input @xmath20 is given by @xmath21 and results in @xmath22 being received .",
    "the conditional probability and the shannon information @xmath23 for the @xmath14-product of the channel can now be introduced completely analogously to eq .",
    "( [ eq : single_shannon ] ) , with the summations over @xmath17 instead of @xmath24 .    the maximum amount of information that can be sent with @xmath14 channel uses is now given by @xmath25    due to the fact that @xmath26 , the limit @xmath27 exists . using shannon s coding theorem",
    ", we see that @xmath28 is the least upper bound of the rate of information that can be transmitted with asymptotically vanishing error .",
    "the hsw theorem @xcite gives an expression for this classical product state capacity of noisy memoryless quantum channels , @xmath29 where @xmath30 is the holevo @xmath30 quantity @xmath31 due to the convexity of the von neumann entropy , the supremum can in fact be taken over pure states @xmath2 .",
    "the memoryless channel restriction has recently been weakened to include , so called , _ forgetful _ memory channels . for such channels ,",
    "the classical product state capacity has been shown @xcite to correspond to @xmath32 where @xmath33 is a channel representing the transmission of @xmath14 states , with the noise on subsequent transmissions is correlated .",
    "see @xcite for details or section [ sec : depolarizing ] for an example .",
    "treating information or noise sources as independent random variables is a successful but crude first approximation . to improve the modelling process and to achieve better performance in real world applications ,",
    "the independence assumption needs to be removed .",
    "the first step in this direction is to introduce forgetful noise memory .",
    "a forgetful noise process is one which after sufficiently long time , ` forgets ' or is independent of previous noise .",
    "thus , here the independence is pushed further away , allowing a space to study the effects of short - term memory . with the theoretical tools in place , it is instructive to study even very simple models to see the effects of memory on the classical capacity .",
    "the forgetful channel is constructed by combining two memoryless single qubit depolarizing channels ( @xmath34 and @xmath35 ) , switching between them using a two - state markov chain ( @xmath36 ) .",
    "thus , @xmath37 is the @xmath38 markovian channel selection matrix with @xmath39 being the probability of switching from channel @xmath10 to channel @xmath9 .",
    "hence , @xmath40 and @xmath41 for @xmath42 .",
    "it is forgetful , in the case when the markov chain is _ aperiodic _ and _",
    "irreducible_.    the depolarizing channels can be written as : @xmath43 .",
    "these single qubit channels can be thought of as probabilistically mixing the identity channel ( with probability @xmath44 ) and ` flip ' channel ( with probability @xmath45 ) acting on a single qubit density operator @xmath46 .",
    "however this rewriting is only completely positive for @xmath47 .    the built - up channel @xmath33 , corresponding to @xmath14 successive uses is @xmath48 the sum is over all possible paths @xmath49 and each term is a tensor product of the selected sub - channels weighted by the probability of occurrence ( @xmath50 is the initial probability of selection set to the stationary distribution of the markov process : @xmath51 ) .",
    "we calculate the capacity with this @xmath14-use form of the channel and regularize by taking the limit @xmath52 as in eq .",
    "( [ eq : capacity_forgetful ] ) .",
    "since we are looking at the product state capacity , we choose @xmath53 where the @xmath54 are arbitrary pure qubit states .    applying the channel @xmath33",
    ", we get @xmath55 where @xmath56 denotes the qubit state with a flipped bloch vector with respect to @xmath57 @xmath58    by expanding the product above we see that the eigenvalues of the output state are given by @xmath59 note that these eigenvalues are independent of the choice of the input state .",
    "the channel output can now be written as @xmath60 hence , if we calculate the first term in the holevo @xmath30 quantity for @xmath61 , the uniform distribution ( @xmath62 ) , and @xmath63 going over all the @xmath64 , we see that @xmath65 since @xmath66 goes over all possible combinations , so does @xmath67 , so we can relabel them @xmath68 since the eigenvalues in eq .",
    "( [ eq : eigenvalues ] ) sum to one , we see that @xmath69 is the maximally mixed state @xmath70 thus , @xmath71 is maximal and is equal to @xmath72 .",
    "the second term in the holevo @xmath30 quantity is @xmath73 since the eigenvalues @xmath74 of @xmath75 do not depend on the choice of @xmath2 , this term does not influence the maximization .",
    "hence our choice of @xmath61 and @xmath46 maximizes the holevo @xmath30 quantity .",
    "thus , the final expression for the regularized capacity eq .",
    "( [ eq : capacity_forgetful ] ) is    @xmath76    if we were to calculate the output entropy using the eigenvalues in eq .",
    "( [ eq : eigenvalues ] ) , the calculation would be exponentially long in @xmath14 .",
    "therefore , other techniques are needed . the way we approach the problem is by reformulating it as a hidden markov process .",
    "the eigenvalues of the output state correspond to the probabilities of such a process .",
    "a hidden markov process can be defined as follows .",
    "if we have a translation - invariant measure @xmath77 with the markov property on @xmath78 , where @xmath79 is a finite set , then a hidden markov measure can be constructed on @xmath80 through a function @xmath81 , with the following local densities @xmath82 where @xmath83 and @xmath84 . for obvious reasons , these processes are also called functions of markov processes .",
    "an algebraic measure , @xmath85 , is a translational - invariant measure on a set @xmath86 , with probabilities determined by matrices @xmath87 with positive entries , one for each of the @xmath88 states .",
    "the probability of a sequence is obtained by applying a positive linear functional @xmath89 to a matrix product of the corresponding matrices of the states of the sequence : @xmath90 .",
    "this matrix algebraic construction is the reason for the name _ algebraic measure _ , studied in detail in ref .",
    "as we shall see , the hidden markov processes correspond to a set of algebraic measures with a specific positivity structure and remarkably , the converse holds too .",
    "in @xcite it was shown that hidden markov processes correspond to manifestly positive algebraic measures .",
    "the local densities of such a manifestly positive algebraic measure on an infinite chain @xmath80 of classical state spaces @xmath91 are of the form @xmath92 where @xmath93 , @xmath94 and @xmath89 are vectors in @xmath95 with non - negative elements ( denoted @xmath96 ) and the @xmath97 are @xmath98 real matrices with non - negative elements ( denoted @xmath99 ) .    as an example of these manifestly algebraic measures ,",
    "let us look at a regular markov chain @xmath100 on @xmath101 . if we choose @xmath94 , @xmath89 and the @xmath97 as @xmath102 one can check that @xmath103 indeed gives the correct densities .    from this example",
    "it is easy to see that if we have a hidden markov process on @xmath78 defined by a map @xmath104 and a markov measure @xmath85 on @xmath105 with corresponding matrices @xmath87 , the manifestly positive algebraic measure corresponding to the hidden markov measure is given by the same vectors @xmath89 and @xmath94 as before and the following matrices : @xmath106    for a proof of the converse , which is namely , that every manifestly positive algebraic measure corresponds to a hidden markov measure , we refer to @xcite .",
    "we will now briefly summarize how the algebraic measure approach allows for a simpler approach to finding the entropy density @xcite .",
    "the entropy of a state @xmath85 on @xmath80 restricted to a region @xmath107 is defined by @xmath108    @xmath109 can be shown to be bounded by @xmath110 , monotonically increasing in @xmath107 and strongly subadditive , that is @xmath111    using the strong subadditivity of the entropy and the translational invariance of the measure , one can show that @xcite @xmath112    we can then use this relation together with the expression for the local densities of the manifestly positive measures to reformulate the convergence of the mean entropy into a dynamical system of converging measures on the set of @xmath113-dimensional probability measures @xmath114 as @xmath115 where @xmath116    if we define the linear transformation @xmath117 on functions on @xmath118 , one can show that @xmath119 .",
    "@xmath117 is a contraction map , so a fixed point argument can be used to show that @xmath120 converges to a unique measure @xmath121 that is invariant under @xmath117 @xmath122 this measure allows us then to calculate the mean entropy @xmath123    our goal in the remaining part of the article is to translate the switching depolarizing channel into the setting of algebraic measures and to try and find the invariant measure that allows us to calculate the mean entropy .",
    "the relationship between the hidden markov measure , say @xmath124 on @xmath80 , and the underlying markov measure @xmath77 with the markov property on @xmath78 is through a ` tracing ' function @xmath81 , as is shown in eq . ( [ eq : hidden_markov ] ) .",
    "the underlying markov process for the overall quantum channel has a four state configuration space corresponding to channel selection and error occurrence : @xmath125 .",
    "the first index indicates which depolarizing channel has been chosen and the second indicates whether a bit flip occurred .",
    "the elements of the transition matrix , @xmath126 , for this process are then given by @xmath127 the probability of going from @xmath128 to @xmath129 is given by the switching probability @xmath130 from channel @xmath10 to @xmath131 , multiplied by the probability @xmath132 that channel @xmath131 produces the error - occurrence @xmath133 .",
    "the function that produces the correct hidden markov process is then given by @xmath134 this function reflects the fact that we are unaware of the choice of channel that has been made .",
    "the only effect that is visible from the outside is whether or not an input qubit has been flipped .",
    "thus , @xmath135 has to ` trace out ' the choice of channel .",
    "@xmath135 maps into the two - state error configuration space containing ` no flip ' and ` flip ' : @xmath136 .    using the fact that the matrices @xmath137 defining the algebraic measure of a markov process ( ( sec .",
    "[ sec : manifest ] , pg . ) , @xmath138 ) have only one non - zero row and eq .",
    "( [ eq : hidden_algebraic ] ) , we get the matrices @xmath139 and @xmath140 that define the algebraic measure corresponding to @xmath124 .",
    "the matrix corresponding to @xmath141 , the first element of @xmath79 is given by @xmath142 and similarly for 1 , the second element of @xmath79 .",
    "the hidden markov process then gives us almost the same probabilities as the eigenvalues in eq .",
    "( [ eq : eigenvalues ] ) @xmath143 note that according to our discussion in section [ sec : algebraic ] , the vector @xmath94 is the stationary distribution of the full matrix @xmath126 . using eq .",
    "( [ eq : trans_elements ] ) , one can see that the invariant distribution @xmath94 is in fact @xmath144 , so the probabilities of the hidden markov process coincide with the eigenvalues in eq .",
    "( [ eq : eigenvalues ] ) .    having constructed the correct algebraic measure , we can determine @xmath117 explicitly and use it to greatly simplify the corresponding invariant measure @xmath121 .",
    "the expression for @xmath117 , as can be found in @xcite , is @xmath145 where @xmath146 is any @xmath147-dimensional vector such that @xmath148 and @xmath149 is a continuous real - valued function on the set of such vectors . for the case of our hidden markov measure",
    ", the form of this transformation can be greatly simplified . due to the stochasticity of the matrix @xmath126 , we have the following : @xmath150 if we furthermore denote the four row vectors of @xmath126 by @xmath151 , @xmath152 , @xmath153 and @xmath154 , we can write @xmath155 on top of this , @xmath156 and @xmath157 , so the total form of the transformation becomes @xmath158    from this form of the transformation",
    ", we can already greatly restrict the support of @xmath121 .",
    "our claim is that the support of @xmath121 is restricted to the set of convex combinations of @xmath159 and @xmath160 @xmath161   \\rbrace\\;.\\ ] ]    to show this , let s suppose that @xmath162 and @xmath163   \\rbrace $ ] . take @xmath164 a function on @xmath114 such that @xmath165 for all @xmath166 and @xmath167 , then @xmath168\\;.\\\\\\end{aligned}\\ ] ] however , this integral is equal to zero , since the arguments to @xmath164 run over the set @xmath169 .",
    "therefore , we have for @xmath170 , @xmath171 with @xmath172 a measure on @xmath173 $ ] .",
    "now let us look at @xmath121 acting on the transformed @xmath149 : @xmath174 \\nonumber \\\\ & = \\int_0 ^ 1 d\\lambda(a)\\bigl[(\\hat{\\mu}_{a,1 } + \\hat{\\mu}_{a,3})f\\bigl(\\frac{\\hat{\\mu}_{a,1}\\hat{\\mu}_1 + \\hat{\\mu}_{a,3}\\hat{\\mu}_{3}}{\\hat{\\mu}_{a,1 } + \\hat{\\mu}_{a,3}}\\bigr ) \\nonumber \\\\ & + ( \\hat{\\mu}_{a,2 } + \\hat{\\mu}_{a,4})f\\bigl(\\frac{\\hat{\\mu}_{a,2}\\hat{\\mu}_1 + \\hat{\\mu}_{a,4}\\hat{\\mu}_{3}}{\\hat{\\mu}_{a,2 } + \\hat{\\mu}_{a,4}}\\bigr)\\bigr ] \\ ; , \\label{eq : lambda_iterate}\\end{aligned}\\ ] ] where @xmath175    by invariance ( sec .",
    "[ sec : mean_entropy ] , pg . )",
    ", we can equate eq .",
    "( [ eq : lambda_measure ] ) and the above eq .",
    "( [ eq : lambda_iterate ] ) to discover an invariance concerning @xmath172 .",
    "we thus arrive at the following symmetry of @xmath172 : @xmath176 = a \\mapsto c_1(a ) \\lambda[f_1(a ) ] + c_2(a ) \\lambda[f_2(a)]\\;.\\ ] ]    the two functions @xmath177 and @xmath178 are relatively simple shrink functions about two separate points in the domain @xmath173 $ ] , that shrink the @xmath173 $ ] domain into two ( possibly overlapping ) sub - intervals of @xmath173 $ ] .",
    "we can turn this analytic symmetry into a cyclic definition or iterative procedure to generate @xmath172 up to some approximation @xmath179 .",
    "@xmath180    we still have not defined @xmath181 , but taking a look the iterative procedure , we see that there exist fixed points of the two shrink functions , call them @xmath182 and @xmath183 , @xmath184 \\;.\\ ] ]    with this observation the idea is to begin the iteration procedure with two dirac delta s at these fixed points , @xmath185 note that @xmath186 , as a measure should be . since there is unique convergence then the initial weightings should not matter @xcite .    to see that this is a good starting point and to get further insight into the support of @xmath172",
    ", it can be seen that the support will grow , but most importantly , once a point is within the support of @xmath187 it remains there for all @xmath188 .",
    "so if the procedure is taken to infinity the support is fixed and countably infinite .",
    "thus , we arrive at the following expression for the full support , @xmath189:\\exists n \\in \\mathbf{n } , \\exists k_i\\in \\{0,1\\}\\forall i \\in [ 1,n]\\\\ & f_{k_{n}}\\circ f_{k_{n-1}}\\circ \\ldots \\circ",
    "f_{k_1}(a_1~\\textrm{or}~a_2 ) = a   \\ } \\;.\\end{aligned}\\ ] ]    we use this iterative procedure to generate @xmath179 and then use it in eq .",
    "( [ eq : lambda_measure ] ) to approximate the measure .",
    "the entropy in eq .",
    "( [ eq : entropy ] ) can then be calculated and finally we use the entropy to calculate the capacity through eq .",
    "( [ eq : capacity ] ) .",
    "it is the capacity and its dependence on memory that we are interested in .",
    "in constructing our channel we defined certain parameters . it is useful to introduce a new set of suggestive parameters in terms of the old and also to reduce their number by making some assumptions .",
    "firstly , we assume that the sub - channels switch symmetrically , that is , the probabilities of reuse are the same for both sub - channels .",
    "this makes the markov matrix doubly stochastic and allows us to use its non - one eigenvalue as a useful characterizing parameter @xmath190 .",
    "thus , we set @xmath191 and @xmath192 .",
    "the domain of @xmath190 is @xmath193 , with @xmath194 corresponding to no noise correlations .",
    "secondly , we parametrize the error probabilities by their average and difference : @xmath195 , @xmath196 .",
    "the main result is that the capacity increases with stronger noise - correlations .",
    "this manifests itself in two ways .",
    "firstly , if we make the switching more correlated ( @xmath190 away from @xmath141 ) the capacity increases and secondly , if we increase the difference between the two sub - channels the capacity also increases .",
    "similar results have been found for the quantum capacity of the dephasing channel with markovian memory @xcite .",
    "[ tl][tr]@xmath197 [ bl][br]@xmath198 [ tr][br]@xmath199    [ br][br]@xmath200 [ br][br]@xmath201 [ br][br]@xmath202    [ tr][tr]@xmath203 [ tr][tr]@xmath141 [ tr][tr]@xmath202    [ cr][cr]@xmath141 [ cr][cr]@xmath204 [ cr][cr]@xmath202        in figure [ fig : capacity3d ] , @xmath113 is set to the maximum possible value while keeping an average of @xmath205 ( @xmath206 $ ] ) .",
    "remember that both @xmath207 and @xmath208 have to lie in the @xmath209 $ ] interval for the two sub - channels to be completely positive .",
    "the capacity is plotted against varying @xmath205 and @xmath190 .",
    "we can see that the capacity increases as the noise - correlation ( @xmath190 ) gets stronger .",
    "when @xmath210 , @xmath113 attains its maximum ( @xmath211 ) and the effect of increasing @xmath190 on the capacity is greatest .",
    "another interesting observation is the case when the two sub - channels average to the maximally mixing channel ( @xmath212 , which ignoring memory , has zero capacity ) , taking into account memory effects there is a non - zero capacity .",
    "[ cc][cc ] [ cl][cl]@xmath198 [ bc][tc]@xmath199    [ cr][cr]@xmath141 [ cr][cr]@xmath213 [ cr][cr]@xmath214 [ cr][cr]@xmath202    [ tc][tc]@xmath204 [ tc][tc]@xmath215 [ tc][tc]@xmath202    ]    to better illustrate the last point and to further explore the relationship between the capacity of the memory channel and its sub - channels , we plot in figure [ fig : capacity_vs_a ] , slices of figure [ fig : capacity3d ] of fixed @xmath190 together with plots of the underlying sub - channel capacities .",
    "thus , in the ` avg capacity ' curve of figure [ fig : capacity_vs_a ] , we see the edge of figure [ fig : capacity3d ] ( for fixed @xmath216 , equivalently @xmath217 , not actually attained ) , which corresponds to the average of the capacities of the sub - channels .",
    "the sub - channels separate capacities are plotted in curves labelled ` low noise sub ' and ` noisier sub ' .",
    "they are chosen to have maximum allowed separation for each point as @xmath205 varies ( and thus the artificial discontinuities ) . in a real world example , this separation parameter is fixed by the channel and the sub - channels and their capacities would not be accessible .",
    "the capacity of the average channel , labelled ` avg channel ' , corresponds to a slice of fixed @xmath194 ( the center of figure [ fig : capacity3d ] ) , since a no - memory / non - biased markov walk factors into a tensor product of the average of the sub - channels , which is thus equivalent to just one depolarizing channel with the average error probability . the curve , ` with memory ' , is a smooth intermediary between the ` avg channel ' and ` avg capacity ' and is an example slice of figure [ fig : capacity3d ] for @xmath218 , which illustrates how taking memory into account improves the capacity .",
    "of course , again , in a real world example this parameter is specified by the channel . the smooth transformation is not straightforward nor linear , which can be seen in the way figure [ fig : capacity3d ] curves for varying @xmath190 .    [ cc][cc ] [ cc][cl]@xmath197 [ cc][cc]@xmath199 [ cr][cr]@xmath219 [ cr][cr]@xmath213 [ cr][cr]@xmath220 [ cr][cr]@xmath221 [ cc][cc]@xmath204 [ cc][cc]@xmath202     using many iterations and including full markov calculation ]    to see the last point more clearly and also to indicate the convergence of the iteration procedure we next plot a slice of figure [ fig : capacity3d ] for fixed @xmath205 . in figure [ fig : capacity_vs_s ] we plot the regularized capacity against @xmath190 with the following fixed parameters : @xmath222 , @xmath223 .",
    "we can see that the capacity increases as the noise - correlation gets stronger .",
    "the blue dots are calculated using a simplified ( @xmath216 ) full markov walk calculation ( @xmath224 steps ) which does nt suffer from the usual exponential blow - up .",
    "the horizontal green line is the output entropy for @xmath194 , which is corresponds to no correlations and is equivalent to having only one depolarizing channel .      to complete the discussion concerning correlations we need to look at the two extreme cases : @xmath216 , corresponding to the case where a sub - channel is selected and used for every channel use afterwards , and @xmath217 , corresponding to the case where the choice of sub - channel is flipped with every channel use",
    "therefore , in constructing the overall channel and taking into account the initial random channel selection , we just have the mixing of two @xmath14-use channels .",
    "specifically , in the @xmath216 case , we have the mixing of the two @xmath14-fold tensor products of the two sub - channels separately , and in the @xmath217 case , we have the mixing of two @xmath14-use channels where each deterministically alternates between the sub - channels but starting with a different sub - channel .",
    "both these extreme cases are non - forgetful since the initial sub - channel selection ( the initial noise ) is ` remembered ' and the forgetful holevo capacity theorem no longer applies ( the markov selection matrix is periodic in the @xmath217 case and reducible in the @xmath216 case ) . while our forgetful channel approach breaks down",
    "there are alternate theoretical frameworks that do actually capture these extreme cases .",
    "for @xmath217 the capacity can be calculated using @xcite and agrees with the limit of the forgetful approach , the capacity is the average capacity of the two sub - channels separately . however , for @xmath216 case there is a discontinuity and the capacity suddenly drops to the minimum capacity of the sub - channels @xcite .",
    "the intuition is that in the @xmath217 case , the deterministic flip can be used to determine ` on - the - fly ' which sub - channel is being used and then it is the same as using the two channels separately each half the time , so the capacity must be the average capacity . for the @xmath216 case once you have the poorer channel you are stuck with it forever and so because of the mixture you can only guarantee the lower capacity .",
    "we have constructed a simple forgetful noise - memory quantum channel .",
    "the noise - correlation is a function of the underlying hidden markov process .",
    "this setup allowed us to construct a corresponding algebraic measure .",
    "we used the measure in an algebraic asymptotic entropy expression . without this",
    ", the entropy would be very difficult to compute , involving exponentially many paths in configuration space .",
    "we studied the effects that the noise correlations had on the classical capacity and discovered that the capacity increases with stronger correlations .",
    "this is sensible because the correlations can be used to combat the noise when coding information .",
    "we have arrived at the understanding that stronger correlations increases the capacity from that of the average channel to the average capacity of the sub - channels with very interesting limiting behaviour .",
    "further work includes using other approximation techniques , arriving at a full analytic expression of the capacity and looking at other similarly constructed channels .",
    "we are also confident and hopeful that the hidden markov technique could be successfully employed in other contexts .",
    "we would like to acknowledge n. datta and t. dorlas for the idea of the channel construction and valuable assistance .",
    "this work is based upon research supported by the south african research chair initiative of the department of science and technology and national research foundation",
    ".    9 a.s .",
    "holevo , _ the capacity of the quantum channel with general signal states _ , ieee trans .",
    "theory 44 , 269 , 1998 ; b.  schumacher and m.d .",
    "westmoreland , _ sending classical information via noisy quantum channels _ , phys . rev .",
    "a , 56 , 131 , 1997 .",
    "kay and b.s .",
    "kay , _ monotonicity with volume of entropy and of mean entropy for translationally invariant systems as consequences of strong subadditivity _ , j. phys . a : math .",
    "365 - 382 , 2001 ."
  ],
  "abstract_text": [
    "<S> the classical product state capacity of a noisy quantum channel with memory is investigated . </S>",
    "<S> a forgetful noise - memory channel is constructed by markov switching between two depolarizing channels which introduces non - markovian noise correlations between successive channel uses . </S>",
    "<S> the computation of the capacity is reduced to an entropy computation for a function of a markov process . a reformulation in terms of algebraic measures then enables its calculation . </S>",
    "<S> the effects of the hidden - markovian memory on the capacity are explored . </S>",
    "<S> an increase in noise - correlations is found to increase the capacity . </S>"
  ]
}