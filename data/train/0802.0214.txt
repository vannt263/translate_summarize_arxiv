{
  "article_text": [
    "in the last two decades , multivariate time series have received considerable attention with the emphasis being placed on state space models ( ltkepohl , 1993 , west and harrison , 1997 , chapter 16 ; durbin and koopman , 2001 , chapter 3 ; de gooijer and hyndman , 2006 ) . from an econometrics standpoint time - varying volatility models have been widely developed , recognizing the essence that the volatility and the correlation of assets change over time .",
    "although univariate volatility models are useful in estimating and forecasting volatility , it is widely recognized ( bauwens _ et al .",
    "_ , 2006 ) that multivariate models , which can model the serial and cross correlation of the assets , should be employed .    from a time series standpoint ,",
    "volatility models are developed within two main families of models : the multivariate generalized autoregressive conditional heteroskedastic ( mgarch ) , including the multivariate arch , and the multivariate stochastic volatility ( msv ) families .",
    "multivariate arch models include the diagonal vech model ( bollerslev _ et al .",
    "_ , 1988 ) , the constant conditional correlation model ( bollerslev , 1990 ) , the factor - arch model ( engle _ et al . _ , 1990 ) , the bekk model ( engle and kroner , 1995 ) and the latent factor arch model ( diebold and nerlove , 1989 ) ; see also wong and li ( 1997 ) , tse and tsui ( 2002 ) , comte and lieberman ( 2003 ) , and audrino and barone - adesi ( 2006 ) .",
    "msv models have also received a lot of attention , see e.g. harvey _ et al . _",
    "( 1994 ) , jacquier _ et al . _",
    "( 1995 ) , kim _ et al . _",
    "( 1998 ) , pitt and shephard ( 1999 ) , aguiliar and west ( 2000 ) and meyer _ et al . _",
    "a number of estimation procedures have been suggested for msv models ; for instance , see bauwens _",
    "( 2006 ) , yu and meyer ( 2006 ) , liesenfeld and richard ( 2006 ) , asai _ et al . _",
    "( 2006 ) and maasoumi and mcaleer ( 2006 ) . in this context",
    ", several variations of computationally expensive markov chain monte carlo ( mcmc ) methods are commonly used following papers by shephard ( 1993 ) , jacquier _ et al . _",
    "( 1994 ) , kim _ et al . _",
    "( 1998 ) , shephard and pitt ( 1997 ) , uhlig ( 1997 ) , chib _ et al . _",
    "( 2002 ) and philipov and glickman ( 2006a , 2006b ) .    most of the proposed models are aimed at specific applications , or they impose restrictions in the parameter space , or they are only available for data with low dimensionality",
    ". in particular , it would be desirable to obtain estimation algorithms , for which the model would estimate not only the volatility covariance matrix , but also shocks in the levels of the returns .",
    "in addition to that , it is desirable to construct a model that will not rely on monte carlo or any other simulation procedures and also will not target data of specific applications .    in this paper",
    "we develop a general state space model , which allows the volatility covariance matrix to be estimated with a fast bayesian algorithm .",
    "the proposed algorithm is achieved by considering a stochastic multiplicative model for the volatility , which is based on wishart and singular multivariate beta distributions .",
    "a diagonal matrix of degrees of freedom is used in a variance discounting approach in order to update the estimates and the forecasts of the volatility from time @xmath0 to time @xmath1 .",
    "this has a unique advantage that different volatilities can be discounted at different rates , for example one can have two assets , the volatility of the first changes at a rate according to a discount factor of @xmath2 and the volatility of the second changes at a slower rate according to a discount factor of @xmath3 .",
    "the algorithm is fast and provides not only one - step ahead forecasts of the volatility , but also the entire one - step ahead forecast distribution of the volatility .",
    "a bayesian algorithm is outlined for sequential model comparison .",
    "the proposed methodology is illustrated by considering data , consisting of spot prices of aluminium , copper , lead and zinc from the london metal exchange .",
    "it is found that the volatilities of aluminium and zinc prices are driven from a common factor and the volatilities of copper and lead prices are driven from another factor , while the respective correlations are around @xmath4 .",
    "the performance of the model is discussed by using several diagnostic toolkits , including the mean of squared standardized forecast errors , the log - likelihood function and value - at - risk .",
    "the paper is organized as follows .",
    "section [ model : dwr ] defines the model , for which inference is developed in section [ estimation ] .",
    "section [ test ] discusses diagnostic tests and model comparison , and the following section analyzes data from the london metal exchange market . in section [ conclusions ]",
    "we discuss the advantages of the proposed approach as compared with existing garch estimation procedures .",
    "the appendix gives full mathematical details ( including the proofs ) of arguments in sections [ estimation ] and [ test ] .",
    "matrix - variate dynamic linear models ( mv - dlms ) are introduced in quintana and west ( 1987 ) and they are further developed in salvador _",
    "( 2003 ) , salvador and gargallo ( 2004 ) , salvador _ et al . _",
    "( 2004 ) , triantafyllopoulos and pikoulas ( 2002 ) and triantafyllopoulos ( 2006a ) ; matrix - variate dlms are reported in some detail in west and harrison ( 1997 ,  16.4 ) . for the purpose of this paper the discussion",
    "is restricted to vector - valued time series ; the general description for matrix - valued time series can be found in salvador _",
    "we should note that from a frequentist standpoint , mv - dlms have been developed in harvey ( 1986 , 1989 ) , harvey and snyder ( 1990 ) , and fernndez and harvey ( 1990 ) .",
    "suppose that the @xmath5-dimensional response vector @xmath6 follows a matrix - variate dlm so that @xmath7 where @xmath8 is a @xmath9-dimensional design vector , @xmath10 is a @xmath11 evolution matrix and @xmath12 is a @xmath13 state matrix .",
    "conditional on @xmath14 , the innovations @xmath15 and @xmath16 follow , respectively , multivariate and matrix - variate normal distributions , i.e. @xmath17 where @xmath14 is the unknown @xmath18 volatility covariance matrix of the innovations @xmath15 , and @xmath19 is a @xmath11 covariance matrix of the innovation @xmath16 .",
    "the distribution of @xmath20 may also be written as @xmath21 where @xmath22 denotes the column stacking operator of a matrix and @xmath23 denotes the kronecker product .",
    "it is assumed that the innovation series @xmath24 and @xmath25 are internally and mutually uncorrelated and also they are uncorrelated with the assumed priors @xmath26 for some known @xmath27 , @xmath28 , @xmath29 and @xmath30 .",
    "here @xmath31 denotes the inverted wishart distribution with @xmath32 degrees of freedom and parameter matrix @xmath33 with density function @xmath34 where @xmath35 denotes the multivariate gamma function , @xmath36 denotes the exponent of a trace of a matrix , and @xmath37 denotes the determinant of @xmath33 .",
    "then @xmath38 follows the wishart distribution @xmath39 .",
    "let @xmath40 be a positive integer and write @xmath41 the information set comprising observations up to time @xmath1 , for @xmath42 .",
    "the covariance matrix @xmath19 is specified with at most @xmath9 discount factors @xmath43 so that @xmath44 where @xmath45 .",
    "thus @xmath19 is the implied covariance matrix obtained after discounting is used in order to increase the covariance matrix from time @xmath0 to time @xmath1 , given information @xmath46 .",
    "the above equation justifies that @xmath47 implying @xmath48 , where @xmath49 is the left covariance matrix of @xmath50 , so that @xmath51 ( in section [ estimation ] it is shown that @xmath49 is calculated routinely ) .",
    "it is proposed that the above setting for @xmath19 is carried out for the covariance matrix @xmath52 .",
    "this setting , which generalizes the single discounting approach of west and harrison ( 1997 ) , is necessary to consider in order to retain conjugate forms in the updating of the posterior distribution of @xmath53 ( see section [ estimation ] ) .",
    "multiple discount factors are useful in capturing the different structural characteristics of trend , seasonal and regression coefficient elements of the evolution matrix @xmath19 .",
    "the volatility matrix @xmath14 imposes complications in inference , but , it is a very useful consideration in the model because in financial time series , high frequency data exhibit short - term or long - term heteroscedastic behaviour . in the remainder of this section",
    "we describe the stochastic model governing the evolution of @xmath14 .    at time @xmath0",
    "we assume that @xmath54 , conditional on @xmath46 , follows an inverted wishart distribution , @xmath55 , for some @xmath56 and @xmath57 .",
    "the precision matrix is indicated by @xmath58 and , following a choleski decomposition , we write @xmath59 , where @xmath60 is the unique upper triangular matrix of the choleski decomposition .",
    "the law governing the evolution of the @xmath14 or @xmath61 from time @xmath0 to time @xmath1 is represented by @xmath62 where @xmath63 is diagonal matrix of discount factors @xmath64 and @xmath65 , which given @xmath46 , is assumed to be independent of @xmath66 , is a random matrix following the singular multivariate beta distribution with parameters @xmath67 and @xmath68 ; we write @xmath69 . in section",
    "[ estimation ] we will see that @xmath70 . of course @xmath56 is defined for @xmath71 , for @xmath72 . for more details on the singular multivariate beta distribution the reader is referred to uhlig ( 1994 ) , daz - garca and gutirrez ( 1997 ) , and srivastava ( 2003 ) .",
    "the evolution ( [ vol : evol ] ) is motivated from the univariate case ( @xmath73 ) , for which ( [ vol : evol ] ) reduces to @xmath74 in this case the multivariate singular beta reduces to a standard beta distribution and as @xmath65 is independent of @xmath66 , we have @xmath75 and @xmath76 , since @xmath77 .",
    "this defines a random walk type evolution for @xmath61 .",
    "the above evolution for a scalar volatility @xmath14 is studied in harrison and west ( 1987 ) , west and harrison ( 1997 ,  10.8 ) , and triantafyllopoulos ( 2007 ) .",
    "returning to the case when @xmath78 , suppose that @xmath79 and so @xmath80 . in this case the evolution ( [ vol : evol ] ) reduces to @xmath81 where @xmath82 . in proposition [ pro1 ] of the appendix",
    ", it is shown that @xmath83 or @xmath84 and so @xmath85 which is different than @xmath86 , unless @xmath87 .",
    "it follows that the random walk type evolution of ( [ evoluni ] ) is retained for values of @xmath88 close to 1 , but otherwise the evolution ( [ vol : evol ] ) defines a shrinkage type evolution , for which @xmath89 .",
    "our empirical results of section [ analysis ] show that the estimator of @xmath14 , which is generated from evolution ( [ vol : evol ] ) , performs well for relatively high values of the discount matrix @xmath90",
    ". for @xmath73 , west and harrison ( 1997 ,  10.8 ) suggest a slow evolution ( [ evoluni ] ) , for which a discount factor close to 1 is proposed .",
    "in particular on page 361 of the above reference it is stated `` we note that practically suitable variance discount factors take values near unity , typically between 0.95 and 0.99 '' .",
    "this is in agreement with our proposal , in the general case of @xmath78 , so that the shrinkage effect in ( [ vol : evol ] ) is small .",
    "however , our empirical results in section [ analysis ] suggest that the modeller should allow for smaller values of the discount factors in the range of 0.6 and 0.99 so that shocks in the volatility can be estimated .",
    "the evolution ( [ evolsmall ] ) makes the assumption that all elements of @xmath61 are discounted at the same rate via the single discount factor @xmath88 .",
    "equation ( [ vol : evol ] ) introduces a flexible evolution , where each of the diagonal elements of @xmath61 are discounted at different rate via the @xmath5 discount factors @xmath91 .",
    "from evolution ( [ vol : evol ] ) and proposition [ pro1 ] of the appendix , the prior density of @xmath92 is the inverted wishart density @xmath93 where @xmath70 .    without loss in clarity of the presentation ,",
    "we denote by @xmath94 the probability density function of a random matrix @xmath95 , avoiding to explicitly write @xmath96 .",
    "thus if @xmath95 and @xmath97 denote two different random matrices , @xmath94 and @xmath98 denote respectively the densities of @xmath95 and @xmath97 .    from model ( [ model2 ] ) , given @xmath14 and @xmath46 , the joint distribution of @xmath99 and @xmath12 is @xmath100 \\sim \\n_{(d+1)\\times p}\\left ( \\left [ \\begin{array}{c } f_t'g_tm_{t-1 } \\\\",
    "g_tm_{t-1 } \\end{array}\\right ] , \\left [ \\begin{array}{cc } f_t'r_{t}f_t & f_t'r_{t } \\\\",
    "r_{t } f_t & r_{t}\\end{array}\\right ] , \\sigma_{t } \\right),\\ ] ] where @xmath101 and the covariance of @xmath102 and @xmath103 is determined by @xmath104    from ( [ eq : joint ] ) and the inverted wishart prior ( [ prior : v ] ) it follows that the joint forecast density of @xmath102 and @xmath103 , given only @xmath46 is a @xmath105 multivariate student @xmath1 density ( see theorem 4.2.1 of gupta and nagar , 1999 ,  4.2 ) , i.e. @xmath106 & \\sim & \\t_{p\\times 1}\\bigg(p^{-1}\\textrm{tr}(\\beta ) n+p-1 , \\left [ \\begin{array}{c } f_t'g_tm_{t-1 }",
    "\\\\ g_tm_{t-1 } \\end{array}\\right ] , \\left [ \\begin{array}{cc } f_t'r_{t}f_t & f_t'r_{t } \\\\",
    "r_{t } f_t & r_{t}\\end{array}\\right ] , \\\\ & &   \\beta^{1/2 } s_{t-1 } \\beta^{1/2 } \\bigg )   \\equiv   \\t_{p\\times 1 } ( \\nu , m , u , s),\\end{aligned}\\ ] ] with density @xmath107 $ ] .    applying bayes theorem , the posterior distribution of @xmath108 results to be an inverted wishart . to detail the derivations of this result we need to note that the likelihood function of @xmath109 from the single observation @xmath110 is @xmath111 , whilst the prior of @xmath109 is given by ( [ prior : v ] ) .",
    "thus the posterior of @xmath109 given @xmath112 is @xmath113,\\end{aligned}\\ ] ] which is proportional to the inverted wishart distribution @xmath114 , with @xmath115 where @xmath116 is the one - step forecast error and @xmath117 .",
    "the recursions of @xmath118 and @xmath119 are calculated routinely , by writing down the posterior distribution of @xmath120 , i.e. @xmath121 , where from an application of the kalman filter , we have @xmath122 and @xmath123 .",
    "the second parameter of the singular multivariate beta distribution ( see lemma [ lemma ] in appendix ) , denoted by @xmath124 , needs to satisfy two requirements ( a ) @xmath125 must be positive integer number and ( b ) @xmath126 must equal @xmath127 .",
    "( a ) is needed for the singular multivariate beta distribution to be defined ( uhlig , 1994 ) and ( b ) is needed for the distribution of the prior wishart of @xmath128 ( see proposition [ pro1 ] in the appendix ) .",
    "these two requirements result to the adoption of the prior @xmath129 where @xmath90 may be close , but not equal to @xmath130 . with the above prior of @xmath56 ,",
    "the degrees of freedom of equation ( [ eq30 ] ) become @xmath131    define @xmath132 , the residual error vector .",
    "then we have that @xmath133 from this , it follows that equation ( [ eq30 ] ) can be written as @xmath134 , or @xmath135 the posterior expectation of @xmath14 is @xmath136 , for @xmath137 . from equation ( [ prior :",
    "v ] ) , the one - step forecast mean of @xmath14 is @xmath138 , for @xmath139 .",
    "the above estimation procedure is valid for @xmath140 , while from equation ( [ vol : evol ] ) if @xmath141 , then @xmath142 and the volatility is unchanged from @xmath0 to @xmath1 .",
    "note that if @xmath143 we have @xmath80 and in this special case all elements of @xmath14 are discounted in the same rate .",
    "the advantage of employing the discount matrix @xmath90 is that different elements of the volatility estimator @xmath14 can be discounted at different rate .",
    "for example for @xmath144 , one can set @xmath145 , so that with @xmath146 , the variance @xmath147 has constant volatility , but the variance @xmath148 is discounted at a rate according to a discount factor of 0.9 .",
    "the situation @xmath149 , is leading to a time - invariant volatility @xmath150 , for all @xmath1 , and this is usually impractical . in this case , the posterior distribution of @xmath151 is the inverted wishart @xmath152 , with @xmath153 , where @xmath29 are the initial degrees of freedom . in the next result",
    "we relate the above posterior estimate @xmath154 with the maximum likelihood estimator of @xmath151 .",
    "first note that conditional on @xmath151 , the posterior distribution of @xmath12 is @xmath155 then we have the following result .    [ th0b ] in the mv - dlm ( [ model2 ] )",
    "suppose that , for all @xmath1 , @xmath150 , and so conditional on @xmath151 , the posterior distribution of @xmath12 is given by equation ( [ dwr5a ] ) .",
    "then the maximum likelihood estimator of @xmath151 , based on data @xmath156 , is @xmath157 where @xmath158 is the one - step forecast error vector and @xmath132 is the residual error vector .    for @xmath159 and @xmath160 ,",
    "the estimator of @xmath151 , which results from the above inverted wishart prior is @xmath161 and so the posterior estimator of @xmath151 equals to the maximum likelihood estimator of @xmath151 . however ,",
    "when @xmath14 is a time - dependent volatility matrix , a similar procedure for the maximum likelihood estimator of @xmath14 is not available in closed form and so the above sequential bayesian estimation procedure is thought to be advantageous and preferable as opposed to approximate likelihood estimation procedures ( durbin and koopman , 2001 ) .",
    "the log - likelihood function when @xmath14 is time - dependent is given in theorem [ th2 ] of the next section .",
    "from equation ( [ prior : v ] ) we have that the one - step forecast mean of @xmath14 is @xmath162 , where @xmath139 .",
    "the one - step forecast error distribution is a @xmath5-variate @xmath1 distribution , i.e. @xmath163 where @xmath164 .",
    "note that the condition @xmath139 ensures that @xmath165 , hence , given @xmath46 , the covariance matrix of @xmath166 exists . by defining @xmath167 the one - step standardized forecast errors , we obtain @xmath168 where @xmath169 denotes the square root of @xmath170 , based on the choleski decomposition , or based on the spectral decomposition . from this",
    "it follows that @xmath171 and @xmath172 and so , by writing @xmath173'$ ] , one measure of goodness of fit is the mean of squared standardized one - step forecast errors ( msse ) , defined by @xmath174',\\ ] ] which should be close to @xmath175'$ ] , if the model produces a good fit to the data .",
    "of course when @xmath149 , the above @xmath1 distributions can not be defined , since @xmath176 .",
    "in this case we have @xmath177 and then , with @xmath178 , we get @xmath179 and hence all other definitions remain unchanged . other measures of goodness of fit are the mean absolute one - step forecast errors ( mae ) and mean error ( me ) , defined , respectively , by @xmath180'\\quad \\textrm{and } \\quad \\textrm{me}=\\frac{1}{n}\\sum_{t=1}^ne_t,\\ ] ] where @xmath181'$ ] and @xmath182 denotes the modulus of @xmath183 , for @xmath184 .",
    "another method of model diagnostics and model comparison is based on the value - at - risk ( var ) , which in laid words is the amount of money of an asset that one expects to lose with some probability over a certain time horizon .",
    "there are several ways of calculating the var of a portfolio , but here we mention only the most popular , which is termed as the variance - covariance approach and it is due to morgan ( 1996 ) .",
    "the var of a portfolio has a single value ( under a specific model ) , which according to brooks and persand ( 2003 ) is @xmath185 where @xmath186 is the var of a portfolio at time @xmath40 and percentage significance level @xmath187 , @xmath188 is the distribution function of the standardized portfolio returns @xmath189 , and @xmath190 is the conditional volatility of @xmath191 . for known weights @xmath192 satisfying @xmath193 and @xmath194 , we define the portfolio returns @xmath195 and so its volatility is @xmath196 , where @xmath197 . for their internal evaluation of market risk ,",
    "investment banks typically use @xmath198 significance levels , leading to less tight evaluation of var , i.e. the resulting from var amount of money will cover @xmath198 of probable loses .",
    "the basel committee on banking supervision ( 1996 , 1998 ) uses a tight @xmath199 confidence percentage to ensure coverage of @xmath199 losses .",
    "clearly @xmath200 , since there is needed more money to cover larger proportion of probable loses .",
    "more details on var and its evaluation may be found in tsay ( 2002 , chapter 7 ) and chong ( 2004 ) .",
    "another measure of goodness of fit , is based on the evaluation of the log - likelihood function , as a means of model design ( e.g. choosing values of the discount matrices @xmath201 and @xmath90 ) and model comparison .",
    "the next result gives an expression of the log - likelihood function .",
    "[ th2 ] in the mv - dlm ( [ model2 ] ) denote with @xmath202 the log - likelihood function of @xmath203 , based on data @xmath156 .",
    "then it is @xmath204 where @xmath63 , @xmath205 and @xmath206 is the diagonal matrix with diagonal elements the positive eigenvalues of @xmath207 , with @xmath208 .",
    "note that if @xmath149 , then @xmath150 , for all @xmath1 , and the log - likelihood function of @xmath151 reduces to @xmath209 is clearly provided conditional on the values of @xmath201 and @xmath90 and so , replacing @xmath14 by @xmath210 ( the posterior mean of @xmath14 ) in the log - likelihood , one way to choose these values is by maximizing the log - likelihood over a range of candidate values for @xmath201 and @xmath90 .    in model comparison",
    ", the log - likelihood function is particularly useful , as it can be used forming likelihood ratios in order to compare and contrast the performance of two models .",
    "a similar idea can be implemented by considering sequential model monitoring , for which , two models are compared by using sequential bayes factors of the standardized errors @xmath211 . following the ideas of west and harrison ( 1997 , chapter 11 ) and salvador and gargallo ( 2004 ) , we consider two models @xmath212 and @xmath213 , which differ in some quantitative form , e.g. in the values of the discount matrices , and by writing all densities conditional on these two models , we form the log bayes factor @xmath214 , \\quad t=1,2,\\ldots , n.\\ ] ] then , at time @xmath1 , @xmath212 is in favour of @xmath213 ( equiv .",
    "@xmath213 is in favour of @xmath212 ) , if @xmath215 ( equiv .",
    "@xmath216 ) , while when @xmath217 , the two models are equivalent , in the sense that they produce similar forecasts and similar standardized forecast errors .",
    "some algorithms have been proposed in the literature about how the above test can be done efficiently .",
    "some work includes monte carlo simulation ( salvador _ et al .",
    "_ , 2004 ) , some work is restricted in the case of a time - invariant volatility matrix ( salvador and gargallo , 2004 ) and most of the work refers to univariate processes ( west and harrison , 1997 , salvador and gargallo , 2004 , 2005 , 2006 ) .",
    "triantafyllopoulos ( 2006b ) proposes a general procedure , according to which , a modified exponentially weighted moving average control chart is applied to the univariate process @xmath218 and control signals indicate model preference .",
    "the above ideas of model comparison , based on bayes factors , can also be applied to the problem of sequential monitoring of a single model .",
    "this approach , which is explored in detail in west and harrison ( 1997 , chapter 11 ) and in salvador and gargallo ( 2004 , 2005 , 2006 ) , proposes the adoption of a set of alternative models , compares the current model with these and makes a sequential decision adopting the best model , according to the behaviour of the bayes factor .",
    "the london metal exchange ( lme ) is the world s premier non - ferrous metals market , with highly liquid contracts",
    ". its trading customers may be metal industries or individuals ( sellers or buyers ) .",
    "the metals currently traded in the exchange are : aluminium , copper grade a , standard lead , primary nickel , tin , and zinc .",
    "more details about the lme can be found on its web site : http://www.lme.co.uk .",
    "the importance of the lme and its operations has recently invited considerable interest . here , from a statistical point of view , we mention the work of mckenzie _ et al . _",
    "( 2001 ) and the review of watkins and mcaleer ( 2004 ) .",
    "triantafyllopoulos ( 2006a ) gives a brief account to the statistical work on the lme .    in this paper",
    "we concentrate on spot prices of four metals exchanged in the lme , namely aluminium , copper , lead and zinc .",
    "we have 4 variables of interest collected in the observation vector @xmath219'$ ] .",
    "each variable comprises the spot price per tonne of metal : @xmath220 is the spot variable , which indicates the daily / current ask price per tonne of aluminium ; the remaining three variables are the relevant spot ask prices of copper , lead and zinc , respectively .",
    "the data are collected for every trading day from 4 january 2005 to 28 april 2006 , and are plotted in figure [ fig1 ] . after excluding week - ends and bank holidays",
    ", there are @xmath221 trading days .",
    "the data have been obtained from the lme web site : http://www.lme.co.uk .      here",
    "we consider the compound return time series @xmath222 with @xmath223 , for @xmath224 .",
    "most of the current literature in econometrics is focused on modelling only the volatility of the series , but for the mv - dlms considered in this paper , one can model with the same model the returns ( for forecasting purposes ) and estimate the volatility matrix .",
    "we use the model @xmath225 where @xmath226 , @xmath227 and @xmath228 is the level of the series at time @xmath1 .",
    "the design vector @xmath229'$ ] is invariant of time and a random walk evolution for the states @xmath12 has been chosen , which is suitable for modelling the compound returns ( tsay , 2002 , cuaresma and hlouskova , 2005 ) .",
    "the volatility of the series is measured with the volatility matrix @xmath14 , which is subject to estimation",
    ". there might be some uncertainty on the dimension @xmath9 of the rows of @xmath12 , but here for parsimonious modelling we choose a low value for @xmath9 . it might be worthwhile to consider @xmath9 as random , but this can add computational delays to the estimation process .",
    "the @xmath230 evolution covariance matrix @xmath19 can be specified with two discount factors @xmath231 and @xmath232 according to the discussion in section [ model : dwr ] .",
    "however , it can be seen that since @xmath233 is time invariant model ( [ example ] ) can be decomposed as @xmath234 which is a random walk plus noise model . since @xmath235 , where @xmath236 , it can be seen that only @xmath231 has a contribution to the model and in particular model ( [ example ] ) is equivalent to a model with a single discount factor , i.e. @xmath237 and @xmath238 .",
    "so there are five discount factors of interest : @xmath239 , which is the discount factor responsible for the random walk evolution of the level @xmath228 , and @xmath240 , which are responsible for the evolution of the @xmath241 volatility matrix @xmath14 ; @xmath242 is the discount factor for the volatility of the compound series @xmath243 , where @xmath244'$ ] and @xmath245'$ ] .",
    "we specify the priors @xmath246 where @xmath247 .",
    "table [ table1 ] shows two performance measures , namely the msse and the log - likelihood function ( see section [ test ] ) .",
    "two values of @xmath239 are picked and compared with ; a small value @xmath248 ( corresponding to an adapting , but not smooth evolution for the level @xmath228 ) and a high value @xmath249 ( corresponding to a smooth evolution for the level @xmath228 ) . the me was found to be constant throughout the range of @xmath90 , but changing for the two values of @xmath239 ; for @xmath248 it was @xmath250'$ ] and for @xmath249 it was @xmath251'$ ] . from figure [ fig1 ]",
    "it is apparent that the aluminium and the zinc evolve together ( their difference appears to be in their levels ) and likewise the copper and the lead evolve together .",
    "this can be reflected in our model by choosing @xmath252 and @xmath253 so that the volatilities of say aluminium and zinc will be similar .",
    "table [ table1 ] shows the two performance measures ( msse and logl ) for a range of admissible values of @xmath88 and @xmath254 , given that @xmath255 so that the one - step forecast mean of @xmath14 exists ( see section [ estimation ] ) . for all @xmath256 and for @xmath248 , the log - likelihood function",
    "is maximized for @xmath257 @xmath258 , but this value can not be allowed , because @xmath259 .",
    "the highest value of logl is achieved for @xmath260 and @xmath261 , but this produces poor performance in the msse .",
    "our choice is for @xmath262 and @xmath263 , returning reasonable values of the msse and a not very low value for the logl .",
    "when comparing the performance of the models for the discount factors @xmath248 and @xmath249 , we note that the log - likelihood function corresponding to @xmath249 is smaller than that of @xmath248 .",
    "similarly the me produced with @xmath249 is too large and the msse does not achieve a decent value for all four variables .",
    "therefore we conclude that a high discount factor @xmath239 should not be chosen .    table [ table1 ] also reveals that a choice of @xmath264 is inadequate , leading to poor performance in the msse .",
    "it is clear that there are two main factors driving the volatilities of the metals and these factors are expressed here by the two discount factors @xmath88 and @xmath254 .",
    "the log - likelihood function for @xmath265'$ ] ( e.g. when @xmath150 ) , is @xmath266 when the formula of theorem [ th2 ] is used ( due to the infinity at the value of @xmath267 ) , but this likelihood is just -10344.66 when formula ( [ loglsmall ] ) is used . the fact that this log - likelihood appears to be the maximum likelihood , is due to the fact that in the likelihood ( [ loglsmall ] ) the part of @xmath268 does not appear .",
    "likelihood ( [ loglsmall ] ) should only be used when there is strong evidence to suggest that the volatility is constant , which clearly is not the case in this data set .",
    ".mean square one - step forecast standardized errors ( msse ) and log - likelihood function ( logl ) evaluated at the posterior mean @xmath210 , where @xmath269'$ ] . [ cols=\"^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "figure [ fig3 ] shows the one - step forecast of the volatilities ( diagonal elements of @xmath14 ) and figure [ fig4 ] shows the respective forecasts of the correlations of @xmath14 .",
    "figure [ fig3 ] illustrates that the volatilities of aluminium and zinc have a similar pattern and the volatilities of copper and lead have a similar pattern .",
    "copper and zinc appear to be the most volatile and this is expected if we look at figure [ fig1 ] , where the trends of copper and zinc are less smooth than those of aluminium and zinc .",
    "figure [ fig4 ] confirms that the aluminium and the zinc are more correlated than the aluminium and the lead .",
    "this figure also indicates that the correlations are not very high in modulus .    from figure [ fig1 ]",
    "we can clearly see that the aluminium and the zinc are locally co - integrated of order 1 , and the copper and lead are also locally co - integrated of order 1 .",
    "here we use the term _ locally co - integrated _ of order @xmath9 to indicate that a linear combination of each of the two variables are , after @xmath9 steps of integration , locally stationary ( in the sense that for a time period , known also as _ regime _ the time series is weakly stationary ) . the aluminium and the copper are not co - integrated and the same applies for the copper and zinc .",
    "this fact is apparent in the volatilities ( figure [ fig3 ] ) and in the model this is reflected by the choice of two distinct elements in the discount matrix @xmath90 , i.e @xmath252 and @xmath253 . there are two distinct factors driving the volatilities of the four metals and a factor volatility model could be applied to reduce the complexity ( aguilar and west , 2000 , tsay , 2002 ,  9.4 ) .",
    "this paper develops a new bayesian procedure for estimation and forecasting of multivariate volatility .",
    "it is proposed that the evolution of the unknown volatility covariance matrix is modelled with a multiplicative stochastic model , based on wishart and singular multivariate beta distributions .",
    "the resulting algorithm is capable to estimate the volatility element by element .",
    "this is achieved by employing variance discounting using several discount factors and thus allowing different volatilities to be discounted at different rates .    in the last two decades",
    "many models have been developed for multivariate volatility estimation ( see section [ intro ] ) .",
    "here we provide a discussion of the advantages of our proposal compared to the multivariate garch ( mgarch ) models , reviewed in bauwens _",
    "some of the mgarch models result as generalizations of univariate garch models ( e.g. the vec , the constant - correlation garch , and the bekk models , see also section [ intro ] ) . from these models",
    "the constant - correlation garch model makes the strong and usually unrealistic assumption of a constant correlation matrix , whilst the vec and even the bekk have too many parameters to estimate .",
    "the large number of parameters to be estimated , restrict these models to applications of relatively low dimensions , usually not exceeding @xmath270 .",
    "the factor garch models ( e.g the factor - bekk model ) overcomes this difficulty , but in practice the specification of the factors is not simple ( tsay , 2002 ,  9.4 ) . the dynamic - correlation models ( bauwens _ et al .",
    "_ , 2006 ; audrino and barone - adesi , 2006 ) aim to combine the flexibility of the constant - correlation garch , but to overcome the main drawback of that model by introducing a specific time - dependent structure on the correlation matrix .",
    "this can be done in several ways , but its main drawback is that , if the dimension of the parameters is to be manageable , the correlation matrix is driven by scalar parameters , which means that all correlations have the same weight of change . perhaps , this is not a major issue for bivariate time series data , but for higher dimensions it is unlikely to hold true . in our mv - dlm model",
    "we overcome this problem by introducing the matrix of discount factors @xmath90 and by discounting the volatilities and the corresponding correlations at different rates .",
    "the usual setting of a mgarch model is that of @xmath271 , where @xmath228 is the level of the series @xmath6 ( usually the series will be the compound returns of some assets or exchange rates ) , with @xmath15 being the innovation series , following @xmath272 and @xmath14 represents the volatility matrix subject to estimation . while it is recognized that the volatility can affect the level , in some mgarch studies the level is time - invariant ( bauwens _ et al .",
    "_ , 2006 ) , and in some other studies the level is assumed to have a simple evolution , e.g. to follow an autoregressive model of order one ( audrino and barone - adesi , 2006 ) . in the latter case estimation",
    "is usually performed separately in the ar and garch components , which may not be desirable for on - line forecasting .",
    "our proposed model does in fact allow for much more complicated structure in @xmath228 , through @xmath273 and through the evolution equation of @xmath12 , see equation ( [ model2 ] ) .",
    "this can include structural characteristics such as trend and seasonal components and applying the principle of superposition of state space models ( west and harrison , 1997 , chapter 6 ) , one can build complex multivariate time series models , for which estimation of the states is accompanied by simultaneous estimation of the volatility .",
    "this is not achievable , by neither arima type models , nor by mgarch models alone . in order to build such models",
    "one has to consider a multivariate arima model , with errors following mgarch models . in such models",
    "there are inferential problems regarding to estimation and in the literature simple models have been considered ; for a univariate discussion on this topic see fiorentini and maravall ( 1996 ) and audrino and barone - adesi ( 2006 ) .",
    "an important issue , which is discussed in bauwens _",
    "( 2006 ) , is that of marginalization .",
    "if @xmath6 follows a mgarch model , the question is whether @xmath274 , follows the same type of mgarch model , where @xmath275 is a @xmath276 matrix of constants .",
    "this is an important problem , because if the model is closed under linear transformations ( or else if it is invariant under linear transformations ) , then one can easily study the volatility of a linear combination of some assets , for example to estimate the volatility of a portfolio and hence the value at risk of a portfolio . as pointed out in bauwens _",
    "( 2006 ) not all garch models are invariant under the above linear transformation .",
    "the mv - dlm is invariant , under some regulatory assumptions .",
    "consider model ( [ model2 ] ) and define @xmath275 a @xmath276 matrix of rank @xmath124 .",
    "then , we can write @xmath277 where @xmath278 , @xmath279 , @xmath280 , @xmath281 and the remaining components of the model is as in ( [ model2 ] ) .",
    "although , in the above model we can not obtain an explicit formula for the precision @xmath282 , it is clear that using distribution theory , we can establish that the linear transformation @xmath274 follows a mv - dlm with dimensions @xmath124 and @xmath9 .",
    "for example , we can readily see that from the posterior @xmath283 we have @xmath284 , where @xmath285 .",
    "it follows that all scalar @xmath286 , with @xmath287'$ ] , follow univariate dlms of the form of west and harrison ( 1997 ,  10.8 ) and the posterior distributions of the diagonal elements @xmath288 of @xmath289 are inverted gamma .",
    "the bayesian estimation approach of the mv - dlms is preferred to the usual maximum likelihood estimation approach of most of the mgarch models or to bayesian estimation based on monte carlo simulation .",
    "the proposed bayesian approach is delivered in closed form and thus it is available for on - line estimation . in the maximum likelihood estimation approach , adopted in many mgarch models ,",
    "given a sample , the aim is to estimate a set of parameters , sometimes a reasonably large number of them and sometimes the maximization will be computationally expensive and time consuming . this procedure may not be suitable for sequential application , since the parameters and their estimates seem to lose one of their dynamic power , which is to adapt and to update as new information comes in .",
    "our model is adaptive to new information and it is computationally cheap , which makes it suitable for volatility estimation of high dimensional data .",
    "i am grateful to giovanni montana and to tony ohagan , for several helpful comments and suggestions on an earlier draft of the paper .",
    "i wish to thank an anonymous referee , for providing detailed comments that led to a considerably improved version of the paper .",
    "in this appendix we detail the proofs of arguments in sections [ estimation ] and [ test ] . we begin with the prior distribution ( [ prior : v ] ) .",
    "[ pro1 ] consider model ( [ model2 ] ) with the priors ( [ dwr3 ] ) and the evolution ( [ vol : evol ] ) .",
    "let the posterior precision at time @xmath0 be @xmath290 . then",
    ", with the prior degrees of freedom @xmath70 , the prior distribution of @xmath61 is @xmath291 .",
    "the proof of the maximization of the log - likelihood function requires matrix - differentiation , in particular , first and second order differentiation in terms of @xmath151 .",
    "here we follow the matrix - differentiation notation of harville ( 1997 ) and the proof mimics the early work on log - likelihood maximization of harvey ( 1986 , 1989 ,  8.3 ) .",
    "an alternative proof can be obtained by employing the log - likelihood maximization procedure , used for var models , of ltkepohl ( 1993 , pages 80 - 82 ) .    with the posterior ( [ dwr5a ] ) , the forecast distribution of @xmath293 is @xmath294 , where @xmath295 and @xmath296 and @xmath297 are defined in section [ estimation ] .",
    "the log likelihood of @xmath151 is @xmath298 taking the first derivative of @xmath299 we get @xmath300 and this leads to @xmath301 since @xmath302 to prove that the second partial derivative of @xmath303 with respect to @xmath151 is a negative definite matrix , first we show that the second partial derivative of @xmath304 with respect to @xmath38 is a negative definite matrix . denote with @xmath305 the duplication matrix ( i.e. @xmath306 , where @xmath307 is the column stacking operator of a lower portion of a symmetric matrix ) and write @xmath308 to be any left inverse of @xmath305 ( i.e. @xmath309 ) .",
    "one choice for @xmath308 is @xmath310 .",
    "for any vector @xmath311 , let @xmath312 denote the diagonal matrix with diagonal elements the elements of @xmath311 .",
    "write @xmath313 and @xmath314 .",
    "from equation ( [ app : var ] ) we have @xmath315h_p(\\sigma\\otimes\\sigma ) d_p<0,\\label{app : var2}\\end{aligned}\\ ] ] which is a negative definite matrix , since both @xmath316 and @xmath317 are positive definite .    now using the chain rule for matrix differentiation we have @xmath318 and at @xmath319 we have that @xmath320 which from ( [ app : var2 ] )",
    "is a negative definite matrix and so @xmath321 maximizes the log - likelihood function @xmath322 .",
    "[ lemma ] suppose that the @xmath18 matrix @xmath323 follows the singular multivariate beta distribution @xmath324 , with density @xmath325 , @xmath326 , @xmath327 is the diagonal matrix with diagonal elements the positive eigenvalues of @xmath328 , and @xmath329 is a matrix with orthogonal columns , i.e. @xmath330 . for any non - singular matrix @xmath275 , the density of @xmath331 , is @xmath332 .",
    "first note that @xmath95 is a non - singular matrix and @xmath333 . from daz - garca and gutirrez ( 1997 ) ,",
    "the jacobian of @xmath323 with respect to @xmath95 is @xmath334 where @xmath327 is defined as in the theorem .",
    "then from the singular multivariate beta density of @xmath323 we obtain latexmath:[\\[\\begin{aligned } p(x)&=&\\pi^{(n^2-pn)/2 } \\frac{\\gamma_p\\{(m+n)/2\\}}{\\gamma_n(n/2)\\gamma_p(m/2 ) } |a|^n      first we derive the likelihood function @xmath336 .",
    "we have @xmath337 by bayes theorem the last part of the right hand side is @xmath338 and so applying the last equation repeatedly we have @xmath339 the density @xmath340 is a multivariate normal density , since from the kalman filter @xmath341 .",
    "the density @xmath342 is the density @xmath94 of lemma [ lemma ] with @xmath343 , @xmath344 , @xmath205 and @xmath345 .",
    "the required formula of the log - likelihood function is obtained from ( [ logl1 ] ) by taking the logarithm of @xmath336 , for @xmath346 .",
    "daz - garca , j.a . and gutirrez , j.r .",
    "( 1997 ) proof of the conjectures of h. uhlig on the singular multivariate beta and the jacobian of a certain matrix transformation .",
    "_ annals of statistics _ , * 25 * , 2018 - 2023 .",
    "salvador , m. and gargallo , p. ( 2006 ) . automatic detection and identification of shocks in gaussian state - space models : a bayesian approach .",
    "_ applied stochastic models in business and industry _ * 22 * , 17 - 39 .",
    "tse , y.k . and",
    "tsui , a.k.c .",
    "( 2002 ) a multivariate generalized autoregressive conditional heteroscedasticity model with time - varying correlations .",
    "_ journal of business and economic statistics _ ,",
    "* 20 * , 351 - 362 ."
  ],
  "abstract_text": [
    "<S> this paper develops a bayesian procedure for estimation and forecasting of the volatility of multivariate time series . </S>",
    "<S> the foundation of this work is the matrix - variate dynamic linear model , for the volatility of which we adopt a multiplicative stochastic evolution , using wishart and singular multivariate beta distributions . </S>",
    "<S> a diagonal matrix of discount factors is employed in order to discount the variances element by element and therefore allowing a flexible and pragmatic variance modelling approach . </S>",
    "<S> diagnostic tests and sequential model monitoring are discussed in some detail . </S>",
    "<S> the proposed estimation theory is applied to a four - dimensional time series , comprising spot prices of aluminium , copper , lead and zinc of the london metal exchange . </S>",
    "<S> the empirical findings suggest that the proposed bayesian procedure can be effectively applied to financial data , overcoming many of the disadvantages of existing volatility models .    _ </S>",
    "<S> some key words : _ time series , volatility , multivariate , dynamic linear model , bayesian , forecasting , state space , kalman filter , garch , london metal exchange . </S>"
  ]
}