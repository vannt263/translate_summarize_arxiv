{
  "article_text": [
    "there is multiple converging evidence @xcite that synapses determine the complex processing of information in the brain .",
    "an aspect of this statement is illustrated by attractor neural networks .",
    "these show that synapses can efficiently store patterns that are afterwards retrieved with only partial information on them .",
    "in addition to this long  time effect , artificial neural networks should contain some synaptic noise , however .",
    "that is , actual synapses exhibit short  time fluctuations , which seem to compete with other mechanisms during the transmission of information , not to cause unreliability but to ultimately determine a variety of computations @xcite . in spite of some recent efforts , a full understanding of how the brain complex processes depend on such fast synaptic variations is lacking see below and @xcite ,",
    "for instance. a specific matter under discussion concerns the influence of short  time noise on the fixed points and other details of the retrieval processes in attractor neural networks @xcite .",
    "the observation that actual synapses endure short  time _ depression _ and/or _",
    "facilitation _ is likely to be relevant in this context .",
    "that is , one may understand some observations by assuming that periods of elevated presynaptic activity may cause either decrease or increase of the neurotransmitter release and , consequently , that the postsynaptic response will be either _ depressed _ or _ facilitated _ depending on presynaptic neural activity @xcite .",
    "motivated by the neurobiological findings , we report in this paper on effects of presynaptic depressing noise on the functionality of a neural circuit .",
    "we study in detail a network in which the neural activity evolves at random in time regulated by a temperature  parameter .",
    "in addition , the values assigned to the synaptic intensities by a _ learning _",
    "( e.g. , hebb s ) rule are constantly perturbed with _ microscopic",
    "_ fast noise .",
    "a new parameter is involved by this perturbation that allows for a continuum transition from depression to normal operation .    as a main result",
    ", this paper illustrates that , in general , the addition of fast synaptic noise induces a nonequilibrium condition .",
    "that is , our systems can not asymptotically reach equilibrium but tend to nonequilibrium steady states whose features depend , even qualitatively , on dynamics @xcite .",
    "this is interesting because , in practice , thermodynamic equilibrium is rare in nature . instead , the simplest conditions one observes are characterized by a steady flux of energy or information , for instance .",
    "this makes the model mathematically involved , e.g. , there is no general framework such as the powerful ( equilibrium ) gibbs theory , which only applies to systems with a single kelvin temperature and a unique hamiltonian .",
    "however , our system still admits analytical treatment for some choices of its parameters and , in other cases , we discovered the more intricate model behavior by a series of computer simulations .",
    "we thus show that fast presynaptic depressing noise during external stimulation may induce the system to scape from the attractor , namely , the stability of fixed point solutions is dramatically modified .",
    "more specifically , we show that , for certain versions of the system , the solution destabilizes in such a way that computational tasks such as class identification and categorization are favored .",
    "it is likely this is the first time such a behavior is reported in an artificial neural network as a consequence of biologically  motivated stochastic behavior of synapses .",
    "similar instabilities have been reported to occur in monkeys @xcite and other animals @xcite , and they are believed to be a main feature in odor encoding @xcite , for instance .",
    "our interest is in a neural network in which a local stochastic dynamics is constantly influenced by presynaptic _ noise_. consider a set of @xmath0 binary neurons with configurations @xmath1",
    "any two neurons are connected by synapses of intensity : in ( [ ws ] ) instead of @xmath2 would impede some of the algebra in sections [ sect3 ] and [ sect4 ] . ]",
    "@xmath3 here , @xmath4 is fixed , namely , determined in a previous _ learning _ process , and @xmath2 is a stochastic variable .",
    "this generalizes the hypothesis in previous studies of attractor neural networks with noisy synapses ; see , for instance , @xcite .",
    "once @xmath5 is given , the state of the system at time @xmath6 is defined by setting @xmath7 and @xmath8 these evolve with time after the learning process which fixes @xmath9 via the familiar master equation , namely , @xmath10   \\nonumber \\\\",
    "+ \\int_{\\mathbf{{x^{\\prime } } } } \\sum_{\\mathbf{{s^{\\prime } } } } \\,c[(\\mathbf{{\\ s^{\\prime } } } , \\mathbf{{x^{\\prime } } } ) \\rightarrow ( \\mathbf{s},\\mathbf{x } ) ] p_{t}(\\mathbf{{s^{\\prime } } } , \\mathbf{{x^{\\prime } } } ) .   \\label{gme}\\end{aligned}\\ ] ] we further assume that the _ transition rate _ or probability per unit time of evolving from @xmath11 to @xmath12 is @xmath13=p\\,c^{\\mathbf{x}}[\\mathbf{s}\\rightarrow \\mathbf{{s^{\\prime } } } ] \\delta ( \\mathbf{x}-\\mathbf{{x^{\\prime } } } )   \\nonumber \\\\   + ( 1-p)\\,c^{\\mathbf{s}}[\\mathbf{x } \\rightarrow \\mathbf{{x^{\\prime } } } ] \\delta _ { \\mathbf{s},\\mathbf{{s^{\\prime } } } } .",
    "\\label{gtr}\\end{aligned}\\ ] ] this choice @xcite amounts to consider competing mechanisms .",
    "that is , neurons @xmath14 evolve stochastically in time under a noisy dynamics of synapses @xmath15 the latter evolving @xmath16 times faster than the former . depending on the value of @xmath17 three main classes",
    "may be defined @xcite :    1 .   for @xmath18",
    "both the synaptic fluctuation and the neuron activity occur on the same temporal scale .",
    "this case has already been preliminary explored @xcite .",
    "the limiting case @xmath19 this corresponds to neurons evolving in the presence of a quenched synaptic configuration , i.e. , @xmath20 is constant and independent of @xmath21 the _ hopfield model _",
    "@xcite belongs to this class in the simple case that @xmath22 3 .",
    "the limiting case @xmath23 the rest of this paper is devoted to this class of systems .",
    "our interest for the latter case is a consequence of the following facts .",
    "firstly , there is adiabatic elimination of fast variables for @xmath24 which decouples the two dynamics @xcite .",
    "therefore , some exact analytical treatment though not the complete solution is then feasible . to be more specific , for @xmath25",
    "the neurons evolve as in the presence of a steady distribution for @xmath26 if we write @xmath27 where @xmath28 stands for the conditional probability of @xmath29 given @xmath30 one obtains from ( [ gme ] ) and ( [ gtr ] ) , after rescaling time @xmath31 ( technical details are worked out in @xcite , for instance ) that @xmath32 \\nonumber \\\\ + \\sum_{\\mathbf{{\\ s^{\\prime } } } } \\bar{c}[\\mathbf{{s^{\\prime } } } \\rightarrow   \\mathbf{s}]p_{t}(\\mathbf{{s^{\\prime } } } ) .",
    "\\label{geme}\\end{aligned}\\ ] ] here , @xmath33\\equiv",
    "\\int \\mathrm{d } \\mathbf{x}\\,p^{\\mathrm{st}}(\\mathbf{x}|\\mathbf{s})\\,c^{\\mathbf{x}}[\\mathbf{s } \\rightarrow \\mathbf{{s^{\\prime } } } ] ,   \\label{getr}\\ ] ] and @xmath34 is the stationary solution that satisfies @xmath35\\,p^{\\mathrm{st}}(\\mathbf{{x^{\\prime } } } |\\mathbf{s})}{\\int \\mathrm{d } \\mathbf{{x^{\\prime } } } \\,c^{\\mathbf{s}}[\\mathbf{x}\\rightarrow \\mathbf{{\\ x^{\\prime } } } ] } .",
    "\\label{pst}\\ ] ] this formalism will allows us for modelling fast synaptic noise which , within the appropiate context , will induce sort of synaptic depression , as explained in detail in section [ sect4 ] .",
    "the superposition ( [ getr ] ) reflects the fact that activity is the result of competition between different elementary mechanisms .",
    "that is , different underlying dynamics , each associated to a different realization of the stochasticity @xmath36 compete and , in the limit @xmath25 an _ effective _ rate results from combining * @xmath37 $ ] * with probability * @xmath34 * for varying @xmath26 each of the elementary dynamics tends to drive the system to a well - defined equilibrium state .",
    "the competition will , however , impede equilibrium and , in general , the system will asymptotically go towards a _ nonequilibrium _ steady state @xcite .",
    "the question is if such a competition between synaptic noise and neural activity , which induces nonequilibrium , is at the origin of some of the computational strategies in neurobiological systems .",
    "our study below seems to indicate that this is a sensible issue . as a matter of fact",
    ", we shall argue below that @xmath24 may be realistic _ a priori _ for appropriate choices of * @xmath38 *    for the sake of simplicity , we shall be concerned in this paper with sequential updating by means of single neuron or spin ",
    "flip  dynamics .",
    "that is , the elementary dynamic step will simply consist of local inversions @xmath39 induced by a bath at temperature @xmath40 the elementary rate @xmath41 $ ] then reduces to a single site rate that one may write as @xmath42.$ ] here , @xmath43 where @xmath44 is the net presynaptic current arriving to or local field acting on the ( postsynaptic ) neuron @xmath21 the function @xmath45 is arbitrary except that , for simplicity , we shall assume @xmath46 @xmath47 and @xmath48 @xcite .",
    "we shall report on the consequences of more complex dynamics in a forthcomming paper @xcite .",
    "let us define a function @xmath49 through the condition of detailed balance , namely , @xmath50}{\\bar{c}[\\mathbf{s } ^{i}\\rightarrow \\mathbf{s}]}=\\exp \\left\\ { -\\left [ h^{\\mathrm{eff}}(\\mathbf{s } ^{i})-h^{\\mathrm{eff}}(\\mathbf{s})\\right ] t^{-1}\\right\\ } .",
    "\\label{heff}\\]]here , @xmath51 stands for @xmath7 after flipping at @xmath52 @xmath53 we further define the effective local fields  @xmath54 by means of @xmath55 nothing guaranties that @xmath49 and @xmath56 have a simple expression and are therefore analytically useful .",
    "this is because the superposition ( [ getr ] ) , unlike its elements @xmath57 does not satisfy detailed balance , in general . in other words ,",
    "our system has an essential nonequilibrium character that prevents one from using gibbs s statistical mechanics , which requires a unique hamiltonian .",
    "instead , there is here one energy associated with each realization of @xmath58 this is in addition to the fact that the synaptic weights @xmath59 in ( [ ws ] ) may not be symmetric .    for some choices of",
    "both the rate @xmath60 and the noise distribution @xmath61 the function @xmath62 may be considered as a true effective hamiltonian @xcite .",
    "this means that @xmath49 then generates the same nonequilibrium steady state than the stochastic time ",
    "evolution equation which defines the system , i.e. , equation ( [ geme ] ) , and that its coefficients have the proper symmetry of interactions . to be more explicit ,",
    "assume that * * @xmath63**@xmath64 factorizes according to @xmath65 and that one also has the factorization @xmath66=\\prod_{j\\neq i}\\int \\mathrm{d } x_{j}\\,p(x_{j}|s_{j})\\,\\psi ( 2t^{-1}s_{i}\\overline{w}_{ij}x_{j}s_{j } ) .",
    "\\label{ceff}\\ ] ] the former amounts to neglect some global dependence of the factors on @xmath67 ( see below ) , and the latter restricts the possible choices for the rate function .",
    "some familiar choices for this function that satisfy detailed balance are : the one corresponding to the metropolis algorithm , i.e. , @xmath68;$ ] the glauber case @xmath69^{-1};$ ] and @xmath70 @xcite . the latter fulfills @xmath71 which is required by ( [ ceff ] ) . by its maximum value .",
    "therefore , the normalization happens to depend on temperature and on the number of stored patterns .",
    "it follows that this normalization is irrelevant for the properties of the steady state , namely , it just rescales the time scale . ]",
    "it then ensues after some algebra that @xmath72 ,   \\label{gelf}\\ ] ] with @xmath73 where @xmath74 and @xmath75 this generalizes a case in the literature for random @xmath7 independent fluctuations @xcite . in this case",
    ", one has @xmath76 and , consequently , @xmath77 @xmath78 however , we here are concerned with the case of @xmath7dependent disorder , which results in a non  zero threshold , @xmath79    in order to obtain a true effective hamiltonian , the coefficients @xmath80 in ( [ gelf ] ) need to be symmetric . once @xmath45 is fixed , this depends on the choice for @xmath81 i.e. , on the fast noise details .",
    "this is studied in the next section . meanwhile",
    ", we remark that the effective local fields @xmath82 defined above are very useful in practice .",
    "that is , they may be computed at least numerically for any rate and noise distribution .",
    "as far as @xmath71 and @xmath83 factorizes , as in ( [ prod ] ) . the same result ( [ etr2 ] ) holds for the choice that we shall introduce in the next section , for instance .",
    "] it follows an effective transition rate as @xmath84=\\exp \\left ( -s_{i}h_{i}^ { \\mathrm{eff}}/t\\right ) .",
    "\\label{etr2}\\ ] ] this effective rate may then be used in computer simulation , and it may also serve to be substituted in the relevant equations .",
    "consider , for instance , the _ overlaps _ defined as the product of the current state with one of the stored patterns : @xmath85 here , @xmath86 are @xmath87 random patterns previously stored in the system , @xmath88 after using standard techniques @xcite ; see also @xcite , it follows from ( [ geme ] ) that @xmath89 which is to be averaged over both thermal noise and pattern realizations .",
    "alternatively , one might perhaps obtain dynamic equations of type ( [ kinetic ] ) by using fokker - planck like formalisms as , for instance , in @xcite .",
    "the above discussion and , in particular , equations ( [ gelf ] ) and ( [ alphas ] ) , suggest that the system emergent properties will importantly depend on the details of the synaptic noise  @xmath26 we now work out the equations in section [ sect3 ] for different hypothesis concerning the stationary distribution ( [ pst ] ) .",
    "consider first ( [ prod ] ) with the following specific choice : @xmath90 this corresponds to a simplification of the stochastic variable @xmath91 that is , for @xmath92 @xmath93 the noise modifies @xmath4 by a factor @xmath94 when the presynaptic neuron is firing , @xmath95 while the learned synaptic intensity remains unchanged when the neuron is silent . in general , @xmath96 with probability @xmath97 @xmath98 here , @xmath99 stands for some information concerning the presynaptic site @xmath100 such as , for instance , a local threshold or @xmath101    our interest for case ( [ bimod ] ) is two fold , namely , it corresponds to an exceptionally simple situation and it reduces our model to two known cases .",
    "this becomes evident by looking at the resulting local fields : @xmath102 \\overline{w}_{ij}.   \\label{locelf}\\ ] ] that is , exceptionally , symmetries here are such that the system is described by a _",
    "true _ effective hamiltonian .",
    "furthermore , this corresponds to the hopfield model , except for a rescaling of temperature and for the emergence of a threshold @xmath103 @xcite . on the other hand",
    ", it also follows that , concerning stationary properties , the resulting effective hamiltonian ( [ eh ] ) reproduces the model as in @xcite .",
    "in fact , this would correspond in our notation to @xmath104 where @xmath105 stands for the stationary solution of certain dynamic equation for @xmath91 the conclusion is that ( except perhaps concerning dynamics , which is something worth to be investigated ) the fast noise according to ( [ prod ] ) with ( [ bimod ] ) does not imply any surprising behavior . in any case , this choice of noise illustrates the utility of the effective  field concept as defined above .",
    "our interest here is in modeling the noise consistent with the observation of short - time synaptic depression @xcite .",
    "in fact , the case ( [ bimod ] ) in some way mimics that increasing the mean firing rate results in decreasing the synaptic weight . with the same motivation , a more intriguing behavior ensues by assuming , instead of ( [ prod ] ) , the factorization @xmath106 with @xmath107 \\mathrm { \\ } \\delta ( x_{j}-1 ) .   \\label{genbimod}\\ ] ] here",
    ", @xmath108 is the @xmath87-dimensional overlap vector , and @xmath109 stands for a function of @xmath110 to be determined .",
    "the depression effect here depends on the overlap vector which measures the net current arriving to postsynaptic neurons .",
    "the non  local choice ( [ xp])([genbimod ] ) thus introduces non  trivial correlations between synaptic noise and neural activity , which is not considered in ( [ bimod ] ) .",
    "note that , therefore , we are not modelling here the synaptic depression dynamics in an explicity way as , for instance , in @xcite .",
    "instead , equation ( [ genbimod ] ) amounts to consider fast synaptic noise which naturally depresses the strengh of the synapses after repeated activity , namely , for a high value of @xmath111    several further comments on the significance of ( [ xp])-([genbimod ] ) , which is here a main hypothesis together with @xmath25 are in order .",
    "we first mention that the system time relaxation is typically orders of magnitude larger than the time scale for the various synaptic fluctuations reported to account for the observed high variability in the postsynaptic response of central neurons @xcite . on the other hand ,",
    "these fluctuations seem to have different sources such as , for instance , the stochasticity of the opening and closing of the vesicles ( s. kilfiker , private communication ) , the stochasticity of the postsynaptic receptor , which has its own several causes , variations of the glutamate concentration in the synaptic cleft , and differences in the potency released from different locations on the active zone of the synapses @xcite .",
    "is this complex situation the one that we try to capture by introducing the stochastic variable @xmath112 in ( [ ws ] ) and subsequent equations .",
    "it may be further noticed that the nature of this variable , which is `` microscopic '' here , differs from the one in the case of familiar phenomenological models .",
    "these often involve a `` mesoscopic '' variable , such as the mean fraction of neurotransmitter , which results in a deterministic situation , as in @xcite .",
    "the depression in our model rather naturally follows from the coupling between the synaptic `` noise '' and the neurons dynamics via the overlap functions .",
    "the final result is also deterministic for @xmath24 but only , as one should perhaps expect , on the time scale for the neurons . finally , concerning also the reality of the model",
    ", it should be clear that we are restricting ourselves here to fully connected networks just for simplicity .",
    "however , we already studied similar systems with more realistic topologies such as scale - free , small - world and diluted networks @xcite , which suggests one to generalize the present study in this sense .",
    "it is to be remarked that our case ( [ xp])-([genbimod ] ) also reduces to the hopfield model but only in the limit @xmath113 for any @xmath114 otherwise , the competition results in a rather complex behavior . in particular , the noise distribution @xmath115 lacks with ( [ genbimod ] ) the factorization property which is required to have an effective hamiltonian with proper symmetry .",
    "nevertheless , we may still write @xmath50}{\\bar{c}[\\mathbf{s } ^{i}\\rightarrow \\mathbf{s}]}=\\prod_{j\\neq i}\\frac{\\int { d}x_{j}\\,p(x_{j}| \\mathbf{s})\\,\\psi ( s_{i}x_{j}s_{j}\\beta _ { ij})}{\\int { d}x_{j}\\,p(x_{j}| \\mathbf{s}^{i})\\,\\psi ( -s_{i}x_{j}s_{j}\\beta _ { ij})}.   \\label{db2}\\ ] ] then , using ( [ genbimod ] ) , we linearize around @xmath116 i.e. , @xmath117 for @xmath118 this is a good approximation for the hebbian learning rule @xcite @xmath119 which is the one we use hereafter , as far as this rule only stores completely uncorrelated , random patterns .",
    "in fact , fluctuations in this case are of order @xmath120 for finite @xmath87 ( or order @xmath121 for finite @xmath122 ) which tends to vanish for a sufficiently large system , e.g. , in the macroscopic ( thermodynamic ) limit @xmath123 it then follows the effective weights : @xmath124 \\right\\ } \\overline{w}_{ij } ,   \\label{weff}\\ ] ] where @xmath125 @xmath126 and @xmath127 is the binary @xmath87dimensional stored pattern .",
    "this shows how the noise modifies synaptic intensities .",
    "the associated effective local fields are @xmath128 the condition to obtain a true effective hamiltonian , i.e. , proper symmetry of ( [ weff ] ) from this , is that @xmath129 this is a good approximation in the thermodynamic limit , @xmath123    otherwise , one may proceed with the dynamic equation ( [ kinetic ] ) after substituting ( [ gloelf ] ) , even though this is not then a true effective hamiltonian .",
    "one may follow the same procedure for the hopfield case with asymmetric synapses @xcite , for instance .",
    "further interest on the concept of local effective fields as defined in section [ sect3 ] follows from the fact that one may use quantities such as ( [ gloelf ] ) to importantly simplify a computer simulation , as it is made below .    to proceed further",
    ", we need to determine the probability @xmath130 in ( [ genbimod ] ) . in order to model activity  dependent mechanisms acting on the synapses , @xmath131should be an increasing function of the net presynaptic current or field .",
    "in fact , @xmath132 simply needs to depend on the overlaps , besides to preserve the @xmath133 symmetry .",
    "a simple choice with these requirements is @xmath134 ^{2 } ,   \\label{psi}\\ ] ] where @xmath135 we describe next the behavior that ensues from ( [ weff])([psi ] ) as implied by the noise distribution ( [ genbimod ] ) .",
    "let us first study the retrieval process in a system with a single stored pattern , @xmath136 when the neurons are acted on by the local fields ( [ gloelf ] ) .",
    "one obtains from ( [ etr2])([kinetic ] ) , after using the simplifying ( mean - field ) assumption @xmath137 that the steady solution corresponds to the overlap : @xmath138 \\right\\ } ,   \\label{stoe}\\ ] ] @xmath139 which preserves the symmetry @xmath140 local stability of the solutions of this equation requires that @xmath141    the behavior ( [ stoe ] ) is illustrated in figure [ fig1 ] for several values of @xmath142 this indicates a transition from a _",
    "ferromagnetic  like _ phase , i.e. , solutions @xmath143 with associative memory , to a _",
    "paramagnetic  like _ phase , @xmath144 the transition is continuous or second order only for @xmath145 and it then follows a critical temperature @xmath146 figure [ fig2 ] shows the tricritical point at @xmath147 and the general dependence of the transition temperature with @xmath142    it is to be remarked that a discontinuous phase transition allows for a much better performance of the retrieval process than a continuous one .",
    "this is because the behavior is sharp just below the transition temperature in the former case .",
    "consequently , the above indicates that our model performs better for large negative @xmath148 @xmath149    we also performed monte carlo simulations .",
    "these concern a network of @xmath150 neurons acted on by the local fields ( [ gloelf ] ) and evolving by sequential updating via the effective rate ( [ etr2 ] ) .",
    "except for some finite  size effects , figure [ fig1 ] shows a good agreement between our simulations and the equations here ; in fact , the computer simulations also correspond to a mean  field description given that the fields ( [ gloelf ] ) assume fully connected neurons .",
    "as shown above , a noise distribution such as ( [ genbimod ] ) may model activity - dependent processes reminiscent of short - time synaptic depression . in this section ,",
    "we study the consequences of this type of fast noise on the retrieval dynamics under external stimulation .",
    "more specifically , our aim is to check the resulting sensitivity of the network to external inputs .",
    "a high degree of sensibility will facilitate the response to changing stimuli .",
    "this is an important feature of neurobiological systems which continuously adapt and quickly respond to varying stimuli from the environment .",
    "consider first the case of one stored pattern , @xmath151 a simple external input may be simulated by adding to each local field a driving term @xmath152 with @xmath153 @xcite .",
    "a negative drive in this case of a single pattern assures that the network activity may go from the attractor , @xmath154 to the antipattern , @xmath155 it then follows the stationary overlap : @xmath156   \\label{stoie}\\ ] ] with @xmath157 .",
    "\\label{f}\\ ] ]    figure [ fig3 ] shows this function for @xmath158 and varying @xmath142 this illustrates two different types of behavior , namely , ( local ) stability @xmath159 and instability @xmath160 of the attractor , which corresponds to @xmath161 that is , the noise induces instability , resulting in this case in switching between the pattern and the antipattern .",
    "this is confirmed in figure [ fig4 ] by monte carlo simulations .",
    "the simulations corresponds to a network of @xmath162 neurons with one stored pattern , @xmath151 this evolves from different initial states , corresponding to different distances to the attractor , under an external stimulus @xmath163 for different values of @xmath164 the two left graphs in figure [ fig4 ] show several independent time evolutions for the model with fast noise , namely , for @xmath165 the two graphs to the right are for the hopfield case lacking the noise @xmath166 these , and similar graphs one may obtain for other parameter values , clearly demonstrate how the network sensitivity to a simple external stimulus is qualitatively enhanced by adding presynaptic noise to the system .",
    "figures [ fig5 ] and [ fig6 ] illustrate a similar behavior in monte carlo simulations with several stored patterns .",
    "figure [ fig5 ] is for @xmath167 patterns with mutual overlaps @xmath168 more specifically , each pattern consits of three equal initially white ( silent neurons ) horizontal stripes , with one of them black colored ( firing neurons ) located in a different position for each pattern .",
    "the system in this case begins with the first pattern as initial condition and , to avoid dependence on this choice , it is let to relax for 3x10@xmath169 monte carlo steps ( mcs ) .",
    "it is then perturbed by a drive @xmath170 where the stimulus @xmath171 changes @xmath172 every 6x10@xmath173 mcs .",
    "the top graph shows the network response in the hopfield case .",
    "there is no visible structure of this signal in the absence of fast noise as far as @xmath174 as a matter of fact , the depth of the basins of attraction are large enough in the hopfield model , to prevent any move for small @xmath175 except when approaching a critical point ( @xmath176 ) , where fluctuations diverge .",
    "the bottom graph depicts a qualitatively different situation for @xmath177 that is , adding fast noise in general destabilizes the fixed point for the interesting case of small @xmath178 far from criticality .",
    "figure [ fig6 ] confirms the above for patterns , e.g. @xmath179 and @xmath180 that is , this shows the response of the network in a similar simulation with 400 neurons at @xmath181 for @xmath182 random , othogonal patterns .",
    "the initial condition is again @xmath183 and the stimulus is here @xmath184 with @xmath171 changing every @xmath185 mcs .",
    "thus , we conclude that the switching phenomena is robust with respect to the type of pattern stored .",
    "the set of equations ( [ geme])([pst ] ) provides a general framework to model activity  dependent processes . motivated by the behavior of neurobiological systems",
    ", we adapted this to study the consequences of fast noise acting on the synapses of an attractor neural network with a finite number of stored patterns .",
    "we present in this paper two different scenarios corresponding to noise distributions fulfilling ( [ prod ] ) and ( [ xp ] ) , respectively .",
    "in particular , assuming a local dependence on activity as in ( [ bimod ] ) , one obtains the local fields ( [ locelf ] ) , while a global dependence as in ( [ genbimod ] ) leads to ( [ gloelf ] ) . under certain assumptions , the system in the first of these cases is described by the effective hamiltonian ( [ eh ] ) .",
    "this reduces to a hopfield system i.e . , the familiar attractor neural network without any synaptic noise with rescaled temperature and a threshold .",
    "this was already studied for a gaussian distribution of thresholds @xcite . concerning stationary properties",
    ", this case is also similar to the one in @xcite .",
    "a more intriguing behavior ensues when the noise depends on the total presynaptic current arriving to the postsynaptic neuron .",
    "we studied this case both analytically , by using a mean  field hypothesis , and numerically by a series of monte carlo simulations using single - neuron dynamics .",
    "the two approaches are fully consistent with and complement each other .",
    "our model involves two main parameters .",
    "one is the _ temperature _ @xmath186 which controls the stochastic evolution of the network activity .",
    "the other parameter , @xmath148 controls the depressing noise intensity .",
    "varying this , the system describes from normal operation to depression phenomena .",
    "a main result is that the presynaptic noise induces the occurrence of a tricritical point for certain values of these parameters , @xmath187 this separates ( in the limit @xmath188 ) first from second order phase transitions between a retrieval phase and a non  retrieval phase .",
    "the principal conclusion in this paper is that fast presynaptic _ noise _ may induce a nonequilibrium condition which results in an important intensification of the network sensitivity to external stimulation .",
    "we explicitly show that the noise may turn unstable the _ attractor _ or fixed point solution of the retrieval process , and the system then seeks for another attractor . in particular , one observes switching from the stored pattern to the corresponding antipattern for @xmath136 and switching between patterns for a larger number of stored patterns , @xmath189 this behavior is most interesting because it improves the network ability to detect changing stimuli from the environment .",
    "we observe the switching to be very sensitive to the forcing stimulus , but rather independent of the network initial state or the thermal noise .",
    "it seems sensible to argue that , besides recognition , the processes of class identification and categorization in nature might follow a similar strategy .",
    "that is , different attractors may correspond to different objects , and a dynamics conveniently perturbed by fast noise may keep visiting the attractors belonging to a class which is characterized by a certain degree of correlation between its elements @xcite .",
    "in fact , a similar mechanism seems at the basis of early olfactory processing of insects @xcite , and instabilities of the same sort have been described in the cortical activity of monkeys @xcite and other cases @xcite .    finally , we mention that the above complex behavior seems confirmed by preliminary monte carlo simulations for a macroscopic number of stored patterns , i.e. , a finite loading parameter @xmath190 on the other hand , a mean  field approximation ( see below ) shows that the storage capacity of the network is @xmath191 as in the hopfield case @xcite , for any @xmath192 while it is always smaller for @xmath193 this is in agreement with previous results concerning the effect of synaptic depression in hopfield  like systems @xcite .",
    "the fact that a positive value of @xmath194 tends to shallow the basin thus destabilizing the attractor may be understood by a simple ( mean  field ) argument which is confirmed by monte carlo simulations @xcite .",
    "assume that the stationary activity shows just one overlap of order unity .",
    "this corresponds to the _ condensed pattern ; _ the overlaps with the rest , @xmath195 stored patterns is of order of @xmath121 ( _ non  condensed patterns _ ) @xcite .",
    "the resulting probability of change of the synaptic intensity , namely , @xmath196 is of order unity , and the local fields ( [ gloelf ] ) follow as @xmath197 therefore , the storage capacity , which is computed at @xmath198 is the same as in the hopfield case for any @xmath192 and always lower otherwise .",
    "we acknowledge financial support from mcyt ",
    "feder ( project no . bfm2001 - 2841 and a _ ramn y cajal _ contract ) .",
    "laurent , g. , stopfer , m. , friedrich , r.  w. , rabinovich , m.  i. , volkovskii , a. , and abarbanel , h. d.  i. ( 2001 ) .",
    "odor encoding as an active , dynamical process : experiments , computation and theory .",
    ", 24,263297 ."
  ],
  "abstract_text": [
    "<S> we study both analytically and numerically the effect of presynaptic noise on the transmission of information in attractor neural networks . </S>",
    "<S> the noise occurs on a very short  time scale compared to that for the neuron dynamics and it produces short  time synaptic depression . </S>",
    "<S> this is inspired in recent neurobiological findings that show that synaptic strength may either increase or decrease on a short  time scale depending on presynaptic activity . </S>",
    "<S> we thus describe a mechanism by which fast presynaptic noise enhances the neural network sensitivity to an external stimulus . </S>",
    "<S> the reason for this is that , in general , the presynaptic noise induces nonequilibrium behavior and , consequently , the space of fixed points is qualitatively modified in such a way that the system can easily scape from the attractor . as a result , the model shows , in addition to pattern recognition , class identification and categorization , which may be relevant to the understanding of some of the brain complex tasks .    to appear in neural computation , 2005    corresponding author : jesus m. cortes    mailto:jcortes@ugr.es </S>"
  ]
}