{
  "article_text": [
    "fully automatic understanding of 3d scenes is of particular interest for many attractive applications that demand interaction with objects and/or primitive parts that make up the scene @xcite .",
    "such knowledge is indispensable for a robot to be able to perform fully autonomously basic interactions with its environment , like moving objects , clearing the clutter , stacking objects on top of others , or searching for objects in their likely locations .",
    "these actions require richer understanding of the scene than the per - image labels from image classification approaches or object bounding boxes provided by object detectors .",
    "we believe that a key step towards whole scene understanding is the semantic segmentation of the scene .",
    "our work brings together two established directions towards the goal of 3d scene understanding : 3d reconstruction and deep learning - based semantic segmentation .",
    "here , we exploit the inherent dependency between reconstruction and segmentation  per - frame labels are fused using their respective camera poses returned by the reconstruction system . in doing so , we particularly stress the importance of treating data coming as a video stream . on an average ,",
    "segmentations from different viewpoints , when fused , should yield a result better than a segmentation from any particular view .",
    "our system is directly related to hermans s work @xcite , who fuse per - frame segmentations obtained with randomised decision forests from rgb - d images ; they use 2d and 3d dense crfs @xcite to smooth the per - frame 2d segmentations and the fused 3d segmentation , respectively .",
    "we harness recent advances made in deep learning to obtain per - frame dense predictions . our deep architecture , inspired from @xcite , is composed of stacked autoencoders and trained modularly . for all our experiments",
    ", we use depth data as the only cue for 3d scene understanding .",
    "the motivation of using depth images is twofold : firstly , depth discontinuities are very important for object recognition as has been shown in @xcite , and secondly , the convenience in obtaining depth data .",
    "using only depth cues spares us from the complications of dealing with the infinite space of possible textures and lighting setups , making it tractable to collect a representative set of scenes in terms of scene layout and objects distribution .",
    "the challenge in this context is to investigate if depth data is a _ sufficient _ input for semantic segmentation .",
    "we make publicly available a new library  synthcam3d  consisting of a significant number of labelled synthetic 3d scenes and associated code for generating depth maps and their corresponding annotations .",
    "the scenes belong to different semantic categories and have been compiled together from various online 3d repositories @xcite , and manually annotated .",
    "large public repositories ( trimble warehouse ) of 3d cad models have existed in the past , but they have mainly served the graphics community .",
    "it is only recently that we have started to see emerging interest in synthetic data for computer vision .",
    "the advantages of synthetic 3d models can not be overstated , especially when considering scenes : once a 3d annotated model is available , it allows rendering as many 2d annotated views as desired , at any resolution and frame - rate . in comparison ,",
    "existing datasets of real data are fairly limited both in the number of annotations and the amount of data .",
    "nyuv2 @xcite provides only 795 training images for 894 classes ; hence learning any meaningful features characterising a class of objects becomes prohibitively hard .",
    "synthcam3d is particularly useful for :              * generating potentially unlimited high - quality annotated depth data for different types of scenes ( fig .",
    "[ fig : annots ] ) . * benchmarking large scale depth - only slam systems on complex scenes , by providing ground truth geometry @xcite @xcite . * enabling training generative models similar to @xcite , to learn common scene layouts and object relationships , which can then be used to synthesize more scenes effortlessly .    in the following ,",
    "we describe synthcam3d and briefly outline our system trained using data generated from the library .",
    "preliminary results show the usefulness of the proposed library for training deep architectures for semantic segmentation of real world scenes . with a careful choice of input features to our deep learning network and using depth maps raycasted by the reconstruction system , we are able to bypass the domain adaptation issues that have been observed in the past the system trained on synthetic depth data",
    "can be directly applied to segment real depth data , without the need of noise modelling at training time .",
    ".different scene categories and the number of annotated 3d models for each category . [ cols=\"<,^\",options=\"header \" , ]     synthcam3d contains 3d models from five different scene categories : bedroom , office , kitchen , living - room , and bathroom , with at least 10 annotated scenes per category .",
    "importantly , all the 3d models are in metric scale .",
    "each scene is composed of up to around 50150 objects and the complexity can be controlled algorithmically .",
    "the granularity of the annotations can be adapted by the user depending on the application , in our experiments on bedroom scenes we condensed the number of classes down to 15 for generating data and understanding only functional categories of objects .",
    "the models are provided in _ .obj _ format , together with the code and camera settings needed to set up the rendering using pov - ray . a simple opengl based gui allows the user to place virtual cameras in the synthetic scene at desired locations to generate a possible trajectory for rendering at different viewpoints . fig .",
    "[ fig : annots ] shows samples of rendered annotated views of a simple office scene .",
    "we use the popular ray - tracer povray for our rendering purposes , being inspired by the past work of handa @xcite . to render depth maps with associated annotations from the _ .obj _ models , we first need to convert the _ .obj _ models to their corresponding povray files using . then the camera extrinsic parameters are set with a 3@xmath04 matrix inside the main povray file ( having the _ .pov _ extension ) .",
    "eventually , a rendering trajectory can be obtained by varying the camera parameters inside the main povray file .",
    "each rendering operation outputs an annotation file , a depth map , and a text file containing the associated camera intrinsic and extrinsic parameters .",
    "these files are parsed with the codes available from @xcite .",
    "since we only need depth and annotations , the rendering procedure is fast , taking less than one second per view on a standard desktop machine for vga resolution .",
    "our system relies on reconstruction front - end running in real - time and deep learning back - end that takes in 4d input channels namely , depth , height from ground , angle with gravity vector , and curvature ( dhac ) .",
    "the labels obtained from different viewpoints are then fused together with the classic bayesian filtering @xcite on a voxelised volume using the camera poses returned by the reconstruction system .",
    "we observe immediate benefits of performing 3d mapping and semantic segmentation in parallel threads : first , at test time , we can use depth maps raycasted from the mapping volume , which have superior quality compared to raw depth maps ; this results in improved segmentation results .",
    "second , we can improve the overall segmentation of the scene by label fusion .",
    "we briefly describe reconstruction and our deep learning architecture below .",
    "our reconstruction system is a custom implementation of the well - known kinectfusion algorithm @xcite , wherein depth maps are averaged with their truncated sign distance representation on a voxelised 3d volume . for all our segmentation experiments , we use raycasted depth maps and camera poses obtained via this system module .",
    "finally , we align the local reference frame of the reconstruction with the inertial frame , using the simple and effective optimisation proposed in @xcite to obtain the required rotation matrix .",
    "this allows us to compute features that are invariant to rotation about the gravity axis , height from the ground plane and angle with gravity vector .",
    "our segmentation module is inspired by the deep architecture used in @xcite .",
    "it is composed of a sequence of stacked auto - encoders , with supervised modular training of each layer to capture the representative features of the scene at different scales and produce dense predictions for each pixel in the input depth map .",
    "we use this architecture primarily due to its lightweight structure , compared to @xcite , which has prohibitive memory requirements .",
    "we perform preliminary experiments with this network on simple scenes composed of chairs and tables . in all our experiments",
    ", we segment the scene into 5 different classes : chairs , tables , floor , ceiling , and wall .",
    "figure [ fig : layers ] shows the segmentation results on training data where a clear improvement of the results is evident as layers are added progressively to the network . figure [ fig : results_on_chairs_tables ] and [ fig : chairs ] show results on real world scenes where we are able to get good segmentations ; the training was done exclusively on synthetic scenes containing chairs and tables .",
    "video links : http://robotvault.bitbucket.org/results.html",
    "we are working towards a real - time system for semantic scene understanding that combines the strengths of 3d reconstruction and semantic segmentation .",
    "we investigate the possibilities of using only depth data for this task and we make publicly available a new library containing the data and the code necessary to generate high - quality annotations for indoor scenes .",
    "future work includes expanding the repository with new synthesised scenes @xcite to learn effective models for indoor semantic segmentation .",
    "www.crazy3dfree.com c. farabet , and c. couprie , and l. najman , and y. lecun . learning hierarchical features for scene labeling , tpami , 35(8):1915 1929 , 2013 .",
    "r. guo , c. zou , and d. hoiem .",
    "predicting complete 3d models of indoor scenes .",
    "arxiv preprint arxiv:1504.02437 , 2015 .",
    "s. gupta , p. arbelaez , and j. malik .",
    "perceptual organization and recognition of indoor scenes from rgb - d images . in cvpr .",
    "s. gupta , r. girshick , p. arbelaez , and j. malik .",
    "learning rich features from rgb - d images for object detection and segmentation . in eccv .",
    "a. handa , r. a. newcombe , a. angeli , and a. j. davison .",
    "real - time camera tracking : when is high frame - rate best ? in eccv .",
    "a. handa , t. whelan , j. b. mcdonald , and a. j. davison .",
    "a benchmark for rgb - d visual odometry , 3d reconstruction and slam . in ieee",
    "intl . conf . on robotics and automation , icra , 2014 .",
    "a. hermans , g. floros , and b. leibe .",
    "dense 3d semantic mapping of indoor scenes from rgb - d images . in interna- tional conference on robotics and automation , 2014 .",
    "p. krahenbuhl and v. koltun .",
    "efficient inference in fully connected crfs with gaussian edge potentials . in j.",
    "shawe- taylor , r. zemel , p. bartlett , f. pereira , and k. weinberger , editors , advances in neural information processing systems 24 , pages 109117 .",
    "curran associates , inc . , 2011 t. liu , s. chaudhuri , v. g. kim , q .- x",
    ". huang , n. j. mi- tra , and t. funkhouser .",
    "creating consistent scene graphs us- ing a probabilistic grammar .",
    "acm transactions on graphics ( proc . of siggraph asia ) , 33(6 ) , 2014 .",
    "j. long , e. shelhamer , and t. darrell .",
    "fully convolutional networks for semantic segmentation .",
    "cvpr ( to appear ) , 2015 .",
    "r. a. newcombe , s. izadi , o. hilliges , d. molyneaux , d. kim , a. j. davison , p. kohli , j. shotton , s. hodges , and a. fitzgibbon .",
    "kinectfusion : real - time dense surface map- ping and tracking . in proceedings of the 2011 10th ieee international symposium on mixed and augmented reality , ismar 11 , pages 127136 , 2011 .",
    "m. ranzato , f. j. huang , y .- l",
    ". boureau , and y. lecun .",
    "un- supervised learning of invariant feature hierarchies with ap- plications to object recognition . in cvpr ,",
    "pages 18 , 2007 .",
    "n. silberman , d. hoiem , p. kohli , and r. fergus .",
    "indoor segmentation and support inference from rgbd images . in eccv , 2012 .",
    "s. thrun , w. burgard , and d. fox .",
    "probabilistic robotics ( intelligent robotics and autonomous agents ) . the mit press , 2005 ."
  ],
  "abstract_text": [
    "<S> we are interested in automatic scene understanding from geometric cues . to this end , we aim to bring semantic segmentation in the loop of real - time reconstruction . </S>",
    "<S> our semantic segmentation is built on a deep autoencoder stack trained exclusively on synthetic depth data generated from our novel 3d scene library , _ </S>",
    "<S> synthcam3d_. importantly , our network is able to segment real world scenes without any noise modelling . </S>",
    "<S> we present encouraging preliminary results . </S>"
  ]
}