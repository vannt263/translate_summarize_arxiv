{
  "article_text": [
    "one of the primary focuses in data mining and machine learning is finding a succinct and effective representation for original high dimensional samples ( hastie et al .",
    "@xcite ; kriegel et al .",
    "@xcite ; ding and li @xcite ; ding et al .",
    "@xcite ; li et al .",
    "@xcite ; tao et al .",
    "@xcite ; tao et al .",
    "linear dimensionality deduction is such a tool that projects the original samples from a high dimensional space to a low dimensional subspace .",
    "meanwhile some particular information , e.g. , manifold structure and discriminative information , of the original high dimensional samples will be well preserved while noises will be removed in the selected subspace .      in the past decades",
    ", a dozen of algorithms have been developed and extensive experimental results have demonstrated that duly selected subspace is effective and efficient for subsequent utilizations . in this paper , we categorize popular dimensionality reduction algorithms into the following three groups :    1 .",
    "conventional linear dimensionality reduction algorithms , e.g. , principal components analysis ( pca ) ( hotelling @xcite ) , fisher s linear discriminant analysis ( flda ) ( fisher @xcite ) , regularized flda , and the geometric mean based subspace selection ( tao et al .",
    "all of these algorithms assume samples are drawn from different gaussians .",
    "pca maximizes the mutual information between original high - dimensional gaussian distributed samples and projected low - dimensional samples .",
    "pca , which is unsupervised , does not utilize the class label information . while , lda finds a projection matrix that maximizes the trace of the between - class scatter matrix and minimizes the trace of the within - class scatter matrix in the projected subspace simultaneously .",
    "the same as pca , flda and regularized flda assume samples are drawn from homoscedastic gaussians . therefore , flda and regularized flda can not work well when gaussians are heteroscedastic .",
    "additionally , they always merge classes which are close in the high dimensional space .",
    "although the geometric mean based subspace selection and its harmonic mean based extension ( bian and tao @xcite ) assume samples are drawn from heteroscedastic gaussians and do not tend to merge close classes , they basically work for gaussian distributed samples .",
    "manifold learning based dimensionality reduction algorithms : e.g. , locally linear embedding ( lle ) ( roweis and saul @xcite ) , isomap ( tenenbaum et al .",
    "@xcite ) , laplacian eigenmaps ( le ) ( belkin and niyogi @xcite ; li et al .",
    "@xcite ) , hessian eigenmaps ( hlle ) ( donoho and grimes @xcite ) , generative topographic mapping ( gtm ) ( bishop et al .",
    "@xcite ; fyfe @xcite ) and local tangent space alignment ( ltsa ) ( zhang and zha @xcite ) .",
    "lle uses linear coefficients , which reconstruct a given measurement by its neighbours , to represent the local geometry , and then seeks a low - dimensional embedding , in which these coefficients are still suitable for reconstruction .",
    "isomap preserves global geodesic distances of all pairs of measurements .",
    "le preserves proximity relationships by manipulations on an undirected weighted graph , which indicates neighbour relations of pairwise measurements .",
    "ltsa exploits the local tangent information as a representation of the local geometry and this local tangent information is then aligned to provide a global coordinate .",
    "hlle obtains the final low - dimensional representations by applying eigen - analysis to a matrix which is built by estimating the hessian over neighbourhood .",
    "all these algorithms have the out of sample problem and thus a dozen of linearizations have been proposed , e.g. , locality preserving projections ( lpp ) ( he and niyogi @xcite ) , neighborhood preserving embedding ( npe ) ( he et al .",
    "@xcite ) , and orthogonal neighbourhood preserving projections ( onpp ) .",
    "recently , we provide a systematic framework , i.e. , patch alignment ( zhang et al .",
    "@xcite ; zhang et al .",
    "@xcite ) , for understanding the common properties and intrinsic difference in different algorithms including their linearizations . in particular",
    ", this framework reveals that : i ) algorithms are intrinsically different in the patch optimization stage ; and ii ) all algorithms share an almost - identical whole alignment stage .",
    "another unified view of popular manifold learning algorithms is the graph embedding framework ( yan et al .",
    "@xcite ) . based on both frameworks ,",
    "different algorithms have been developed , e.g. , the discriminative locality alignment ( liu et al .",
    "@xcite ) , manifold regularization ( belkin et al .",
    "@xcite ) and marginal fisher s analysis ( wang et al .",
    "sparse learning based dimensionality reduction algorithms : e.g. , lasso ( tibshirani @xcite ) , elastic net ( zou and hastie @xcite ) , the smoothly clipped absolute deviation penalty ( scad ) ( fan and li @xcite ) , sure independence screening ( fan and lv @xcite ) , dantzig selector ( candes and tao @xcite ) and dantzig selector with sequential optimization ( dasso ) ( james et al .",
    "conventional linear dimensionality reduction algorithms and manifold learning based dimensionality reduction algorithms produce a low dimensional subspace and each basis of the subspace is a linear combination of all the original bases ( i.e. , variables or features ) used for high dimensional sample representation .",
    "therefore , results can not be interpreted psychologically and physiologically .",
    "sparse learning based dimensionality reduction algorithms are developed not only to achieve the dimensionality reduction but also to reduce the number of explicitly used variables .",
    "a direct method to reduce the number of variables for representation is setting very small coefficients as zero .",
    "however , this strategy is problematic because small coefficients could be very important . because each of new bases is a linear combination of original ones , it is reasonable to consider each new basis as the response of several variables , i.e. , the original features .",
    "then the problem of sparse learning becomes a similar problem to variables selection and coefficients shrinkage . in linear regression , lp norm penalty is always combined with the loss function to reduce over - fitting . in particular ,",
    "@xmath0-norm ( or lasso ) owns a good property to drive a good number of coefficients to zero and lead to a sparse model between responses and variables because of its singularity in the origin ( park and hastie @xcite ; huang and chris ding @xcite ) .",
    "the number of lasso selected variables is no larger than the number of samples .",
    "moreover , lasso randomly selects one from the group of variables that are high correlated .",
    "therefore , elastic net is proposed to address the above problems and achieve the grouping effect by adding the @xmath1 penalty to lasso .    in recent years ,",
    "sparse learning becomes popular , because :    1 .",
    "sparsity can make the data more succinct and simpler , so the calculation of the low dimensional representation and the subsequent processing , e.g. , classification and regression , becomes more efficient .",
    "parsimony is especially an important factor when the dimension of the original samples is very high and the number of samples is very large ; 2 .",
    "sparsity can control the weights of original variables and decrease the variance brought by possible over - fitting with the least increment of the bias .",
    "therefore , the learn model can generalize better ; and 3 .",
    "sparsity provides a good interpretation of a model , thus reveals an explicit relationship between the objective of the model and the given variables .",
    "this is important for understanding practical problems , especially when the number of variables is larger than that of the samples .",
    "however , it is not easy to find the optimal solution of a sparse learning model .",
    "in the original lasso , the residue sum of squares is minimized subject to the sum of the absolute value of the coefficients being less than a constant .",
    "the quadratic programming is sequentially utilized to get the solution and thus the time cost is not acceptable for practical applications .",
    "recently , the least angle regression ( lars ) is proposed to seek a close form solution to the path of coefficients in each step without using the quadratic programming , so it is more efficient and less greedy than the original optimization algorithm used in lasso .    hitherto , most of sparse dimensionality reduction algorithms are designed for linear regression and only a few can be applied for subsequent classification , e.g. , sparse principal component analysis ( spca ) ( zou and hastie @xcite ) , nonnegative sparse principal component analysis ( zass and shashua @xcite ) , sparse linear discriminant analysis ( slda ) , sparse projections over graph ( spog ) ( cai et al .",
    "@xcite ; cai et al .",
    "@xcite ) and spca using semi - definite programming ( aspremont et al .",
    "@xcite ) . both spca and spca using semi - definite programming do not consider the sample label information and thus some discriminative information will be removed after dimensionality reduction .",
    "slda can work well for binary class classification but it can not be applied for multi - class classification .",
    "spog utilizes a particular manifold learning based dimensionality reduction algorithm , e.g. , locality preserving projections ( lpp ) , to obtain the dense projection matrix and then applies lasso to regress the corresponding sparse projection matrix .",
    "absolutely the problem is indirectly formulated to obtain the sparse projection matrix .",
    "a direct formulation should be imposing the lasso penalty over a loss function ( i.e. , a criterion ) of a dimensionality reduction algorithm .",
    "however , it is difficult to use lars to obtain its optimal solution because the objective function is not a direct regression problem .",
    "therefore , researchers currently take indirect routs to obtain sparse projection matrices .      in this paper",
    ", we propose the manifold elastic net ( men ) , which obtains a sparse projection matrix for subsequent classification .",
    "men directly imposes the elastic net penalty ( i.e. , the combination of the lasso penalty and the @xmath1-norm penalty ) over the loss ( i.e. , the criterion ) of a discriminative manifold learning based dimensionality reduction algorithm . by using a series of complex linear algebra equivalent transformations ,",
    "the objective function of men can be rewritten as a lasso penalized least square problem and thus lars can be applied to obtain the optimal sparse solution of men .    in detail , we first apply the part optimization of the patch alignment framework to encode the local geometry of a set of training samples . in the second step ,",
    "the whole alignment of the patch alignment framework is applied to calculate the unified coordinate system for local patches obtained in the first step . for low dimensional data representation ,",
    "the linearization or the linear approximation is adopted in men .",
    "although we can impose some discriminative information preservation criterion ( e.g. , margin maximization ) over the part optimization stage , it is not directly relevant to the classification error minimization . therefore , we put a new item that minimizes the classification error in the third step . to obtain a sparse projection matrix with the grouping effect , in the fourth step , the elastic net penalty is adopted in men .",
    "so far , the objective function of men is fully constructed .    with the well defined men",
    ", we then apply lars to obtain the optimal solution of men .",
    "we transform men into a form in which the correlation of basis can be written as the correlation of coefficients .",
    "active set is built according to lars . in each step ,",
    "no more than one element of the basis is added to the active set according to its correlation .",
    "all elements in the active set are changed in each step with special direction and distance in the space of coefficients .",
    "the direction and distance of a path in each step have closed form solution according to the extended simplex .",
    "the sparsity of the projection matrix is controlled by the cardinality of the active set . because the lars for men generates bases in an independent way",
    ", the same procedure is conducted multiple times to obtain a set of bases . under this procedure , these bases are orthogonal .",
    "thorough experiments on face recognition ( shakhnarovich and moghaddam @xcite ) task based on popular face datasets show the effectiveness of the proposed men by comparing against the top level dimensionality reduction algorithms .    the rest of the paper is organized as follows .",
    "section 2 presents the proposed manifold elastic net ( men ) including the objective function of men and the lars optimization for men .",
    "section 3 shows the effectiveness of men for face recognition over different face datasets .",
    "section 4 concludes .",
    "consider in the discriminative dimensionality reduction problem with training samples and corresponding class labels .",
    "let @xmath2^t\\in \\mathbb r^{n\\times p}$ ] be a given training set in a high dimensional space @xmath3 and @xmath4^t\\in \\mathbb r^n$ ] be the corresponding class label vector .",
    "the objective here is to find a projection matrix @xmath5^t\\in \\mathbb r^{p\\times d}$ ] that projects samples @xmath6 in the high dimensional space onto a low dimensional subspace , i.e. , @xmath7 , such that samples from different classes can be well separate , i.e. , the classification error can be extremely minimized .",
    "manifold learning based dimensionality reduction aims to find the corresponding low dimensional representation z in a low dimensional euclidean space of x to preserve ( actually approximate ) the data intrinsic structure .",
    "popular manifold learning based dimensionality reduction algorithms , however , have the following two problems : 1 ) the classification error is not directly and explicitly considered , although some algorithms compound discriminative information preservation criteria , e.g. , margin maximization ; and 2 ) the obtained low dimensional representation linear combines of all variables in the high dimensional space , so it is difficult to clear interpret and efficiently represent data .",
    "sparse learning provides sparse data representation via variable selection , and has the following advantages : 1 ) the sparsity improves the parsimony in computation , i.e. , the computational cost can be significantly reduce ; 2 ) the penalties and the constraints introduced in a learning model discourage the possible over - fitting of the model ; and 3 ) the learned model can be well interpreted .",
    "however , existing sparse learning algorithms are designed for linear regression problems and the data intrinsic structure is usually ignored .    to achieve the merits of manifold learning based dimensionality reduction and the advantages of sparse learning , in this paper , we propose the manifold elastic net ( men ) , which is a general framework to obtain the sparse solution of the manifold learning based discriminative dimensionality reduction .",
    "there are few research results on combining sparse learning and discriminative dimensionality reduction because the projection matrix of a lasso penalized model can not be obtained directly by using the least angle regression ( lars ) .",
    "men is not a direct combination of the manifold learning based dimensionality reduction and the sparse learning .",
    "it however finds the optimal sparse solution of every manifold learning based discriminative dimensionality reduction algorithm via the patch alignment framework and a new classification error minimization based criterion .",
    "in particular , men encodes the local geometry of a set of samples and finds an aligned coordinate system for data representation under the patch alignment framework ; men utilizes the classification error minimization criterion to directly link the classification error with the selected subspace ; and men incorporates the elastic net regularization to sparsify the projection matrix .",
    "different manifold learning algorithms encode different types of local geometry of samples , e.g. , locally linear embedding ( lle ) applies linear coefficients to reconstruct a sample by its neighbors .",
    "the patch alignment framework has well demonstrated that different algorithms have different optimization criteria to encode different local geometry over patches .    in men , the same as the part optimization in the patch alignment framework , each patch is constructed by a particular sample @xmath8 and its @xmath9 related ones @xmath10 . the patch is denoted by @xmath11^t\\in \\mathbb r^{(k+1)\\times p}$ ] .",
    "men finds a linear mapping @xmath12 that projects the patch @xmath13 to a low dimensional subspace @xmath14 , i.e. , @xmath15 , where @xmath16^t\\in \\mathbb r^{(k+1)\\times d}$ ] .",
    "the part optimization maximizes the similarity of the local geometry represented by @xmath17 and that described by @xmath18 : @xmath19 where @xmath20 encodes the local geometry of the patch @xmath17 and it is different over different dimensionality reduction algorithms .    for a given sample @xmath8 ,",
    "its @xmath9 related ones are divided into two groups : the @xmath21 ones in the same class with @xmath8 and the @xmath22 ones from different classes with @xmath8 .",
    "these two groups are selected independently and denoted by @xmath23 and @xmath24 respectively .",
    "therefore , the patch for @xmath8 is defined by @xmath25^t\\in \\mathbb r^{\\left(k_1+k_2 + 1\\right)\\times p}.\\end{aligned}\\ ] ] the corresponding the low dimensional representation is @xmath26^t\\in \\mathbb r^{\\left(k_1+k_2 + 1\\right)\\times d}.\\end{aligned}\\ ] ] let @xmath27 to be the index set . in the low dimensional subspace , we expect that the distances between the given sample and the group of related samples from different classes are as large as possible , while the distances between the sample and the group of related samples in the same class are as small as possible .",
    "therefore the part optimization is : @xmath28 where @xmath29 is a trade - off parameter to control the impacts of the two parts .",
    "define the coefficient vector : @xmath30^t , \\label{equ : part1}\\end{aligned}\\ ] ] then we can obtain the part optimization matrix , @xmath31      each patch @xmath17 for @xmath32 has a corresponding low dimensional representation @xmath18 . to unify all low dimensional patches @xmath33^t$ ] for @xmath32 together into a consistent coordinate system , according to the patch alignment framework",
    ", we assume that the coordinate of @xmath18 is selected from the global coordinate @xmath34^t\\in \\mathbb r^{n\\times d}$ ] by a using sample selection matrix @xmath35 : @xmath36 where the selection matrix @xmath37 is defined by @xmath38 according to eq.[equ : zi ] , the part optimization defined in eq.[equ : partopt ] can be rewritten as : @xmath39 after summing over all part optimizations together , the whole alignment is given by : @xmath40 where @xmath41 is the alignment matrix .",
    "it is obtained by an iterative procedure : @xmath42    it is worth emphasizing that the mapping @xmath43 from the high dimensional space to the low dimensional subspace can be nonlinear and implicit . however , the linear approximation @xmath44 is adopted , i.e. , we expect the difference between @xmath45 and @xmath46 is minimized",
    ". in particular , @xmath5\\in \\mathbb r^{p\\times d}$ ] .",
    "therefore , the objective function is : @xmath47      in men , although the discriminative information for classification is considered duly in eq.[equ : obj1 ] , the classification error is not directly modeled . to further enhance the performance of men for classification problems ,",
    "it is necessary to provide an explicit way to represent the classification error minimization in the objective function .",
    "the least square error minimization is usually adopted in binary classification , @xmath48 however , it is very challenging to apply eq.[equ : errormin ] to multi - class classification .",
    "this is mainly because the class label vector @xmath49 can not be directly utilized as the output ( response ) @xmath50 .",
    "recently , the least squares linear discriminant analysis",
    "( ye @xcite ; sun et al .",
    "@xcite ) or ls - lda for short is proposed and presents the equivalence relationship between the least square formulation and the conventional linear discriminant analysis ( lda ) for multi - class classification under a mild condition . however , the dimension of the indicator matrix is the number of classes @xmath51 .",
    "therefore , ls - lda can only reduce the original data to a @xmath52 dimensional subspace .",
    "it is pretty fine when samples are drawn from homoscedastic gaussians because the bayes optimal is achieved iff the dimension of the subspace is @xmath52 .",
    "however , for practical applications , samples are usually not sampled from homoscedastic gaussians and a dozen of experimental evidences show that we usually achieve the best classification performance in a subspace lower than @xmath52 when @xmath51 is large .    in this paper ,",
    "we propose a flexible method to design the indicator matrix @xmath50 and the dimension of the selected subspace is allowed to be any number between @xmath53 and @xmath52 . in comparing with ls - lda ,",
    "the proposed indicator design method is more flexible and powerful to gain a lower dimensional representation and higher recognition rate .",
    "therefore , the new method meets most demands for practical applications , e.g. , face recognition .    the nearest - neighbor ( nn ) rule is commonly applied in classification problmes . in nn , it would be perfect when samples in the same class are projected onto the same point after dimensionality reduction , and this point is the low dimensional representation of the corresponding class center .",
    "meanwhile the variance of these projected class centers is expected to be maximized .",
    "as a consequence , the low dimensional projection of class centers can be conveniently obtained by the weighted principal component analysis ( pca ) .    in detail , suppose the given @xmath54 samples belong to @xmath51 classes , and there are @xmath55 samples in the @xmath56 class .",
    "the @xmath56 class center is @xmath57 , wherein @xmath58 is the @xmath59 sample in the @xmath56 class and is a row vector in @xmath60 .",
    "the proportion of the @xmath56 class is @xmath61 .",
    "therefore , the weighted covariance matrix of class centers is given by : @xmath62 suppose we expect to find a @xmath63 dimensional subspace .",
    "the @xmath63 eigenvectors associated with the largest @xmath63 eigenvalues @xmath64 $ ] of @xmath65 are selected to calculate the low dimensional representation of the class center @xmath66 according to @xmath67 therefore , the indicator matrix @xmath68^t$ ] is given by @xmath69 .",
    "on combining eq.[equ : obj1 ] and eq.[equ : errormin ] , we have @xmath70 where @xmath71 and @xmath72 are trade - off parameters to control the impacts of different parts .      in men , we expect to obtain a sparse projection matrix for explicit data representation and effective interpretation ,",
    "i.e. , control the number of nonzero elements in each column of the projection matrix .",
    "this nonzero number of the entries of the projection matrix can be characterized by the @xmath73-norm of the projection matrix .",
    "we can impose it over the objective function defined in eq.[equ : obj2 ] as a penalty .",
    "however , it turns to be an np - hard problem and thus it is always impossible to be solved in a polynomial time , because the penalty is nonconvex ( lv and fan @xcite ) .",
    "therefore , the @xmath0-norm of the projection matrix , i.e. , lasso , is usually adopted as a relaxation of the @xmath73 penalty . although lasso is convex , it is difficult to find the solution of the lasso regularized model .",
    "this is because the lasso term is not differentiable .",
    "least angle regression or lars for short has been proposed to greedily search the optimal solution of the lasso penalized linear regression problem .",
    "lars continuously shrinks the particular coefficients ( entries of the projection matrix w ) towards zeros , while simultaneously preserves high prediction accuracy .",
    "however , the lasso penalty has the following two disadvantages : 1 ) the number of selected variables is limited by the number of observations and 2 ) the lasso penalized model can only selects one variable from a group of correlated ones and does not care which one is selected . by imposing an @xmath1-norm of the projection matrix on the lasso penalized problem , similar to the elastic net",
    ", we can overcome the aforementioned two disadvantages and retain the favorable properties of the lasso penalty .",
    "in detail , the @xmath1-norm of the projection matrix is helpful to increase the dimension ( and the rank ) of the combination of the data matrix and the response .",
    "in addition , the combination of the @xmath0 and @xmath1 of the projection matrix is convex with respect to the projection matrix and thus the obtained projection matrix has the grouping effect property .",
    "therefore , to obtain a sparse projection matrix w with the grouping effect , both @xmath0-norm and @xmath1-norm of the projection matrix are added as penalties to the objective function defined in eq.[equ : obj2 ] and we obtain the full definition of men : @xmath74      it has been demonstrated that lars is effective and efficient to find the optimal solution of the lasso or the elastic net ( the combination of @xmath0 and @xmath1 ) penalized multiple linear regression .",
    "therefore , it can be directly applied to penalized least squares only .",
    "however , the proposed men defined in eq.[equ : obj3 ] , at the first glance , is not a penalized least square .    in this section",
    ", we detail utilizing lars to obtain the optimal solution of men .",
    "although lars is designed to solve the penalized multiple linear regression where the coefficients are a vector rather than a matrix , the column vectors of the projection matrix @xmath75 in men are independent bases .",
    "therefore , we can calculate them one by one . in the following analysis",
    ", we consider a particular column of @xmath75 , i.e. , @xmath76 , and the corresponding vector @xmath77 in the indicator matrix @xmath50 . to simplify the notations below",
    ", we keep using @xmath75 and @xmath50 instead of @xmath76 and @xmath77 .",
    "because the low dimensional representation @xmath45 and the projection matrix @xmath75 are independent , we can eliminate @xmath45 in the objective function . in detail",
    ", @xmath45 is obtained by setting the differentiate of the objective function @xmath78 with respect to @xmath45 as @xmath79 , i.e. , @xmath80 therefore , we have @xmath81 according to eq.[equ : z ] , we can eliminate @xmath45 in the objective function defined in eq.[equ : obj3 ] , and thus we have : @xmath82 where this @xmath83 is an asymmetric matrix computed from @xmath41 : @xmath84 to apply lars to obtain the optimal solution of eq.[equ : obj4 ] , we expect the first item in it to be a quadratic form . because @xmath85 and the eigenvalue decomposition of @xmath86 can be written as @xmath87 , the objective function defined in eq.[equ : obj4 ] without the elastic net penalty",
    "can be rewritten as : @xmath88 the constant item can be ignored in optimization without loss of generality .",
    "we further set @xmath89 @xmath90 in eq.[equ : obj4 ] , and then we get @xmath91 where @xmath92 and @xmath93 .    according to eq.[equ : obj5 ] , the lars algorithm can be applied to obtain the optimal solution of the proposed men .",
    "lars provides an efficient algorithm to solve the lasso penalized multiple linear regression .",
    "below we sketch lars for the transformed men defined in eq.[equ : obj5 ] and provide novel viewpoints to lars , which are helpful to better understand the proposed men .",
    "we begin with a coefficient vector @xmath94 ( a column in the projection matrix with @xmath56 entry @xmath95 with all zero entries .",
    "a variable ( a column vector in @xmath96 , i.e. , a particular feature ) in @xmath97 is most correlated with the objective function is added to the active set @xmath83 .",
    "then the corresponding coefficient in @xmath94 increases as large as possible until a second variable ( another column vector in @xmath96 , i.e. , another feature ) in @xmath97 has the same correlation as the first variable . instead of continuously increasing the coefficient vector in the direction of the first variable , lars",
    "proceeds on a direction equiangular over all variables in the active set @xmath83 until a new variable earns its way into @xmath83 . to make the coefficient vector @xmath94 becomes @xmath98-sparse ( at most @xmath98 nonzero entries ) , we conduct the above procedure for @xmath98 loops .",
    "the optimization path direction and the corresponding path length ( step size ) in lars are determined by the correlations , which are the negative gradient of the objective function defined in eq.[equ : obj5 ] without the lasso penalty , i.e. , @xmath99^t .",
    "\\label{equ : corr}\\end{aligned}\\ ] ] the constant @xmath100 can be simply ignored without loss of generality in the following analysis .",
    "the larger the correlation @xmath55 is , the more important the corresponding variable will be , and thus the larger the corresponding coefficient @xmath95 in @xmath94 will be . in sparse learning , important variables are added to the active set @xmath83 sequentially according to their corresponding correlations defined in eq.[equ : corr ] , and then the direction and distance of coefficient vector of all the important variables are determined .",
    "let @xmath83 be the active set of `` most correlated '' variables whose coefficients are nonzero , while the other variables form an inactive set @xmath101 .",
    "thus the sparsity is determined by the cardinality of @xmath83 .",
    "the correlations of variables in @xmath83 are always identical to each other in @xmath83 and larger than the correlations of variables in @xmath101 .",
    "those correlations of variables in @xmath101 are usually different to each other .",
    "initially , all the variables are in inactive set @xmath101 and thus the corresponding coefficients are all zero .    to make @xmath94 @xmath98-sparse , we need to conduct the following three steps for @xmath98 loops . in the first step",
    ", the variable in the inactive set @xmath101 with the largest correlation is added to the active set @xmath83 , i.e. , @xmath102 where @xmath103 is the current correlation of the @xmath59 variable .    in the second step ,",
    "the direction of the coefficient vector @xmath94 is calculated . to make the optimization more global and less greedy , the correlations of the active variables are required to decrease equally in preferred direction . in the @xmath104 loop ,",
    "if the direction vector is @xmath105 , then the current correlation is given by @xmath106 where @xmath107 contains all variables in @xmath83 and each its column is sampled from @xmath108 , @xmath109 is the correlation in the @xmath110 loop , @xmath111 is a constant that is irrelevant to the direction computation , @xmath112 stores directions associated with variables in @xmath83 , and the change of the correlation at this step is @xmath113 .",
    "the sign of @xmath112 , i.e. , @xmath114 , is identical to that of @xmath109 , so we can calculate the magnitude of @xmath112 directly and then assign its sign as @xmath114 .",
    "this @xmath115 is an extended simplex with vertices defined by active variables .",
    "we project the @xmath56 column of @xmath108 , i.e. , @xmath116 , onto @xmath115 and thus we get @xmath117 .",
    "because the correlations of the active variables are required to decrease equally in preferred direction , i.e. , @xmath117 equals to each other over the index @xmath118 , the only possible solution of @xmath115 is the normal vector through the origin in the simplex space .",
    "therefore , we have @xmath119 where @xmath120 is the gram matrix of @xmath107 . in lars ( efron et al .",
    "2004 ) , @xmath112 is obtained by minimizing the squared distance between the point @xmath115 on the simplex and the origin , subject to @xmath121 .    to normalize the change of the correlation @xmath122 to a unit vector @xmath123 , we need to update @xmath124 and @xmath112 , and thus we obtain a normalized @xmath123 , i.e. , @xmath125    in the third step , we calculate the distance or magnitude of changes @xmath126 . to have an efficient optimization procedure , this @xmath126 should be as large as possible . at the same time",
    ", we have to guarantee that correlations of variables in @xmath83 are always identical to each other in @xmath83 and larger than correlations of variables in @xmath101 .",
    "therefore @xmath111 is increased until the correlation of a particular variable in @xmath101 is equivalent to the correlations of active variables , i.e. , @xmath127 where @xmath128 is the complement of @xmath83 , @xmath129 , @xmath130 is the @xmath59 entry of @xmath131 , @xmath132 is the largest correlation defined in eq.[equ : addactive ] and obtained in the first step , and @xmath126 is a possible candidate of @xmath111 mentioned in eq.[equ : ck ] .",
    "according to lars , to obtain an identical solution to men defined in eq.[equ : obj5 ] , the lasso modification is considered , i.e. , the argument of the distance @xmath111 stops increasing when a coefficient of variables in @xmath83 is zero , or mathematically , @xmath133 where @xmath134 is another possible candidate of @xmath111 defined in eq.[equ : ck ] . according to eq.[equ : calrho2 ] , we can obtain @xmath135 therefore , the distance of @xmath94 , i.e. , @xmath111 , is the minimum of @xmath126 and @xmath134 , i.e. , @xmath136    in each loop , one new variable is added to the active set @xmath83 according to eq.[equ : addactive ] , the direction and distance of the coefficient vector @xmath94 are calculated according to eq.[equ : omega ] and eq.[equ : rho ] .",
    "after @xmath98 loops , @xmath94 is @xmath98-sparse . according to the elastic net , to eliminate the double shrinkage ,",
    "the optimal @xmath75 should be corrected : @xmath137      lars is inefficient when the size of the training set is large , because the time cost for calculating the inverse of the gram matrix @xmath138 defined in eq.[equ : rho1 ] is huge .",
    "because the dimension of this @xmath138 is increasing at each of the @xmath98 loops , according to ( golub and van loan @xcite ) , the inverse of @xmath138 can be obtained incrementally , i.e. , the inverse of the gram matrix @xmath139 in the @xmath104 loop can be updated from @xmath140 in the previous loop .",
    "particularly , in the @xmath104 loop , a new variable @xmath141 is added to the active set @xmath83 , and thus we have @xmath142    let @xmath83 , @xmath143 , @xmath49 and @xmath144 be the blocks of @xmath138 , i.e. , @xmath145 , @xmath146 , @xmath147 , and @xmath148 .",
    "let @xmath149 to be the schur complement of a , i.e. , @xmath150 . according to rules of the block matrix calculation ,",
    "@xmath139 is given by : @xmath151 where @xmath152 is the inverse of the gram matrix obtained in the previous loop .",
    "the time cost for calculating the inverse of the gram matrix in the @xmath104 loop can be reduced from @xmath153 to @xmath154 ( @xmath155 is the size of active set in the @xmath104 loop ) when the inverse of the gram matrix in the previous loop is available .",
    "we can further accelerate the computation of lars for men by taking the advantage of the sparse structure of @xmath108 .",
    "for example , when calculating the equiangular vector @xmath131 and the inner product @xmath138 , the block matrix calculation can reduce the time cost as well .      in this paper",
    ", we propose an efficient framework men for discriminative dimensionality reduction with sparse projection .",
    "based on the discussion in the above six subsections , men is shown in algorithm 1 .",
    "* input : *  training data matrix @xmath156\\in \\mathbb r^{n\\times p}$ ] ; +  class label vector @xmath4^t$ ] ; +  @xmath5\\in \\textbf{0}^{p\\times d}$ ] , where @xmath63 is the dimensions of subspace ; +  the number of loops @xmath98 , small @xmath98 induces sparser @xmath75 .",
    "* output : *  sparse projection matrix @xmath5\\in \\mathbb r^{p\\times d}$ ] . * initialize : * @xmath157 .",
    "step 1 : optional pca reconstruction of original data @xmath96 .",
    "step 2 : part optimization : build @xmath54 patches for the @xmath54 given samples according to definition of +  manifold , calculate matrix @xmath158 for each patch using eq.[equ : part1 ] and eq.[equ : part2 ] .",
    "step 3:whole alignment : unify the patches in a global coordinate , compute big matrix @xmath41 +  using eq.[equ : whole1 ] .",
    "step 4 : classification error minimization : calculate the indicator matrix @xmath50 using scaled +  pca for class centers using eq.[equ : classerror ] .",
    "step 5 : new data matrix and indicator matrix : calculate @xmath108 and @xmath159 from @xmath96 and @xmath50 using +  eq.[equ : newx ] and eq.[equ : newy ] .",
    "step 6 : column by column loops for @xmath75,@xmath160 .",
    "* initialize : * @xmath161 .",
    "@xmath162 . update active set : add the variable with largest correlation to @xmath83 using eq.[equ : corr ] and eq.[equ : addactive ] .",
    "direction calculation using eq.[equ : aa ] , eq.[equ : omega ] and fast lars",
    "eq.[equ : fastlars ] .",
    "distance calculation using eq.[equ : rho1 ] , eq.[equ : rho2 ] and eq.[equ : rho ] .",
    "update @xmath163 using eq.[equ : updatew ] .",
    "step 7 : update projection matrix @xmath75 by adding @xmath163 into @xmath75 .",
    "* return * @xmath75 .    in men ,",
    "after necessary initializations , we first build patches for all training samples by calculating @xmath158 of each patch in the part optimization according to eq.[equ : part2 ] in subsection 1",
    ". then these @xmath158 matrixes are unified in a global coordinate system into one matrix @xmath41 according to eq.[equ : whole1 ] in whole alignment step explained in subsection 2 .",
    "afterwards , the indicator matrix @xmath50 is computed according to the weighted pca over class centers defined in eq.[equ : classerror ] in subsection 3 . a matrix @xmath83 defined in eq.[equ : obj3 ] in the objective function can be obtained from @xmath41 and other parameters .",
    "the eigenvalue decomposition is conducted over @xmath86 to construct the new data matrix @xmath108 and the new indicator matrix @xmath159 according to eq.[equ : newx ] and eq.[equ : newy ] , respectively .",
    "then the lars algorithm is applied to calculate a sparse projection matrix .",
    "the direction and distance of each loop are computed according to eq.[equ : omega ] and eq.[equ : rho ] . the incremental method to obtain the inverse of the gram matrix defined in eq.[equ : fastlars ]",
    "is considered speeding up lars .",
    "this process is conducted several times and the projection matrix is computed column by column . finally a sparse projection matrix is obtained as the output of men .",
    "this matrix is ready to project a given sample in @xmath60 to a low dimensional subspace @xmath14 with @xmath98-sparse .",
    "men is an efficient algorithm with high convergence velocity , because the computation in lars explained in subsections 5 and 6 is equivalent to the cost of a least square fit . given a training set @xmath164 , to obtain a sparse matrix @xmath165 each column of which contains k nonzero elements , d times of lars are required in men .",
    "most steps in lars are simple matrix computations . for @xmath166",
    ", men requires @xmath167 operations .",
    "men integrates the merits of both manifold learning and sparse learning via a unified framework .",
    "it is not a direct combination of these two popular learning schemes but a complementary embedding of both . through the patch alignment framework ,",
    "the local geometry of a given dataset is retained in men .",
    "the weighted lasso and @xmath1 penalties are added to produce a sparse projection matrix with the grouping effect . the combined lasso and @xmath1 is also termed as the elastic net .",
    "therefore , we term the proposed framework as the manifold elastic net . as a consequence ,",
    "men is superior to existing dimensionality reduction algorithms , because of its powerful variable selection function and consideration of the intrinsic structure of the dataset .",
    "it has been well demonstrated that lars is effective and efficient to solve a lasso regularized least square problem .",
    "therefore , to apply lars to find the optimal solution of men , it is essential to prove that men is equivalent to a lasso regularized least square problem and lars converges for optimization . in particular , we prove that lars can optimize a general form of the lasso regularized problem , which contains both men and the lasso regularized least square problem as special cases .",
    "lars can solve a general form of the lasso regularized problem defined below : @xmath168 where @xmath169 and @xmath170 ( could be an asymmetric square matrix ) , @xmath171 , and @xmath49 and @xmath172 are constants .",
    "it is equivalent to prove that the problem defined in eq.[equ : theo1 ] is equivalent to a lasso regularized least square problem .",
    "the objective function defined in eq.[equ : theo1 ] without the lass penalty can be written as : @xmath173 where @xmath174 is a symmetric matrix and its eigenvalue decomposition is @xmath175 .",
    "therefore , we have : @xmath176 to simply represent the above objective function , without loss of generality , let @xmath177 and ignore the constant .",
    "therefore , we can transform the problem defined in eq.[equ : theo1 ] to @xmath178 which is a lasso regularized least square problem .",
    "it is not difficult to prove that men is a special case of the problem defined in eq.[equ : theo1 ] .",
    "therefore , lars can be applied to solve men and the problem defined in eq.[equ : theo1 ] .",
    "lars converges in optimizing the problem defined in eq.[equ : theo1 ] in theorem 1 .",
    "let the objective function defined in eq.[equ : theo1 ] without the lasso penalty be @xmath78 .",
    "after the @xmath104 loop , assume the estimate of the objective function becomes @xmath179 .",
    "if @xmath78 is smooth in each loop , we have : @xmath180,\\end{aligned}\\ ] ] where @xmath181 is the @xmath56 element in coefficient vector @xmath72 , and @xmath105 is the change of @xmath72 between two consecutive loops , i.e. , @xmath182^t$ ] .    in lars for the problem defined in eq.[equ : theo1 ] , the sign of @xmath105 is the negative gradient of objective function @xmath78 on @xmath183 , i.e. , @xmath184    in each loop of lars , when correlation of one active variable becomes zeros , the length of the coefficient path will stop increasing . therefore , the sign vector of correlations will not change in one loop , i.e. , @xmath185 according to the analyses , we can obtain the sign of @xmath186 : @xmath187    according to the above equation , the objective function @xmath78 is monotonic .",
    "in addition , @xmath78 is bounded .",
    "therefore , we can safely draw the conclusion that lars converges in optimizing the problem defined in eq.[equ : theo1 ] .",
    "in this section , we evaluate the performance of men by comparing against six representative dimensionality reduction algorithms , i.e. , principal component analysis ( pca ) , fisher s linear discriminant analysis ( flda ) , discriminative locality alignment ( dla ) ( zhang et al .",
    "@xcite ; zhang et al .",
    "@xcite ) , supervised locality preserving projection ( slpp ) , neighborhood preserving embedding ( npe ) , and sparse principal somponent analysis ( spca ) , on three standard face image databases , i.e. , umist ( graham and allinson @xcite ) , feret ( phillips et al .",
    "@xcite ) and yale ( belhumeur et al .",
    "@xcite ) .",
    "pca is an unsupervised linear dimensionality reduction algorithm which projects the data along the direction of maximal variance .",
    "flda is a supervised linear dimensionality reduction method .",
    "slpp is a supervised modification of the locality preserving projections , which is a linearization of the laplacian eigenmap .",
    "npe is a linear approximation to the locally linear embedding ( lle ) .",
    "spca is a sparse dimensionality reduction algorithm which combines the lasso penalty with pca to produce sparse loadings .",
    "three standard face image datasets , e.g. , umist , feret and yale , are utilized in this paper to evaluate the proposed men for discriminative dimensionality reduction .",
    "there are @xmath188 face images from @xmath189 individuals in the umist dataset .",
    "the samples demonstrate variations in race , gender , pose and appearance .",
    "the feret dataset consists of @xmath190 face images from @xmath191 individuals .",
    "the images vary in size , gender , pose , illumination , facial expression and age .",
    "we randomly select @xmath192 individuals , each of which has @xmath193 images from feret for performance evaluation .",
    "the yale dataset contains @xmath194 face images of @xmath195 individuals .",
    "lighting conditions , gender , facial expressions and configurations are different among these images .",
    "all images from these three databases are normalized to @xmath196 pixel arrays with @xmath197 gray levels per pixel .",
    "fig.[fig : sampleface ] shows sample images from these three datasets .",
    "each image is reshaped to a long vector by concatenating its pixel values in a particular order .",
    "different algorithms follow an equivalent procedure for all face recognition experiments on various datasets .",
    "firstly , the database is randomly divided into two separate sets : training set and testing set .",
    "then the training set is used to learn the low dimensional subspace and corresponding projection matrix through given algorithm .",
    "after this , samples in the testing set are projected to a low dimensional subspace via the projection matrix .",
    "finally , the nearest neighbor classifier is used to recognize testing samples in the subspace .",
    "we apply pca to reduce dimensions of original high dimensional face images before flda , dla , lpp ( with supervised setting ) and npe ( with supervised setting ) . for flda",
    ", we retain @xmath198 dimensions in the pca projection , where @xmath54 is the number of samples and @xmath51 is the number of classes .",
    "we project samples to the pca subspace with @xmath199 dimensions for dla , slpp and npe .    for umist and yale , we randomly select @xmath200 images per individual for training , while the remaining images are used as testing samples . for feret ,",
    "@xmath201 images per individual are selected as training set , and the remaining for testing .",
    "all experiments are repeated five times , and the average recognition rates are calculated .",
    "the results of these dimensionality reduction algorithms on two settings of feret are shown in fig.[fig : rateferet ] .",
    "these seven algorithms can be divided into 3 groups according to their performance : pca and spca are at the bottom level , because they are unsupervised and the label information is not considered .",
    "pca is slightly better than spca , because spca is designed to approximate pca but with less information retained to hold the sparse property .",
    "lpp , npe and lda are at the middle level .",
    "they are much better than pca and spca because they consider the class label information .",
    "lpp and npe preserve the local geometry based on the neighborhood information of samples , while lda ignores the local geometry .",
    "lpp and npe can not perform as well as dla and men because both of them ignore the margin maximization or the inter - class information .",
    "men and dla are at the top level .",
    "men outperforms dla because it reduces the noises by using the elastic net penalty .",
    "experimental results on umist are shown in fig.[fig : rateumist ] .",
    "men outperforms the other six algorithms consistently .",
    "note the fact that men keeps having the highest recognition rate when the dimension of the selected subspace is low .",
    "this verifies the robustness of men in low dimension situation .",
    "in addition , the computational cost is proportional to the dimension of the selected subspace . therefore men produces better results with less computational cost than other dimensionality reduction methods .",
    "fig.[fig : rateyale ] shows men outperforms the other six algorithms on the yale dataset .",
    "the curves of men are smoother than those of the other algorithms .",
    "this implicates that men is more stable than the other algorithms .",
    "men has high recognition rate even when the training set is small and the dimensions of the selected subspace is low .",
    "the priority of men can be attributes to its supervised learning property , consideration of data manifold structure , feature selection ability brought by sparsity and the grouping effect .",
    "the sparsity of men filters out classification irrelevant features , which bring unnecessary noises for classification .",
    "this is effective especially when the number of classes is much smaller than the number of the original features .",
    "furthermore , the sparse projection matrix brings better interpretation and lower computational cost for subsequent calculation than dense projection matrices .",
    "table [ table : recognitionrate ] lists the best recognition rate and the corresponding subspace dimension for each algorithm in the experiments on the three face image datasets .",
    "sparse dimensionality reduction algorithm including men and spca always arrive their best recognition rate in lower dimensional subspace than other five algorithms .",
    "this is because the sparsity brought by the lasso penalty is able to select the most significant features .",
    "however , because spca does not consider the class label information , it always performs more poorly than other supervised algorithms . for each algorithm ,",
    "the dimension of the best recognition rate is decreasing with the increasing of training samples .",
    "this is because more training samples make the low dimensional representation more stable and reliable .",
    ".best recognition rate ( % ) on three databases . for men , dla , lpp ( slpp ) , npe ,",
    "lda ( flda ) , pca , spca ( sparse pca ) , the numbers in the parentheses behind the recognition rates are the subspace dimensions .",
    "numbers in the second column denote the number of training samples per individual . [ cols=\"^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     boxplots of the experimental results of these seven dimensionality reduction algorithms on the three face image datasets are shown in fig.[fig : boxferet ] , fig.[fig : boxumist ] and fig.[fig : boxyale ] , respectively .",
    "each boxplot produces a box and whisker plot for each method .",
    "the box has lines at the lower quartile , median , and upper quartile values .",
    "whiskers extend from each end of the box to the adjacent values in the data - by default and the most extreme values within @xmath202 times the interquartile range from the ends of the box .",
    "men achieves the most robust recognition rate , because it considers the sparse property , the local geometry of intra - class samples , and the margin maximization and classification error minimization of inter - class samples .",
    "men selects features with the largest correlation and eliminates the most unstable ones .",
    "manifold learning methods , such as lpp , dla and npe , as well as lda are more stable than pca and spca according to these boxplots because they consider the class label information .     to @xmath203 ) on feret with @xmath204 ( @xmath205 ) training samples per person . for every dimension , from left to right ,",
    "the seven boxes refer to men , dla , lpp , npe , flda , pca , and spca . ]     to @xmath206 ) on umist with @xmath205 ( @xmath193 ) training samples per person . for every dimension , from left to right ,",
    "the seven boxes refer to men , dla , lpp , npe , flda , pca , and spca . ]     to @xmath207 ) on yale with @xmath205 ( @xmath193 ) training samples per person . for every dimension , from left to right ,",
    "the seven boxes refer to men , dla , lpp , npe , flda , pca , and spca . ]",
    "fig.[fig : faceferet ] , fig.[fig : faceumist ] and fig.[fig : faceyale ] show the columns of the projection matrix of the seven algorithms on the three face image datasets .",
    "the low dimensional subspace is spanned by the column vectors , which is called bases .",
    "the bases of pca are called eigenfaces ( turk and pentland @xcite ) , while the bases of lda are called fisherfaces ( he et al .",
    "@xcite ) in previous literatures .",
    "similar methods can be applied to dla , slpp , npe , spca and men .",
    "the bases of men are sparser and have less noise than pca and dla because of its sparsity , and more grouping than spca because of its grouping effect adopted from the @xmath1 penalty .",
    "sparse bases lead to computational efficiency and good interpretation . according to fig.[fig : faceferet ] , fig.[fig : faceumist ] and fig.[fig : faceyale ] , `` men faces '' retain the most discriminative facial features , e.g. , eyebrows , eyes , nose , mouth , ears and facial contours , while leave the other parts blank . `` spca faces '' are sparse but without the grouping effect , their facial contours and organs are represented by some isolate points .",
    "`` lpp faces '' and `` npe faces '' are very similar in appearances and this fact well explains that they perform comparably in these datasets . `` dla faces '' have better description of features and less noises than those obtained by lpp , npe and flda .",
    "bases obtained from @xmath193 dimensionality reduction algorithms on feret for each column , from top to bottom : men , dla , lpp , npe , flda , pca , and spca ]     bases obtained from @xmath193 dimensionality reduction algorithms on umist for each column , from top to bottom : men , dla , lpp , npe , flda , pca , and spca ]     bases obtained from @xmath193 dimensionality reduction algorithms on yale for each column , from top to bottom : men , dla , lpp , npe , flda , pca , and spca ]    in each lars loop of the men algorithm , according to the algorithm listed in algorithm 1 , all entries of one column in the men projection matrix are zeros initially .",
    "they are sequentially added into the active set according to their importance .",
    "the values of active ones are increased with equal altering correlation . in this process , the @xmath0-norm of the column vector is augmented .",
    "fig.[fig : tree ] shows the altering tracks of some entries of the column vector in one lars loop .",
    "we called these tracks `` coefficient path '' in lars .",
    "in fig.[fig : tree ] , every coefficient path starts from zero when the corresponding variable becomes active , and changes its direction when another variable is added into the active set .",
    "all the paths keep in the directions which make the correlations of their corresponding variables equally altering .",
    "the @xmath0-norm is increasing along the greedy augment of entries .",
    "the coefficient paths proceed along the gradient decent direction of objective function on the subspace , which is spanned by the active variables .",
    "-norm in one lars loop of men ]    fig.[fig : simulation ] shows @xmath208 of the @xmath209 coefficient paths from laps loop for the first base in experiment on feret dataset .",
    "men selects ten important variables ( facial features ) sequentially here .",
    "each feature , its corresponding coefficient path and the``men fac '' when the feature is added into active set are assigned the same color which is different with the other @xmath210 features . in",
    "each `` men face '' , the new added active feature is marked by a small circle , and all the active features are marked by white crosses .",
    "the features selected by men can produce explicit interpretation of the relationship between facial features and face recognition : feature 1 is the left ear , feature 2 is the top of nose , feature 3 is on the head contour , feature 4 is the mouth , feature 5 and feature 6 are on the left eye , feature 7 is the right ear , and feature 8 is the left corner of mouth .",
    "these features are already verified of great importance in face recognition by many other famous face recognition methods .",
    "moreover , fig.[fig : simulation ] also shows men can group correlated features , for example , feature 5 and feature 6 are selected sequentially because they are both on the left eye .",
    "in addition , features which are not very important , such as feature 9 and feature 10 in fig.[fig : simulation ] , are selected after the selection of the other more significant features and assigned smaller value than those more important ones .",
    "therefore , men is a powerful algorithm in variable ( feature ) selection .",
    "entries ( features ) in one column vector ]",
    "in this paper , we propose a unifying framework which obtains a sparse projection matrix for subsequent classification , termed manifold elastic net or men for short .",
    "men incorporates the advantages of both manifold learning based dimensionality reduction and sparse learning based dimensionality reduction , but it is not a direct combination of these two . to obtain a sparse projection matrix ,",
    "men imposes the elastic net penalty over a loss function that is defined under the patch alignment framework .",
    "the objective function of men can be transformed into a lasso penalized least square problem by using a series of complex linear algebra equivalent transformations , and thus the least angle regression ( lars ) can be applied to obtain the optimal sparse projection matrix .    in men ,",
    "the patch alignment framework is first used to construct local patches of data and unifies these patches into a global coordinate system .",
    "secondly , the classification error is minimized directly via weighted principal component analysis ( pca ) over class centers .",
    "thirdly , to obtain a sparse projection matrix with the grouping effect , the elastic net penalty is added to the objective function . after a series of equivalent transformations ,",
    "men can be rewritten as a lasso - type regression .",
    "therefore , lars can be applied to solve the problem efficiently . in each lars loop for men optimization",
    ", important variables are added into the active set sequentially according to their correlation .",
    "all the elements in the active set are altered along a special direction with a special distance in each step . the special direction and distance",
    "keep the correlation of active elements identical and the largest in a lars loop .",
    "the procedure is conducted several times to obtain a set of sparse bases because these bases are independent .",
    "men enjoys advantages in several aspects : 1 ) the local geometry of intra - class samples is well preserved for low dimensional data representation , 2 ) both the margin maximization and the classification error minimization are considered for discriminative information preservation , 3 ) the sparsity of the projection matrix of men improves the parsimony in computation , 4 ) the elastic net penalty reduces the over - fitting problem , and 5 ) the projection matrix of men can be interpreted psychologically and physiologically .",
    "experimental results of face recognition on umist , feret and yale show that men performs better and more stable than popular dimensionality reduction algorithms , such as the principal component analysis ( pca ) , fisher s linear discriminant analysis ( flda ) , the discriminative locality alignment ( dla ) , the locality preserving projections with supervised setting ( lpp ) , the neighborhood preserving embedding with supervised setting ( npe ) , and the sparse principal component analysis ( spca ) .",
    "there are still many interesting properties of men which have not been targeted and formally proved in this paper . in the future",
    ", we will analyze its error bounds under different situations .",
    "another important problem in men is how to choose the optimal sparsity , so that we can remove most noise and retain most discriminative information for subsequent classification .",
    "the compressed sensing may be an effective tool to address the above concern .",
    "it is also valuable to replace the lasso penalty with the @xmath73-norm penalty to further improve men with more `` accurate sparsity '' .",
    "the lasso penalty is a relaxation of @xmath73-norm penalty , and there are alternatives which could perform better than the lasso penalty , e.g. , the smoothly clipped absolute deviation penalty ( scad ) ( fan and li @xcite ) , the reweighted @xmath0 minimization ( candes et al .",
    "@xcite ) , the adaptive lasso ( zou @xcite ) and the adaptive elastic net ( zou and zhang @xcite ) .",
    "the advantages of these methods can be adopted in men to further enhance the variable selection ability of men , and there is still a long way to go .",
    "this work was supported by ntu nap grant with project number m58020010 and the open project program of the state key lab of cad and cg ( grant no .",
    "a1006 ) , zhejiang university .                          ding , c. , li , t. : adaptive dimension reduction using discriminant analysis and k - means clustering .",
    "in : icml 07 : proceedings of the 24th international conference on machine learning .",
    "acm , new york , ny , usa ( 2007 )                    graham , d.b . ,",
    "allinson , n.m .",
    ": characterizing virtual eigensignatures for general purpose face recognition .",
    "face recognition : from theory to applications , nato asi series f , computer and system science 163 , 446456 ( 1936 )    hastie , t. , tibshirani , r. , friedman , j. : the elements of statistical learning : data mining , inference , and prediction , second edition .",
    "in statistics , springer , 2nd ed . 2009 . corr .",
    "3rd printing edn .",
    "( 2009 )    he , x. , cai , d. , yan , s. , zhang , h.j . :",
    "neighborhood preserving embedding . in : proceedings of ieee international conference on computer vision .",
    "vol .  2 , pp . 12081213 .",
    "ieee computer society , washington , dc , usa ( 2005 )                    liu , w. , tao , d. , liu , j. : transductive component analysis . in : icdm 08 : proceedings of the 2008 eighth ieee international conference on data mining .",
    "ieee computer society , washington , dc , usa ( 2008 )        phillips , p.j . , moon , h. , rizvi , s.a . ,",
    "rauss , p.j . : the feret evaluation methodology for face - recognition algorithms .",
    "pattern analysis and machine intelligence , ieee transactions on 22(10 ) , 10901104 ( 2000 )        sun , l. , ji , s. , ye , j. : a least squares formulation for canonical correlation analysis . in : icml 08 : proceedings of the 25th international conference on machine learning .",
    ". 10241031 .",
    "acm , new york , ny , usa ( 2008 )                wang , f. , chen , s. , zhang , c. , li , t. : semi - supervised metric learning by maximizing constraint margin . in : cikm 08 : proceeding of the 17th acm conference on information and knowledge management .",
    "acm , new york , ny , usa ( 2008 )      yan , s. , xu , d. , zhang , b. , zhang , h.j . , yang , q. , lin , s. : graph embedding and extensions : a general framework for dimensionality reduction .",
    "ieee trans .",
    "pattern analysis and machine intelligence .",
    "29(1 ) , 4051 ( 2007 )"
  ],
  "abstract_text": [
    "<S> it is difficult to find the optimal sparse solution of a manifold learning based dimensionality reduction algorithm . </S>",
    "<S> the lasso or the elastic net penalized manifold learning based dimensionality reduction is not directly a lasso penalized least square problem and thus the least angle regression ( lars ) ( efron et al . </S>",
    "<S> @xcite ) , one of the most popular algorithms in sparse learning , can not be applied . </S>",
    "<S> therefore , most current approaches take indirect ways or have strict settings , which can be inconvenient for applications . in this paper , we proposed the manifold elastic net or men for short . </S>",
    "<S> men incorporates the merits of both the manifold learning based dimensionality reduction and the sparse learning based dimensionality reduction . by using a series of equivalent transformations , we show men is equivalent to the lasso penalized least square problem and thus lars is adopted to obtain the optimal sparse solution of men . </S>",
    "<S> in particular , men has the following advantages for subsequent classification : 1 ) the local geometry of samples is well preserved for low dimensional data representation , 2 ) both the margin maximization and the classification error minimization are considered for sparse projection calculation , 3 ) the projection matrix of men improves the parsimony in computation , 4 ) the elastic net penalty reduces the over - fitting problem , and 5 ) the projection matrix of men can be interpreted psychologically and physiologically . </S>",
    "<S> experimental evidence on face recognition over various popular datasets suggests that men is superior to top level dimensionality reduction algorithms . </S>"
  ]
}