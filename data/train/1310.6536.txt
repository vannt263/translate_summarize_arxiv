{
  "article_text": [
    "the perceptron was introduced in the 50s as a simple model of how neurons learn @xcite .",
    "it has been extremely influential , counting both deep learning architectures and support vector machines amongst its descendants .",
    "unfortunately however , the perceptron and related models badly misrepresent important features of neurons .",
    "in particular , they treat outputs symmetrically ( @xmath3 ) rather than asymmetrically ( 0/1 ) .",
    "this is crucial since spikes ( 1s ) are more metabolically expensive than silences ( 0s ) .",
    "further , by - and - large neurons only update their synapses after spiking .",
    "by contrast , perceptrons update their weights after every misclassification .    to build a tighter link between learning algorithms and neurocomputational models , we recently discretized standard models of neuronal dynamics and plasticity to obtain the selectron @xcite .",
    "this section shows that , suitably regularized , the selectron is almost identical to linear regression .",
    "the difference is a _ selectivity _ term  arising from the spike / silence asymmetry  that encourages neurons to specialize .",
    "[ [ the - selectron . ] ] the selectron .",
    "+ + + + + + + + + + + + + +    let @xmath4 denote the set of inputs and @xmath5 the set of possible synaptic weights .",
    "let @xmath6 , where threshold @xmath7 is fixed . given @xmath8 , neuron @xmath9 with synaptic weights @xmath10 outputs 0 or 1 according to @xmath11 we model neuromodulatory signals via @xmath12 where @xmath13 , with positive values corresponding to desirable outcomes and conversely .",
    "signals may arrive after a few hundred millisecond delay , which we do not model explicitly .",
    "a threshold neuron is a selectron if its reward function takes the form @xmath14.\\ ] ]    the reward function is continuously differentiable ( in fact , linear ) as a function of @xmath15 everywhere except at the kink @xmath16 where it is continuous but not differentiable .",
    "we can therefore maximize the reward via gradient ascent to obtain synaptic updates @xmath17    [ t : limit ] discretizing gerstner s spike response model @xcite yields .",
    "discretizing spike - timing dependent plasticity ( stdp ) @xcite yields .",
    "finally , stdp is gradient ascent on a reward function whose discretization is @xmath18 .    if rewards are more common than punishments then leads to overpotentiation ( and eventually epileptic seizures ) .",
    "neuroscientists therefore introduced a depotentiation bias into stdp .",
    "alternatively , @xcite , introduced an @xmath19-constraint on synaptic weights enforced during sleep .",
    "below , we interpolate between the two approaches by replacing the constraint with a regularizer .    [",
    "[ linear - regression . ] ] linear regression .",
    "+ + + + + + + + + + + + + + + + + +    we first recall linear regression to aid the comparison .",
    "given data @xmath20 , regression finds parameters @xmath21 minimizing the mean squared error @xmath22.\\ ] ] one way to solve this is by gradient descent using @xmath23.\\ ] ]    [ [ selective - linear - regression . ] ] selective linear regression .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    now , if we regularize the reward in as follows @xmath24\\\\      \\notag      = & \\operatorname*{argmin}_{{\\mathbf{w}}_j } \\hat{{{\\mathbb e}}}\\big [ \\frac{1}{2}\\big (      \\underbrace{\\mu - \\langle{\\mathbf{w}}_j,{{\\mathbf x}}\\rangle_\\vartheta}_{\\text{regression } }      \\cdot \\underbrace{f_{{\\mathbf{w}}_j}({{\\mathbf x}})}_{\\text{selectivity } }      \\big)^2\\big]\\end{aligned}\\ ] ] the result is _ selective _ linear regression : a neuron s excess current predicts neuromodulation _ when it spikes .",
    "_ since synaptic weights are non - negative , the neuron will not fire for @xmath25 such that @xmath26<0 $ ] .",
    "computing gradient ascent obtains @xmath27      = \\sum_{\\{\\iota|n_j \\text { spikes}\\}}\\big ( \\mu^\\iota-\\langle{\\mathbf{w}}_j,{{\\mathbf x}}^\\iota\\rangle_\\vartheta\\big)\\cdot{{\\mathbf x}}^\\iota.\\ ] ] the synaptic updates are @xmath28 . in other words , if neuron @xmath9 receives spike @xmath29 and produces spike @xmath30 , then it modifies synapse @xmath31 proportional to how much greater the rescaled neuromodulatory signal is than the excess current .",
    "the selectivity term in makes biological sense .",
    "there are billions of neurons in cortex , so it is necessary that they specialize .",
    "neuromodulatory signals are thus ignored unless the neuron spikes  providing a niche wherein the neuron operates .",
    "[ [ multi - view - learning - in - cortex . ] ] multi - view learning in cortex .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    there are two main types of excitatory synapse : ampa and nmda .",
    "so far we have modeled ampa synapses which are typically feedforward and `` driving ''  they cause neurons to initiate spikes .",
    "nmda synapses differs from ampa in that @xcite    * they _ multiplicatively modulate _ synaptic updates and * they prolong , but do not initiate , spiking activity .",
    "we model the two types of synaptic inputs as ampa , @xmath25 , and nmda , @xmath32 , views with synaptic weights @xmath33 and @xmath15 respectively . in accord with the observations",
    "above , we extend by adding a _ multiplicative _ modulation term . the nmda view is encouraged to align with neuromodulators and is regularized the same as ampa .",
    "the nmda view has no selectivity term since it does not initiate spikes .",
    "finally , we obtain a ( discretized ) neuronal co - optimization algorithm , which simultaneously attempts to maximize how well each view predicts neuromodulatory signals and aligns the two views on unlabeled data : @xmath34.\\end{gathered}\\ ] ]    the next section describes a semi - supervised regression algorithm inspired by .",
    "this section translates the multiview optimization above into a workable learning algorithm .",
    "[ [ from - neurons - to - machine - learning . ] ] from neurons to machine learning .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we make three observations about cortical neurons :    * each neuron s inputs are preprocessed by millions of upstream neurons , providing a non - linear feature map analogous to the kernel trick @xcite .",
    "* a neuron s synaptic contacts ( not weights ) are random to first approximation @xcite . *",
    "the `` _ _ modulation _ _ - _ regularizers _ '' terms in resemble canonical correlation analysis @xcite : @xmath35 }          { \\sqrt{\\hat{{{\\mathbb e}}}[\\langle{\\mathbf{v}},{{\\mathbf x}}\\rangle^2 ]          \\cdot \\hat{{{\\mathbb e}}}[\\langle{\\mathbf{w}},{{\\mathbf z}}\\rangle^2 ] } }          = \\frac{\\text{``\\emph{modulation}''}}{\\text{``\\emph{regularizers}''}}.\\ ] ]    * input : * labeled data : @xmath36 and unlabeled data : @xmath37    sample @xmath38 uniformly from the dataset , compute the eigendecompositions of the sub - sampled kernel matrices @xmath39 and @xmath40 which are constructed from the samples @xmath41 and @xmath42 respectively , and featurize the input : @xmath43}{^{\\top}}\\text { for } \\mu\\in\\{1,2\\}.\\ ] ]    compute cca bases @xmath44 , @xmath45 and canonical correlations @xmath46 for the two views and set @xmath47 solve @xmath48    * output : * @xmath49    the observations suggest neurons perform an analog of    * kernelized multiview regression * with random views and * a cca penalty .    to check these form a viable combination , we put the pieces together to develop correlated nystrm views ( ` xnv ` ) , see algorithm  [ alg : xnv ] and @xcite . reassuringly",
    ", ` xnv`beats the state - of - the - art in semi - supervised learning @xcite .    [ [ multiview - regression . ] ] multiview regression .",
    "+ + + + + + + + + + + + + + + + + + + + +    the main ingredient in ` xnv`is multiview regression , which we now describe .",
    "suppose the loss of the best regressor in each view is within @xmath50 of the best joint estimator .",
    "@xmath51 introduce the canonical norm @xmath52 where @xmath53 are orthogonal solutions to with correlation coefficients @xmath54 .",
    "multiview regression is then @xmath55.\\ ] ] penalizing with the canonical norm biases the estimator towards features that are correlated across both views ( the signal ) and away from features that are uncorrelated ( the noise ) .",
    "multiview regression is thus a specific instantiation of the general co - training principle that good regressors agree across views whereas bad regressors may not .",
    "the multiview estimator s error , , compared to the best linear predictor @xmath56 , is bounded by @xmath57-\\operatorname{loss}(f )          \\leq 5\\epsilon + \\frac{\\sum_i \\lambda_i^2}{n}.\\ ] ]    according to the theorem , a slight increase in bias compared to ordinary regression is potentially more than compensated for by a large drop in variance .",
    "the reduction in variance depends on how quickly the correlation coefficients decay .",
    "for example , in the trivial case where the two views are identical , there is no benefit from multiview regression . to work well",
    ", the algorithm requires sufficiently different views ( where most basis vectors are uncorrelated ) that nevertheless both contain good regressors ( that is , the few correlated directions are of high quality ) .",
    "[ [ randomization . ] ] randomization .",
    "+ + + + + + + + + + + + + +    to convert multiview regression into a general - purpose tool we need to construct pairs of views  for any data  satisfying two requirements .",
    "first , they should contain good regressors .",
    "second , they should differ enough that their correlation coefficients decay rapidly .",
    "a computationally cheap approach that does not depend on specific properties of the data is to generate random views .",
    "to do so , we used the nystrm method @xcite and random kitchen sinks @xcite .",
    "a recent theorem of bach implies nystrm views contain good regressors in expectation @xcite and a similar result holds for random kitchen sinks .",
    "although there are currently no results on correlation coefficients across random views , recent lower bounds on the frobenius norm of the nystrm approximation suggest `` medium - sized '' views ( a few hundred dimensional ) differ sufficiently @xcite .",
    "empirical performance is discussed below .    [ [ performance . ] ] performance .",
    "+ + + + + + + + + + + +    table  [ t : performance ] summarizes experiments evaluating ` xnv`on 18 datasets , see @xcite for details .",
    "we do not report on random kitchen sinks , since they performed worse than nystrm views .",
    "performance is evaluated against kernel ridge regression ( ` krr ` ) and a randomized version of ` sssl ` , a semi - supervised algorithm with state - of - the - art performance @xcite .",
    ", the randomized version of ` sssl ` , performs similarly to the original in a fraction of the runtime . ]    ` xnv`typically outperforms ` sssl`@xmath58 by between 10% and 15% , with about 30% less variance .",
    "both semi - supervised algorithms achieve less than half the error of kernel ridge regression .",
    ".average performance of ` xnv`against ` krr ` and randomized ` sssl ` on 18 datasets . [ cols=\"<,>,>,>,>,>\",options=\"header \" , ]     importantly , ` xnv`is _",
    "fast_. for @xmath59 points , ` xnv`runs in @xmath60 on a laptop whereas ( unrandomized ) ` sssl ` takes @xmath61 . for @xmath62 points ,",
    "` xnv ` s runtime is @xmath63 whereas ` sssl ` takes unfeasibly long .",
    "we describe work in progress on multiview neuronal regression .    [ [ selective - co - regularized - least - squares . ] ] selective co - regularized least squares .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the multiview optimization in can be rewritten , essentially , as co - regularized least squares @xcite with a selectivity term encouraging specialization : @xmath64      \\\\",
    "\\notag      + \\ ; & \\hat{{{\\mathbb e}}}_{\\text{unlabeled}}\\big [ \\alpha_{co}\\cdot       \\underbrace{\\big(\\langle{\\mathbf{v}}_j,{{\\mathbf x}}\\rangle_\\vartheta           -\\langle{\\mathbf{w}}_j,{{\\mathbf z}}\\rangle\\big)^2}_{\\text{unsupervised co - regularization } }      \\big].\\end{aligned}\\ ] ]",
    "the model closely resembles ` xnv ` , with a penalty that is easier for neurons to implement .    the selectivity term in ensures that neurons only predict the neuromodulatory signals when they spike . in other words , neurons have the flexibility to search for an ampa view containing a good regressor .",
    "the nmda weights are then simultaneously aligned with the neuromodulatory signal and the ampa weights by the remaining two terms .",
    "[ [ guarantees . ] ] guarantees .",
    "+ + + + + + + + + + +    theoretical guarantees for co - regularization are provided in @xcite .",
    "as above , they depend on having good regressors in sufficiently different views .",
    "we briefly sketch how these apply .",
    "the benefit from co - training depends on the extent to which co - regularizing with unlabeled shrinks the function space applied to the labeled data . in short ,",
    "it depends on the rademacher complexity of @xmath65 + \\hat{{{\\mathbb e}}}_{\\textrm{unl}}\\big[\\alpha_{co}\\cdot|f({{\\mathbf x}})-g({{\\mathbf z}})|^2\\big]\\leq 1\\big\\}.\\ ] ] denote the gram matrices of the two views by @xmath66 where @xmath67 and @xmath68 are dot - products of unlabeled data on the respective views , and other blocks are similarly constructed using mixed and labeled data .",
    "it is shown in @xcite that @xmath69 the last term is of particular interest .",
    "each column @xmath70 represents a labeled point in the first view by its dot - product with unlabeled points , and similarly for @xmath71 in the second view .",
    "the greater the _ difference _ between the representations in the two views , as measured by , the lower the rademacher complexity and the better the generalization bounds on co - training .",
    "it remains to be seen how selective multiview regression performs empirically , and to what extent provides a good guide to the improvement in generalization performance of the original _ undiscretized _ models .",
    "co - training and randomization are two simple , powerful methods that complement each other well  and which neurons appear to use in conjunction .",
    "no doubt there are more tricks waiting to be discovered .",
    "it is particularly intriguing that nell , one of the more ambitious ai projects in recent years , uses various co - training strategies as basic building blocks @xcite . despite a large body of research on how humans learn categories and relations",
    ", it remains unknown how ( or whether ) individual neurons learn categories .",
    "although the results sketched here are suggestive , they fall far short of the full story . for example , since neurons learn online  and only when they spike  they face similar explore / exploit dilemmas to those investigated in the literature on bandits . it will be interesting to see if new ( randomized ) bandit algorithms can be extracted from models of synaptic plasticity ."
  ],
  "abstract_text": [
    "<S> despite its size and complexity , the human cortex exhibits striking anatomical regularities , suggesting there may simple meta - algorithms underlying cortical learning and computation . </S>",
    "<S> we expect such meta - algorithms to be of interest since they need to operate quickly , scalably and effectively with little - to - no specialized assumptions .    </S>",
    "<S> this note focuses on a specific question : how can neurons use vast quantities of _ unlabeled _ data to speed up learning from the comparatively rare labels provided by reward systems ? as a partial answer , we propose randomized co - training as a biologically plausible meta - algorithm satisfying the above requirements . as evidence </S>",
    "<S> , we describe a biologically - inspired algorithm , correlated nystrm views ( ` xnv ` ) that achieves state - of - the - art performance in semi - supervised learning , and sketch work in progress on a neuronal implementation .    </S>",
    "<S> although staggeringly complex , the human cortex has a remarkably regular structure @xcite . </S>",
    "<S> for example , even expert anatomists find it difficult - to - impossible to distinguish between slices of tissue taken from , say , visual and prefrontal areas . </S>",
    "<S> this suggests there may be simple , powerful meta - algorithms underlying neuronal learning .    </S>",
    "<S> consider one problem such a meta - algorithm should solve : taking advantage of massive quantities of unlabeled data . </S>",
    "<S> evolution has provided mammals with neuromodulatory systems , such as the dopamenergic system , that assign labels ( e.g. pleasure or pain ) to certain outcomes . </S>",
    "<S> however , these labels are rare ; an organism s interactions with its environment are typically indifferent . </S>",
    "<S> nevertheless , mammals often generalize accurately from just a few good or bad outcomes . </S>",
    "<S> our problem is therefore to understand how organisms , and more specifically individual neurons , use unlabeled data to learn quickly and accurately .    </S>",
    "<S> next , consider some properties a semi - supervised _ neuronal _ learning algorithm should have </S>",
    "<S> . it should be :    * fast ; * scalable ; * effective ; * broadly applicable ( that is , requiring few - to - no specialized assumptions ) ; and * biologically plausible .    </S>",
    "<S> the first four requirements are of course desirable properties in any learning algorithm . </S>",
    "<S> the fourth requirement is particularly important due to the wide range of environments , both stochastic and adversarial , that organisms are exposed to .    </S>",
    "<S> regarding the fifth requirement , it is unlikely that evolution has optimized all of the cortex s @xmath0 connections , especially given the explosive growth in brain size over the last few million years . </S>",
    "<S> a simpler explanation , fitting neurophysiological evidence , is that the macroscopic connectivity ( largely the white matter ) was optimized , and the details are filled in randomly .    </S>",
    "<S> [ [ contribution . ] ] contribution . </S>",
    "<S> + + + + + + + + + + + + +    this note proposes _ randomized co - training _ as a semi - supervised meta - algorithm that satisfies the five criteria above . </S>",
    "<S> in particular , we argue that randomizing cortical connectivity is not only necessary but also beneficial .    [ </S>",
    "<S> [ co - training . ] ] co - training . </S>",
    "<S> + + + + + + + + + + + +    a co - training algorithm takes labeled @xmath1 and unlabeled data @xmath2 consisting of two views @xcite . </S>",
    "<S> examples of views are audio and visual recordings of objects or photographs taken from different angles . </S>",
    "<S> the key insight is that good predictors will agree on both views whereas bad predictors may not @xcite . </S>",
    "<S> co - training algorithms therefore use unlabeled data to eliminate predictors disagreeing across views , shrinking the search space used on the labeled data  resulting in better generalization bounds and improved empirical performance @xcite .    the most spectacular application of co - training is perhaps never - ending language learning ( nell ) , a semi - autonomous agent that updates a massive database of beliefs about english language categories and relations based on information extracted from the web @xcite .    </S>",
    "<S> however , despite its conceptual elegance , co - training remains a niche method . </S>",
    "<S> a possible reason for the lack of applications is that it is difficult to find naturally occurring views satisfying the technical assumptions required to improve performance . </S>",
    "<S> constructing randomized views is a cheap workaround that dramatically extends co - training s applicability .    </S>",
    "<S> [ [ outline . ] ] outline . </S>",
    "<S> + + + + + + + +    section  [ s : selectron ] shows that discretizing standard models of synaptic dynamics and plasticity @xcite leads to a small tweak on linear regression . incorporating nmda synapses into the model as a second view leads to a neuronal co - training algorithm .    </S>",
    "<S> section  </S>",
    "<S> [ s : xnv ] reviews recent work which translated the above observations about neurons into a learning algorithm . </S>",
    "<S> we introduce correlated nystrm views ( ` xnv ` ) , a state - of - the - art semi - supervised learning algorithm that combines multi - view regression with random views via the nystrm method @xcite .    </S>",
    "<S> finally ,  [ s : rcc ] returns to neurons and sketches preliminary work analyzing the benefits of randomized co - training in cortex . </S>"
  ]
}