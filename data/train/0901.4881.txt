{
  "article_text": [
    "different regression models have been proposed for lifetime data such as those based on the gamma , lognormal and weibull distributions .",
    "these models typically provide a satisfactory fit in the middle portion of the data , but very often fail to deliver a good fit at the tails , where only a few observations are generally available .",
    "the family of distributions proposed by birnbaum and saunders ( 1969 ) can also be used to model lifetime data and it is widely applicable to model failure times of fatiguing materials .",
    "this family has the appealing feature of providing satisfactory tail fitting .",
    "this family of distributions was originally obtained from a model for which failure follows from the development and growth of a dominant crack .",
    "it was later derived by desmond ( 1985 ) using a biological model which followed from relaxing some of the assumptions originally made by birnbaum and saunders ( 1969 ) .",
    "the random variable @xmath0 is said to be birnbaum ",
    "saunders distributed with parameters @xmath1 , say @xmath2-@xmath3 , if its cumulative distribution function ( cdf ) is given by @xmath4,\\quad t > 0,\\ ] ] where @xmath5 is the standard normal distribution function and @xmath6 and @xmath7 are shape and scale parameters , respectively .",
    "it is easy to show that @xmath7 is the median of the distribution : @xmath8 . for any @xmath9 , then @xmath10-@xmath11 .",
    "mccarter  ( 1999 ) considered parameter estimation under type ii data censoring for the @xmath2-@xmath12 distribution .",
    "lemonte et al .",
    "( 2007 ) derived the second - order biases of the maximum likelihood estimates ( mles ) of @xmath6 and @xmath7 , and obtained a corrected likelihood ratio statistic for testing the parameter @xmath6 .",
    "lemonte et al .",
    "( 2008 ) proposed several bootstrap bias corrected estimates of @xmath6 and @xmath7 .",
    "further details on the birnbaum  saunders distribution can be found in johnson et al .",
    "( 1995 ) .",
    "rieck and nedelman ( 1991 ) proposed a log - linear regression model based on the birnbaum ",
    "saunders distribution .",
    "they showed that if @xmath13-@xmath12 , then @xmath14 is sinh - normal distributed , say @xmath15 , with shape , location and scale parameters given by @xmath6 , @xmath16 and @xmath17 , respectively .",
    "their model has been widely used as an alternative model to the gamma , lognormal and weibull regression models ; see rieck and nedelman ( 1991 ,   7 ) .",
    "diagnostic tools for the birnbaum  saunders regression model were developed by galea et al .",
    "( 2004 ) , leiva et al .",
    "( 2007 ) and xie and wei ( 2007 ) , and the bayesian inference was considered by tisionas ( 2001 ) .    in this paper",
    "we propose a class of birnbaum  saunders nonlinear regression models which generalizes the regression model introduced by rieck and nedelman ( 1991 ) .",
    "we discuss maximum likelihood estimation of the regression parameters and obtain the fisher information matrix .",
    "as is well known , however , the mles , although consistent , are typically biased in finite samples . in order to overcome this shortcoming , we derive a closed - form expression for the bias of the mle in these models which is used to define a bias corrected estimate .    bias adjustment has been extensively studied in the statistical literature .",
    "in fact , cook et al .",
    "( 1986 ) proposed bias correction in normal nonlinear models .",
    "young and bakir ( 1987 ) obtained bias corrected estimates for a generalized log - gamma regression model .",
    "cordeiro and mccullagh ( 1991 ) gave general matrix formulae for bias correction in generalized linear models , whereas paula ( 1992 ) derive the second - order biases in exponential family nonlinear models .",
    "cordeiro et al .",
    "( 2000 ) obtained bias correction for symmetric nonlinear regression models .",
    "more recently , vasconcellos and cribari  neto ( 2005 ) calculate the biases of the mles in a new class of beta regression .",
    "cordeiro and demtrio ( 2008 ) propose formulae for the second - order biases of the maximum quasi - likelihood estimates , whereas cordeiro and toyama ( 2008 ) derive the second - order biases in generalized nonlinear models with dispersion covariates .",
    "the rest of the paper is as follows .",
    "section  [ reg_nonlinear ] introduces the class of birnbaum - saunders nonlinear regression models and discusses maximum likelihood estimation . using general results from cox and snell ( 1968 ) , we derive in section  [ bias ] the second - order biases of the mles of the nonlinear parameters in our class of models and define bias corrected estimates",
    "some special models are considered in section 4 .",
    "simulation results are presented and discussed in section  [ simulation ] for two nonlinear regression models .",
    "we show that the bias corrected estimates are nearly unbiased with mean squared errors very close to the corresponding ones of the uncorrected estimates .",
    "section  [ application ] gives an application of the proposed regression model to a real fatigue data set , which provides a better fit at the tail of the data .",
    "finally , section  [ conclusions ] concludes the paper .",
    "let @xmath13-@xmath3 .",
    "the density function of @xmath18 has the form ( rieck and nedelman , 1991 ) @xmath19 this distribution has a number of interesting properties ( rieck , 1989 ) : ( i ) it is symmetric around the location parameter @xmath20 ; ( ii ) it is unimodal for @xmath21 and bimodal for @xmath22 ; ( iii ) the mean and variance of @xmath23 are @xmath24 and var@xmath25 , respectively .",
    "there is no closed - form expression for @xmath26 , but rieck ( 1989 ) obtained asymptotic approximations for both small and large values of @xmath6 ; ( iv ) if @xmath27 , then @xmath28 converges in distribution to the standard normal distribution when @xmath29 .",
    "we define the nonlinear regression model @xmath30 where @xmath31 is the logarithm of the @xmath32th observed lifetime , @xmath33 is an @xmath34 vector of known explanatory variables associated with the @xmath32th observable response @xmath31 , @xmath35 is a vector of unknown nonlinear parameters , and @xmath36 .",
    "we assume a nonlinear structure for the location parameter @xmath37 in model  ( [ eq1 ] ) , say @xmath38 , where @xmath39 is assumed to be a known and twice continuously differentiable function with respect to @xmath40 . for the linear regression @xmath41 , the model  ( [ eq1 ] ) reduces to rieck and nedelman s ( 1991 ) model .    the log - likelihood function for the vector parameter @xmath42 from a random sample @xmath43 obtained from  ( [ eq1 ] ) , except for constants ,",
    "can be expressed as @xmath44 where @xmath45/2)$ ] , @xmath46/2)$ ] for @xmath47 .",
    "the function @xmath48 is assumed to be regular ( cox and hinkley , 1974 , ch .",
    "9 ) with respect to all @xmath40 and @xmath6 derivatives up to third order .",
    "further , the @xmath49 local matrix @xmath50 of partial derivatives of @xmath51 with respect to @xmath40 is assumed to be of full rank , i.e. , rank(@xmath52 for all @xmath40 .",
    "the nonlinear predictors @xmath53 are embedded in an infinite sequence of @xmath54 vectors that must satisfy these regularity conditions for the asymptotics to be valid . under these assumptions ,",
    "the mles have good asymptotic properties such as consistency , sufficiency and normality .",
    "the derivatives with respect to the components of @xmath40 and @xmath6 are denoted by : @xmath55 , @xmath56 , @xmath57 , @xmath58 , @xmath59 , @xmath60 , etc .",
    "further , we use the following notation for joint cumulants of log - likelihood derivatives : @xmath61 , @xmath62 , @xmath63 , etc .",
    "let @xmath64 , etc .",
    "all @xmath65 s and their derivatives are assumed to be of order @xmath66 . also , we adopt the notation @xmath67 and @xmath68 for the first and second partial derivatives of @xmath37 with respect to the elements of @xmath40 .",
    "it is easy to see by differentiating  ( [ eq2 ] ) that @xmath69 @xmath70 @xmath71 the score function for @xmath40 is @xmath72 , where @xmath73 is an @xmath74-vector whose @xmath32th element is equal to @xmath75 .",
    "it is well - known that , under general regularity conditions ( cox and hinkley , 1974 , ch .",
    "9 ) , the mles are consistent , asymptotically efficient and asymptotically normal .",
    "let @xmath76 be the mle of @xmath42 .",
    "we can write @xmath77 for @xmath74 large , where @xmath78 denotes approximately distributed , @xmath79 is the block - diagonal fisher information matrix given by @xmath80 , @xmath81 is its inverse , @xmath82 is the information matrix for @xmath40 and @xmath83 is the information for @xmath6 .",
    "also , @xmath84 where @xmath85 is the error function given by @xmath86 details on @xmath87 can be found in gradshteyn and ryzhik ( 2007 ) . since @xmath79 is block - diagonal , the vector @xmath40 and the scalar @xmath6 are globally orthogonal ( cox and reid , 1987 ) and @xmath88 and @xmath89 are asymptotically independent .",
    "it can be shown ( rieck , 1989 ) that @xmath90 for @xmath6 small and @xmath91 for @xmath6 large .",
    "the mle @xmath92 satisfies @xmath93 equations @xmath94 for the components of @xmath40 and @xmath6 .",
    "the fisher scoring method can be used to estimate @xmath40 and @xmath6 simultaneously by iteratively solving the equations @xmath95 where @xmath96 and @xmath97 for @xmath98 .",
    "the above equations show that any software with a weighted linear regression routine can be used to calculate the mles of @xmath40 and @xmath6 iteratively . initial approximations @xmath99 and @xmath100 for the iterative algorithm are used to evaluate @xmath101 and @xmath102 from which these equations can be used to obtain the next estimates @xmath103 and @xmath104 .",
    "these new values can update @xmath105 and @xmath106 and so the iterations continue until convergence is achieved .",
    "we now obtain some joint cumulants of log - likelihood derivatives and their derivatives : @xmath107 @xmath108 @xmath109    let @xmath110 and @xmath111 be the @xmath112 biases of @xmath113 ( @xmath114 ) and @xmath89 , respectively .",
    "the use of cox and snell s ( 1968 ) formula to obtain these biases is greatly simplified , since @xmath40 and @xmath6 are globally orthogonal and the cumulants corresponding to the parameters in @xmath40 are invariant under permutation of these parameters . from now on we use einstein summation convention with the indices varying over the corresponding parameters .",
    "we have @xmath115 and @xmath116 where @xmath117 is the @xmath118th element of the inverse @xmath119 of the information matrix for @xmath40 , @xmath120 and @xmath121 denotes the summation over all combinations of parameters @xmath122 .",
    "first , we consider equation  ( [ vies - beta ] ) from which we readily have that the second sum is zero since @xmath123 .",
    "it follows that @xmath124 by rearranging the summation terms we obtain @xmath125 let @xmath126 @xmath127 and @xmath128 @xmath129 be vectors containing the first and second partial derivatives of the mean @xmath37 with respect to the @xmath130 s .",
    "we can write the above equation in matrix notation as @xmath131 where @xmath132 is the @xmath133th row of the @xmath134 identity matrix and vec@xmath135 is the operator which transforms a matrix into a vector by stacking the columns of the matrix one underneath the other .",
    "it is straightforward to check that @xmath136 where @xmath137 and @xmath138 are @xmath49 and @xmath139 matrices of the first and second partial derivatives of the mean vector @xmath140 with respect to @xmath40 , respectively .",
    "the @xmath112 bias vector @xmath141 of @xmath88 can then be written as @xmath142 where @xmath143 is an @xmath144 vector defined as @xmath145 .",
    "we now calculate the @xmath112 bias of @xmath89 . using  ( [ vies - alpha ] ) , we obtain @xmath146 where @xmath147 denotes the trace operator .",
    "now , making use of the fact that @xmath148 , we can rewrite the @xmath112 bias of @xmath89 as @xmath149    equations  ( [ bias - beta ] ) and  ( [ bias - alpha ] ) represent the main results of the paper . the bias vector @xmath141 can be obtained from a simple ordinary least - squares regression of @xmath143 on the columns of @xmath150 .",
    "it depends on the nonlinearity of the regression function @xmath151 and the parameter @xmath6 .",
    "the bias vector @xmath141 will be small when @xmath143 is orthogonal to the columns of @xmath150 .",
    "also , it can be large when @xmath152 and @xmath74 are both small .",
    "equation  ( [ bias - beta ] ) is easily handled algebraically for any type of nonlinear regression , since it involves simple operations on matrices and vectors .",
    "for special models with closed - form information matrix for @xmath40 , it is possible to obtain closed - form expressions for @xmath141 . for linear models , the matrix @xmath153 and the vector @xmath143 vanish and hence @xmath154 , which is in agreement with the result due to rieck and nedelman ( 1991 , p.  54 ) that the mles are unbiased to order @xmath112 .",
    "expression ( 6 ) depends directly on the nonlinear structure of the regression model only through the rank @xmath155 of @xmath150 .",
    "it shows that the bias is always a linear function of the dimension @xmath155 of @xmath40 .    in the right - hand sides of expressions  ( [ bias - beta ] ) and  ( [ bias - alpha ] ) , which are both of order @xmath112",
    ", consistent estimates of the parameters @xmath40 and @xmath6 can be inserted to define bias corrected estimates @xmath156 and @xmath157 , where @xmath158 and @xmath159 are the values of @xmath141 and @xmath111 , respectively , at @xmath160 . the bias corrected estimates @xmath161 and",
    "@xmath162 are expected to have better sampling properties than the classical mles @xmath88 and @xmath89 .",
    "in fact , we present some simulations in section  [ simulation ] to show that @xmath161 and @xmath162 have smaller biases than their corresponding uncorrected estimates , thus suggesting that these bias corrections have the effect of shrinking the adjusted estimates toward to the true parameter values .",
    "however , we can not say that the bias corrected estimates offer always some improvement over the mles , since they can have mean squared errors larger .",
    "it is worth emphasizing that there are other methods to obtain bias corrected estimates . in regular parametric problems , firth ( 1993 ) developed the so - called `` preventive '' method , which also allows for the removal of the second - order bias .",
    "his method consists of modifying the original score function to remove the first - order term from the asymptotic bias of these estimates . in exponential families with canonical parameterizations ,",
    "his correction scheme consists in penalizing the likelihood by the jeffreys invariant priors .",
    "this is a preventive approach to bias adjustment which has its merits , but the connections between our results and his work are not pursued in this paper since they could be developed in future research .",
    "additionally , it should be mentioned that it is possible to avoid cumbersome and tedious algebra on cumulant calculations by using efron s bootstrap ( efron and tibshirani , 1993 ) .",
    "we use the analytical approach here since this leads to a nice formula .",
    "moreover , the application of the analytical bias approximation seems to generally be the most feasible procedure to use and it continues to receive attention in the literature .",
    "we now calculate the second - order bias @xmath163 of the mle @xmath164 of the @xmath32th mean @xmath38 .",
    "we can easily show by taylor series expansion that @xmath165 where @xmath166 is a @xmath134 matrix of second partial derivatives @xmath167",
    "( for @xmath168 ) , @xmath169 is the asymptotic covariance matrix of @xmath88 and the vectors @xmath170 and @xmath141 were mentioned previously .",
    "all quantities in the above equation should be evaluated at @xmath88 .",
    "the asymptotic variance of @xmath171 can also be expressed explicitly in terms of the covariance of @xmath88 by @xmath172",
    "equation  ( [ bias - beta ] ) is easily handled algebraically for any type of nonlinear model , since it involves simple operations on matrices and vectors .",
    "this equation , in conjunction with a computer algebra system such as maple ( abell and braselton , 1994 ) will compute @xmath141 algebraically with minimal effort .",
    "in particular , ( [ bias - beta ] ) may simplify considerably if the number of nonlinear parameters is small .",
    "moreover , for any nonlinear special model , we can calculate the bias @xmath141 numerically via a software with numerical linear algebra facilities such as ox ( doornik , 2001 ) and r ( r development core team , 2008 ) .",
    "first , we consider a nonlinear regression model which depends on a single nonlinear parameter .",
    "equation  ( [ bias - beta ] ) gives @xmath173 where @xmath174 and @xmath175 .",
    "the constants @xmath176 and @xmath177 are evaluated at @xmath178 and @xmath89 to yield @xmath179 and the corrected estimate @xmath180 . for example , the simple exponential model @xmath181 leads to @xmath182 and @xmath183 .    as a second example",
    ", we consider a partially nonlinear regression model defined by @xmath184 where @xmath185 is a known @xmath186 matrix of full rank , @xmath187 is an @xmath144 vector , @xmath188 , @xmath189 and @xmath7 and @xmath190 are scalar parameters .",
    "this class of models occurs very often in statistical modeling ; see cook et al .",
    "( 1986 ) and cordeiro et al .",
    "( 2000 ) . for example , @xmath191 ( gallant , 1975 ) , @xmath192 ( darby and ellis , 1976 ) and @xmath193 ( stone , 1980 ) .",
    "ratkowsky ( 1983 , ch .",
    "5 ) discusses several models of the form  ( [ mspecial ] ) which include the asymptotic regression and weibull - type models given by @xmath194 and @xmath195 , respectively .",
    "the @xmath49 local model matrix @xmath150 takes the form @xmath196 $ ] and , after some algebra , we can obtain from  ( [ bias - beta ] ) @xmath197,\\ ] ] where @xmath198 is a @xmath199 vector with a one in the last position and zeros elsewhere , @xmath200 ) is simply the set of coefficients from the ordinary regression of the vector @xmath201 on the matrix @xmath150 , and @xmath202 and @xmath203 are the large - sample second moments obtained from the appropriate elements of the asymptotic covariance matrix @xmath204 .",
    "it is clear from  ( [ b1 ] ) that @xmath141 does not depend explicitly on the linear parameters in @xmath205 and it is proportional to @xmath206 .",
    "further , the covariance term @xmath203 contributes only to the bias of @xmath207 .",
    "we now use monte carlo simulation to evaluate the finite - sample performance of the mles of the parameters and of their corrected versions in two nonlinear regression models .",
    "the mles of the parameters were obtained by maximizing the log - likelihood function using the bfgs quasi - newton method with analytical derivatives .",
    "this method is generally regarded as the best - performing nonlinear optimization method ( mittelhammer et al . , 2000 , p.  199 ) .",
    "the covariate values were selected as random draws from the uniform @xmath208 distribution and for fixed @xmath74 those values were kept constant throughout the experiment . also , the number of monte carlo replications was 10,000 .",
    "all simulations were performed using the ox matrix programming language ( doornik , 2001 ) .    in order to analyze the performance of the estimates ,",
    "we computed , for each sample size and for each estimate : the relative bias ( the relative bias of an estimate @xmath209 , defined as @xmath210 , is obtained by estimating @xmath211 by monte carlo ) and the root mean square error ( @xmath212 ) , where mse is the estimated mean square error from the 10,000 monte carlo replications .",
    "first , we consider the nonlinear regression model @xmath213 where @xmath36 for @xmath47 .",
    "the sample sizes were @xmath214 and 45 . without loss of generality ,",
    "the true values of the regression parameters were taken as @xmath215 , @xmath216 , @xmath217 , @xmath218 and @xmath219 and @xmath220 .",
    "table  [ tab1 ] gives the relative biases of both uncorrected and corrected estimates to show that the bias corrected estimates are much closer to the true parameters than the unadjusted estimates .",
    "for instance , when @xmath221 and @xmath222 , the average of the estimated relative biases for the estimates of the model parameters is @xmath223 , whereas the average of the estimated relative biases for the corrected estimates is @xmath224 .",
    "hence , the average bias ( in absolute value ) of the mles is almost four times greater than the average bias of the corrected estimates",
    ". this fact suggests that the second - order bias of the mles should not be ignored in samples of small to moderate size since they can be non - negligible .",
    "the figures in table 2 show that the root mean squared errors of the uncorrected and corrected estimates are very close .",
    "hence , the figures in both tables suggest that the corrected estimates have good properties .    ccl rrrrr@xmath6 & @xmath74 & & @xmath225 & @xmath226 & @xmath227 & @xmath228 & @xmath229 + 0.5 & 15 & mle & 0.0006 & @xmath230 & 0.0011 & 0.0020 & @xmath231 + & & bce & 0.0007 & @xmath232 & 0.0001 & 0.0008 & @xmath233 + & 30 & mle & 0.0001 & @xmath230 & 0.0013 & 0.0009 & @xmath234 + & & bce & 0.0002 & @xmath235 & 0.0007 & @xmath236 & @xmath237 + & 45 & mle & 0.0003 & @xmath235 & 0.0007 & 0.0008 & @xmath238 + & & bce & 0.0003 & @xmath232 & 0.0003 & 0.0001 & @xmath239 +    1.5 & 15 & mle & @xmath240 & @xmath224 & 0.0248 & 0.0197 & @xmath241 + & & bce & @xmath242 & @xmath243 & 0.0113 & 0.0056 & @xmath244 + & 30 & mle & @xmath245 & @xmath246 & 0.0079 & 0.0078 & @xmath247 + & & bce & @xmath232 & @xmath248 & 0.0027 & 0.0012 & @xmath249 + & 45 & mle & @xmath250 & @xmath251 & 0.0052 & 0.0026 & @xmath252 + & & bce & @xmath253 & @xmath248 & 0.0023 & @xmath254 & @xmath255 +    ccl rrrrr@xmath6 & @xmath74 & & @xmath225 & @xmath226 & @xmath227 & @xmath228 & @xmath229 + 0.5 & 15 & mle & 0.4093 & 0.4920 & 0.2707 & 0.0924 & 0.1234 + & & bce & 0.4093 & 0.4921 & 0.2709 & 0.0922 & 0.1067 + & 30 & mle & 0.3006 & 0.3806 & 0.2113 & 0.0688 & 0.0763 + & & bce & 0.3006 & 0.3806 & 0.2114 & 0.0686 & 0.0702 + & 45 & mle & 0.2434 & 0.2874 & 0.1768 & 0.0567 & 0.0590 + & & bce & 0.2434 & 0.2874 & 0.1769 & 0.0566 & 0.0555 +    1.5 & 15 & mle & 1.6302 & 1.1230 & 0.9756 & 0.3235 & 0.3938 + & & bce & 1.6333 & 1.1274 & 0.9819 & 0.3152 & 0.3315 + & 30 & mle & 0.9684 & 0.7003 & 0.5785 & 0.1931 & 0.2399 + & & bce & 0.9693 & 0.7011 & 0.5807 & 0.1908 & 0.2155 + & 45 & mle & 0.6505 & 0.5575 & 0.3895 & 0.1318 & 0.1837 + & & bce & 0.6507 & 0.5577 & 0.3901 & 0.1311 & 0.1700 +    when the parameter @xmath6 increases , the finite - sample performance of the mles deteriorates ( see tables  [ tab1 ] and  [ tab2 ] ) .",
    "for instance , when @xmath221 , the relative biases of @xmath207 ( mle ) and @xmath256 ( bce ) were 0.0020 and 0.0008 ( for @xmath219 ) and 0.0197 and 0.0056 ( for @xmath222 ) , which indicate an increase in the relative biases of nearly 10 and 7 times , respectively . also , the root mean squared errors in the same order were 0.0924 and 0.0922 ( for @xmath219 ) and 0.3235 and 0.3152 ( for @xmath222 ) .",
    "next , we consider the very known michaelis  menton model , which is very useful for estimating growth curves , where it is common for the response to approach an asymptote as the stimulus increases .",
    "the michaelis ",
    "menton model ( ratkowsky , 1983 ) provides an hyperbolic form for @xmath37 against @xmath257 given by @xmath258 where the curve has an asymptote at @xmath259 . here",
    ", the sample sizes were @xmath260 and 50 . also , the true values of the regression parameters were taken as @xmath261 and @xmath262 , with @xmath219 .",
    "table  [ tab3 ] gives the relative biases and root mean squared errors of the uncorrected and corrected estimates .",
    "the figures in this table reveal that the mles of the parameters can be substantially biased , even when @xmath263 , and that the bias correction is very effective . in terms of mse ,",
    "the adjusted estimates are slightly better than the ordinary mles .",
    "cl rrr|rrr & & & + @xmath74 & & @xmath7 & @xmath190 & @xmath6 & @xmath7 & @xmath190 & @xmath6 + 20 & mle & 0.0476 & 0.1718 & @xmath264 & 0.6984 & 0.3947 & 0.0859 + & bce & @xmath245 & @xmath265 & @xmath266 & 0.5264 & 0.2783 & 0.0847 +    30 & mle & 0.0313 & 0.1077 & @xmath267 & 0.5245 & 0.2750 & 0.0684 + & bce & @xmath268 & @xmath269 & @xmath270 & 0.4478 & 0.2252 & 0.0678 +    40 & mle & 0.0215 & 0.0754 & @xmath271 & 0.4222 & 0.2207 & 0.0582 + & bce & @xmath236 & @xmath272 & @xmath273 & 0.3835 & 0.1954 & 0.0578 +    50 & mle & 0.0160 & 0.0558 & @xmath274 & 0.3609 & 0.1862 & 0.0516 + & bce & @xmath275 & @xmath236 & @xmath254 & 0.3380 & 0.1710 & 0.0514 +",
    "obviously , due to the genesis of the birnbaum  saunders distribution , the fatigue processes are by excellence ideally modeled by this model .",
    "we now consider an application to a biaxial fatigue data set reported by rieck and nedelman ( 1991 ) on the life of a metal piece in cycles to failure .",
    "the response @xmath276 is the number of cycles to failure and the explanatory variable @xmath277 is the work per cycle ( mj / m@xmath278 ) .",
    "the data of forty six observations were taken from table 1 of galea et al .",
    "( 2004 ) .",
    "rieck and nedelman ( 1991 ) proposed the following model for the biaxial fatigue data : @xmath279 where @xmath280 and @xmath281 , for @xmath282 .",
    "the mles ( the corresponding standard errors in parentheses ) are : @xmath283(0.3942 ) , @xmath284(0.1096 ) and @xmath285(0.0428 ) .",
    "we take the logarithm of @xmath277 to ensure a linear relationship between the response variable ( @xmath286 ) and the covariate in  ( [ rnmodel ] ) ; see galea et al .",
    "( 2004 , figure 1 ) .",
    "however , figure  [ fig1 ] suggests a nonlinear relationship between the response variable and the covariate @xmath277 .    here , we proposed the nonlinear regression model @xmath287 where @xmath36 .",
    "the mles ( the standard errors in parentheses ) are : @xmath288(0.7454 ) , @xmath289(0.5075 ) , @xmath290(7.3778 ) and @xmath291(0.0417 ) .",
    "the bias corrected estimates are : @xmath292(0.7734 ) , @xmath293(0.5266 ) , @xmath294(7.6548 ) and @xmath295(0.0433 ) .",
    "hence , the uncorrected estimates are slightly different from the bias corrected estimates even for large samples ( @xmath296 observations ) .",
    "figure  [ fig2 ] gives the scatter - plot of the data , the fitted model  ( [ nonlinear ] ) and the fitted straight line , say @xmath297 , where the mles are : @xmath298(0.1622 ) , @xmath299(0.0036 ) and @xmath300(0.0542 ) .",
    "figure  [ fig2 ] shows that the nonlinear model ( 10 ) ( unlike the linear model ) fits satisfactorily to the fatigue data . the @xmath301th observation ( the one with work per cycle near 100 )",
    "can be an influential data .",
    "however , it is not possible to say whether this observation is influential or not without using an efficient way to detect influential observations in the new class of models .",
    "influence diagnostic analysis for this class of models will be developed in future research .        following xie and wei ( 2007 )",
    ", we obtain the residuals @xmath302 and @xmath303 .",
    "figure  [ fig3 ] gives the scatter - plot of @xmath304 versus the predicted values @xmath171 for both fitted models : ( i ) @xmath305 ; and ( ii ) @xmath306 .",
    "figure  [ fig3 ] shows that the distribution of @xmath304 is approximately normal for model ( ii ) but this is not true for model ( i ) .",
    "based upon the fact that @xmath307 if @xmath308 , then the residual @xmath309 should follow approximately a sinh - normal distribution .     versus @xmath171.,title=\"fig:\",width=453,height=340 ]   versus @xmath171.,title=\"fig:\",width=453,height=340 ]",
    "the birnbaum  saunders distribution is widely used to model times to failure for materials subject to fatigue . the purpose of the paper was two fold .",
    "first , we propose a new class of birnbaum  saunders nonlinear regression models which generalizes the regression model described in rieck and nedelman ( 1991 ) .",
    "second , we give simple formulae for calculating bias corrected maximum likelihood estimates of the parameters of these models .",
    "the simulation results presented show that the bias correction derived is very effective , even when the sample size is large . indeed , the bias correction mechanism adopted yields adjusted maximum likelihood estimates which are nearly unbiased .",
    "we also present an application to a real fatigue data set that illustrates the usefulness of the proposed model .",
    "future research will be devoted to a study of diagnostics and influence analysis in the new class of nonlinear models .",
    "we gratefully acknowledge grants from fapesp and cnpq ( brazil ) .",
    "the authors are also grateful to an associate editor and two referees for helpful comments and suggestions ."
  ],
  "abstract_text": [
    "<S> we introduce , for the first time , a new class of birnbaum  saunders nonlinear regression models potentially useful in lifetime data analysis . </S>",
    "<S> the class generalizes the regression model described by rieck and nedelman [ 1991 , a log - linear model for the birnbaum  saunders distribution , _ technometrics _ , * 33 * , 5160 ] . </S>",
    "<S> we discuss maximum likelihood estimation for the parameters of the model , and derive closed - form expressions for the second - order biases of these estimates . </S>",
    "<S> our formulae are easily computed as ordinary linear regressions and are then used to define bias corrected maximum likelihood estimates . </S>",
    "<S> some simulation results show that the bias correction scheme yields nearly unbiased estimates without increasing the mean squared errors . </S>",
    "<S> we also give an application to a real fatigue data set .    </S>",
    "<S> bias correction , birnbaum  saunders distribution , maximum likelihood estimation , nonlinear regression . </S>"
  ]
}