{
  "article_text": [
    "there has been much recent interest in performing bayesian inference in models where the posterior is intractable and , in particular , we have the situation in which the posterior distribution @xmath0 , can not be evaluated pointwise .",
    "this intractability typically occurs occurs due to the intractability of the likelihood , i.e. @xmath1 can not be evaluated pointwise .",
    "example scenarios include :    1 .   the use of big data sets , where @xmath1 consists of a product of a large number of terms ; 2 .",
    "the existence of a large number of latent variables @xmath2 , with @xmath1 known only as a high dimensional integral @xmath3 ; 3 .   when @xmath4 , with @xmath5 being an intractable normalising constant ( inc ) for the tractable term @xmath6 ( e.g. when @xmath7 factorises as a markov random field ) ; 4 .   where it is possible to sample from @xmath8 , but not to evaluate it , such as when the distribution of the data given @xmath9 is modelled by a complex stochastic computer model .    each of these ( overlapping ) situations has been considered in some detail in previous work and",
    "each has inspired different methodologies . in this paper",
    "we focus on the third case , in which the likelihood has an inc .",
    "this is an important problem in its own right ( @xcite refer to it as `` one of the main challenges to methodology for computational statistics currently '' ) .",
    "there exist several competing methodologies for inference in this setting ( see @xcite ) .",
    "in particular , the _ `` exact '' approaches _ of @xcite and @xcite exploit the decomposition @xmath4 , whereas _",
    "`` simulation based '' methods _ such as approximate bayesian computation ( abc ) @xcite do not depend upon such a decomposition and can be applied more generally : to situation 1 in @xcite ; situations 2 and 3 ( e.g. @xcite ) and situation 4 ( e.g. @xcite ) .",
    "this paper considers the problem of bayesian model comparison in the presence of an inc .",
    "we explore both exact and simulation - based methods , and find that elements of both approaches may also be more generally applicable . specifically :    for exact methods we find that approximations are required to allow practical implementation , and this leads us to investigate the use of approximate weights in importance sampling ( is ) and sequential monte carlo ( smc ) .",
    "we examine the use of both _ exact - approximate _ approaches ( as in @xcite ) and also `` _ _ inexact - approximate _ _ '' methods , in which complete flexibility is allowed in the approximation of weights , at the cost of losing the exactness of the method .",
    "this work is a natural counterpart to @xcite , which examines ananalogous question ( concerning the acceptance probability ) for markov chain monte carlo ( mcmc ) algorithms .",
    "these generally applicable methods , `` noisy mcmc '' @xcite and `` noisy smc '' ( this paper ) have some potential to address situations 1 - 3 .",
    "we provide some comparison of these inexact approximations with simulation - based methods , including the `` synthetic likelihood '' ( sl ) of @xcite . in the applications considered here we find this to be a viable alternative to abc .",
    "our results are suggestive that this , and related methods , may find success in scenarios in which abc is more usually applied .    in the remainder of this section",
    "we briefly outline the problem of , and methods for , parameter inference in the presence of an inc .",
    "we then detail the problem of bayesian model comparison in this context , before discussing methods for addressing it in the following two sections .      in this section",
    "we consider the problem of simulating from @xmath10 using mcmc . this problem has been well studied , and such models are termed _ doubly intractable _ because the acceptance probability in the metropolis - hastings ( mh ) algorithm @xmath11 can not be evaluated due to the presence of the inc .",
    "we first review exact methods for simulating from such a target in sections [ sub : single - and - multiple]-[sub : russian - roulette ] , before looking at simulation - based methods in sections [ sub : approximate - bayesian - computation ] and [ sub : synthetic - likelihood ] .",
    "the methods described here in the context of mcmc form the basis of the methods for evidence estimation developed in the remainder of the paper .",
    "@xcite avoid the evaluation of the inc by augmenting the target distribution with an extra variable @xmath12 that lies on the same space as @xmath13 , and use an mh algorithm with target distribution @xmath14 where @xmath15 is some ( normalised ) arbitrary distribution .",
    "as the mh proposal in @xmath16-space they use @xmath17 giving an acceptance probability of @xmath18    note that , by viewing @xmath19 as an unbiased is estimator of @xmath20 , this algorithm can be seen as an instance of the _ exact approximations _ described in @xcite and @xcite , where it is established that if an unbiased estimator of a target density is used appropriately in an mh algorithm , the @xmath9-marginal of the invariant distribution of this chain is the target distribution of interest .",
    "this automatically suggests extensions to the _ single auxiliary variable ( sav ) _ method described above , where @xmath21 importance points are used , yielding : @xmath22 @xcite show that the reduced variance of this estimator leads to a reduced asymptotic variance of estimators from the resultant markov chain .",
    "the variance of the is estimator is strongly dependent on an appropriate choice of is target @xmath23 , which should have lighter tails than @xmath8 .",
    "@xcite suggest that a reasonable choice may be @xmath24 , where @xmath25 is the maximum likelihood estimator of @xmath9 .",
    "however , in practice @xmath23 can be difficult to choose well , particularly when @xmath13 lies on a high dimensional space .",
    "motivated by this , annealed importance sampling ( ais ) @xcite can be used as an alternative to is , leading to the _ multiple auxiliary variable ( mav ) _ method of @xcite .",
    "ais makes use of a sequence of @xmath26 targets , which in @xcite are chosen to be @xmath27 between @xmath8 and @xmath23 .",
    "after the initial draw @xmath28 , the auxiliary point is taken through a sequence of @xmath26 mcmc moves which successively have target @xmath29 for @xmath30 .",
    "the resultant is estimator is given by @xmath31 this estimator has a lower variance ( although at a higher computational cost ) than the corresponding is estimator .",
    "we note that ais can be viewed as a particular case of smc without resampling and one might expect to obtain additional improvements at negligible cost by incorporating resampling steps within such algorithms ( see @xcite for an illustration of the potential improvement and some discussion ) ; we do not pursue this here as it is not the focus of this work .",
    "an alternative approach to avoiding the ratio of incs in equation is given by @xcite , in which it is suggested to use the acceptance probability @xmath32 where @xmath33 , motivated by the intuitive idea that @xmath34 is a single point is estimator of @xmath35 .",
    "this method is shown to have the correct invariant distribution , as is the extension in which ais is used in place of is .",
    "a potential extension might seem to be using multiple importance points @xmath36 to obtain an estimator of @xmath35 that has a smaller variance , with the aim of improving the statistical efficiency of estimators based on the resultant markov chain .",
    "this scheme is shown to work well empirically in @xcite . however",
    ", this chain does not have the desired target as its invariant distribution .",
    "instead it can be seen as part of a wider class of algorithms that use a noisy estimate of the acceptance probability : _ noisy monte carlo _ algorithms ( also referred to as _",
    "`` inexact approximations '' _ in @xcite ) .",
    "@xcite shows that under uniform ergodicity of the ideal chain , a bound on the expected difference between the noisy and true acceptance probabilities can lead to bounds on the distance between the desired target distribution and the iterated noisy kernel .",
    "it also describes additional noisy mcmc algorithms for approximately simulating from the posterior , based on langevin dynamics .",
    "@xcite    when a rejection sampler is available for simulating from @xmath37 , @xcite introduce an alternative exact algorithm that has some favourable properties compared to the exchange algorithm . since a rejection sampler is not available in many cases , we do not pursue this approach further .",
    "approximate bayesian computation @xcite refers to methods that aim to approximate an intractable likelihood @xmath1 through the integral @xmath38 where @xmath39 gives a vector of summary statistics and @xmath40 is the density of a symmetric kernel with bandwidth @xmath41 , centered at @xmath42 and evaluated at @xmath43 .",
    "as @xmath44 , this distribution becomes more concentrated , so that in the case where @xmath39 gives sufficient statistics for estimating @xmath9 , as @xmath44 the approximate posterior becomes closer to the true posterior .",
    "this approximation is used within standard monte carlo methods for simulating from the posterior .",
    "for example , it may be used within an mcmc algorithm , where using an exact - approximation argument it can be seen that it is sufficient in the calculation of the acceptance probability to use the monte carlo approximationfor the likelihood at @xmath45 at each iteration , where .",
    "whilst the exact - approximation argument means that there is no additional bias due to this monte carlo approximation , the approximation introduced through using a tolerance @xmath46 or insufficient summary statistics may be large .",
    "for this reason it might be considered a last resort to use abc on likelihoods with an inc , but previous success on these models ( e.g @xcite and @xcite ) lead us to consider them further in this paper .",
    "abc is essentially using , based on simulations from @xmath7 , a nonparameteric estimator of @xmath47 , the distribution of the summary statistics of the data given @xmath9 . in some situations , a parametric model might be more appropriate .",
    "for example , if the statistic is the sum of independent random variables , a central limit theorem ( clt ) might imply that it would be appropriate to assume that @xmath47 is a multivariate gaussian .",
    "the sl approach @xcite proceeds by making exactly this gaussian assumption and uses this approximate likelihood within an mcmc algorithm .",
    "the parameters ( the mean and variance ) of this approximating distribution for a given @xmath9 are estimated based on the summary statistics of simulations @xmath48 . concretely , the estimate of the likelihood is @xmath49 where @xmath50 with @xmath51 @xcite applies this method in a setting where the summary statistics are regression coefficients , motivated by their distribution being approximately normal .",
    "one of the approximations inherent in this method , as in abc , is the use of summary statistics rather than the whole dataset .",
    "however , unlike abc , there is no need to choose a bandwidth @xmath41 : this approximation is replaced with that arising from the discrepancy between the normal approximation and the exact distribution of the chosen summary statistic .",
    "the sl method remains approximate even if the summary statistic distribution is gaussian as @xmath52 is not an unbiased estimate of the density and so the exact - approximation results do not apply . rather , this is a special case of noisy mcmc , and we do not expect the additional bias introduced by estimating the parameters of @xmath52 to have large effects on the results , even if the parameters are estimated via an internal mcmc chain targeting @xmath8 as described in section [ sub : russian - roulette ] .",
    "sl is related to a number of other simulation based algorithms under the umbrella of bayesian indirect inference @xcite .",
    "this suggests a number of extensions to some of the methods presented in this paper that we do not explore here .",
    "the main focus of this paper is estimating the _ marginal likelihood _",
    "( also termed the _ evidence _ )",
    "@xmath53 and _ bayes factors _ : _ _ ratios of evidences for different models ( @xmath54 and @xmath55 , say ) , @xmath56 .",
    "these quantities can not usually be estimated reliably from mcmc output , and commonly used methods for estimating them require @xmath1 to be tractable in @xmath9 .",
    "this leads @xcite to label their estimation as _",
    "`` triply intractable '' _ when @xmath7 has an inc . to our knowledge",
    "the only published approach to estimating the evidence for such models is in @xcite , with this paper also giving one of the only approaches to estimating bfs in this setting . for estimating bfs",
    ", abc provides a viable alternative @xcite , at least for models within the exponential family .",
    "@xcite starts from chib s approximation , @xmath57 where @xmath58 can be an arbitrary value of @xmath9 and @xmath59 is an approximation to the posterior distribution .",
    "such an approximation is intractable when @xmath7 has an inc .",
    "@xcite devises a `` population '' version of the exchange algorithm that simulates points @xmath60 from the posterior distribution , and which also gives an estimate @xmath61 of the inc at each of these points .",
    "the points @xmath60 can be used to find a kernel density approximation @xmath59 , and estimates @xmath61 of the inc .",
    "these are then used in a number of evaluations of at points ( generated by the population exchange algorithm ) in a region of high posterior density , which are then averaged to find an estimate of the evidence .",
    "this method has a number of useful properties ( including that it may be a more efficient approach for parameter inference than the standard exchange algorithm ) , but for evidence estimation it suffers the limitation of using a kernel density estimate which means that , as noted in the paper , its use is limited to low - dimensional parameter spaces .    in this paper",
    "we explore the alternative approach of methods based on is , making use of the likelihood approximations described earlier in this section .",
    "these is methods are outlined in section [ sec : importance - sampling - approaches ] . in section [ sec : importance - sampling - approaches ] we note the good empirical performance of an inexact - approximate method and examine such approaches in more detail . as is",
    "is itself not readily applicable to high dimensional parameter spaces , in section [ sec : sequential - monte - carlo ] we look at natural extensions to the is methods based on smc .",
    "particular care is required when considering approximations within iterative algorithms : we provide a preliminary study of approximation in this context demonstrating theoretically that the resulting error can be controlled uniformly in time , under very favorable assumptions .",
    "this , and the associated empirical study are intended to provide motivation and proof of concept ; caution is still required if approximation is used within such methods in practice but the results presented suggest that further investigation is warranted .",
    "the algorithms presented later in the paper are viable alternatives to the mcmc approaches to parameter estimation described in this section , and may outperform the corresponding mcmc approach in some cases .",
    "in particular they all automatically make use of a population of points , an idea previously explored in the mcmc context by @xcite and @xcite . in section [ sec : conclusions ] we draw conclusions .",
    "in this section we investigate the use of is for estimating the evidence and bfs for models with incs .",
    "we consider an `` ideal '' importance sampler that simulates @xmath62 points @xmath63 from a proposal @xmath64 and calculates their weight , in the presence of an inc , using @xmath65 with an estimate of the evidence given by @xmath66 to estimate a bf we simply take the ratio of estimates of the evidence for the two models under consideration .",
    "however , the presence of the inc in the weight expression in means that importance samplers can not be directly implemented for these models . to circumvent this problem we will investigate the use of the techniques described in section [ sub : parameter - inference ] in importance sampling .",
    "we begin by looking at exact - approximation based methods in section [ sub : auxiliary - variable - is ] .",
    "we then examine the use to approximate likelihoods based on simulation , including abc and sl in section [ sub : simulation - based - methods ] , before looking at the performance of all of these methods on a toy example in section [ sub : toy - example ] .",
    "finally , in sections [ sub : application - to - social ] and [ sub : application - to - ising ] we examine applications to exponential random graph models ( ergms ) and ising models , the latter of which leads us to consider the use of inexact - approximations in is ( first introduced in section [ sub : is - with - biased ] ) .      to avoid the evaluation of the inc in , we propose the use of the auxiliary variable method used in the mcmc context in section [ sub : single - and - multiple ] .",
    "specifically , consider is using the sav target @xmath67 noting that it has the same normalizing constant as @xmath68 , with proposal @xmath69 this results in weights @xmath70 and the estimate of the evidence .    in this method , which we will refer to as single auxiliary variable",
    "is ( savis ) , we may view @xmath71 as an unbiased importance sampling ( is ) estimator of @xmath72 .",
    "although we are using an unbiased estimator of the weights in place of the ideal weights , the result is still an exact importance sampler .",
    "savis is an exact - approximate is method , as seen previously in @xcite , @xcite and @xcite . as in the mcmc setting , to ensure the variance of estimators produced by this scheme is not large we must ensure the variance of estimator of @xmath72 is small .",
    "thus in practice we found extensions to this basic algorithm were useful : using multiple @xmath12 importance points for each proposed @xmath60 as in ; and using ais , rather than simple is , for estimating @xmath72 as in ( giving an algorithm that we refer to as multiple auxiliary variable is ( mavis ) , in common with the terminology in @xcite ) . using @xmath24 , as described in section [ sub : single - and - multiple ] , and @xmath73 as in , we obtain @xmath74 in this case the ( a)is methods are being used as unbiased estimators of the ratio @xmath75 and again smc could be used in their place .      @xcite investigate the use of the abc approximation when",
    "using is for estimating marginal likelihoods . in this case",
    "the weight equation becomes @xmath76 where @xmath77 , and using the notation from section [ sub : approximate - bayesian - computation ] .",
    "however , using these weights within gives an estimate for @xmath78 rather than , as desired , an estimate of the evidence @xmath79 .",
    "fortunately , there are cases in which abc may be used to estimate bfs . @xcite establish that , for the bf for two exponential family models : if @xmath80 is sufficient for the parameters in model 1 and @xmath81 is sufficient for the parameters in model 2 , then using @xmath82 gives @xmath83 outside the exponential family , making an appropriate choice of summary statistics is harder @xcite .",
    "just as in the parameter estimation case , the use of a tolerance @xmath46 results in estimating an approximation to the true bf .",
    "an alternative approximation , not previously used in model comparison , is to use sl ( as described in section [ sub : synthetic - likelihood ] ) . in this case",
    "the weight equation becomes @xmath84 where @xmath85 are given by . as in parameter estimation",
    ", this approximation is only appropriate if the normality assumption is reasonable .",
    "the choice of summary statistics is as difficult as in the abc case .      in this section",
    "we have discussed three alternative methods for estimating bfs : mavis , abc and sl . to further understand their properties we now investigate the performance of each method on a toy example .",
    "consider i.i.d .",
    "observations @xmath86 of a discrete random variable that takes values in @xmath87 .",
    "for such a dataset , we will find the bf for the models    1 .",
    "@xmath88 , @xmath89 @xmath90 2 .",
    "@xmath91 , @xmath92 @xmath93    in both cases we have rewritten the likelihoods @xmath94 and @xmath95 in the form @xmath96 in order to use mavis . due to the use of conjugate priors",
    "the bf for these two models can be found analytically . as in @xcite we simulated ( using an approximate rejection sampling scheme ) 1000 datasets for which @xmath97 roughly uniformly cover the interval [ 0.01,0.99 ] , to ensure that testing is performed in a wide range of scenarios . for each algorithm we used the same computational effort , in terms of the number of simulations ( @xmath98 ) from the likelihood",
    "( such simulations dominate the computational cost of all of the algorithms considered ) .",
    "our results are shown in figure [ fig : bayes-factors - for ] , with the algorithm - specific parameters being given in figure [ fig : a - box - plot ] .",
    "we note that we achieved better results for mavis when : devoting more computational effort to the estimation of @xmath99 ( thus we used only 100 importance points in @xmath9-space , compared to 1000 for the other algorithms ) ; and using more intermediate bridging distributions in the ais , rather than multiple importance points ( thus , in equation we used @xmath100 and @xmath101 ) . in the abc case we found that reducing @xmath41 much further than 0.1 resulted in many importance points with zero weight ( note that here , and throughout the paper we use the uniform kernel for @xmath102 ) . from the box plots in figure [ fig : a - box - plot ]",
    ", we might infer that overall sl has outperformed the other methods , but be concerned about the number of outliers .",
    "figures [ fig : the  of ] to [ fig : the  of-2 ] shed more light on the situations in which each algorithm performs well .    in figure",
    "[ fig : the  of ] we observe that the non - zero @xmath41 results in a bias in the bf estimates ( represented by the shallower slope in the estimated bfs compared to the true values ) . in this example",
    "we conclude that abc has worked quite well , since the bias is only pronounced in situations where the true bf favours one model strongly over the other , and this conclusion would not be affected by the bias .",
    "for this reason it might be more relevant in this example to consider the deviations from the shallow slope , which are likely due to the monte carlo variance in the estimator ( which becomes more pronounced as @xmath41 is reduced ) .",
    "we see that the choice of @xmath41 essentially governs a bias - variance trade - off , and that the difficulty in using the approach more generally is that it is not easy to evaluate whether a choice of @xmath41 that ensures a low variance also ensures that the bias is not significant in terms of affecting the conclusions that might be drawn from the estimated bf ( see section [ sub : application - to - social ] ) .",
    "figure [ fig : the ",
    "of-1 ] suggests that sl has worked extremely well ( in terms of having a low variance ) for the most important situations , where the bf is close to 1 .",
    "however , we note that the large biases introduced due to the limitation of the gaussian assumption when the bf is far from 1 . figure [ fig : the  of-2 ] indicates that there is little or no bias when using mavis , but that there is appreciable variance ( due to using is on the relatively high - dimensional @xmath12-space ) .",
    "these results highlight that the three methods will be most effective in slightly different situations .",
    "the approximations in abc and sl introduce a bias , the effect of which might be difficult to assess . in abc",
    "( assuming sufficient statistics ) this bias can be reduced by an increased computational effort allowing a smaller @xmath41 , however it is essentially impossible to assess when this bias is `` small enough '' .",
    "sl is the simplest method to implement , and seems to work well in a wide variety of situations , but the advice in @xcite should be followed in checking that the assumption of normality is appropriate .",
    "mavis is limited by the need to perform importance sampling on the high - dimensional @xmath16 space but consequently avoids specifying summary statistics , its bias is small , and this method is able to estimate the evidence of individual models .      in this section",
    "we use our methods to compare the evidence for two alternative ergms for the gamaneg data previously analysed in @xcite ( who illustrate the data in their figure 3 ) .",
    "an ergm has the general form @xmath103 where @xmath43 is a vector of statistics of a network @xmath13 and @xmath9 is a parameter vector of the same length .",
    "we take @xmath104 @xmath105 of edges @xmath106 in model 1 and @xmath107 # of edges , # of two stars@xmath106 in model 2 . as in @xcite",
    "we use the prior @xmath108 .    using a computational budget of @xmath109 simulations from the likelihood ( each simulation consisting of an internal mcmc run of length 1000 as a proxy for an exact sampler , as described in section [ sub : russian - roulette ] )",
    ", @xcite finds that the evidence for model 1 is @xmath110 that for model 2 .",
    "using the same computational budget for our methods , consisting of 1000 importance points ( with 100 simulations from the likelihood for each point ) , we obtained the results shown in table [ tab : gamaneg ] .",
    ".model comparison results for gamaneg data .",
    "note that the abc ( @xmath111 ) estimate was based upon just 5 sample points of non - zero weight .",
    "mavis also provides estimates of the individual evidence ( @xmath112=-69.6 $ ] , @xmath113=-73.3$]).[tab : gamaneg ] [ cols=\"^,^,^,^,^\",options=\"header \" , ]     this example highlights the issue with the bias - variance trade - off in abc , with @xmath114 having too large a bias and @xmath111 having too large a variance .",
    "sl performs well  in this particular case the gaussian assumption appears to be appropriate .",
    "one might expect this , since the statistics are sums of random variables .",
    "however , we note that this is not usually the case for ergms , particularly when modelling large networks , and that sl is a much more appropriate method for inference in the ergms with local dependence @xcite",
    ". a more sophisticated abc approach might exhibit improved performance , possibly outperforming sl .",
    "however , the appeal of sl is in its simplicity , and we find it to be a useful method for obtaining good results with minimal tuning .",
    "the implementation of mavis in the previous section is not an exact - approximate method for two reasons :    1 .",
    "an internal mcmc chain was used in place of an exact sampler ; 2 .",
    "the @xmath115 term in was estimated before running this algorithm ( by using a standard smc method , with initial distribution being the bernoulli random graph ( which can be simulated from exactly ) and final distribution @xmath116 to estimate @xmath117 ( being the normalising constant of @xmath118 ) , and taking the reciprocal ) with this fixed estimate being used throughout .    however , in practice , we tend to find that such `` inexact - approximations '' do not introduce large errors into bayes factor estimates , particularly when compared to standard implementations of abc ( as seen in the previous section ) .",
    "this example suggests that in practice it may sometimes be advantageous to use biased rather than unbiased estimates of importance weights within a random weight is algorithm : an observation that is somewhat analogous to that made in @xcite in the context of mcmc .",
    "this section provides an initial theoretical exploration as to whether this might be a useful strategy in is . in order to analyse the behaviour of importance sampling with biased weights",
    ", we consider biased estimates of the weights in equation .",
    "let @xmath119 we consider biased randomised weights that admit an additive decomposition , @xmath120 in which @xmath121 - w(\\theta)$ ] is a deterministic function describing the bias of the weights and @xmath122 is a random variable ( more precisely , there is an independent copy of such a random variable associated with every particle ) , which conditional upon @xmath9 is of mean zero and variance @xmath123",
    ". this decomposition will not generally be available in practice , but is flexible enough to allow the formal description of many settings of interest .",
    "for instance , one might consider the algorithms presented here by setting @xmath124 to the ( conditional ) expected value of the difference between the approximate and exact weights and @xmath122 to the difference between the approximate weights and their expected value .",
    "we have immediately that the bias of such an estimate is , using a subscript of @xmath125 to denote expectations and variances with respect to @xmath126 , @xmath127 $ ] . by a simple application of the law of total variance ,",
    "its variance is @xmath128+\\mathbb{e}_{q}\\left[\\grave{\\sigma}_{\\theta}^{2}\\right]\\right\\}\\ ] ] consequently , the mean squared error of this estimate is : @xmath129+\\mathbb{e}_{q}[\\grave{\\sigma}_{\\theta}^{2}]\\right\\ } + \\mathbb{e}_{q}[b(\\theta)]^{2}.\\nonumber\\end{aligned}\\ ] ] if we compare such a biased estimator with a second estimator in which we use the same proposal distribution but instead use an unbiased random weight @xmath130 where @xmath131 has conditional expectation zero and variance @xmath132 , then it s clear that the biased estimator has smaller mean squared error for small enough samples if it has sufficiently smaller variance , i.e. , when ( assuming , otherwise one estimator dominates the other for all sample sizes ) : @xmath133+\\mathbb{e}_{q}[\\grave{\\sigma}_{\\theta}^{2}]\\right\\ } + \\mathbb{e}_{q}[b(\\theta)]^{2}\\nonumber\\\\",
    "< & \\frac{1}{p}\\left\\ { \\textrm{var}_{q}\\left[w(\\theta)\\right]+\\mathbb{e}_{q}[\\acute{\\sigma}_{\\theta}^{2}]\\right\\ } \\nonumber\\end{aligned}\\ ] ] which holds when @xmath62 is inferior to @xmath134-\\textrm{var}_{q}\\left[b(\\theta)\\right]-2\\textrm{cov}_{q}\\left[w(\\theta),b(\\theta)\\right]}{\\mathbb{e}_{q}[b(\\theta)]^{2}}.\\nonumber\\end{aligned}\\ ] ] in the artificially simple setting in which @xmath135 is constant , this would mean that the biased estimator would have smaller mse for samples smaller than the ratio of the difference in variance to the square of that bias suggesting that qualitatively a biased estimator might be better if the square of the average bias is small in comparison to the variance reduction that it provides . given a family of increasingly expensive biased estimators with progressively smaller bias",
    ", one could envisage using such an argument to manage the trade - off between less biased estimators and larger sample sizes . in practice a negative covariance between @xmath124 and @xmath136",
    "might also lead to favourable performance by biased estimators .      in the current section",
    "we investigate this type of approach further empirically , estimating bayes factors from data simulated from ising models .",
    "in particular we reanalyse the data from @xcite , which consists of 20 realisations from a first - order @xmath137 ising model and 20 realisations from a second - order @xmath137 ising model for which accurate estimates ( via @xcite ) of the evidence serve as a ground truth for comparison .",
    "we also analyse data from a @xmath138 ising model .      as in the toy example",
    ", we examine several different configurations of the is and ais estimators of the @xmath75 term in the weight , using different values of @xmath140 , @xmath26 and @xmath141 , the burn in of the internal mcmc , that yield the same computational cost ( in terms of the number of gibbs sweeps used to simulate from the likelihood ) .",
    "note that for small values of @xmath141 these estimators are biased ; a bias that decreases as @xmath141 increases .",
    "the empirical results in @xcite , use a total @xmath142 gibbs sweeps to estimate one bayes factor , to allow comparison of our results with those in that paper . here , estimating a marginal likelihood is done in three stages : firstly @xmath25 is estimated ; followed by @xmath117 , then finally the marginal likelihood .",
    "we took @xmath25 to be the posterior expectation , estimated from a run of the exchange algorithm of @xmath143 iterations .",
    "@xmath117 was then estimated using smc with an mcmc move , with 200 particles and 100 targets , with the @xmath144th target being @xmath145 , employing stratified resampling when the effective sample size ( ess ; @xcite ) falls below 100 .",
    "the total cost of these three stages is @xmath146 gibbs sweeps ( @xmath147 of the cost of population exchange ) with the final is stage costing @xmath148 sweeps ( @xmath149 of the cost of population exchange ) .",
    "we note that the cost of the first two stages has been chosen conservatively - less computational effort here can also yield good results .",
    "the importance proposal used in all cases was a multivariate normal distribution , with mean and variance taken to be the sample mean and variance from the initial run of the exchange algorithm .",
    "this proposal would clearly not be appropriate in high dimensions , but is reasonable for the low dimensional parameter spaces considered here .",
    "figure [ fig : box - plots - of ] shows the results produced by these methods in comparison with those from @xcite .",
    "we observe : improvements of the new methods over population exchange ; an overall robustness of the new methods to different choices of parameters ; and that there is a bias - variance tradeoff in the `` internal '' estimate of @xmath75 in terms of producing the best behaviour of the bayes factor estimates .",
    "recall that as @xmath141 increases the bias of the internal estimate ( the results of which can be observed in the results when using @xmath150 ) decreases , but for a fixed computational effort it is beneficial to use a lower @xmath141 and to instead increase @xmath140 , using more importance points to decrease the variance . as in @xcite , we observe that it may be useful to move away from the exact - approximate approaches , and in this case , to simply use the best available estimator of @xmath75 ( taking into account its statistical and computational efficiency ) regardless of whether it is unbiased . in this example",
    "there is little observed difference in using our fixed computational budget on more ais moves ( @xmath26 ) in place of using more importance points ( @xmath140 ) . in general",
    "we might expect using more ais moves to be more productive when the estimates of the @xmath75 for @xmath9 far from @xmath25 are required .",
    "in this section we use savis for estimating the marginal likelihood for a first order ising model on data of size @xmath151 pixels simulated from an ising model with parameter @xmath152 . again , estimating a marginal likelihood is done in three stages : firstly @xmath25 is estimated ; followed by @xmath117 , then finally the marginal likelihood .",
    "the methods use for the first two stages are identical to those used in section [ sub:10by10 ] , as is the choice of proposal distribution .",
    "the third stage is performed using savis with @xmath153 and @xmath154 . from 20 runs of this third stage ,",
    "a five - number summary of the @xmath155 evidence estimates was ( -5790.251 , -5790.178 , -5790.144 , -5790.119 , -5790.009 ) , with the average ess being 80.75 .",
    "note the low variance over these runs of the algorithm and the high ess , which were also found for different configurations of the algorithm ( including for more importance points and larger values of @xmath140 and @xmath141 ) .",
    "one might expect this example to be more difficult than the @xmath139 grids considered in the previous section , due to the need to find good estimates of @xmath75 that are here normalising constants of distributions on a space of higher dimensions .",
    "however , since the posterior has lower variance in this case , only values of @xmath9 close to @xmath25 are proposed , which makes estimating @xmath75 much easier , yielding the good results in this section .      in this section",
    "we have compared the use of abc - is , sl - is , mavis ( and alternatives ) for estimating marginal likelihoods and bayes factors .",
    "the use of abc for model comparison has received much attention , with much of the discussion centring around appropriate choices of summary statistics .",
    "we have avoided this in our examples by using exponential family models , but in general this remains an issue affecting both abc and sl .",
    "it is the use of summary statistics that makes abc and sl unable to provide evidence estimates .",
    "however , it is the use of summary statistics , usually essential in these settings , that provides abc and sl with an advantage over mavis , in which importance sampling must be performed over the high dimensional data - space . despite this disadvantage",
    ", mavis avoids the approximations made in the simulation based methods ( illustrated in figures [ fig : the  of ] to [ fig : the ",
    "of-2 ] , with the accuracy depending primarily on the quality of the estimate of @xmath156 used ) . in section [ sub : application - to - ising ] we saw that there can be advantages of using biased , but lower variance estimates in place of standard is .",
    "the main weakness of all of the methods described in this section is that they are all based on standard is and are thus not practical for use when @xmath9 is high dimensional . in the next section",
    "we examine the use of smc samplers as an extension to is for use on triply intractable problems , and in this framework discuss further the effect of inexact approximations .",
    "smc samplers @xcite are a generalisation of is , in which the problem of choosing an appropriate proposal distribution in is is avoided by performing is sequentially on a sequence of target distributions , starting at a target that is easy to simulate from , and ending at the target of interest . in standard",
    "is the number of monte carlo points required in order to obtain a particular accuracy increases exponentially with the dimension of the space , but @xcite show ( under appropriate regularity conditions ) that the use of smc circumvents this problem and can thus be practically useful in high dimensions .    in this section",
    "we introduce smc algorithms for simulating from doubly intractable posteriors which have the by - product that , like is , they also produce estimates of marginal likelihoods . in section [ sub : smc - samplers - in ]",
    "we describe these algorithms , before examining an application to estimating the precision matrix of a gaussian distribution in high dimensions in section [ sub : application - to - precision ] . in [ sec : biased - smc ] we provide a preliminary investigation of the consequences of using biased weight estimates in an smc framework .",
    "this section introduces two alternative smc samplers for use on doubly intractable target distributions .",
    "the first , marginal smc , directly follows from the is methods in the previous section .",
    "the second ,",
    "smc - mcmc , requires a slightly different approach , but is more computationally efficient .",
    "finally we briefly discuss simulation - based smc samplers in section [ sub : simulation - based - smc - samplers ] .    to begin , we introduce notation that is common to all algorithms that we discuss .",
    "smc samplers perform sequential is using @xmath62 `` particles '' @xmath60 , each having ( normalised ) weight @xmath157 , using a sequence of targets @xmath158 to @xmath159 , with @xmath159 being the distribution of interest , in our case @xmath0 . in this section",
    "we will take @xmath160 . at target @xmath161 ,",
    "a `` forward '' kernel @xmath162 is used to move particle @xmath163 to @xmath164 , with each particle then being reweighted to give unnormalised weight @xmath165 here , @xmath166 represents a `` backward '' kernel that we chose differently in the alternative algorithms below .",
    "we note the presence of the inc , which means that this algorithm can not be implemented in practice in its current form .",
    "the weights are then normalised to give @xmath167 , and a resampling step is carried out . in the following sections the focus is on the reweighting step :",
    "this is the main difference between the different algorithms . for more detail on these methods ,",
    "see @xcite .",
    "@xcite describe how bfs can be estimated directly by smc samplers , simply by taking @xmath168 to be one model and @xmath159 to be the other ( with the @xmath169 being intermediate distributions ) .",
    "this idea is also explored for gibbs random fields in @xcite . however , the empirical results in @xcite suggest that in some cases this method does not necessarily perform better than estimating marginal likelihoods for the two models separately and taking the ratio of the estimates . here",
    "we do not investigate these algorithms further , but note that they offer an alternative to estimating the marginal likelihood separately .",
    "[ [ smc - with - an - mcmc - kernelsubsmc - with - an ] ] smc with an mcmc kernel[sub : smc - with - an ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    suppose we were able to use a reversible mcmc kernel @xmath170 with invariant distribution @xmath171 , and choose the @xmath166 kernel to be the time reversal of @xmath170 with respect to its invariant distribution , we obtain the following incremental weight : @xmath172 once again , we can not evaluate this incremental weight due to the presence of a ratio of normalising constants .",
    "also , such an mcmc kernel can not generally be directly constructed  the mh update itself involves evaluating the ratio of intractable normalising constants .",
    "however , appendix [ sec : using - sav - and ] shows that precisely the same weight update results when using either sav or exchange mcmc moves in place of a direct mcmc step .    in order",
    "that this approach may be implemented we might consider , in the spirit of the approximations suggested in section [ sec : importance - sampling - approaches ] , using an estimate of the ratio term @xmath173 .",
    "for example , an unbiased is estimate is given by @xmath174 where @xmath175 .",
    "although this estimate is unbiased , we note that the resultant algorithm does not have precisely the same extended space interpretation as the methods in @xcite .",
    "appendix [ sec : an - extended - space ] gives an explicit construction for this case , which incorporates a pseudomarginal - type construction @xcite .",
    "[ [ data - point - temperingsubdata - point - tempering ] ] data point tempering[sub : data - point - tempering ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for the smc approach to be efficient we require that the sequence of distributions @xmath176 be chosen such that @xmath158 is easy to simulate from , @xmath159 is the target of interest and the intermediate distributions provide a `` route '' between them .",
    "for the applications in this paper we found the data tempering approach of @xcite to be particularly useful .",
    "suppose that the data @xmath13 consists of @xmath177 points , and that @xmath177 is exactly divisible by @xmath178 for ease of exposition .",
    "we then take @xmath179 and for @xmath180 @xmath181 with @xmath182 i.e. essentially we incorporate @xmath183 additional data points for each increment of @xmath161 . on this sequence of targets",
    "we then propose to use the smc sampler with an mcmc kernel as described in the previous section .",
    "the only slightly non - standard point is the estimation of @xmath184 @xmath185 , since in this case @xmath186 and @xmath185 are the normalising constants of distributions on different spaces .",
    "we use @xmath187 where @xmath175 and @xmath188 and @xmath189 are subvectors of @xmath190 .",
    "@xmath189 is in the space of the additional variables added when moving from @xmath191 to @xmath192 ( providing the argument in an arbitrary auxiliary distribution @xmath193 ) and @xmath188 is in the space of the existing variables . for @xmath194",
    "this becomes @xmath195 with @xmath196 .",
    "analogous to the sav method , a sensible choice for @xmath197 might be to use @xmath198 , where @xmath199 is on the same space as    [ [ marginal - smc ] ] marginal smc + + + + + + + + + + + +    an alternative method commonly used in abc applications arises from the use of an approximation to the optimal backward kernel @xcite . in this case the weight update is @xmath200 for an arbitrary forward kernel @xmath170 .",
    "this results in a computational complexity of @xmath201 compared to @xmath202 for a standard smc method , but we include it here in order to note that the @xmath203 term in could be dealt with in the same way as in the simple is case .",
    "considering the savm posterior , where in target @xmath161 we use the distribution @xmath15 for the auxiliary variable @xmath204 , and the savm proposal , where @xmath205 we arrive at the weight update :",
    "@xmath206 in which normalising constant appears in this weight update .",
    "we include this approach for completeness but do not investigate it further in this paper .      section [ sub : simulation - based - methods ] describes how the abc and sl approximations may be used within is . the same approximate likelihoods may be used in smc . in abc @xcite , where the sequence of targets is chosen to be @xmath207 with a decreasing sequence @xmath208 , this idea provides a useful alternative to mcmc for exploring abc posterior distributions , whilst also providing estimates of bayes factors @xcite .",
    "the use of smc with sl does not appear to have been explored previously .",
    "one might expect smc to be useful in this context ( using , for example , the sequence of targets @xmath209 ) , particularly when @xmath52 is concentrated relative to the prior .      in this section",
    "we examine the performance of the smc sampler , with mcmc proposal and data - tempered target distributions , for estimating the evidence in an example in which @xmath9 is of moderately high dimension .",
    "we consider the case in which @xmath210 is an unknown precision matrix , @xmath1 is the @xmath211-dimensional multivariate gaussian distribution with zero mean and @xmath212 is a wishart distribution @xmath213 with parameters @xmath214 and @xmath215 .",
    "suppose we observe @xmath216 observations , where @xmath217 the true evidence can be calculated analytically , and is given by @xmath218 where @xmath219 denotes the @xmath211-dimensional gamma function . for ease of implementation , we parametrise the precision using a cholesky decomposition @xmath220 with @xmath221 a lower triangular matrix whose @xmath222th element is denoted @xmath223 .",
    "as in section [ sub : toy - example ] , we write @xmath1 as @xmath96 as follows @xmath224 where in some of the experiments that follow , @xmath225 is treated as if it is an inc . in the wishart",
    "prior , we take @xmath226 and @xmath227 .    taking @xmath228 ,",
    "@xmath229 points were simulated using @xmath230 .",
    "the parameter space is thus 55-dimensional , motivating the use of an smc sampler in place of is or the population exchange method , neither of which are suited to this problem . in the smc sampler , in which we used @xmath231 particles , the sequence of targets is given by data point tempering .",
    "specifically , the sequence of targets is to use @xmath232 when @xmath233 and @xmath234 for @xmath235 ( with @xmath236 ) .",
    "the parameters are @xmath237 .",
    "we use single component mh kernels to update each of the parameters , with one ( deterministic ) sweep consisting of an update of each in turn . for each @xmath223",
    "we use a gaussian random walk proposal , where at target @xmath161 , the variance for the proposal used for @xmath223 is taken to be the sample variance of @xmath223 at target @xmath238 . for updating the weights of each particle we used equation [ eq : smc_z_tempering-1 ] , where we chose @xmath239 with @xmath240 the maximum likelihood estimate of the precision @xmath241 , and chose @xmath242 `` internal '' importance sampling points .",
    "systematic resampling was performed when the effective sample size ( ess ) fell below @xmath243 .",
    "we estimated the evidence 10 times using the smc sampler and compared the statistical properties of each algorithm using these estimates . for our simulated data , the @xmath155 of the true evidence was @xmath244 . over the 10 runs of the smc sampler",
    "a five - number summary of the @xmath155 evidence estimates was ( @xmath245 , @xmath246 , @xmath247 , @xmath248 , @xmath249 ) .      in this section",
    "we apply the random weight smc sampler to the ising model data considered in section [ sub:10by10 ] .",
    "we use smc to estimate the marginal likelihood of both the first and second order ising models , then take the ratio of these estimates to estimate the bayes factor . note that in this case the size of the parameter space is much smaller than in the precision example , with the models having parameter spaces of sizes 1 and 2 respectively .",
    "the excellent results achieved by is in section [ sub:10by10 ] might seem to imply that smc samplers are not required for this problem , but recall that we required preliminary runs of the exchange algorithm in order to design an appropriate importance proposal , along with an smc sampler in order to estimate the normalising constant @xmath117 of the distribution @xmath250 used for the auxiliary variables @xmath251 .",
    "an smc sampler offers a cleaner approach that requires less user tuning .",
    "we applied the random weight smc sampler described in section [ sub : random - weight - smc ] , with 500 particles , data point tempering ( adding one pixel at a time , taking @xmath252 to be @xmath253 ) , and using the estimate of the ratio of normalising constants in the weight update from equation with @xmath254 importance points .",
    "each of these estimates requires simulating a single point from @xmath255 using a gibbs sampler , which had a burn in of @xmath256 iterations , yielding a total computational budget of 200 gibbs sweeps for estimating the ratio of normalising constants .",
    "note that , as considered in section [ sub:10by10 ] , this use of a gibbs sampler results in an inexact algorithm , but this level of burn in was found to be sufficient for this bias to be minimal in the random weight is algorithms .",
    "the mcmc kernel of the exchange algorithm was used ( with proposal taken to be the sample variance of the set of particles at each smc iteration ) , using the approximate version where a gibbs sampler with burn in @xmath256 iterations is used to simulate from @xmath257 , with a total cost of @xmath258 gibbs sweeps and hence around a quarter of that of the algorithm of @xcite .",
    "figure [ fig : box - plots - smc ] shows the results produced by this method in comparison with those from @xcite .    ]",
    "we observe that the median of the random weight smc estimates is more accurate than that of the population exchange estimates - the bias introduced through using an internal gibbs sampler in place of an exact sampler does not appear to accumulate sufficiently to affect the results ( this issue is explored further in the following section ) . however , it has slightly higher variance than population exchange ( much higher than savis and mavis ) .",
    "this high variance can be attributed to two factors :    1 .",
    "since the smc sampler begins with points sampled from the prior , larger changes in @xmath9 are considered than in the is approaches , thus the estimates of the ratio of the normalising constants require more importance points to be accurate - the results suggest that the budget of 200 gibbs sweeps is insufficient .",
    "this is the opposite situation to that encountered in section [ sub:100by100 ] , where the changes in @xmath9 are small and the estimates of the ratio of the normalising constants are accurate with small numbers of importance points . 2 .",
    "it s been frequently observed ( cf .",
    "@xcite ) that , as suggested by the asymptotic variance expansion , in some instances the first few iterations of an smc sampler contribute substantially to the ultimate error .",
    "this issue arises since the forgetting of the sampler does nt suppress the terms that the initial errors contribute to the asymptotic variance enough to compensate for the fact that they re much larger than the final ones .",
    "this is due , when using data point tempering in the manner we have here , to the much larger relative discrepancy between the first few distributions in the sequence than between later distributions .",
    "we conclude that the random weight smc method is a viable approach to estimating bayes factors for these models , but that care should be taken in tuning the weight estimates and choosing the sequence of smc distributions .",
    "we now examine the effect of using inexact weights on estimates produced by smc samplers . by way of theoretical motivation of such an approach , we demonstrate that under strong , but standard ( cf .",
    "@xcite ) , assumptions on the mixing of the sampler , if the approximation error is sufficiently small , then this error can be controlled uniformly over the iterations of the algorithm and will _ not _ accumulate unboundedly over time ( and that it can in principle be made arbitrarily small by making the relative bias small enough for the desired level of accuracy ) .",
    "we do not here consider the particle system itself , but rather the sequence of distributions which are being approximated by monte carlo in the approximate version of the algorithm and in the idealised algorithm being approximated .",
    "the monte carlo approximation of this sequence can then be understood as a simple mean field approximation and its convergence has been well studied , see for example @xcite .    in order to do this",
    ", we make a number of identifications in order to allow the consideration of the approximation in an abstract manner .",
    "we allow @xmath259 to denote the incremental weight function at time @xmath161 , and @xmath260 to denote the _ exact _ weight function which it approximates ( any auxiliary random variables needed in order to obtain this approximation are simply added to the state space and their sampling distribution to the transition kernel ) .",
    "the transition kernel @xmath261 combines the proposal distribution of the smc algorithm together with the sampling distribution of any needed auxiliary variables .",
    "we allow @xmath2 to denote the full collection of variables sampled during an iteration of the sampler , which is assumed to exist on the same space during each iteration of the sampler .",
    "we employ the following assumptions ( we assume an infinite sequence of algorithm steps and associated target distributions , proposals and importance weights ; naturally , in practice only a finite number would be employed but this formalism allows for a straightforward statement of the result ) :    a1 : :    ( bounded relative approximation error ) there exists    @xmath262 such that :    @xmath263 a2 : :    ( strong mixing ; slightly stronger than a global doeblin condition )    there exists    @xmath264 a3 : :    ( control of potential ) there exists    @xmath265    the first of these assumptions controls the error introduced by employing an inexact weighting function ; the others ensure that the underlying dynamic system is sufficiently ergodic to forget it s initial conditions and hence limit the accumulation of errors .",
    "we demonstrate below that the combination of these properties suffices to transfer that stability to the approximating system .",
    "we consider the behaviour of the distributions @xmath266 and @xmath267 which correspond to the target distributions at iteration @xmath268 of the exact and approximating algorithms , prior to reweighting , at iteration @xmath268 in the following proposition , the proof of which is provided in appendix [ sec : ubound_proof ] , which demonstrates that if the approximation error , @xmath118 , is sufficiently small then the accumulation of error over time is controlled :    [ th : ubound]if a1 , a2 and a3 hold then : @xmath269    this result is not intended to do any more than demonstrate that , qualitatively , such forgetting can prevent the accumulation of error even in systems with `` biased '' importance weighting potentials . in practice , one would wish to make use of more sophisticated ergodicity results such as those of @xcite , within this framework to obtain results which are somewhat more broadly applicable : assumptions a2 and a3 are very strong , and are used only because they allow stability to be established simply .",
    "although this result is , in isolation , too weak to justify the use of the approximation schemes introduced here in practice , together with the empirical results presented below , it does suggest that further investigation of such approximations is warranted particularly in settings in which unbiased estimators are not available .",
    "we use the precision example introduced in section [ sub : application - to - precision ] to investigate the effect of using biased weights in smc samplers .",
    "specifically we take @xmath270 and use a simulated dataset @xmath13 where @xmath271 points were simulated using @xmath272 . in this case",
    "there is only a single parameter to estimate , @xmath273 , and we examine the bias of estimates of the evidence using four alternative smc samplers , each of which use a data - tempered sequence of targets ( adding one data point at each target ) . for this data",
    "we can calculate analytically the true value of the marginal likelihood after receiving each data point , thus we can estimate the bias of each sampler at each iteration .",
    "the first smc sampler ( the `` exact weight '' sampler ) is the method where the true value of @xmath173 is used in the weight update .",
    "the second is the same `` unbiased random weight '' sampler used in section [ sub : application - to - precision ] , which uses an unbiased is weight estimate , here with @xmath254 `` internal '' importance sampling points .",
    "the third , which we refer to as the `` biased random weight '' sampler , uses a biased bridge estimator instead , specifically we use in place of @xmath274^{1/2}\\right)\\left/ \\right.\\nonumber\\ ] ] @xmath275^{1/2}\\right),\\label{eq : smc_z_tempering}\\ ] ] where @xmath276 , @xmath277 so that @xmath278 , and @xmath279 with @xmath280 and @xmath281 being the corresponding subvectors of @xmath282 .    motivated by the theoretical argument presented previously , we investigate the effect of improving the mixing of the kernel used within the smc . in this model",
    "the exact posterior is available at each smc target , so we may replace the use of an mcmc move to update the parameter with a direct simulation from the posterior . in this extreme case ,",
    "there is no dependence between each particle and its history ; we refer to this , the fourth smc sampler we consider , as `` biased random weight with perfect mixing '' .",
    "each smc sampler was run 20 times , using 50 particles .",
    "figures [ fig : smc_bias ] and [ fig : smc_mse ] show the estimated bias and mean square error of the @xmath155 evidence estimates of each sampler at each iteration .",
    "no bias is observed in the algorithm with true weights , and only a small bias is observed in the unbiased random weight sampler ( this bias is likely to be due to the relatively small number of replications ) .",
    "bias does accumulate in the biased random weight sampler , but we note that the level of bias appears to stabilise .",
    "this accumulation of bias means that one should exercise caution in the use of smc samplers with biased weights .",
    "however , we observe that perfect mixing substantially decreases the bias in the evidence estimates from the algorithm .",
    "also , in this case we observe that the bias does not accumulate sufficiently to give poor estimates of the evidence . here",
    "the standard deviation of the final @xmath155 evidence estimate over the random weight smc sampler runs is approximately 0.4 , so the bias is not large by comparison .",
    "evidence estimates of the true ( black solid ) , unbiased random weight ( black dashed ) , biased random weight ( grey solid ) smc algorithms using mcmc kernels , and the estimated bias when using the biased random weight algorithm with perfect mixing ( grey dashed ) .",
    "[ fig : smc_bias ] ]     evidence estimates of the four smc samplers ( same key as figure [ fig : smc_bias ] ) .",
    "[ fig : smc_mse ] ]      in section [ sub : application - to - ising ] we observed clearly that the use of biased weights in is can be useful for estimating the evidence in doubly intractable models , but we have not observed the same for smc with biased weights . when applied to the precision example in section [ sub : application - to - precision ] , an inexact sampler ( using the bridge estimator ) did not outperform the exact sampler , despite the mean square error of the biased bridge weight estimates being substantially improved compared to the unbiased is estimate . over 10 runs the mean square error in the @xmath155 evidence was 0.34 for the inexact sampler , compared to 0.28 for the exact sampler .",
    "this experience suggests that samplers with biased weights should be used with caution : weight estimates with low variance do not guarantee good performance due to the accumulation of bias in the smc .",
    "however , the theoretical and empirical investigation in this section suggests that this idea is worth further investigation , possibly for situations involving some of the other intractable likelihoods listed in section [ sec : introduction ] .",
    "our results suggest that improved mixing can help combat the accumulation of bias , which may imply that there may be situations where it is useful to perform many iterations of a kernel at a particular target , rather than the more standard approach of using many intermediate targets at each of which a single iteration of a kernel is used .",
    "other variations are also possible , such as the calculation of fast cheap biased weights at each target in order only to adaptively decide when to resample , with more accurate weight estimates ( to ensure accurate resampling and accurate estimates based on the particles ) only calculated when the method chooses to resample .",
    "this paper describes several is and smc approaches for estimating the evidence in models with incs that outperform previously described approaches .",
    "these methods may also prove to be useful alternatives to mcmc for parameter estimation .",
    "several of the ideas in the paper are also applicable more generally , in particular the use of synthetic likelihood in the is context and the notion of using biased weight estimates in is and smc .",
    "we note that the bias in these biased weight methods may be small compared to errors resulting from commonly accepted approximate techniques such as abc",
    ".    for biased is , in section [ sub : is - with - biased ] we show that the error of estimates from low - variance biased methods can be less than those from unbiased methods of higher variance .",
    "this is comparable to a result for biased mcmc methods @xcite , where it is shown that the error of estimates from a computationally cheap biased mcmc can be less than those from an expensive exact mcmc . in both cases ,",
    "given a finite computational budget , it is not always the case that this budget should be spent on guaranteeing the exactness of the sampler if minimizing approximation error is the objective .    a similar choice concerning the allocation of computational resources arises in smc . here",
    ", one does need to be especially careful about the use of biased smc , due to the possible accumulation of bias over smc iterations .",
    "one might expect this accumulated bias to outweigh any benefits a reduced variance may bring .",
    "for this reason we advise caution in the use of biased smc in general .",
    "this paper does , however , indicate that there may exist cases where biased smc is useful , through : the theoretical result that under strong mixing conditions bias does not accumulate unboundedly ; the empirical evidence that fast mixing may reduce the accumulation of bias ; and the empirical results where we observe ( in a situation where the distance between successive targets decreases ) that the rate at which bias accumulates decreases with time .",
    "alquier p , friel n , everitt rg , boland a ( 2015 ) noisy monte carlo : convergence of markov chains with approximate transition kernels .",
    "statistics and computing in press .",
    "andrieu c , roberts go ( 2009 ) the pseudo - marginal approach for efficient monte carlo computations .",
    "the annals of statistics 37(2):697725    andrieu c , vihola m ( 2012 ) convergence properties of pseudo - marginal markov chain monte carlo algorithms .",
    "arxiv ( 1210.1484 )    beaumont ma ( 2003 ) estimation of population growth or decline in genetically monitored populations .",
    "genetics 164(3):11391160    beskos a , crisan d , jasra a , whiteley n ( 2011 ) error bounds and normalizing constants for sequential monte carlo in high dimensions .",
    "arxiv ( 1112.1544 )    caimo a , friel n ( 2011 ) bayesian inference for exponential random graph models .",
    "social networks 33:4155    chopin n ( 2002 ) a sequential particle filter method for static models .",
    "biometrika 89(3):539552    chopin n , jacob pe , papaspiliopoulos o ( 2013 ) @xmath283 : an efficient algorithm for sequential analysis of state space models",
    ". journal of the royal statistical society : series b 75(3):397426    del  moral p ( 2004 ) feynman - kac formulae : genealogical and interacting particle systems with applications .",
    "probability and its applications , springer , new york    p , doucet a , jasra a ( 2006 ) sequential monte carlo samplers .",
    "journal of the royal statistical society : series b 68(3):411436    p , doucet a , jasra a ( 2007 ) sequential monte carlo for bayesian computation .",
    "bayesian statistics 8:115148    didelot x , everitt rg , johansen am , lawson dj ( 2011 ) likelihood - free estimation of model evidence .",
    "bayesian analysis 6(1):4976    drovandi cc , pettitt an , lee a ( 2015 ) bayesian indirect inference using a parametric auxiliary model .",
    "statistical science 30(1):7295    everitt rg ( 2012 ) bayesian parameter estimation for latent markov random fields and social networks .",
    "journal of computational and graphical statistics 21(4):940960    fearnhead p , papaspiliopoulos o , roberts go , stuart am ( 2010 ) random - weight particle filtering of continuous time processes .",
    "journal of the royal statistical society series b 72(4):497512    friel n ( 2013 ) evidence and bayes factor estimation for gibbs random fields .",
    "journal of computational and graphical statistics 22(3):518532    friel n , rue h ( 2007 ) recursive computing and simulation - free inference for general factorizable models .",
    "biometrika 94(3):661672    girolami ma , lyne am , strathmann h , simpson d , atchade y ( 2013 ) playing russian roulette with intractable likelihoods .",
    "arxiv ( 1306.4032 )    grelaud a , robert cp , marin jm ( 2009 ) abc likelihood - free methods for model choice in gibbs random fields .",
    "bayesian analysis 4(2):317336    johndrow",
    "je , mattingly jc , mukherjee s and dunson d ( 2015 ) approximations of markov chains and high - dimensional bayesian inference .",
    "arxiv ( 1508.03387 )    klaas m , de  freitas n , doucet a ( 2005 ) toward practical @xmath284 monte carlo : the marginal particle filter . in : proceedings of the 20th international conference on uncertainty in artificial intelligence    kong a , liu js , wong wh ( 1994 ) sequential imputations and bayesian missing data problems .",
    "journal of the american statistical association 89(425):278288    lee a , whiteley n ( 2015 ) variance estimation and allocation in the particle filter arxiv ( 2015.0394 )    marin jm , pillai ns , robert cp , rousseau j ( 2014 ) relevant statistics for bayesian model choice .",
    "journal of the royal statistical society : series b ( statistical methodology ) 76(5):833859    marjoram p , molitor j , plagnol v , tavare s ( 2003 ) markov chain monte carlo without likelihoods .",
    "proceedings of the national academy of sciences of the united states of america 100(26):15,32415,328    meng xl , wong wh ( 1996 ) simulating ratios of normalizing constants via a simple identity : a theoretical exploration .",
    "statistica sinica 6:831860    mller j , pettitt an , reeves rw , berthelsen kk ( 2006 ) an efficient markov chain monte carlo method for distributions with intractable normalising constants .",
    "biometrika 93(2):451458    murray i , ghahramani z , mackay djc ( 2006 ) mcmc for doubly - intractable distributions . in : proceedings of the 22nd annual conference on uncertainty in artificial intelligence ( uai ) , pp 359366    neal rm ( 2001 ) annealed importance sampling . statistics and",
    "computing 11(2):125139    neal rm ( 2005 ) estimating ratios of normalizing constants using linked importance sampling .",
    "arxiv ( 0511.1216 )    nicholls gk , fox c , watt am ( 2012 ) coupled mcmc with a randomized acceptance probability .",
    "arxiv ( 1205.6857 )    peters gw ( 2005 ) topics in sequential monte carlo samplers .",
    "thesis , unviersity of cambridge    picchini u , forman jl ( 2013 ) accelerating inference for diffusions observed with measurement error and large sample sizes using approximate bayesian computation : a case study .",
    "arxiv ( 1310.0973 )    prangle d , fearnhead p , cox mp , biggs pj , french np ( 2014 ) semi - automatic selection of summary statistics for abc model choice .",
    "statistical applications in genetics and molecular biology 13(1):6782    rao v , lin l , dunson db ( 2013 ) bayesian inference on the stiefel manifold .",
    "arxiv ( 1311.0907 )    robert cp , cornuet jm , marin jm , pillai ns ( 2011 ) lack of confidence in approximate bayesian computation model choice .",
    "proceedings of the national academy of sciences of the united states of america 108(37):15,1127    schweinberger m , handcock m ( 2015 ) local dependence in random graph models : characterization , properties and statistical inference .",
    "journal of the royal statistical society : series b in press .",
    "sisson sa , fan y , tanaka mm ( 2007 ) sequential monte carlo without likelihoods .",
    "proceedings of the national academy of sciences of the united states of america 104(6):17601765    skilling j ( 2006 ) nested sampling for general bayesian computation .",
    "bayesian analysis 1(4):833859    tavar s , balding dj , griffiths rc , donnelly pj ( 1997 ) inferring coalescence times from dna sequence data .",
    "genetics 145(2):505518    tran mn , scharth m , pitt mk , kohn r ( 2013 ) @xmath285 for bayesian inference in latent variable models .",
    "arxiv ( 1309.3339 )    whiteley n ( 2013 ) stability properties of some particle filters .",
    "annals of applied probability 23(6):25002537    wilkinson rd ( 2013 ) approximate bayesian computation ( abc ) gives exact results under the assumption of model error .",
    "statistical applications in genetics and molecular biology 12(2):129141    wood sn ( 2010 ) statistical inference for noisy nonlinear ecological dynamic systems .",
    "nature 466(august):11021104    zhou y , johansen am , aston jad ( 2015 ) towards automatic model comparison : an adaptive sequential monte carlo approach .",
    "journal of computational and graphical statistics in press .",
    "let us consider the savm posterior , with @xmath26 being the mcmc move used in savm . in this case",
    "the weight update is @xmath286 which is the same update as if we could use mcmc directly .",
    "@xcite show the exchange algorithm , when set up to target @xmath171 in the manner described in section [ sub : exchange - algorithms ] , simulates a transition kernel that is in detailed balance with @xmath287 .",
    "this follows from showing that it satisfies a `` very detailed balance '' condition , which takes account of the auxiliary variable @xmath12 .",
    "the result is that the derivation of the weight update follows exactly that of .",
    "the following extended space construction justifies the use of the `` approximate '' weights in via an explicit sequential importance ( re)sampling argument along the lines of @xcite , albeit with a slightly different sequence of target distributions .",
    "consider an actual sequence of target distributions @xmath288 .",
    "assume we seek to approximate a normalising constant during every iteration by introducing additional variables @xmath289 during iteration @xmath290 .",
    "define the sequence of target distributions : @xmath291\\end{aligned}\\ ] ] where @xmath292 has the same rle and interpretation as it does in a standard smc sampler .",
    "assume that at iteration @xmath161 the auxiliary variables @xmath293 are sampled independently ( conditional upon the associated value of the parameter , @xmath294)and identically according to @xmath295 and that @xmath170 denotes the incremental proposal distribution at iteration @xmath161 , just as in a standard smc sampler .    in the absence of resampling",
    ", each particle has been sampled from the following proposal distribution at time @xmath161 : @xmath296 and hence its importance weight , @xmath297 , should be : @xmath298}{\\prod_{s=1}^{t}\\prod_{m=1}^{m}f_{s}(u_{s}^{m}|\\theta_{s-1})}\\\\ = & \\frac{\\pi_{t}(\\theta_{t})\\prod_{s=0}^{t-1}l_{s}(\\theta_{s+1},\\theta_{s})}{\\mu_{0}(\\theta_{0})\\prod_{s=1}^{t}k_{s}(\\theta_{s-1},\\theta_{s})}\\prod_{s=1}^{t}\\frac{1}{m}\\sum_{m=1}^{m}\\frac{f_{s-1}(u_{s}^{m}|\\theta_{s-1})}{f_{s}(u_{s}^{m}|\\theta_{s-1})}\\\\ = & w_{t-1}(\\widetilde{x}_{t-1})\\cdot\\frac{\\pi_{t}(\\theta_{t})l_{t-1}(\\theta_{t},\\theta_{t-1})}{\\pi_{t-1}(\\theta_{t-1})k_{t}(\\theta_{t-1},\\theta_{t})}\\\\   & \\qquad \\frac{1}{m}\\sum_{m=1}^{m}\\frac{f_{t-1}(u_{t}^{m},\\theta_{t-1})}{f_{t}(u_{t}^{m}|\\theta_{t-1})},\\end{aligned}\\ ] ] which yields the natural sequential importance sampling interpretation .",
    "the validity of the incorporation of resampling follows by standard arguments .    if one has that @xmath299 and employs the time reversal of @xmath170 for @xmath166 then one arrives at an incremental importance weight , at time @xmath161 of : @xmath300 yielding the algorithm described in section [ sub : smc - with - an ] as an exact smc algorithm on the described extended space .",
    "a little notation is required .",
    "we allow @xmath301 to denote the common state space of the sampler during each iteration , @xmath302 the collection of continuous , bounded functions from @xmath303 to @xmath304 , and @xmath305 the collection of probability measures on this space .",
    "we define the boltzmann - gibbs operator associated with a potential function @xmath306 as a mapping , @xmath307 , weakly via the integrals of any function @xmath308 @xmath309    the integral of a set @xmath310 under a probability measure @xmath311 is written @xmath312 and the expectation of a function @xmath313 of @xmath314 is written @xmath315 .",
    "the supremum norm on @xmath302 is defined @xmath316 and the total variation distance on @xmath305 is @xmath317 .",
    "markov kernels , @xmath318 induce two operators , one on integrable functions and the other on ( probability ) measures : @xmath319    having established this notation , we note that we have the following recursive definition of the distributions we consider : @xmath320 and for notational convenience define the transition operators as @xmath321 we make use of the ( nonlinear ) dynamic semigroupoid , which we define recursively , via it s action on a generic probability measure @xmath311 , for @xmath322 : @xmath323 with @xmath324 and @xmath325 defined correspondingly .",
    "we begin with a lemma which allows us to control the discrepancy introduced by bayesian updating of a measure with two different likelihood functions .",
    "[ lemma : bgerror ] if a1 .",
    "holds , then @xmath326 and any @xmath322 : @xmath327    let @xmath328 and consider a generic @xmath308 : @xmath329    considering the absolute value of this discrepancy , making using of the triangle inequality : @xmath330 noting that @xmath260 is strictly positive , we can bound @xmath331 with @xmath332 and thus with @xmath333 and apply a similar strategy to the first term : @xmath334 ( noting that @xmath335 by integration of both sides of a1 ) .",
    "we now demonstrate that , if the local approximation error at each iteration of the algorithm(characterised by @xmath118 ) is sufficiently small then it does not accumulate unboundedly as the algorithm progresses .",
    "we begin with a telescopic decomposition ( mirroring the strategy employed for analysing particle approximations of these systems in @xcite ) : @xmath336 we thus establish ( noting that @xmath337 ) : @xmath338    turning our attention to an individual term in this expansion , noting that : @xmath339 we have , by application of a standard dobrushin contraction argument and lemma [ lemma : bgerror ] @xmath340 which controls the error introduced instantaneously during each step .",
    "we now turn our attention to controlling the accumulation of error .",
    "we make use of ( * ? ? ?",
    "* proposition 4.3.6 ) which , under assumptions a2 and a3 , allows us to deduce that for any probability measures @xmath341 : @xmath342 where @xmath343    returning to decomposition ( ) , applying the triangle inequality and this result , before finally inserting ( ) we arrive at : @xmath344 this is trivially bounded over all @xmath161 by the geometric series and a little rearrangement yields the result : @xmath345",
    "this appendix contains the simplest form of the random weight smc sampler used in the data point tempering examples in section [ sec : sequential - monte - carlo ] , in which resampling is performed at every step .",
    "essentially , any standard improvements to smc algorithms can be applied ."
  ],
  "abstract_text": [
    "<S> models for which the likelihood function can be evaluated only up to a parameter - dependent unknown normalizing constant , such as markov random field models , are used widely in computer science , statistical physics , spatial statistics , and network analysis . however , bayesian analysis of these models using standard monte carlo methods is not possible due to the intractability of their likelihood functions . </S>",
    "<S> several methods that permit exact , or close to exact , simulation from the posterior distribution have recently been developed . </S>",
    "<S> however , estimating the evidence and bayes factors ( bfs ) for these models remains challenging in general . </S>",
    "<S> this paper describes new random weight importance sampling and sequential monte carlo methods for estimating bfs that use simulation to circumvent the evaluation of the intractable likelihood , and compares them to existing methods . in some cases </S>",
    "<S> we observe an advantage in the use of _ biased _ weight estimates . </S>",
    "<S> an initial investigation into the theoretical and empirical properties of this class of methods is presented . </S>",
    "<S> some support for the use of biased estimates is presented , but we advocate caution in the use of such estimates .    </S>",
    "<S> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}