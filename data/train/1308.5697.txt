{
  "article_text": [
    "almost any method one can think of in data analysis and scientific computing relies on matrix algorithms . in the era of ` big data ' , we must now routinely deal with matrices of enormous sizes and reliable algorithmic solutions for computing solutions to least - squares problems , for computing approximate qr and svd factorizations and other such fundamental decompositions are urgently needed .",
    "fortunately , the development of randomized algorithms for numerical linear algebra has seen a new surge in recent years and we begin to see computational tools of a probabilistic nature with the potential to address some of the great challenges posed by big data . in this paper , we study one of the most frequently discussed algorithms in the literature for dimensionality reduction , and provide a novel analysis which gives sharp performance bounds .",
    "we are concerned with the fundamental problem of approximately factorizing an arbitrary @xmath0 matrix @xmath1 as @xmath2 where @xmath3 is the desired rank .",
    "the goal is to compute @xmath4 and @xmath5 such that @xmath6 is as small as possible .",
    "typically , one measures the quality of the approximation by taking either the spectral norm @xmath7 ( the largest singular value , also known as the 2 norm ) or the frobenius norm @xmath8 ( the root - sum of squares of the singular values ) of the residual @xmath9 .",
    "it is well - known that the best rank-@xmath10 approximation , measured either in the spectral or frobenius norm , is obtained by truncating the singular value decomposition ( svd ) , but this can be prohibitively expensive when dealing with large matrix dimensions .",
    "recent work @xcite introduced a randomized algorithm for matrix factorization with lower computational complexity .",
    "input : @xmath0 matrix @xmath1 and desired rank @xmath10 .",
    "sample an @xmath11 test matrix @xmath12 with independent mean - zero , unit - variance gaussian entries .",
    "compute @xmath13 .",
    "construct @xmath14 with columns forming an orthonormal basis for the range of @xmath15 .",
    "* return * the approximation @xmath16 , @xmath17 .",
    "the algorithm is simple to understand : @xmath18 is an approximation of the range of @xmath1 ; we therefore project the columns of @xmath1 onto this approximate range by means of the orthogonal projector @xmath19 and hope that @xmath20 . of natural interest",
    "is the accuracy of this procedure : how large is the size of the residual ?",
    "specifically , how large is @xmath21 ?",
    "the subject of the beautiful survey @xcite as well as @xcite is to study this problem and provide an analysis of performance .",
    "before we state the sharpest results known to date , we first recall that if @xmath22 are the ordered singular values of @xmath1 , then the best rank-@xmath10 approximation obeys @xmath23 it is known that there are choices of @xmath1 such that @xmath24 is greater than @xmath25 by an arbitrary multiplicative factor , see e.g.  @xcite .",
    "for example , setting @xmath26 and @xmath27 , direct computation shows that @xmath28 .",
    "thus , we write @xmath29 ( where @xmath30 ) and seek @xmath31 and @xmath32 such that @xmath33 we are now ready to state the best results concerning the performance of algorithm [ alg ] we are aware of .",
    "[ teo : hmt ] let @xmath1 be an @xmath0 matrix and run algorithm [ alg ] with @xmath34 , then @xmath35 \\sigma_{k+1}.\\ ] ]    it is a priori unclear whether this upper bound correctly predicts the expected behavior or not .",
    "that is to say , is the dependence upon the problem parameters in the right - hand side of the right order of magnitude ? is the bound tight or can it be substantially improved ?",
    "are there lower bounds which would provide ultimate limits of performance ?",
    "the aim of this paper is merely to provide some definite answers to such questions .",
    "it is convenient to write the residual in algorithm [ alg ] as @xmath36 as to make the dependence on the random test matrix @xmath12 explicit .",
    "our main result states that there is an explicit random variable whose size completely determines the accuracy of the algorithm .",
    "this statement uses a natural notion of stochastic ordering ; below we write @xmath37 if and only if the random variables @xmath38 and @xmath39 obey @xmath40 for all @xmath41 .    [ newtheorem ] suppose without loss of generality that @xmath42 . then in the setup of theorem [ teo : hmt ] , for each matrix @xmath43 , @xmath44 where @xmath45 is the random variable @xmath46 here , @xmath47 and @xmath48 are respectively @xmath49 and @xmath50 matrices with i.i.d .",
    "@xmath51 entries , @xmath52 is a @xmath53 diagonal matrix with the singular values of a @xmath54 gaussian matrix with i.i.d .",
    "@xmath51 entries , and @xmath55 is the @xmath56-dimensional identity matrix .",
    "furthermore , @xmath47 , @xmath48 and @xmath52 are all independent ( and independent from @xmath12 ) . in the other direction , for any @xmath57 , there is a matrix @xmath1 with the property @xmath58 in particular , this gives @xmath59    the proof of the theorem is in section [ sec : newbounds_subsection ] . to obtain very concrete bounds from this theorem",
    ", one can imagine using monte carlo simulations by sampling from @xmath45 to estimate the worst error the algorithm commits .",
    "alternatively , one could derive upper and lower bounds about @xmath45 by analytic means .",
    "the corollary below is established in section [ sec : corollaries ] .",
    "[ cor : newthm ] with @xmath45 as in ( recall @xmath60 ) , @xmath61    in the regime of interest where @xmath62 is very large and @xmath63 , the ratio between the upper and lower bound is practically equal to 1 so our analysis is very tight .",
    "furthermore , corollary [ cor : newthm ] clearly emphasizes why we would want to take @xmath64 . indeed , when @xmath65 , @xmath52 is square and nearly singular so that both @xmath66 and the lower bound become very large .",
    "in contrast , increasing @xmath67 yields a sharp decrease in @xmath68 and , thus , improved performance .",
    "it is further possible to derive explicit bounds by noting ( lemma [ lem : pseudoinverseexpectedvalue ] ) that @xmath69 plugging the right inequality into improves upon from @xcite . in the regime",
    "where @xmath63 ( we assume throughout this section that @xmath60 ) , taking @xmath70 , for instance , yields an upper bound roughly equal to @xmath71 and a lower bound roughly equal to @xmath72 , see figure [ fig : empirical ] .    when @xmath73 and @xmath67 are reasonably large , it is well - known ( see lemma [ lem : almostsurelaws ] ) that @xmath74 so that in the regime of interest where @xmath63 , both the lower and upper bounds in are about equal to @xmath75 we can formalize this as follows : in the limit of large dimensions where @xmath76 , @xmath77 with @xmath78 ( in such a way that @xmath79 ) , we have almost surely @xmath80 conversely , it holds almost surely that @xmath81 a short justification of this limit behavior may also be found in section [ sec : corollaries ] .",
    "whereas the analysis in @xcite uses sophisticated concepts and tools from matrix analysis and from perturbation analysis , our method is different and only uses elementary ideas ( for instance , it should be understandable by an undergraduate student with no special training ) .",
    "in a nutshell , the authors in @xcite control the error of algorithm [ alg ] by establishing an upper bound about @xmath82 holding for all matrices @xmath38 ( the bound depends on @xmath38 ) . from this , they deduce bounds about @xmath83 in expectation and in probability by integrating with respect to @xmath12 .",
    "a limitation of this approach is that it does not provide any estimate of how close the upper bound is to being tight .",
    "in contrast , we perform a sequence of reductions , which ultimately identifies the worst - case input matrix .",
    "the crux of this reduction is a monotonicity property , which roughly says that if the spectrum of a matrix @xmath1 is larger than that of another matrix @xmath4 , then the singular values of the residual @xmath84 are stochastically greater than those of @xmath85 , see lemma [ lem : monotonicity_forward_statement ] in section [ sec : monotonicity_forward_statement ] for details .",
    "hence , applying the algorithm to @xmath1 results in a larger error than when the algorithm is applied to @xmath4 . in turn",
    ", this monotonicity property allows us to write the worst - case residual in a very concrete form . with this representation",
    ", we can recover the deterministic bound from @xcite and immediately see the extent to which it is sub - optimal .",
    "most importantly , our analysis admits matching lower and upper bounds as discussed earlier .",
    "our analysis of algorithm [ alg ] , presented in section [ sec : newbounds_subsection ] , shows that the approximation error is heavily affected by the spectrum of the matrix @xmath1 past its first @xmath86 singular values .",
    "past @xmath87 . ]",
    "in fact , suppose @xmath42 and let @xmath88 be the diagonal matrix of dimension @xmath89 equal to @xmath90 .",
    "then our method show that the worst case error for matrices with this tail spectrum is equal to the random variable @xmath91 in turn , a very short argument gives the expected upper bound below :    [ cor : our_mixednorm ] take the setup of theorem [ teo : hmt ] and let @xmath92 be the @xmath93th singular value of @xmath1 .",
    "then @xmath94 substituting @xmath68 with the upper bound in recovers theorem 10.6 from @xcite",
    ".    this bound is tight in the sense that setting @xmath95 essentially yields the upper bound from corollary [ cor : newthm ] , which as we have seen , can not be improved .      to examine the tightness of our analysis of performance , we apply algorithm [ alg ] to the ` worst - case ' input matrix and compute the spectral norm of the residual , performing such computations for fixed values of @xmath96 , @xmath62 , @xmath73 and @xmath67 .",
    "we wish to compare the sampled errors with our deterministic upper and lower bounds , as well as with the previous upper bound from theorem [ teo : hmt ] and our error proxy . because of lemma [ maninv ] ,",
    "the worst - case behavior of the algorithm does not depend on @xmath96 and @xmath62 separately but only on @xmath97 .",
    "hence , we set @xmath98 in this section .    0.70     of the residual with worst - case matrix of dimension @xmath99 as input with @xmath62 varying between @xmath100 and @xmath101 .",
    "each grey dot represents the error of one run of algorithm [ alg ] .",
    "the lines are bounds on the spectral norm : the red dashed line plots the previous upper bound .",
    "the red ( resp .",
    "blue ) solid line is the upper ( resp .",
    "lower ) bound combining corollary [ cor : newthm ] and .",
    "the black line is the error proxy . in the top plot , @xmath102 . keeping fixed ratios results in constant error .",
    "holding @xmath73 and @xmath67 fixed in the bottom plot while increasing @xmath62 results in approximations of increasing error.,title=\"fig : \" ] [ fig : kpfixedratio ]    0.70     of the residual with worst - case matrix of dimension @xmath99 as input with @xmath62 varying between @xmath100 and @xmath101 .",
    "each grey dot represents the error of one run of algorithm [ alg ] .",
    "the lines are bounds on the spectral norm : the red dashed line plots the previous upper bound .",
    "the red ( resp .",
    "blue ) solid line is the upper ( resp .",
    "lower ) bound combining corollary [ cor : newthm ] and .",
    "the black line is the error proxy . in the top plot , @xmath102 . keeping fixed ratios results in constant error .",
    "holding @xmath73 and @xmath67 fixed in the bottom plot while increasing @xmath62 results in approximations of increasing error.,title=\"fig : \" ] [ fig : kpvariedratio ]    figure [ fig : empirical ] reveals that the new upper and lower bounds are tight up to a small multiplicative factor , and that the previous upper bound is also fairly tight .",
    "further , the plots also demonstrate the effect of concentration in measure  the outcomes of different samples each lie remarkably close to the yellow rule of thumb , especially for larger @xmath62 , suggesting that for practical purposes the algorithm is deterministic . hence , these experimental results reinforce the practical accuracy of the error proxy in the regime @xmath103 since we can see that the worst - case error is just about .",
    "0.49     ( recall that @xmath104 ) , @xmath73 and @xmath67 and plot the approximation error @xmath45 across @xmath105 independent runs of the algorithm .",
    "for the larger value of @xmath73 and @xmath67 , we see reduced variability ",
    "the approximation errors range from about @xmath106 to @xmath107 for @xmath108 and @xmath109 to @xmath110 for @xmath111 , much less variability in both absolute and percentage terms . similarly ,",
    "the empirical standard deviation with @xmath108 is approximately @xmath112 while with @xmath111 it falls to @xmath113 .",
    ", title=\"fig : \" ] [ fig : samples , n=1e5,k=1e2,p=1e2 ]    0.49     ( recall that @xmath104 ) , @xmath73 and @xmath67 and plot the approximation error @xmath45 across @xmath105 independent runs of the algorithm . for the larger value of @xmath73 and @xmath67 ,",
    "we see reduced variability  the approximation errors range from about @xmath106 to @xmath107 for @xmath108 and @xmath109 to @xmath110 for @xmath111 , much less variability in both absolute and percentage terms . similarly ,",
    "the empirical standard deviation with @xmath108 is approximately @xmath112 while with @xmath111 it falls to @xmath113 .",
    ", title=\"fig : \" ] [ fig : samples , n=1e5,k=1e3,p=1e3 ]    figure [ fig : empirical_samples ] gives us a sense of the variability of the algorithm for two fixed values of the triple @xmath114 . as expected , as @xmath73 and @xmath67 grow the variability of the algorithm decreases , demonstrating the effect of concentration of measure .",
    "in order to make the paper a little more self - contained , we briefly review some of the literature as to explain the centrality of the range finding problem ( algorithm [ alg ] ) . for instance , suppose we wish to construct an approximate svd of a very large matrix @xmath115 matrix @xmath1 . then this can be achieved by running algorithm [ alg ] and then computing the svd of the ` small ' matrix @xmath17 , check algorithm [ alg - svd ] below , which returns an approximate svd @xmath116 . assuming the extra steps in algorithm [ alg - svd ]",
    "are exact , we have @xmath117 so that the approximation error is that of algorithm [ alg ] whose study is the subject of this paper .",
    "input : @xmath0 matrix @xmath1 and desired rank @xmath10 . run algorithm [ alg ] .",
    "compute @xmath118 .",
    "compute the svd of @xmath119 .",
    "* return * the approximation @xmath120 , @xmath52 , @xmath121 .",
    "the point here of course is that since the matrix @xmath17 is @xmath122we typically have @xmath123the computational cost of forming its svd is on the order of @xmath124 flops and fairly minimal .",
    "( for reference , we note that there is an even more effective single - pass variant of algorithm [ alg - svd ] in which we do not need to re - access the input matrix @xmath1 once @xmath125 is available , please see @xcite and references therein for details . )",
    "naturally , we may wish to compute other types of approximate matrix factorizations of @xmath1 such as eigenvalue decompositions , qr factorizations , interpolative decompositions ( where one searches for an approximation @xmath126 in which @xmath4 is a subset of the columns of @xmath1 ) , and so on .",
    "all such computations would follow the same pattern : ( 1 ) apply algorithm [ alg ] to find an approximate range , and ( 2 ) perform classical matrix factorizations on a matrix of reduced size .",
    "this general strategy , namely , approximation followed by standard matrix computations , is of course hardly anything new . to be sure , the classical businger - golub algorithms for computing partial qr decompositions follows this pattern .",
    "again , we refer the interested reader to the survey @xcite .",
    "we have seen that when the input matrix @xmath1 does not have a rapidly decaying spectrum , as this may be the case in a number of data analysis applications , the error of approximation algorithm [ alg ] commits  the random variable @xmath45 in theorem [ newtheorem]may be quite large .",
    "in fact , when the singular values hardly decay at all , it typically is on the order of the error proxy .",
    "this results in poor performance . on the other hand ,",
    "when the singular values decay rapidly , we have seen that the algorithm is provably accurate , compare theorem [ cor : our_mixednorm ] .",
    "this suggests using a power iteration , similar to the block power method , or the subspace iteration in numerical linear algebra : algorithm [ alg - power ] was proposed in @xcite .",
    "input : @xmath0 matrix @xmath1 and desired rank @xmath10 .",
    "sample an @xmath11 test matrix @xmath12 with independent mean - zero , unit - variance gaussian entries .",
    "compute @xmath127 .",
    "construct @xmath14 with columns forming an orthonormal basis for the range of @xmath15 .",
    "* return * the approximation @xmath16 , @xmath17 .",
    "the idea in algorithm [ alg - power ] is of course to turn a slowly decaying spectrum into a rapidly decaying one at the cost of more computations : we need @xmath128 matrix - matrix multiplies instead of just one .",
    "the benefit is improved accuracy .",
    "letting @xmath129 be any orthogonal projector , then a sort of jensen inequality states that for any matrix @xmath1 , @xmath130 see @xcite for a proof .",
    "therefore , if @xmath125 is computed via the power trick ( algorithm [ alg - power ] ) , then @xmath131 this immediately gives a corollary to theorem [ newtheorem ] .",
    "[ cor : power ] let @xmath1 and @xmath45 be as in theorem [ newtheorem ] .",
    "applying algorithm [ alg - power ] yields @xmath132    it is amusing to note that with , say , @xmath133 , @xmath134 , and the error proxy for @xmath45 , the size of the error factor @xmath135 in is about 3.41 when @xmath136 .",
    "further , we would like to note that our analysis exhibits a sequence of matrices that have approximation errors that limit to the worst case approximation error when @xmath137 . however , when @xmath138 , this same sequence of matrices limits to having an approximation error exactly equal to @xmath139 , which is the best possible since this is the error achieved by truncating the svd .",
    "it would be interesting to study the tightness of the upper bound and we leave this to future research .      in section [ sec : discussion ] , we shall see that among all possible test matrices , gaussian inputs are in some sense optimal .",
    "otherwise , the rest of the paper is mainly devoted to proving the novel results we have just presented .",
    "before we do this , however , we pause to introduce some notation that shall be used throughout .",
    "we reserve @xmath140 to denote the @xmath99 identity matrix .",
    "when @xmath12 is an @xmath141 random gaussian matrix , we write @xmath142 to save space .",
    "hence , @xmath143 is a random variable .",
    "we use the partial ordering of @xmath62-dimensional vectors and write @xmath144 if @xmath145 has nonnegative entries . similarly , we use the semidefinite ( partial ) ordering and write @xmath146 if @xmath147 is positive semidefinite . we also introduce a notion of stochastic ordering in @xmath148 : given two @xmath62-dimensional random vectors @xmath149 and @xmath150 , we say that @xmath151 if @xmath152 for all @xmath153 . if instead @xmath154 then we say that @xmath149 and @xmath150 are equal in distribution and write @xmath155 .",
    "a function @xmath156 is monotone non - decreasing if @xmath157 implies @xmath158 .",
    "note that if @xmath159 is monotone non - decreasing and @xmath151 , then @xmath160 .",
    "the key insight underlying our analysis is this :    [ lem : monotonicity_forward_statement ] if @xmath1 and @xmath4 are matrices of the same dimensions obeying @xmath161 , then @xmath162    the implications of this lemma are two - fold .",
    "first , to derive error estimates , we can work with diagonal matrices without any loss of generality .",
    "second and more importantly , it follows from the monotonicity property that @xmath163 the rest of this section is thus organized as follows :    * the proof of the monotonicity lemma is in section [ sec : monotonicity_proof_section ] . * to prove theorem [ newtheorem ] , it suffices to consider a matrix input as @xmath164 in with @xmath165 .",
    "this is the object of section [ sec : newbounds_subsection ] .",
    "* bounds on the worst case error ( the proof of corollary [ cor : newthm ] ) are given in section [ sec : corollaries ] .",
    "* the mixed norm theorem [ cor : our_mixednorm ] is proved in section [ sec : mixed ] .",
    "additional supporting materials are found in the appendix .",
    "we begin with an intuitive lemma .",
    "[ maninv ] let @xmath43 , @xmath166 , @xmath167 be arbitrary matrices with @xmath168 and @xmath121 orthogonal",
    ". then @xmath169 . in particular ,",
    "if @xmath170 is a singular value decomposition of @xmath1 , then @xmath171 .    observe that by spherical symmetry of the gaussian distribution , a test vector @xmath159 ( a column of @xmath12 ) has the same distribution as @xmath172 .",
    "we have @xmath173 and thus , @xmath174 hence , @xmath175 , which means that the distribution of @xmath176 depends only upon @xmath177 . by induction",
    ", one establishes @xmath178 in the same fashion .",
    "the induction step uses lemma [ lem : chaining ] , which states that @xmath179 , where @xmath180 and @xmath181 are a partition of the columns of @xmath12 .",
    "a consequence of this lemma is that we only need to show the monotonicity property for pairs of diagonal matrices obeying @xmath182 .",
    "the lemma below proves the monotonicity property in the special case where @xmath27 .",
    "[ deterministicinequality ] let @xmath183 and @xmath184 be @xmath99 diagonal and positive semidefinite matrices obeying @xmath185 .",
    "let @xmath159 be an arbitrary vector in @xmath148 .",
    "then for all @xmath186 , @xmath187 this implies that @xmath188 .",
    "introduce @xmath189 in which @xmath190 is the diagonal matrix with @xmath92 on the diagonal .",
    "clearly , it suffices to show that @xmath191 to prove the first claim .",
    "putting @xmath192 , @xmath193 is given by @xmath194 taking derivatives gives @xmath195 hence , @xmath196 .",
    "the second part of the lemma follows from lemma [ hornandjohnsoninequality ] below , whose result is a consequence of corollary 4.3.3 in @xcite and the following fact : @xmath197 for all @xmath186 if and only if @xmath198 .",
    "[ hornandjohnsoninequality ] if @xmath197 for all @xmath186 , then @xmath161 .",
    "we now extend the proof to arbitrary @xmath10 , which finishes the proof of the monotonicity property .",
    "[ lem : strongestmonotonicity ] take @xmath183 and @xmath184 as in lemma [ deterministicinequality ] and let @xmath199 be a test matrix .",
    "then for all @xmath153 , @xmath200 again , this implies that @xmath201 .",
    "fix @xmath153 and put @xmath202 .",
    "the vector @xmath203 uniquely decomposes as the sum of @xmath204 and @xmath205 , where @xmath205 belongs to the range of @xmath206 and @xmath204 is the orthogonal component .",
    "now , since @xmath205 is in the range of @xmath206 , there exists @xmath159 such that @xmath207 .",
    "we have @xmath208 therefore , lemma [ deterministicinequality ] gives @xmath209 the last inequality follows from @xmath159 being in the range of @xmath12 . in details , let @xmath210 ( resp .",
    "@xmath211 ) be the orthogonal projector onto the range of @xmath212 ( resp .",
    "@xmath213 ) .",
    "then pythagoras theorem gives @xmath214 the second part of the lemma is once more a consequence of lemma [ hornandjohnsoninequality ] .",
    "we let @xmath88 be an @xmath56-dimensional diagonal matrix and work with @xmath215 set @xmath216 and partition the rows as @xmath217 next , introduce an svd for @xmath180 @xmath218 and partition @xmath219 as @xmath220 a simple calculation shows that @xmath221 hence , @xmath15 and @xmath222 have the same column space .",
    "if @xmath223 is an orthonormal basis for the range of @xmath224 , we conclude that @xmath225 have the same column space as @xmath15 .",
    "note that the last @xmath67 columns of @xmath226 are orthonormal .",
    "continuing , we let @xmath4 be the first @xmath73 columns of @xmath226 .",
    "then lemma [ appendix : almostorthogonal ] below allows us to decompose @xmath4 as @xmath227 where @xmath125 is orthogonal with the same range space as @xmath4 and @xmath228 has a spectral norm at most @xmath229 .",
    "further since the first @xmath73 columns of @xmath226 are orthogonal to the last @xmath67 columns , we have @xmath230 where @xmath231 is orthogonal with the same range space as @xmath226 and @xmath232 has a spectral norm also at most @xmath229 .",
    "this gives @xmath233 we have reached the conclusion @xmath234 when @xmath235 , this gives our theorem .",
    "[ appendix : almostorthogonal ] let @xmath236 .",
    "then @xmath1 is @xmath237 in frobenius norm away from a matrix with the same range and orthonormal columns .",
    "we need to construct a matrix @xmath238 obeying @xmath239 and such that ( 1 ) @xmath240 is orthogonal and ( 2 ) @xmath125 and @xmath1 have the same range .",
    "let @xmath241 be a reduced svd decomposition for @xmath1 , @xmath242 clearly , @xmath125 is an orthonormal matrix with the same range as @xmath1 .",
    "next , the @xmath93th singular value of @xmath1 obeys @xmath243 hence , @xmath244 which proves the claim .      put @xmath245 . since @xmath246 is an orthogonal projector , @xmath247 and gives @xmath248 also ,",
    "@xmath249 standard estimates in random matrix theory ( lemma [ lem : expectedvalue ] ) give @xmath250 hence , @xmath251    conversely , letting @xmath93 be the index corresponding to the largest entry of @xmath252 , we have @xmath253 .",
    "therefore , @xmath254 where @xmath203 is the @xmath93th column of @xmath47 .",
    "now @xmath255 is the projection of a gaussian vector onto a plane of dimension @xmath256 drawn independently and uniformly at random .",
    "this says that @xmath257 , where @xmath39 is a chi - square random variable with @xmath258 degrees of freedom . if @xmath159 is a nonnegative random variable , then with @xmath259 , @xmath260 .",
    "] @xmath261 since @xmath262 and @xmath263 , we have @xmath264    hence , @xmath265 which establishes corollary [ cor : newthm ] .",
    "the limit bounds and are established in a similar manner .",
    "the upper estimate is a consequence of the bound @xmath266 together with lemma [ lem : almostsurelaws ] .",
    "the lower estimate follows from @xmath267 , where @xmath39 is a chi - square as before , together with lemma [ lem : almostsurelaws ] .",
    "we forgo the details .",
    "take @xmath164 as in with @xmath268 on the diagonal of @xmath88 .",
    "applying gives @xmath269 we follow ( * ? ? ?",
    "* proof of theorem 10.6 ) and use an inequality of gordon to establish @xmath270 where @xmath271 is expectation over @xmath47 .",
    "further , it is well - known that @xmath272 the reason is that @xmath273 , where @xmath274 is a wishart matrix @xmath275 .",
    "the identity follows from @xmath276 ( * ? ? ?",
    "* exercise 3.4.13 ) . in summary",
    ", the expectation of the right - hand side in is bounded above by @xmath277 since @xmath278 , the conclusion of the theorem follows .",
    "we have developed a new method for characterizing the performance of a well - studied algorithm in randomized numerical linear algebra and used it to prove sharp performance bounds . a natural question to ask when using algorithm [ alg ] is if one should draw @xmath12 from some distribution other than the gaussian .",
    "it turns out that for all values @xmath279 and @xmath67 , choosing @xmath12 with gaussian entries minimizes @xmath280 this is formalized as follows :    [ lem : optimal_gaussian ] choosing @xmath281 with i.i.d .",
    "gaussian entries minimizes the supremum of @xmath282 across all choices of @xmath1 .",
    "fix @xmath1 with @xmath283 ( this is no loss of generality ) and suppose @xmath284 is sampled from an arbitrary measure with probability @xmath139 of being rank @xmath10 ( since being lower rank can only increase the error ) .",
    "the expected error is @xmath285 , where for arbitrary matrices , @xmath286 in which @xmath129 is the orthogonal projection onto the range of @xmath1 .",
    "suppose further that @xmath168 is drawn uniformly at random from the space of orthonormal matrices . then if @xmath12 is sampled from the gaussian distribution , @xmath287 since @xmath288 chooses a subspace uniformly at random as does @xmath12 .",
    "therefore , there exists @xmath289 with the property @xmath290 , whence @xmath291 hence , the expected error using a test matrix drawn from the gaussian distribution on @xmath1 is smaller or equal to that when using a test matrix drawn from another distribution on @xmath292 . since the singular values of @xmath1 and @xmath292 are identical since @xmath289 is orthogonal , the gaussian measure ( or any measure that results in a rotationally invariant choice of rank @xmath73 subspaces ) is worst - case optimal for the spectral norm .",
    "the analysis presented in this paper does not generalize to a test matrix @xmath12 drawn from the subsampled random fourier transform ( srft ) distribution as suggested in @xcite . despite their inferior performance in the sense of lemma [ lem : optimal_gaussian ] , srft test matrices are computationally attractive since they come with fast algorithms for matrix - matrix multiply .",
    "we use well - known bounds to control the expectation of the extremal singular values of a gaussian matrix .",
    "these bounds are recalled in @xcite , though known earlier .",
    "the upper bound is the same as is used in @xcite and follows from the work of @xcite . for the lower bound , set @xmath298 which has an inverse wishart distribution , and observe that @xmath299 where @xmath300 is the entry in the @xmath301 position .",
    "it is known that @xmath302 , where @xmath39 is distributed as a chi - square variable with @xmath303 degrees of freedom ( * ? ? ?",
    "* page 72 ) . hence , @xmath304            we assume @xmath316 and use induction for larger @xmath73 .",
    "@xmath317 is the projection of @xmath1 onto the subspace orthogonal to @xmath318 and @xmath319 is the projection of @xmath317 onto the subspace orthogonal to @xmath320 .",
    "however , @xmath318 and @xmath320 are orthogonal subspaces and , therefore , @xmath319 is the projection of @xmath1 onto the subspace orthogonal to @xmath321 ; this is @xmath322 .",
    "e. c. is partially supported by nsf via grant ccf-0963835 and by a gift from the broadcom foundation .",
    "we thank carlos sing - long for useful feedback about an earlier version of the manuscript .",
    "these results were presented in july 2013 at the european meeting of statisticians .",
    "n. halko , p - g .",
    "martinsson , and j. a. tropp , `` finding structure with randomness : probabilistic algorithms for constructing approximate matrix decompositions '' .",
    "_ siam review _",
    "53 , pp .  217288 , 2011 .",
    "martinsson , v. rokhlin , and m. tygert , `` a fast randomized algorithm for the approximation of matrices '' .",
    "_ appl .  comp .",
    "_ , vol .  30 , pp .",
    "4768 , 2011 .",
    "( early version published as yaleu / dcs / tr-1361 , 2006 . )      m. rudelson , and r. vershynin , `` non - asymptotic theory of random matrices : extreme singular values '' . in _ proceedings of the international congress of mathematicians _ , vol .",
    "iii , pp .  15761602 , hindustan book agency , new delhi , 2010 ."
  ],
  "abstract_text": [
    "<S> the development of randomized algorithms for numerical linear algebra , e.g.  for computing approximate qr and svd factorizations , has recently become an intense area of research . </S>",
    "<S> this paper studies one of the most frequently discussed algorithms in the literature for dimensionality reduction  specifically for approximating an input matrix with a low - rank element . </S>",
    "<S> we introduce a novel and rather intuitive analysis of the algorithm in @xcite , which allows us to derive sharp estimates and give new insights about its performance . </S>",
    "<S> this analysis yields theoretical guarantees about the approximation error and at the same time , ultimate limits of performance ( lower bounds ) showing that our upper bounds are tight . </S>",
    "<S> numerical experiments complement our study and show the tightness of our predictions compared with empirical observations . </S>"
  ]
}