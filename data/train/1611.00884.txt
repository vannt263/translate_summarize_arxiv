{
  "article_text": [
    "the maximum likelihood estimator ( mle ) is perhaps the most common and widely accepted estimator of a parameter in a statistical model denoted by @xmath0 , where @xmath1 denote respectively the sample space , the parameter space and the probability density function ( pdf ) .",
    "we will take @xmath2 , and @xmath3 . in the standard case of independent and identically distributed observations , @xmath4 , where @xmath5 is the pdf of @xmath6 .",
    "given data @xmath7 , the likelihood function is @xmath8 and the mle  of the parameter @xmath9 is defined as that value @xmath10 which globally maximizes @xmath11 .",
    "_ mathematica _ ( wolfram , 1996 ) has been widely used in the study of fundamental and general aspects of maximum likelihood estimation  see andrews and stafford ( 1993 ) ; stafford and andrews ( 1993 ) ; stafford , andrews and wang ( 1994 ) .",
    "as well _ mathematica _ has been used for obtaining symbolically exact maximum likelihood estimators in situations where the use of numerical techniques are less convenient such as with grouped or censored data or logistic regression  see cabrera ( 1989 ) ; currie ( 1995 ) .    for simplicity we will deal with the case where @xmath12 is one - dimensional .",
    "the multidimensional case may in general be reduced to the one - dimensional case by using marginal , conditional or concentrated likelihoods or by integrating over the nuisance parameters whichever is more suitable in a particular situation . under the usual regularity conditions ,",
    "the mle , @xmath13 , is approximately normally distributed with mean @xmath9 and covariance matrix @xmath14 , where @xmath15 denotes the fisher information matrix .",
    "it is also true that the mean likelihood estimator ( mele ) is equally efficient in large samples .",
    "in general the mele , @xmath16 is defined by @xmath17 where @xmath11 is the likelihood function .",
    "it should be noted that although the mele  is identical to the bayes estimator with a uniform prior , it is not often considered in frequentist settings even though pitman ( 1938 ) showed that when the problem is location invariant , the mele  is the best invariant estimator .",
    "barnard , jenkins and winsten ( 1962 ) recommended the mele  for time series problems and suggested that it will often have lower mse  than the mle . in changepoint analysis ,",
    "where the usual regularity conditions for the mle  do not hold and the mle  is inefficient but the mele  works well ( ritov , 1990 ; rubin and song , 1995 ) .",
    "unlike the mle  the mele  is not invariant under reparameterization .",
    "although the mele  has a bayesian interpretation , it is not the bayesian estimator that is usually recommended . in order that the estimator share mle  property of being invariant under parameter transformation ,",
    "the jeffrey s noninformative prior is recommended when there is no prior information available ( box and tiao , 1973 ,  1.3 ) .",
    "the jeffrey s prior is given by @xmath18 .",
    "there are situations , such as in the first - order moving - average model ( ma(1 ) ) where the mle  in finite samples has non - zero probability of lying on the boundary of the parameter region but this phenomenon does not happen with the meleor bayesian estimator as can be seen from the following result .",
    "* theorem 1 : * let @xmath19 $ ] then @xmath20 .    *",
    "proof : * the likelihood function , @xmath11 , defined below , is easily seen to be continuous and differentiable in the interval @xmath21 $ ] and non - negative .",
    "it then follows from the generalized mean - value theorem ( borowski and borwein , 1991 , p.371 ) that @xmath22    in many cases the mle  is easy to compute using pen and paper . however with _",
    "mathematica _ we can now easily obtain the mele  by numerical integration and sometimes symbolically .",
    "in fact , for problems where the likelihood function is complicated or difficult to evaluate the mele  may be computationally easier to compute than the traditional mle .",
    "as shown in theorem 2 , both the mle  and mele  are first order efficient .",
    "* theorem 2 : * under the usual regularity conditions for maximum likelihood estimators , @xmath23 .    *",
    "proof : * the likelihood function , @xmath11 , is to @xmath24 equal to the normal probability density function with mean @xmath9 and variance @xmath14 ( tanner , 1993 , p.16 ) .",
    "the result then follows directly from this approximation .",
    "@xmath25    now consider an estimator @xmath26 of @xmath9 .",
    "the mean - square error ( mse ) of an estimator @xmath26 is defined as @xmath27 .",
    "the relative efficiency of @xmath26 vs @xmath13 is defined as @xmath28 . clearly , from theorem 2 , as @xmath29 .",
    "barnard , jenkins and winsten ( 1962 ) suggested that in many small sample situations the mele  is preferred by the mean - square error criterion and hence at least for some values of @xmath9 , @xmath30 , where @xmath13 and @xmath16 denote the mle  and mele  respectively .",
    "pitman ( 1937 ) formulated a useful alternative to the mse  in the situation where no explicit loss function is known .",
    "consider two estimators , @xmath26 and @xmath31 , and assume that with probability one , @xmath32 then the pitman measure of closeness for comparing @xmath26 vs @xmath31 is defined as @xmath33 = \\pr \\left\\ { | \\hat{\\theta_1}-\\theta | < | \\hat{\\theta_2}-\\theta | \\right\\}. \\label{pmcone}\\ ] ] when @xmath34 > 1/2 $ ] , @xmath26 is preferred to @xmath31 .",
    "the monograph of keating , mason and sen ( 1993 ) provides an extensive survey of recent work and applications of the pmc .",
    "additionally , volume 20 ( 11 ) of _ communications in statistics : theory and methods _ contains an entire issue on the pmc .",
    "unlike the mse  and relative efficiency , the pmc depends on the bivariate distribution of the two estimators .",
    "the pmc is more appropriate in many scientific and industrial applications in which the estimator which is closer to the truth is required .",
    "sometimes it is felt that the mse  and other risk criteria give too much weight to large deviations which may seldom occur .",
    "rao and other researchers ( keating , mason and sen , 1993 ,  3.3 ) have found that risk functions such as mse  and mean - absolute - error can often be shrunk but that this shrinkage occurs at the expense of the pmc .",
    "the mse  or some other risk function is more appropriate than pmc in the decision theory framework when there is some economic or other loss associated with the estimation error . in practice",
    "it is often useful to consider both the pmc and mse  and in many situations there appears to be a high level of concordance between these estimators ( keating , mason and sen , 1993 ,  2.5 ) .",
    "as originally pointed by pitman ( 1937 ) the pmc criterion is intransitive but it is arguable whether this is a practical limitation .",
    "this point as well as other limitations and extensions of the pmc are discussed by keating , mason and sen ( 1993 , ch.3 )    * theorem 3 : * @xmath16 and @xmath13 are not necessarily asymptotically equivalent under the pmc .",
    "* proof : * see eqn .",
    "[ eqbinomiallimit ] .",
    "@xmath25    the next theorem shows that the mele  minimizes the mean likelihood of the squared error .",
    "* theorem 4 : * choosing @xmath35 minimizes @xmath36 , where @xmath37    * proof : * using calculus , the result follows directly .",
    "@xmath25    * theorem 5 : * @xmath16 is a function of the sufficient statistic for @xmath38 , if there is one .    in general ,",
    "the mele  is a biased estimator .",
    "* theorem 6 : * if @xmath12 has compact support and @xmath39 then @xmath40 .",
    "theorems 5 and 6 are derived in quenneville ( 1993 ) .",
    "the mele  is formally equivalent to the bayes estimator under a locally uniform prior with the squared error risk function and many of the above theorems have their well - known bayesian analogues .",
    "we are now going to make comparisons between these three estimators for three statistical models : bernouilli trials , exponential lifetimes and the first - order moving average process .",
    "the symbolic , numeric and graphical computations will all be done using _",
    "mathematica_. the interested reader can reproduce or extend our computations using the _ mathematica _ notebooks we have provided ( mcleod and quenneville , 1999 ) .",
    "frequentist analysis of bayesian estimators is not often done but dempster ( 1998 ) and quenneville and singh ( 1999 ) have argued that frequentist considerations are obviously informative even in the bayesian setting .",
    "we will now examine the performance of these three estimators in the estimation of the parameter @xmath41 in a sequence of @xmath42 bernoulli trials where @xmath7 is the observed number of successes and @xmath41 is the probability of success .",
    "the probability function is @xmath43 so if @xmath7 successes are observed in @xmath42 trials , the likelihood function may be written @xmath44 and the mle  may be derived by calculus , @xmath45 . using _",
    "it is easily shown that the mele  of @xmath41 is @xmath46 and that @xmath47 provided @xmath48 as shown in figure  [ binomialmse ] , the mele  is always more efficient over most of the range and the relative efficiency tends to @xmath49 as @xmath50 .",
    "it is interesting to compare the mele  with bayes estimate under a jeffrey s prior .",
    "the jeffrey s prior for @xmath41 is ( box and tiao , p.35 ) , @xmath51 .",
    "combining with the likelihood we can use _ mathematica _ to show that the resulting bayes estimator is @xmath52 . from figure  [ binomialmse ]",
    ", we see that the bayes estimator with jeffrey s prior tends have smaller mean - square error over an even slightly larger range of @xmath41 than the melebut the gain in efficiency with the mele can be greater . as with the mele , the relative efficiency tends to @xmath49 as @xmath50 . once again , using _",
    "mathematica _ we can show that @xmath53 provided @xmath54    the pmc criterion given in eqn .",
    "[ pmcone ] is not applicable in the case of the binomial since due to the discreteness there can be ties in the values of the estimators .",
    "keating , mason and sen ( 1993 , ",
    "3.4.1 ) and one of the referees have suggested the following modified version of pitman s measure of closeness , @xmath55 = \\pr \\left\\ { | \\bar{\\theta}-\\theta | < | \\hat{\\theta}-\\theta | \\right\\ } + { 1\\over 2 } \\pr \\left\\ { | \\bar{\\theta}-\\theta | = | \\hat{\\theta}-\\theta | \\right\\}.\\ ] ] with this modification , pmc is transitive and reflexive .",
    "figure  [ pmcbinomial ] suggests the following asymptotic result , @xmath56 this result may be established using the geary - rao theorem ( keating , mason and sen , p.103 ) .",
    "figure  [ pmcbinomial ] also suggests that in terms of the pmc the advantage over the mle  of the mele  or of the bayes estimate with a jeffrey s prior disappears when there is no prior information about @xmath41 .",
    "consider a sample of size @xmath42 denoted by @xmath57 from an exponential distribution with mean @xmath58 and let @xmath59 .",
    "the likelihood function for @xmath58 can be written @xmath60 , the mle  of @xmath58 is given by @xmath61 and the mele  of @xmath58 is @xmath62 .",
    "the jeffrey s prior for @xmath58 can be taken as @xmath63 which produces a bayesian estimate , @xmath64 .",
    "a simple computation with _ mathematica _ gives the relative efficiency , @xmath65 similarly , @xmath66 .",
    "figure  [ rlife ] shows that the mele  can be much less efficient .",
    "since @xmath67 has a standard gamma distribution with shape parameter @xmath42 and scale parameter @xmath58 , the pmc is easily evaluated using the geary - rao theorem ( keating , mason and sen , 1993 , p.103 ) . letting @xmath68 or @xmath69",
    ", we can write @xmath70 where @xmath71 or @xmath72 according as @xmath68 or @xmath69 .",
    "notice that without loss of generality we may take @xmath73 since @xmath74 . from figure  [ pmclife ] , @xmath75 for both @xmath68 or @xmath69 .",
    "it is sometimes mistakenly thought that theorem 4 or its bayesian analogue guarantees that at least over some region of the parameter space , the meleand the bayes estimator will have outperform the mlebut this need not be the case .",
    "the ma(1 ) time series with mean @xmath58 may be written @xmath76 , where @xmath77 denotes the observation at time @xmath78 and @xmath79 , the innovation at time @xmath80 , is assumed to be a sequence of independent normal random variables with mean zero and variance @xmath81 .",
    "the parameter @xmath9 determines the autocorrelation structure of the series and for identifiability we will assume that @xmath82 . when @xmath83 , the model is invertible ( brockwell and davis , 1991 ,  3.1 ) . for simplicity we will examine the case where @xmath84 .",
    "such ma(1 ) models often arise in practical applications as the model for a differenced nonstationary time series .",
    "the non - invertible case @xmath85 occurs when a series is over - differenced .    in large - samples ,",
    "standard asymptotic theory suggests that the maximum likelihood estimate for @xmath9 , denoted by @xmath13 , is approximately normal with mean @xmath9 and variance @xmath86 where @xmath42 is the length of the observed time series .",
    "cryer and ledolter ( 1981 ) established the somewhat surprising result that @xmath87 .",
    "this result holds for all finite @xmath42 and for all values of @xmath9 .",
    "for example when @xmath88 and @xmath89 ( cryer and ledolter , 1981 , table 2 ) .",
    "let @xmath16 denote the mean likelihood estimate of @xmath9 .",
    "in view of theorem 1 , this problem does not occur with @xmath16 .",
    "now we will show that the mele  dominates the mle  both for the mse  and pmc criteria when @xmath90 . when @xmath91 , the mele  is better than the mle  unless the parameter @xmath9 is very close to @xmath92 .",
    "since even the useless estimator obtained by ignoring the data and setting the estimate to @xmath49 does better when @xmath85 , we can conclude that mele  is generally a better estimator .",
    "further mean - square error computations which support this conclusion for other values of @xmath42 are given by quenneville ( 1993 ) and can be verified by the reader using the electronic supplement .",
    "given a gaussian time series of length @xmath93 , generated from the first - order moving average equation @xmath94 , where @xmath79 are independent normal random variables with mean zero and variance @xmath81 .",
    "let @xmath95 . then given data , @xmath96 , the exact concentrated likelihood function for @xmath9 is ( cryer and ledolter , 1981 ; quenneville , 1993 ) , @xmath97 and @xmath98 $ }   \\\\",
    "\\frac{1- \\sqrt{1 - 16w^2}}{4w } & \\mbox{$w \\in ( -0.25 , 0.25 ) , w \\not= 0 $ } \\\\ 0 & \\mbox{$w = 0 $ }   \\\\ 1 & \\mbox{$w \\in [ 0.25 , 0.5]$. } \\end{array } \\right.\\ ] ]    unfortunately @xmath16 , can not be evaluated symbolically .",
    "however using * nintegrate * we can obtain it numerically .",
    "numerical evaluation suggests that @xmath16 is either a linear or close to a linear function of @xmath99 . to speed up our computations for the mean - square error of @xmath16",
    ", we use the * functioninterpolation * in _ mathematica _ to construct @xmath100 .",
    "the mse  and pmc for @xmath16 and @xmath13 are easily evaluated numerically using the pdf of @xmath101 , derived by quenneville ( 1993 ) , @xmath102    from figures  [ rma1n2 ] and [ pmcma1n2 ] , it is seen that both the mele  and bayesian estimator dominate the mle  both for the mse  and pmc criteria .",
    "the mele  is slightly better according to the mse  but according to the pmc the bayes estimator is slightly better than the mele .",
    "consider the ma(1 ) process defined by @xmath103 , where @xmath79 is assumed to be normal and independently distributed with mean zero and variance @xmath81 .",
    "given @xmath42 observations @xmath104 the exact log likelihood function of an arma process can be written ( newbold , 1974 ) , @xmath105 where @xmath106 and @xmath107 where @xmath108 is the @xmath109 by @xmath42 matrix , @xmath110 maximizing over @xmath81 the concentrated log likelihood is given by @xmath111 - \\frac{1}{2 } \\log(d).\\ ] ] this expression for the concentrated loglikelihood is just as easy to write in _ mathematica _ notation as it is in ordinary mathematical notation . moreover ,",
    "it can be evaluated symbolically or numerically .",
    "newbold s algorithm can be made much more efficient when only numerical values of the log likelihood are needed by using the _ mathematica _ compiler and by re - writing the calculations involved to make more use of efficient _ mathematica _ functions such as * nestlist * , * foldlist * and * apply*. first consider the computation of the vector @xmath112 which is of length @xmath113 . after some simplifications , we see that @xmath114 , where @xmath115 is the first element and the remaining elements are defined recursively by @xmath116 , where @xmath117 . this computation is efficiently performed by _ mathematica _ s * foldlist*. when we are just interested in numerical evaluation we use the compile function to generate native code which runs much faster .    .... getlz = compile[{{t,_real},{z , _ real , 1 } } ,    foldlist[(#1 t + # 2)&,0,z ] ] ; ....    the determinant , @xmath118 , is efficiently computed using * nestlist * to generate the individual terms and then summing .",
    ".... detma = compile[{{t,_real},{n , _ integer } } ,    apply[plus , nestlist[#1 t & , 1,n]^2 ] ] ; ....    next , we evaluate the term @xmath119 . since @xmath120 we can use horner s rule to efficiently compute this sum .",
    "horner s rule is implemented in _",
    "mathematica _ using the function * fold*.    ....",
    "getu0 = compile[{{t,_real},{lz , _ real , 1},{detma , _ real } } ,         -fold[#1 t + # 2&,0,reverse[lz]]/detma ] ; ....    the computation of the sum of squares function @xmath121 is straightforward .",
    "the _ mathematica _ compiler can be used to optimize the vector computations .",
    ".... getsumsq =    compile[{{t,_real},{lz , _ real , 1},{u , _",
    "real},{n , _ integer } } ,      apply[plus,(lz+nestlist[#1 t & , 1,n ] u)^2 ] ] ; ....    finally , the concentrated loglikelihood function is defined .",
    "the computation speed is increased by about a factor of @xmath122 times when @xmath91 and is even larger for larger @xmath42 .    ....",
    "loglma1f[t _ , z _ ] : =    module[{n = length[z ] } ,      lz = getlz[t , z ] ;      detma = detma[t , n ] ;      u = getu0[t , lz , detma ] ;      s = getsumsq[t , lz , u , n ] ;      -(1/2 ) log[detma]- ( n/2)log[s / n ] ] ; ....    this function can be maximized using _ mathematica",
    "_ s nonlinear optimization function * findminimum*.    the mean likelihood estimate @xmath16 can be evaluated using * nintegrate*.    .... meanle[z_]:=    nintegrate[t e^loglma1f[t , z],{t,-1,1}]/      nintegrate[e^loglma1f[t , z],{t,-1,1 } ] ....    notice that in the above expression the loglikelihood function is evaluated separately in both the numerator and denominator .",
    "hence , we can save function evaluations by using our own numerical quadrature routine .    ....",
    "simpsonquadratureweights[k_,a _ , b_]:=    with[{h=(2 k)/3 } ,     { a+(b - a)range[0,2 k]/(2k ) ,     prepend[append[drop[flatten[table[{4,2},{k}]],{-1}],1],1 ] } ]    { x , w}=simpsonquadratureweights[100,-1,1 ] ;    getmeanlef=    compile[{{z , _ real , 1 } ,             { w , _ real , 1},{x , _ real , 1},{f , _ real , 1 } } ,             plus@@(x f)/plus@@f ] ;              meanlef[z_]:=    with[{f = plus@@w e^(loglma1f[#1,z]&/@x ) } ,      getmeanlef[z , w , x , f ] ] ;                     ....    our tests indicate acceptable accuracy and about a @xmath123 improvement in speed as compared with _ mathematica",
    "_ s more sophisticated * nintegrate * function .      using the _ mathematica _ algorithms for the mle  and mele  derived above , we determined @xmath124 confidence intervals for @xmath125 and @xmath126 based on @xmath127 simulations for each of the @xmath128 parameter values @xmath129 . figures  [ rma1n50mele ] and [ pmcma1n50mele ] show that the mele  dominates except for the cases @xmath130 .",
    "we can safely conclude that the mele  is a better overall estimator than the mle .",
    "of course , as already pointed out another cogent reason for preferring the mele  to the mle  is that it does not produce noninvertible models .",
    "if prior information is available then even better results can be obtained .",
    "marriott and newbold ( 1998 ) have developed an ingenious approach to the unit root problem in time series by noting this fact .",
    "the simulations were repeated with the mean @xmath58 estimated by the sample average and there was no major change is results .",
    "the reader may like compare the estimators for other values of @xmath42 using the _ mathematica _ functions available in the electronic supplement .    in the standard bayesian analysis of the ma(1 ) model the prior",
    "is given by ( box and jenkins , 1976 , p. 250258 ) @xmath131 the computations were repeated using this prior and as shown in figures  [ rma1n50bayes ] and [ pmcma1n50bayes ] the bayes estimate with a jeffrey s prior performs about the same as the mele .",
    "previously copas ( 1966 ) found that for ar(1 ) models , the mele  had lower mseover much of the parameter region .",
    "our results show that for the ma(1 ) the improvement is even somewhat better .",
    "the mse  is lower over a broader range and the piling - up effect on the mle  is avoided .",
    "quenneville ( 1993 ) investigated the small sample properties of the mele  for many other time series models and gave a general algorithm for the mele  in arma models and found that in many cases the mele  produced estimates with smaller mse  over most of the parameter region .",
    "this work is further extended to state space prediction in quenneville and singh ( 1999 ) .",
    "we would also like to mention that in our opinion _ mathematica _ provides an excellent and indeed unparalled environment for many types of fundamental mathematical statistical research . in comparison ,",
    "no other computing environment provides such high quality capabilities _ simultaneously _ in : symbolics , numerics , graphics , typesetting and programming .",
    "the importance of a powerful user - oriented programming language for researchers is sometimes lacking in other environments .",
    "stephan wolfram once said that in his opinion the apl computing language had many good ideas in this direction and that _ mathematica _ has incorporated all these capabilities and much more .",
    "a partial check on this is given in mcleod ( 1999 ) where it was found that most apl idioms could be more clearly expressed in _",
    "however , for applied statistics and data analysis , splus may still be advantageous due to the wide usage by researchers and the high quality functions for advanced statistical methods that are available with splus and in the associated infrastructure . from the educational viewpoint",
    "though this advantage may not be so important since many students and researchers like to understand the principles involved and with _ mathematica _ it is as easy to write out the necessary functions in _ mathematica _ notation as it would be to explain the procedures in a traditional mathematical notation .",
    "1 .   andrews , d.f .",
    "( 1997 ) asymptotic expansions of moments and cumulants . _ technical report_. 2 .",
    "andrews , d.f . and stafford , j.e .",
    "( 1993 ) tools for the symbolic computation of asymptotic expansions .",
    "_ journal of the royal statistical society , b 55 _ , 613627 .",
    "3 .   barnard , g.a . , jenkins , g.m . and",
    "winsten , c.b .",
    "( 1960 ) likelihood inference and time series _ journal of the royal statistical society series _ a 125 , 321372 .",
    "borowski , e.j . and borwein , j.m .",
    "( 1991 ) _ the harpercollins dictionary of mathematics_. new york : harpercollins . 5 .",
    "box , g.e.p . and jenkins , g.m .",
    "( 1976 ) _ time series analysis : forecasting and control_. san francisco : holden - day",
    ".   box , g.e.p . and tiao , g.c .",
    "( 1973 ) _ bayesian inference in statistical analysis_. reading : addison - wesley . 7 .",
    "brockwell , p.j . and davis , r.a .",
    "( 1991 ) _ time series theory and methods_. new york : springer - verlag .",
    "cabrera , j. f. ( 1989 ) some experiments with maximum likelihood estimation using symbolic manipulations .",
    "_ proceedings of the 21st symposium on the interface of statistics and computer science _ , edited by k. berk .",
    "copas , j. b. ( 1966 ) monte carlo results for estimation in a stable markov time series .",
    "_ journal of the royal statistical society , a 129 _ , 110116 . 10 .",
    "cryer , j.d . and",
    "ledolter , j. ( 1981 ) small - sample properties of the maximum likelihood estimator in the first - order moving average model .",
    "_ biometrika , 68 _ , 691694 .",
    "currie , i.d .",
    "( 1995 ) maximum likelihood estimation and mathematica .",
    "it applied statistics , 44 , 379394 .",
    "dempster , a.p .",
    "( 1998 ) logicist statistics i. models and modeling . _ statistical science _ 13 , 248276 . 13 .",
    "johnson , n.i . and kotz , s. ( 1972 ) it distributions in statistics : discrete distributions .",
    "new york : wiley .",
    "keating , j.p . ,",
    "mason , r.l . and pranab , k.s .",
    "( 1993 ) _ pitman s measure of closeness_. philadelphia : siam . 15 .",
    "newbold , p. ( 1974 ) the exact likelihood function for a mixed autoregressive - moving average process .",
    "_ biometrika , 61 _ , 423426 .",
    "mcleod , a.i .",
    "( 1999 ) _ apl_-idioms in _",
    "mathematica_. http://www.stats.uwo.ca/mcleod/epubs/idioms .",
    "mcleod , a.i . and quenneville , b. ( 1999 ) _ mathematica _ notebooks to accompany mean likelihood estimation .",
    "marriott , j. and newbold , p. ( 1998 ) bayesian comparison of arima and stationary arma models , _ international statistical review _ 66 , 323336 .",
    "pitman , e.j.g .",
    "( 1937 ) the closest estimates of statistical parameters , _ proceedings of the cambridge philosophical society _ 33 , 212222 . _ biometrika _ 30 , 391421 . 20 .",
    "pitman , e.j.g .",
    "( 1938 ) the estimation of the location and scale parameters of a continuous population of any given form , _ biometrika _ 30 , 391421 . 21 .",
    "quenneville , b. ( 1993 ) _ mean likelihood estimation and time series analysis_. ph.d .",
    "thesis , university of western ontario . 22 .",
    "quenneville , b. and singh , a.c . ( 1999 ) bayesian prediction mse for state space models with estimated parameters",
    ". _ journal of time series analysis _",
    "( to appear ) . 23 .",
    "ritov , y. ( 1990 ) asymptotic efficient estimators of the change point with unknown distributions . _",
    "the annals of statistics _ 18 , 18291839",
    "rubin , h. and song , k.s .",
    "( 1995 ) exact computation of the asymptotic efficiency of the maximum likelihood estimators of a discontinuous signal in a gaussian white noise .",
    "_ the annals of statistics _ 23 , 732739",
    "stafford , j.e .",
    "and andrews , d.f .",
    "( 1993 ) a symbolic algorithm for studying adjustments to the profile likelihood .",
    "_ biometrika , 80 _ , 715730 .",
    "stafford , j.e . ,",
    "andrews , d.f . and",
    "wang , y. ( 1994 ) symbolic computation : a unified approach to studying likelihood .",
    "_ statistics and computing , 4 _ , 235245 .",
    "tanner , m.a .",
    "_ tools for statistical inference_. new york : springer - verlag .",
    "wolfram , s. ( 1996 ) _",
    "mathematica_. champaign : wolfram research inc ."
  ],
  "abstract_text": [
    "<S> the use of _ mathematica _ in deriving mean likelihood estimators is discussed . </S>",
    "<S> comparisons between the maximum likelihood estimator , the mean likelihood estimator and the bayes estimate based on a jeffrey s noninformative prior using the criteria mean - square error and pitman measure of closeness . based on these criteria </S>",
    "<S> we find that for the first - order moving - average time series model , the mean likelihood estimator outperforms the maximum likelihood estimator and the bayes estimator with a jeffrey s noninformative prior .    </S>",
    "<S> _ mathematica _ was used for symbolic and numeric computations as well as for the graphical display of results . </S>",
    "<S> a _ mathematica _ </S>",
    "<S> notebook is available which provides supplementary derivations and code from http://www.stats.uwo.ca/mcleod/epubs/mele . </S>",
    "<S> the interested reader can easily reproduce or extend any of the results in this paper using this supplement .    </S>",
    "<S> * keywords : * binomial distribution ; efficient likelihood computation ; exponential distribution ; first - order moving - average time series model ; mean square error criterion ; pitman measure of closeness </S>"
  ]
}