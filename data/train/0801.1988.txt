{
  "article_text": [
    "it is well known that the cross entropy method ( cem ) has @xcite similarities to many other selection based methods , such as genetic algorithms , estimation - f - distribution algorithms , ant colony optimization , and maximum likelihood parameter estimation . in this paper",
    "we provide two online variants of the basic cem .",
    "the online variants reveal similarities to several other optimization methods like stochastic gradient or simulated annealing . however , it is not our aim to analyze the similarities and differences between these methods , nor to argue that one method is superior to the other .",
    "here we provide asymptotic convergence results for the new ce variants , which are online .",
    "the cross - entropy method is shown in figure [ alg : cem ] . for an explanation of the algorithm and its derivation ,",
    "see e.g. @xcite .",
    "extensions of the method allow various generalizations , e.g. , decreasing @xmath0 , varying population size , added noise etc . in this paper",
    "we restrict our attention to the basic algorithm .    ....       % inputs :       % population size $ n$       % selection ratio $ \\rho$       % smoothing factor $ \\alpha$       % number of iterations $ t$         $ \\b p_0 : = $ initial distribution parameters       for $ t$ from 0 to $ t-1 $ ,          % draw $ n$ samples and evaluate them          for $ i$ from 1 to $ n$ ,              draw $ \\bx^{(i)}$ from distribution $ g(\\b p_t)$              $ f_i : = f(\\bx^{(i)})$          sort $ \\ { ( \\bx^{(i ) } , f_i ) \\}$ in descending order w.r.t .",
    "$ f_i$          % compute new elite threshold level          $ \\gamma_{t+1 } : = f_{\\lceil\\rho\\cdot n\\rceil}$          % get elite samples          $ e_{t+1 } : = \\ { \\bx^{(i ) } \\mid f_i \\geq \\gamma_{t+1 } \\}$          $ \\b p ' : = \\textrm{cebatchupdate}(e_{t+1 } , \\b p_t , \\alpha)$       end loop ....      consider the following problem :    _ the combinatorial optimization task_. let @xmath1 , @xmath2 and @xmath3 . find a vector @xmath4 such that @xmath5 .",
    "to apply the ce method to this problem , let the distribution @xmath6 be the product of @xmath7 independent bernoulli distributions with parameter vector @xmath8^n$ ] and set the initial parameter vector to @xmath9 . for bernoulli distributions ,",
    "the parameter update is done by the following simple procedure :    ....       procedure $ \\b p_{t+1 } : = \\textrm{cebatchupdate}(e , \\b p_t , \\alpha)$       % $ e$ : set of elite samples       % $ \\b p_t$ : current parameter vector       % $ \\alpha$ : smoothing factor         $ n_b = \\lceil \\rho \\cdot n \\rceil$       $ \\b p ' : = \\bigl(\\sum_{\\bx \\in e } \\bx \\bigr)/ n_b$       $ \\b p_{t+1 } : = ( 1-\\alpha ) \\cdot \\b p_t + \\alpha \\cdot \\b p'$ ....      the algorithm performs batch updates , the sampling distribution is updated once after drawing and evaluating @xmath10 samples .",
    "we shall transform this algorithm into an online one .",
    "batch processing is used in two steps of the algorithm :    * in the update of the distribution @xmath11 , and * when the elite threshold is computed ( which includes the sorting of the @xmath10 samples of the last episode ) .    as a first step ,",
    "note that the contribution of a single sample in the distribution update is @xmath12 , if the sample is contained in the elite set and zero otherwise .",
    "we can perform this update immediately after generating the sample , provided that we know whether it is an elite sample or not . to decide this",
    ", we have to wait until the end of the episode .",
    "however , with a small modification we can get an answer immediately : we can check whether the new sample is among the best @xmath13-percentile of the _",
    "last @xmath10 samples_. this corresponds to a sliding window of length @xmath10 .",
    "algorithmically , we can implement this as a queue @xmath14 with at most @xmath10 elements .",
    "the algorithm is summarized in figure [ alg : cem_online ] .    ....",
    "% inputs :       % window size $ n$       % selection ratio $ \\rho$       % smoothing factor $ \\alpha$       % number of samples $ k$         $ \\b p_0 : = $ initial distribution parameters",
    "$ q:=\\{\\}$       for $ t$ from 0 to $ k-1 $ ,          % draw one samples and evaluate it          draw $ \\bx^{(t)}$ from distribution $ g(p_t)$          $ f_t : = f(\\bx^{(t)})$          % add sample to queue          $ q : = q \\cup \\{(t,\\bx^{(t)},f_t)\\}$          if lengthof($q$)$>n$ , % no updates until we have collected $ n$ samples              delete oldest element of $ q$              % compute new elite threshold level              $ \\{f'_t\\ } : = $ sort $ f$-values in $ q$ in descending order              $ \\gamma_{t+1 } : = f'_{\\lceil\\rho\\cdot n\\rceil}$              if $ f(\\bx^{(t ) } ) \\geq \\gamma_{t+1}$ then                  % $ \\bx^{(t)}$ is an elite sample                  $ \\b p_{t+1 } : = \\textrm{ceonlineupdate}(\\bx^{(t ) } , \\b p_t , \\alpha/\\lceil\\rho\\cdot n\\rceil ) $              endif          endif       end loop ....    for bernoulli distributions , the parameter update is done by the simple procedure shown in fig [ alg : online_update ] .    ....",
    "procedure $ \\b p_{t+1 } : = \\textrm{ceonlineupdate}(\\bx , \\b p_t , \\alpha_1)$       % $ \\bx$ : elite sample       % $ \\b p_t$ : current parameter vector       % $ \\alpha_1 $ : stepsize         $ \\b p_{t+1 } : = ( 1-\\alpha_1 ) \\cdot \\b p_t + \\alpha_1 \\cdot \\bx$ ....    note that the behavior of this modified algorithm is slightly different from the batch version , as the following example highlights : suppose that the population size is @xmath15 , and we have just drawn the 114th sample . in the batch version , we will check whether this sample belongs to the elite of the set @xmath16 ( after all of these samples are known ) , while in the online version , it is checked against the set @xmath17 ( which is known immediately ) .      the sliding window online cem algorithm ( fig .",
    "[ alg : cem_online ] ) is fully incremental in the sense that each sample is processed immediately , and the per - sample processing time does not increase with increasing @xmath18 .",
    "however , processing time ( and required memory ) does depend on the size of the sliding window @xmath10 : in order to determine the elite threshold level @xmath19 , we have to store the last @xmath10 samples and sort them .",
    "if insertion sort is used : in each step , there is only one new element to be inserted into the sorted queue . ] in some applications ( for example , when a connectionist implementation is sought for ) , this requirement is not desirable .",
    "we shall simplify the algorithm further , so that both memory requirement and processing time is constant .",
    "this simplification will come at a cost : the performance of the new variant will depend on the range and distribution of the sample values .",
    "consider now the sample at position @xmath20 , the value of which determines the threshold .",
    "the key observation is that its position can not change arbitrarily in a single step .",
    "first of all , there is a small chance that it will be removed from the queue as the oldest sample .",
    "neglecting this small - probability event , the position of the threshold sample can either jump up or down one place or remain unchanged .",
    "more precisely , there are four possible cases , depending on ( 1 ) whether the new sample belongs to the elite and ( 2 ) whether the sample that just drops out of the queue belonged to the elite    1 .",
    "both the new sample and the dropout sample are elite .",
    "the threshold position remains unchanged .",
    "so does the threshold level except with a small probability when the new or the dropout sample were exactly at the boundary .",
    "we will ignore this small - probability event .",
    "the new sample is elite but the dropout sample is not .",
    "the threshold level increases to @xmath21 ( ignoring a low - probability event ) 3 .",
    "neither the new sample nor the dropout sample are elite .",
    "the threshold remains unchanged ( with high probability ) .",
    "the new sample is not elite but the dropout sample is .",
    "the threshold level decreases to @xmath22 .",
    "let @xmath23 denote the @xmath24-algebra generated by knowing all random outcomes up to time step @xmath18 .",
    "assuming that the positions of the new sample and the dropout sample are distributed uniformly , we get that @xmath25 where we introduced the notation @xmath26 . similarly , @xmath27 using the approximation that @xmath28 .",
    "@xmath29 can drift as @xmath18 grows , and its exact value can not be computed without storing the @xmath30-values .",
    "therefore , we have to use some approximation .",
    "we present three possibilities :    1 .",
    "use a constant stepsize @xmath31 .",
    "clearly , this approximation works best if the distribution of @xmath30-value differences does not change much during the optimization process .",
    "2 .   assume that function values are distributed uniformly over an interval @xmath32 $ ] .",
    "in this case , @xmath33 . on the other hand ,",
    "let @xmath34 .",
    "@xmath35 and @xmath36 are independent , uniformly distributed samples , so we obtain @xmath37 , i.e. , @xmath38 with @xmath39 . from this , we can obtain an online approximation scheme @xmath40 where @xmath41 is an exponential forgetting parameter .",
    "3 .   assume that function values have a normal distribution @xmath42 .",
    "in this case , @xmath43 , where @xmath44 is the gaussian error function . on the other hand ,",
    "let @xmath45 .",
    "@xmath35 and @xmath36 are independent , normally distributed samples , so we obtain @xmath46 , i.e. , @xmath47 with @xmath48 . from this , we can obtain an online approximation scheme @xmath49 where @xmath41 is an exponential forgetting parameter .",
    "4 .   we can obtain a similar approximation for many other distributions @xmath30 , but the constant @xmath50 does not necessarily have an easy - to - compute form .",
    "the resulting algorithm using option ( 1 ) is summarized in fig .",
    "[ alg : cem_memoryless ] .    ....",
    "% inputs :       % window size $ n$       % selection ratio $ \\rho$       % smoothing factor $ \\alpha$       % number of samples $ k$         $ \\b p_0 : = $ initial distribution parameters       $ \\gamma_0 : = $ arbitrary       for $ t$ from 0 to $ k-1 $ ,          % draw one samples and evaluate it          draw $ \\bx^{(t)}$ from distribution $ g(\\b p_i)$          if $ f(\\bx^{(t ) } ) \\geq \\gamma_{t}$ then              % $ x^{(t)}$ is an elite sample              % compute new elite threshold level              $ \\gamma_{t+1 } : = \\gamma_t + ( 1-\\rho)\\cdot \\delta$              $ \\b p_{t+1 } : = \\textrm{ceonlineupdate}(\\bx^{(t ) } , \\b p_t , \\alpha/(\\rho\\cdot n ) ) $          else              % compute new elite threshold level              $ \\gamma_{t+1 } : = \\gamma_t - \\rho \\cdot \\delta$          endif          % optional step : update $ \\delta$          % $ \\delta : = ( 1-\\beta ) \\delta + \\beta \\cdot \\delta_0 \\bigl| f(\\bx^{(t ) } ) - f(\\bx^{(t-1 ) } ) \\bigr|$       end loop ....",
    "in this section we show that despite the various approximations used , the three variants of the ce method possess the same asymptotical convergence properties .",
    "naturally , the actual performance of these algorithms may differ from each other .",
    "if the basic ce method is used for combinatorial optimization with smoothing factor @xmath0 , @xmath51 and @xmath52 for each @xmath53 , then @xmath54 converges to a 0/1 vector with probability 1 .",
    "the probability that the optimal probability is generated during the process can be made arbitrarily close to 1 if @xmath0 is sufficiently small .",
    "the statements of the theorem are rather weak , and are not specific to the particular form of the algorithm : basically they state that ( 1 ) the algorithm is a `` trapped random walk '' : the probabilities may change up an down , but eventually they converge to either one of the two absorbing values , 0 or 1 ; and ( 2 ) if the random walk can last for a sufficiently long time , then the optimal solution is sampled with high probability .",
    "we shall transfer the proof to the other two algorithms below .",
    "if either variant of the online ce method is used for combinatorial optimization with smoothing factor @xmath0 , @xmath51 and @xmath52 for each @xmath55 , then @xmath54 converges to a 0/1 vector with probability 1 .",
    "the probability that the optimal probability is generated during the process can be made arbitrarily close to 1 if @xmath0 is sufficiently small .",
    "the proof follows closely the proof of theorems 1 - 3 in @xcite .",
    "we begin with introducing several notations .",
    "let @xmath56 denote the optimum solution , let @xmath23 denote the @xmath24-algebra generated by knowing all random outcomes up to time step @xmath18 .",
    "let @xmath57 the probability that the optimal solution is generated at time @xmath18 and @xmath58 the probability that component @xmath59 is identical to that of the optimal solution .",
    "clearly , @xmath60 and @xmath61 .",
    "let @xmath62 and @xmath63 denote the minimum and maximum possible value of @xmath64 , respectively . in each step of the algorithms ,",
    "@xmath64 is either left unchanged or modified with stepsize @xmath65 .",
    "consequently , @xmath66 and @xmath67 using these quantities , @xmath68    let @xmath69 denote the event that the optimal solution was not generated up to time @xmath18 .",
    "let @xmath70 denote the set of possible values of @xmath71 . clearly , for all @xmath72 , @xmath73 .",
    "note also that @xmath74 by the construction of the random sampling procedure of ce .",
    "then @xmath75 using this , we can estimate the probability that the optimum solution has not been generated up to time step @xmath76 : @xmath77 using the fact that @xmath78 , we obtain @xmath79 let @xmath80 with this notation , @xmath81 however , @xmath82 as @xmath83 , so @xmath84 can be made arbitrarily close to zero , if @xmath85 is sufficiently small .    to prove the second part of the theorem , define @xmath86 . for the sake of notational convenience , we fix a component @xmath59 and omit it from the indices . note that @xmath87 if and only if @xmath88 is considered an elite sample .",
    "clearly , if @xmath88 is not elite , then no probability update is made .",
    "on the other hand , an update modifies @xmath89 towards either 0 or 1 .",
    "since @xmath90 with no equality allowed , this update will change the probabilities indeed .",
    "consider the subset of time indices when probabilities are updated , @xmath91 .",
    "we need to show that @xmath92 .",
    "this is the only part of the proof where there is a slight extra work compared to the proof of the batch variant .",
    "we will show that each unbroken sequence of zeros in @xmath93 is finite with probability 1 .",
    "consider such a 0-sequence that starts at time @xmath94 , and suppose that it is infinite .",
    "then , the sampling distribution @xmath89 is unchanged for @xmath95 , and so is the distribution @xmath96 of the @xmath30-values .",
    "let us examine the first online variant of the cem .",
    "divide the interval @xmath97 to @xmath98-step long epochs .",
    "the contents of the queue at time step @xmath99 are independent and identically distributed , because ( a ) the samples are generated independently from each other and ( b ) the different queues have no common elements . for a given queue @xmath100 ( with all elements sampled from distribution @xmath96 ) and a new sample @xmath88 ( also from distribution @xmath96 ) , the probability that @xmath88 is not elite is exactly @xmath101 .",
    "therefore the probability that no sample is considered elite for @xmath102 is at most @xmath103 .",
    "the situation is even simpler for the memoryless variant of the online cem : suppose again that no sample is considered elite for @xmath102 , and all samples are drawn from the distribution @xmath96 .",
    "@xmath96 is a distribution over a finite domain , so it has a finite minimum @xmath104 . as all samples are considered non - elite , the elite threshold is decreased by a constant amount @xmath105 in each step , eventually becoming smaller than @xmath104 , which results in a contradiction .      from now on",
    ", the proof continues identically to the original .",
    "we will show that @xmath108 changes signs for a finite number of times with probability 1 . to this end , let @xmath109 be the random iteration number when @xmath110 changes sign for the @xmath111th time . for all @xmath111 ,      from this point on , the proof of theorem 3 in @xcite can be applied without change , showing that the number of sign changes is finite with probability 1 , then proving that this implies convergence to either 0 or 1 ."
  ],
  "abstract_text": [
    "<S> the cross - entropy method @xcite is a simple but efficient method for global optimization . in this paper </S>",
    "<S> we provide two online variants of the basic cem , together with a proof of convergence . </S>"
  ]
}