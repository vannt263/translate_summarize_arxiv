{
  "article_text": [
    "cloud computing is a disruptive it model allowing enterprises to focus on their core business activities . instead of investing in their own it infrastructures , they can now rent ready - to - use preconfigured virtual resources from cloud providers in a `` pay - as - you - go '' manner .",
    "organisations relying on fixed size private infrastructures often realise it can not match their dynamic needs , thus frequently being either under or overutilised .",
    "in contrast , in a cloud environment one can automatically acquire or release resources as they are needed  a distinctive characteristic known as _",
    "autoscaling_.    this is especially important for large scale web applications , since the number of users fluctuates over time and is prone to flash crowds as a result of marketing campaigns and product releases .",
    "most such applications follow the 3-tier architectural pattern and are divided in three standard layers / tiers  @xcite :    * * presentation layer *  the end user interface . * * business / domain layer *  implements the business logic .",
    "hosted in one or several application servers ( as ) .",
    "* * data layer *  manages the persistent data . deployed in one or several database ( db ) servers .",
    "a user interacts with the presentation layer , which redirects the requests to an as which in turn can access the data layer .",
    "the presentation layer is executed on the client s side ( e.g. in a browser ) and thus scalability is not an issue . scaling",
    "the db layer is a notorious challenge , since system architects have to balance between consistency , availability and partition tolerance following the results of the cap theorem  @xcite .",
    "this field has already been well explored ( cattel surveys more than 20 related projects  @xcite ) .",
    "furthermore , google has published about their new database which scales within and across data centres without violating transaction consistency  @xcite .",
    "hence data layer scaling is beyond the scope of our work .    in general , autoscaling the application servers ( as ) is comparatively straightforward . in an infrastructure as a service ( iaas ) cloud environment , the as vms are deployed `` behind '' a load balancer which redirects the incoming requests among them . whenever the servers capacity is insufficient , one or several new as vms are provisioned and associated with the load balancer and the db layer",
    " see figure  [ fig:3tier ] .",
    "_ but what should be the type of the new as vm ? _ most major cloud providers like amazon ec2 and google compute engine offer a predefined set of vm types with different performance capacities and prices",
    ". currently , system engineers `` hardcode '' preselected vm types in the autoscaling rules based on their intuition or at best on historical performance observations .",
    "however , user workload characteristics vary over time leading to constantly evolving as capacity requirements . for example",
    ", the proportion of browsing , bidding and buying requests in an e - commerce system can change significantly during a holiday season , which can change the server utilisation patterns .",
    "middleware and operating system updates and reconfigurations can lead to changes in the utilisation patterns as well  @xcite .",
    "this can also happen as a result of releasing new application features or updates .",
    "moreover , vm performance can vary significantly over time because of other vms collocated on the same physical host causing resource contentions @xcite .",
    "hence even vm instances of the same type can perform very differently . from the viewpoint of the cloud s client this can not be predicted .    to illustrate better ,",
    "let us consider a large scale web application with hundreds of dedicated as vms .",
    "its engineers can analyse historical performance data to specify the most appropriate vm type in the autoscaling rules .",
    "however , they will have to reconsider their choice every time a new feature or a system upgrade is deployed .",
    "they will also have to constantly monitor for workload pattern changes and to react by adjusting the austoscaling rules .",
    "given that vm performance capacities also vary over time , the job of selecting the most suitable vm type becomes practically unmanageable .",
    "this can result in significant financial losses , because of using suboptimal vms .    to address this , the key * contributions * of our work are ( i ) a machine learning approach which continuously learns the application s resource requirements and ( ii ) a dynamic vm type selection ( dvts ) algorithm , which selects a vm type for new as vms .",
    "since both workload specifics and vm performance vary over time , we propose an online approach , which learns the application s behaviour and the typical vm performance capacities in real time .",
    "it relieves system maintainers from having to manually reconfigure the autoscaling rules .",
    "the rest of the paper is organised as follows : in section  [ related_work ] we describe the related works .",
    "section  [ overview ] provides a succinct overview of our approach .",
    "section  [ learning ] discusses the machine learning approaches we employ to `` learn '' the application s requirements in real time .",
    "section  [ selection ] describes how to select an optimal vm type .",
    "section  [ prototype ] details the architecture of our prototype and the benchmark we use for evaluation .",
    "section  [ experiments ] describes our experiments and results .",
    "finally , section  [ conclusion ] concludes and defines pathways for future work .",
    "the area of static computing resource management has been well studied in the context of grids , clouds , and even multi - clouds  @xcite . however , the field of dynamic resource management in response to continuously varying workloads , which is especially important for web facing applications  @xcite , is still in its infancy .",
    "horizontal autoscaling policies are the predominant approach for dynamic resource management and thus they have gained significant attention in recent years .    lorido - botran et al .",
    "classify autoscaling policies as _ reactive _ and _ predictive _ or _ proactive _",
    "the most widely adopted _ reactive _ approaches are based on threshold rules for performance metrics ( e.g. cpu and ram utilisation ) . for each such characteristic",
    "the system administrator provides a lower and upper threshold values .",
    "resources are provisioned whenever an upper threshold is exceeded . similarly ,",
    "if a lower threshold is reached resources are released .",
    "how much resources are acquired or released when a threshold is reached is specified in user defined autoscaling rules .",
    "there are different `` flavours '' of threshold based approaches .",
    "for example in amazon auto scaling  @xcite one would typically use the average metrics from the virtual server farm , while rightscale  @xcite provides a voting scheme , where thresholds are considered per vm and an autoscaling action is taken if the majority of the vms `` agree '' on it .",
    "combinations and extensions of both of these techniques have also been proposed @xcite .",
    "_ predictive _ or _ proactive _ approaches try to predict demand changes in order to allocate or deallocate resources .",
    "multiple methods using approaches like reinforcement learning  @xcite , queuing theory  @xcite and kalman filters  @xcite to name a few have been proposed .",
    "our work is complementary to all these approaches .",
    "they indicate at what time resources should be provisioned , but do not select the resource type .",
    "our approach selects the best resource ( i.e. vm type ) once it has been decided that the system should scale up horizontally .",
    "fernandez et al .",
    "propose a system for autoscaling web applications in clouds  @xcite .",
    "they monitor the performance of different vm types to infer their capacities .",
    "our approach to this is different , as we inspect the available to each vm cpu capacity and measure the amount of `` stolen '' cpu instructions by the hypervisor from within the vm itself .",
    "this allows us to normalise the vms resource capacities to a common scale , which we use to compare them and for further analysis .",
    "furthermore , their approach relies on a workload predictor , while ours is usable even in the case of purely reactive autoscaling .",
    "singh et al .",
    "use k - means clustering to analyse the workload mix ( i.e. the different type of sessions ) and then use a queueing model to determine each server s suitability  @xcite .",
    "however , they do not consider the performance variability of virtual machines , which we take into account . also , they do not select the type of resource ( e.g. vm ) to provision and assume there is only one type , while this is precisely the focus of our work",
    ".    a part of our work is concerned with automated detection of application behaviour changes through a hierarchical temporal memory ( htm ) model .",
    "similar work has been carried out by cherkasova et al .",
    "@xcite , who propose a regression based anomaly detection approach . however , they analyse only the cpu utilisation .",
    "moreover they consider that a set of user transactions types is known beforehand .",
    "in contrast , our approach considers ram as well and does not require application specific information like transaction types .",
    "tan et al .",
    "propose the prepare performance anomaly detection system  @xcite .",
    "however , their approach can not be used by a cloud client , as it is built on top of the xen virtual machine manager to which external clients have no access .",
    "another part of our method is concerned with automatic selection of the _ learning rate _ and _ momentum _ of an artificial neural network ( ann ) .",
    "there is a significant amount of literature in this area as surveyed by moreira and fiesler  @xcite .",
    "however , the works they overview are applicable for static data sets and have not been applied to learning from streaming online data whose patterns can vary over time .",
    "moreover , they only consider how the intermediate parameters of the backpropagation algorithm vary and do not use additional domain specific logic .",
    "although our approach is inspired by the work of vogl et al .",
    "@xcite as it modifies the _ learning rate _ and _ momentum _ based on the prediction error , we go further and we modify them also based on the _ anomaly score _ as reported by the hierarchical temporal memory ( htm ) models .",
    "figure  [ fig : overview ] depicts an overview of our machine learning approach and how the system components interact . within each as",
    "vm we install a monitoring program which periodically records utilisation metrics .",
    "these measurements are transferred to an _ autoscaling component _ , which can be hosted either in a cloud vm or on - premises .",
    "it is responsible for ( i ) monitoring as vms performance ( ii ) updating machine learning models of the application behaviour and ( iii ) autoscaling .    within each as vm the _ utilisation monitors _ report statistics about the cpu , ram , disk and network card utilisations and the number of currently served users .",
    "these records are transferred every 5 seconds to the _ autoscaling _ component , where they are normalised , as different vms have different de facto resource capacities . in the machine learning approaches",
    "we only consider the cpu and ram utilisations , as disk and network utilisations of as vms are typically small  @xcite . for each as vm the _ autoscaler _ maintains a separate single - region hierarchical temporal memory ( htm ) model  @xcite , which is overviewed in a later section .",
    "in essence we use htms to detect changes in the application behaviour of each as vm .",
    "we prefer htm to other regression based anomaly detection approaches , as it can detect anomalies on a stream of multiple parameters ( e.g. cpu and ram ) .",
    "whenever monitoring data is retrieved from an as vm , the _ autoscaler _ trains its htm with the received number of users , cpu and ram utilisations and outputs an _ anomaly score _ defining how `` unexpected '' the data is .    as a next step we use these utilisation measurements to train a 3-tier artificial neural network ( ann ) about the relationship between the number of served users and resource consumptions .",
    "we choose to use an ann because of its suitability for online data streams .",
    "other `` sliding window '' approaches operate only on a portion of the data stream . as",
    "a system s utilisation patterns can remain the same for long time intervals , the window sizes may need to become impractically large or even be dynamically adjusted . on the contrary , an ann does not operate on a fixed time window and is more adept with changes in the incoming data stream , as we will detail in a later section .",
    "there is only one ann and training samples from all as vms are used to train it .",
    "in essence the ann represents a continuously updated regression model , which given a number of users predicts the needed resources to serve them within a single vm without causing resource contentions .",
    "thus , we need to filter all training samples , which were taken during anomalous conditions ( e.g. insufficient cpu or ram capacity causing intensive context switching or disk swapping respectively ) . such samples are not indicative of the relationship between number of users and the resource requirements in the absence of resource contentions .",
    "furthermore , we use the _ anomaly score _ of each training sample ( extracted from htm ) to determine the respective _ learning speed _ and _ momentum _ parameters of the back propagation algorithm so that the ann adapts quickly to changes in the utilisation patterns .    training the ann and the htms happens online from the stream of vm measurements in parallel with the running application . simultaneously we also maintain a _ vm capacity repository _ of the latest vm capacity measurements .",
    "when a new vm is needed by the autoscaling component , we use this repository to infer the potential performance capacity of all vm types . at that time",
    "the ann is already trained adequately and given the predicted performance capacities can be used to infer how many users each vm type could serve simultaneously . based on",
    "that we select the vm type , with minimal cost to number of users ratio .",
    "to measure vm performance utilisation , we use the _ sar _ , _ mpstat _ , _ vmstat _ and _ netstat _ linux monitoring tools .",
    "we use the mpstat  _ % idle _ metric to measure the percentage of time during which the cpu was idle .",
    "the _ % steal _ metric describes the percentage of `` stolen '' cpu cycles by a hypervisor ( i.e. the proportion of time the cpu was not available to the vm ) and can be used to evaluate the actual vm cpu capacity .",
    "similarly , sar provides the _ % util _ and _ % ifutil _ metrics as indicative of the disk s and network card s utilisations .    measuring the ram utilisation is more complex as operating systems keep in memory cached copies of recently accessed disk sectors in order to reduce disk access  @xcite .",
    "although in general this optimisation is essential for vm performance , web application servers ( as ) are not usually i / o bound , as most of the application persistence is delegated to the data base layer . hence , using the _ vmstat _ ram utilisation metrics can be an overestimation of the actual memory consumption as it includes rarely accessed disk caches .",
    "thus , we use the _ `` active memory '' _ _ vmstat _ metric to measure memory consumption instead . it denotes the amount of recently used memory , which is unlikely to be claimed for other purposes .",
    "lastly , we need to evaluate the number of concurrently served users in an as vm .",
    "this could be extracted from the as middleware , but that would mean writing specific code for each type of middleware . moreover",
    ", some proprietary solutions may not expose this information .",
    "therefore , we use the number of distinct ip addresses with which the server has an active tcp socket , which can be obtained through the _ netstat _ command . typically ,",
    "the as vm is dedicated to running the as and does not have other outgoing connections except for the connection to the persistence layer .",
    "therefore , the number of addresses with active tcp sockets is a good measure of the number of currently served users .      before proceeding to train the machine learning approaches",
    ", we need to normalise the measurements which have different `` scales '' , as the vms have different ram sizes and cpus with different frequencies . moreover , the actual cpu capacities within a single vm vary over time as a result of the dynamic collocation of other vms on the same host .    as a first step in normalising the cpu load , we need to evaluate the actual cpu capacity available to each vm .",
    "this can be extracted from the _ /proc",
    "/ cpuinfo _ linux kernel file . if the vm has @xmath0 cores , _ /proc",
    "/ cpuinfo _ will list meta information about the physical cpu cores serving the vm including their frequencies @xmath1 .",
    "the sum of these frequencies is the maximal processing capacity the vm can get , provided the hypervisor does not `` steal '' any processing time . using the _",
    "% steal _ mpstat parameter we can actually see what percentage of cpu operations have been taken away by the hypervisor .",
    "subtracting this percentage from the sum of frequencies gives us the actual vm cpu capacity at the time of measurement .",
    "to normalise we further divide by the maximal cpu core frequency @xmath2 multiplied by the maximal number of cores @xmath3 of all considered vms in the cloud provider .",
    "this is a measure of the maximal vm cpu capacity one can obtain from the considered vm types . as clouds are made of commodity hardware , we will consider @xmath4 .",
    "this ensures that all values are in the range @xmath5 $ ] , although for some cloud providers all values may be much lower than 1 , depending on the underlying hardware they use .",
    "this is formalised in eq .",
    "[ eq : cpucapacity ] .",
    "@xmath6    having computed the vm cpu capacity , we store it into the _ vm capacity repository _ , so we can use it later on to infer the capacities of future vms .",
    "each repository record has the following fields :    * _ time _ - a time stamp of the capacity estimation ; * _ vm - type _ - an identifier of the vm type - e.g. `` m1.small '' ; * _ vm - id _ - a unique identifier of the vm instance - e.g. its ip or elastic dns address ; * _ cpucapacitynorm _ - the computed cpu capacity .",
    "if we further subtract the _",
    "% idle _ percentage from the capacity we will get the actual cpu load given in eq .  [ eq : cpunorm ] .",
    "@xmath7    normalising the ram load and capacity is easier , as they do not fluctuate like the cpu capacity .",
    "we divide the _ active memory _ by the maximal amount of memory @xmath8 in all considered virtual machine types in the cloud - see eq .",
    "[ eq : ramnorm ] .",
    "@xmath9    whenever a new as vm is needed , we have to estimate the cpu and ram capacities of all available vm types based on the _ capacity repository _ and their performance definitions provided by the provider .",
    "the normalised ram capacity of a vm type is straightforward to estimate as we just need to divide the capacity in the provider s specification by @xmath8 . to estimate the cpu capacity of a vm type we use the mean of the last 10 entries capacities for this type in the _ capacity repository_.",
    "if there are no entries for this vm type in the repository ( i.e. no vm of this type has been instantiated ) we can heuristically extrapolate the cpu capacity from the capacities of the other vm types .",
    "typically iaas providers specify an estimation of each vm type s cpu capacity - e.g. google compute engine units ( gceu ) in google compute engine or elastic compute units ( ecu ) in aws .",
    "hence given an unknown vm type @xmath10 we can extrapolate its normalised cpu capacity as :    @xmath11    where @xmath12 is the set of vm types present in the _ capacity repository _ and whose cpu capacity can be determined from previous measurements , @xmath13 is its cardinality , and @xmath14 defines the cloud provider s estimation of a vm type s capacity - e.g. number of gceus or ecus .",
    "the hierarchical temporal memory ( htm ) model is inspired by the structure and organisation of the neocortex .",
    "it has been developed and commercialised by the grok company  @xcite ( formerly numenta  @xcite ) , and follows the concepts from jeff hawkins book `` on intelligence ''  @xcite .",
    "the model creators build upon the seminal work of mountcastle  @xcite that the neocortex is predominantly uniform in structure and function even in regions handling different sensory inputs - e.g. visual , auditory , and touch .",
    "the htm model tries to mimic this structure in a computational model .",
    "there are several differences compared to the biological structure of the neocortex in order to be computationally viable as described in the implementation white paper  @xcite .",
    "grok s implementation is available as an open source project called nupic  @xcite . in this section ,",
    "we provide only a brief overview of htm to introduce the reader to this concept .",
    "the interested reader is referred to the official documentation  @xcite .",
    "htms consist of one or several stacked regions . during inference",
    ", input arrives into the lowest region , whose output serves as input to the successive one and so forth until the topmost region outputs the final result .",
    "the purpose of a region is to convert noisy input sequences to more stable abstract representations .",
    "conceptually , the different regions represent different levels of abstraction in the learning process - i.e. the lowest level recognises low - level patterns , while each higher level layer recognises more complex ones based on the result of the previous one . in this work ,",
    "we use single - region htms and we will focus on them in the rest of the section .",
    "a htm region consists of columns of cells , which are most often arranged in a three dimensional grid - see figure  [ fig : htm ] .",
    "each cell can be in one of three possible states : ( i ) active form feed forward input , ( ii ) active from lateral input ( i.e. predicted ) , or ( iii ) inactive .",
    "conceptually , active cells represent the state of the last input and predicted cells represent the likely state after future inputs .",
    "a htm region receives as input a bit sequence .",
    "special _ encoders _ are used to convert input objects into bitwise representations , so that objects which are `` close '' in the sense of the target domain have similar bit representations . upon receiving new binary input",
    "the htm changes the states of the columns based on several rules summarised below .    as a first step the htm has to decide",
    "which columns cells will be activated for a given input - an algorithm known as _",
    "spatial pooling_. it nullifies most of the 1 bits , so that only a small percentage ( by default 2% ) are active .",
    "each column is connected with a fixed sized ( by default 50% of the input length ) random subset of input bits called the _",
    "potential pool_. each column s connection to an input bit has a ratio number in the range [ 0,1 ] associated with it known as the _ permanence_. htm automatically adjusts the _ permanence _ value of a connection after a new input record arrives , so that input positions whose value have been 0 or 1 and are members of the _ potential pool _ of a selected column are decreased or increased respectively . connections with _ permanences _ above a predefined thresholds are considered active . given an input , for each column the htm defines its _ overlap score _ as the number of active bits with active connections .",
    "having computed this for every column , htm selects a fixed sized ( by default 2% ) set of columns with the highest _ overlap score _ , so that no two columns within a predefined radius are active .    as a second step ,",
    "htm decides which cells within these columns to activate .",
    "this is called _",
    "temporal pooling_. within each of the selected columns the htm activates only the cells which are in _ predicted _ state .",
    "if there are no cells in predicted state within a column , then all of its cells are activated , which is also known as _",
    "bursting_.    next , the htm makes a prediction of what its future state will be - i.e. which cells should be in predicted state .",
    "the main idea is that when a cell activates it establishes connections to the cells which were previously active .",
    "each such connection is assigned a weight number . over time",
    "if the two nodes of a connection become active in sequence again , this connection is strengthened , i.e. the weight is increased .",
    "otherwise , the connection slowly decays , i.e. the weight is gradually decreased .",
    "once a cell becomes active , all non - active cells having connections to it with weights above a certain threshold are assigned the predicted state .",
    "this is analogous to how synapses form and decay between neurons dendrites in the neocortex in response to learning patterns .",
    "the presence of predicted cell columns allows a htm to predict what will be its likely state in terms of active cells after the next input .",
    "however , it also allows for the detection of anomalies .",
    "for example , if just a few predicted states become active this is a sign that the current input has not been expected .",
    "thus the _ anomaly_score _ is defined as the proportion of active spatial pooler columns that were incorrectly predicted and is in the range @xmath15 $ ] .    in our environment",
    "every 5 seconds we feed each htm with a time stamp , the number of users and the cpu and ram utilisations of the respective vm .",
    "we use the standard nupic scalar and date encoders to convert the input to binary input . as a result",
    "we get an _ anomaly score _ denoting how expected the input is , in the light of the previously described algorithms .",
    "figure  [ fig : ann ] depicts the topology of the artificial neural network ( ann ) .",
    "it has one input  the number of users .",
    "the hidden layer has 250 neurons with the sigmoid activation function .",
    "the output layer has two output nodes with linear activation functions , which predict the normalised cpu and ram utilisations within an as vm .",
    "once a vm s measurements are received and normalised and the _ anomaly score _ is computed by the respective htm region , the ann can be trained . as discussed , we need to filter out the vm measurements which are not representative of normal , contention free application execution , in order to `` learn '' the `` right '' relationship between number of users and resource utilisations .",
    "we filter all vm measurements in which the cpu , ram , hard disk or network card utilisations are above a certain threshold ( e.g. 70% ) .",
    "similarly , we filter measurements with negligible load  i.e. less than 25 users or less than 10% cpu utilisation .",
    "we also ignore measurements from periods during which the number of users has changed significantly  e.g. in the beginning of the period there were 100 users and at the end there were 200 .",
    "such performance observations are not indicative of an actual relationship between number of users and resource utilisations .",
    "thus , we ignore measurements for which the number of users is less than 50% or more than 150% of the average of the previous 3 measured numbers of users from the same vm .    since we are training the ann with streaming data , we need to make sure it is not overfitted to the latest training samples .",
    "for example if we have constant workload for a few hours we will be receiving very similar training samples in the ann during this period .",
    "hence the ann can become overfitted for such samples and lose its fitness for the previous ones .",
    "to avoid this problem , we filter out measurements / training samples , which are already well predicted . more specifically , if a vm measurement is already predicted with a _ root mean square error _ ( rmse ) less than @xmath16 it is filtered out and the ann is not trained with it .",
    "we call this value @xmath17 because it is obtained for each training sample before the ann is trained with it .",
    "it is computed as per eq .",
    "[ eq : rmse ] , where @xmath18 and @xmath19 are the values of the output neurons and the expected values respectively .",
    "@xmath20    with each measurement , which is not filtered out , we perform one or several iterations / epochs of the back - propagation algorithm with the number of users as input and the normalised cpu and ram utilisations as expected output .",
    "the back - propagation algorithm has two important parameters  the _ learning rate _ and the _ momentum_. in essence , the _ learning rate _ is a ratio number in the interval @xmath21 which defines the amount of weight update in the direction of the gradient descent for each training sample  @xcite . for each weight update ,",
    "the _ momentum _ term defines what proportion of the previous weight update should be added to it .",
    "it is also a ratio number in the interval @xmath21 . using a _ momentum _",
    "the neural network becomes more resilient to oscillations in the training data by `` damping '' the optimisation procedure  @xcite .",
    "for our training environment we need a low _ learning rate _ and a high _ momentum _ , as there are a lot of oscillations in the incoming vm measurements .",
    "we select the _ learning rate _ to be @xmath22 and the _ momentum _ @xmath23 .",
    "we call these values the _ ideal parameters _ , as these are the values we would like to use once the ann is close to convergence . however , the low _ learning rate _ and high _ momentum _ result in slow convergence in the initial stages , meaning that the ann may not be well trained before it is used . furthermore ,",
    "if the workload pattern changes , the ann may need a large number of training samples and thus time until it is tuned appropriately .",
    "hence the actual _ learning rate _ and _ momentum _ must be defined dynamically .",
    "one approach to resolve this is to start with a high _ learning rate _ and low _ momentum _ and then respectively decrease / increase them to the desired values  @xcite .",
    "this allows the back - propagation algorithm to converge more rapidly during the initial steps of the training .",
    "we define these parameters in the initial stages using the asymptotic properties of the sigmoid function , given in eq .",
    "[ eq : sigmoid ] .",
    "@xmath24    as we need to start with a high _ learning rate _ and then decrease it gradually to @xmath25 , we could define the learning rate @xmath26 for the @xmath27-th training sample as @xmath28 .",
    "however , the sigmoid function decreases too steeply for negative integer parameters and as a result the learning rate is higher than @xmath25 for just a few training samples . to solve this we use the square root of @xmath27 instead and thus our first approximation of the _ learning rate _ is :    @xmath29    as a result @xmath30 gradually decreases as more training samples arrive .",
    "figure  [ fig : lr - mom ] depicts how it changes over time .",
    "we also need to ensure that it increases in case unusual training data signalling a workload change arrives and thus we need to elaborate @xmath30 . for this",
    "we keep a record of the last 10 samples _ anomaly scores _ and errors ( i.e. @xmath17 ) .",
    "the higher the latest anomaly scores , the more `` unexpected '' the samples are and therefore the _ learning rate _ must be increased .",
    "similarly , the higher the sample s @xmath17 compared to the previous errors , the less fit for it the ann is and thus the _ learning rate _ must be increased as well .",
    "thus our second elaborated approximation of the _ learning rate _ is :    @xmath31    where @xmath32 and @xmath33 are the _ anomaly score _ and the error of the @xmath27-th sample and @xmath34 is the average error of the last 10 samples . note that we use the sigmoid function for the anomaly scores in order to diminish the effect of low values .    in some cases the _ learning rate _ can become too big in the initial training iterations , which will in fact hamper the convergence . to overcome this problem , for each sample",
    "@xmath27 we run a training iteration with @xmath35 , compute its rmse @xmath36 and then revert the results of this iteration . by comparing @xmath33 and @xmath36",
    "we can see if training with this @xmath35 will contribute to the convergence  @xcite .",
    "if not , we use the ideal parameter @xmath25 instead .",
    "thus we finally define the _ learning rate _",
    "parameter @xmath26 in eq .",
    "[ eq : lrk ] :    @xmath37    similarly we have to gradually increase the _ momentum _ as we decrease the _ learning rate _ until the ideal _ momentum _ is reached .",
    "if a workload change is present we need to decrease the _ momentum _ in order to increase the learning speed .",
    "hence , we can just use the ratio of the ideal learning rate @xmath25 to the current one as shown in eq .",
    "[ eq : mk ] .",
    "@xmath38    figure  [ fig : lr - mom ] depicts how the _ learning rate _ and _ momentum _ change during the initial training stages , given there are no anomalies , accuracy losses and @xmath39  i.e. when @xmath40 .",
    "figure  [ fig : lr - init ] shows the actual @xmath26 given realistic workload .",
    "furthermore , to speed up convergence it is beneficial to run multiple _ epochs _",
    "( i.e. repeated training iterations ) with the first incoming samples and with samples taken after a workload change .",
    "the ideal _ learning rate _ @xmath25 and its approximation @xmath35 already embody this information and we could simply use their ratio .",
    "however , @xmath41 can easily exceed 300 given @xmath22 , resulting in over - training with particular samples .",
    "hence we take the logarithm of it as in eq .",
    "[ eq : ek ] :    @xmath42",
    "@xmath43 null @xmath44 0    return @xmath45    when a new vm has to be provisioned the ann should be already trained so that we can estimate the relationship between number of users and cpu and ram requirements .",
    "the procedure is formalised in algorithm  [ algo : vmtselect ] .",
    "we loop over all vm types  @xmath46 ( line 3 ) and for each one we estimate its normalised cpu and ram capacity based on the _ capacity repository _ as explained earlier ( lines 5 - 6 ) .",
    "the vm cost per time unit ( e.g. hour in aws or minute in google compute engine ) is obtained from the provider s specification ( line  7 ) .",
    "next we approximate the number of users that a vm of this type is expected to be able to serve ( lines 10 - 18 ) .",
    "we iteratively increase @xmath0 by @xmath47 starting from @xmath48 , which is the minimal number of users we have encountered while training the neural network .",
    "we use the procedure @xmath49 ( defined separately in algorithm  [ algo : utilest ] ) to estimate the normalised cpu and ram demands that each of these values of @xmath0 would cause .",
    "we do so until the cpu or ram demands exceed the capacity of the inspected vm type .",
    "hence , we use the previous value of @xmath0 as an estimation of the number of users a vm of that type can accommodate . finally , we select the vm type with the lowest cost to number of users ratio ( lines 20 - 23 ) .",
    "algorithm  [ algo : utilest ] describes how to predict the normalised utilisations caused by @xmath0 concurrent users .",
    "if @xmath0 is less than the maximum number of users @xmath50 we trained the ann with , then we can just use the ann s prediction ( line 5 ) . however , if @xmath0 is greater than @xmath50 the ann may not predict accurately . for example if we have used a single _ small _ vm to train the ann , and then we try to predict the capacity of a _ large _",
    "vm , @xmath0 can become much larger than the entries of the training data and the regression model may be inaccurate .",
    "thus , we extrapolate the cpu and ram requirements ( lines 7 - 11 ) based on the range of values we trained the ann with and the performance model we have proposed in a previous work  @xcite .",
    "@xmath51 0 @xmath52 0    return @xmath53",
    "there are two main approaches for experimental validation of a distributed system s performance  through a simulation or a prototype .",
    "discrete event simulators like cloudsim  @xcite have been used throughout industry and academia to quickly evaluate scheduling and provisioning approaches for large scale cloud infrastructure without having to pay for expensive test beds .",
    "unfortunately , such simulators work on a simplified cloud performance model and do not represent realistic vm performance variability , which is essential for testing our system .",
    "moreover , simulations can be quite inaccurate when the simulated system serves resource demanding workloads , as they do not consider aspects like cpu caching , disk data caching in ram and garbage collection  @xcite .",
    "therefore , we test our method through a prototype and a standard benchmark deployed in a public cloud environment .",
    "we validate our approach with the cloudstone  @xcite web benchmark deployed in amazon aws .",
    "it follows the standard 3-tier architecture . by default cloudstone",
    "is not scalable , meaning that it can only use a single as .",
    "thus we had to extend it to accommodate multiple servers .",
    "our installation scripts and configurations are available as open source code . for space considerations we will not discuss these technical details and will only provide an overview .",
    "the interested readers can refer to our online documentation and installation instructions .",
    "the benchmark deployment topology is depicted in figure  [ fig : cloustone ] .",
    "cloudstone uses the _",
    "harness to manage the runs and to emulate users .",
    "faban driver _ , which is deployed in the client vm communicates with the _ faban agents _ deployed in other vms to start or stop tests .",
    "it also emulates the incoming user requests to the application .",
    "these requests arrive at a haproxy _ load balancer _ which distributes them across one or many application servers ( as ) .",
    "cloudstone is based on the olio application , which is a php social network website deployed in a nginx server . in the beginning",
    "we start with a single as `` behind '' the _",
    "load balancer_. when a new as vm is provisioned we associate it with the _ load balancer_. we update its weighted round robin policy , so that incoming request are distributed among the as vms proportionally to their declared cpu capacity ( i.e. ecu ) .",
    "the persistent layer is hosted in a mysql server deployed within a separate db vm .",
    "cloudstone has two additional components - ( i ) a geocoding service called _ geocoder _ , hosted in an apache tomcat server and ( ii ) a shared _ file storage _ hosting media files .",
    "they are both required by all application servers .",
    "we have deployed the geocoding service in the db vm .",
    "the file storage is deployed in a network file system ( nfs ) server on a separate vm with 1 tb ebs storage , which is mounted from each as vm .",
    "we use `` m3.medium '' vms for the client , load balancer and db server and `` m1.small '' for the nfs server .",
    "the types of the as vms are defined differently for each experiment .",
    "all vms run 64bit ubuntu linux 14.04 .",
    "our prototype of an autoscaling component is hosted on an on - premises physical machine and implements the previously discussed algorithms and approaches .",
    "it uses the jclouds  @xcite multi - cloud library to provision resources , and thus can be used in other clouds as well .",
    "we use the nupic  @xcite and fann  @xcite libraries to implement htm and ann respectively .",
    "we ignore the first 110 _ anomaly scores _ reported from the htm , as we observed that these results are inaccurate ( i.e. always 1 or 0 ) until it receives initial training . whenever a new as vm is provisioned we initialise it with a deep copy of the htm of the first as vm , which is the most trained one .",
    "the monitoring programs deployed within each vm are implemented as bash scripts , and are accessed by the autoscaling component through ssh .",
    "our implementation of algorithm  [ algo : utilest ] uses @xmath54 .",
    "previously we discussed that the number of current users could be approximated by counting the number of distinct ip addresses to which there is an active tcp session .",
    "however , in cloudstone all users are emulated from the same client vm and thus have the same source ip address .",
    "thus , we use the number of recently modified web server session files instead .",
    "our autoscaling component implementation follows the amazon auto scaling  @xcite approach and provisions a new as vm once the average utilisation of the server farm reaches 70% for more than 10 seconds .",
    "hence , we ensure that in all experiments the as vms are not overloaded . thus ,",
    "even if there are sla violations , they are caused either by the network or the db layer , and the as layer does not contribute to them .",
    "we also implement a _ cool down _ period of 10 minutes .",
    "in our experiments , we consider three vm types : _",
    "m1.small_ , _",
    "m1.medium _ and _ m3.medium_. table  [ tbl : vmt ] summarises their cost and declared capacities in the sydney aws region which we use .    .aws",
    "vm type definitions . [ cols=\"<,>,<,<\",options=\"header \" , ]     [ tbl : vmt ]    in all experiments we use the same workload .",
    "we start by emulating 30 users and each 6 minutes we increase the total number of users with 10 until 400 users are reached . to achieve this",
    "we run a sequence of cloudstone benchmarks , each having 1 minute ramp - up and 5 minutes steady state execution time .",
    "given cloudstone s start - up and shut - down times , this amounts to more than 5 hours per experiment .",
    "the goal is to gradually increase the number of users , thus causing the system to scale up multiple times .    to test our approach in the case of a workload characteristic change",
    "we `` inject '' such a change 3.5 hours after each experiment s start . to do",
    "so we manipulate the _ utilisation monitors _ to report higher values .",
    "more specifically they increase the reported cpu utilisations with 10% and the reported ram utilisation with 1 gb plus 2 mb for every currently served user .",
    "we implement one experiment , which is initialised with a _",
    "m1.small _ as vm and each new vm s type is chosen based on our method ( dvts ) .",
    "we also execute 3 baseline experiments , each of which statically selects the same vm type whenever a new vm is needed , analogously to the standard aws auto scaling rules .",
    "first we investigate the behaviour of dvts before the workload change .",
    "it continuously trains one htm for the first as vm and the ann . in the initial stages",
    "the ann _ learning rate _ and _ momentum _ decrease and increase respectively to facilitate faster training .",
    "for example , the _ learning rate _ @xmath26 ( defined in eq .",
    "[ eq : lrk ] ) during the initial stages is depicted in fig  [ fig : lr - init ] .",
    "it shows how @xmath26 drastically reduces as the ann improves its accuracy after only a few tens of training samples .",
    "once the as vm gets overloaded we select a new vm type . at this point",
    "we only have information about _",
    "m1.small _ in the _ capacity repository _ and therefore we infer the other cpu capacities based on eq .",
    "[ eq : cpucapest ] .",
    "finally using algorithm  [ algo : vmtselect ] we select _",
    "m3.medium _ as the type for the second vm .",
    "after the new vm is instantiated , the autoscaling component starts its monitoring .",
    "it trains the ann and a new dedicated htm with its measurements",
    ". it also updates the _ capacity repository _ with the cpu capacity of the new vm .",
    "surprisingly , we observe that on average its cpu capacity is about 35% better than the one of the _ m1.small _",
    "vm , even though according to the specification _",
    "m3.medium _ has 3 ecus and _ m1.small _ has 1 .",
    "therefore , the previous extrapolation of _",
    "m3.medium_ s capacity has been an overestimation .",
    "hence , when a new vm is needed again , the algorithm selects _",
    "m1.small _ again .",
    "3.5 hours after the start of the experiment the workload change is injected .",
    "this is reflected in the htms anomaly scores @xmath32 and the ann s errors .",
    "consequently , the _ learning rate _ @xmath26 , the _ momentum _ @xmath55 and the _ epochs _ @xmath56 also change to speed up the learning process as per equations  [ eq : lrk ] ,  [ eq : mk ] and  [ eq : ek ] and as a result the ann adapts quickly to the workload change .",
    "as discussed for each sample we compute its error ( rmse - pre ) before updating the ann .",
    "figure  [ fig : rmse - inj ] depicts how these errors increase when the change is injected and decrease afterwards as the ann adapts timely .",
    "eventually the load increases enough so the system needs to scale up again . due to the injected change",
    ", the workload has become much more memory intensive , which is reflected in the ann s prediction . hence _",
    "m1.small _ can serve just a few users , given it has only 1.7 gb ram . at that point",
    "the cpu capacity of _",
    "m1.medium _ is inferred from the capacities of _ m1.small _ and _ m3.medium _ as per eq .",
    "[ eq : cpucapest ] , since it has not been used before .",
    "consequently algorithm  [ algo : vmtselect ] selects _",
    "m1.medium _ for the 4th vm just before the experiment completes .    for each experiment , figure  [ fig : timeline ] depicts the timelines of the allocated vms and the total experiment costs . for each vm the type and cost",
    "are specified to the right .",
    "our selection policy is listed as _",
    "dvts_. the baseline policy which statically selects _",
    "m1.small _ allocates 8 new vms after the workload change as _",
    "m1.small _ can serve just a few users under the new workload .",
    "in fact , if there was no _ cool down _ period in the autoscaling , this baseline would have exceeded the aws limit of allowed number of vm instances before the end of the experiment . the baselines which select _ m1.medium _ and _ m3.medium _",
    "fail to make use of _ m1.small _ instances before the change injection , which offers better performance for money .",
    "admittedly , in the beginning dvts did a misstep with the selection of _",
    "m3.medium_ , because it started with an empty _ capacity repository _ and had to populate it and infer cpu capacities `` on the go '' .",
    "this could have been avoided by prepopulating the _ capacity repository _ with test or historical data .",
    "we could expect that such inaccuracies are avoided at later stages , once more capacity and training data is present .",
    "still , our approach outperformed all baselines in terms of incurred costs with more than 20% even though its effectiveness was hampered by the lack of contextual data in the initial stages .",
    "our experiments tested dvts and the baselines with a workload , which is lower than what is observed in some applications . while our tests did not allocate more than 12 vms ( in the baseline experiment , which statically allocates _",
    "m1.small_ ) many real world systems allocate hundreds or even thousands of servers .",
    "we argue that in such cases , dvts will perform better than demonstrated , as there will be much more training data and thus the vm types capacity estimations will be determined more accurately and the machine learning approaches will converge faster . as discussed , that would allow some of the initial missteps of dvts to be avoided",
    ". moreover , as the number of as vms grows , so does the cost inefficiency caused by the wastage of allocated resources , which can be reduced by dvts .    finally ,",
    "the response times in the dvts experiment and all baseline experiments were equivalent .",
    "all experiments scale up once the as vms utilisations exceed the predefined thresholds , and thus never become overloaded enough to cause response delays .",
    "the load balancer is equally utilised in all experiments , as it serves the same number of users , although it redirects them differently among the as vms .",
    "similarly , the db layer is equally utilised , as it always serves all users from all as vms .",
    "in this work we have introduced an approach for vm type selection when autoscaling application servers .",
    "it uses a combination of heuristics and machine learning approaches to `` learn '' the application s performance characteristics and to adapt to workload changes in real time . to validate our work ,",
    "we have developed a prototype , extended the cloudstone benchmark and executed experiments in aws ec2 .",
    "we have made improvements to ensure our machine learning techniques train quickly and are usable in real time .",
    "also we have introduced heuristics to approximate vm resource capacities and workload resource requirements even if there is no readily usable data , thus making our approach useful given only partial knowledge .",
    "results show that our approach can adapt timely to workload changes and can decrease the cost compared to typical static selection policies .",
    "our approach can achieve even greater efficiency , if it periodically replaces the already running vms with more suitable ones in terms of cost and performance , once there is a workload change .",
    "we will also work on new load balancing policies , which take into account the actual vm capacities .",
    "another promising avenue is optimising the scaling down mechanisms  i.e. selecting which vms to terminate when the load decreases .",
    "also , we plan to extend our approach , which currently optimises cost , to also consider other factors like energy efficiency .",
    "this would be important when executing application servers in private clouds .",
    "finally , we plan to incorporate in our algorithms historical data about vm types resource capacity and workload characteristics .",
    "we thank rodrigo calheiros , amir vahid dastjerdi , adel nadjaran toosi , and simone romano for their comments on improving this work .",
    "we also thank amazon.com , inc for their support through the aws in education research grant .",
    "+            e.  brewer , `` towards robust distributed systems , '' in _ proceedings of the annual acm symposium on principles of distributed computing _ , vol .",
    "19.1em plus 0.5em minus 0.4emnew york , ny , us : acm , jul 2000 , pp .",
    "710 .        j.  c. corbett , j.  dean , m.  epstein , a.  fikes , c.  frost , j.  j. furman , s.  ghemawat , a.  gubarev , c.  heiser , p.  hochschild , w.  hsieh , s.  kanthak , e.  kogan , h.  li , a.  lloyd , s.  melnik , d.  mwaura , d.  nagle , s.  quinlan , r.  rao , l.  rolig , y.  saito , m.  szymaniak , c.  taylor , r.  wang , and d.  woodford , `` spanner : google s globally distributed database , '' _ acm trans .",
    "_ , vol .  31 , no .  3 , pp . 8:18:22 , aug . 2013 .",
    "l.  cherkasova , k.  ozonat , n.  mi , j.  symons , and e.  smirni , `` automated anomaly detection and performance modeling of enterprise applications , '' _ acm transactions on computer systems _ , vol .",
    "27 , no .  3 , pp . 132 ,",
    "o.  tickoo , r.  iyer , r.  illikkal , and d.  newell , `` modeling virtual machine performance : challenges and approaches , '' _ acm sigmetrics performance evaluation review _ , vol .",
    "37 , no .  3 , pp .",
    "5560 , jan .",
    "j.  dejun , g.  pierre , and c .- h .",
    "chi , `` ec2 performance analysis for resource provisioning of service - oriented applications , '' in _ proceedings of the international conference on service - oriented computing ( icsoc 2009 ) _ , ser .",
    "icsoc / servicewave09.1em plus 0.5em minus 0.4emberlin , heidelberg : springer - verlag , 2009 , pp .",
    "197207 .",
    "j.  schad , j.  dittrich , and j .- a .",
    "quian - ruiz , `` runtime measurements in the cloud : observing , analyzing , and reducing variance , '' _ the proceedings of the vldb endowment ( pvldb ) _ , vol .  3 , no . 1 - 2 , pp .",
    "460471 , sep .",
    "j.  tordsson , r.  s. montero , r.  moreno - vozmediano , and i.  m. llorente , `` cloud brokering mechanisms for optimized placement of virtual machines across multiple providers , '' _ future generation computer systems _ , vol .",
    "28 , no .  2 ,",
    "pp . 358367 , 2012 .",
    "t.  lorido - botrn , j.  miguel - alonso , and j.  a. lozano , `` auto - scaling techniques for elastic applications in cloud environments , '' department of computer architecture and technology , university of the basque country , tech .",
    "ehu - kat - ik-09 - 12 , 2012 .",
    "t.  chieu , a.  mohindra , a.  karve , and a.  segal , `` dynamic scaling of web applications in a virtualized cloud computing environment , '' in _ proceedings of the ieee international conference on e - business engineering ( icebe 2009)_.1em plus 0.5em minus 0.4emieee , oct .",
    "2009 , pp .",
    "281286 .",
    "t.  chieu , a.  mohindra , and a.  karve , `` scalability and performance of web applications in a compute cloud , '' in _ proceedings of the ieee international conference on e - business engineering _ , 2011 , pp .",
    "317323 .",
    "b.  simmons , h.  ghanbari , m.  litoiu , and g.  iszlai , `` managing a saas application in the cloud using paas policy sets and a strategy - tree , '' in _ proceedings of the 7th international conference on network and services management _ , ser .",
    "cnsm 11.1em plus 0.5em minus 0.4emlaxenburg , austria , austria : international federation for information processing , 2011 , pp .",
    "343347 .",
    "e.  barrett , e.  howley , and j.  duggan , `` applying reinforcement learning towards automating resource allocation and application scalability in the cloud , '' _ concurrency and computation : practice and experience _ , vol .  25 , no .  12 , pp . 16561674 , 2013 .",
    "x.  dutreilh , s.  kirgizov , o.  melekhova , j.  malenfant , n.  rivierre , and i.  truck , `` using reinforcement learning for autonomic resource allocation in clouds : towards a fully automated workflow , '' in _ proceedings of the 7th international conference on autonomic and autonomous systems ( icas 2011 ) _ , may 2011 , pp .",
    "a.  ali - eldin , j.  tordsson , and e.  elmroth , `` an adaptive hybrid elasticity controller for cloud infrastructures , '' in _ network operations and management symposium ( noms ) , 2012 ieee _ , april 2012 , pp . 204212 .",
    "a.  gandhi , p.  dube , a.  karve , a.  kochut , and l.  zhang , `` adaptive , model - driven autoscaling for cloud applications , '' in _",
    "11th international conference on autonomic computing , icac 14 _",
    ", 2014 , pp . 5764 .",
    "r.  singh , u.  sharma , e.  cecchet , and p.  shenoy , `` autonomic mix - aware provisioning for non - stationary data center workloads , '' in _ proceedings of the 7th international conference on autonomic computing _",
    "icac 10.1em plus 0.5em minus 0.4emnew york , ny , usa : acm , 2010 , pp . 2130 .",
    "y.  tan , h.  nguyen , z.  shen , x.  gu , c.  venkatramani , and d.  rajan , `` prepare : predictive performance anomaly prevention for virtualized cloud systems , '' in _ proceedings of the 32nd international conference on distributed computing systems ( icdcs ) _ , june 2012 , pp .",
    "285294 .",
    "w.  lloyd , s.  pallickara , o.  david , j.  lyon , m.  arabi , and k.  rojas , `` performance implications of multi - tier application deployments on infrastructure - as - a - service clouds : towards performance modeling , '' _ future generation computer systems _ , vol .",
    "29 , no .  5 , pp . 12541264 , 2013 .",
    "v.  mountcastle , `` an organizing principle for cerebral function : the unit model and the distributed system , '' in _ the mindful brain _ , g.  edelman and v.  mountcastle , eds.1em plus 0.5em minus 0.4emcambridge , ma , us : mit press , 1978 .",
    "r.  n. calheiros , r.  ranjan , a.  beloglazov , c.  a. f.  d. rose , and r.  buyya , `` cloudsim : a toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms , '' _ software : practice and experience _ , vol .",
    "41 , no .  1 ,",
    "pp . 2350 , january 2011 .",
    "w.  sobel , s.  subramanyam , a.  sucharitakul , j.  nguyen , h.  wong , a.  klepchukov , s.  patil , a.  fox , and d.  patterson , `` cloudstone : multiplatform , multi - language benchmark and measurement tools for web 2.0 , '' in _ proceedings of cloud computing and its applications ( cca 08 ) _ , ser .",
    "cca 08 , 2008 ."
  ],
  "abstract_text": [
    "<S> autoscaling is a hallmark of cloud computing as it allows flexible just - in - time allocation and release of computational resources in response to dynamic and often unpredictable workloads . </S>",
    "<S> this is especially important for web applications whose workload is time dependent and prone to flash crowds . </S>",
    "<S> most of them follow the 3-tier architectural pattern , and are divided into presentation , application / domain and data layers . in this work we focus on the application layer . </S>",
    "<S> reactive autoscaling policies of the type _ `` instantiate a new virtual machine ( vm ) when the average server cpu utilisation reaches x% '' _ have been used successfully since the dawn of cloud computing . </S>",
    "<S> but which vm type is the most suitable for the specific application at the moment remains an open question . in this work , </S>",
    "<S> we propose an approach for dynamic vm type selection . </S>",
    "<S> it uses a combination of online machine learning techniques , works in real time and adapts to changes in the users workload patterns , application changes as well as middleware upgrades and reconfigurations . </S>",
    "<S> we have developed a prototype , which we tested with the cloudstone benchmark deployed on aws ec2 . </S>",
    "<S> results show that our method quickly adapts to workload changes and reduces the total cost compared to the industry standard approach . </S>"
  ]
}