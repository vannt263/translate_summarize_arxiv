{
  "article_text": [
    "the method of least squares has by now a long and well - trodden history , which we will not attempt to address in this work . toward the end of the 20th century",
    ", @xcite referred to the method of least squares as the automobile of ( then ) modern statistical analysis .",
    "today , 30 years later , as automobiles have modernized , including technological and efficiency improvements , so too have the methods of least squares changed .",
    "let us recall the classical least squares problem : given outcome vector @xmath0 and design matrix @xmath1 , the least squares problem is typically posed as @xmath2 the related normal equations are @xmath3    we could have almost equivalently written the equations above in terms of the metric projection problem @xmath4 with classical least squares by setting @xmath5 the column space of @xmath6 .",
    "this metric projection problem is the first of the basic tools in what we will refer to here as `` the geometry of least squares in the 21st century . ''",
    "while written as a function above , the map ( [ eqmetricproj ] ) is not always a function , there may be many minimizers for a given @xmath7 . for a set @xmath8 ,",
    "define the _",
    "critical radius _ of @xmath8 as @xmath9 note that for convex @xmath8 , @xmath10 .",
    "the metric projection problem also makes sense for other sets and other metrics .",
    "for instance , suppose @xmath11 , the unit @xmath12 sphere in @xmath13 with distance @xmath14 .",
    "one might also consider the spherical metric projection @xmath15 with the critical radius ( [ eqradius ] ) being similarly defined .",
    "we will see in section  [ secgaussian ] that the above critical radius plays a part in the supremum of a class of gaussian processes [ @xcite ] , one of the other important class of objects associated to gauss s name .",
    "gaussian processes suffer some of the same deficiencies identified in @xcite : they make many assumptions and have their limitations . nevertheless , they are a crucial inferential tool in analyzing the behavior of ( [ eqmetricproj ] ) .",
    "hence , we refer to these as the second of the basic tools in the geometry of least squares in the 21st century .",
    "in the classical setting @xmath16 the system ( [ eqlskkt ] ) often has a unique solution , the familiar @xmath17 in many parametric models , the least squares model is of course too simple . in the exponential family",
    "setting [ @xcite , @xcite ] , the normal equations are similar , with @xmath18 replaced by the observed fisher information .",
    "we have focused on squared error - loss for its simplicity of exposition .    in high - dimensional settings",
    ", @xmath19 is often less than @xmath20 and there is of course no unique solution to ( [ eqlskkt ] ) .",
    "many modifications are possible , for instance , ridge or tikhonov regularization which adds a strongly convex quadratic term to ( [ eqls ] ) .",
    "the addition of such quadratic terms changes the quadratic part of the loss but does not fundamentally change much else until we begin to make assumptions about whether or not the model is correct , and how much bias might be incurred by such regularization .    in modern high - dimensional settings",
    "the regularization term , or penalty , of choice is often a norm , with the lasso [ @xcite ] being the most popular .",
    "the lasso problem is @xmath21 the duality between norms allows us to write @xmath22 with @xmath23 the @xmath24 ball of radius 1 in @xmath25 and for any set @xmath26 @xmath27 is the support function of the set @xmath26 which we assume to be closed and containing 0 . in this notation ,",
    "the lasso problem can be expressed as @xmath28    our canonical problem is therefore @xmath29 with @xmath26 being a closed , convex set containing 0 . for one of many possible infinite dimensional formulations of this canonical problem , see @xcite whose author is also associated to one of the most famous tools in the theory of gaussian process , the borell tis inequality [ @xcite ] .",
    "the normal equations of the least squares problem are replaced with the kkt conditions [ @xcite ] for ( [ eqcanonical ] ) . for our canonical problem ,",
    "the kkt conditions are @xmath30 where the @xmath31 denotes the sub differential . in what follows ,",
    "we denote a solution to this problem as @xmath32 to denote the dependence on the penalty @xmath33 and the penalty parameter @xmath34.=-1    as we can encode linear or cone constraints in the support function , it is safe to say that a huge number of problems fit into this framework .",
    "some examples include :    * lasso [ @xcite ] ; * compressed sensing [ @xcite , @xcite ] ; * group lasso [ @xcite , @xcite ] ; * graphical lasso [ @xcite , @xcite ] ; * matrix completion [ @xcite , @xcite ] ; * sign restricted regression [ @xcite ] ; * hierarchically constrained models [ @xcite , @xcite ] ; * generalized lasso [ @xcite ] ; * total variation denoising [ @xcite ] .",
    "the relation between ( [ eqcanonical ] ) and the metric projection map is through a particular dual function , also developed by legendre .",
    "the canonical dual problem is @xmath35 with @xmath36 the row space of the matrix @xmath6 .",
    "this dual problem can be derived by minimizing the following lagrangian with respect to @xmath37 @xmath38 after a sign change , the problem in ( [ eqcanonicaldual ] ) is fairly easily seen to be equivalent to @xmath39\\ ] ] with the constraints in ( [ eqcanonicaldual ] ) encoding the fact that @xmath40 whenever @xmath41 any pair @xmath42 is related through @xmath43    choosing an orthonormal basis for @xmath36 , we see that the dual problem can be phrased as the metric projection problem of @xmath44 onto @xmath45 . alternatively ,",
    "if we are interested only in the fitted values @xmath46 , the original problem ( [ eqcanonical ] ) can be expressed as the residual @xmath47 from @xmath48 where @xmath49 or , in another form , @xmath50 we see that our canonical regression problem is in fact a metric projection problem .",
    "having posed the canonical high - dimensional regression problem as a metric projection problem , we now try to describe how metric projection is related to some fundamental issues in the understanding of this problem both from an algorithmic view and an inferential view .",
    "the general problem is phrased in terms of an arbitrary @xmath26 . for many high - dimensional problems ,",
    "this @xmath26 is chosen to emphasize expected structure in the data .",
    "for example , it is well known that the lasso yields sparse solutions , the group lasso yields groups of nonzero coefficients , etc .",
    "this special structure is based on a particular structure encoded in @xmath26 .",
    "further , for many canonical choices of @xmath26 used in high dimensional statistics , such as those cited in section  [ secproblem ] , the following metric projection is simple @xmath51.\\ ] ]    such optimization problems are referred to as problems in composite form [ becker , bobin and cands ( @xcite ) , @xcite , @xcite ] . for such problems ,",
    "many modern first order solvers use a version of generalized gradient descent .",
    "the steps in generalized gradient descent are essentially iterations of this metric projection map . specifically , to solve the canonical problem ( [ eqcanonical ] ) for a given step - size @xmath52 a simple generalized gradient algorithm reads as @xmath53",
    "the first line in the update above is the usual form of updates for generalized gradient descent , while the second line expresses this step as the residual after an application of the metric projection map .",
    "accelerated schemes can do much better with slightly different updates above , see @xcite , @xcite , @xcite . with modern computing techniques",
    ", such simple algorithms can scale to huge problems , see @xcite , @xcite .",
    "having solved the canonical problem , what can we say about its solution ? as this special issue is devoted to the appearance of one of the first proofs of the law of large numbers , we should at least hope to provide such an answer .    in the classical setting , assuming independence , and the usual linear regression model @xmath54 with noise @xmath55 having scale @xmath56 , the central limit theorem can often be applied to ( [ eqls ] ) yielding the usual result @xmath57 under the null @xmath58 .",
    "of course , this forms the basis of much inference in modern ( and not so modern ) applied statistics in the fixed @xmath20 , @xmath19 growing regime . in terms of the parameters themselves , this implies the weaker statement @xmath59 for some random variable @xmath60 .    in the classical setting , assuming @xmath6 is full rank , the bound ( [ eqlln ] )",
    "is a simple two line proof followed by some assertions .",
    "if we write @xmath61 , then @xmath62 with @xmath63 denoting the smallest eigenvalue of @xmath64 .",
    "we see that we can take @xmath65    in the high - dimensional setting , of course this fails as @xmath66 .",
    "what , then , can we say about our canonical estimator @xmath67 is there even a weak law of large numbers ? without any assumptions on @xmath26 , the answer is clearly no : if @xmath68 , then this is the original ill - posed problem .    under an assumption of decomposability of @xmath26 recent progress has been made in providing bounds on the estimation error in ( [ eqcanonical ] ) , see @xcite .",
    "the notion of decomposability in @xcite has a precise definition which we will not dwell on here",
    ". however , a large set of examples of decomposable penalties are penalties of the form @xmath69 that is , convex sets that can be expressed as products of convex sets lead to decomposable penalties .",
    "every example of a decomposable norm in @xcite has this form except the nuclear norm . for such penalties , the generalized gradient algorithms described in section  [ secalgorithm ] decompose into many smaller subproblems .",
    "many efficient coordinate descent algorithms exploit this fact , see @xcite , @xcite .    a similar notion of decomposability",
    "we refer to as _ additivity _",
    "is explored in taylor and tibshirani ( @xcite ) in which case @xmath26 can be expressed as @xmath70 with the sum being minkowski addition of sets . in this case",
    ", the penalty has the form @xmath71 one concrete example of this is the @xmath24 ball in @xmath25 . for @xmath72 a partition of @xmath73 into _ active _ and _ inactive _ variables , we can write @xmath74 any penalty of the form ( [ eqproduct ] ) can be expressed in the form ( [ eqadditive ] ) in a similar fashion . if we are allowed to introduce a linear constraint to ( [ eqcanonical ] ) , then any problem with a penalty of the form ( [ eqadditive ] ) can be expressed as a problem with a penalty of the form ( [ eqproduct ] ) subject to an additional set of linear constraints .",
    "the weak law of large numbers presented in the literature follow a similar path to the argument above .",
    "of course , for precise results , the specific penalty as well as the data generating mechanism must be more precisely specified .    in the interest of space",
    ", we do not pursue such precise statements here .",
    "rather , we will just attempt to paraphrase these results , of which there exist many in the literature [ cf .",
    "@xcite , @xcite , obozinski , wainwright and jordan ( @xcite ) , @xcite ] with @xcite being a particularly nice place to read in detail . under various assumptions on the tails of @xmath55 , as well as the assumption @xmath75 and @xmath76",
    "is bounded , @xmath77 for any @xmath26 , the seminorm @xmath78 is the dual seminorm of @xmath79 . ]",
    "the canonical result has the form for @xmath80 @xmath81 with high probability for some universal @xmath82 where @xmath83 is referred to as a _ compatibility _",
    "constant relating the @xmath12 norm and the @xmath84 seminorm ; the quantity @xmath85 replaces @xmath63 and is referred to as a _ restricted strong convexity _ ( rsc ) constant .",
    "the literature varies in their assumptions on the noise and the design matrix @xmath6 .",
    "for instance , in the fixed design case one might consider the above probability only with respect to noise , while in a random design setting the dependence of the constants on @xmath6 are typically expressed with respect to the law that generates the design matrix @xmath6 .",
    "having established a bound such as ( [ eqerror ] ) , if one considers problems indexed by @xmath19 , then one can obtain a law of large numbers for the problem ( [ eqcanonical ] ) so long as the parameters are chosen so the right - hand side decays to 0 and the bound holds with sufficiently high probability . again in the interest of space , we refer readers to the literature for more precise statements for specific versions of the problem such as the lasso .",
    "we have tried to give a summary of some of the results related to the canonical problem , though we have barely exposed the tip of the iceberg . under more specific assumptions much more can be said .",
    "for example , see @xcite .      for @xmath86 ,",
    "the error ( [ eqerror ] ) depends on the quantity @xmath87 for fixed @xmath6 , this quantity is referred to as the gaussian width of @xmath33 and it also intimately related to our first tool in the toolbox , the metric projection .",
    "specifically , consider the tube of radius @xmath88 around @xmath89 .",
    "that is , @xmath90 then , a classical result of steiner in the case of convex bodies and weyl in the case of manifolds says that the lebesgue measure of the tube , assuming @xmath89 is bounded , can be expressed as @xmath91 where the @xmath92 are referred to as the intrinsic volumes of @xmath89 and @xmath93 is the lebesgue measure of the unit @xmath12 ball in @xmath94 .",
    "see @xcite , @xcite , @xcite , @xcite for more details on such volume of tubes formulae .",
    "when @xmath89 is unbounded , federer s curvature measures [ @xcite ] can be used to define the volume of local tubular neighborhoods . using gaussian process techniques [ @xcite ] , it can be shown that @xmath95 where @xmath96 .",
    "for @xmath97 a smooth domain , that is convex set with non - empty interior bounded by a smooth hypersurface and for @xmath98 @xmath99 with @xmath100 the @xmath101th elementary symmetric polynomial of the so - called principal curvatures of @xmath102 at @xmath103 [ @xcite , @xcite ] .",
    "these are just the eigenvalues of the second fundamental form in the unit inward normal direction .",
    "this formula can be derived by considering the inverse of the metric projection map ( [ eqmetricproj ] ) .",
    "the inverse takes @xmath104 defined on the extended outward normal bundle of @xmath105 .",
    "the inverse of the map is simply the exponential map restricted to the outward normal bundle , or , more simply @xmath106 a fairly straightforward calculation yields the relation ( [ eqintrinsicvolumes ] ) .",
    "the main take away message above is that the functionals in the tube formula , such as the gaussian width ( [ eqwidth ] ) , are related to properties of the metric projection map onto  @xmath33 .",
    "as written above , the intrinsic volumes are defined implicitly through a volume calculation , and it is not clear that they extend to the infinite dimensional setting . under the right conditions of course , such extensions are indeed possible . see @xcite for a nice discussion of this problem .",
    "an alternative definition of intrinsic volumes specific to the gaussian case was considered in @xcite , which were defined as coefficients in an expansion of the _ gaussian _ measure of @xmath107 as described in the gaussian kinematic formula [ @xcite , @xcite ] .",
    "another quantity of interest for our problem ( [ eqcanonical ] ) is an estimate of how much `` fitting '' we are performing , as a function of @xmath34 .",
    "one quantitative measure of this is captured by stein s estimate of risk [ @xcite ] , also known as sure .",
    "suppose now that @xmath108 and we estimate @xmath109 by the estimator @xmath110 .",
    "the sure estimate is an unbiased estimate of @xmath111 the estimated degrees of freedom of this estimator is one part of the sure estimate and is defined as @xmath112 suppose @xmath108 and consider our residual form of the original estimation problem for @xmath113 @xmath114 if @xmath26 possesses a nice stratification , as do all the examples mentioned above , then , for almost every @xmath7 , @xmath115 is in the relative interior of some fixed stratum @xmath116 of the normal bundle of @xmath33 over which the dimension of the tangent space is constant , and the normal bundle has a locally conic structure @xmath117 of tangent and normal directions [ @xcite , @xcite ] . having fixed this stratum , we can write @xmath118 in ( ortho)normal coordinates centered at @xmath119 . in these coordinates",
    "@xmath120 in order to relate the above to the problem ( [ eqcanonical ] ) , one should invert the above chart to find @xmath121 in terms of @xmath7 . in the residual form ( [ eqresid ] ) it is easy to show that @xmath122 the derivatives along the normal directions yield a purely dimensional term , while directions in the tangent directions yield curvature terms .",
    "this observation is enough to derive the following form of the degrees of freedom @xmath123 above , @xmath124 is the tangential part of the stratum containing @xmath125 and @xmath126 is the second fundamental form of @xmath124 in @xmath127 as described [ @xcite ] .",
    "when @xmath26 is a polyhedral set , the second term disappears and the degrees of freedom can be computed by computing the rank of a certain matrix [ @xcite , @xcite ] .      another fundamental tool in inference for least squares models is the ability to form hypothesis tests , as well as confidence intervals for the `` true '' mean .",
    "such concepts clearly need a model , which we might take to be the usual model @xmath128 under the assumption that @xmath129 , classical inference in linear models ( assuming @xmath64 is full rank ) yields confidence intervals and hypothesis tests for any linear functional @xmath130 based on the coefficients @xmath131 .",
    "what can we say about our canonical problem ( [ eqcanonical ] ) ?",
    "this is an area in which we still do nt know all the answers . in some sense",
    ", we are in the situation analogous to bernoulli having proved a weak law of large numbers without the central limit theorem .",
    "some progress has been made for specific models of the design matrix @xmath6 for the lasso as well as group lasso models , see @xcite , @xcite , @xcite , @xcite , wasserman and roeder ( @xcite ) , @xcite .",
    "other recent work [ @xcite ] gives some hints at what inferential tools may prove useful in this weak convergence theory .",
    "as described in the lars algorithm [ @xcite , @xcite ] , an entire path of solutions @xmath132 can be formed for the lasso , that is , when @xmath133 .",
    "these paths are piecewise linear , with knots at points where the _ active set _ changes .",
    "the covariance statistic measures some change in the correlation of the fitted values between two knots @xmath134 and @xmath135 in the lasso path .",
    "it has the form @xmath136 for some random scaling @xmath137 related to the active set and the variable added to the active set at @xmath135 .",
    "the form for @xmath138 is particularly simple : suppose that @xmath139 and @xmath6 is such the first variable in the lars path , that is , @xmath140 then , @xmath141 for @xmath142 , the form of the test statistic is slightly more complicated , though it can be expressed in terms of @xmath143 , the active set at step @xmath144 as well as @xmath145 , the signs of the active variables at step @xmath144 as well as the active set @xmath146 and @xmath147 , see section 2.3 of @xcite for the full expression .",
    "for a wide range of ( sequences ) covariance matrices , if the active set at @xmath134 already contains all the strong active variables , then it is shown in @xcite that @xmath148 in particular , under the global null @xmath149 as long as the design matrix satisfies some minimum growth condition , @xmath150 .",
    "the main tools used in the above proof relate to the maxima of ( discrete ) gaussian processes and the generalization of an argument previously applied to smooth gaussian processes in @xcite and @xcite .",
    "ongoing work suggests that such a limiting distribution will hold for many ( sequences ) of @xmath26 and design matrices @xmath6 .    as for confidence intervals for the parameters related to @xmath144 strong variables ,",
    "the relation to extreme values of gaussian processes suggest that the bias - corrected or relaxed lasso estimate of the active coefficients will have accurate coverage .",
    "this is ongoing work .",
    "in the special case that @xmath26 is a cone , we saw that the distribution of a particular likelihood ratio test could be expressed in terms of the supremum of a gaussian process indexed by a subset of the sphere .",
    "equivalently , this supremum could be expressed in terms of the metric projection onto the cone @xmath33 .",
    "it is well - known that the distribution of this likelihood ratio test statistic is a mixture of @xmath151 s of varying degrees of freedom .",
    "this distribution is sometimes referred to as a @xmath152 distribution .",
    "the mixture weights can be expressed in terms of the geometry of the set @xmath153 .",
    "in particular , it is known [ @xcite , takemura and kuriki ( @xcite ) ] that if @xmath154 for @xmath155 = 6.5pt = 6.5pt @xmath156 where @xmath157 and @xmath158 are the standard hermite polynomials .",
    "the functions ( [ eqrho ] ) are known as the ec or euler characteristic densities for a gaussian field , see @xcite , @xcite , @xcite , @xcite , @xcite . while the sum above is written as an infinite sum it terminates at @xmath159 .",
    "note that this implies @xmath160 which is an analogous way to derive gaussian width ( [ eqgausswidth ] ) .",
    "one of the derivations of the above formula , the so - called _ volume of tubes _ approach [ @xcite , @xcite , @xcite ] involves studying the jacobian of the inverse of the spherical metric projection map ( [ eqmetricprojsphere ] ) , that is , the exponential map on @xmath161 which sends a pair @xmath121 to @xmath162 another approach , the expected euler characteristic approach [ @xcite , @xcite ] involves counting critical points of the gaussian process above the level @xmath163 according to saddle type and applying morse theory and the rice  kac formula [ @xcite , @xcite ] to count the expected number of such points .",
    "neither of these approaches strictly require convexity of the cone generated by the parameter set @xmath164 .",
    "rather , they depend on a notion of local or infinitesimal convexity referred to as positive reach [ @xcite ] . hence , the parameter sets may have finite critical radius .",
    "they are both approaches used to form an approximation @xmath165 for some centered , smooth , gaussian process @xmath166 having constant variance 1 on a ( possibly stratified ) manifold @xmath164 . in the volume of tubes approach , the critical radius appears in a natural way and enters into an estimate of the error of the volume of tubes approach . in both approaches , though it is clearer in the expected euler characteristic approach , the spherical critical radius is in fact the spherical critical radius of @xmath167 where @xmath168 where @xmath169 is the unit sphere in @xmath170 , the reproducing kernel hilbert space of @xmath166 .",
    "either approach yields roughly the same estimate of error : for @xmath163 large enough @xmath171 the above says that the _ relative error _ in the approximation is exponentially small whenever @xmath172 .",
    "the critical radius of @xmath164 in the expected euler characteristic approach arises in terms of a functional of the original process @xmath166 .",
    "specifically , if we assume @xmath164 is a manifold without boundary , then define for each @xmath173 the process introduced in @xcite @xmath174 then , @xmath175 hence , the critical radius depends in an explicit way on the covariance function of the process @xmath166 .    as mentioned previously , the argument related to derivation of ( [ eqfxy2 ] ) in the smooth case led directly to the exponential limit in ( [ eqexp1 ] ) .",
    "such a connection suggests a relation between the distribution of the maxima of smooth gaussian processes , specifically the spacings of the extreme values , can be used to derive weak convergence results for high - dimensional inference .",
    "this is ongoing work .",
    "we have described what we call the two basic tools of the geometry of least squares that are just as relevant as when gauss and legendre disputed their original discovery over 200 years ago . while these tools are not the most technically sophisticated tools , ceding that ground to exponential families for the canonical model ( [ eqcanonical ] ) and empirical processes for the fluctuation theory in section  [ secinference ] , they nevertheless provide guiding principles for these more precise tools .",
    "we would argue that the gaussian picture provided by the geometry of least squares , gets much of the picture correct under sufficient moment conditions . for heavier tailed results , of course much of section  [ secinference ] would have to be reframed and section  [ secgaussian ] paints quite a different picture [ adler , samorodnitsky and taylor ( @xcite ) ] .    as bernoulli found himself with just a law of large numbers , the field of statistics is roughly at this same stage in high - dimensional inference .",
    "we are hopeful that the geometry of least squares will eventually guide the field to weak convergence results in high - dimensional inference for the canonical problem ( [ eqcanonical ] ) .",
    "supported in part by nsf grant dms-12 - 08857 and afosr grant 113039 ."
  ],
  "abstract_text": [
    "<S> it has been over 200 years since gauss s and legendre s famous priority dispute on who discovered the method of least squares . </S>",
    "<S> nevertheless , we argue that the normal equations are still relevant in many facets of modern statistics , particularly in the domain of high - dimensional inference . even today </S>",
    "<S> , we are still learning new things about the law of large numbers , first described in bernoulli s _ ars conjectandi _ 300 years ago , as it applies to high dimensional inference .    the other insight the normal equations provide is the asymptotic gaussianity of the least squares estimators . </S>",
    "<S> the general form of the gaussian distribution , gaussian processes , are another tool used in modern high - dimensional inference . </S>",
    "<S> the gaussian distribution also arises via the central limit theorem in describing weak convergence of the usual least squares estimators . in terms of high - dimensional inference </S>",
    "<S> , we are still missing the right notion of weak convergence .    in this mostly expository work </S>",
    "<S> , we try to describe how both the normal equations and the theory of gaussian processes , what we refer to as the `` geometry of least squares , '' apply to many questions of current interest . </S>"
  ]
}