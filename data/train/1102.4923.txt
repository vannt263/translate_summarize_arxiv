{
  "article_text": [
    "relative entropy or kullback - leibler divergence @xmath0 between two probability measures is a fundamental quantity that arises in a variety of situations in probability , statistics , and information theory .",
    "it serves as a measure of dissimilarity or divergence between two probability measures @xmath1 and @xmath2 on a given measure space . in information theory",
    ", it is well known that @xmath0 is the penalty in expected compressed length , i.e. , its gap from shannon entropy @xmath3 , when the compressor assumes that the ( finite - alphabet ) source probability measure is @xmath2 instead of the true probability measure @xmath1 .",
    "rnyi entropies @xmath4 for @xmath5 play the role of shannon entropy when the normalized cumulant of compression length is considered instead of expected compression length .",
    "indeed , campbell @xcite showed that @xmath6 \\to h_{\\alpha } ( p ) ~ ( \\mbox{as } n \\to \\infty)\\ ] ] for an independent and identically distributed ( iid ) source with marginal @xmath1 .",
    "the minimum is over all compression strategies that satisfy the kraft inequality , @xmath7 , and @xmath8 is the cumulant parameter .",
    "we also have @xmath9 , so that rnyi entropy may be viewed as a generalization of shannon entropy .    if the compressor assumed that the true probability measure had marginal @xmath2 , instead of @xmath1 , then the gap in the normalized cumulant s growth exponent from the optimal value ( rnyi entropy ) is an analogous parametric divergence quantity ( introduced by blumer and mceliece @xcite and studied further by sundaresan @xcite ) , which we shall denote @xmath10 .",
    "the same quantity also arises when we study the gap from optimality of mismatched guessing exponents ( see arikan @xcite as well as sundaresan @xcite ) .",
    "all these results are applicable to more general non - iid sources .",
    "as one might expect , it is known that ( see for example , johnson and vignat ( * ? ? ?",
    "* a.1 ) ) @xmath11 , so that we may think of relative entropy as @xmath12 , and therefore @xmath13 as a generalization of relative entropy , i.e. , an @xmath14-relative entropy .",
    "furthermore , for probability measures on a finite alphabet set , @xmath13 behaves like squared euclidean distance , and satisfies a `` pythagorean property '' @xcite like relative entropy and squared euclidean distance .",
    "one purpose of this paper is to extend this property to probability measures on a general measure space with some common dominating measure .",
    "the maximum entropy principle is a well - known selection rule , in the presence of uncertainty , in statistics . for a source alphabet @xmath15 with finite cardinality , by noting that @xmath16 with @xmath17 taken as the uniform measure on the finite alphabet set @xmath15 , the maximum entropy principle is the same as the minimum relative entropy principle , an idea that goes back to boltzmann , and one which is supported by the theory of large deviations . indeed , suppose that certain ensemble average measurements can be made on a realization of a sequence of iid random variables ( mean , second moment , etc . ) .",
    "the resulting realization must have an empirical measure that obeys the constraints placed by the observations .",
    "in particular , the empirical measure belongs to a convex ( and possibly closed ) set .",
    "large deviations theory tells us that , amongst the measures that respect the constraints , the one that minimizes relative entropy is exponentially more likely than the others .",
    "the resulting measure is called @xmath18-projection and was extensively studied by csiszr @xcite , @xcite , and more recently by csiszr and mat @xcite .",
    "@xmath18-minimization arises similarly in the contraction principle of large deviations theory ( see for example dembo and zeitouni s @xcite ) .    as a natural alternative selection principle , the maximum rnyi entropy principle has been recently considered .",
    "this principle is equivalent to maximizing the tsallis entropy , which is a monotone function of the rnyi entropy .",
    "see for example jizba and arimitsu @xcite , and references therein .",
    "more interestingly , jizba and arimitsu @xcite indicate that maximum rnyi entropy principle may be viewed as a maximum shannon entropy principle with multifractal constraints .",
    "this selection principle has been of recent interest in statistical physics settings because rnyi entropy maximizers under a covariance constraint are distributions with a power - law decay ( when @xmath19 ) .",
    "see costa et al .",
    "@xcite or johnson and vignat @xcite . several empirical observations in naturally arising physical and socio - economic systems possess a power - law decay . without going into these aspects",
    ", we remark that @xmath20 , so that both the maximum rnyi entropy principle and the maximum tsallis entropy principle are equivalent to a minimum @xmath21-relative entropy ( minimum @xmath13 ) principle .",
    "thus one needs to find amongst empirical measures that meet the observation constraints , the one that minimizes @xmath13 .",
    "we shall call this the @xmath13-projection . while existence and uniqueness of @xmath13-projection was proved by sundaresan @xcite for the finite alphabet case ,",
    "the second purpose of this paper is to extend these results to more general measure spaces .",
    "it is known ( see for example @xcite ) that @xmath10 is the more commonly studied rnyi divergence of order @xmath22 , not of the original measures @xmath1 and @xmath2 , but of their tilts @xmath23 and @xmath24 , where @xmath25 , and @xmath26 is the normalization that makes @xmath23 a probability measure .",
    "@xmath24 is similarly defined . while the rnyi divergences arise naturally in hypothesis testing problems",
    "( see for example csiszr @xcite ) , @xmath13 arises more naturally as a redundancy for mismatched compression .",
    "@xmath13 is also a certain monotone function of csiszr s @xmath27-divergence between @xmath23 and @xmath24 . as a consequence of the appearance of the tilts , the data - processing property satisfied by @xmath27-divergences",
    "does not hold for the @xmath14-relative entropy .",
    "surprisingly though , the pythagorean property holds .",
    "the rest of the paper is organized as follows . in section [ sec : projection ] , we provide the definitions and demonstrate the existence of @xmath13 projections on certain closed and convex sets . in section [ sec : pythagorenproperty ] , we extend the pythagorean property to general measure spaces ( with a common dominating measure ) , and identify the consequences with respect to iterated projections . in section [ sec : concludingremarks ] , we summarize our results .",
    "we first formalize the definition of @xmath14-relative entropy to a general probability space .",
    "let @xmath1 and @xmath2 be two probability measures on a measure space @xmath28 .",
    "let @xmath29 with @xmath30 . by setting @xmath31",
    "we have the reparameterization in terms of @xmath32 with @xmath33 and @xmath34 .",
    "let @xmath35 be a dominating @xmath36-finite measure on @xmath28 with respect to which @xmath1 and @xmath2 are both absolutely continuous , denoted @xmath37 and @xmath38 . we denote @xmath39 and @xmath40 and assume that they are in the complete metric space @xmath41 with metric @xmath42 we shall use the notation @xmath43 even though it is not a norm for @xmath44 .",
    "( the dependence of this quantity on @xmath14 should be borne in mind ) .",
    "the rnyi entropy of @xmath1 of order @xmath14 ( with respect to @xmath35 ) is given by @xmath45 consider the tilted measures @xmath23 and @xmath24 given by @xmath46 @xmath23 and @xmath24 are also dominated by @xmath35 . with @xmath47 csiszr s @xmath27-divergence @xcite between two measures @xmath1 and @xmath2 , both absolutely continuous with respect to @xmath35 , is given by @xmath48 since @xmath27 is strictly convex when @xmath34 , by jensen s inequality , @xmath49 with equality if and only if @xmath50 .",
    "we now define the @xmath14-relative entropy to be @xmath51.\\ ] ] abusing notation a little , when speaking of densities , we shall some times write @xmath52 for @xmath10 .",
    "we now summarize the anticipated properties of @xmath14-relative entropy .",
    "the following properties hold .",
    "\\1 ) @xmath53 with equality if and only if @xmath54 .",
    "\\2 ) under certain regularity conditions , @xmath55 .",
    "\\3 ) let @xmath56 and let @xmath35 be the lebesgue measure on @xmath57 . for @xmath58 and @xmath30",
    ", define the constant @xmath59 . with @xmath60 a positive definite covariance matrix , the function @xmath61^{\\frac{1}{\\alpha-1}}_+,\\ ] ] with @xmath62_+ :",
    "= \\max \\{a,0 \\}$ ] and @xmath63 the normalization constant , is the density function of a probability measure on @xmath57 whose covariance matrix is @xmath60 .",
    "furthermore , if @xmath64 is the density function of any other random variable with covariance matrix @xmath60 , then @xmath65 consequently @xmath66 is the density function of the rnyi entropy maximizer among all @xmath57-valued random vectors with covariance matrix @xmath60 .",
    "\\4 ) let @xmath67 and let @xmath17 be the uniform probability mass function on @xmath15 .",
    "then @xmath68 @xmath69    we only give an outline here .",
    "statement 1 ) follows by an application of hlder s inequality by considering the hlder conjugates @xmath14 and @xmath70 , and the functions @xmath71 and @xmath72 .",
    "statement 2 ) follows by an application of lhpital s rule and some conditions that enable interchange of differentiation with respect to the parameter @xmath14 and integration with respect to @xmath35 .",
    "statement 3 ) was proved by lutwak et al .",
    "@xcite . see also johnson and vignat @xcite . for relative entropy , the analog of ( [ eqn : moment - entropy ] ) under a covariance constraint would be @xmath73 , where @xmath74 is differential entropy and @xmath75 is the gaussian distribution with the same covariance as @xmath64 .",
    "the last statement follows from the definition .",
    "we next prove an inequality relating @xmath27-divergences .",
    "this yields parallelogram identity for relative entropy ( @xmath76 ) @xcite .",
    "[ parallel ] let @xmath44 .",
    "let @xmath77 be probability measures that are absolutely continuous with @xmath35 , and let the corresponding radon - nikodym derivatives @xmath78 and @xmath79 be in @xmath41 .",
    "assume @xmath80 .",
    "we then have @xmath81+(1-\\lambda ) [ i_f(p'_2,r')-f(1 ) ] } \\nonumber \\\\      & -\\lambda   [ i_f(p'_1,r'_{1,2})-f(1)]-(1-\\lambda ) [ i_f(p'_2,r'_{1,2})-f(1 ) ] \\nonumber \\\\      \\label{eqn : parallelogram }      & \\ge   [ i_f(r'_{1,2},r')-f(1)],\\end{aligned}\\ ] ] where @xmath82 when @xmath19 , the reversed inequality holds in ( [ eqn : parallelogram ] ) .",
    "@xmath69    we briefly outline the steps . let @xmath83 .",
    "observe that since @xmath84 , a consequence of jensen s inequality indicated earlier , all terms within square brackets are nonnegative .",
    "the left - hand side of inequality can be expanded to @xmath85 d\\mu } \\\\    & & \\hspace*{-.1in}+ \\text{sgn}(\\rho ) \\int \\frac{(1-\\lambda ) p_2}{\\|p_2\\| }                                    \\left [ \\left ( \\frac{r}{\\|r\\|}\\right)^{\\alpha-1 }                                           \\hspace*{-.1 in } - \\left ( \\frac{r_{1,2}}{\\| r_{1,2 } \\| } \\right)^{\\alpha-1 }                                    \\right ] d\\mu \\\\    & = & \\text{sgn}(\\rho ) \\int \\frac{r_{1,2}}{\\| r_{1,2 } \\| }                                   \\left [ \\left ( \\frac{r}{\\|r\\| } \\right)^{\\alpha-1 }                                          - \\left ( \\frac{r_{1,2}}{\\| r_{1,2 } \\| } \\right)^{\\alpha-1 }                                   \\right ] d \\mu\\\\    & & \\times \\left [ \\frac{\\lambda}{\\|p_1\\|}+\\frac{1-\\lambda}{\\|p_2\\| } \\right ] \\| r_{1,2}\\| \\\\    & = & \\left [ \\frac{\\lambda}{\\|p_1\\|}+\\frac{1-\\lambda}{\\|p_2\\| } \\right ] \\| r_{1,2}\\| \\cdot [ i_f(r'_{1,2 } , r')-f(1 ) ] .\\end{aligned}\\ ] ] applying minkowski s inequality in ( [ eqn : rstar ] ) with @xmath44 , we get @xmath86 this inequality gets reversed when @xmath19 , again by a version of minkowski s inequality . since @xmath87 , the lemma follows .",
    "let us define what we mean by an @xmath13-projection .",
    "if @xmath88 is a set of probability measures on @xmath28 such that @xmath89 for some @xmath90 , a measure @xmath91 satisfying @xmath92 is called the @xmath13-projection of @xmath93 on @xmath88 .",
    "@xmath69    let @xmath88 be a set of probability measures on @xmath94 .",
    "let @xmath35 be a common ( @xmath36-finite ) dominating measure for @xmath88 .",
    "write @xmath95 and assume that @xmath96 .",
    "now define @xmath97    we are now ready to state our main result on the existence of @xmath13-projection .",
    "[ thm : min ] let @xmath5 and @xmath30 .",
    "let @xmath88 be a set of probability measures with dominating @xmath36-finite measure @xmath35 such that the subset of functions @xmath98 is convex and closed in @xmath41 .",
    "let @xmath93 be a probability measure and suppose that @xmath99 for some @xmath100 .",
    "then @xmath93 has an @xmath13-projection on @xmath88 .",
    "@xmath69    the closure of @xmath98 in @xmath41 , for @xmath76 , would be closure in the total variation metric , which is one of the hypotheses in csiszr s ( * ? ? ?",
    "* th.2.1 ) .",
    "the proof ideas are different for the two cases @xmath44 and @xmath19 .",
    "the proof for @xmath44 is a modification of csiszr s approach in @xcite .",
    "the proof for @xmath19 exploits properties of sets that are convex and closed under the weak topology .",
    "we are indebted to pietro majer for suggesting some key steps on the mathoverflow.net forum .",
    "\\(a ) we first consider the case @xmath44 .",
    "pick a sequence @xmath101 such that @xmath102 and @xmath103 by lemma ( [ parallel ] ) , we have @xmath104\\end{aligned}\\ ] ] where @xmath105 on account of the convexity of @xmath88 . rearranging ( [ eqn : projectionproofstep1 ] ) and using @xmath106",
    ", we get @xmath107.\\end{aligned}\\ ] ] take the limit as @xmath108 .",
    "the expression on the right - most side is at most 1 because @xmath109 and @xmath110 approach the infimum value , and @xmath111 is at least this infimum value for each @xmath112 and @xmath113 .",
    "since we also have @xmath114 and @xmath115 , it follows that @xmath116 = 0.\\ ] ] from ( * ? ? ?",
    "1 ) , a generalization of pinsker s inequality , we get that the total variation metric , denoted @xmath117 , is small if @xmath118 is small .",
    "this fact and the above limit imply that @xmath119 which , together with the triangle inequality for the total variation metric , yields @xmath120 i.e. , the sequence @xmath121 is a cauchy sequence in @xmath122 .",
    "it must thus converge to some @xmath64 in @xmath122 , i.e. , @xmath123 there is then a subsequence , over which one gets a.e.@xmath124 $ ] convergence .",
    "reindexing to operate on this subsequence , we get @xmath125.\\ ] ] we will now demonstrate that an @xmath13-projection , say @xmath2 , is in @xmath88 and has @xmath35-density proportional to @xmath126 .    in view of the a.e.@xmath124 $ ] convergence , and after observing that @xmath127,\\ ] ] we can apply the generalized dominated convergence theorem ( * ? ? ?",
    "* ch.2 , problem.20 ) to get @xmath128    we next claim that @xmath129 suppose not ; then working on a subsequence if needed , we have @xmath130 . as @xmath131 ,",
    "given any @xmath132 , @xmath133 and hence @xmath134 in @xmath124$]-measure , which would be a contradiction to the fact that @xmath135 for all @xmath113 .",
    "thus ( [ eqn : norm - pn - bounded ] ) holds , and so we can find a subsequence that converges to some @xmath136 .",
    "reindex and work on this subsequence to get @xmath137 in @xmath41 .",
    "since @xmath98 is closed in @xmath41 , we obtain @xmath138 for some @xmath139 , @xmath140 , and @xmath141 .",
    "let @xmath2 be the probability measure in @xmath88 with @xmath142 .    to complete the proof",
    ", we need to demonstrate that @xmath143 for every @xmath100 . to see this , note that ( [ eqn : projectionproofstepk ] ) implies that @xmath144 in @xmath145 , and by a change of measure , @xmath146 in @xmath147 , and hence in @xmath148$]-measure . but @xmath27 is continuous , and so @xmath149 in @xmath148$]-measure .",
    "fatou s lemma then implies @xmath150 since @xmath151 , equality must hold , and @xmath2 is an @xmath13-projection of @xmath93 on @xmath88 .",
    "this completes the proof for the case when @xmath44 .",
    "\\(b ) we next consider the case when @xmath19 .",
    "note that @xmath32 is negative , and so the @xmath152 in ( [ eqn : projection ] ) becomes a @xmath153 as follows .",
    "the @xmath13-projection @xmath2 must satisfy ( [ eqn : projection ] ) which can be rewritten as @xmath154 \\\\      & = & \\frac{1}{\\rho } \\log \\left [ \\sup_{h \\in \\hat{\\mathcal{e } } } \\int h g ~ d\\mu \\right ] , \\nonumber\\end{aligned}\\ ] ] where @xmath155 and @xmath156 , an element of the dual space @xmath157 .",
    "we now claim that @xmath158 assume the claim .",
    "since @xmath41 is a reflexive space , the closed and convex set @xmath159 is closed under the weak topology . since @xmath159",
    "is also contained in the unit sphere in @xmath41 , the unit sphere being compact in the weak topology in a reflexive space , @xmath159 must be compact in the weak topology .",
    "the supremum is thus of a bounded linear functional over the weakly compact set @xmath159 .",
    "it is therefore attained in @xmath159 . since the linear functional increases with @xmath160 , the supremum is attained with @xmath161 .",
    "thus the supremum in ( [ eqn : inftosup ] ) over @xmath162 is attained .",
    "we now proceed to show the claim ( [ eqn : claim - closed - convex ] ) .",
    "to see convexity , let @xmath163 and @xmath164 .",
    "then @xmath165 where @xmath166 by the convexity of @xmath98 . from minkowski s inequality ( for @xmath19 ) , we also have @xmath167 and this establishes the convexity of @xmath159 .    to see that @xmath159 is closed in @xmath41 , let @xmath168 be a cauchy sequence in @xmath41",
    ". then @xmath169 , with @xmath170 and @xmath171 , converges to some @xmath64 in @xmath41 . by taking norms",
    ", we see that @xmath172 .",
    "if @xmath173 a.e.@xmath124 $ ] , then @xmath174 by taking @xmath175 , and we are done .",
    "otherwise we can assume that @xmath176 for all @xmath113 by focusing on a subsequence if needed , and that @xmath177 .",
    "we can thus conclude that @xmath178 in @xmath41 . since @xmath179 , the same argument that showed ( [ eqn : norm - pn - bounded ] ) shows that @xmath180 is bounded , and by focusing on a subsequence , we may assume that it converges to some constant @xmath136 .",
    "hence @xmath181 in @xmath41 .",
    "since @xmath98 is closed , we must have @xmath182 for some @xmath183 , @xmath184 , and @xmath185 . since we already established that @xmath186 , it follows that @xmath174 .",
    "this completes the proof .",
    "we close this section with a result on the continuity or the lower semicontinuity of @xmath14-relative entropy .    for a fixed @xmath187 , consider @xmath188 as a function on @xmath41 .",
    "this function is continuous for @xmath189 and lower semicontinuous for @xmath190 .",
    "@xmath69    let us first consider the case when @xmath189 .",
    "let @xmath191 in @xmath41 .",
    "then @xmath192 and so @xmath193 in @xmath41 .",
    "as mentioned in the proof of theorem [ thm : min](b ) , @xmath52 is a monotone function of a bounded linear functional in @xmath71 .",
    "hence @xmath194 is continuous in @xmath195 .",
    "for @xmath196 we write @xmath197.\\ ] ] let @xmath191 in @xmath41 . then @xmath198",
    "and since @xmath199 , the generalized dominated convergence theorem yields @xmath200 i.e. , @xmath201 in @xmath122 .",
    "this is the same as saying @xmath202 in @xmath203 , and thus in @xmath204$]-measure .",
    "hence it follows that @xmath205 in @xmath204$]-measure . by fatou",
    "s lemma , @xmath206 as increasing function of a lower semicontinuous function is lower semicontinuous , the result is established for @xmath190 .",
    "in this section , we state the pythagorean property for @xmath14-relative entropy .",
    "we define the @xmath13-sphere with center @xmath93 and radius @xmath79 as @xmath207 .",
    "[ thm : pythagorean ] let @xmath208 and @xmath209 .",
    "let @xmath35 be a common dominating @xmath36-finite measure .    1 .",
    "[ item : inequality]if @xmath210 and @xmath211 are finite , `` the segment joining @xmath1 and @xmath2 '' does not intersect the @xmath13-sphere @xmath212 with radius @xmath213 , i.e. , @xmath214 for @xmath215\\ ] ] if and only if @xmath216 2 .   [",
    "item : equality ] if @xmath217 and @xmath211 is finite , then the segment joining @xmath1 and @xmath218 does not intersect @xmath212 with @xmath213 , if and only if @xmath219 and @xmath220 .",
    "@xmath69    for the proof(see appendix ) , we proceed as in @xcite where it is proved for the finite alphabet case , with appropriate functional analytic justifications for the general alphabet case .",
    "once theorem [ thm : pythagorean ] is established in generality , the proofs of the following results are exactly as in @xcite .",
    "the following statements hold .",
    "\\1 ) ( _ projection _ ) a @xmath221 is an @xmath13-projection of @xmath93 on the convex set @xmath88 iff every @xmath90 satisfies ( [ eqn : pythagoreaninequality ] ) .",
    "if the @xmath13-projection is an algebraic inner point of @xmath88 then @xmath222 and ( [ eqn : pythagoreaninequality ] ) holds with equality .",
    "\\2 ) ( _ uniqueness of @xmath13-projection",
    "_ ) if @xmath13-projection exists , it is unique .",
    "\\3 ) ( _ iterative projection _ ) let @xmath88 and @xmath223 be convex sets of probability measures , let @xmath93 have @xmath13-projection @xmath2 on @xmath88 and @xmath224 on @xmath225 , and suppose that ( [ eqn : pythagoreaninequality ] ) holds with equality for every @xmath100 .",
    "then @xmath224 is the @xmath13-projection of @xmath2 on @xmath225 .",
    "we studied a parametric extension of relative entropy @xmath13 for @xmath226 and @xmath30 .",
    "these arose naturally as redundancies under mismatched compression and when normalized cumulants of compression lengths are considered ( @xmath227 ) .",
    "we first studied @xmath13 minimization problems and showed that projections exist on convex and closed sets ( in @xmath228 when the sets are dominated by a @xmath36-finite measure @xmath35 .",
    "we then extended the pythagorean property to general measure spaces .",
    "as a consequence , one also gets an iterated projections property .",
    "axiomatic characterizations that lead to @xmath13 minimization and rnyi entropy maximization are currently under investigation .",
    "the first author was supported by a council for scientific and industrial research ( csir ) fellowship .",
    "the work was supported in part by the university grants commission by grant part ( 2b ) ugc - cas-(ph.iv ) .",
    "[ item : inequality ] ) we first prove statement [ item : inequality ] ) .",
    "we begin with the `` only if '' part . under the hypothesis",
    ", it suffices to show that @xmath229 now , @xmath230 therefore it suffices to show that @xmath231 now @xmath232 where @xmath233 clearly , @xmath214 for @xmath234 implies that @xmath235 therefore the limiting value as @xmath236 , the derivative of @xmath237 with respect to @xmath238 evaluated at @xmath239 , should be @xmath240 .",
    "we then have @xmath241\\\\     & = & \\text{sgn}(\\rho)~\\int \\left(\\frac{p_{\\lambda}-q}{\\lambda}\\right)~ ( r')^{-\\rho}d\\mu\\\\     & = & \\text{sgn}(\\rho)~\\int ( p - q)~(r')^{-\\rho}d\\mu\\\\&=&\\text{sgn}(\\rho)\\left[\\int p~(r')^{-\\rho}d\\mu-\\int q~(r')^{-\\rho}d\\mu\\right].\\end{aligned}\\ ] ] so @xmath242 exists and equals the above expression .",
    "for @xmath189 , we have @xmath243 while for @xmath190 , we have @xmath244 and both upper bounds are in @xmath122 for a fixed @xmath245 .",
    "therefore by chain rule and ( * ? ? ?",
    "* th . 2.27 )",
    ", we get @xmath246^{\\frac{1}{\\alpha}-1 } \\cdot \\int ( p_{\\lambda})^{\\alpha-1}(p - q ) d\\mu\\ ] ] for each @xmath245 . taking @xmath236 , we get @xmath247 thus @xmath248 } \\\\    & = & \\frac{1}{t(\\lambda)t(0 ) } \\left[t(0)\\frac{s(\\lambda)-s(0)}{\\lambda}-s(0)\\frac{t(\\lambda)-t(0)}{\\lambda}\\right].\\end{aligned}\\ ] ] it follows that the derivative of @xmath249 exists at @xmath239 and",
    "is given by @xmath250 .",
    "equation ( [ eqn : derivativeoff - divergence ] ) together with @xmath251 imply that @xmath252 consequently , @xmath253 is necessarily finite . substituting the values of @xmath254 and @xmath253 in ( [ eqn : nonnegativity ] )",
    "we get the required inequality ( [ eqn : equivalentpythagoreanidentity ] ) .    to prove the converse `` if '' part , let us assume that @xmath255 which is the same as ( [ eqn : equivalentpythagoreanidentity ] ) .",
    "it also implies that @xmath10 is also finite . from the trivial statement",
    "@xmath256 , we have @xmath257 a @xmath238-weighted linear combination of ( [ eqn : equivalentpythagoreanidentity ] ) and ( [ eqn : equality ] ) yields , @xmath258 i.e. , @xmath259    [ item : equality ] ) we next prove statement [ item : equality ] ) . from @xmath211 being finite , we claim that @xmath210 and @xmath260 are also finite . from ( [ eqn : convex combination ] ) , it is clear that @xmath261 and thus @xmath262 . as a consequence",
    ", we have @xmath263 integrating with respect to @xmath264 , we get @xmath265    taking the sign of @xmath32 appropriately , it immediately follows that @xmath266 finite constant , and is therefore finite .",
    "similarly @xmath260 is also finite . applying the first part of the theorem",
    ", we get @xmath267 if either of these were a strict inequality , then the linear combination @xmath268 will satisfy ( [ eqn : equality ] ) with strict inequality , a contradiction .",
    "so both the above must be equalities proving the `` only if '' part .",
    "the converse `` if '' part trivially follows from ( [ item : inequality ] ) . @xmath269"
  ],
  "abstract_text": [
    "<S> this paper extends some geometric properties of a one - parameter family of relative entropies . </S>",
    "<S> these arise as redundancies when cumulants of compressed lengths are considered instead of expected compressed lengths . </S>",
    "<S> these parametric relative entropies are a generalization of the kullback - leibler divergence . </S>",
    "<S> they satisfy the pythagorean property and behave like squared distances . </S>",
    "<S> this property , which was known for finite alphabet spaces , is now extended for general measure spaces . </S>",
    "<S> existence of projections onto convex and certain closed sets is also established . </S>",
    "<S> our results may have applications in the rnyi entropy maximization rule of statistical physics . </S>"
  ]
}