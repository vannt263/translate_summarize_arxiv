{
  "article_text": [
    "field - programmable gate arrays ( fpga ) offer a number of special options in computation . utilizing the unique properties of an fpga",
    ", some algorithms that are impractical to implement on a more traditional architecture can become both convenient to create and resource - efficient .",
    "the programmable array of look - up tables commonly found on an fpga provide both flexibility in creating logic to suit specific needs and naturally lend themselves to great parallelism in computations .",
    "fast operations on matrices are of great practical interest .",
    "ways to speed up certain matrix calculations still find their way into numerous applications .",
    "faster implementations of matrix algorithms can be achieved either from a `` software '' point of view , by improving upon the algorithm itself , or from a `` hardware '' point of view , by using faster or differently structured architectures .",
    "theoretical improvements on matrix algorithms include strassen s algorithm @xcite and the coppersmith - winograd algorithm @xcite .",
    "the naive algorithm for matrix multiplication is a well - known @xmath0 algorithm .",
    "strassen s algorithm uses an idea similar to the karatsuba - multiplication .",
    "it has a time complexity of @xmath1 by dividing the matrices into sub - matrices .",
    "then by multiplying them in a different arrangement , it manages an overall lower multiplication count compared to the classical algorithm .",
    "research implementing it on the cell broadband engine can be found in @xcite .",
    "strassen s algorithm and its applicability to the project is briefly discussed in section 7 .",
    "the coppersmith - winograd algorithm further improves the complexity to @xmath2 by combining the idea of strassen with the salem - spencer theorem .",
    "@xcite discusses and compares the performance of implementations of these algorithms .",
    "numerous research has been done on creating efficient realizations of different matrix operations on different architectures .",
    "@xcite and @xcite both use fpgas to perform matrix inversion .",
    "the design presented here is an implementation of matrix multiplication on an fpga .",
    "works of similar nature can be found in @xcite and @xcite , dealing with fpga configurations used for floating point matrix multiplication .",
    "@xcite uses an fpga design for digital signal processing .",
    "@xcite discusses another fpga implementation for accelerating matrix multiplication .",
    "the research in this paper is related to an algorithm for the construction of pseudo random number generators .",
    "it requires the exponentiation of large matrices to an extremely high power .",
    "this allows for numerous optimizations to be made on the fpga implementation , resulting in an extremely fast design .",
    "a speedup factor of @xmath3200 is achieved compared to a highly optimized program on a more traditional architecture .",
    "we give the details of a design implemented on a virtex-5 xc5vlx110 t fpga that multiplies two @xmath4 sized matrices .",
    "the matrices are defined over the mod 4 residue class ring . using this property and the fact that the hardware uses 6-luts ( lookup tables ) ,",
    "we describe first a module that computes the dot product of vectors taken from @xmath5 in a single clock cycle at 100mhz clock speed . with these modules",
    "we construct a matrix multiplier module that computes the @xmath6 product matrix of @xmath7 and @xmath8 in @xmath9 clock cycles at 100mhz .",
    "the significance of the value 28 in the implementation and its experimental determination is also discussed .",
    "finally , we describe how to use these modules for multiplying matrices taken from @xmath10 . the proposed algorithm deals with the management of stored data in such a way that it can be accomplished completely in parallel with the computations .",
    "the resulting design completes the multiplication in 64800 clock cycles at 100mhz .    future work for increasing the size of the used matrices , and further optimizing the design s performance using strassen s algorithm is also described .",
    "the present work is initiated by a method for the construction of uniformly distributed pseudo random number generators .",
    "( see @xcite . )",
    "the generator uses recurring sequences modulo powers of 2 of the form @xmath11 the theoretical background can be found in @xcite .",
    "the construction assumes that the values @xmath12 are such that @xmath13 holds for some @xmath14 irreducible polynomial .",
    "it is practical to choose @xmath14 to have maximal order , since the order of @xmath15 is closely related to the period length of the corresponding recurring sequence .",
    "the sequence @xmath16 obtained this way does not necessarily have uniform distribution , however exactly one of the following four sequences does : @xmath17 for the details see @xcite . finding the sequence with uniform distribution is of interest .",
    "let    @xmath18    be the companion matrix of sequence @xmath19 . to find which of the above sequences has a uniform distribution",
    ", we have to compute @xmath20 .",
    "if @xmath20 equals the identity matrix , then the period length of @xmath16 is @xmath21 , which means it is not the sequence we are searching for .",
    "the exponentiation of matrices to high powers can quickly become time consuming on traditional computers .",
    "the aim of the project was to utilize the special properties of an fpga to achieve a significant upgrade in speed compared to implementations on more traditional architectures .",
    "the project was implemented on a xilinx xupv505-lx110 t development platform .",
    "the board features a variety of ports for communication with the device . as a first approach",
    "the rs-232 serial port was used to send data between the board and a pc .",
    "a high - speed pci express connection is also available if the amount of data transferred would necessitate its use .",
    "the board s most prominent feature is the virtex-5 xc5vlx110 t fpga .",
    "the fpga s main tool for computation is the array of 6-input look - up tables , arranged into 17280 slices , with four look - up tables found in each slice , adding up to a total of 69120 luts .",
    "a single 6-input lut can store 64 bits of data , where its six input bits are used as an address to identify the single bit of data that is to be outputted . by manipulating the 64 bit content of the look - up table ,",
    "it can be configured to carry out arbitrary boolean functions with at most six input bits . in our design",
    "they are used to create luts performing a multiply - accumulate function , which are hierarchically arranged into larger and more complex modules .",
    "one out of four luts on the device can also be used as a 32 bit deep shift register ; these are the basis to implement containers storing the data , which is directly fed to the computational module .    attached to the board , there is a 256 mb ddr2 sodimm module , which is used for storing data exceeding the amount that can be practically stored on the fpga .",
    "the basic elements of the design are the luts denoted by @xmath22 , where @xmath23 and @xmath24 are two - digit binary numbers .",
    "the function carried out by @xmath25 is a multiply - accumulate ( for short : ma ) function , i.e. : @xmath26 let @xmath27 , @xmath28 , @xmath29 , @xmath30 , where @xmath31 @xmath32 , and @xmath33 where @xmath34 and @xmath35 are two single bit luts , according to the following : +    * @xmath36 * @xmath37    , width=304,height=115 ]    we remark that while @xmath38 needs only three input bits to accomplish its function , @xmath39 requires all six bits of input .",
    "the luts @xmath35 and @xmath34 were configured to the values shown in table 1 and table 2 to perform the multiply - accumulate function .",
    "| c | c | c | & 0 & 1 + ( 0,0 ) & 0 & 1 + ( 0,1 ) & 0 & 1 + ( 1,0 ) & 0 & 1 + ( 1,1 ) & 1 & 0 +     | c | c | c | c | c | & 0 & 1 & 2 & 3 + ( 0,0 ) & 0 & 0 & 1 & 1 + ( 0,1 ) & 0 & 0 & 1 & 1 + ( 0,2 ) & 0 & 0 & 1 & 1 + ( 0,3 ) & 0 & 0 & 1 & 1 + ( 1,0 ) & 0 & 0 & 1 & 1 + ( 1,1 ) & 0 & 1 & 1 & 0 + ( 1,2 ) & 1 & 1 & 0 & 0 + ( 1,3 ) & 1 & 0 & 0 & 1 + ( 2,0 ) & 0 & 0 & 1 & 1 + ( 2,1 ) & 1 & 1 & 0 & 0 + ( 2,2 ) & 0 & 0 & 1 & 1 + ( 2,3 ) & 1 & 1 & 0 & 0 + ( 3,0 ) & 0 & 0 & 1 & 1 + ( 3,1 ) & 1 & 0 & 0 & 1 + ( 3,2 ) & 1 & 1 & 0 & 0 + ( 3,3 ) & 0 & 1 & 1 & 0 +    with the help of these basic units one can compute the dot product @xmath40 of two vectors @xmath41 and @xmath42 . let us define a module @xmath43,l[1],\\ldots , l[n-1])$ ] by cascading @xmath44 ma units denoted by @xmath45 $ ] .",
    "in this module @xmath46 we use the output of a given ma unit as the sum input of the next unit , i.e. @xmath47 for @xmath48 , where @xmath49 and @xmath50 are the @xmath24 input and @xmath51 output of @xmath45 $ ] .    , width=432,height=115",
    "]    therefor @xmath46 is a function that accepts a pair of vectors @xmath52 of two - digit numbers of length @xmath44 and outputs on @xmath53 the two - digit dot - product of the two vectors , i.e. @xmath54 .    in total , the number of luts used in @xmath46 is @xmath55 .",
    "note that vectors of arbitrary length can be used in the computation if we connect the output of module @xmath46 to the sum input of @xmath56 $ ] ( @xmath57 ) , and then iteratively shift @xmath19 and @xmath58 onto the module s input by @xmath44 elements at a time : +   +   +    function = @xmath59  // @xmath60 + 1 .",
    "define @xmath61 , @xmath62 + 2 . for = @xmath63 to @xmath64",
    "fill @xmath58 and @xmath19 with 0 s + 3 . if @xmath65 then @xmath66 else @xmath67 + 4 . if @xmath65 then @xmath68 else @xmath69 + 5 .",
    "end for + 6 .",
    "define @xmath70 , let @xmath71 + 7 . for @xmath63 to @xmath72 do  //",
    "shift @xmath73 and @xmath74 to @xmath75 and @xmath76 + 8 .",
    "@xmath77 + 9 .",
    "@xmath78 + 10 .",
    "@xmath79 + 11 .",
    "end for + 12 .",
    "return @xmath40 + end function +    here @xmath74 and @xmath73 are the extensions of @xmath19 and @xmath58 by 0 s .",
    "we shall see that the number chosen for @xmath44 is critical in setting many characteristics of the entire project .",
    "the experiment used for determining @xmath44 will be discussed in the following chapter .",
    "our aim is to obtain a module that performs the matrix multiplication of @xmath80 , where @xmath81 is the mod 4 residue class ring . in the following ,",
    "let @xmath82 be the output matrix , such that @xmath83 .",
    "furthermore , let @xmath84 be the @xmath85th row of matrix @xmath86 and let @xmath87 be the @xmath88th column of matrix @xmath89 .",
    "the multiplier units denoted by @xmath46 are used to create more complex modules in a hierarchical manner .",
    "first , by taking ten @xmath46 multiplier blocks we create a row of multipliers @xmath90 .",
    "this is used to compute ten consecutive elements of a single row of the output matrix : @xmath91 where @xmath92 .",
    "the input vector @xmath93 is used by all ten multiplier units of @xmath94 .",
    "the length of these vectors , as mentioned above , can be arbitrary , but vectors of length greater than @xmath44 will need to be iteratively shifted to the input of @xmath94 .    by taking ten row multipliers we can create a unit @xmath95 which outputs a @xmath96 sub - matrix of @xmath97 :    @xmath98    @xmath99    finally , four such units are arranged so that a @xmath100 sub - matrix of @xmath97 could be obtained as output :    @xmath101 @xmath102 the @xmath103 s inputs are twenty vectors from both matrices @xmath86 and @xmath89 .",
    "because of hardware constraints  in particular the number of luts on the used device  a larger arrangement of multipliers would be impractical to implement . the module @xmath103 is comprised of 400 @xmath46 multiplier units .",
    "figure 3 shows the hierarchy of units used to build @xmath103 .",
    ", width=375,height=174 ]    the @xmath103 unit can be used iteratively to multiply matrices of arbitrary size , producing @xmath100 sub - matrices of the output matrix @xmath97 with each iteration .",
    "after inputting twenty rows from matrix @xmath86 and twenty columns from matrix @xmath89 and obtaining the desired output , we can simply repeat the process for a set of rows and columns of @xmath86 and @xmath89 respectively , until we obtain the entire output matrix @xmath97 :    function = @xmath104 + 1 .",
    "define @xmath105 , @xmath106 + 2 . for = @xmath63 to @xmath107",
    "do +  3 . for @xmath108 to @xmath107 do + 4 .",
    "if @xmath65 and @xmath109 @xmath110 else @xmath111 + 5 .",
    "if @xmath65 and @xmath109 @xmath112 else @xmath113 + 6 .",
    "end for end for + 7 . for @xmath63 to @xmath114 do +  8 .",
    "for @xmath108 to @xmath114 do + 9 .",
    "@xmath115=m_{20 \\times 20}(a_{i},a_{i+1},\\ldots , a_{i+19},b_{j},b_{j+1},\\ldots , b_{j+19})$ ] + 10 .",
    "end for end for + 11 .",
    "return @xmath116 $ ] + end function +    here @xmath117= \\left ( \\begin{matrix } c'_{i , j } & c'_{i , j+1 } & \\cdots & c'_{i , l } \\\\ c'_{i+1,j } & c'_{i+1,j+1 } & \\cdots & c'_{i+1,l } \\\\",
    "\\vdots & \\ddots \\\\ c'_{k , j } & c'_{k , j+1 } & \\cdots & c'_{k , l } \\\\",
    "\\end{matrix } \\right).\\ ] ]    note that in the naive algorithm @xmath104 , during the main loop ( lines 7 - 10 ) , for each twenty rows read from @xmath86 , the entire matrix @xmath89 is read . during the whole procedure , matrix @xmath86 will be read entirely exactly once , while matrix @xmath89 will be read @xmath118 times .",
    "methods improving on this number are described in section 6 .    since for almost all practical cases the size @xmath119 of matrices @xmath80 will be greater than the parameter @xmath44 , the vectors taken from these matrices will need to be iteratively shifted onto the input of the multiplier @xmath103 , @xmath44 elements at a time .",
    "therefore , an efficient way to both store and then use the vectors taken from the matrices is the creation of fifo type containers made of shift registers .",
    "let @xmath120 be a shift register of width @xmath44 and depth @xmath9 .",
    "it means that @xmath120 can store at most @xmath9 vectors of length @xmath44 , or equivalently a single vector of length at most @xmath121 .",
    "we choose @xmath9 such that @xmath122 , thus it can store one row or column from the input matrices @xmath86 or @xmath89 . let the vector filling @xmath120 be @xmath123 , where @xmath124 . in practice",
    ", @xmath120 is a queue data structure . in a single step ,",
    "@xmath120 outputs a vector of length @xmath44 and shifts its content by @xmath44 places . for the @xmath125 activation , the container will output @xmath126 .",
    "after @xmath9 activations , the container becomes empty .",
    "one container @xmath120 is used to store a single row or column of matrices @xmath86 or @xmath89 respectively .",
    "connecting twenty of them in parallel , denoted by @xmath127,t_{n}^{d}[1],\\ldots , t_{n}^{d}[19])$ ] , we obtain a container that stores twenty rows or columns .",
    "this is exactly the amount of data the @xmath103 multiplier structure requires as input in @xmath9 iteration steps .",
    "after @xmath9 activations @xmath128 has shifted all its stored data to @xmath103 , broken up into pieces of length @xmath44 for each activation .",
    "two such @xmath128 containers are connected to @xmath103 , one for the rows taken from matrix @xmath86 and one for the columns taken from matrix @xmath89 .",
    ", width=336,height=153 ]    using @xmath103 and @xmath129 in a proper structure , we can execute one iteration cycle of the computation . after filling one @xmath128 container with the desired twenty rows from matrix @xmath86 and one @xmath128 container with the desired twenty columns from matrix @xmath89",
    ", we simply send @xmath9 activation signals to the containers .",
    "this will shift the data onto @xmath103 , which computes the @xmath100 product matrix in the way described in function @xmath59 .",
    "the number of steps in one iteration cycle is @xmath9 .",
    "now , we turn to the determination of @xmath44 ( how many ma modules should be connected into a single multiplier @xmath46 ) .",
    "this sets the length of the vectors that we use in the computation in a single step and thus has an effect on many other technical parameters of the design .",
    "the goal was to find the greatest number such that the multiplier would still reliably produce the correct dot product in a single clock cycle .",
    "clearly , this number dependents on the used hardware and the clock frequency .",
    "for the device used , the chosen clock frequency was 100 mhz , the default frequency provided by the board .",
    "the following experiment was devised to determine the value of @xmath44 :    let @xmath130 be a multiplier @xmath46 , called the `` subject '' , and let @xmath131 be ten more @xmath46 multipliers , called the `` examiners '' .",
    "informally , the examiners duty was to verify the answers given by the subject to questions they already knew the answer to .",
    "the `` questions '' here are test data : two vectors @xmath132 of length @xmath44 generated by the following sequence to obtain suitable pseudo - random values : @xmath133 where @xmath134",
    ".    more formally , let @xmath135 be a counter that cycles between values @xmath136 , incrementing its value by one with each clock cycle , and returning to value @xmath137 after @xmath138 . for each clock cycle during the experiment ,",
    "the following happens depending on the value of @xmath139 :    * the output of @xmath130 is checked for equality with the output of @xmath140 . if inequality is detected , then an error is noted .",
    "* the test data @xmath141 is currently working on is given to @xmath130 .",
    "* new test data is given to @xmath142 .",
    "procedure = testing + 1 .",
    "let @xmath143 be @xmath46 multipliers + 2 .",
    "let @xmath144 be the test data generator + 3 .",
    "let @xmath145 + 4 .",
    "forever = do + 5 .",
    "@xmath146 + 6 . @xmath147 + 7 .",
    "if @xmath148 then return error + 8 . @xmath149 + 9 .",
    "@xmath150 + 10 .",
    "end forever + end procedure +    , width=432,height=278 ]    note that the output of @xmath130 is checked every clock cycle , which yields that @xmath130 has only a single cycle to calculate its answer to the question it was given in the preceding clock cycle .",
    "a given examiner , however , has ten times more time to work on its test data .",
    "once in every ten clock cycles , new data is given to the examiner to work on , and its output is only checked nine clock cycles later , just before it is given new input again . this way",
    "the examiners have enough time to compute the correct answer to the question by the time it is needed .    as the initial value for @xmath44 ,",
    "we have chosen 16 , a number small enough to be reasonably expected to pass the criteria set for @xmath44 , but large enough to be of interest . if the experiment reported no error , meaning the subject was flawlessly able to calculate the dot product for a sufficiently long time , then the value of @xmath44 was increased and the experiment repeated .",
    "after the first error was encountered , meaning the subject was not able to keep up with the calculations , the largest value was chosen for @xmath44 for which there were no errors .",
    "on the used device , the largest such value was found to be @xmath151 at a clock speed of 100 mhz and setting the length of @xmath46 multipliers to @xmath152 were able to work error - free for days without interruption .",
    "in the section 4 we gave an algorithm for using the described modules for computing the product of large matrices .",
    "following the description , the implemented design would make use of the parallelism offered by the fpga only in the computation of dot products .",
    "making further use of parallel operations , the design s performance can be significantly improved . in this section",
    "we describe the implementation choices made to raise the overall performance .    the biggest factor to consider is the management of data .",
    "when computing the product of large matrices , the amount of data to store and to move between the computation modules can easily exceed the size which can be practically stored on the fpga .",
    "fortunately , as mentioned before , a 256 mb ddr2 sodimm is connected to the board as the main data storage device .",
    "a module is generated using the memory interface generator v3.5 intellectual property core provided by xilinx to implement the logic needed to communicate with the ddr2 ram .",
    "the module is structured hierarchically , connecting the memory device to a user interface . all communication with the device",
    "is done through two fifo queues : one queue to send the command and address signals , while the other queue is used for write data and write data mask ( when masking is allowed ) .",
    "a naive utilization of the memory would be to simply read the required data before each iteration of the computation , and writing the output back after it is finished .",
    "an undesirable effect of this approach would be that the design would spend significantly more time with memory management than with the actual computation",
    ". the desired result would be that memory management ( and all other auxiliary operations ) were done during the time interval of the computation .",
    "note that since both the size of the matrices and the multiplier module is fixed , the time the multiplication consumes is a fixed constant , which can not be lowered .",
    "optimally , the time of the computation should be an upper bound for the running time of the entire design .",
    "the difficulty of reaching this optimum lies in the high speed of the multiplier modules compared to the memory module .",
    "one way to resolve the problem caused by slow transmission speed is to increase the amount of data stored on the fpga .",
    "informally , the main idea is to keep enough data in a prepared state , i.e. by the time the multiplier module finishes all of its computations , we have enough new data to continue working . more formally ,",
    "let us define the following quantities :    * let @xmath9 be the time necessary to complete one iteration of the computation . as described in the previous sections , this is equal to the depth of the containers @xmath128 .",
    "* let @xmath105 , where @xmath119 is the size of the matrices .",
    "( @xmath153 ) this quantity is already used in algorithm @xmath154 . for the rest of the section ,",
    "it is practical to think of @xmath86 and @xmath89 as @xmath155 sized block matrices , where each element is a @xmath100 matrix .",
    "* let @xmath156 be an arbitrary algorithm executing matrix multiplication on @xmath86 and @xmath89 , including the memory management needed for the computation .",
    "let @xmath157 be the number of times the algorithm needs to fill a @xmath128 container , i.e. the number of times it has to read twenty rows or columns from the matrices .",
    "note that completely reading either input matrices once means filling @xmath128 containers @xmath118 times , since one @xmath128 can store twenty rows or columns at a time .",
    "s main loop ( starting at line 7 ) reads twenty rows from matrix @xmath86 ( filling a @xmath128 once ) and reads matrix @xmath89 entirely for each step .",
    "since the loop has @xmath118 steps , it follows that @xmath158 .",
    "* let @xmath159 be the time it takes to fill a @xmath128 container .",
    "this quantity depends on both the width and depth of the container .",
    "the total time @xmath156 spends on reading from memory to fill the containers is @xmath160 .",
    "* let @xmath161 be the total time the design has to spend with memory management .",
    "this is the sum of the time it spends on reading matrices @xmath86 and @xmath89 from the memory and the time it spends on writing the product matrix @xmath97 into the memory .",
    "the number of times @xmath162 has to read @xmath86 and @xmath89 from the memory depends on @xmath162 .",
    "note that since the size of the total output matrix @xmath97 is the same as the size of @xmath86 and @xmath89 , writing @xmath97 into the memory takes time equal to reading either matrices once from the memory .",
    "in other words , it takes @xmath163 time . the total time the design has to spend with memory management",
    "is @xmath164 .",
    "* let @xmath165 be the time @xmath156 spends on the computation itself . from the definition of @xmath9 and @xmath118 it follows that @xmath166 .",
    "the goal here is to reduce @xmath157 in such a way that the data required for the next iteration of the computation is always ready by the time the previous iteration ends .",
    "if this arrangement is achieved then @xmath167 becomes the upper bound for the running time of the design .",
    "storing more data on the fpga can be done by adding more @xmath128 containers to the design . during an iteration",
    "only two such containers are used directly .",
    "the rest can be used to load data necessary for the forthcoming iteration steps .",
    "suppose the design has @xmath168 pieces of @xmath128 containers .",
    "we assign @xmath169 of the containers to store rows from matrix @xmath86 , called `` row - stores '' , and two of them to store columns from matrix @xmath89 , called `` column - stores '' . with this arrangement",
    ", we can carry out @xmath170 iterations of the computation , using up the data stored in @xmath170 row - stores and one column - store .",
    "this leaves one row - store and one column - store to load new data into during the computation . using the above definitions",
    ", the allover computation takes @xmath171 time .",
    "if we use all @xmath169 row - stores and one column - store for the computation while the remaining column - store is devoted to loading new columns into , then we would have to load all @xmath169 row - stores with new rows once we read all the columns before we can continue the computation .",
    "this would take @xmath172 time for each case where we read all the columns but havent read all the rows yet , which happens @xmath173 times . in total",
    ", it would add @xmath174 to the running time .",
    "instead , the computation of the output matrix moves slightly diagonally . see figure 7 .",
    "the @xmath170 row - stores used in the computations store a total of @xmath175 rows .",
    "initially , the row - stores are filled with rows @xmath176 .",
    "new rows are loaded in at a slower pace than columns are . by the time",
    "all columns are read once , the contents of the row - stores have shifted exactly to the next segment of data needed , the next @xmath177 rows .",
    "after matrix @xmath89 is completely read once , the row - stores are filled with rows @xmath178 .",
    "reading rows and columns proceeds in this manner until we ve completely read matrix @xmath86 once .",
    "for this reason , it is practical to choose @xmath169 such that @xmath179 .",
    "all together we read matrix @xmath89 @xmath180 times and matrix @xmath86 once . during each @xmath170 iterations shown in figure 6 , twenty new columns and @xmath181 new rows",
    "are loaded into the column - store and row - store currently unused by the computation .",
    "when the unused row - store is filled with twenty new rows , it becomes active , to be used in the following iterations .",
    "the row - store containing the rows with the least index becomes inactive in the computation and starts accepting the new rows read .",
    ", width=451,height=249 ]    function = @xmath182 + 1 .",
    "define @xmath183 , @xmath106 + 2 . for = @xmath63 to @xmath107 do + 3 .",
    "for @xmath108 to @xmath107 do + 4 . if @xmath65 and @xmath109 then @xmath110 else @xmath111 + 5 . if @xmath65 and @xmath109 then @xmath112 else @xmath113 + 6 . end for end for + 7 .",
    "fill the row - stores with rows @xmath184 + 8 .",
    "fill the column - stores with columns @xmath185 + 9 .",
    "for @xmath186 to @xmath187 + do in parallel : = @xmath188perform @xmath170 iterations of the computation + @xmath188read the next 20 columns mod @xmath189 + @xmath188read the next @xmath181 rows mod @xmath189 + @xmath188write the result of the previous @xmath170 iterations + 11 .",
    "return @xmath116 $ ] + end function +    the possible values for the parameters used in this section depend on the used hardware .",
    "the size of the matrices used in the implementation are determined by parameters @xmath152 and @xmath190 .",
    "the luts on the device that comprise the @xmath128 containers can be configured as @xmath190 bit deep shift registers . for this reason the matrices are of size @xmath4 .",
    "rows with length @xmath191 are the largest that can be stored in containers that are one lut deep , making them any larger would double the number of luts needed for creating a @xmath128 . because of the limited number of luts which can be used for storage purposes , @xmath192 was chosen .",
    "this yields that twelve @xmath128 containers are defined in the design .",
    "dealing with matrices larger than @xmath191 is part of future work .    for convenience ,",
    "time quantities are measured in clock cycles at 100mhz , the clock speed of the @xmath103 multiplier .",
    "the value of @xmath159 depends on the ddr2 ram used .",
    "the device was used at 200mhz , and has a 64 bit wide physical data bus .    from these values",
    "we determine the following parameters :    * @xmath193 , * @xmath194 , * @xmath195 clock cycles at 100mhz , * @xmath196 clock cycles at 100mhz , * @xmath197 clock cycles at 100mhz .",
    "the goal of @xmath198 is achieved , meaning that the running time of the design is equal to the time used by the computation .",
    "the speedup provided by the configuration can be shown by comparing its performance to a similar implementation created on a more traditional architecture .",
    "a highly optimized c++ program was created for a machine using an intel e8400 3ghz dual core processor with 2 gb ram .",
    "the algorithm is strongly specialized for the task , making use of all available options for increasing performance .",
    "it uses 64 bit long variables to perform multiplication on 16 pairs of two - digit elements at once in parallel on both processor cores .",
    "the running time of the multiplication of matrices of the same size is over 100 ms .",
    "the fpga implementation , as mentioned above , achieves a runtime of @xmath30.6 ms . on average ,",
    "a speedup factor of 200 is reached using the described fpga design .",
    "the future course of research will focus on increasing the size of the used matrices .    as mentioned in the previous section , simply increasing the depth @xmath9 of the @xmath128 containers would be impractical . since a single lut on the device can only be configured as a 32 bit deep shift register , setting @xmath199 would double the number of luts needed for a @xmath128 , and the design is already using well over half of the device s luts that can be configured this way ( 13440 out of 17280 , to be exact ) . increasing the size of the matrices this way would require the restructuring of both the multiplier module and the algorithm used for memory management .    instead , the currently implemented module can be used as a basic unit for the multiplication of larger matrices .",
    "then the entries of the large matrices are @xmath4 blocks .",
    "this also allows for further optimization using strassen s algorithm .",
    "suppose we double the matrix sizes , interpreting them as matrices with four blocks . using the classical algorithm , multiplying two @xmath200 sized matrices would take eight multiplication of the blocks . using a divide - and - conquer strategy",
    ", we can exchange one multiplication for a few extra additions .",
    "@xmath201 \\cdot \\left",
    "[ \\begin{matrix } b_{11 } & b_{12 } \\\\ b_{21 } & b_{22 } \\\\",
    "\\end{matrix } \\right ] = \\left [ \\begin{matrix } -d_2+d_4+d_5+d_6 & d_1+d_2 \\\\ d_3+d_4 & d_1-d_3+d_5-d_7 \\\\",
    "\\end{matrix } \\right],\\ ] ] where @xmath202 this algorithm , with its @xmath1 time complexity , could speed up the design on large matrices .",
    "we should note however , that the speed of the extra additions have to be carefully considered . since the multiplication is already extremely fast",
    ", a similar improvement may also be necessary for additions if the overall performance upgrade is to remain significant .",
    "research supported by the tmop 4.2.1/b-09/1/konv-2010 - 0007 project and taripar3 project grant nr .",
    "tech 08-a2/2 - 2008 - 0086 .",
    "d. http://www.arnetminer.org/viewperson.do?naid=1131379&name=d.%2520coppersmith[coppersmith ] , s. winograd , matrix multiplication via arithmetic progressions , http://www.elsevier.com/wps/find/journaldescription.cws_home/622902/description[_j .",
    "symbolic comput .",
    "_ ] * 9 , * 3 ( 1990 ) 251280 .    n. http://arnetminer.org/viewperson.do?naid=1089695[dave ] , k. http://arnetminer.org/viewperson.do?naid=1407331[fleming ] , m. king , m. pellauer , m. vijayaraghavan , hardware acceleration of matrix multiplication on a xilinx fpga ,",
    "_ memocode 07 proc .",
    "5th ieee / acm international conference on formal methods and models for co - design , _",
    "nice , france , 2007 , pp .",
    "97100 .",
    "y. dou , s. http://ce.et.tudelft.nl/person.php?id=2[vassiliadis ] , g. k. kuzmanov , g. n. gaydadjiev , 64-bit floating - point fpga matrix multiplication , _ proc .",
    "2005 acm / sigda 13th international symposium on field - programmable gate arrays _ , monterey , ca , usa , 2005 , pp . 8695 .",
    "t. http://www.inf.unideb.hu/~herendi/[herendi ] , uniform distribution of linear recurrences modulo prime powers , http://www.elsevier.com/wps/find/journaldescription.cws_home/622831/description[_j .",
    "finite fields _ ] _ appl . _ * 10 , * 1 ( 2004 ) 123 .",
    "a. http://ucsd.academia.edu/aliirturk/about[irturk ] , s. mirzaei , r. kastner , an efficient fpga implementation of scalable matrix inversion core using qr decomposition , _ ucsd technical report _ ,",
    "cs2009 - 0938 , 2009 .",
    "b. http://cs.stanford.edu/people/boyko/[kakaradov ] , ultra - fast matrix multiplication , an empirical analysis of highly optimized vector algorithms , _ stanford undergraduate _",
    "http://surj.stanford.edu/2004/pdfs/kakaradov.pdf[_research journal _ ] * 3 * ( 2004 ) 3336 .",
    "m. http://www.ece.rice.edu/~marjan/[karkooti ] , j. r. http://www.ece.rice.edu/~cavallar/[cavallaro ] , c. dick , fpga implementation of matrix inversion using qrd - rls algorithm , _ proc .",
    "39th asilomar conference on signals , systems , and computers _ , pacific grove , ca , usa , 2005 , pp .",
    "16251629 .",
    "s. m. qasim , a. a. http://faculty.ksu.edu.sa/atelba/[telba ] , a. y. almazroo , fpga design and implementation of matrix multiplier architectures for image and signal processing applications , _ ijcsns international journal of _",
    "http://www.ijcsns.org/[_computer science _ ] _ and network security _ * 10 , * 2 ( 2010 ) 168176 .    v. http://www.math.uni-konstanz.de/~strassen/[strassen ] , gaussian elimination is not optimal , http://www.springer.com/mathematics/numerical+and+computational+mathematics/journal/211[_numer .",
    "_ ] * 13 * ( 1969 ) 354356 ."
  ],
  "abstract_text": [
    "<S> we describe an efficient fpga implementation for the exponentiation of large matrices . the research is related to an algorithm for constructing uniformly distributed linear recurring sequences . </S>",
    "<S> the design utilizes the special properties of both the fpga and the used matrices to achieve a very significant speedup compared to traditional architectures . </S>"
  ]
}