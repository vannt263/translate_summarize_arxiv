{
  "article_text": [
    "word sense disambiguation is a crucial task in many nlp applications , such as machine translation  @xcite , parsing  @xcite and text retrieval  @xcite .",
    "given the growing utilization of machine readable texts , word sense disambiguation techniques have been variously used in corpus - based approaches  @xcite .",
    "unlike rule - based approaches , corpus - based approaches release us from the task of generalizing observed phenomena in order to disambiguate word senses .",
    "our system is based on such an approach , or more precisely it is based on an example - based approach  @xcite . since this approach requires a certain number of examples of disambiguated verbs , we have to carry out this task manually , that is , we disambiguate verbs appearing in a corpus prior to their use by the system .",
    "a preliminary experiment on ten japanese verbs showed that the system needed on average about one hundred examples for each verb in order to achieve 82% of accuracy in disambiguating verb senses . in order to build an operational system ,",
    "the following problems have to be taken into account :    1 .",
    "since there are about one thousand basic verbs in japanese , a considerable overhead is associated with manual word sense disambiguation .",
    "2 .   given human resource limitations ,",
    "it is not reasonable to manually analyze large corpora as they can provide virtually infinite input .",
    "3 .   given the fact that example - based natural language systems , including our system , search the example - database ( database , hereafter ) for the most similar examples with regard to the input , the computational cost becomes prohibitive if one works with a very large database size  @xcite .",
    "all these problems suggest a different approach , namely to _ select _ a small number of optimally informative examples from a given corpora .",
    "hereafter we will call these examples `` samples . ''",
    "our method , based on the utility maximization principle , decides on which examples should be included in the database .",
    "this decision procedure is usually called _",
    "selective sampling_. selective sampling directly addresses the first two problems mentioned above .",
    "the overall control flow of systems based on selective sampling can be depicted as in figure [ fig : concept ] , where `` system '' refers to dedicated nlp applications .",
    "the sampling process basically cycles between the execution and the training phases . during the execution phase ,",
    "the system generates an interpretation for each example , in terms of parts - of - speech , text categories or word senses . during the training phase",
    ", the system selects samples for training from the previously produced outputs . during this phase",
    ", a human expert provides the correct interpretation of the samples so that the system can then be trained for the execution of the remaining data .",
    "several researchers have proposed such an approach .",
    "lewis et al . proposed an example sampling method for statistics - based text classification  @xcite . in this method",
    ", the system always selects samples which are not certain with respect to the correctness of the answer .",
    "dagan et al .",
    "proposed a committee - based sampling method , which is currently applied to hmm training for part - of - speech tagging  @xcite .",
    "this method selects samples based on the training utility factor of the examples , i.e. the informativity of the data with respect to future training .",
    "however , as all these methods are implemented for statistics - based models , there is a need to explore how to formalize and map these concepts into the example - based approach .    with respect to problem 3 , a possible solution would be the generalization of redundant examples  @xcite .",
    "however , such an approach implies a significant overhead for the manual training of each example prior to the generalization .",
    "this shortcoming is precisely what our approach allows to avoid : reducing both the overhead as well as the size of the database .",
    "section [ sec : vader ] briefly describes our method for a verb sense disambiguation system .",
    "the next section [ sec : sampling ] elaborates on the example sampling method , while section [ sec : eval ] reports on the results of our experiment . before concluding in section [ sec : conclusion ] , discussion is added in section [ sec : discussion ] .",
    "[ cols= \" < ,",
    "< , < \" , ]     [ tab : corpus ]    we at first estimated the system s performance by its precision , that is the ratio of the number of correct outputs , compared to the number of inputs . in this experiment , we set @xmath0 in equation ( [ eq : certainty ] ) , and @xmath1 in equation ( [ eq : utility_temp ] ) .",
    "the influence of ccd , i.e. @xmath2 in equation ( [ eq : ccd ] ) , was extremely large so that the system virtually relied solely on the sim of the case with the greatest ccd .",
    "figure [ fig : precision ] shows the relation between the size of the training data and the precision of the system . in figure [ fig : precision ] , when the x - axis is zero , the system has used only the seeds given by ipal .",
    "it should be noted that with the final step , where all examples in the training set have been provided to the database , the precision of both methods is equal . looking at figure [ fig : precision ] one can see that the precision of random sampling was surpassed by our training utility sampling method .",
    "it solves the first two problems mentioned in section [ sec : intro ] .",
    "one can also see that the size of the database can be reduced without degrading the system s precision , and as such it can solve the third problem mentioned in section [ sec : intro ] .",
    "we further evaluated the system s performance in the following way .",
    "integrated with other nlp systems , the task of our verb sense disambiguation system is not only to output the most plausible verb sense , but also the interpretation certainty of its output , so that other systems can vary the degree of reliance on our system s output .",
    "the following are properties which are required for our system :    * the system should output as many correct answers as possible , * the system should output correct answers with great interpretation certainty , * the system should output incorrect answers with diminished interpretation certainty .",
    "motivated by these properties , we formulated a new performance estimation measure , pm , as shown in equation ( [ eq : performance ] ) .",
    "a greater accuracy of performance of the system will lead to a greater pm value .",
    "@xmath3 in equation ( [ eq : performance ] ) , @xmath4 is the maximum value of the interpretation certainty , which can be derived by substituting the maximum and the minimum interpretation score for @xmath5 and @xmath6 , respectively , in equation ( [ eq : certainty ] ) .",
    "following table [ tab : kuro ] , we assign 11 and 0 to be the maximum and the minimum of the interpretation score , and therefore @xmath4 = 11 , disregarding the value of @xmath7 in equation ( [ eq : certainty ] ) .",
    "@xmath8 is the total number of the inputs and @xmath9 is a coefficient defined as in equation ( [ eq : delta ] ) .",
    "@xmath10 in equation ( [ eq : delta ] ) , @xmath11 is the parametric constant to control the degree of the penalty for a system error . for our experiment , we set @xmath12 , meaning that pm was in the range @xmath13 to 1 .",
    "figure [ fig : performance ] shows the relation between the size of the training data and the value of pm . in this experiment",
    ", it can be seen that the performance of random sampling was again surpassed by our training utility sampling method , and the size of the database can be reduced without degrading the system s performance .",
    "in this section , we will discuss several remaining problems .",
    "first , since in equation ( [ eq : utility ] ) , the system calculates the similarity between @xmath14 and each example in @xmath15 , computation of @xmath16 becomes time consuming . to avoid this problem , a method used in efficient database search techniques  @xcite , in which the system can search some neighbour examples of @xmath14 with optimal time complexity , can be potentially used .",
    "second , there is a problem as to when to stop the training : that is , as mentioned in section [ sec : intro ] , it is not reasonable to manually analyze large corpora as they can provide virtually infinite input .",
    "one plausible solution would be to select a point when the increment of the total interpretation certainty of remaining examples in @xmath15 is not expected to exceed a certain threshold .",
    "finally , we should also take the semantic ambiguity of case fillers ( noun ) into account .",
    "let us consider figure [ fig : uncertain ] , where the basic notation is the same as in figure [ fig : certainty ] , and one possible problem caused by case filler ambiguity is illustrated .",
    "let `` x1 '' and `` x2 '' denote different senses of a case filler `` x. '' following the basis of equation ( [ eq : certainty ] ) , the interpretation certainty of `` x '' is small in both figure [ fig : uncertain - a ] and [ fig : uncertain - b ] .",
    "however , in the situation as in figure [ fig : uncertain - b ] , since ( a ) the task of distinction between the _ verb _ senses 1 and 2 is easier , and ( b ) instances where the sense ambiguity of case fillers corresponds to distinct verb senses will be rare , training using either `` x1 '' or `` x2 '' will be less effective than as in figure [ fig : uncertain - a ] .",
    "it should also be noted that since _ bunruigoihyo _ is a relatively small - sized thesaurus and does not enumerate many word senses , this problem is not critical in our case .",
    "however , given other existing thesauri like the edr electronic dictionary  @xcite or wordnet  @xcite , these two situations should be strictly differentiated .",
    "[ fig : uncertain - a ]    [ fig : uncertain - b ]",
    "in this paper we proposed an example sampling method for example - based verb sense disambiguation .",
    "we also reported on the system s performance by way of experiments .",
    "the experiments showed that our method , which is based on the notion of training utility , has reduced the overhead for the training of the system , as well as the size of the database .    as pointed out in section [ sec : intro ] , the generalization of examples  @xcite is another method for reducing the size of the database .",
    "whether coupling these two methods would increase overall effectivity is an empirical matter requiring further exploration .",
    "future work will include more sophisticated methods for verb sense disambiguation and methods of acquiring seeds , the acquisition of which is currently based on an existing dictionary .",
    "we will also build an experimental database for natural language processing using our example sampling method .",
    "the authors would like to thank dr .",
    "manabu okumura ( jaist , japan ) , mr .",
    "timothy baldwin ( titech , japan ) , and dr .",
    "michael zock and dr .",
    "dan tufis ( limsi , france ) for their comments on an earlier version of this paper ."
  ],
  "abstract_text": [
    "<S> this paper proposes an efficient example selection method for example - based word sense disambiguation systems . to construct a practical size database , a considerable overhead for manual sense disambiguation </S>",
    "<S> is required . </S>",
    "<S> our method is characterized by the reliance on the notion of the training utility : the degree to which each example is informative for future example selection when used for the training of the system . </S>",
    "<S> the system progressively collects examples by selecting those with greatest utility . </S>",
    "<S> the paper reports the effectivity of our method through experiments on about one thousand sentences . compared to experiments with random example selection , </S>",
    "<S> our method reduced the overhead without the degeneration of the performance of the system .    </S>",
    "<S> = 15.82 cm = 23.39 cm = -1.5 cm    definecountersubfloatnumber t@bletable @subfloatcaptionhead subfloatcaptionheadtrue    @subfloatwithcaptionhead    @ption    addtoresetsubfloatnumbercaptypecaptypet@ble    @subfloatnumberthecaptypecurrentlabelthe@subfloatnumberdblargsubcaptioncaptype    subcaption#1[#2]#3    captypet@ble    makecaptionwoheading#1#2    10@ tempboxa tempboxa > # 1 # 2    to </S>"
  ]
}