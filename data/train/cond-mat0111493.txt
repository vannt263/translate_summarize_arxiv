{
  "article_text": [
    "one of the more interesting properties of neural networks is their ability to learn from examples . in on - line",
    "learning processes a student network updates its couplings after the presentation of each example in order to make its outputs agree with the outputs of the teacher . in the standard situation",
    "the student knows only the inputs and the corresponding outputs of the teacher and has no further knowledge of the rule used by the latter .",
    "furthermore , in the course of learning the student is able to classify correctly also new examples , which it has never seen before .",
    "the latter property is called generalisation .",
    "various aspects of learning and generalisation in neural networks have been intensively studied in many different contexts . for about a decade now statistical mechanical methods have been used successfully in these studies ( for recent reviews see , for example @xcite ) .",
    "a lot of the theoretical research has been concentrated on the simplest models , such as the binary perceptron .",
    "parallel to the progress in these investigations , new more realistic models have been considered , e.g. , models with multi - state neurons @xcite , models with multi - neuron interactions @xcite , models with many layers ( see , e.g , @xcite )",
    ".    in this paper we study on - line learning and generalisation in a recently introduced model , allowing two different types of binary neurons at each site , possibly having different functions @xcite . more specifically ,",
    "this so called ashkin - teller ( at ) perceptron contains , besides two - neuron interaction terms , also a four - neuron interaction term . for the underlying biological motivation for the introduction of different types of neurons we refer to @xcite . here , we recall that the maximal capacity of the at perceptron model ii introduced in @xcite can be larger than the one of the standard binary perceptron @xcite and that the corresponding recurrent network model can be a more efficient associative memory than a sum of two hopfield models @xcite",
    ". a natural question is then how this at perceptron performs in on - line learning and generalisation tasks .",
    "two learning scenarios turn out to be of interest . in the first scenario where the student and the teacher are independent at perceptrons ,",
    "we show that the resulting learning curves do not differ very much from the already known ones for perceptrons with multi - state neurons . for some particular values of the network parameters",
    "we precisely reproduce the learning curve of the 4-state potts perceptron @xcite .    in the second scenario both the student and the teacher",
    "are represented by a simple perceptron but they are coupled by an at type four - neuron interaction term . hence , contrary to the standard setup , they are not independent .",
    "this can be considered as a sort of `` hardware '' coupling . as a result ,",
    "also the teacher mapping is changing in the process of learning .",
    "we obtain a set of learning curves which qualitatively differ from those found in the independent setup .",
    "we also find different asymptotic behaviour when the number of examples increases to infinity . for certain values of the network parameters such",
    "a coupling describes the realistic situation that the rule used by the teacher is partially shared by the student .",
    "the rest of the paper is organised as follows . in section [ model ] the model and the learning scenarios",
    "are introduced .",
    "the formulas for the generalisation error are derived in section [ generr ] .",
    "the differential equations for the evolution of the order parameters are obtained in section [ dif ] .",
    "their solutions , compared with numerical simulations can be found in section [ results ] . in section [ conclus ]",
    "some concluding remarks are presented .",
    "finally , two appendices contain some technical details of the derivations .",
    "the at perceptron is defined as a mapping of the binary ( @xmath0 ) inputs @xmath1 , @xmath2 into two binary ( @xmath0 ) outputs @xmath3 and @xmath4 : @xmath5 where @xmath6 is the heaviside step function and @xmath7 , @xmath8 , denote the strength of the local fields @xmath9 which are defined as follows & & h_1 = _ i j_i^(1 ) s_i ,   h_2= _ i j_i^(2 ) _ i ,   + & & h_3 = _ i j_i^(3 ) s_i _ i ,   n_r^2 = _ i(j_i^(r))^2 .",
    "[ fields ] the mapping ( [ maps])-([mapsigma ] ) can be equivalently represented by the set of three equations ( cfr . model i in @xcite ) s&=&(_1h_1 + _ 3h_3 ) [ e.s1 ] + & = & ( _ 2h_2+s _ 3h_3)[e.s2 ] + s&=&(_1h_1 + s _ 2h_2)[e.s3 ] .",
    "for @xmath10 the outputs @xmath3 and @xmath4 are completely independent and defined like in the simple perceptron s&=&(h_1)[smap0 ] + & = & ( h_2)[sigmamap0 ] .      first , we consider the standard situation where the student and the teacher are two completely independent networks . in our case",
    "they are represented by at perceptrons meaning that the outputs of the teacher @xmath11 and of the student @xmath12 are both determined by the mapping ( [ maps])-([mapsigma ] ) but with different couplings : @xmath13 and @xmath14 respectively , with @xmath15 .",
    "initially , the student and the teacher couplings are not correlated . at each time",
    "step @xmath16 an example is presented to the student .",
    "the student network then updates its couplings according to the following learning rule @xmath17 @xmath18 where = \\{s_i } ,  = \\{_i } ,   = \\{s_i_i }  . in this scenario",
    "we consider only hebbian learning for which @xmath19 .",
    "furthermore , examples are chosen randomly with equal probability out of the complete set of examples .",
    "alternatively , the at perceptron can also be seen as two coupled perceptrons , with outputs @xmath3 and @xmath4 . in the second scenario we precisely analyse learning between such coupled perceptrons ( or branches of the at perceptron ) .",
    "the outputs of the student @xmath3 and the teacher @xmath4 are defined by the equations ( [ maps ] ) and ( [ mapsigma ] ) respectively .",
    "when @xmath20 , the teacher and the student use two different mixtures of two perceptron mappings defined by the couplings @xmath21 and @xmath22 .",
    "it implies that @xmath3 and @xmath4 are always equal to @xmath23 or @xmath24 and sometimes , depending on the relation between @xmath25 , @xmath26 and @xmath27 , @xmath28 . in the limit @xmath29 the student and the teacher network become so strongly coupled that one always has @xmath28 and the mapping ( [ maps])-([mapsigma ] ) can be simplified to s==(h )   h=\\{h_x : |h_x|>|h_y| ; x , y=1,2 } .[onerule+ ] for @xmath30 , the situation is quite different . even with @xmath31 there is always a non - zero fraction of disagreements between the student and the teacher , as long as @xmath32 . in the limit @xmath33 the student always disagrees with the teacher , and",
    "the mapping ( [ maps])-([mapsigma ] ) can be written in the form : s= .",
    "[ onerule- ] for any value of the coupling field @xmath34 and @xmath10 the student and the teacher are independent and they use the mappings defined by only one coupling vector ( cfr .",
    "( [ smap0])-([sigmamap0 ] ) ) .    in the sequel",
    "we take @xmath35 because the student and the teacher must have the same inputs .",
    "we remark that this implies that @xmath36 ( cfr.([fields ] ) ) . again , at each time step @xmath16 an example is presented to the student network and its coupling vector @xmath21 is updated as follows _ 1(t+1)=_1(t)+f(_1h_1,_3h_3,s , ) ( t)(t ) .",
    "furthermore , at each time step a new coupling vector @xmath37 is generated thus making the coupling between the perceptrons random .",
    "the coupling vector of the teacher , @xmath22 , is not changed in the process of learning , but later on we average over all possible teachers . in this scenario",
    "we consider three learning rules @xmath17 :    hebbian @xmath38    perceptron @xmath39    adatron @xmath40",
    "a quantity of interest in the sequel is the generalisation error .",
    "it is defined as the probability that the student and the teacher disagree , i.e. that their outputs are different .",
    "when the teacher and the student are simple independent perceptrons the generalisation error @xmath41 is a simple function of the overlap @xmath42 between the student and the teacher couplings , which in this case plays the role of an order parameter .",
    "unfortunately , for more complicated models this relation takes a much more involved form ( see , e.g. , @xcite ) .      in the first scenario",
    "the definition of the generalisation error reads _ g(_1,_2,_3)=",
    "1-(1+s_ts_s)(1+_t_s ) _ i [ gerr ] , with the overlaps @xmath43 defined by _",
    "r= , and with @xmath44 denoting the average over the teacher field , @xmath45 , and the student field , @xmath46 , which have a joint probability distribution @xmath47 .",
    "the averages over these fields are double averages , one over the examples and one over the couplings .",
    "this arises because the couplings and the examples enter the mapping ( [ maps])-([mapsigma ] ) and the learning rules only through the local fields .",
    "we assume that the examples are taken randomly with equal probability out of the full training set .",
    "then , in the thermodynamic limit the local fields become correlated gaussian variables and the joint probability distribution @xmath48 can be written down in the form p_i(*h^t*,*h^s*)= ( ( 1-_1 ^ 2)(1-_2 ^ 2)(1-_3 ^ 2))^-1/2 \\ { + + .",
    "+ . - } [ p1 ] . performing the averages in ( [ gerr ] ) explicitly leads to the expression _",
    "g(_1,_2,_3)=-_r=1 ^ 3 i_r with i_r=_0^h_r^t ( ) + + ( h_r^t , h_r^s ) ( a^+_rr-a^-_rr ) ( a^+_rr-a^-_rr)+ ( a^+_rr+a^-_rr)(a^+_rr+a^-_rr  ) ( h^t_r h^s_r ) , + a_rr^= ( 1- ( ) ) -_-^-h^t_r ( ) , where @xmath49 is the gaussian measure , @xmath50 ( @xmath51 ) and where ( h_r^t , h_r^s)= \\{- } is a correlated gaussian .      in the second scenario",
    "the generalisation error is given by _",
    "g ( ) = 1-(1+s)_ii = p_ii(*h*)(1-(1+s ) ) , with the overlap @xmath52 defined by = . here",
    "again , as in the first scenario , the average over the examples and the couplings is done through averaging over the local fields .",
    "the examples are chosen randomly with equal probability out of the full set of examples . in the thermodynamic limit this leads to a gaussian distribution of the local fields . since the behaviour of the system strongly depends on the sign of the coupling field @xmath34 we consider three different field distributions @xmath53 p_(*h*)&= & ( ( 2)^3(1-^2))^-1/2 \\{- ( + h_3 ^ 2 ) } [ ppm ] + p_+(*h*)&=&2 p_(*h * ) ( h_3)[pp ] + p_-(*h*)&=&2 p_(*h * ) ( -h_3)[pm ] . in the case of the distribution @xmath54 the components of the vector @xmath37",
    "are taken randomly ( with equal probability ) from some interval @xmath55 , with @xmath56 a positive real number . in the case of the distributions @xmath57 and @xmath58",
    "these components are chosen in the same way but those values which lead to negative respectively positive values of the field @xmath34 are omitted . the generalisation error in these three situations reads , with obvious notation _",
    "g^c ( ) =( ) + i_c  c=,+,-[gerr2 ] where i_=(u_12^u_12^++u_21^u_21^+ ) ,  i_+=-u_12^+-u_21^+ ,  i_-=u_12 ^ -+u_21 ^ - [ ies ] and u_rr^&=&_-^0h_2(1 + ( ) ) ( 1 + ( ) ) .",
    "[ integ2 ] it is easy to realize that only for positive @xmath34 ( i.e. for @xmath59 ) the generalisation error @xmath60 goes to zero as @xmath52 goes to 1 .",
    "it is also equal to zero for any @xmath52 when @xmath59 and @xmath61 .",
    "as can be seen from the formulas written down in the last section , the generalisation error is a function of the overlaps @xmath52 or @xmath43 , which play the role of order parameters in the learning process .",
    "their evolution is coupled with the evolution of the norms of the couplings @xmath62 and in the thermodynamic limit @xmath63 it can be described by ordinary differential equations @xcite .    in the first scenario a standard calculation ( for a review see , e.g. , @xcite ) leads to the following result for hebbian learning n_r=^t_r",
    "h_r^s_i +   _ r=^t_r(h_r^t- _ r h_r^s ) _",
    "i-  r=1,2,3 [ dif1 ] where @xmath64 , @xmath65 , @xmath66 and @xmath67 is the number of examples scaled with the size of the system .",
    "it becomes continuous in the thermodynamic limit . after performing the averages we arrive at = _",
    "r b_r+",
    "= b_r- [ dif1a ] with the quantity @xmath68 given by b_r&=&\\ { . + & + & . }",
    "+ c_rr&=&1+()^2  .",
    "[ defc ] for @xmath69 this quantity simplifies to b_r=2(1- ( ) ) 1.21635 .",
    "we remark that the differential equations ( [ dif1a ] ) for a given @xmath70 have the same form as the differential equations found for the simple perceptron with hebbian learning @xcite .",
    "more specifically , they differ only by the value of the coefficient @xmath68 , which for the simple perceptron is equal to @xmath71 .    for the hebbian learning we are considering , it is possible to construct a simple expression for @xmath43 as a function of @xmath72 .",
    "following opper and kinzel @xcite we slightly modify the update rule ( [ evj1 ] ) , ( [ evj2 ] ) , ( [ evj3 ] ) ( substituting @xmath73 by @xmath74 ) and easily arrive at _ r=[easyro ] where we have taken as initial condition @xmath75 and where a_r=2_0^b  b  .",
    "this expression differs from the solution of ( [ dif1a ] ) only for small values of @xmath72 and has the advantage of having a simple form .",
    "the evolution of @xmath52 in the case of simple perceptrons is described by the single equation ( [ easyro ] ) , but with a coefficient @xmath76 .",
    "since these results are very similar to the results obtained for the simple perceptron we do not test other algorithms in this scenario because we expect that also in those cases a strong resemblance to the simple perceptron occurs .    in the second scenario with the learning rule f defined in subsection [ scen2 ] we have to solve the following set of differential equations : n_1&=&h_1 f(_1h_1,_3h_3,s,)_ii + f^2(_1h_1,_3h_3,s,)_ii[dif2a ] + & = & f(_1h_1,_3h_3,s , ) ( h_2-h_1)_ii- f^2(_1h_1,_3h_3,s,)_ii .",
    "[ dif2b ] performing the averages leads to much more complicated expressions than those obtained in the first scenario .",
    "the explicit form of these expressions obtained for hebbian , perceptron and adatron learning with the distributions @xmath54 and @xmath57 can be found in [ explicit ] .",
    "in this section we discuss the numerical solutions of the differential equations ( [ dif1 ] ) and ( [ dif2a])-([dif2b ] ) and compare them with the results of simulations . because only the ratios of the strength parameters @xmath77 , @xmath78 and @xmath79 are important we take @xmath80 , and vary only @xmath79 .",
    "the learning curves for small values of the number of examples @xmath72 obtained in the first scenario using formula ( [ easyro ] ) are presented in figure [ atp2atp ] .",
    "all curves start with an initial generalisation error @xmath81 corresponding to random guessing in four - state models . for @xmath10 learning between two independent perceptrons is described . for @xmath82",
    "the learning curve is identical with the one of the 4-state potts perceptron @xcite ( cfr .",
    "@xcite ) . in the limit @xmath83",
    ", @xmath84 decays like @xmath85 for all values of @xmath79 , precisely like in the case of learning between simple perceptrons .",
    "a careful analysis of expression ( [ gerr2 ] ) leads to the conclusion that in the second scenario the generalisation error can be nonzero even when the normalised angle between the student and the teacher couplings , @xmath86 , is equal to zero .",
    "this happens when we allow the field @xmath34 to take negative values .",
    "therefore , we follow the evolution of two dynamical variables in the sequel : the generalisation error @xmath84 and the normalised angle between the student and the teacher @xmath87 . for all the learning algorithms and distributions of the fields that we have considered , we observe an abrupt change in the asymptotic behaviour in @xmath72 when @xmath79 changes from @xmath88 to some non - zero value .",
    "logarithmic plots of the learning curves for two distributions of the fields , @xmath54 and @xmath57 , are presented in figures [ batp2batppm1]-[batp2batpp3 ] .",
    "the learning curves for the distribution @xmath58 are qualitatively very similar to the curves obtained for @xmath54 .",
    "let us first analyse the results obtained for the distribution @xmath54 in more detail . for @xmath90 ,",
    "the generalisation error saturates at some non - zero value . for hebbian and perceptron learning the angle @xmath87 between the student and the teacher is asymptotically decreasing to zero at a higher rate than in the decoupled case @xmath10 . for hebbian learning",
    "we find that in the limit @xmath83 , @xmath91 , versus @xmath92 for @xmath10 , while in the case of the perceptron algorithm @xmath92 , versus @xmath93 for @xmath10 .",
    "for the adatron algorithm @xmath87 and @xmath84 both saturate at some non - zero value . in spite of the fact that the generalisation error never vanishes the student is able to learn the couplings of the teacher using the hebbian or perceptron algorithm .",
    "we observe that for all algorithms the generalisation error goes asymptotically to zero .",
    "for hebbian and perceptron learning it decreases faster than in the decoupled case . in the limit @xmath83",
    ", we get @xmath94 for hebbian learning while @xmath95 for perceptron learning . for adatron learning",
    "we obtain the same decay exponent as in the decoupled case .",
    "surprisingly , for the perceptron and adatron algorithms the decay of the angle between the student and the teacher , @xmath87 , is slower than in the decoupled case in the limit @xmath83 . for the perceptron we have @xmath96 and for the adatron we find @xmath92 . on the contrary , for hebbian learning",
    "@xmath92 as for the decoupled case .",
    "since an analytic analysis of the differential equations ( see [ explicit ] ) is rather involved , the asymptotic exponents discussed above have been determined numerically . only in the case of hebbian learning with the field distribution @xmath57",
    "the numerical analysis was not entirely unambiguous .",
    "therefore , we have derived the corresponding exponents analytically .",
    "details can be found in [ asympt ] .",
    "the initial generalisation error is a function of the strength parameter @xmath79 , which measures the strength of the coupling between the two perceptrons .",
    "the larger the @xmath79 the bigger the common knowledge between the student and the teacher , so the smaller the initial error . for @xmath29 ,",
    "the student and the teacher use precisely the same rule ( [ onerule+ ] ) in order to determine their outputs .",
    "finally , the numerical solution of the equations ( [ dif2a])-([dif2b ] ) suggests that there is a simple relation between the decay exponents of @xmath87 and @xmath84 , denoted by @xmath97 and @xmath98 respectively , y_g=2y_[exprel ] .",
    "this relation can also be derived analytically ( see [ asympt ] ) .",
    "for @xmath99 we find in the limit @xmath83 ( and @xmath100 ) that _ g^+~^2  , confirming the observation ( [ exprel ] ) .      to check the analytic results described above we have performed numerical simulations .",
    "the system sizes have been varied between @xmath101 and @xmath102 neurons .",
    "an excellent agreement has been found for both scenarios and all learning algorithms , even for relatively small @xmath103 . as a representative example we present a comparison between simulations and analytic results obtained in the second scenario with the adatron algorithm for @xmath104 and @xmath105 .",
    "for the sake of clarity we show the results obtained for small and big @xmath72 separately .",
    "the analytic results for small @xmath72 are compared with simulations for a system with @xmath102 neurons ( fig .",
    "[ simad2 ] ) . for bigger @xmath72",
    "we have made simulations for smaller systems ( @xmath101 ) , which are displayed in fig .",
    "[ simad2bis ] . in both cases",
    "only the results obtained for one sample are shown .    for small @xmath72 the simulations are smoothly aligned along the theoretical curves .",
    "this points to the self averaging property of the learning process . for bigger values of @xmath72",
    "very strong fluctuations occur around the theoretical result .",
    "this happens only for the adatron algorithm and @xmath105 and , hence , can not be explained entirely by the relatively small size of the system .",
    "indeed , as has been noticed in section [ results ] , in this case there is always a non - zero fraction of disagreement between the student and the teacher .",
    "so , a strategy used by the adatron algorithm which updates the couplings proportional to the error made by the student , must lead to rather big random changes .",
    "nevertheless the simulation points in fig .",
    "[ simad2bis ] are evenly distributed on both sides of the theoretical curve .",
    "in this paper we have studied on - line learning and generalisation using the at perceptron .",
    "two learning scenarios have been considered .",
    "the results obtained in the first scenario , where the student and the teacher are represented by independent at perceptrons , are very similar to the results obtained for the simpler models @xcite . for a particular choice of the network parameters the learning curve",
    "precisely reproduces that found for the 4-state potts perceptron @xcite .    in the second scenario the student and the teacher",
    "are taken to be simple perceptrons coupled by a four - neuron interaction term .",
    "particular results depend crucially on the distribution of the couplings @xmath37 .    for the field distribution",
    "@xmath89 the generalisation error always saturates at some non - zero value .",
    "this is not surprising since this distribution allows the field @xmath34 to take negative values what inevitably leads to a non - vanishing fraction of disagreements between the student and the teacher even when @xmath31 ( cfr .",
    "( [ maps ] ) , ( [ mapsigma ] ) ) . in spite of this ,",
    "for hebbian and perceptron learning the student manages to learn the couplings of the teacher perfectly ( in the limit @xmath83 ) .",
    "this does not happen , however , for the adatron algorithm , which in the standard ( decoupled ) situation proved to be the fastest @xcite .",
    "the reason is that this algorithm changes the couplings of the student proportionally to the error made by the latter .",
    "since this error is non - zero even for @xmath31 , this can not be a good strategy .",
    "hence , the more `` blind '' updates ( hebbian and perceptron ) appear to be more effective .    for @xmath59",
    "we have obtained quite different results . in this case",
    "the generalisation error goes to zero when @xmath52 goes to @xmath106 . for hebbian and perceptron learning",
    "we observe faster decay of @xmath84 than in the decoupled case . for adatron learning",
    "the decay exponent of @xmath84 is the same as for @xmath10 .",
    "surprisingly , for all algorithms we find the same or slower decay of @xmath87 compared with the decoupled case .",
    "the best asymptotic decay of the generalisation error has been obtained for @xmath59 with the adatron rule : @xmath107 . comparing with the case of independent perceptrons",
    "we see that it is better than the lower bound for on - line learning @xcite ( @xmath108 ) and worse than the bayesian lower bound @xcite ( @xmath109 ) .",
    "we remark that in the course of a learning process in the second scenario also the teacher mapping is changed but not the teacher couplings .",
    "this can be interpreted as a kind of effective mutual learning caused by the ( `` hardware '' ) coupling of the two perceptrons .",
    "this is different from the mutual learning process analysed in @xcite , the only other learning process of this type known to us .",
    "there , in contrast to our setup , the teacher explicitly learns from the student . in our model",
    "the decay exponent of @xmath84 is not influenced by a particular value of the strength parameter @xmath79 as long as it is nonzero .",
    "the model analysed in the second scenario with @xmath59 where a part of the learning rule is shared by the teacher and the student , can be compared to a real life situation in which both of them , e.g. , have the same cultural background , followed the same education @xmath110 one can expect that in such a situation the learning process is much more efficient since the student and the teacher speak in a sense the same language .",
    "it corresponds to a faster asymptotic decay of the generalisation error in our model .",
    "it would be interesting to see , e.g. , whether an optimisation of the learning process @xcite would still improve these results .",
    "this work was supported in part by the fund for scientific research , flanders ( belgium ) .",
    "the set of differential equations ( [ dif2a])-([dif2b ] ) for the order parameters in the second learning scenario can be written down in the following form : & = & f_1(,_13 , _",
    "23)+ f_2(,_13 , _ 23 ) + & = & f_3(,_13 , _ 23)- f_2(,_13 , _ 23 )  , with @xmath111 and where the explicit form of @xmath112 , @xmath113 and @xmath114 depends on the algorithm used and on the distribution of the fields .    in the case of the distribution @xmath89 we have for    hebbian learning f_1(,_13,_23)&=&f_21+g_21 + f_2(,_13,_23)&=&1 + f_3(,_13,_23)&=&f_21(1-^2)-g_21    perceptron learning f_1(,_13,_23)&=&(f_21-f_12+g_21 ) + f_2(,_13,_23)&=&()+i",
    "_ + f_3(,_13,_23)&= & ( f_21(1-^2)-g_12-g_21 )    adatron learning f_1(,_13,_23)&= & -_1 ( f_a - f_12^++f_12 ^ -+ ) + & -&_3 ( t_12-t_21 + ( + ) ) + f_2(,_13,_23)&= & _ 1 ^ 2(f_a - f_12^++f_12 ^ -+ ) -_3 ^ 2(()-i_- ) + & + & _ 1_3 ( t_12 - 2t_21 + ( + ) ) -_2_3 t_21 + f_3(,_13,_23)&= & _ 1((+ ( ) ) + g^a_21+g^a_12+(f_a+f_21^+-f_21 ^ - ) ) + & + & _ 3 ( t_21(1-^2 ) + ( + + + ) )    with f_rr=-_0^h_r  h_r ( 1- ( ) ) +  , + g_rr= ( 1- ( ) ) - ( 1- ( ) )  , + g^a_rr=\\ { + }  , + f_a=_-^0h_1  h_1 ^ 2 ( ) + 2(1-^2)^ + _ 0^h_2 ( 1- ( ) ) _ _ 21 h_2^h_1  h_1 ^ 2 ( h_1 h_2 )  , + f_rr^=_-^0h_r   h_r^2 ( 1 + ( ) ) ( )  , + t_rr^=-(+1)^- +  , t_rr=t_rr^+-t_rr^-  , + c_r r^=1+(_r3)^2 +  ,   a_rr=  , + b_rr=  , where @xmath115 is given by expression ( [ ies ] ) and @xmath116 is defined in expression ( [ defc ] ) .    in the case of the distribution @xmath59 we have for    hebbian learning f_1(,_13,_23)&=&f_21^++g_21^+ + f_2(,_13,_23)&=&1 + f_3(,_13,_23)&=&(1-^2)f_21^+-g_21^+",
    "perceptron learning f_1(,_13,_23)&= & ( f_21^+-f_12^++g_21^+ ) + f_2(,_13,_23)&= & ( ) + i_+ + f_3(,_13,_23)&= & ( f_21^+(1-^2)-g_12^+-g_21^+ )    adatron learning f_1(,_13,_23)&= & -_1 + & + & _ 3 + f_2(,_13,_23)&= & _ 1 ^ 2-_3 ^ 2 + & + & _ 1_3 - 2_2_3t_21^+ + f_3(,_13,_23)&= & _ 1 + & + & _ 3  ,    with f_a^+=_-^0h_1  h_1 ^ 2 ( ) -2(1-^2)^ + _",
    "-^0h_2(1 + ( ) ) _",
    "-^-_21|h_2|h_1   h_1 ^ 2 ( -h_1 h_2 )  , + f_rr^+=+2_-^0h  h ( 1 + ( ) )  , + g_a=_-^0h_1   h_1 ^ 2  ,   g_rr^+= ( 1- ( ) ) + g^b_rr= . and where @xmath117 is given by expression ( [ ies ] ) .",
    "because the dependence of the generalisation error @xmath118 on the overlap @xmath52 is rather complicated ( see ( [ gerr2 ] ) ) we derive the asymptotic form for @xmath118 in two steps .",
    "first , we find the asymptotic relation between @xmath118 and @xmath87 and then we determine the behaviour of @xmath87 as a function of @xmath72 in the limit @xmath119 .",
    "the generalisation error @xmath118 is defined as ( see ( [ gerr2 ] ) ) : _",
    "g^+=-u_12^+-u_21^+= -u_12^+-u_21^+ , with the integrals @xmath120 , @xmath121 given by ( [ integ2 ] ) .",
    "we now expand these integrals as a function of @xmath87 for small values of @xmath87 .",
    "first , we change the variables to get u_rr^+=_-^0 x ( 1+(ax))(1+(cx ) ) _",
    "-^0x f(,x )  , where a=  ,   c=  .expanding @xmath122 with respect to @xmath87 and taking @xmath80 we get u_rr^+=-^2-o(^3)and this leads to _ g^+=^2+o(^3)[epsfi ]  .",
    "the differential equations ( [ dif2a])-([dif2b ] ) can be written down in terms of the variables @xmath123 and @xmath87 .",
    "for hebbian learning and @xmath59 this gives & = & f_1((),_13,_23 ) + f_2((),_13,_23)[difa1 ] + & = & - + f_2 ( ( ) , _",
    "13,_23)[difa2 ] the functions @xmath124 , @xmath125 and @xmath126 are defined in [ explicit ] .",
    "expanding the r.h.s of the differential equations ( [ difa1])-([difa2 ] ) around @xmath127 up to the first non - vanishing term we can easily find that for @xmath99 = ^-combining this result with ( [ epsfi ] ) we obtain the asymptotic formula for the generalisation error : _",
    "g^+=^-1 + o(^- )",
    "99 opper m and kinzel w 1996 in _ models of neural networks iii _ ed .",
    "e. domany et al ( springer ) p  151 mace c w h and coolen a c c 1998 _ statistics and computing _ * 8 * 55 saad d ed .",
    "1998 _ on - line learning in neural networks _",
    "( cambridge university press ) engel a and van den broeck c 2001 _ statistical mechanics of learning _ ( cambridge university press ) watkin t l h , rau a , boll d and van mourik j 1992 _ j. phys",
    "i france _ * 2 * 167 botelho e , de almeida r m c and iglesias j r 1995 1879 yoon h and oh j - h 1998 7771 hansel d , mato g and meunier c 1992 _ europhys .",
    "lett . _ * 20 * 471 kabashima y 1994 1917 copelli m and caticha n 1995 1615 kozowski p and boll d 2001 in _ disordered and complex systems _ eds .",
    "p. sollich at al .",
    "( american institute of physics ) p  49 boll d and kozowski p 2001 _ phys .",
    "e _ * 64 * 011915 boll d and kozowski p 1999 8577 kinouchi o and caticha n 1992 6243 opper m and hausler d 1991 _ phys .",
    "_ * 66 * 2677 kinzel w , metzler r and kanter i 2000 l141 metzler r , kinzel w and kanter i 2000 _ phys .",
    "e _ * 62 * 2555"
  ],
  "abstract_text": [
    "<S> we study supervised learning and generalisation in coupled perceptrons trained on - line using two learning scenarios . in the first scenario </S>",
    "<S> the teacher and the student are independent networks and both are represented by an ashkin - teller perceptron . in the second scenario the student and the teacher are simple perceptrons but are coupled by an ashkin - teller type four - neuron interaction term . </S>",
    "<S> expressions for the generalisation error and the learning curves are derived for various learning algorithms . </S>",
    "<S> the analytic results find excellent confirmation in numerical simulations . </S>"
  ]
}