{
  "article_text": [
    "in many contexts , there is interest in selecting important variables from a very large collection",
    ". think for instance of gene expression data or neuroimaging data , where the number of potential features ( predictors ) exceeds the sample size . to deal with this problem ,",
    "many variable selection methods have been developed that scale well to large numbers of variables , with lasso/@xmath1-penalization providing one example .",
    "a major downside of many of these fast methods is that they do not provide a well - defined measure of statistical significance .",
    "being able to evaluate statistical evidence about whether a variable should be included or not is however crucial in making informed variable selection decisions .",
    "in fact , in many biomedical applications , the main emphasis is on identifying which variables should be included , while reporting the level of evidence in the data that these are important variables ; prediction is not directly of interest .",
    "modeling the data in a bayesian fashion provides a natural framework to evaluate statistical evidence via the posterior . even though many bayesian variable selection methods exist  @xcite , they typically rely on monte carlo sampling for inference  @xcite , which does not scale well with the number of candidate predictors . this led us to develop a general approximation framework for marginal posteriors in bayesian linear regression , which can handle large numbers of candidate predictors by treating all but one of their coefficients as nuisance parameters and integrating them out ( approximately ) from the likelihood  @xcite .",
    "as an example , we will focus on the spike - and - slab prior for variable selection .",
    "our method provides an estimate of the posterior probability of inclusion for each potential predictor .",
    "our contributions can be summarized as follows :    * this paper presents a novel framework for marginal posterior approximation which is scalable and can handle highly non - gaussian prior and posterior distributions . *",
    "it is shown how two state - of - the - art methods  bcr and amp  can be used within the marginal approximation framework by providing an approximation of the posterior predictive distribution of rotated data . * the framework is applied to the problem of bayesian variable section with a spike - and - slab prior on both simulated data and a real study relating brain networks to creative reasoning .      consider the standard linear regression model , @xmath2 where @xmath3 is a fixed @xmath4 matrix of features , @xmath5 is a @xmath6 vector of unknown coefficients , @xmath7 is an @xmath8 response vector , @xmath9 is an @xmath8 residual vector and @xmath10 is the error variance . assuming that @xmath5 is drawn according to a prior distribution @xmath11",
    ", the posterior distribution obeys @xmath12 in practice , the joint posterior is difficult to visualize and interpret , and one routinely bases inferences on summaries of marginal posterior distributions for univariate functionals of the parameters .",
    "the _ marginal posterior distribution _ of coefficient @xmath13 is obtained by marginalizing out the other coefficients @xmath14 , and is given by @xmath15 the _ posterior predictive distribution _ is the distribution of the response @xmath16 to a new vector @xmath17 conditional on having observed @xmath18 , and is given by @xmath19 as expressed above , both the posterior marginal distribution and the posterior predictive distribution require computing a high - dimensional integral over the posterior distribution .",
    "these integrals are challenging to compute in general , a massive literature has focused on scalable approximation methods .      the problem of variable selection is to identify which entries of the coefficient vector are nonzero .",
    "a standard approach in bayesian variable selection is to assign a spike - and - slab prior on the coefficients having the form @xmath20 where @xmath21 is a point mass at zero , @xmath22 is a gaussian distribution with mean zero and variance @xmath23 , and @xmath24 is the prior inclusion probability .    let @xmath25 be a binary @xmath26 vector where @xmath27 is an indicator of the event @xmath28 .",
    "the posterior marginal inclusion probability of the @xmath29th coefficient is given by @xmath30 where @xmath31 can be expressed explicitly as : @xmath32 with @xmath33 . note that the marginal inclusion probabilities are available in a simple analytic form involving evaluation of normal densities and simple weights .",
    "however , the complexity of the summation grows exponentially in @xmath0 and thus direct computation is infeasible for large @xmath0 .",
    "mcmc and related sampling algorithms have been employed for posterior inference  @xcite .",
    "such algorithms attempt to sample efficiently from the massive dimensional space of @xmath34 possible models ; for @xmath0 even moderately large ( in the 100s to 1000s ) the space is so huge that there is no hope of visiting more than a vanishingly small proportion of the models . for example ,",
    "this leads to high monte carlo error in estimating posterior model probabilities , with almost all models estimated to have zero probability ( as they are never visited ) .",
    "the topology of the model space makes efficient computation even more challenging , with local regions containing `` good '' models often separated by large regions containing relatively poor models .",
    "sampling algorithms have trouble efficiently moving between these isolated regions .",
    "this has motivated a rich literature on better samplers  @xcite .",
    "however , one can argue that sampling is intrinsically intractable ( e.g. , even the best samplers can find a much better model after 10 million iterations ) , motivating our fast approximation approach .",
    "this section describes our framework for approximation of the posterior marginal distributions for an arbitrary iid prior on the coefficient vector @xmath5 .    1 .",
    "the first step is to apply a rotation to the observed data that decouples the dependence between an unknown coefficient of interest and the other unknown coefficients , which are viewed as nuisance parameters .",
    "this leads to a representation of the marginal posterior in terms of the posterior predictive distribution of a modified linear regression problem .",
    "2 .   the second step is to replace the posterior predictive distribution obtained in the first step with a ( non - standard ) gaussian approximation .",
    "interestingly , this approximation can be highly accurate even if the prior and posterior are highly non - gaussian .",
    "recent techniques in the literature are used to compute the mean and variance .",
    "this section describes how the posterior marginal distribution of the @xmath29th unknown coefficient can be expressed in terms of the posterior predictive distribution of a rotated regression problem .    for a fixed index @xmath29 ,",
    "consider the rotated data @xmath36 defined by @xmath37 where @xmath38 is the unit vector in the direction of the @xmath29th column of @xmath3 and @xmath39 is an @xmath40 matrix chosen arbitrarily subject to the constraint that @xmath41 .",
    "since the @xmath42 matrix @xmath43 $ ] is full rank , the mapping between @xmath7 and @xmath36 is one - to - one . to characterize these terms",
    ", we introduce the notation @xmath44 the first three terms are functions of @xmath3 and the last term is an auxiliary variable that can not be observed directly , since @xmath13 is unknown . following from the rotational invariance of the gaussian distribution , the distribution of the rotated data can now be expressed as @xmath45 the important property of this decomposition is that @xmath46 does not depend on @xmath13 .",
    "thus , conditioned on @xmath47 , the posterior predictive distribution @xmath48 is a sufficient statistic for inference about @xmath13 .",
    "this means that it is now sufficient to consider the scalar model @xmath49 where @xmath50 denotes the posterior predictive distribution @xmath51 .",
    "the posterior marginal distribution of @xmath13 obeys @xmath52      the main challenge in using the formulation of the previous section to efficiently approximate the marginal posterior of @xmath13 is that computation of the exact posterior predictive distribution is intractable .",
    "the key insight underlying our approach is that , in many cases of interest , the posterior predictive distribution can be well - approximated by a gaussian density , even if the prior and posterior distributions of the unknown coefficients are highly _ non - gaussian_.    to obtain an approximation of @xmath53 , we propose to first obtain a gaussian approximation @xmath54 for the posterior predictive distribution @xmath50 , and then plug this approximation into to compute the approximation of the posterior on @xmath13 , @xmath55 the approximation @xmath54 has the form @xmath56 , where the mean @xmath57 and variance @xmath58 are functions of the data set @xmath59 .",
    "the problem of computing @xmath60 can be attacked adapting a variety of recent techniques in the literature .",
    "two of these are described in sec .",
    "[ sec : methods ] .",
    "we now show how our marginal approximation framework can be applied to the problem of variable selection described in sec .",
    "[ sec : variable_selection ] .",
    "let @xmath11 be the spike - and - slab prior in and let @xmath56 be the approximation of @xmath50 .",
    "the approximation for the posterior marginal distribution of @xmath13 is then a spike - and - slab distribution of the form @xmath61 where @xmath62 , @xmath63 , and @xmath64 is the approximation of the posterior marginal inclusion probability : @xmath65      a particularly useful property of our approximation framework is that the discrepancy between the posterior predictive distribution @xmath50 and its approximation @xmath54 does not depend on the unknown coefficient @xmath13 whose posterior marginal distribution we are trying to compute . as a consequence ,",
    "if @xmath54 converges to @xmath50 under a suitable metric , then it follows under very weak assumptions on the prior @xmath11 that the posterior marginal approximation also converges to the true posterior marginal .    as a heuristic justification for the gaussian approximation of @xmath66 ,",
    "consider a setting in which the entries of @xmath67 are of the same order .",
    "then , the a priori distribution of @xmath68 is approximately gaussian by the central limit theorem for sums of independent variables . provided that the entries of @xmath14 given @xmath46 are weakly correlated , it can then be argued that the posterior distribution of @xmath68 is also approximately gaussian . using ideas from @xcite , this line of reasoning",
    "can be made rigorous for certain classes of large random matrices @xmath3 .",
    "it is important to note that approximate gaussianity of the predictive distribution does not hold in the setting where a small number of other predictors are highly collinear with @xmath69 .",
    "in contrast to many of the existing approximation methods , our framework can handle posteriors which are multimodal and posteriors which are discrete - continuous mixtures .",
    "this is not possible using methods based on direct normal - type approximations or laplace s method  @xcite .",
    "also , we note that previous work has shown how confidence intervals can be obtained for various m - estimators @xcite . our work differs in that we can handle an arbitrary prior distribution , our two - stage procedure decouples the interaction between the coefficient of interest and the approximation , and our framework permits the use of a variety of methods to compute the posterior predictive distribution .",
    "the framework described in sec .",
    "[ framework ] requires the approximation of the posterior predictive distribution of a rotated linear regression problem .",
    "this section shows how two recent methods  bcr and amp  can be used to obtain this approximation .",
    "both of these methods are scalable and have theoretical performance guarantees in the high - dimensional setting .    throughout this section ,",
    "the methods are described in the context of the usual posterior predictive distribution problem given in sec .",
    "[ sec : bayesian_linear_regression ] . for the purposes of our approximation framework , however , it is important to remember that these methods are not applied to the original data @xmath18 , but rather to the rotated data @xmath70 defined in sec .  [",
    "sec : connection ] .",
    "* input : * data @xmath18 , new vector @xmath17 , bcr parameters @xmath71 .    1 .",
    "run the bcr algorithm from @xcite for @xmath72 random projections : + draw @xmath73 sample each element in the @xmath74 matrix @xmath75 from @xmath76 with probabilities @xmath77 but such that @xmath75 is full rank .",
    "orthonormalize @xmath75 using a gram - schmidt process .",
    "compute the predictive mean and variance of new response @xmath16 based on bcr with projection matrix @xmath75 according to @xmath78 compute the model weight @xmath79 which equals the marginal likelihood of the linear regression model @xmath80 with prior @xmath81 2 .",
    "model average the means and variances of new response @xmath16 according to @xmath82    * return : * approximate posterior predictive distribution @xmath83 .",
    "bcr is inspired by data squashing and compressive sensing but is fundamentally different as it reduces only the number of predictors but not the sample size  @xcite .",
    "it computes the exact posterior after randomly projecting the @xmath84 matrix @xmath3 of features , using a random @xmath74 ( @xmath85 ) compression matrix @xmath75 , to the compressed @xmath86 matrix with compressed features @xmath87 .",
    "the coefficients @xmath5 are then replaced with an vector @xmath88 , which is assigned a normal prior .",
    "we now have the likelihood @xmath80 .",
    "conditional on the projection , this then readily yields as posterior predictive for @xmath89 a normal distribution @xmath90 .",
    "see algorithm  [ bcr ] for the details , using the prior @xmath81 and the random projection from @xcite . as in @xcite ,",
    "we model average over @xmath72 different random projections .    for the spike - and - slab",
    "prior in , we will set @xmath91 while @xmath92 can be chosen based on @xmath93 . to see the latter ,",
    "the prior on @xmath88 induces a singular prior on @xmath5 that lives in an @xmath92-dimensional hyperplane in @xmath94 so a lower @xmath92 implies a more restricted @xmath5 which is a bit similar to using a smaller @xmath93 in .",
    "[ simulation ] and [ application ] use @xmath95 .",
    "@xcite compressed all features to obtain a computationally efficient approach to prediction without the possibility of variable selection ; here , we use bcr in a fundamentally new and different manner by leveraging the posterior approximation framework from sec .",
    "[ framework ] .",
    "the theoretical support for bcr applies to the high - dimensional setting where both @xmath96 and @xmath0 tend to infinity with @xmath97 for @xmath98 and @xmath99 . in this",
    "setting , concentration of the posterior approximation @xmath54 is shown in @xcite under some regularity conditions on @xmath3 and @xmath5 . combining this result with theorem  1 in @xcite",
    "shows that , under the spike - and - slab prior , the approximation @xmath54 converges in probability to the true predictive distribution @xmath50 .",
    "the second method we consider is approximate message passing ( amp ) @xcite .",
    "this algorithm is based on gaussian and quadratic approximations of loopy belief propagation in graphical models .",
    "it can also be viewed as a forward - backward primal - dual method for minimizing an approximation to the large - system - limit bethe free energy @xcite .",
    "the algorithm is defined by scalar denoising functions , and optimal performance is obtained when this function is matched to the prior distribution of the coefficients @xcite .    for certain classes of large random matrices @xmath3 the behavior of amp can be characterized rigorously as a function of the prior distribution on the coefficients @xcite .",
    "furthermore , in these settings , simulations show that the algorithm converges rapidly and is often much faster than other state - of - the - art optimization methods . for general matrices ,",
    "however , analysis of amp is more challenging . in practice ,",
    "convergence of the algorithm may require dampening @xcite or serial updates @xcite .",
    "the output of amp can be viewed as an approximation to the posterior marginal distribution of the coefficients , and is thus directly related the framework described in this paper . a key difference , however , is that we do not run amp on the entire data .",
    "instead , we first use amp to obtain an estimate of the posterior predictive distribution of @xmath16 and then combine this with the scalar measurement @xmath47 .",
    "this two - stage procedure has the advantage that the first stage is independent of the coefficient of interest .",
    "a high - level overview of our implementation of amp for approximation of the posterior predictive distribution is given in algorithm 2 .",
    "the theoretical support for amp applies to the high - dimensional setting where both @xmath96 and @xmath0 tend to infinity with @xmath100 for a fixed ratio @xmath101 .",
    "then , under the assumption that the entries of @xmath3 are iid zero - mean gaussian variables , it follows from results in @xcite that the approximation @xmath54 converges in probability to the true predictive distribution @xmath50 , provided that @xmath102 where @xmath103 depends on the prior @xmath11 and the error level @xmath10 .",
    "[ alg : amp ]    * input : * data @xmath18 , new vector @xmath17 , prior distribution @xmath11 , error variance @xmath10 .    1 .",
    "run sum - product approximate message passing with optimal nonlinearity defined by the prior @xmath11 and obtain point estimates of the posterior mean @xmath92 and posterior marginal variance @xmath104 of the regression coefficients . for full details , see e.g.  ( * ? ? ?",
    "* algorithm 1 ) or ( * ? ? ?",
    "2 .   compute the mean and variance of new response @xmath16 according to @xmath105    * return : * approximate posterior predictive distribution @xmath106 .",
    "to test our methods to approximate posterior inclusion probabilities , we apply them to an example where we can compute the exact inclusion probabilities .",
    "consider the linear model @xmath107 with @xmath108 , the @xmath109 parameters @xmath110 and the matrix with features @xmath3 such that its columns are identically normally distributed with correlation @xmath111 between column @xmath112 and @xmath29 and the elements within a column are iid , similar to example  1 from @xcite . according to this ,",
    "we generate 100 data sets for each @xmath113 with @xmath10 such that the signal - to - noise ratio equals 2 and we set @xmath114",
    ". then we compute the exact posterior inclusion probabilities and run our algorithm with amp and with bcr ( @xmath115 ) where @xmath116 .",
    "summaries of the mse of the inclusion probabilities provided by our algorithms compared to the true posterior inclusion probabilities are shown in fig .",
    "[ simul1 ] .",
    "we see that both approximation methods do a good job for @xmath117 . as the collinearity in @xmath3 increases , the mse increases as well but is still sufficiently small for the inclusion probability estimates to be meaningful .    in practice ,",
    "@xmath10 and @xmath93 are parameters that need to be tuned . for the application in the next section",
    ", we build this tuning into our algorithms . for amp",
    ", we use iterative updating of @xmath10 very similar to what is proposed in @xcite and also update @xmath93 analogously based on the current inclusion probability estimates at each iteration .",
    "bcr allows for marginalizing out @xmath10  @xcite . for this",
    ", we use a @xmath118 prior after standardization of both @xmath3 and @xmath7 .",
    "we then iterate the bcr algorithm where each time we update @xmath93 in the same manner as for the amp algorithm .",
    "convergence of @xmath93 required only a few iterations .    .17   and @xmath93 unknown and with @xmath29 the index of the parameter.,title=\"fig : \" ]       .17   and @xmath93 unknown and with @xmath29 the index of the parameter.,title=\"fig : \" ]       .17   and @xmath93 unknown and with @xmath29 the index of the parameter.,title=\"fig : \" ]       .17   and @xmath93 unknown and with @xmath29 the index of the parameter.,title=\"fig : \" ]       .17   and @xmath93 unknown and with @xmath29 the index of the parameter.,title=\"fig : \" ]    .17   and @xmath93 unknown and with @xmath29 the index of the parameter.,title=\"fig : \" ]       .17   and @xmath93 unknown and with @xmath29 the index of the parameter.,title=\"fig : \" ]       .17   and @xmath93 unknown and with @xmath29 the index of the parameter.,title=\"fig : \" ]       .17   and @xmath93 unknown and with @xmath29 the index of the parameter.,title=\"fig : \" ]       .17   and @xmath93 unknown and with @xmath29 the index of the parameter.,title=\"fig : \" ]    to check whether these additions to our methods are valid , we created the box plots in figs .",
    "[ simul2bcr ] and [ simul2amp ] , each based on 200 simulated data sets .",
    "the design matrix is generated as earlier in this section but now @xmath119 with @xmath120 .",
    "note that amp is more aggressive than bcr in setting inclusion probabilities to the extreme values 0 and 1 .",
    "furthermore , amp struggles with @xmath121 .",
    "the results show that our methods can provide meaningful posterior inclusion probabilities even when @xmath93 and @xmath10 are unknown .",
    "this section applies our approximation framework to neuroscience data .",
    "we use the brain network data from @xcite which is available as the mrn-111 data set from http://openconnecto.me / data / public / mr / migraine_v1_0/. these are connectomes with counts of the number of connections between the 70 different brain regions from the desikan atlas  @xcite .",
    "the dependent variable is the composite creativity index ( cci ) for @xmath122 persons from @xcite .",
    "out of all pairs of brain regions , 1802 have a connection for at least on person .",
    "the connection counts for these @xmath123 pairs form the linear predictors in our model .",
    "these counts are zero for half of these pairs with a mean of 1477 and a maximum of 58090 .",
    ".4        .4     see fig .",
    "[ results ] for a summary of the inclusion probability estimates provided by amp and bcr ( @xmath124 ) .",
    "it is noteworthy that fig .",
    "[ resultsamp ] has similarities with fig .  10 from @xcite which considers the same data set but using a bayesian nonparametric model that is substantially more complex .",
    "for instance , both contain edges from node 18l to 3r , 18r and 20r .",
    "we also note that this is a highly challenging example due to the ill - conditioned , discrete and heavy - tailed nature of the feature matrix .",
    "this likely leads to some of the differences between the bcr and amp - based approaches , though both tend to include more cross - hemisphere connections , which is consistent with previous evidence that more creative individuals have more connections between the right and left hemispheres .",
    "sec .  [ framework ] presented a novel general framework for marginal posterior approximation . via a rotation , the parameter of interest and the other ` nuisance ' parameters",
    "are separated in the likelihood .",
    "this reduces the @xmath0-dimensional problem to a scalar one dependent on the influence of the nuisance parameters .",
    "this influence , summarized in a posterior predictive , appears to be well approximated by a gaussian for large @xmath0 even when the full posterior is far from gaussian .",
    "[ sec : methods ] provided bcr and amp as state - of - the - art methods for approximating this posterior predictive .",
    "we then focused on the spike - and - slab prior but note that the framework readily applies to many iid priors on @xmath5 .",
    "the first simulation in sec .",
    "[ simulation ] showed that the framework both with bcr and amp is able to estimate the posterior inclusion probabilities accurately .",
    "the proposed approach represents a substantial paradigm - shift in methods for estimating marginal posterior distributions in variable selection .",
    "there is an enormous literature proposing a wide variety of carefully designed sampling algorithms ; the proposed novel framework leads to order of magnitude speed ups and improvements in stability .",
    "this same approach should be applicable much more widely than the gaussian linear regression setting considered here .",
    "this material is based upon work supported in part with funding from the laboratory for analytic sciences ( las ) .",
    "any opinions , findings , conclusions , or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the las and/or any agency or entity of the united states government .",
    "the authors would like to thank rex e. jung and sephira g. ryman for the brain connectivity data and creativity scores funded by the john templeton foundation ( grant 22156 ) entitled `` the neuroscience of scientific creativity . ''"
  ],
  "abstract_text": [
    "<S> in many contexts , there is interest in selecting the most important variables from a very large collection , commonly referred to as support recovery or variable , feature or subset selection . </S>",
    "<S> there is an enormous literature proposing a rich variety of algorithms . in scientific applications , </S>",
    "<S> it is of crucial importance to quantify uncertainty in variable selection , providing measures of statistical significance for each variable . </S>",
    "<S> the overwhelming majority of algorithms fail to produce such measures . </S>",
    "<S> this has led to a focus in the scientific literature on independent screening methods , which examine each variable in isolation , obtaining @xmath0-values measuring the significance of marginal associations . </S>",
    "<S> bayesian methods provide an alternative , with marginal inclusion probabilities used in place of @xmath0-values . </S>",
    "<S> bayesian variable selection has advantages , but is impractical computationally beyond small problems . in this article </S>",
    "<S> , we show that approximate message passing ( amp ) and bayesian compressed regression ( bcr ) can be used to rapidly obtain accurate approximations to marginal inclusion probabilities in high - dimensional variable selection . </S>",
    "<S> theoretical support is provided , simulation studies are conducted to assess performance , and the method is applied to a study relating brain networks to creative reasoning . </S>"
  ]
}