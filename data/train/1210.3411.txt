{
  "article_text": [
    "any physical system is an information channel , since it is  communicating its past to its future through its present \" @xcite .",
    "a series of data measured for such a system is thus a message , a sequence of symbols .",
    "information theory is the natural framework for the quantitative study of messages @xcite .",
    "the entropy @xmath4 , also called the information , plays a central role in the theory .",
    "it is a measure of uncertainty or disorder .",
    "shannon s theory of communication has found wide application in genetics @xcite , dynamical systems @xcite and a variety of other fields in physics @xcite .",
    "it has also been used extensively in statistical inference problems @xcite and in this light has provided an interesting way of interpreting the maximization of entropy in statistical mechanics @xcite . in this work the entropy density @xmath0 , the entropy per symbol , is used as a measure of the information content in a 2d turbulent flow @xcite .    when a physical system is probed , it reports to the experimenter an ordered sequence of signals @xmath5 in the present experiments , the measured signal is a spatial sequence of velocities @xmath6 in a turbulent 2d flow .",
    "they fluctuate in magnitude about a mean flow speed .    presumably , the disorder @xmath0 of fluid flow is relatively small if the flow is almost laminar . in this limit of small reynolds number @xmath1 ,",
    "one expects @xmath0 to increase with @xmath1 . in the opposite limit of large @xmath1 , a so - called inertial range of correlated eddies of various sizes develops . increased correlations implies added constraints or redundancies , which always decrease the uncertainty and information content of any message @xcite .",
    "one therefore expects that after passing through a maximum , @xmath0 will decrease with increasing @xmath1 .",
    "turbulence is both a temporal and spatial phenomenon .",
    "the fundamental work of kolmogorov treated the spatial structure of turbulence only @xcite .",
    "the work of kraichnan and others suggest that the spatial features are of primary importance @xcite .",
    "thus , the expectation that @xmath0 decreases ought to be true for a spatial series but may not be true for time a series .",
    "the present experiments probe a turbulent system at high @xmath1 , where indeed @xmath0 is seen to decrease with @xmath1 for a spatial velocity sequence .",
    "the near - laminar regime , where one expects @xmath0 to increase with @xmath1 , is not experimentally accessible .    treating a physical system as a source of information has its roots in the early development of nonlinear dynamics and chaos @xcite .",
    "pesin s theorem , which for many chaotic systems equates the sum of positive lyapunov exponents @xmath7 with the kolmogorov - sinai entropy @xmath8 @xcite , is a nice example of the connection between chaotic dynamics and information theory .",
    "if initial conditions separate exponentially , then as two initially almost indistinguishable trajectories separate , new details are uncovered .",
    "if @xmath7 is large , new information is revealed faster .",
    "the methods and ideas of dynamical systems have been applied to turbulence from the earliest stages @xcite . in many cases",
    "turbulence was the original motivation @xcite .",
    "it has been shown by several novel experiments that the onset of turbulence in many systems can be described by a low - dimensional strange attractor , thus solving a long - standing riddle about the transition from laminar flow @xcite .",
    "( the onset of turbulence in pipe flow is a murkier subject @xcite . ) in the case of taylor - couette flow @xcite , @xmath0 and the largest @xmath7 increase with the reynolds number @xmath1 , which can be thought of as a measure of the nonlinearity or strength of the flow .",
    "the usual expectation is that this trend continues as @xmath1 increases as suggested by some models and analytic work @xcite .",
    "the motivation for this study is twofold .",
    "firstly , this seems to be the first study of the spatial disorder of a turbulent velocity field as a function of @xmath1 . by characterizing the flow with the entropy density @xmath0",
    ", the fundamental role of the cascade in producing correlations is clearly manifested .",
    "secondly , @xmath0 is one of several fundamental quantities necessary to describe how a system creates and communicates information @xcite .",
    "uncovering how a system produces , stores and transfers information , should prove to be useful .",
    "it provides a new and interesting description of nature but has not yet been applied to many physical systems @xcite .    in order to treat a physical system as a message ,",
    "the experimental data must be converted to symbols @xcite .",
    "a partition is defined which separates the data into disjoint slices of size @xmath9 .",
    "then the data values in each specified range ( slice ) are assigned to a unique symbol @xcite .",
    "that is , if data points are @xmath9 apart , they correspond to different symbols .",
    "( in some sense all experiments do this because of their limited precision . )",
    "the size and location of the divisions can be chosen to faithfully represent the original system even for seemingly coarse partitions @xcite .",
    "correctly identifying those partitions which completely describe the system ( called generating ) can be extremely difficult @xcite . however , much can and has been learned about complex systems such as the brain or turbulence even after converting a data series into a simple binary alphabet @xcite .",
    "approximate treatments are usually necessary and often useful , as long as they still represent the underlying system @xcite . for a chaotic time",
    "series , the entropy rate @xmath0 may approach a constant value ( @xmath8 ) as @xmath9 decreases @xcite . for a spatially extended system",
    ", one may expect the same .",
    "the entropy of a message is usually defined as @xcite @xmath10 where @xmath11 is the probability of the @xmath12th symbol occurring in the message .",
    "the natural logarithm is used , giving the entropy in  nats \" .",
    "one may consider @xmath13 as a measure of the information gained from any one symbol .",
    "thus the entropy is the average information of the message .",
    "if the message is completely random , then the surprise and the amount of new information @xmath4 is maximal .",
    "@xmath4 is generally large for broad distributions @xcite .",
    "by contrast , a constant , unchanging stream of data will have zero entropy .",
    "the message contains no new information and no uncertainty .",
    "however , one must take correlations into account since these always reduce the amount of information a message contains .",
    "consider sequential blocks of symbols of length @xmath14 .",
    "the probability of any unique block @xmath15 is @xmath16 .",
    "the shannon entropy of single symbols can then be generalized to define the block entropy @xmath17 where the sum is over all blocks @xmath15 .",
    "this block entropy will diverge as @xmath14 goes to infinity .",
    "therefore one defines a quantity @xmath0 @xcite @xmath18 this @xmath0 is the extra information one gets from measuring one more symbol .",
    "the limit exists for stationary processes @xcite and may be reached much sooner than @xmath19 . in spatially",
    "extended systems , such as these turbulence measurements , @xmath0 is called the entropy density . for a time series",
    ", @xmath0 is called the entropy rate or metric entropy @xcite .",
    "although this distinction does not affect the analysis , it does influence the interpretation of the turbulence results for @xmath0 .",
    "( for instance , there is no pesin s theorem for the entropy density . )",
    "the @xmath0 estimated here is very different from that considered in previous work where it has been estimated for time series @xcite .",
    "the above definition already suggests problems one might have in estimating @xmath0 , since the infinite limit is impossible for a finite data set .",
    "fortunately , @xmath20 for real data reaches an asymptote sooner than infinity since correlations are usually finite in scale . some of the techniques designed to overcome the finite data issues can be found in @xcite .",
    "most methods involve making an assumption about the distribution of rare events .",
    "a technique proposed in @xcite is used here , although the results are not changed much by its use ( see fig . [ h_l ] ) .    in this work",
    "the block entropies are used to make an estimate of @xmath0 by looking for the asymptote of @xmath20 defined by eq .",
    "[ eqn : entropyrate ] as shown in fig .",
    "[ h_l ] . in fig .",
    "[ h_l ] , the naive ( frequency count ) estimate of @xmath20 is plotted vs. @xmath14 along with the grassberger estimate from @xcite . the dotted line is the value of @xmath0 determined by the inflection point of @xmath20 . the asymptote is usually reached around @xmath21 .",
    "an alternative method for determining a message s information content is based on data compression .",
    "the lower limit for the length @xmath22 to which a message can be compressed from its original length @xmath23 , for any compression algorithm , is its entropy : @xmath24 @xcite .",
    "compression algorithms operate by finding redundancies and correlations in data and re - expressing the message in a shorter form .",
    "compression provides a nice way of thinking about how much information is contained in a message , since it reduces the message to its  essentials `` .",
    "there is no way to shorten a completely random message since each symbol is independent of the others and there can be no compression . for a repetitive stream of symbols ( like ' '",
    "... 111111 ... \" ) the message is trivially compressed to almost zero size .",
    "the information content is then @xcite @xmath25 where @xmath26 is the alphabet size ( @xmath27 for a binary alphabet @xmath28 ) .",
    "the lempel - ziv algorithm is optimal in the sense that @xmath29 converges to @xmath0 in the limit of infinite @xmath23 , so it can be used as another estimate of @xmath0 and a check on @xmath30 .",
    "the value of @xmath29 is independent of file type but does require that the compression program be based on the lempel - ziv algorithm . in order to account for the  overhead \" ( file headers , etc . ) ,",
    "a random data set is compressed and that compression ratio is used to normalize the real data @xcite .",
    "traditionally , @xmath29 has been given the name of algorithmic or kolmogorov complexity @xcite .",
    "this is a measure of the computational complexity of the data set in question .",
    "even if @xmath29 is not equal to @xmath0 , it is still a measure of the information content of the data @xcite .",
    "it is important to recognize the many limitations involved in calculating information content .",
    "at best @xmath30 and @xmath29 are approximations to @xmath0 , but this does not make them meaningless ; they can still be used for comparison @xcite .",
    "the estimates @xmath30 and @xmath29 are first applied to the logistic map as a test of the method as well as to illustrate some principles regarding @xmath0 for chaotic systems .",
    "the logistic map is a simple one - dimensional nonlinear map which nicely illustrates chaotic behavior @xcite : @xmath31 where @xmath3 is a parameter that increases the strength of the nonlinearity . as @xmath3 increases",
    ", the system goes through a series of period - doubling bifurcations and eventually becomes chaotic at @xmath32 ( @xmath33 ) . as usual , @xmath34 and @xmath35 $ ] .",
    "as mentioned earlier , pesin s theorem states that the sum of the positive lyapunov exponents @xmath7 is equal to @xmath8 , as long as the system satisfies certain conditions @xcite .",
    "for the logistic map , which is one - dimensional , there is only one @xmath7 for each value of @xmath3 .",
    "the value of @xmath7 has been calculated as a function of @xmath3 , using the algorithm in @xcite , and is compared with @xmath30 and @xmath29 in fig .",
    "[ logistic_entropy ] . for each value of @xmath3",
    ", a randomly chosen initial condition is iterated @xmath36 times .",
    "two partitions are shown in fig .",
    "[ logistic_entropy ] .",
    "a binary partition is used where @xmath37 if @xmath38 and @xmath39 if @xmath40 , so @xmath41 .",
    "( the location of this partition divider is important @xcite . )",
    "the second partitioning involves simply rounding the data to the first decimal point ( @xmath42 ) and assigning a symbol to each distinct data value .",
    "the estimate @xmath30 performs very well , while @xmath29 shows significant deviations for the @xmath43 partition . despite its shortcomings in estimating @xmath0",
    ", @xmath29 is nonetheless useful as a measure of the information contained in these finite sequences , as discussed above .",
    "it follows the same trend as @xmath7 and reveals the logistic map s information dependence on @xmath3 .",
    "the values of @xmath3 for which @xmath7 is negative have @xmath44 , since it is positive definite .",
    "partitions with as few as 2 slices or as many as 1000 slices give essentially the same @xmath30 and @xmath29 .",
    "this is because the partitions are all generating , @xmath45 they represent the dynamics faithfully and the entropy calculated for any of them is the kolmogorov - sinai entropy @xmath8 @xcite . in other words ,",
    "anything smaller than a binary partition is overkill .",
    "this is nt always true , but suggests that crude representations of data can still capture important features .",
    "this emboldens us to do the same for turbulence , to be discussed in section c.    once the transition to chaos occurs , @xmath7 and the estimates of @xmath0 increase almost monotonically .",
    "there are several isolated regions where the logistic map returns to periodic behavior @xcite and so @xmath46 and @xmath44 .",
    "the general behavior appears to be that as the strength of the nonlinearity increases ( see fig .",
    "[ logistic_entropy ] ) , so does @xmath0 .",
    "chaos creates information .",
    "similar behavior was observed at the onset of turbulence in taylor - couette flow @xcite .",
    "this increase in @xmath0 for the logistic map is accompanied by a decrease in the strength of correlations , as will be shown shortly .      in order to get a better picture of the importance of correlations ,",
    "a modified logistic map is introduced to explicitly increase correlations through a term that couples to previous iteration values further back than one . denoting @xmath47",
    "the modified logistic map is defined as @xmath48}\\ ] ] where @xmath49 is the coupling strength .",
    "this modification is really a kind of logistic delay map @xcite .",
    "now using three random intial conditions , this map is also iterated @xmath36 times and the compression estimate is used to compare @xmath0 for different values of @xmath49 .",
    "the results are shown in fig .",
    "[ logistic_corr ] .    even for small @xmath49 ,",
    "@xmath29 is changed drastically . as @xmath49 is increased , @xmath29 is decreased more and the transition to chaos shifts to larger values of @xmath3 .",
    "this suggests that in addition to decreasing @xmath0 , correlations can also act to suppress the chaotic transition .    in order to quantify correlations for messages ,",
    "it is useful to introduce the mutual information @xmath50 @xcite .",
    "this is a measure of the information shared between two variables . for two variables @xmath51 and @xmath52 it is @xmath53 where @xmath54 is the joint probability of @xmath51 and @xmath52 and the second equality shows that the mutual information may also be thought of as the information about variable @xmath51 minus the information about @xmath51 given knowledge of @xmath52 . when @xmath51 and @xmath52 are uncorrelated , @xmath55 .",
    "when the two variables @xmath51 and @xmath52 are symbols separated by a certain number of symbols @xmath56 ( @xmath57 , @xmath58 ) , then @xmath59 becomes like an autocorrelation function for symbolic sequences @xcite .",
    "for the logistic map and modified logistic map , @xmath56 is a temporal interval while for the turbulence measurements @xmath56 is a spatial interval .",
    "the mutual information is observed to decay exponentially for the chaotic regime of the logistic map and the logistic delay map , with a decay rate @xmath60 that increases with @xmath3 ( see fig .",
    "[ logistic_mutual_info ] ) .",
    "put another way @xmath61 , which can be thought of as a correlation time , decreases with @xmath3 .",
    "the correlations are thus decreasing as the strength of the nonlinearity increases , which corresponds well with the understanding that @xmath0 is reduced by correlations .",
    "figure [ logistic_mutual_info ] shows @xmath60 as a function of @xmath3 for three different values of @xmath49 .",
    "the addition of coupling has increased the strength of the correlations , and mirrors the drop in @xmath29 .",
    "now consider the real physical system of a turbulent soap film , which is a good approximation to 2d turbulence since the film is only several @xmath62 m thick @xcite .",
    "the soap solution is a mixture of dawn@xmath63 ( 2@xmath64 ) detergent soap and water with 1.5 @xmath62 m particles added for laser doppler velocimetry ( ldv ) measurements .",
    "figure [ setup ] is a diagram of the experimental setup .",
    "the soap film is suspended between two vertical blades connected to a nozzle above and a weight below by nylon fishing wire .",
    "the nozzle is connected by tubes to a valve and a reservoir which is constantly replenished by a pump that brings the soap solution back up after it has flowed through .",
    "the flow is gravity - driven .",
    "typical centerline speeds @xmath65 are several hundred cm / s with rms fluctuations @xmath66 ranging from roughly 1 to 30 cm / s .",
    "the channel width @xmath67 is usually several cm .",
    "turbulence in the soap film is generated by either ( 1 ) inserting a row of rods ( comb ) perpendicular to the film or ( 2 ) replacing one or both smooth walls with rough walls ( saw blades ) , with the comb removed . when protocol ( 1 ) is used decaying 2d turbulence results",
    "which is almost always accompanied by the direct enstrophy cascade @xcite .",
    "if procedure ( 2 ) is used , then forced 2d turbulence can be generated with an inverse energy cascade @xcite . the ability to see the inverse energy cascade depends sensitively on the flux and channel width .",
    "this sensitivity is decreased if two rough blades are used .",
    "the type of cascade is determined by measuring the one - dimensional velocity energy spectrum @xmath68 , where @xmath69    although a condensate has been observed in some 2d turbulent systems @xcite , it is not present in this one .",
    "a condensate is revealed by a sharp spike in @xmath68 , which is never observed .",
    "in other experimental arrangements , two slopes are seen in a log - log plot of @xmath68 vs. @xmath70 , indicating a dual cascade of both energy and enstrophy @xcite . for these experiments",
    "only one slope is observed .",
    "measurements of the velocity are usually taken near the vertical middle of the channel . in all cases ,",
    "the data are obtained for the longitudinal velocity component at the horizontal center of the channel .",
    "the data rate is @xmath71 5000 hz and the time series typically had more than @xmath36 data points . for this system",
    ", the time series should be thought of as a spatial series by virtue of taylor s frozen turbulence hypothesis @xcite .",
    "its validity has been thoroughly tested for this system @xcite .",
    "the fact that these measurements involve a spatial series rather than a time series is a crucial point .    with this high data rate ,",
    "the smallest turbulent scales are easily resolved .",
    "a number of measurements were taken near the top of the channel where the flow is still quite slow .",
    "in this case there is no power law scaling in @xmath68 and so apparently no cascade , although the flow is not laminar ( @xmath72 ) .",
    "some representative spectra are shown in fig .",
    "[ spectra ] .    before converting the velocity data into symbols ,",
    "the mean velocity is subtracted out and the result divided by @xmath66 .",
    "this was done to have a similar alphabet size for different @xmath1 and seems a natural way to treat the data .",
    "the velocity data were then partitioned in a similar way to the logistic map .",
    "that is , the data were separated into slices of various sizes and then converted into symbols . in the turbulence case",
    "a binary partition means that a 1 is assigned if the velocity is above the mean value and 0 if below .",
    "the main results of this paper appear in fig .",
    "[ turbulence_entropy ] , which is a plot of @xmath30 and @xmath29 vs. @xmath1 , where @xmath73 and @xmath74 is the kinematic viscosity .",
    "the reynolds number @xmath1 is a measure of the nonlinearity of the system , much like @xmath3 for the logistic map .",
    "four different estimates of @xmath0 are shown in fig .",
    "[ turbulence_entropy ] .",
    "the open circles ( @xmath75 ) and squares ( @xmath76 ) show @xmath30 and @xmath29 respectively for the binary partition .",
    "the two upper data sets ( @xmath77,@xmath78 ) are @xmath30 and @xmath29 for a finer partition where the velocity data are distinguished by their first significant figure ( @xmath79 ) . the same trend is shown by both partitions and for all partitions studied , namely that @xmath30 and @xmath29 are decreasing functions of @xmath1 .",
    "note that @xmath30 and @xmath29 are very weakly dependent on @xmath1 : @xmath80 after an initial plateau .",
    "the decrease begins as soon as a cascade appears , as seen in fig .",
    "[ turbulence_entropy ] .",
    "the decrease is independent of the type of cascade , as both the energy and enstrophy cascade data are present in the figure .",
    "the flat region at low @xmath1 corresponds to the data without a cascade .    at first glance",
    "this result seems surprising , but the decrease is in accord with the common picture of the turbulent cascade @xcite .",
    "the energy ( enstrophy ) flows from one scale @xmath3 to nearby spatial scales .",
    "the eddies participating in the cascade are necessarily correlated and the extent of the inertial range ( cascade region ) increases with @xmath1 . since laminar flow is not disordered at all , this implies that @xmath0 passes through a local maximum at an intermediate value of @xmath1 .",
    "it is regrettable that the soap film is not stable at low @xmath1 , thus hindering the observation of this local maximum .",
    "although the system under study here is two dimensional , the same decrease in @xmath0 should also hold for three dimensional turbulence in the fully developed regime .",
    "it should be noted that the results in fig .",
    "[ turbulence_entropy ] are somewhat similar to that of wijesekera _ et .",
    "_ in their study of spatial density fluctuations in the ocean @xcite .",
    "however , the behavior of the spatial entropy density @xmath0 vs. @xmath1 observed here is quite different from that of the temporal entropy rate studied previously @xcite .",
    "the spatial correlations in the flow are becoming increasingly important as @xmath1 increases .",
    "this is evidenced by the decrease in the ( spatial ) decay rate @xmath60 for the mutual information @xmath50(@xmath56 ) as shown in fig .",
    "[ turbulence_decay ] .",
    "the increased strength of the correlations is responsible for the decrease in @xmath0 , which follows from its definition in eq .",
    "[ eqn : entropyrate ] .    unlike the logistic map ,",
    "the turbulence data is more sensitive to the size of the partition @xmath9 when converting to symbols .",
    "the location of the dividers is important for the coarser partitions @xcite , but as the partition size decreases the results are not sensitive to this placement .",
    "it is generally true that @xmath0 depends on the partition size @xmath9 @xcite as shown in fig .",
    "[ entropy_partition ] . in fig .",
    "[ entropy_partition ] , @xmath30 is plotted as a function of @xmath9 for three different values of @xmath1 .",
    "although @xmath30 increases as @xmath9 decreases , the curves never cross for the different @xmath1 .",
    "as @xmath9 decreases , more detailed information is described by the symbols .",
    "although @xmath29 is not a reliable estimate of @xmath0 at the finer partitions , it is still an indicator of the information content of the data streams and also shows the same decrease with @xmath1 @xcite .",
    "the important point is that the general behavior of @xmath29 and @xmath30 is the same for partitions of all sizes .",
    "although the selection of a correct partition is trickier for the turbulent data @xmath30 and @xmath29 are more sensitive to @xmath9 for the turbulence data , they show that the spatial disorder decreases with @xmath1 at each level of descriptive precision . an estimate can be made for the smallest size of the partition needed to capture the entire inertial range , based on the smallest eddy s characteristic velocity @xmath81 @xcite .",
    "simple estimates show that the smaller partitions are fine enough to resolve a fluctuation of this magnitude for all @xmath1 .",
    "it is surprising that even a binary partition captures the main features : @xmath2 .",
    "this suggests that one may fruitfully study turbulence just by looking at the 1s and 0s .",
    "similar studies of complex systems such as the brain and heart and even turbulence @xcite have also used very coarse partitions .    as an additional test of the validity of this coarse - graining approach ,",
    "the decay rates calculated from the raw data using the autocorrelation method and with the mutual information were compared for various partitions ( data not shown ) .",
    "the @xmath1-dependence was almost exactly the same , although there is a shift by a factor of @xmath82 for the mutual information method .",
    "since the entropy is fundamentally connected to correlations , this is strong evidence for the validity of this coarse - graining approach .",
    "entropy maximization is a familiar principle for solving a variety of problems and is a fundamental principle in equilibrium statistical mechanics @xcite . in these problems ,",
    "understanding the constraints is of paramount importance . in a turbulent system",
    "the constraints are correlations that span a wider range of scales as @xmath1 increases .",
    "however , this begs the question as to why this happens .",
    "perhaps the organization of the cascade is the response to the system s effort to more efficiently transfer energy ( enstrophy ) between scales .",
    "treating turbulence as a message enables one to quantify the information content in the system through the entropy density @xmath0 .",
    "estimates show that @xmath0 , a measure of disorder , is a decreasing function of @xmath1 at large @xmath1 .",
    "the cascade reduces spatial randomness by introducing correlations .",
    "cascades in turbulence are often thought to arise naturally because of the wide separation between the forcing scale and the dissipative scale , as well as because of some essential features of the navier - stoke s equation @xcite .",
    "however , this may not be the only way of looking at the issue , just as in mechanics one can use either newton s laws or a variational principle and get the same answer .",
    "perhaps the underlying reason for the cascade can be connected to the decrease of @xmath0 as @xmath1 increases .",
    "the authors are grateful to mahesh bandi for introducing us to this topic .",
    "comments from various referees were also very helpful . this work is supported by nsf grant no .",
    "1044105 and by the okinawa institute of science and technology ( oist ) .",
    "r.t.c . is supported by a mellon fellowship through the university of pittsburgh ."
  ],
  "abstract_text": [
    "<S> we treat a turbulent velocity field as a message in the same way as a book or a picture . </S>",
    "<S> all messages can be described by their entropy per symbol @xmath0 , defined as in shannon s theory of communication . in a turbulent flow , as the reynolds number @xmath1 increases , more correlated degrees of freedom are excited and participate in the turbulent cascade . </S>",
    "<S> experiments in a turbulent soap film suggest that the spatial entropy density @xmath0 is a decreasing function of @xmath1 , namely @xmath2 + const . in the logistic map , also analyzed here , increasing the control parameter @xmath3 increases @xmath0 . </S>",
    "<S> a modified logistic map with additional coupling to past iterations suggests the significance of correlations . </S>"
  ]
}