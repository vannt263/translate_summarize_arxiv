{
  "article_text": [
    "the development of amr @xcite was meant to provide high resolution simulations for much lower computational cost than fixed grid methods would allow .",
    "the use of highly parallel systems and the algorithms that go with them were also meant to allow higher resolution simulations to be run faster ( relative to wall clock time ) .",
    "the parallelization of amr algorithms , which should combine the cost / time savings of both methods is not straight forward however and there have been many different approaches @xcite .",
    "while parallelization of a uniform mesh demands little communication between processors , amr methods can demand considerable communication to maintain data consistency across the unstructured mesh as well as shuffling new grids from one processor to another to balance workload .    in this paper",
    "we report the development and implementation of new algorithms for the efficient parallelization of amr designed to scale to very large simulations .",
    "the new algorithms are part of the astrobear package for simulation of astrophysical fluid multi - physics problems @xcite .",
    "the new algorithmic structure described in this paper constitutes the development of version 2.0 of the astrobear code .",
    "amr methods come in many varieties .",
    "meshes can either be unstructured or semi - structured .",
    "semi - structured methods can be further divided into those which allow grids to be of arbitrary size ( patch based ) and those which require grids to be of a fixed size ( block based or cell - based if the block size is 1 ) . with block ( or cell )",
    "based amr , the additional constraints imposed on the structure of the mesh allow for a simpler type of connectivity within a tree .",
    "for example in block based amr , any given block will have exactly 8 children or none ( if it is a leaf ) and will have at most 6 face sharing neighbors . with patch based amr , there is no limit to the number of children or neighbors .",
    "in addition , the operation of regridding in block based amr is much simpler .",
    "as the grid changes a given block will either persist if the physical region continues to require refinement or be destroyed . in patch based amr , a given region",
    "may subsequently be better covered by patches of a different shape requiring transfer of data between physically overlapping previous patches and new patches .",
    "this adds an additional dimension to the tree structure and increases the complexity of maintaining a distributed tree .    for both block ( or cell ) and patch based amr , the actual grid data ( fluid variables etc . )",
    "are always distributed across the various processors .",
    "usually some overlap in grid data ( guard / ghost cells ) is desired to allow for frequent access to neighboring values without the need for additional communication .",
    "but the metadata that describes the shape and distribution of the grid data is usually stored on every processor . for 100 s or 1000 s of cores ,",
    "this global tree typically requires less memory than that required for the local grid data and it allows for easy access to data from any part of the domain .",
    "for instance after regridding , the entire tree can be updated and stored locally and then finding new neighbors for local cells / blocks / patches can be done without the need for any further communication . for patch - based amr it also allows for better load balancing",
    "as each processor can determine from the entire tree which section of data it should be responsible for and can use various knap - sack type algorithms to optimize the degree of interprocessor communication .",
    "cell based amr engines typically use an octree data structure to handle the tree metadata and implementations have been developed that support a memory - distributed tree @xcite , or various ways of compressing the global tree @xcite that rely on the simple structure of the tree .",
    "for patch based amr , algorithms for implementing a distributed tree have not yet been published though implemented in the package samrai @xcite as well as astrobear . in addition",
    "the chombo library @xcite recently has developed a method of compressing the metadata to avoid having to distribute the tree .",
    "here we document the distributed tree algorithm used in astrobear 2.0 in which no processor has access to the entire tree but rather each processor is only aware of the amr structure it needs to manage in order to carry out its computations and perform the necessary communications .",
    "while currently , this additional memory is small compared to the resources typically available to a cpu , future clusters will likely have much less memory per processor similar to what is already seen in gpu s .",
    "additionally each processor only sends and receives the portions of the tree necessary to carry out its communication as opposed to a pruning approach in which every processor receives every new patch , but only keeps those necessary to maintain the local tree .",
    "astrobear 2.0 also uses extended ghost cells to decouple advances on various levels of refinement . as we show below this",
    "allows for each level s advance to be computed independently on separate threads .",
    "such inter - level threading allows for total load balancing across all refinement levels instead of balancing each level independently .",
    "independent load balancing becomes especially important for deep simulations ( simulations with low filling fractions but many levels of amr ) as opposed to shallow simulations ( high filling fractions and only a few levels of amr ) .",
    "processors with coarse grids can advance their grids simultaneously while processors with finer grids advance theirs .",
    "without such a capability , each level would need to have enough cells to be able to be distributed across all of the processors .",
    "variations in the filling fractions from level to level can make the number of cells on each level very different .",
    "if there are enough cells on the level with the fewest to be adequately distributed , there will likely be far too many cells on the highest level to allow the computation to be completed in a reasonable wall clock time .",
    "this often restricts the number of levels of amr that can be practically used .",
    "with inter - level threading this restriction is lifted .",
    "inter - level threading also allows processors to remain busy while waiting for messages from other processors .    in what follows we provide descriptions of the new code and its structure as well as providing tests which demonstrate its effective scaling . in section  [ amr_alg ]",
    "we review patch based amr . in section  [ distributedtree ]",
    "we describe in detailt the distributed tree algorithm for patch - based amr , in section  [ threading ] we will discuss the inter - level threading of the advance , in section  [ loadbalancing ] we will discuss the load balancing algorithm , and in section  [ results ] we will present our scaling results and we will conclude in section  [ conclusion ] .",
    "here we give a brief overview of patch based amr introducing our terminology along the way .",
    "the fundamental unit of the amr algorithm is a patch or grid .",
    "each grid contains a regular array of cells in which the fluid variables are stored .",
    "grids with a common resolution or cell width @xmath3 belong to the same level @xmath4 and on all but the coarsest level are always nested within a coarser  parent \" grid of level @xmath5 and resolution @xmath6 where @xmath7 is the refinement ratio .",
    "the collection of grids comprises the amr mesh , an example of which is shown in figure  [ meshtree ] . in addition to the computations required to advance the fluid variables , each grid needs to exchange data with its parent grid ( on level @xmath5 ) as well as any child grids ( on level @xmath8 ) .",
    "grids also need to exchange data with physically adjacent neighboring grids ( on level @xmath4 ) . in order to exchange data ,",
    "the physical connections between parents , children , and neighboring grids are stored in the amr tree as connections between nodes .",
    "each grid has a corresponding node in the amr tree .",
    "thus there is a one to one correspondence between nodes and grids .",
    "the grids hold the actual fluid dynamical data while the nodes hold the information about each grid s position and its connections to parents , children and neighbors .",
    "figure  [ meshtree ] shows one example of an amr mesh made of grids and the corresponding amr tree made of nodes .",
    "note that what matters in terms of connections between nodes is the physical proximity of their respective grids .",
    "while siblings share a common parent , they will not necessarily be neighbors , and neighbors are not always siblings but may be 1st cousins , 2nd cousins , etc ...     [ meshtree ]    additionally since the mesh is adaptive there will be successive iterations of grids on each level as the simulation progresses .",
    "thus the fluid variables need to be transferred from the previous iteration of grids to the current iteration as shown in figure  [ overlaps ] .",
    "thus nodes can have `` neighbors '' within a 4 dimensional space - time .",
    "nodes that are temporally adjacent ( belonging to either the previous or next iteration ) and spatially coincident are classified as preceding or succeeding overlaps respectively instead of temporal neighbors , reserving the term neighbor to refer to nodes of the same iteration that are spatially adjacent and temporally coincident .",
    "nodes on level @xmath4 therefore have a parent connection to a node on level @xmath5 , child connections to nodes on level @xmath8 , neighbor connections to nodes on level @xmath4 of the same iteration , and overlap connections to nodes on level @xmath4 of the previous or successive iteration in time .",
    "[ overlaps ]      we use the term  iteration \" to refer to successive generations of the grid distribution for each level .",
    "thus at some point in the simulation a distribution of level @xmath4 grids will cover some fraction of the computational space .",
    "when , as the simulation proceeds , the grid generation subroutine is called again the next iteration of level @xmath4 grids will be laid down over the computational domain .",
    "while in principle level @xmath4 could be regridded each level @xmath4 time step ,",
    "it is convenient for restricting data to parent grids , to wait until after @xmath7 level @xmath4 steps , or equivalently after @xmath9 parent level @xmath10 step so that child and parent grids have advanced to the same time . therefore for each level @xmath11 time step , there is @xmath9 iteration of level @xmath9 grids , @xmath7 iterations of level @xmath12 grids , @xmath13 iterations of level @xmath14 grids , and so on .",
    "since new child grids are created after each parent step , each parent will have multiple iterations of children .",
    "this additional branching of the amr tree in time allows for temporally adjacent ( overlap ) grids to be classified as temporal siblings or 1st , 2nd , 3rd temporal cousins etc ... additionally parents will have connections to multiple iterations of children ( although only the two most current need to be kept ) .",
    "note that there are different ways of indexing the iteration of grids on a given level .",
    "one can chose a global indexing that begins with the initiation of the simulation , i.e the iterations can be indexed by counting the number of successive iterations of grids on that level from the beginning of the simulation .",
    "another method is to index iterations relative only to the level above ( ie which iteration of children from a given parent does a grid correspond to . )",
    "we will use the latter indexing scheme , since it is relevant to the way overlap connections between grids of different iterations are formed .",
    "figure  [ overlapfigure ] shows this indexing system ( where we have assumed @xmath15 for simplicity ) . here",
    "the circles represent iterations of the entire level ( i.e. the entire collection of nodes for a given level at the specified iteration ) . since figure  [ overlapfigure ] shows the evolution of the amr tree _ in time _ , it s useful to make a connection with the the way the tree appears at any given moment _ in space _ in terms of spatial connections between nodes ( figure  [ meshtree ] ) . to understand the relation between figure  [ meshtree ] and figure  [ overlapfigure ] imagine taking the tree in figure  [ meshtree ] and rotating it into the plane .",
    "neighbor relationships would now disappear and all of the nodes of a given level would visually merge . each visible circle in figure  [ overlapfigure ] represents a collection of nodes of the same level and iteration as shown in figure  [ meshtree ] .",
    "each level @xmath16 has multiple iterations that stretch both forward and backward in time .",
    "note that the level 0 grids are static and there are no successive iterations .",
    "level 1 iterations go from @xmath9 to @xmath17 since the level 0 grids continue to create successive iterations of children .",
    "level 2 iterations and higher are indexed from @xmath9 to @xmath15 .",
    "it is important to stress here , that each level s iteration in figure  [ overlapfigure ] represents a collection of nodes on that level .",
    "additionally each level s iteration represents multiple time steps on that level .",
    "the nodes of the level 0 iteration actually take many time steps , while all other level iterations take @xmath15 time steps .",
    "preceding overlap connections ( those going backward in time ) are only needed before the 1st of @xmath7 time steps , and the succeeding overlap connections ( those going forward in time ) are only needed after the last of @xmath7 time steps . after the 1st time step and before the last , ghost overlap data is shared between neighboring grids of the same iteration instead of preceding / succeeding overlaps .",
    "while there will be many iterations of grids on each level , only the two most current iterations are stored in the tree .",
    "after a grid finishes its advance it becomes  old \" . when this occurs the previous generation of old grid iterations",
    "are discarded .",
    "the current and old grids are shown schematically in figure  [ overlapfigure ] .",
    "the current level @xmath4 iteration will always contain children of the current level @xmath5 iteration .",
    "for the old level @xmath4 iteration there are two cases .",
    "first the old level @xmath4 iteration may be the old iteration of children of the current @xmath5 iteration as is the case for the old level 3 grids in figure  [ overlapfigure ] .",
    "secondly , the old level @xmath4 iteration may be the last iteration of children of the old level @xmath5 iteration as is the case for the old level 2 grids in figure  [ overlapfigure ] .",
    "that is , they are either old children of the current parent level grids , or last children of the old parent level grids .",
    "[ overlapfigure ]      the execution of the various stages in the amr algorithm is shown in the upper panel of figure  [ threadingfig ] ( for @xmath15 ) .",
    "there are 5 steps in the algorithm : overlaps ( o ) ; prolongation ( p ) ; advances ( a ) ; synchronization ( s ) and restriction ( r ) . a pseudo - code description of their order of operations with @xmath18 being the highest level of refinement is written below .",
    "note each root step is initialized by calling amr(@xmath11 ) .",
    "recursive subroutine amr(@xmath4 )    * * o**verlap data from preceding iterations of grids on level @xmath4    do @xmath19=1 , @xmath7    * * a**dvance grids on level @xmath4    if ( @xmath20 ) then    * * p**rolongate initial ( pre - advanced ) data from level @xmath4 to level @xmath8    call amr(@xmath8 )    * * r**estrict data from level @xmath8    end if    * * s**ynchronize fluxes between neighboring level @xmath4 grids and update ghost cells    end do    end recursive subroutine    the subroutine above describes a process in which grids are created , populated with data , advanced and brought in line with higher and lower refinement representations of the data . to be more specific , after each iteration of level @xmath4 grids are created their cells ( including ghost regions ) are initialized with a combination of prolongated data from the parent grid as well as data from the preceding set of level @xmath4 grids that physically overlap with the computational space that the new grids now describe .",
    "ghost regions are needed to update the fluid variables within the grid .",
    "the new level @xmath4 grids then determine which cells to refine and lay down a first set of level @xmath8 child grids to cover those cells . the parent level @xmath4 grids take one step of @xmath21 .",
    "meanwhile their child grids advance @xmath7 steps of @xmath22 .",
    "next the level @xmath4 grids merge restricted data from their children with their own updated data since both levels have now reached the same time @xmath23 .",
    "these level @xmath4 grids then synchronize fluxes with any adjacent neighboring grids ( also of level @xmath4 ) and update any ghost cells . the level @xmath4 grids are now ready to repeat the process ( creating children ; advance a time step ; restricted child data ; synchronize fluxes with neighbors ) .",
    "when the level @xmath4 grids have completed @xmath7 steps relative to the level above ( @xmath5 ) , their own data and fluxes are restricted and applied to their parent ( @xmath5 ) grids .    after completing its advances , the data from each level @xmath4 grid",
    "is stored until it can be copied onto the next iteration of level @xmath4 grids .",
    "after the data is copied onto _ succeeding _ overlaps , the old level @xmath4 grids are destroyed . throughout a level @xmath4 grid",
    "s lifetime it must therefore share data with its parent grid @xmath10 , its preceding overlaps ( @xmath4 ) , multiple iterations of it own child grids ( @xmath24 , its neighbor grids @xmath25 , and succeeding overlaps @xmath25 .",
    "these connections for a given grid are held by its node and the web of connections between grids / nodes form the amr tree . while a grid / node may have many successive iterations of children , only connections that belong to the two most current iterations of the child level",
    "are kept .",
    "refined grids need to take multiple ( @xmath7 ) time steps . for the first step ,",
    "prolongated ghost cells from the coarser grid can be used to advance the internal cells .",
    "however further time steps can not be taken without updating the ghost cells in time as well .",
    "if a grid is surrounded by neighboring grids , it can update its ghost cells in time by copying the advanced data from its neighbors internal cells .",
    "however , if a grid is isolated ( ie . has no neighbors ) or even if it is partially isolated , it must advance those ghost cells in another way .",
    "there are two solutions to this problem shown in figure  [ extendedghostcells ] .",
    "one is to use the time derivative for the fluid variables calculated on the coarse grid to update the ghost cells where needed .",
    "this solution requires parent grids to advance before their children and can lead to large errors if shocks are not completely resolved since spatial discontinuities lead to delta functions for time derivatives which , in turn , produce large errors upon discretization .",
    "the alternative method is to use extended ghost cells that allow grids to successively update smaller and smaller regions so that there is always enough ghost cells to complete the final step . in general",
    "if @xmath26 is the number of ghost cells needed to take a time step and @xmath7 is the refinement ratio , then the extended ghost region needs to initially be @xmath27 cells wide .",
    "while only isolated grids need to update extended ghost cells , it can be difficult to implement efficient advance schemes for partially isolated grids , and each grid is often assumed to be isolated . while this can result in a large overhead for small grids , ( especially when the refinement ratio @xmath7 , or @xmath26 is large ) the ability to allow coarse grids to be advanced independently of fine grids can be exploited to increase the speed of the code .",
    "we have taken this approach in astrobear to allow for more efficient distribution of grids as well as the creation of multiple advance threads described in section  [ threading ] .",
    "[ extendedghostcells ]",
    "many current amr codes store the entire amr tree on each processor .",
    "this , however , can become a problem for simulations run on many processors . to demonstrate this",
    "let us first assume that each amr grid requires @xmath28 bytes per node to store its meta data ( ie 6 bytes to store its physical bounds for a 3d simulation and 1 byte to store the processor containing the grid ) .",
    "we also assume that each grid requires on average @xmath29 bytes for the actual data .",
    "if there are , on average , @xmath30 grids on each of @xmath31 processors , then the memory per processor would be @xmath32 . the second term @xmath33 represents the meta - data for the _ entire _ amr tree .",
    "the memory requirement for just the nodes in the amr tree without storing any connections becomes comparable to the local actual data when @xmath34 .",
    "if we assume a 3d isothermal hydro run where each cell contains @xmath35 with a typical average grid size of 8x8x8 then @xmath36 .",
    "while this additional memory requirement is negligible for problems run using typical cpu s on 100 s of processors , it can become considerable when the number of processors @xmath37 .",
    "since it is expected that both efficient memory use and management will be required for ever larger hpc ( high performance computing ) clusters down the road , astrobear 2.0 is designed to use a _ distributed _ tree algorithm . in this scheme",
    "each processor is only aware of the section of the tree containing nodes that connect to its own grids nodes .",
    "additionally , new nodes are communicated to other processors on a proscriptive `` might need to know basis '' . since maintaining these local trees as the mesh adapts is not trivial , we describe the process below .",
    "because of the nested nature of grids , neighbor and overlap relationships between nodes can always be inherited from parent relationships .",
    "first consider the neighbors of the @xmath38 iteration of a node s children .",
    "the nested nature of the grids restricts each of the child s neighbors to either be a sibling of that child ( having the same parent node and be of the same iteration ) , or to be a member of a neighbor s @xmath38 iteration of children .",
    "thus the neighbors of a level @xmath4 node s children ( on level @xmath8 ) will always be a child of that level @xmath4 node s neighbors .    for overlaps ( temporal neighbors ) the situation is a bit more complicated .",
    "if we consider figure  [ overlapfigure ] we can identify two different types of overlap connections .",
    "first there are overlap connections between iterations with a common parent which we will refer to as temporal siblings. for example the current and old iterations shown on level 3 are different iterations of children of the same level 2 iteration .",
    "there are also overlap connections between children that come from different iterations of the parent grids .",
    "these can be thought of as temporal cousins. for example the current and old child iterations shown on level 2 come from different iterations of the parent nodes one level above them . in principle temporal cousins can be 1st cousins , 2nd cousins , and so on , but the distinction does not matter for the way connectivity is treated in the algorithm .",
    "overlaps between temporal siblings on level @xmath4 will be between successive child iterations of the current level @xmath5 iteration .",
    "note that grids in the current @xmath5 iteration do not physically overlap .",
    "since grids are always nested , the overlaps between the grid s current ( @xmath38 ) iteration of children must belong to the grid s old ( @xmath39 ) iteration of children ( for completeness these are what are referred to as preceding overlaps ) .",
    "however , while grids do not physically overlap , their ghost cells may .",
    "thus overlapping data for a level @xmath4 grid s children may also come from a neighbor level @xmath4 grid s old children . thus the ( preceding )",
    "overlaps for a node s children may be an old child of the node s neighbors . likewise , every node s old child s succeeding overlap may be a node s neighbor s current child .    for temporal cousins",
    ", the current iteration of level @xmath5 grids / nodes will be the first iteration of children of the current level @xmath4 grids / nodes . and",
    "the old iteration of level @xmath5 grids will be the last iteration of children of the old level @xmath4 grids .",
    "because of the nested nature of the grids , we can say that for the first iteration of children , every node s child s preceding overlap must be a node s preceding overlap s child .",
    "the same follows for the succeeding overlaps of a node s last iteration of children .",
    "that is , every node s child s succeeding overlap must be a node s succeeding overlap s child .",
    "these relationships are summarized for @xmath15 in table [ simpleinheritance ] and a generalized table is given in table [ geninheritance ] .",
    "neighbors & node s child s preceding overlaps & node s child s succeeding overlaps + 1@xmath40 & node s neighbors 1@xmath40 children & node s preceding overlaps 2nd children & node s neighbor s 2@xmath41 children + 2@xmath41 & node s neighbors 2@xmath41 children & node s neighbors 1@xmath40 children & node s succeeding overlaps 1@xmath40 children +      for parallel applications , the grids are distributed across the processors .",
    "in addition to data for the local grids , each processor needs to know where to send and receive data for the parents , neighbors , overlaps , and children of those local grids .",
    "this is information contained within the nodes . in order for a processor to know where to send data ,",
    "each one must maintain a local sub - tree containing its own ",
    "local \" nodes ( corresponding to local grids ) as well as all remote nodes ( living on other processors ) directly connected to the local nodes .",
    "it is also possible , though not desirable , that an individual processor have data from disjoint regions of the simulation . in that case",
    "each processor would have multiple disjoint sections of the amr tree , but these disjoint sections would collectively be considered the processor s sub - tree .",
    "each time new grids on level @xmath8 are created ( by local parent grids on level @xmath4 ) , each processor determines how the new child grids should be distributed ( i.e. which processor should get the new grids ) . this distribution is carried out in the manner described in section  [ loadbalancing ] below .",
    "connections between the new level @xmath8 nodes and the rest of the tree must then be formed .",
    "because of the inheritability of the neighbor / overlap connections , even if a child grid is distributed to another processor , the connections between that child node and its neighbors / overlaps / parent are first made on the processor that created the grid ( ie the processor containing the grid s parent ) .",
    "if a processor s local grid has a remote parent , then that processor will always receive information about that local grid s neighbors / overlaps from the processor containing the remote parent .",
    "this is true of neighbor and preceding overlap connections of new grids as well as succeeding overlap connections of old grids .",
    "before processors containing parents of level @xmath8 grids ( both new and old ) can send connection information to remote children ( if they exist ) , these processors must first share information about the creation of children with each other .",
    "neighbor connections between new nodes on level @xmath8 require each processor to cycle through its local level @xmath4 nodes and identify those with remote neighbors living on other processors .",
    "once remote neighbors have been identified the information about new children from the local nodes is sent to the processor(s ) containing the remote neighbors .",
    "not all children need to be sent to every remote neighbor . only those that are close enough to potentially be adjacent to the remote neighbor s children are necessary .",
    "the information must flow in both directions meaning a individual processor also needs to _ receive _ information about potential new children from all other processors containing remote neighbors .",
    "overlaps are again a bit more complicated because of the different situations for temporal siblings and temporal cousins as described above . for temporal cousins ( ie",
    "if the new level @xmath8 child grids are the 1st iteration from level @xmath4 parents ) , each processor must cycle through local current level @xmath4 nodes with preceding overlaps that live on remote processors .",
    "the processor must then send new children of local nodes to the processor(s ) containing the remote preceding overlaps .",
    "similarly , each processor must cycle through the local * old * level @xmath4 nodes with remote succeeding overlaps sending children of those old local nodes to the processor(s ) containing remote succeeding overlaps .",
    "again , not all children need to be sent to the processors containing remote overlaps .",
    "only information about those nodes close enough to potentially overlap the remote node s children must be sent .",
    "the information must flow in both directions and processors also need to receive information about potential children from those same processors with remote overlaps .    for overlaps between temporal siblings ,",
    "no communication is required .",
    "this is because previous level @xmath8 nodes that could overlap a new level @xmath8 child node would either be old children of the same parent or be old children of the parent s neighbors",
    ". the parent would already be aware of its old children , and the old children of the parent s neighbors would have been previously communicated when establishing the neighbor relationships of the parent s old children .",
    "this process is summarized in table [ disttreealg ] .",
    "many if not all current amr codes tend to perform grid updates across all levels in a prescribed order that traverses the levels of the amr hierarchy in a sequential manner .",
    "thus the code begins at the base grid ( level 0 ) , moves down to the highest refinement level and then cycles up and down across levels based on time step and synchronization requirements(for a simulation with 3 levels the sequence would be : 0 , 1 , 2 , 2 , 1 , 2 , 2 , 0 ... ) in the top panel of figure  [ threadingfig ] the basic operations of ( p)rolongating , ( o)verlapping , ( a)dvancing , ( s)ynchronizing , and ( r)estricting are shown for each level along with the single ( serial ) control thread .",
    "good parallel performance requires each level update to be independently balanced across all processors ( or at least levels with a significant fraction of the workload ) .",
    "load balancing each level , however , requires the levels to contain enough grids to be effectively distributed among the processors .",
    "such a requirement demands each level to be fairly  large \" in the sense of having many grids or allowing each level s spatial coverage be artificially fragmented into small pieces .",
    "the former situation leads to broad simulations ( large base grid leaving resources for only a few levels of amr ) , while the later situation leads to inefficient simulations due to the fair amount of overhead required for ghost cell calculations .",
    "the problem becomes worse when @xmath42 is large or when using extended ghost cells with refinement ratios @xmath43 .",
    "consider an isolated level @xmath4 grid of size @xmath44 inside of a parent grid on level @xmath4 .",
    "let s assume that the coarsening ratio @xmath15 and that the grid must therefore take two steps each of which requires @xmath45 ghost cells .",
    "on the first step it must update a region that is @xmath46 and then on the second step a region that is @xmath44 for a total of @xmath47 cell updates .",
    "if that @xmath44 grid were split into 4 @xmath48 pieces to be distributed on 4 processors , then instead of 160 cell updates there would now be @xmath49 per processor or 416 total cell updates and the parallelization efficiency for that grid would be @xmath50 at best . even if one uses prolongated time derivatives instead of extended ghost cells ,",
    "there is still a fair amount of overhead for ghost cell computations for small grids and the efficiencies would still be @xmath51 depending on the particular method used .    in the bottom panel of figure  [ threadingfig ]",
    "we show a schematic of the astrobear 2.0 amr algorithm . in this figure",
    "basic operations of ( p)rolongating , ( o)verlapping , ( a)dvancing , ( s)ynchronizing , and ( r)estricting are shown again however this time the level advances are independent and exist on separate threads of computation .",
    "there is an overarching control thread which handles all of the communications and computations required for prolongation , overlapping , synchronizing , and restricting as well as the finest level advances .",
    "each coarser level advance has its own thread and can be carried forward independently with preference being given to the threads that must finish first ( which is always the finer level threads ) .",
    "in addition to relaxing the requirement of balancing every level , the existence of multiple threads allows processors to remain busy when the control thread becomes held up because it needs information from another processor .",
    "for example , while waiting for ghost cell data for level 3 it can work on advancing levels 2 , 1 , or 0 .",
    "the simplest implementation of threading would allow for the operating system kernel to manage the various threads with priority given to the control thread followed by the highest level advance and so on all on the same processor .",
    "this would be considered preemptive prioritized threading within a process .",
    "unfortunately the pthreads library implementation under linux does not allow for non - privileged users to give priorities to threads nor to limit a set of threads to use only a single processor .",
    "gnu portable threads do require the threads to remain on a single processor , but do not allow for preemptive thread scheduling .",
    "threads must schedule themselves cooperatively by yielding control to the scheduler or to each other . while gnu portable threads do allow for priorities to be assigned to threads , it is not terribly useful as threads can cooperatively yield control to the appropriate thread without the need for a scheduler . a more complicated implementation , but one that does not require external libraries , is to manually switch ( at the application level ) between grid advances and the control thread .",
    "this is somewhat difficult to implement as the grid advances must be interruptable and any intermediate variables within the advance stack have to be manually cached by the application .",
    "this caching effectively limits the frequency with which the advance thread can be interrupted . in astrobear 2.0",
    "we have implemented gnu portable threads , manual thread switching ( scheduling ) , as well as the serial version of the amr algorithm .",
    "performance results are given in section  [ results ] .",
    "[ threadingfig ]",
    "as was discussed above , threading level advances removes the need for balancing each level independently and instead allows for global load balancing .",
    "it also and perhaps more importantly allows for consideration of the progress of coarser advance threads when successively distributing the workload of finer grids .",
    "this `` dynamic load balancing '' allows adjustments to be made to finer level distributions to compensate for variations in progress made on ongoing coarser advances .",
    "@xmath52 & current workload assigned to processor @xmath31 on level @xmath4 + @xmath53 & amount of current workload @xmath52 completed + @xmath54 & newly created child workload on level @xmath4 by parent grids on level @xmath5 on processor @xmath31 + @xmath55 & average of @xmath52 across processors + @xmath56 & average of @xmath53 across processors + @xmath57 & sum of @xmath52 across processors . also equals sum of @xmath54 across processors .",
    "+ @xmath58 & desired workload assignments for processor @xmath31 and level @xmath4 + @xmath59 & imbalance in @xmath52 ( @xmath60 ) + @xmath61 & imbalance in @xmath53 ( @xmath62 ) + @xmath63 & predicted remaining workload for entire amr step for processor @xmath31 on level @xmath4 + @xmath64 & current number of remaining steps on level @xmath4 within entire amr step + @xmath65 & workload assigned to processor @xmath66 of children created by grids on processor @xmath31 +    here we give a brief example introducing terminology along the way . also see table [ variablelist ] for a complete list of variables used below .",
    "consider a simulation run on three processors with two levels of refinement with a refinement ratio @xmath15 and let us assume ( arbitrarily ) that there are @xmath67 cells on level 0 distributed among the three processors as @xmath68 $ ] .",
    "each processor may have more then one grid , but here we just count the total number of cells in all of the processor s grids . after spawning the level 0",
    "advance thread , each processor continues along the control thread until it eventually creates new child grids on level 1 . at this point",
    "these new child grids must be distributed .",
    "let s assume that at this point no work has yet been accomplished on the level 0 advance thread ( @xmath69 $ ] ) .",
    "we also assume that the number of new child cells created by grids on each processor is @xmath70 $ ] for a total of ( @xmath71 ) cells created on level 1 to be distributed across all three processors .",
    "if we were independently balancing the workload on level 1 the desired distribution @xmath72 would be constant for each processor @xmath73 where @xmath74 is the average new child workload .",
    "for our example this would give a distribution of @xmath75 $ ] .",
    "there is however a workload imbalance on level 0 of @xmath76 $ ] that we would like to compensate for . in what follows",
    "we show how we can successively rewrite @xmath72 in new forms which allow us to better anticipate the best distribution to account for computation within and across levels .",
    "if we completely compensate for the level 0 imbalance @xmath77 then the desired workload distribution for level 1 would be @xmath78 $ ] .",
    "if the level 1 workload is distributed that way , then while processor 1 waits for processors 2 & 3 to complete their 20 level 1 updates , it should have time to complete 12 of its level 0 updates so @xmath79 $ ] . at this point",
    "the remaining workload on level 0 would be balanced at @xmath80 $ ] .",
    "however we need to take an additional step on level @xmath9 .",
    "thus while the remaining level 0 workload is balanced , we see an opportunity to redefine @xmath72 in a way that accounts for this additional level 1 step",
    ". we would like to redistribute the grids on level 1 such that they are balanced at @xmath81 $ ] before taking the second step to avoid processor idling .",
    "this redistribution has an additional cost that can easily be avoided if the imbalance on level 0 is first weighted by the ratio of the number of level 0 steps @xmath82 to level 1 steps @xmath83 remaining . during the first distribution of level 1 grids",
    "there are 2 remaining level 1 steps @xmath84 and one remaining level 0 step @xmath85 .",
    "thus we redefine @xmath86 by modifying equation [ verysimpledistributioneq ] to @xmath87 this would give us @xmath81-[+4,-2,-2]=[12,18,18]$ ] .",
    "now while processor 1 waits for processors 2 & 3 to complete their 18 level 1 updates , it should have time to complete 6 of its level 0 updates effectively reducing the imbalance by a factor of 2 . after completing the first level 1 advance ,",
    "the second distribution of level 1 grids must somehow take into account the coarser workload accomplished .",
    "to do so we include a completed workload imbalance @xmath88 $ ] into equation [ distributioneq2 ] .",
    "@xmath89 note that now with @xmath90 we get the same desired distribution of @xmath91 $ ] for the second level 1 step as for the first .",
    "we could have just used the remaining workload @xmath92 to calculate a remaining workload imbalance @xmath93 and then used equation [ distributioneq2 ] but this would be the same as modifying equation [ simpledistributioneq ] to be @xmath94 . since in our example @xmath95 only when @xmath96 , it makes no difference , but when there are more then two levels of refinement this approach can lead to unnecessary shuffling of grids between processors even with static meshes . for a more detailed analysis see [ staticdist ] .    when there are multiple levels of refinement , the workloads on many coarser levels must be taken into account .",
    "the generalization of equation [ simpledistributioneq ] is given by @xmath97    a more convenient form of equation [ distributioneq ] can be obtained if we replace @xmath98 and @xmath99 with their expansions and then group terms by whether they involve averages across processors or local quantities on a particular processor .",
    "we then arrive at the final general form of the distribution equation    @xmath100    where @xmath101 .",
    "note that @xmath102 is the predicted remaining work on all coarser levels and that if the actual distribution matches the desired distribution ( ie @xmath103 ) then the total predicted remaining workload including the current level is    @xmath104    that is , each distribution attempts to equalize the predicted remaining work over the entire amr advance .",
    "it is not always possible to distribute a discrete set of grids perfectly so that @xmath103 , however small differences on coarser levels can be corrected for on finer level distributions . on the finest level the grids",
    "are artificially split to balance out all of the coarser level imbalances as described below .",
    "thus at each distribution of level @xmath4 grids , processors must collect the new child workloads @xmath54 to calculate @xmath105 ) , and @xmath63 to calculate the desired workload distributions @xmath58",
    ". then the new child workloads @xmath54 are partitioned over @xmath106 as shown in figure  [ partition ] to give the desired workload @xmath65 assigned to processor @xmath66 of children created by grids on processor @xmath31 .",
    "note that @xmath107 and @xmath108 .",
    "each processor @xmath31 can then determine from which ",
    "processors @xmath66 it should expect to receive new child grids from ( @xmath109 ) as well as which `` child '' processors @xmath110 it should distribute new grids to ( @xmath111 ) here the term child processor does not refer to the addition of new processors ( unlike the addition / creation of new child grids ) but only those existing processors which will receive child grids from other processors ( referred to here as  parents \" ) .",
    "when distributing child grids among child processors , each processor @xmath31 will try to assign to each child processor @xmath66 a collection of level @xmath4 grids with a combined workload @xmath65 .",
    "however , arbitrarily partitioning a few discrete grids into exact sizes is often not possible , and the actual workload assigned @xmath112 to each child processor may be different from the desired workload @xmath65 . as a result , the combined child workload from all of a processor s parent processors @xmath113 may be different from the desired workload @xmath114 .",
    "this results in some variation in @xmath115 between processors and a temporary predicted load imbalance .",
    "however , if this variation is small , it can be corrected on finer level distributions or on the next round of level @xmath4 distributions without adversely effecting performance , provided that there remains enough computational work on coarser advance threads to buffer the imbalance .     of new level @xmath4 child workloads @xmath116 onto desired workload shares @xmath117 .",
    "shades of gray correspond to processor rank ( 0 - 3).,title=\"fig : \" ] [ partition ]      before discussing the details of the distribution of new refined grids , it is instructive to first consider the simpler problem of distributing newly refined cells as is the case for cell - based amr . after newly refined cells are created , a space filling curve is generally used to sort the new cells into a prescribed order based on their position along the curve @xcite .",
    "in addition the processors are ordered based on the topology of the network .",
    "then the ordered list of cells can be partitioned over the ordered list of processors in a manner similar to that used in constructing the @xmath65 in figure  [ partition ] except that now instead of globally distributing @xmath54 over @xmath58 to determine @xmath65 we are locally distributing the individual workload of each child cell @xmath118 over the child processor allocations @xmath65 to determine the processor allocations for each individual child cell @xmath119 .",
    "most cells will have only one nonzero allocation @xmath120 and will be assigned to that processor @xmath66 and if a cell has more than one nonzero allocation then the processor with the largest allocation could be used .",
    "this `` rounding '' will only lead to load imbalances that are of order @xmath121 .",
    "this ordering attempts to keep communication between neighboring cells to a minimum . for the partitioning to work",
    ", however , each processor needs to know the global sort index of each of its newly refined cells . sorting the new cells",
    "could be done using traditional sorting algorithms provided each processor was aware of all of the cells . a better alternative , is to maintain a strict grouping of cells by sort order on each processor as shown in figure  [ hilbertfig ] .",
    "that is , a cell with a lower sort index will never be found on a higher numbered processor and vice versa . because of the fractal nature of many space filling curves like the hilbert curves shown in figure  [ hilbertfig ] , the ordering of any two child cells of different parent cells will be the same as that of its parents .",
    "therefore if the parent cells obey a strict grouping by sort order , the child cells will as well .",
    "processors therefore only need to determine the local sort order of their child cells .",
    "they can then determine the global sort order if they know the new child cell counts of every other processor .",
    "an example of these cell counts can be seen in figure  [ hilbertfig ] as well as local and global index of each level 0 and level 1 cells .",
    "the global index of each cell can be constructed by adding the cell counts of lower processors to the local index .",
    "the fact that child cells will obey the same ordering of their parents can be used to locally sort child cells by first sorting the parent cells and then locally sorting the child cells among their siblings .    in cell based amr each processor",
    "can usually partition its newly refined cells among  child processors \" with very little load imbalance since each processor typically has 1000 s of cells to distribute over a few processors .",
    "not being able to split a cell might result in imbalances of few tenths of a percent .",
    "this is not the case for patch based amr .",
    "processors will still have 1000 s of cells , but they will be grouped into a few grids of various sizes . distributing a few grids of various sizes among a few child processors with various workload assignments will in general lead to significant imbalances . often what is done in grid - based amr is a more global distribution of all new child grids among all processors using a knapsack algorithm .",
    "this type of distribution does not rely on a space - filling curves or any global ordering of new grids although it may take the communication costs into account .",
    "this however , requires global knowledge of the amr tree which can result in memory issues for simulations run on many processors . in astrobear",
    ", the amr tree is distributed so a global knapsack type algorithm is not feasible .",
    "instead astrobear uses an approach quite similar to that used for cell - based amr .",
    "astrobear avoids the issues with load imbalances not by artificially fragmenting grids into small pieces , or by locally shuffling the grids in a local knapsack algorithm , but instead through the use of level threading described above in which load imbalances can be corrected for by subsequent distributions .",
    "this approach permits a strict grouping of grids on processors by their sort order .",
    "this is true to at least to the extent that grids obey the same type of child order inheritance true for individual cells .",
    "that is , while the order of two child cells will always be the same as that of their parent cells , it may not hold for grids .",
    "cells have a unique distance along the space filling curve , however grids contain many cells each with different distances . a distance for the grid along the curve",
    "can be approximated by averaging that of all of its cells , or some subset of cells that appropriately samples the grid ( ie the four corners or the center most subset ) .",
    "however this fuzziness in grid distances can result in child grids occasionally being ordered differently then their respective parents .",
    "however , this does not appear to be a major problem and would only result in slight increases in neighbor communication .",
    "we note that instead of using a space - filling curve to order the grids , we could implement a local knapsack algorithm in which each processor distributes its own newly created children among its own child processors in an optimal fashion , however we have not investigated whether this improves performance compared to the ordering provided by a space - filling curve",
    ".      occasionally splitting of a child grid into various pieces to accommodate its processor allocations @xmath119 is desired and there are two such instances in astrobear .",
    "first , since most of the workload resides on the finest level grids , imbalances can sometimes be too large to be compensated for by the coarser threads .",
    "for this reason astrobear always splits the finest level grids when necessary to achieve global load balancing .",
    "in general the number of finest level grids split will be of the order of the number of processors .",
    "since the splitting occurs on the finest level , such fragmentation will not result in subsequent artificial fragmentation of higher level grids .",
    "second , since the base grid in amr simulations can be quite large it can often contain more then a single processor s share of the entire workload .",
    "for this reason the base grid is also split among the processors .",
    "the algorithm for splitting begins by taking a single grid and its non - zero processor allocations @xmath120 that were determined by locally partitioning the set of @xmath65 over the individual child workloads @xmath122 .",
    "this then gives the share of each individual child grid that should be assigned to each child processor .",
    "it then attempts to split the grid into pieces each with a workload equal to the processor allocations @xmath119 .",
    "additionally it attempts to construct the pieces so that they are properly ordered along the hilbert curve .",
    "this is done through a recursive bisection algorithm described below :    1 .",
    "divide the list of weights into two pieces as equal as possible .",
    "for example if the list of weights were [ .1 .2 .1 .3 .3 ] they would be split into [ .1 .2 .1 ] and [ .3 .3 ] 2 .",
    "determine the two possible split points along each dimension that break the grid into two proportionate pieces ( ie either [ .4 .6 ] or [ .6 .4 ] ) .",
    "3 .   for each of the possibilities evaluate the combined advance costs of the resulting grids . this will in general favor splits along the longest grid dimension .",
    "4 .   if several possible splits have comparable combined advance costs then select the one that results in the largest difference in space - filling values between the two pieces with the correct sign . 5 .",
    "if there were only two weights then we are done . otherwise , continue to recursively bisect the left and right pieces .    for a large fixed grid simulation ,",
    "the above algorithm applied to a square base grid in 2d ( or a cube in 3d ) on @xmath123 processors ( @xmath124 in 3d ) , will yield a distribution that traces out a level @xmath30 hilbert curve .",
    "[ hilbertfig ]",
    "it is challenging to characterize the performance of an amr code in general terms .",
    "this is because the performance on any given problem will depend on many factors that are specific to that problem such as the number of levels of refinement , the number of cells on each level ( filling fractions ) , their spatial distribution , and the number of processors used . here",
    "we give a few examples to demonstrate the complexity of the problem .    first consider a fixed grid 16x16 problem run on 5 processors .",
    "ideally the 16x16 grid could be split into 5 even rectangular pieces to distribute among 5 processors .",
    "however , the best arrangement still leaves one processor with 55 cells which is 7% more then the ideal average of 51.2 effectively limiting the performance to 93% .",
    "we could split the grid into more than 5 pieces so that the maximum number of cells on any processor is closer to the mean , but the additional overhead with having smaller grids would likely defeat the purpose .    the same problem however can be easily broken into 16 even pieces ( each 4x4 ) to be perfectly distributed among 16 processors .",
    "however , running the problem on 16 processors instead of 1 will not generally allow the problem to complete in 1/16 of the time for two primary reasons .",
    "first , before each 4x4 grid can update , it most share ghost data with the surrounding grids and there is some network latency in transmitting ghost data between grids .",
    "additionally , while updating the 4x4 grid , calculations will be done using the ghost cells that will be repeated on the neighboring processors .",
    "this redundant calculation is not required if there is only one 16x16 grid . thus updating 16 4x4 grids",
    "requires more calculations then updating 1 16x16 grid , so the cpu time will in general increase as more and more processors are used for the same problem even if the geometry of the problem allows for even distribution .",
    "now consider the same 16x16 grid on 16 processors and lets assume that there are 16 refined regions ( each 2x2 ) that just happen to lie in the center of each 4x4 grid .",
    "these child grids will not be adjacent to other child grids and will only need to communicate with their parent grid .",
    "if each child grid is distributed on the same processor containing the parent , then the communication pattern is the same as that of the fixed grid run and the performance should actually be better because each processor now has more work that can be done independently .",
    "if however , instead of having 16 2x2 regions marked for refinement , the central 8x8 region was marked for refinement , then the communication pattern of the level 1 grids would be very similar to that of the level 0 although there would now be some additional communication between parent and child grids .",
    "thus the simplest and fairest test of an amr code is to keep the number of cells per processor fixed as well as keeping the refined regions spatially connected .",
    "furthermore , if we keep the filling fractions at 1/4 for 2d ( 1/8 for 3d ) then each level will have the same number of cells and the number of cells per level per processor will be a constant .",
    "one still has the freedom in choosing the number of cells per leve per processor and this can have dramatic effects on the scaling .",
    "larger grids will in general allow for better scaling .",
    "a fair test will choose a grid size that will allow for a simulation to complete in a reasonable wall time . to that end we chose a resolution low enough so that a simulation on 10,000 cores would only take 8 hours per crossing time .",
    "this works out to be @xmath125 cells per processor for the fixed grid run .",
    "if we kept the base grid fixed and added 1 additional level of amr , the simulation would take 3 times as long .",
    "so we have to reduce the base grid size as we increase the maximum level of amr to keep the overall walltime per crossing time fixed at 8 hours .",
    "for 1 additional level of amr this works out to be @xmath126 cells per processor per level , and @xmath127 , @xmath128 , & @xmath129 for 2 , 4 , & 8 levels of amr respectively .",
    "as the grids get smaller we expect the additional overhead related to ghost zones to reduce the overall speed - and the scaling will likely also suffer as well .",
    "figure [ weakscaling ] shows the results of weak scaling done on kraken .",
    "for each level of amr we ran simulations on 12 , 48 , 192 , 768 , 3072 , & 12288 cores using 5 different options for the load balancing / threading scheme .",
    "for each simulation we calculated the total number of cell updates divided by the cpu time to get the efficiency of the code in actually solving the problem .",
    "other metrics such as the degree to which a processor remains idle , while interesting , are less relevant to the performance of actual simulations .",
    "the solid black line shows the traditional serial amr approach which in which each level is independently balanced out of necessity .",
    "the solid light and dark grey line shows the results using a scheduling approach to threading in which processors schedule rendevouz times before switching to coarser advance threads .",
    "the light line is for a simulation in which each level is balanced independently and the dark line is for a simulation that attempts to balance across levels or globally .",
    "the dashed light and dark grey lines are for the simulations that actually use a threading library .",
    "this allows for much finer grained switching between the control thread and the coarser advance threads .",
    "again the light line is for balancing within a level , and the dark line is for balancing across levels or globally .",
    "first there is a secular drop in performance for any number of cores in terms of cell updates per second per cpu as we increase the number of levels of amr .",
    "this is due to the decrease in the average grid size required as the number of amr levels increases to keep the wall time constant .",
    "the fixed grid run has typical grid sizes of @xmath125 while the run with 8 levels of amr has typical grid sizes of only @xmath129 .",
    "advancing cells within a @xmath125 grid can be done 4 times as efficiently as grids with only @xmath129 cells due to the overhead with regards to ghost zones etc .",
    "next if we look at the serial amr performance it turns out to be quite good with a worse case performance drop of only @xmath130 from 12 to 12288 cores .",
    "we attribute this excellent performance to a fully parallelized tree and the presence of only a few scalar quantities that must be gathered across processors .",
    "we also note in the upper right plot in figure [ weakscaling ] that for fixed grid there is no difference between the various approaches as expected since there is only 1 thread and no difference between local and global balancing .",
    "there is also a noticeable trend with regards to global vs. local balancing for both the scheduled and threaded approach .",
    "the lower right plot of figure [ weakscaling ] shows most clearly this effect . for 12 cores , which on kraken are on a single node , the globally balanced approach out performs the locally balanced approach .",
    "the globally balanced approach - while allowing for larger average grid sizes and therefore more efficient updating - reduces the degree to which various prolongation / restriction / synchonization operations are parallelized while also requiring more intercore communication .",
    "when the cores reside on the same node , this communication can happen quickly and does not degrade the performance as much as the larger grid sizes improves the performance .",
    "however by 48 cores the additional communication has negated any advantage of having larger grid sizes , and trying to keep grids as large as possible to avoid this sort of overhead does not seem to be effective on large numbers of cores .    finally if we compare the three level balanced approaches , we see that both the scheduled and the truly threaded approaches out perform the serial amr approach by @xmath131 . there is a tendency for the fully threaded level balanced approach to drop in performance at 12288 cores , but this is likely due to the crude implementation of a non - blocking barrier .",
    "the idea behind the threaded approach is that every communication is non - blocking and that while waiting for a message to complete coarser advance threads can be running while periodically checking to see if the communication had completed .",
    "however global reductions can not be done using non - blocking calls .",
    "we dealt with this by writing a crude non - blocking barrier that could be executed before any global reductions and that would ensure every processor was ready to procede with the global mpi call .",
    "the implementation involved every processor sending a ready signal to processor 0 , followed by every processor waiting for a continue signal from processor 0 .",
    "this all - to - one followed by a one - to - all would likely cause significant performance degradation at 12288 cores .",
    "there are more elegant solutions that could be implemented , such as scheduling these barriers or writing a more efficient branching communication pattern .",
    "for @xmath132 levels of amr respectively .",
    "each plot shows the performance of the serial amr approach , scheduled advances with level by level balancing as well as global balancing , and the threaded approach with level by level and global balancing .",
    "the y - axis shows the number of cells updated per processor per second.,title=\"fig:\",scaledwidth=100.0% ] [ weakscaling ]",
    "as discussed in the introduction adaptive mesh refinement methods were designed to allow high resolution simulations to be carried out at low computational cost .",
    "highly parallel systems and their algorithms carried the same promise .",
    "the parallelization of amr algorithms is , however , not straight forward however and there have been a number of different approaches @xcite , to solve the problem . while parallelization of a uniform mesh demands little communication between processors , amr methods can demand considerable communication to maintain data consistency across the unstructured mesh as well as shuffling new grids from one processor to another to balance workload .    in this paper",
    "we have described our attempt to design and implement a new strategy for amr parallelization with an eye towards running codes on large machines with @xmath133 processors .",
    "we have found that a threaded approach to the amr algorithm significantly improves performance by allowing processors to remain busy while waiting for messages as well as to dynamically adjust distributions based on the progress of ongoing coarser grid updates .",
    "we have also shown that a distributed tree algorithm significantly reduces the amount of memory required ( and corresponding communication ) for simulations run on many processors with modest grid sizes .",
    "we believe this threaded , distributed tree approach described in this paper holds considerable promise as a methodology for implementing amr on ever - larger numbers of processors .",
    "future work will entail elaborating refinements to the methods as we experiment with its implementation on different classes of problems and machines",
    ".    1    balsara , d.  s. 2001 , journal of computational physics , 174 , 614    brian van straalen , phil colella , daniel t. graves , and noel keen .",
    "petascale block - structured amr applications without distributed meta - data    m. l. simone , r. m. loy , m. s. shephard and j. e. flaherty    tiankai tu , d. r. ohallaron , and o. ghattas    b. hariharan , a srinivas , 31:311  331 , 2005    a.m and khokhlov .",
    "fully threaded tree algorithms for adaptive refinement fluid dynamics simulations .",
    ", 143(2):519  543 , 1998 .",
    "marsha  j berger and joseph oliger .",
    "adaptive mesh refinement for hyperbolic partial differential equations .",
    ", 53(3):484  512 , 1984 .",
    "berger and p.  colella .",
    "local adaptive mesh refinement for shock hydrodynamics .",
    ", 82(1):64  84 , 1989 .",
    "a.  j. cunningham , a.  frank , p.  varnire , s.  mitran , and t.  w. jones . . ,",
    "182:519542 , june 2009 .",
    "p.  macneice , k.  m. olson , c.  mobarry , r.  de fainchtein , and c.  packer . .",
    ", 126:330354 , april 2000 .",
    "j.  mellor - crummey , d.  whalley , and k.  kennedy .",
    "improving memory hierarchy performance for irregular applications . , 29(3 ) , june 2001 .",
    "b.  w. oshea , g.  bryan , j.  bordner , m.  l. norman , t.  abel , r.  harkness , and a.  kritsuk . . ,",
    "march 2004 .",
    "u.  ziegler . .",
    ", 179:227244 , august 2008 .",
    "in table [ geninheritance ] we outline the procedure for inheriting neighbors , overlaps , from parent relationships for arbitrary number of refinement ratios . and in table [ disttreealg ] we outline the communication steps for maintaining the distributed tree .     & node s child s preceding overlaps & node s child s succeeding overlaps + 1 & node s neighbors 1@xmath40 children & node s preceding overlaps r@xmath134 children & node s neighbor s 2@xmath41 children + 2 & node",
    "s neighbors 2@xmath41 children & node s neighbors 1@xmath40 children & node s neighbors 3@xmath135 children + 3 & node s neighbors 3@xmath135 children & node s neighbors 2@xmath41 children & node s neighbors 4@xmath134 children + ... & ... & ... & .... + r-1 & node s neighbors r@xmath134 - 1 children & node s neighbors r@xmath134 - 2 children & node s neighbors r@xmath134 children + r & node s neighbors r@xmath134 children & node s neighbors r@xmath134 - 1 children & node s succeeding overlaps 1@xmath40 children +         +    1 & _ receive _ new grids & nodes along with their parents , neighbors , and * preceding * overlaps from parent processors & _ receive _ * succeeding * overlaps from parent processors + 2 & + 3 & determine which remote * preceding * nodes might have children that would overlap with its own children and _ send _ the relevant children info & determine which remote * succeeding * nodes might have created children that would overlap with its own children and _ send _ the relevant children info + 4 & + 5 & + 6 & determine which local * preceding * nodes have children that overlap with its own & determine which local * succeeding * nodes have children that overlap with its own + 7 & + 8 & _ receive _ children from remote * preceding * nodes and determine which of the nodes children overlaps with its own & _ receive _ children from remote * succeeding * nodes and determine which of the nodes children overlaps with its own .",
    "+ 9 & for each remote child , _",
    "send _ the child grid s data as well as information about its parents , neighbors , & * preceding * overlaps .",
    "& for each remote child , _ send _ the child s * succeeding * overlaps .",
    "+   + 10 & + 11 & + 12 & + 13 & + 14 & + 15 & +",
    "consider what happens if the mesh is static so that the average workload per level @xmath55 remains constant .",
    "initially there will be a desired set of distributions @xmath58 that will result in an actual set of distributions @xmath52 . on the finest level they will agree , but on coarser levels there will be some difference between @xmath58 and @xmath52 .",
    "it is desirable that each successive desired distribution should match the original so that each successive actual distributions will also remain constant and grids will not be unnecessarily shuffled around .",
    "for example , consider the desired distribution of the highest level @xmath18 after the first level @xmath18 advance . assuming the highest level workload follows the desired workload , @xmath136 .",
    "the amount of time each processor will have waiting for every other processor to complete the level @xmath18 advance is just @xmath137 .",
    "if this time is spent advancing coarser grids , then the new sum of work done on coarser grids @xmath138 $ ] .",
    "this gives a new predicted remaining work @xmath139 $ ] and a new desired distribution    @xmath140}{s_l-1 } \\nonumber \\\\ & = & \\overline{g_l}-\\frac{\\eta_l^p-\\overline{\\eta_l } + g_l^p-\\overline{g_l}}{s_l-1}=\\overline{g_l}-\\frac{\\eta_l^{p}-\\overline{\\eta_l^{p}}}{s_l } = d_l^p \\nonumber \\\\\\end{aligned}\\ ] ]    more generally we can consider the distribution of any given level @xmath4 after completing @xmath141 steps leaving @xmath142 remaining steps .",
    "@xmath143 if @xmath144 , then the desired distribution will remain fixed .",
    "@xmath145 the workload completed on the coarser grids will be the difference between the maximum work completed on all of the previous grids by any processor and the local work completed on all of the previous grids .",
    "this then gives @xmath146 } - \\sum_{l'=0}^l(2^{l'}-s_{l'})g_{l'}^p \\nonumber \\\\\\end{aligned}\\ ] ] and we have @xmath147    now @xmath148 it the initial entire predicted workload and is balanced by the highest level distribution so the value on any processor equals the average and the first term cancels .",
    "the second term can be re - written by recognizing that at the moment we are distributing level @xmath4 , for @xmath149 , @xmath150 .",
    "this gives @xmath151 therefore if the mesh is static , the distribution will remain static as well ."
  ],
  "abstract_text": [
    "<S> current adaptive mesh refinement ( amr ) simulations require algorithms that are highly parallelized and manage memory efficiently . as compute engines grow larger , amr simulations will require algorithms that achieve new levels of efficient parallelization and memory management . </S>",
    "<S> we have attempted to employ new techniques to achieve both of these goals . </S>",
    "<S> patch or grid based amr often employs ghost cells to decouple the hyperbolic advances of each grid on a given refinement level . </S>",
    "<S> this decoupling allows each grid to be advanced independently . in astrobear </S>",
    "<S> we utilize this independence by threading the grid advances on each level with preference going to the finer level grids . </S>",
    "<S> this allows for global load balancing instead of level by level load balancing and allows for greater parallelization across both physical space and amr level . </S>",
    "<S> threading of level advances can also improve performance by interleaving communication with computation , especially in deep simulations with many levels of refinement . while we see improvements of up to @xmath0 on deep simulations </S>",
    "<S> run on a few cores , the speedup is typically more modest ( @xmath1 ) for larger scale simulations . to improve memory management </S>",
    "<S> we have employed a distributed tree algorithm that requires processors to only store and communicate local sections of the amr tree structure with neighboring processors . using this distributed approach we are able to get reasonable scaling efficiency ( @xmath2 ) out to 12288 cores and up to 8 levels of amr - independent of the use of threading . </S>"
  ]
}