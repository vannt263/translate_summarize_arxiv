{
  "article_text": [
    "one of the main challenges of the astroparticle physics reasearch is to determine the chemical composition of the cosmic rays that reach the earth . at the highest cosmic ray energies , analysis of composition",
    "must be made with indirect methods by using the evolution of cosmic ray showers in the atmosphere . in this note",
    ", we use four different methods to estimate the composition given a single variable ( e.g. the @xmath1 distribution ) by using bayesian methods .",
    "a similar work , using monte carlo techniques , to study the efficiency of different discriminators can be found in @xcite .",
    "the note is as follows : in section [ methods ] we discuss the four different methods and we apply them in simple analytical cases in section [ analytical ] . in section [ examples ] , we apply the different methods to two physical cases signal to noise discrimination and two overlapped signals . finally , we evaluate the methods for the specific example of @xmath2 distributions with realistic probability densities and we get a possible composition of the cosmic rays in section [ xmax ] .    to be concrete",
    ", we will consider a two composition scenario .",
    "although the methods discussed can be easily generalized to include more than two distributions .",
    "consider the following problem .",
    "a given data variable is extracted from two different probability distributions with a `` composition '' fraction @xmath3 , ( @xmath4 ) so that the joint probability distribution is given by @xmath5 the probability distributions @xmath6 are known and the problem consists in determining the composition fraction @xmath3 from the measurement of @xmath7 data points @xmath8 , @xmath9 .",
    "if @xmath3 was known , the probability of getting the data @xmath10 is given by @xmath11 where @xmath12 is any prior information we have about the problem , including the prescription of the probabilities @xmath13 .",
    "here we are implicitly assuming that the different data points are independent . using bayes s theorem @xcite",
    ", we can obtain the posterior probability for @xmath3 given the data @xmath14 @xmath15 is the probability of obtaining the given data independently of any value of @xmath3 and here acts as a normalization constant .",
    "@xmath16 is the prior probability for @xmath3 . in our problem",
    ", astrophysical input may give information on the cosmic ray composition and give preference for , say , proton domination . in the absence of any information a flat distribution gives good results .",
    "in the following we will use @xmath17 but all our results will be valid for other choices of prior probabilities .",
    "therefore , we can write eq.([bayes ] ) as @xmath18 where @xmath19 is a normalization constant . in some problems instead of",
    "( [ prob ] ) , where all the data points are given , one has data binned in the variable @xmath20 . in that case",
    "the equation reads @xmath21 where the data now is @xmath22 , the number of events in the bins @xmath23 with center values @xmath24 , and @xmath25 is the integral on the bin of the probability density .",
    "although binning the data makes the problem somehow easier , it wastes information .",
    "( [ prob ] ) ( or alternatively eq . ) contains all the information we have about our problem .",
    "estimation of the composition fraction reduces to the choice of a `` best estimator '' .",
    "now we examine the different choices which are currently used in the literature .",
    "* @xmath26 + we can calculate the mean of the data points and choose @xmath3 such that it coincides with the mean value of the distributions , _",
    "i.e. _ if @xmath27 , then @xmath28 if we define @xmath29 , then we get @xmath30 eq . ( [ alpha_means ] ) has a simple analytical form and is easy to evaluate for any distribution ( provided it has first moments ) .",
    "this is a useful advantage when working with the first few moments ( see e.g. @xcite ) .",
    "however , it can give unphysical results ( ( @xmath31 , @xmath32 ) and gives the largest deviation with respect to the true value for all studied estimators .",
    "* @xmath33 + alternatively , one can choose as the best estimator , the value that maximizes the posterior probability . @xmath34 since @xmath35 is positive an equivalent alternative is to maximize the logarithm of @xmath36 .",
    "then @xmath37 this is the maximum likelihood estimation .",
    "it is known to give very good estimation of @xmath3 in almost all cases , even for small number of events .",
    "it has the disadvantage that an analytical solution is possible only for very small number of events or bins .",
    "this method is used , for instance , in the standard package tfractionfitter of root @xcite .",
    "usually , the solution is found by numerically searching for the maximum of eqs .",
    "( [ ml],[mllog ] ) . * @xmath38 + if the number of events is large , one expects a well defined peak distribution in @xmath3 . near the maximum of",
    "the distribution one can approximate this distribution by a gaussian .",
    "binning the data we can construct a @xmath0 variable for the problem @xmath39 where @xmath40 is the number of data events in bin @xmath41 and @xmath42 is the probability of having an event in bin @xmath41 for a given @xmath3 .",
    "the optimal value of @xmath3 can be found minimizing the @xmath0 @xmath43 the solution of eq .",
    "( ) has the advantage of being an analytical and relatively simple expression @xmath44 it is an asymptotic limit ( for @xmath7 and @xmath40 large ) of the maximum likelihood method and , as such , gives very good results in this limit .",
    "* @xmath45 + once we know the probability density function for @xmath3 , we can obtain the mean value of the distribution @xmath46 although this estimator is not much used , we will shown below that it gives the best performance in most cases .",
    "it has the disadvantage of being difficult to evaluate analytically but for the simplest cases .",
    "* @xmath47 + any other estimator could provide sensible results . as an example , the median of the posterior probability , defined by @xmath48 it is well known that the median is a robust estimator , being invariant against a large set of transformations of the probability distributions . however , it is difficult to evaluate both analytically and numerically .",
    "therefore we do not consider it any further .",
    "the simplest problem of discrimination is the following : assume that the two distributions @xmath49 and @xmath50 are totally separated ( _ i.e. _ they do not overlap ) .",
    "in that case , the actual shape of @xmath51 and @xmath52 is irrelevant and one can bin the data in just two bins @xmath53 such that all the probability is concentrated in either bin @xmath54 or @xmath55 .",
    "so , let the two probability functions be @xmath56 @xmath57 denoting @xmath58 and applying eq .",
    "( [ prob ] ) , then @xmath59 \\label{prob_ana}\\ ] ] where @xmath7 is the number of events with @xmath60 and @xmath61 is the number of events with @xmath62 , and @xmath63 the total number of events .",
    "this is just a problem of determining the probability of having heads or tails in a ( possibly ) loaded coin given the number of heads and tails in an experiment . as could be expected , it is just a binomial distribution .    by direct calculation ,",
    "one obtains @xmath64 we obtain as the estimation of the probability of `` head '' events , the fraction of head events observed . on the other hand the mean @xmath3 gives @xmath65",
    "although this result may be surprising at first sight it is a well known result in the literature .",
    "it is known as laplace s succession rule .",
    "one may notice that in the limit @xmath66 with @xmath67 fixed one recovers eq . .",
    "note that if @xmath68 , then @xmath69 and all the methods are indefinite except the mean value which gives @xmath70 .",
    "this is just the mean value of the prior probability . if @xmath71 , then either @xmath69 or @xmath72 , which would give either @xmath73 or @xmath74 .",
    ". gives @xmath75 or @xmath76 . in section [ examples ]",
    "we show numerically this phenomenon for a more realistic model .      for a more interesting case , consider now the previous example but with a ( possibly small ) contamination between both distributions @xmath77 @xmath78",
    "so that there is a ( small ) probability of a event of type 1 ( `` heads '' ) to be identified in the bin 2 ( `` tails '' ) and vice - versa . the posterior probability , after measuring @xmath79 total events with @xmath80 of type 1 and @xmath81 of type 2 is @xmath82^{n_1 }    \\left [ \\alpha \\epsilon + ( 1-\\alpha )    ( 1-\\delta )   \\right ] ^{n_2}.   \\label{prob_ana_cont}\\ ] ] after some algebra one obtains again @xmath83 .",
    "\\label{coinbest_cont}\\ ] ] the mean value of @xmath3 has not a simple analytical expression .",
    "it is given by @xmath84 where @xmath85 is the incomplete beta function @xcite @xmath86 for @xmath80 and @xmath81 integers @xmath87 is a polynomial in @xmath20 .",
    "one can show that the above equation gives always physical values @xmath4 , even for degenerate cases .",
    "although this model is rather simplistic , it has all the ingredients found in actual cases .",
    "one can interpret eq .",
    "rather easily , the term @xmath88 subtract the expected fraction of events of type 2 which fall into bin 1 . on the other hand",
    "the factor @xmath89 is a measure of the fraction of well identified events .",
    "it is also a measure of the overlapping of the two distribution .",
    "as we will see below , this is a general characteristic of the problem .",
    "another interesting point of eq .",
    "is the fact that it can produce unphysical results . if @xmath90 , the expected fraction is negative",
    "this is so because even for @xmath91 , we expect a number of events in the first bin of @xmath92 .",
    "finally , one can see that the case @xmath93 is ill defined .",
    "but in this case both distributions are equal : no discrimination can be made between the two distributions .",
    "the mean value determination @xmath45 does not suffer from this behavior , always giving physically admissible results . in the case of the two distributions being equal ,",
    "we would obtain @xmath94 , which is easily interpreted .",
    "if the data can not differentiate between the two cases we do not gain any information from the data and the estimation given by our prior is kept .",
    "we now apply the methods discussed previously to several different scenarios . in section [ signal_noise ] , we study a typical problem of signal / noise identification . in section [ two_signals ] ,",
    "we concentrate on the separation of two signals and we study the dependence of the resolution with respect to the distance between the two signals .    a number of distance measures for probabilities has been proposed in the literature . in the appendix [ appendixa ]",
    "we discuss some possibilities and justify the choice of the overlapping area , as our distance . given two distributions @xmath49 and @xmath50",
    "we define the distance between the two distribution as @xmath95 which ranges between 0 and 2 . for",
    "@xmath96 the distributions do not overlap ; for @xmath97 the distributions are equal . for the previous example of heads and tails ,",
    "the distance is given by @xmath98 , which is the pre - factor appearing in .",
    "consider the case of extracting a signal with a well defined peak from events coming from the signal plus a flat noise . to be concrete",
    ", we will choose the following probability density functions @xmath99,\\ ] ] @xmath100.\\ ] ] here @xmath101 $ ] is the range of the variable .",
    "we take the signal to be different from zero in a subrange of this interval , defined by @xmath102 .",
    "@xmath103 is a normalization constant and @xmath104 and @xmath105 are the mean and rms of the gaussian . in the numerical calculations we will choose @xmath106 , @xmath107 , @xmath108 , @xmath109 , and @xmath110 .",
    "the distance @xmath111 for this case is @xmath112 .    in fig .",
    "( [ snpdfs ] ) we show both probability density functions .        as a first numerical evaluation",
    "we calculate the estimated fraction for a true signal fraction of @xmath113 with a fixed number of events of 30 , 300 and 3000 . in fig .",
    "[ sndatas ] we show the data in a typical run .",
    ".,scaledwidth=70.0% ]    in table [ table : signal - noise ] we show the results for all estimators discussed and for the cases with different number of events . in figures",
    "[ snfs ] and [ snchis ] we show the fraction probability and @xmath114 functions obtained .",
    ".results for the signal / noise discrimination . [ cols=\"^,^,^,^,^\",options=\"header \" , ]     in this example , both mean and maximum of the posterior probability give us the same results .",
    "it has been suggested that one can eliminate partially the hadronic model dependence by shifting the mean values of the two distributions so that they coincide @xcite .",
    "however we expect that by centering the distributions one looses information to discriminate between the two compositions . the distance between the two distributions shown in fig .",
    "[ xmaxpdfs ] , if we center the two distributions is @xmath115 to be compared to the actual distance @xmath116 . from our discussion",
    ", therefore , one expects a loss of resolution of the order @xmath117 , _",
    "i.e. _ each event used in the discrimination with centering is worth a factor @xmath118 less than if used without centering .",
    "this has to be compared to the systematic uncertainty due to the hadronic model .",
    "we have studied different estimators for the evaluation of composition fraction between two distributions .",
    "we have shown that the best methods are the maximum and the mean of the probability distribution .",
    "the @xmath0 method gives results comparable to the maximum likelihood estimator , if the number of events is large , but by rebinning the results can be misleading .",
    "also , we obtained the remarkable results that with few events , the mean value of the probability distribution is the best estimator .",
    "we found a measure of distance between the two probabilities which gives us an estimation of the discrimination power for two distributions .",
    "if the distance @xmath111 is small , the discrimination between the two compositions will be poor .",
    "if the distance is large it will be optimal .",
    "we have shown that as a `` rule of thumb '' the discrimination power scales as @xmath119 with @xmath63 the number of events .",
    "generalization of these methods to include an arbitrary number of components is straightforward and will be discussed separately .",
    "we thank j. alvarez - mu~ niz , s. riggi , i. vali~ no , and e. zas for discussions .",
    "we thank alexey yushkov for discussions and for pointing to us the relevance of centering the distributions .",
    "we thank xunta de galicia - consellera de educacin ( grupos de referencia competitivos  consolider xunta de galicia 2006/51 ) ; ministerio de educacin , cultura y deporte ( fpa 2010 - 18410 and consolider cpan - ingenio 2010 ) ; ministerio de economa y competitividad ( fpa2012 - 39489 ) , aspera - augernext ( pri - pimasp-2011 - 1154 ) and feder funds , spain .",
    "we thank cesga ( centro de supercomputacin de galicia ) for computing resources .",
    "there are many measures of distance used in the literature .",
    "a much used measure of distance for probability distributions is the difference between the means @xmath120 where @xmath121 is the mean value of @xmath20 for the two distributions and @xmath105 is a measure of the width of the distributions ( for instance , @xmath122 has been used ) .",
    "it is however too restrictive . if @xmath123 then this distance is zero , suggesting that the two distributions can not be discriminated .    for two square integrable functions one can define the distance @xmath124 this",
    "is much used in physics , but for probability distributions is not useful , since it is not invariant against changes of variables .",
    "a much used distance for probability distributions is the relative entropy distance , also known as the kullback - leibler @xcite metric @xmath125 for us , however , is not the relevant measure to use .",
    "it gives a distance of @xmath126 if there is a no - overlapping region ( @xmath127 and @xmath128 , for instance ) , which is the most relevant case in our problem .",
    "we have found that the best choice of distance is that given by the overlapping area @xmath129 it measures somehow the amount of probability which is not `` separable '' between the two distributions .",
    "it is bounded between 0 and 2 .",
    "@xmath130 implies that the two distributions do not overlap .",
    "@xmath131 means that the two distributions are equal .",
    "999 d. durso for the pierre auger collaboration , `` a monte carlo exploration of methods to determine the uhecr composition with the pierre auger observatory '' , arxiv:0906.2319 [ astro-ph.co ] m.j .",
    "evans and j.s .",
    "rosenthal , `` probability and statistics : the science of uncertainty '' , w.h .",
    "freeman 2009 .",
    "barlow and c. beeston , `` fitting using finite monte carlo samples '' , compu .",
    "commun . 77",
    "( 1993 ) 219 .",
    "m. abramowitz and i.a .",
    "stegun , `` handbook of mathematical functions '' , dover 1972 . p.abreu _",
    "[ pierre auger collaboration ] , `` interpretation of the depths of maximum of extensive air showers measured by the pierre auger observatory , '' jcap * 1302 * ( 2013 ) 026 p. facal for the pierre auger collaboration , `` the distributions of shower maxima of uhecr air showers '' , arxiv:1107.4804 [ astro-ph.he ] .",
    "a. yushkov , private communication .",
    "s. kullback and r.a .",
    "leibler , `` on information and sufficiency '' , ann .",
    "( 1951 ) 79 ."
  ],
  "abstract_text": [
    "<S> in this work we test the most widely used methods for fitting the composition fraction in data , namely maximum likelihood , @xmath0 , mean value of the distributions and mean value of the posterior probability function . </S>",
    "<S> we discuss the discrimination power of the four methods in different scenarios : signal to noise discrimination ; two signals ; and distributions of xmax for mixed primary mass composition . </S>",
    "<S> we introduce a `` distance '' parameter , which can be used to estimate , as a rule of thumb , the precision of the discrimination . </S>",
    "<S> finally , we conclude that the most reliable methods in all the studied scenarios are the maximum likelihood and the mean value of the posterior probability function . </S>"
  ]
}